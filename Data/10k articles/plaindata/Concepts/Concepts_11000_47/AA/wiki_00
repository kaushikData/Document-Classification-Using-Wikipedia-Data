{"id": "105859", "url": "https://en.wikipedia.org/wiki?curid=105859", "title": "Anarchism and violence", "text": "Anarchism and violence\n\nAnarchism and violence have become closely connected in popular thought, in part because of a concept of \"propaganda of the deed\". Propaganda of the deed, or \"attentát\", was espoused by leading anarchists in the late nineteenth century, and was associated with a number of incidents of violence. Anarchist thought, however, is quite diverse on the question of violence. In the name of coherence some anarchists have opposed coercion, while others have supported it, particularly in the form of violent revolution on the path to anarchy. Anarchism includes a school of thought which rejects all violence (anarcho-pacifism).\n\nMany anarchists regard the state to be at the definitional center of structural violence: directly or indirectly preventing people from meeting their basic needs, calling for violence as self-defense.\n\nPerhaps the first anarchist periodical was named \"The Peaceful Revolutionist\", a strain of anarchism that followed Tolstoy's pacifism.\n\nLate in the 19th century, anarchist labor unions began to use the tactic of general strike. This often resulted in violence by both sides and some of the strikes even resulted in the deaths of striking workers, their replacements and security staff.\n\nIn this climate, some anarchists began to advocate and practice of terrorism or assassination, which they referred to as propaganda of the deed. In many cases, newspapers blamed anarchist terrorism on immigrant naïvete, but scholar Richard Bach Jensen explained that \"the emigrant experience may have heightened a pre-existing radicalism or given more precise configuration to its violent expression.\"\n\nAnarcho-pacifism (also pacifist anarchism or anarchist pacifism) is a form of anarchism which completely rejects the use of violence in any form for any purpose. Important proponents include Leo Tolstoy and Bart de Ligt. Mohandas Gandhi is an important influence.\n\nHenry David Thoreau, though not a pacifist himself, influenced both Leo Tolstoy and Mohandas Gandhi's advocacy of Nonviolent resistance through his work \"Civil Disobedience\".\n\nAt some point anarcho-pacifism had as its main proponent Christian anarchism. The first large-scale anarcho-pacifist movement was the Tolstoyan peasant movement in Russia. They were a predominantly peasant movement that set up hundreds of voluntary anarchist pacifist communes based on their interpretation of Christianity as requiring absolute pacifism and the rejection of all coercive authority.\n\n\"Dutch anarchist-pacifist Bart de Ligt’s 1936 treatise \"The Conquest of Violence\" (with its none too subtle allusion to Kropotkin’s \"The Conquest of Bread\") was also of signal importance.\" \"Gandhi's ideas were popularized in the West in books such as Richard Gregg's \"The Power of Nonviolence\" (1935), and Bart de Ligt's \"The Conquest of Violence\" (1937).\nPeter Gelderloos criticizes the idea that nonviolence is the only way to fight for a better world. According to Gelderloos, pacifism as an ideology serves the interests of the state.\n\nAnarchism encompasses a variety of views about violence. The Tolstoyan tradition of non-violent resistance is prevalent among some anarchists. Ursula K. Le Guin's novel \"The Dispossessed\", a fictional novel about a society that practices \"Odonianism\", expressed this anarchism:\n\nOdonianism is anarchism. Not the bomb-in-the-pocket stuff, which is terrorism, whatever name it tries to dignify itself with, not the social-Darwinist economic 'libertarianism' of the far right; but anarchism, as prefigured in early Taoist thought, and expounded by Shelley and Kropotkin, Goldman and Goodman. Anarchism's principal target is the authoritarian State (capitalist or socialist); its principle moral-practical theme is cooperation (solidarity, mutual aid). It is the most idealistic, and to me the most interesting, of all political theories.\n\nEmma Goldman included in her definition of Anarchism the observation that all governments rest on violence, and this is one of the many reasons they should be opposed. Goldman herself didn't oppose tactics like assassination in her early career, but changed her views after she went to Russia, where she witnessed the violence of the Russian state and the Red Army. From then on she condemned the use of terrorism, especially by the state, and advocated violence only as a means of self-defense.\n\nSome anarchists see violent revolution as necessary in the abolition of capitalist society, while others advocate non-violent methods. \nErrico Malatesta, an anarcho-communist, propounded that it is \"necessary to destroy with violence, since one cannot do otherwise, the violence which denies [the means of life and for development] to the workers.\" As he put it in \"Umanità Nova\" (no. 125, September 6, 1921):\n\nAnarchists with this view advocate violence insofar as they see it to be necessary in ridding the world of exploitation, and especially states.\n\nPierre-Joseph Proudhon argued in favor of a non-violent revolution through a process of dual power in which libertarian socialist institutions would be established and form associations enabling the formation of an expanding network within the existing state-capitalist framework with the intention of eventually rendering both the state and the capitalist economy obsolete. The progression towards violence in anarchism stemmed, in part, from the massacres of some of the communes inspired by the ideas of Proudhon and others. Many anarcho-communists began to see a need for revolutionary violence to counteract the violence inherent in both capitalism and government.\n\nAnarcho-pacifism is a tendency within the anarchist movement which rejects the use of violence in the struggle for social change. The main early influences were the thought of Henry David Thoreau and Leo Tolstoy. It developed \"mostly in , Britain, and the United States, before and during the Second World War\". Opposition to the use of violence has not prohibited anarcho-pacifists from accepting the principle of resistance or even revolutionary action provided it does not result in violence; it was in fact their approval of such forms of opposition to power that lead many anarcho-pacifists to endorse the anarcho-syndicalist concept of the general strike as the great revolutionary weapon. Later anarcho-pacifists have also come to endorse the non-violent strategy of dual power.\n\nOther anarchists have believed that violence (especially self-defense) is justified as a way to provoke social upheaval which could lead to a social revolution.\n\n"}
{"id": "57248134", "url": "https://en.wikipedia.org/wiki?curid=57248134", "title": "Andrew Maraniss", "text": "Andrew Maraniss\n\nAndrew Maraniss (pronounced MARE-uh-niss) is an American author, best known for his book \"\"Strong Inside: Perry Wallace and the collision of race and sports in the south\" \", depicting Perry Wallace, the first African-American to play college basketball under an athletic scholarship in the Southeastern Conference (Vanderbilt University) in the 1960s. The book was on the \"New York Times\" best-seller list in both the sports and civil rights categories for four consecutive months. It received the 2015 Lillian Smith Book Award and a Special Recognition Award by the Robert F. Kennedy Book Awards Foundation.\n\nMaraniss attended Vanderbilt University in 1992 on a Fred Russell-Grantland Rice Scholarship for Sports Journalism. He won the school's Charles Foster Alexander Award for excellence in journalism in 1992 and in 2016 was inducted in the Vanderbilt Student Media Hall of Fame. He was designated Vanderbilt's \"Writer-in-Residence\" in 2017. The popularity of the book reoriented his former career in public relations into that of full-time author. Maraniss' father, David Maraniss, is a Pulitzer Prize-winning author and associate editor for the \"Washington Post\".\n\nMaraniss was born in Madison, Wisconsin, the son of David and Linda Maraniss who met there in high school. His grandfather, Elliott Maraniss, was editor of Madison's \"Capital Times\" newspaper in the 1970s. His grandmother, Mary Maraniss, was a 1966 graduate of the University of Wisconsin, Phi Beta Kappa, and book editor for the University of Wisconsin Press. Marannis' father, David, has been associate editor of the \"Washington Post\" since 1977 and won a Pulitzer prize for national reporting in 1993 for his coverage of Bill Clinton's candidacy during the 1992 presidential election. The senior Maraniss said that his son Andrew \"has always been more mature than Linda and I\", a child who would save his candy rather than eat it all at once. When Andrew was young, his father was a budding journalist who found a job at the \"Trenton Times\" in New Jersey, that necessitated the family's move from Wisconsin to the east coast. In the 1970s another job opened up at the \"Washington Post\" so they moved to Washington, D.C. As a youth in the late 1970s, Andrew attended press conferences with his father. When Maraniss was in high school, his father was transferred to Austin, Texas, to serve as southwest bureau chief for the \"Post\".\n\nWhile in high school in Austin, Maraniss was sports editor for the school paper, the Austin High \"Maroon\". He got a job writing sports articles as an intern for the \"Austin American–Statesman\". A poster at school about a college scholarship caught his attention and prompted him to submit his news clippings to apply for it. His writing won him a Fred Russell-Grantland Rice Sportswriting Scholarship to Vanderbilt University in 1988. Sportswriting Hall-of-Famer Fred Russell, for whom the sportswriting scholarship is named, took the four recipients out to lunch a couple of times a year and developed personal relationships with them. The tradition continued until Russell's death in 2003. Maraniss worked his way up to sports editor of the \"Hustler\", the college newspaper. After graduation, he had a job as Vanderbilt basketball sports information director.\n\nIn 1989, as a 19 year old student, he read an article in a student publication about Perry Wallace. One of his professors, Yolette Jones (associate dean in the College of Arts and Sciences as of 2017), suggested that he interview Wallace who was then a law professor in Baltimore. Maraniss did so, and wrote two papers about Wallace and some student newspaper articles. Maraniss recalled, \"I remember sitting on the floor of my dormitory, scribbling in a notebook and feeling like the world was opening up to me as I talked to the most impressive person I had ever talked to before\". After graduation Maraniss stayed on, working as associate director of media relations for Vanderbilt Athletics from 1992 to 1997. In 1998 he moved to Tampa, taking a job as Media Relations Manager for the inaugural season of the Tampa Bay Rays. He then returned to Nashville to join a public relations firm, McNeely, Pigott and Fox and became a partner there for 18 years. Maraniss knew he could write a book (his father had written ten of them) and he was looking for inspiration. At a family gathering he lamented not having a compelling subject to write about. Family members reminded him of his articles about Perry Wallace years prior (which he had not thought of) and the idea clicked. Wallace was fascinating because he was not only a basketball star who broke racial barriers, but also a high school valedictorian, engineering major, law school graduate and eventually a law-school professor. Maraniss had a full-time job, so his research and writing was on nights and weekends and was slow-moving, but he managed to do over 80 interviews in his preparation. To the credit of his research, Maraniss uncovered a transcript of a 1968 statement of Wallace's own words to the Vanderbilt race relations committee: \"An occurrence of no consequence for a white player was transformed into a nightmare\". Maraniss and Wallace became fast friends through their dozens of interviews as well as later one-on-one public interviews which were broadcast.\n\nThe manuscript was published by the Vanderbilt University Press and became the best-selling book in its history. Maraniss said, \"It was seven years before I had a publisher, so I didn't know if it would ever get out there...\" With the book's success, Maraniss was besieged with book signings and requests to appear on talk shows including NBC's \"Meet the Press\", NPR's \"\"All Things Considered \", MSNBC's \"Morning Joe \" ESPN's \"Keith Olberman Show, \" ESPN Radio's \"The Sporting Life \" and others. He has created an adaption of the original book, for young people, with a slightly different title, \"Strong Inside: The True Story of How Perry Wallace Broke College Basketball's Color Line\"\". This edition was named as one of the Top 10 Biographies for Youth and a Top 10 Sports Books for Youth of 2017 by the American Library Association.\n\nFollowing the success of Maraniss' book, a motion picture documentary depicting Wallace's life has been made by filmmaker Rich Gentile, entitled \"Triumph: The Untold Story of Perry Wallace\". It is narrated by Academy Award winning actor Forest Whitaker. The documentary coincides with the 50th anniversary of the integration of SEC basketball, and premiered at Vanderbilt on December 4, 2017. The film was commissioned by Vanderbilt University, and Chancellor Nicholas Zeppos is one of the film's executive producers. As of 2017, the film was not commercially released.\n\nAll first-year students at Vanderbilt are required to read over the summer prior to their first semester, known as the \"Commons Reading\". The Classes of 2020 and 2021 are assigned \"Strong Inside\".\n"}
{"id": "2974488", "url": "https://en.wikipedia.org/wiki?curid=2974488", "title": "Anekantavada", "text": "Anekantavada\n\nAccording to Jainism, no single, specific statement can describe the nature of existence and the absolute truth. This knowledge (\"Kevala Jnana\"), it adds, is comprehended only by the Arihants. Other beings and their statements about absolute truth are incomplete, and at best a partial truth. All knowledge claims, according to the \"anekāntavāda\" doctrine must be qualified in many ways, including being affirmed and denied. Anekāntavāda is a fundamental doctrine of Jainism.\n\nThe origins of \"anekāntavāda \" can be traced back to the teachings of Mahāvīra (599–527 BCE), the 24th Jain . The dialectical concepts of \"syādvāda\" \"conditioned viewpoints\" and \"nayavāda\" \"partial viewpoints\" arose from \"anekāntavāda\" in the medieval era, providing Jainism with more detailed logical structure and expression. The details of the doctrine emerged in Jainism in the 1st millennium CE, from debates between scholars of Jain, Buddhist and Hindu schools of philosophies.\n\nThe word \"anekāntavāda\" is a compound of two Sanskrit words: \"anekānta\" and \"vāda\". The word \"anekānta\" itself is composed of three root words, \"an\" (not), \"eka\" (one) and \"anta\" (end, side), together it connotes \"not one ended, sided\", \"many-sidedness\", or \"manifoldness\". The word \"vāda\" means \"doctrine, way, speak, thesis\". The term \"anekāntavāda\" is translated by scholars as the doctrine of \"many-sidedness\", \"non-onesidedness\", or \"many pointedness\".\n\nThe term \"anekāntavāda\" is not found in early texts considered canonical by Svetambara tradition of Jainism. However, traces of the doctrines are found in comments of Mahavira in these Svetambara texts, where he states that the finite and infinite depends on one's perspective. The word anekantavada was coined by Acharya Siddhasen Divakar to significant the teaching of Mahavira that truth can be expressed in infinite ways. The earliest comprehensive teachings of anekāntavāda doctrine is found in the \"Tattvarthasutra\" by Acharya Umaswami, and is considered to be authoritative by all Jain sects. In the Digambara tradition texts. The 'two-truths theory' of Kundakunda also provides the core of this doctrine.\n\nThe Jain doctrine of \"anekāntavāda\", also known as \"anekāntatva\", states that truth and reality is complex and always has multiple aspects. Reality can be experienced, but it is not possible to totally express it with language. Human attempts to communicate is \"naya\", or \"partial expression of the truth\". Language is not Truth, but a means and attempt to express truth. From truth, according to Māhavira, language returns and not the other way around. One can experience the truth of a taste, but cannot fully express that taste through language. Any attempts to express the experience is \"syāt\", or valid \"in some respect\" but it still remains a \"perhaps, just one perspective, incomplete\". In the same way, spiritual truths are complex, they have multiple aspects, language cannot express their plurality, yet through effort and appropriate karma they can be experienced.\n\nThe \"anekāntavāda\" premises of the Jains is ancient, as evidenced by its mention in Buddhist texts such as the \"Samaññaphala Sutta\". The Jain āgamas suggest that Māhavira's approach to answering all metaphysical philosophical questions was a \"qualified yes\" (\"syāt\"). These texts identify \"anekāntavāda\" doctrine to be one of the key differences between the teachings of the Māhavira and those of the Buddha. The Buddha taught the Middle Way, rejecting extremes of the answer \"it is\" or \"it is not\" to metaphysical questions. The Māhavira, in contrast, taught his followers to accept both \"it is\" and \"it is not\", with \"perhaps\" qualification and with reconciliation to understand the absolute reality. \"Syādvāda\" (predication logic) and \"Nayavāda\" (perspective epistemology) of Jainism expand on the concept of \"anekāntavāda\". \"Syādvāda\" recommends the expression of \"anekānta\" by prefixing the epithet \"syād\" to every phrase or expression describing the nature of existence.\n\nThe Jain doctrine of \"anekāntavāda\", according to Bimal Matilal, states that \"no philosophic or metaphysical proposition can be true if it is asserted without any condition or limitation\". For a metaphysical proposition to be true, according to Jainism, it must include one or more conditions (\"syadvada\") or limitations (\"nayavada\", standpoints).\n\n\"Syādvāda\" () is the theory of \"conditioned predication\", the first part of which is derived from the Sanskrit word \"syāt\" (), which is the third person singular of the optative tense of the Sanskrit verb \"as\" (), 'to be', and which becomes \"syād\" when followed by a vowel or a voiced consonant, in accordance with \"sandhi\". The optative tense in Sanskrit (formerly known as the 'potential') has the same meaning as the present tense of the subjunctive mood in most Indo-European languages, including Hindi, Latin, Russian, French, etc. It is used when there is uncertainty in a statement; not 'it is', but 'it may be', 'one might', etc. The subjunctive is very commonly used in Hindi, for example, in 'kya kahun?', 'what to say?'. The subjunctive is also commonly used in conditional constructions; for example, one of the few English locutions in the subjunctive which remains more or less current is 'were it ०, then ०', or, more commonly, 'if it were..', where 'were' is in the past tense of the subjunctive.\n\nSyat can be translated into English as meaning \"perchance, may be, perhaps\" (it is). The use of the verb 'as' in the optative tense is found in the more ancient Vedic era literature in a similar sense. For example, sutra 1.4.96 of Panini's Astadhyayi explains it as signifying \"a chance, maybe, probable\".\n\nIn Jainism, however, \"syadvada\" and \"anekanta\" is not a theory of uncertainty, doubt or relative probabilities. Rather, it is \"conditional yes or conditional approval\" of any proposition, state Matilal and other scholars. This usage has historic precedents in classical Sanskrit literature, and particularly in other ancient Indian religions (Buddhism and Hinduism) with the phrase \"syad etat\", meaning \"let it be so, but\", or \"an answer that is 'neither yes nor no', provisionally accepting an opponent's viewpoint for a certain premise\". This would be expressed in archaic English with the subjunctive: 'be it so', a direct translation of \"syad etat\". Traditionally, this debate methodology was used by Indian scholars to acknowledge the opponent's viewpoint, but disarm and bound its applicability to certain context and persuade the opponent of aspects not considered.\n\nAccording to Charitrapragya, in Jain context \"syadvada\" does not mean a doctrine of doubt or skepticism, rather it means \"multiplicity or multiple possibilities\". \"Syat\" in Jainism connotes something different from what the term means in Buddhism and Hinduism. In Jainism, it does not connote an answer that is \"neither yes nor no\", but it connotes a \"many sidedness\" to any proposition with a sevenfold predication.\n\n\"Syādvāda\" is a theory of qualified predication, states Koller. It states that all knowledge claims must be qualified in many ways, because reality is many-sided. It is done so systematically in later Jain texts through \"saptibhaṅgīnāya\" or \"the theory of sevenfold scheme\". These \"saptibhaṅgī\" seem to be have been first formulated in Jainism by the 5th or 6th century CE Svetambara scholar Mallavadin, and they are:\n\nEach of these seven predicates state the Jain viewpoint of a multifaceted reality from the perspective of time, space, substance and mode. The phrase \"syāt\" declares the standpoint of expression – affirmation with regard to own substance (\"dravya\"), place (\"kṣetra\"), time (\"kāla\"), and being (\"bhāva\"), and negation with regard to other substance (\"dravya\"), place (kṣetra), time (kāla), and being (\"bhāva\"). Thus, for a ‘jar’, in regard to substance (\"dravya\") – earthen, it simply is; wooden, it simply is not. In regard to place (\"kṣetra\") – room, it simply is; terrace, it simply is not. In regard to time (\"kāla\") – summer, it simply is; winter, it simply is not. In regard to being (\"bhāva\") – brown, it simply is; white, it simply is not. And the word ‘simply’ has been inserted for the purpose of excluding a sense not approved by the ‘nuance’; for avoidance of a meaning not intended.\n\nAccording to Samantabhadra's text \"Āptamīmāṁsā\" (Verse 105), \"\"Syādvāda\", the doctrine of conditional predications, and \"kevalajñāna\" (omniscience), are both illuminators of the substances of reality. The difference between the two is that while \"kevalajñāna\" illumines directly, \"syādvāda\" illumines indirectly\". \"Syadvada\" is indispensable and helps establish the truth, according to Samantabhadra.\n\n\"Nayavāda\" () is the theory of standpoints or viewpoints. \"Nayavāda\" is a compound of two Sanskrit words—\"naya\" (\"standpoint, viewpoint, interpretation\") and \"vāda\" (\"doctrine, thesis\"). Nayas are philosophical perspective about a particular topic, and how to make proper conclusions about that topic.\n\nAccording to Jainism, there are seven \"nayas\" or viewpoints through which one can make complete judgments about absolute reality using \"syadvada\". These seven \"naya\", according to Umaswati, are:\n\nThe \"naya\" theory emerged after about the 5th century CE, and underwent extensive development in Jainism. There are many variants of \"nayavada\" concept in later Jain texts.\n\nA particular viewpoint is called a \"naya\" or a partial viewpoint. According to Vijay Jain, \"Nayavada\" does not deny the attributes, qualities, modes and other aspects; but qualifies them to be from a particular perspective. A \"naya\" reveals only a part of the totality, and should not be mistaken for the whole. A synthesis of different viewpoints is said to be achieved by the doctrine of conditional predications (\"syādvāda\").\n\nAncient India, particularly the centuries in which the Mahavira and the Buddha lived, was a ground of intense intellectual debates, especially on the nature of reality and self or soul. Jain view of soul differs from those found in ancient Buddhist and Hindu texts, and Jain view about \"jiva\" and \"ajiva\" (self, matter) utilizes \"anekantavada\".\n\nThe Upanishadic thought (Hindu) postulated the impermanence of matter and body, but the existence of an unchanging, eternal metaphysical reality of \"Brahman\" and \"Ātman\" (soul, self). The Buddhist thought also postulated impermanence, but denied the existence of any unchanging, eternal soul or self and instead posited the concept of anatta (no-self). According to the vedāntin (Upanishadic) conceptual scheme, the Buddhists were wrong in denying permanence and absolutism, and within the Buddhist conceptual scheme, the vedāntins were wrong in denying the reality of impermanence. The two positions were contradictory and mutually exclusive from each other's point of view. The Jains managed a synthesis of the two uncompromising positions with \"anekāntavāda\". From the perspective of a higher, inclusive level made possible by the ontology and epistemology of \"anekāntavāda\" and \"syādvāda\", Jains do not see such claims as contradictory or mutually exclusive; instead, they are seen as \"ekantika\" or only partially true. The Jain breadth of vision embraces the perspectives of both Vedānta which, according to Jainism, \"recognizes substances but not process\", and Buddhism, which \"recognizes process but not substance\". Jainism, on the other hand, pays equal attention to both substance (\"dravya\") and process (\"paryaya\").\n\nThis philosophical syncretisation of paradox of change through \"anekānta\" has been acknowledged by modern scholars such as Arvind Sharma, who wrote:\nSome Indian writers state that Anekantavada is an inclusivist doctrine positing that Jainism accepts \"non-Jain teachings as partial versions of truth\", a form of sectarian tolerance. Others scholars state this is incorrect and a reconstruction of Jain history because Jainism has consistently seen itself in \"exclusivist term as the one true path\". Classical Jain scholars saw their premises and models of reality as superior than the competing spiritual traditions of Buddhism and Hinduism, both of which Jainism considered inadequate. For instance, the Jain text \"Uttaradhyayana Sutra\" in section 23.63 calls the competing Indian thought to be \"heterodox and heretics\" and that they \"have chosen a wrong path, the right path is that taught by the Jinas\". Similarly, the early Jain scholar Haribhadra, who likely lived between the 6th and 8th century, states that those who do not follow the teachings of Jainism cannot be \"approved or accommodated\".\n\nJohn Koller states \"anekāntavāda\" to be \"epistemological respect for view of others\" about the nature of existence whether it is \"inherently enduring or constantly changing\", but \"not relativism; it does not mean conceding that all arguments and all views are equal\".\n\nIn contemporary times, according to Paul Dundas, the \"Anekantavada\" doctrine has been interpreted by some Jains as intending to \"promote a universal religious tolerance\", and a teaching of \"plurality\" and \"benign attitude to other [ethical, religious] positions\". This is problematic and a misreading of Jain historical texts and Mahavira's teachings, states Dundas. The \"many pointedness, multiple perspective\" teachings of the Mahavira is a doctrine about the nature of Absolute Reality and human existence, and it is sometimes called \"non-absolutism\" doctrine. However, it is not a doctrine about tolerating or condoning activities such as sacrificing or killing animals for food, violence against disbelievers or any other living being as \"perhaps right\". The Five vows for Jain monks and nuns, for example, are strict requirements and there is no \"perhaps, just one perspective\". Similarly, since ancient times, Jainism co-existed with Buddhism and Hinduism, according to Dundas, but Jainism was highly critical of the knowledge systems and ideologies of its rivals, and vice versa.\n\nThe principle of \"anekāntavāda\" is one of the foundational Jain philosophical concept. The development of \"anekāntavāda\" also encouraged the development of the dialectics of \"syādvāda\" (conditioned viewpoints) and \"nayavāda\" (partial viewpoints).\n\nAccording to Karl Potter, the Jain \"anekāntavāda\" doctrine emerged in a milieu that included Buddhists and Hindus in ancient and medieval India. The diverse Hindu schools such as Nyaya-Vaisheshika, Samkhya-Yoga and Mimamsa-Vedanta, all accepted the premise of Atman that \"unchanging permanent soul, self exists and is self-evident\", while various schools of early Buddhism denied it and substituted it with Anatta (no-self, no-soul). Further, for causation theories, Vedanta schools and Madhyamika Buddhists had similar ideas, while Nyaya-Vaisheshika and non-Madhyamika Buddhists generally agreed on the other side. Jainism, using its \"anekāntavāda\" doctrine occupied the center of this theological divide on soul-self (\"jiva\") and causation theories, between the various schools of Buddhist and Hindu thought.\n\nThe origins of \"anekāntavāda\" are traceable in the teachings of Mahāvīra, who used it effectively to show the relativity of truth and reality. Taking a relativistic viewpoint, Mahāvīra is said to have explained the nature of the soul as both permanent, from the point of view of underlying substance, and temporary, from the point of view of its modes and modification.\n\nEarly Jain texts were not composed in Vedic or classical Sanskrit, but in Ardhamagadhi Prakrit language. According to Matilal, the earliest Jain literature that present a developing form of a substantial \"anekantavada\" doctrine is found in Sanskrit texts, and after Jaina scholars had adopted Sanskrit to debate their ideas with Buddhists and Hindus of their era. These texts show a synthetic development, the existence and borrowing of terminology, ideas and concepts from rival schools of Indian thought but with innovation and original thought that disagreed with their peers.\n\nThe early Svetambara canons and teachings do not use the terms \"anekāntavāda\" and \"syādvāda\", but contain teachings in rudimentary form without giving it proper structure or establishing it as a separate doctrine. \"Śvētāmbara\" text, \"Sutrakritanga\" contains references to \"Vibhagyavāda\", which, according to Hermann Jacobi, is the same as \"syādvāda\" and \"saptibhaṅgī\". For example, Jacobi in his 1895 translation interpreted \"vibhagyavada\" as \"syadvada\", the former mentioned in the Svetambara Jain canonical text \"Sutrakritanga\". However, the Digambara Jains dispute this text is canonical or even authentic.\n\nAccording to Upadhyaye, the \"Bhagvatisūtra\" (also called Vyākhyāprajñapti) mentions three primary predications of the \"saptibhaṅgīnaya\". This too is a Svetambara text, and considered by Digambara Jains as unauthentic.\n\nThe earliest comprehensive teachings of anekāntavāda doctrine is found in the \"Tattvarthasutra\" of Umasvati, considered to be authoritative by all Jain sects including Svetambara and Digambara. The century in which Umaswati lived is unclear, but variously placed by contemporary scholars to sometime between 2nd and 5th century.\n\nThe Digambara scholar Kundakunda, in his mystical Jain texts, expounded on the doctrine of \"syādvāda\" and \"saptibhaṅgī\" in \"Pravacanasāra\" and \"Pancastikayasāra\". Kundakunda also used \"nayas\" to discuss the essence of the self in \"Samayasāra\". Kundakunda is believed in the Digambara tradition to have lived about the 1st-century CE, but has been placed by early modern era scholars to 2nd or 3rd century CE. In contrast, the earliest available secondary literature on Kundakunda appears in about the 10th century, which has led recent scholarship to suggest that he may have lived in or after 8th-century. This radical reassessment in Kundakunda chronology, if accurate, would place his comprehensive theories on \"anekantavada\" to the late 1st millennium CE.\n\nThe Jain texts explain the \"anekāntvāda\" concept using the parable of blind men and elephant, in a manner similar to those found in both Buddhist and Hindu texts about limits of perception and the importance of complete context. The parable has several Indian variations, but broadly goes as follows:\n\nThis parable is called \"Andha-gaja-nyaya\" maxim in Jain texts.\n\nTwo of the Jain references to this parable are found in \"Tattvarthaslokavatika\" of Vidyanandi (9th century) and it appears twice in the \"Syādvādamanjari\" of Ācārya Mallisena (13th century). According to Mallisena, whenever anyone takes a partial, unconditional view of the ultimate reality, and denies the possibility of another aspect of that reality, it is an instance of the above parable and a defective view. Mallisena goes further in his second reference to the above parable and states that all reality has infinite aspects and attributes, all assertions can only be relatively true. This does not mean scepticism or doubt is the right path to knowledge, according to Mallisena and other Jain scholars, but that any philosophical assertion is only conditionally, partially true. Any and all viewpoints, states Mallisena, that do not admit an exception are false views.\n\nWhile the same parable is found in Buddhist and Hindu texts to emphasize the need to be watchful for partial viewpoints of a complex reality, the Jain text apply it to isolated topic and all subjects. For example, the \"syadvada\" principle states that all the following seven predicates must be accepted as true for a cooking pot, according to Matilal:\n\n\nĀcārya Haribhadra (8th century CE) was one of the leading proponents of \"anekāntavāda\". He wrote a doxography, a compendium of a variety of intellectual views. This attempted to contextualise Jain thoughts within the broad framework, rather than espouse narrow partisan views. It interacted with the many possible intellectual orientations available to Indian thinkers around the 8th century.\n\nĀcārya Amrtacandra starts his famous 10th century CE work \"Purusathasiddhiupaya\" with strong praise for \"anekāntavāda\": \"I bow down to the principle of \"anekānta\", the source and foundation of the highest scriptures, the dispeller of wrong one-sided notions, that which takes into account all aspects of truth, reconciling diverse and even contradictory traits of all objects or entity.\"\n\nĀcārya Vidyānandi (11th century CE) provides the analogy of the ocean to explain the nature of truth in \"Tattvarthaslokavārtikka\", 116:\n\n, a 17th-century Jain monk, went beyond \"anekāntavāda\" by advocating \"madhāyastha\", meaning \"standing in the middle\" or \"equidistance\". This position allowed him to praise qualities in others even though the people were non-Jain and belonged to other faiths. There was a period of stagnation after Yasovijayaji, as there were no new contributions to the development of Jain philosophy.\n\nThe Jain philosophical concept of Anekantavada made important contributions to ancient Indian philosophy, in the areas of skepticism and relativity. The epistemology of \"anekāntavāda\" and \"syādvāda\" also had a profound impact on the development of ancient Indian logic and philosophy.\n\nWhile employing \"anekāntavāda\", the 17th century Jain scholar Yasovijaya stated that it is not \"anābhigrahika\" (indiscriminate attachment to all views as being true), which is effectively a kind of misconceived relativism. In Jain belief, \"anekāntavāda\" transcends the various traditions of Buddhism and Hinduism.\n\n\"Anekāntavāda\" played a role in the history of Jainism in India, during intellectual debates from Śaivas, Vaiṣṇavas, Buddhists, Muslims, and Christians at various times. According to John Koller, professor of Asian studies, \"anekāntavāda\" allowed Jain thinkers to maintain the validity of their doctrine, while at the same time respectfully criticizing the views of their opponents. In other cases, it was a tool used by Jaina scholars to confront and dispute Buddhist scholars in ancient India, or in the case of Haribhadra justify the retaliation of the killing of his two nephews by Buddhist monks, with capital punishment for all Buddhist monks in the suspected monastery, according to the Buddhist version of Haribhadra's biography.\n\nThere is historical evidence that along with intolerance of non-Jains, Jains in their history have also been tolerant and generous just like Buddhists and Hindus. Their texts have never presented a theory for holy war. Jains and their temples have historically procured and preserved the classic manuscripts of Buddhism and Hinduism, a strong indicator of acceptance and plurality. The combination of historic facts, states Cort, suggest that Jain history is a combination or tolerance and intolerance of non-Jain views, and that it is inappropriate to rewrite the Jainism past as a history of \"benevolence and tolerance\" towards others.\n\nMahatma Gandhi mentioned Anekantavada and Syadvada in the journal \"Young India – 21 Jan 1926\". According to Jeffery D. Long – a scholar of Hindu and Jain studies, the Jain Syadvada doctrine helped Gandhi explain how he reconciled his commitment to the \"reality of both the personal and impersonal aspects of Brahman\", and his view of \"Hindu religious pluralism\":\n\nReferring to the September 11 attacks, John Koller states that the threat to life from religious violence in modern society mainly exists due to faulty epistemology and metaphysics as well as faulty ethics. A failure to respect the life of other human beings and other life forms, states Koller, is \"rooted in dogmatic but mistaken knowledge claims that fail to recognize other legitimate perspectives\". Koller states that \"anekāntavāda\" is a Jain doctrine that each side commit to accepting truths of multiple perspectives, dialogue and negotiations.\n\nAccording to Sabine Scholz, the application of the \"Anekantavada\" as a religious basis for \"intellectual Ahimsa\" is a modern era reinterpretation, one attributed to the writings of A.B. Dhruva in 1933. This view states that \"Anekantavada\" is an expression of \"religious tolerance of other opinions and harmony\". In the 21st century, some writers have presented it as an intellectual weapon against \"intolerance, fundamentalism and terrorism\". Other scholars such as John E. Cort and Paul Dundas state that, while Jainism indeed teaches non-violence as the highest ethical value, the reinterpretation of \"Anekantavada\" as \"religious tolerance of other opinions\" is a \"misreading of the original doctrine\". In Jain history, it was a metaphysical doctrine and a philosophical method to formulate its distinct ascetic practice of liberation. Jain history shows, to the contrary, that it persistently was harshly critical and intolerant of Buddhist and Hindu spiritual theories, beliefs and ideologies. John Cort states that the \"Anekantavada\" doctrine in pre-20th century Jain literature had no relation to religious tolerance or \"intellectual Ahimsa\". Jain intellectual and social history toward non-Jains, according to Cort, has been contrary to the modern revisionist attempts, particularly by diaspora Jains, to present \"Jains having exhibited a spirit of understanding and tolerance toward non-Jains\", or that Jains were rare or unique in practicing religious tolerance in Indian intellectual history. According to Padmanabha Jaini, states Cort, indiscriminate open mindedness and the approach of \"accepting all religious paths as equally correct when in fact they are not\" is an erroneous view in Jainism and not supported by the \"Anekantavada\" doctrine.\n\nAccording to Paul Dundas, in and after the 12th century, the persecution and violence against Jains by Muslim state caused Jain scholars to revisit their theory of Ahimsa (non-violence). For example, Jinadatta Suri in 12th century, wrote during a time of widespread destruction of Jain temples and blocking of Jaina pilgrimage by Muslim armies, that \"anybody engaged in a religious activity who was forced to fight and kill somebody\" in self-defense would not lose any merit. N.L. Jain, quoting Acarya Mahaprajna, states \"Anekantavada\" doctrine is not a principle that can be applied to all situations or fields. In his view, the doctrine has its limits and \"Anekantavada\" doctrine does not mean intellectual tolerance or acceptance of religious violence, terrorism, taking of hostages, proxy wars such as in Kashmir, and that \"to initiate a conflict is as sinful as to tolerate or not oppose it\".\n\nThe reinterpretation of \"Anekantavada\" as a doctrine of religious tolerance is novel, popular but not unusual for contemporary Jains. It is a pattern of reinterpretation and reinvention to rebrand and reposition that is found in many religions, states Scholz.\n\nAccording to Bhagchandra Jain, one of the difference between the Buddhist and Jain views is that \"Jainism accepts all statements to possess some relative (\"anekāntika\") truth\" while for Buddhism this is not the case.\n\nIn Jainism, states Jayatilleke, \"no proposition could in theory be asserted to be categorically true or false, irrespective of the standpoint from which it was made, in Buddhism such categorical assertions were considered possible in the case of some propositions.\" Unlike Jainism, there are propositions that are categorically true in Buddhism, and there are others that are \"anekamsika\" (uncertain, indefinite). Examples of categorically true and certain doctrines are the Four Noble Truths, while examples of the latter in Buddhism are the avyakata-theses. Further, unlike Jainism, Buddhism does not have a Nayavāda doctrine.\n\nAccording to Karl Potter and other scholars, Hinduism developed various theory of relations such as \"satkaryavada\", \"asatkaryavada\", \"avirodhavada\" and others. The \"anekantavada\" overlaps with two major theories found in Hindu and Buddhist thought, according to James Lochtefeld. The \"Anekantavada\" doctrine is \"satkaryavada\" in explaining causes, and the \"asatkaryavada\" in explaining qualities or attributes in the effects. The different schools of Hindu philosophy further elaborated and refined the theory of \"pramanas\" and the theory of relations to establish correct means to structure propositions in their view.\n\nIndologists such as professor John E. Cort state that \"anekāntavāda\" is a doctrine that was historical used by Jain scholars not to accept other viewpoints, but to insist on the Jain viewpoint. Jain monks used \"anekāntavāda\" and \"syādvāda\" as debating weapons to silence their critics and defend the Jain doctrine. According to Paul Dundas, in Jain hands, this method of analysis became \"a fearsome weapon of philosophical polemic with which the doctrines of Hinduism and Buddhism could be pared down to their ideological bases of simple permanence and impermanence, respectively, and thus could be shown to be one-pointed and inadequate as the overall interpretations of reality they purported to be\". The Jain scholars, however, considered their own theory of \"Anekantavada\" self-evident, immune from criticism, needing neither limitations nor conditions.\n\nThe doctrines of \"anekāntavāda\" and \"syādavāda\" are often criticised to denying any certainty, or accepting incoherent contradictory doctrines. Another argument against it, posited by medieval era Buddhists and Hindus applied the principle on itself, that is if nothing is definitely true or false, is \"anekāntavāda\" true or false?\n\nAccording to Karl Potter, the \"Anekantavada\" doctrine accepts the norm in Indian philosophies that all knowledge is contextual, that object and subject are interdependent. However, as a theory of relations, it does not solve the deficiencies in other progress philosophies, just \"compounds the felony by merely duplicating the already troublesome notion of a dependence relation\".\n\nThe Nyaya school criticized the Jain doctrine of \"anekantavada\", states Karl Potter, as \"wanting to say one thing at one time, the other at another\", thereby ignoring the principle of non-contradiction. The Naiyayikas states that it makes no sense to simultaneously say, \"jiva and ajiva are not related\" and \"jiva and ajiva are related\". Jains state that \"jiva\" attaches itself to karmic particles (ajiva) which means there is a relation between ajiva and jiva. The Jain theory of ascetic salvation teaches cleansing of karmic particles and destroying the bound ajiva to the jiva, yet, Jain scholars also deny that ajiva and jiva are related or at least interdependent, according to the Nyaya scholars. The Jain theory of \"anekantavada\" makes its theory of karma, asceticism and salvation incoherent, according to Nyaya texts.\n\nThe Vaisheshika and Shaivism school scholar Vyomashiva criticized the \"Anekantavada\" doctrine because, according to him, it makes all moral life and spiritual pursuits for moksha meaningless. Any spiritually liberated person must be considered under \"Anekantavada\" doctrine to be [a] both liberated and not liberated from one point of view, and [b] simply not liberated from another point of view, since all assertions are to be qualified and conditional under it. In other words, states Vyomashiva, this doctrine leads to a paradox and circularity.\n\n\"Anekāntavāda\" was analyzed and critiqued by Adi Śankarācārya (~800 CE) in his \"bhasya\" on \"Brahmasutra\" (2:2:33–36): He stated that \"anekantavada\" doctrine when applied to philosophy suffers from two problems: \"virodha\" (contradictions) and \"samsaya\" (dubiety), neither of which it is able to reconcile with objectivity.\n\nShankara's criticism of \"anekantavada\" extended beyond the arguments of it being incoherent epistemology in ontological matters. According to Shankara, the goal of philosophy is to identify one's doubts and remove them through reason and understanding, not get more confused. The problem with \"anekantavada\" doctrine is that it compounds and glorifies confusion. Further, states Shankara, Jains use this doctrine to be \"certain that everything is uncertain\". According to Karl Potter, Shankara does not take Jains seriously and admits that he doesn't understand Jainism's theory of relations.\n\nContemporary scholars, states Piotr Balcerowicz, concur that the Jain doctrine of \"Anekantavada\" does reject some versions of the \"law of non-contradiction\", but it is incorrect to state that it rejects this law in all instances.\n\nThe Buddhist scholar Śāntarakṣita, and his student Kamalasila, criticized \"anekantavada\" by presenting his arguments that it leads to the Buddhist premise \"jivas (souls) do not exist\". That is, the two of the most important doctrines of Jainism are mutually contradictory premises. According to Santaraksita, Jains state that \"jiva is one considered collectively, and many considered distributively\", but if so debates Santaraksita, \"jiva cannot change\". He then proceeds to show that changing jiva necessarily means jiva appear and disappear every moment, which is equivalent to \"jiva don't exist\". According to Karl Potter, the argument posited by Śāntarakṣita is flawed, because it commits what is called in the Western logic as the \"fallacy of division\".\n\nThe Buddhist logician Dharmakirti critiqued \"anekāntavāda\" as follows:\nThe medieval era Jain logicians Akalanka and Vidyananda, who were likely contemporaries of Adi Shankara, acknowledged many issues with anekantavada in their texts. For example, Akalanka in his \"Pramanasamgraha\" acknowledges seven problems when \"anekantavada\" is applied to develop a comprehensive and consistent philosophy: dubiety, contradiction, lack of conformity of bases (\"vaiyadhi karanya\"), joint fault, infinite regress, intermixture and absence. Vidyananda acknowledged six of those in the Akalanka list, adding the problem of \"vyatikara\" (cross breeding in ideas) and \"apratipatti\" (incomprehensibility). Prabhācandra, who probably lived in the 11th-century, and several other later Jain scholars accepted many of these identified issues in \"anekantavada\" application.\n\n"}
{"id": "3162", "url": "https://en.wikipedia.org/wiki?curid=3162", "title": "Arbitrage", "text": "Arbitrage\n\nIn economics and finance, arbitrage (, ) is the practice of taking advantage of a price difference between two or more markets: striking a combination of matching deals that capitalize upon the imbalance, the profit being the difference between the market prices. When used by academics, an arbitrage is a (imagined, hypothetical, thought experiment) transaction that involves no negative cash flow at any probabilistic or temporal state and a positive cash flow in at least one state; in simple terms, it is the possibility of a risk-free profit after transaction costs. For example, an arbitrage opportunity is present when there is the opportunity to instantaneously buy something for a low price and sell it for a higher price.\n\nIn principle and in academic use, an arbitrage is risk-free; in common use, as in statistical arbitrage, it may refer to \"expected\" profit, though losses may occur, and in practice, there are always risks in arbitrage, some minor (such as fluctuation of prices decreasing profit margins), some major (such as devaluation of a currency or derivative). In academic use, an arbitrage involves taking advantage of differences in price of a \"single\" asset or \"identical\" cash-flows; in common use, it is also used to refer to differences between \"similar\" assets (relative value or convergence trades), as in merger arbitrage.\n\nPeople who engage in arbitrage are called arbitrageurs —such as a bank or brokerage firm. The term is mainly applied to trading in financial instruments, such as bonds, stocks, derivatives, commodities and currencies.\n\nArbitrage has the effect of causing prices of the same or very similar assets in different markets to converge.\n\n\"Arbitrage\" is a French word and denotes a decision by an arbitrator or arbitration tribunal. (In modern French, \" usually means referee or umpire.) In the sense used here, it was first defined in 1704 by Mathieu de la Porte in his treatise \" as a consideration of different exchange rates to recognise the most profitable places of issuance and settlement for a bill of exchange (\" [, in modern spelling] \".)\n\nIf the market prices do not allow for profitable arbitrage, the prices are said to constitute an arbitrage equilibrium, or an arbitrage-free market. An arbitrage equilibrium is a precondition for a general economic equilibrium. The \"no arbitrage\" assumption is used in quantitative finance to calculate a unique risk neutral price for derivatives.\n\nThis refers to the method of valuing a coupon-bearing financial instrument by discounting its future cash flows by multiple discount rates. By doing so, a more accurate price can be obtained than if the price is calculated with a present-value pricing approach. Arbitrage-free pricing is used for bond valuation and to detect arbitrage opportunities for investors.\n\nFor the purpose of valuing the price of a bond, its cash flows can each be thought of as packets of incremental cash flows with a large packet upon maturity, being the principal. Since the cash flows are dispersed throughout future periods, they must be discounted back to the present. In the present-value approach, the cash flows are discounted with one discount rate to find the price of the bond. In arbitrage-free pricing, multiple discount rates are used.\n\nThe present-value approach assumes that the yield of the bond will stay the same until maturity. This is a simplified model because interest rates may fluctuate in the future, which in turn affects the yield on the bond. The discount rate may be different for each of the cash flows for this reason. Each cash flow can be considered a zero-coupon instrument that pays one payment upon maturity. The discount rates used should be the rates of multiple zero-coupon bonds with maturity dates the same as each cash flow and similar risk as the instrument being valued. By using multiple discount rates, the arbitrage-free price is the sum of the discounted cash flows. Arbitrage-free price refers to the price at which no price arbitrage is possible.\n\nThe ideas of using multiple discount rates obtained from zero-coupon bonds and discount a similar bonds cash flow to find its price is derived from the yield curve. The yield curve is a curve of the yields of the same bond with different maturities. This curve can be used to view trends in market expectations of how interest rates will move in the future. In arbitrage-free pricing of a bond, a yield curve of similar zero-coupon bonds with different maturities is created. If the curve were to be created with Treasury securities of different maturities, they would be stripped of their coupon payments through bootstrapping. This is to transform the bonds into zero-coupon bonds. The yield of these zero-coupon bonds would then be plotted on a diagram with time on the \"x\"-axis and yield on the \"y\"-axis.\n\nSince the yield curve displays market expectations on how yields and interest rates may move, the arbitrage-free pricing approach is more realistic than using only one discount rate. Investors can use this approach to value bonds and find mismatches in prices, resulting in an arbitrage opportunity. If a bond valued with the arbitrage-free pricing approach turns out to be priced higher in the market, an investor could have such an opportunity:\n\nIf the outcome from the valuation were the reversed case, the opposite positions would be taken in the bonds. This arbitrage opportunity comes from the assumption that the prices of bonds with the same properties will converge upon maturity. This can be explained through market efficiency, which states that arbitrage opportunities will eventually be discovered and corrected accordingly. The prices of the bonds in t move closer together to finally become the same at t.\n\nArbitrage is possible when one of three conditions is met:\n\n\nArbitrage is not simply the act of buying a product in one market and selling it in another for a higher price at some later time. The transactions must occur \"simultaneously\" to avoid exposure to market risk, or the risk that prices may change on one market before both transactions are complete. In practical terms, this is generally possible only with securities and financial products that can be traded electronically, and even then, when each leg of the trade is executed the prices in the market may have moved. Missing one of the legs of the trade (and subsequently having to trade it soon after at a worse price) is called 'execution risk' or more specifically 'leg risk'.\n\nIn the simplest example, any good sold in one market should sell for the same price in another. Traders may, for example, find that the price of wheat is lower in agricultural regions than in cities, purchase the good, and transport it to another region to sell at a higher price. This type of price arbitrage is the most common, but this simple example ignores the cost of transport, storage, risk, and other factors. \"True\" arbitrage requires that there is no market risk involved. Where securities are traded on more than one exchange, arbitrage occurs by simultaneously buying in one and selling on the other.\n\nSee rational pricing, particularly § arbitrage mechanics, for further discussion.\n\nMathematically it is defined as follows:\n\nwhere formula_2 and formula_3 denotes the portfolio value at time \"t\".\n\nArbitrage has the effect of causing prices in different markets to converge. As a result of arbitrage, the currency exchange rates, the price of commodities, and the price of securities in different markets tend to converge. The speed at which they do so is a measure of market efficiency. Arbitrage tends to reduce price discrimination by encouraging people to buy an item where the price is low and resell it where the price is high (as long as the buyers are not prohibited from reselling and the transaction costs of buying, holding and reselling are small relative to the difference in prices in the different markets).\n\nArbitrage moves different currencies toward purchasing power parity. As an example, assume that a car purchased in the United States is cheaper than the same car in Canada. Canadians would buy their cars across the border to exploit the arbitrage condition. At the same time, Americans would buy US cars, transport them across the border, then sell them in Canada. Canadians would have to buy American dollars to buy the cars and Americans would have to sell the Canadian dollars they received in exchange. Both actions would increase demand for US dollars and supply of Canadian dollars. As a result, there would be an appreciation of the US currency. This would make US cars more expensive and Canadian cars less so until their prices were similar. On a larger scale, international arbitrage opportunities in commodities, goods, securities and currencies tend to change exchange rates until the purchasing power is equal.\n\nIn reality, most assets exhibit some difference between countries. These, transaction costs, taxes, and other costs provide an impediment to this kind of arbitrage. Similarly, arbitrage affects the difference in interest rates paid on government bonds issued by the various countries, given the expected depreciation in the currencies relative to each other (see interest rate parity).\n\nArbitrage transactions in modern securities markets involve fairly low day-to-day risks, but can face extremely high risk in rare situations, particularly financial crises, and can lead to bankruptcy. Formally, arbitrage transactions have negative skew – prices can get a small amount closer (but often no closer than 0), while they can get very far apart. The day-to-day risks are generally small because the transactions involve small differences in price, so an execution failure will generally cause a small loss (unless the trade is very big or the price moves rapidly). The rare case risks are extremely high because these small price differences are converted to large profits via leverage (borrowed money), and in the rare event of a large price move, this may yield a large loss.\n\nThe main day-to-day risk is that part of the transaction fails – execution risk. The main rare risks are counterparty risk and liquidity risk – that a counterparty to a large transaction or many transactions fails to pay, or that one is required to post margin and does not have the money to do so.\n\nIn the academic literature, the idea that seemingly very low risk arbitrage trades might not be fully exploited because of these risk factors and other considerations is often referred to as limits to arbitrage.\n\nGenerally it is impossible to close two or three transactions at the same instant; therefore, there is the possibility that when one part of the deal is closed, a quick shift in prices makes it impossible to close the other at a profitable price. However, this is not necessarily the case. Many exchanges and inter-dealer brokers allow multi legged trades (e.g. basis block trades on LIFFE).\n\nCompetition in the marketplace can also create risks during arbitrage transactions. As an example, if one was trying to profit from a price discrepancy between IBM on the NYSE and IBM on the London Stock Exchange, they may purchase a large number of shares on the NYSE and find that they cannot simultaneously sell on the LSE. This leaves the arbitrageur in an unhedged risk position.\n\nIn the 1980s, risk arbitrage was common. In this form of speculation, one trades a security that is clearly undervalued or overvalued, when it is seen that the wrong valuation is about to be corrected by events. The standard example is the stock of a company, undervalued in the stock market, which is about to be the object of a takeover bid; the price of the takeover will more truly reflect the value of the company, giving a large profit to those who bought at the current price—if the merger goes through as predicted. Traditionally, arbitrage transactions in the securities markets involve high speed, high volume and low risk. At some moment a price difference exists, and the problem is to execute two or three balancing transactions while the difference persists (that is, before the other arbitrageurs act). When the transaction involves a delay of weeks or months, as above, it may entail considerable risk if borrowed money is used to magnify the reward through leverage. One way of reducing this risk is through the illegal use of inside information, and in fact risk arbitrage with regard to leveraged buyouts was associated with some of the famous financial scandals of the 1980s such as those involving Michael Milken and Ivan Boesky.\n\nAnother risk occurs if the items being bought and sold are not identical and the arbitrage is conducted under the assumption that the prices of the items are correlated or predictable; this is more narrowly referred to as a convergence trade. In the extreme case this is merger arbitrage, described below. In comparison to the classical quick arbitrage transaction, such an operation can produce disastrous losses.\n\nAs arbitrages generally involve \"future\" movements of cash, they are subject to counterparty risk: if a counterparty fails to fulfill their side of a transaction. This is a serious problem if one has either a single trade or many related trades with a single counterparty, whose failure thus poses a threat, or in the event of a financial crisis when many counterparties fail. This hazard is serious because of the large quantities one must trade in order to make a profit on small price differences.\n\nFor example, if one purchases many risky bonds, then hedges them with CDSes, profiting from the difference between the bond spread and the CDS premium, in a financial crisis the bonds may default \"and\" the CDS writer/seller may itself fail, due to the stress of the crisis, causing the arbitrageur to face steep losses.\n\nArbitrage trades are necessarily synthetic, \"leveraged\" trades, as they involve a short position. If the assets used are not identical (so a price divergence makes the trade temporarily lose money), or the margin treatment is not identical, and the trader is accordingly required to post margin (faces a margin call), the trader may run out of capital (if they run out of cash and cannot borrow more) and be forced to sell these assets at a loss even though the trades may be expected to ultimately make money. In effect, arbitrage traders synthesize a put option on their ability to finance themselves.\n\nPrices may diverge during a financial crisis, often termed a \"flight to quality\"; these are precisely the times when it is hardest for leveraged investors to raise capital (due to overall capital constraints), and thus they will lack capital precisely when they need it most.\n\nAlso known as geographical arbitrage, this is the simplest form of arbitrage. In spatial arbitrage, an arbitrageur looks for price differences between geographically separate markets. For example, there may be a bond dealer in Virginia offering a bond at 100-12/23 and a dealer in Washington bidding 100-15/23 for the same bond. For whatever reason, the two dealers have not spotted the difference in the prices, but the arbitrageur does. The arbitrageur immediately buys the bond from the Virginia dealer and sells it to the Washington dealer.\n\nAlso called risk arbitrage, merger arbitrage generally consists of buying/holding the stock of a company that is the target of a takeover while shorting the stock of the acquiring company.\n\nUsually the market price of the target company is less than the price offered by the acquiring company.\nThe spread between these two prices depends mainly on the probability and the timing of the takeover being completed as well as the prevailing level of interest rates.\n\nThe bet in a merger arbitrage is that such a spread will eventually be zero, if and when the takeover is completed. The risk is that the deal \"breaks\" and the spread massively widens.\n\nAlso called \"municipal bond relative value arbitrage\", \"municipal arbitrage\", or just \"muni arb\", this hedge fund strategy involves one of two approaches. The term \"arbitrage\" is also used in the context of the Income Tax Regulations governing the investment of proceeds of municipal bonds; these regulations, aimed at the issuers or beneficiaries of tax-exempt municipal bonds, are different and, instead, attempt to remove the issuer's ability to arbitrage between the low tax-exempt rate and a taxable investment rate.\n\nGenerally, managers seek relative value opportunities by being both long and short municipal bonds with a duration-neutral book. The relative value trades may be between different issuers, different bonds issued by the same entity, or capital structure trades referencing the same asset (in the case of revenue bonds). Managers aim to capture the inefficiencies arising from the heavy participation of non-economic investors (i.e., high income \"buy and hold\" investors seeking tax-exempt income) as well as the \"crossover buying\" arising from corporations' or individuals' changing income tax situations (i.e., insurers switching their munis for corporates after a large loss as they can capture a higher after-tax yield by offsetting the taxable corporate income with underwriting losses). There are additional inefficiencies arising from the highly fragmented nature of the municipal bond market which has two million outstanding issues and 50,000 issuers, in contrast to the Treasury market which has 400 issues and a single issuer.\n\nSecond, managers construct leveraged portfolios of AAA- or AA-rated tax-exempt municipal bonds with the duration risk hedged by shorting the appropriate ratio of taxable corporate bonds. These corporate equivalents are typically interest rate swaps referencing Libor or SIFMA . The arbitrage manifests itself in the form of a relatively cheap longer maturity municipal bond, which is a municipal bond that yields significantly more than 65% of a corresponding taxable corporate bond. The steeper slope of the municipal yield curve allows participants to collect more after-tax income from the municipal bond portfolio than is spent on the interest rate swap; the carry is greater than the hedge expense. Positive, tax-free carry from muni arb can reach into the double digits. The bet in this municipal bond arbitrage is that, over a longer period of time, two similar instruments—municipal bonds and interest rate swaps—will correlate with each other; they are both very high quality credits, have the same maturity and are denominated in the same currency. Credit risk and duration risk are largely eliminated in this strategy. However, basis risk arises from use of an imperfect hedge, which results in significant, but range-bound principal volatility. The end goal is to limit this principal volatility, eliminating its relevance over time as the high, consistent, tax-free cash flow accumulates. Since the inefficiency is related to government tax policy, and hence is structural in nature, it has not been arbitraged away.\n\nNote, however, that many municipal bonds are callable, and that this imposes substantial additional risks to the strategy.\n\nA convertible bond is a bond that an investor can return to the issuing company in exchange for a predetermined number of shares in the company.\n\nA convertible bond can be thought of as a corporate bond with a stock call option attached to it.\n\nThe price of a convertible bond is sensitive to three major factors:\n\n\nGiven the complexity of the calculations involved and the convoluted structure that a convertible bond can have, an arbitrageur often relies on sophisticated quantitative models in order to identify bonds that are trading cheap versus their theoretical value.\n\nConvertible arbitrage consists of buying a convertible bond and hedging two of the three factors in order to gain exposure to the third factor at a very attractive price.\n\nFor instance an arbitrageur would first buy a convertible bond, then sell fixed income securities or interest rate futures (to hedge the interest rate exposure) and buy some credit protection (to hedge the risk of credit deterioration).\nEventually what he'd be left with is something similar to a call option on the underlying stock, acquired at a very low price.\nHe could then make money either selling some of the more expensive options that are openly traded in the market or delta hedging his exposure to the underlying shares.\n\nA depositary receipt is a security that is offered as a \"tracking stock\" on another foreign market. For instance, a Chinese company wishing to raise more money may issue a depository receipt on the New York Stock Exchange, as the amount of capital on the local exchanges is limited. These securities, known as ADRs (American depositary receipt) or GDRs (global depository receipt) depending on where they are issued, are typically considered \"foreign\" and therefore trade at a lower value when first released. Many ADR's are exchangeable into the original security (known as fungibility) and actually have the same value. In this case there is a spread between the perceived value and real value, which can be extracted. Other ADR's that are not exchangeable often have much larger spreads. Since the ADR is trading at a value lower than what it is worth, one can purchase the ADR and expect to make money as its value converges on the original. However, there is a chance that the original stock will fall in value too, so by shorting it one can hedge that risk.\n\nCross-border arbitrage exploits different prices of the same stock in different countries:\n\nExample: Apple is trading on NASDAQ at US$108.84. The stock is also traded on the German electronic exchange, XETRA. If 1 euro costs US$1.11, a cross-border trader could enter a buy order on the XETRA at €98.03 per Apple share and a sell order at €98.07 per share.\n\nSome brokers in Germany do not offer access to the U.S. exchanges. Hence if a German retail investor wants to buy Apple stock, he needs to buy it on the XETRA. The cross-border trader would sell the Apple shares on XETRA to the investor and buy the shares in the same second on NASDAQ. Afterwards, the cross-border trader would need to transfer the shares bought on NASDAQ to the German XETRA exchange, where he is obliged to deliver the stock.\n\nIn most cases, the quotation on the local exchanges is done electronically by high-frequency traders, taking into consideration the home price of the stock and the exchange rate. This kind of high-frequency trading benefits the public as it reduces the cost to the German investor and enables him to buy U.S. shares.\n\nA dual-listed company (DLC) structure involves two companies incorporated in different countries contractually agreeing to operate their businesses as if they were a single enterprise, while retaining their separate legal identity and existing stock exchange listings. In integrated and efficient financial markets, stock prices of the twin pair should move in lockstep. In practice, DLC share prices exhibit large deviations from theoretical parity. Arbitrage positions in DLCs can be set up by obtaining a long position in the relatively underpriced part of the DLC and a short position in the relatively overpriced part. Such arbitrage strategies start paying off as soon as the relative prices of the two DLC stocks converge toward theoretical parity. However, since there is no identifiable date at which DLC prices will converge, arbitrage positions sometimes have to be kept open for considerable periods of time. In the meantime, the price gap might widen. In these situations, arbitrageurs may receive margin calls, after which they would most likely be forced to liquidate part of the position at a highly unfavorable moment and suffer a loss. Arbitrage in DLCs may be profitable, but is also very risky.\n\nA good illustration of the risk of DLC arbitrage is the position in Royal Dutch Shell—which had a DLC structure until 2005—by the hedge fund Long-Term Capital Management (LTCM, see also the discussion below). Lowenstein (2000) describes that LTCM established an arbitrage position in Royal Dutch Shell in the summer of 1997, when Royal Dutch traded at an 8 to 10 percent premium. In total, $2.3 billion was invested, half of which was long in Shell and the other half was short in Royal Dutch (Lowenstein, p. 99). In the autumn of 1998, large defaults on Russian debt created significant losses for the hedge fund and LTCM had to unwind several positions. Lowenstein reports that the premium of Royal Dutch had increased to about 22 percent and LTCM had to close the position and incur a loss. According to Lowenstein (p. 234), LTCM lost $286 million in equity pairs trading and more than half of this loss is accounted for by the Royal Dutch Shell trade.\n\nThe market prices for privately held companies are typically viewed from a return on investment perspective (such as 25%), whilst publicly held and or exchange listed companies trade on a Price to earnings ratio (P/E) (such as a P/E of 10, which equates to a 10% ROI). Thus, if a publicly traded company specialises in the acquisition of privately held companies, from a per-share perspective there is a gain with every acquisition that falls within these guidelines. E.g., Berkshire Hathaway and Halydean Corporation. Private to public equities arbitrage is a term which can arguably be applied to investment banking in general. Private markets to public markets differences may also help explain the overnight windfall gains enjoyed by principals of companies that just did an initial public offering (IPO).\n\nRegulatory arbitrage is where a regulated institution takes advantage of the difference between its real (or economic) risk and the regulatory position. For example, if a bank, operating under the Basel I accord, has to hold 8% capital against default risk, but the real risk of default is lower, it is profitable to securitise the loan, removing the low risk loan from its portfolio. On the other hand, if the real risk is higher than the regulatory risk then it is profitable to make that loan and hold on to it, provided it is priced appropriately. Regulatory arbitrage can result in parts of entire businesses being unregulated as a result of the arbitrage.\n\nThis process can increase the overall riskiness of institutions under a risk insensitive regulatory regime, as described by Alan Greenspan in his October 1998 speech on The Role of Capital in Optimal Banking Supervision and Regulation.\n\nThe term \"Regulatory Arbitrage\" was used for the first time in 2005 when it was applied by Scott V. Simpson, a partner at law firm Skadden, Arps, to refer to a new defence tactic in hostile mergers and acquisitions where differing takeover regimes in deals involving multi-jurisdictions are exploited to the advantage of a target company under threat.\n\nIn economics, regulatory arbitrage (sometimes, tax arbitrage) may be used to refer to situations when a company can choose a nominal place of business with a regulatory, legal or tax regime with lower costs. For example, an insurance company may choose to locate in Bermuda due to preferential tax rates and policies for insurance companies. This can occur particularly where the business transaction has no obvious physical location. In the case of many financial products, it may be unclear \"where\" the transaction occurs.\n\nRegulatory arbitrage can include restructuring a bank by outsourcing services such as IT. The outsourcing company takes over the installations, buying out the bank's assets and charges a periodic service fee back to the bank. This frees up cashflow usable for new lending by the bank. The bank will have higher IT costs, but counts on the multiplier effect of money creation and the interest rate spread to make it a profitable exercise.\n\nExample:\nSuppose the bank sells its IT installations for 40 million USD. With a reserve ratio of 10%, the bank can create 400 million USD in additional loans (there is a time lag, and the bank has to expect to recover the loaned money back into its books). The bank can often lend (and securitize the loan) to the IT services company to cover the acquisition cost of the IT installations. This can be at preferential rates, as the sole client using the IT installation is the bank. If the bank can generate 5% interest margin on the 400 million of new loans, the bank will increase interest revenues by 20 million. The IT services company is free to leverage their balance sheet as aggressively as they and their banker agree to. This is the reason behind the trend towards outsourcing in the financial sector. Without this money creation benefit, it is actually more expensive to outsource the IT operations as the outsourcing adds a layer of management and increases overhead.\n\nAccording to PBS Frontline's 2012 four-part documentary, \"Money, Power, and Wall Street,\" regulatory arbitrage, along with asymmetric bank lobbying in Washington and abroad, allowed investment banks in the pre- and post-2008 period to continue to skirt laws and engage in the risky proprietary trading of opaque derivatives, swaps, and other credit-based instruments invented to circumvent legal restrictions at the expense of clients, government, and publics.\n\nDue to the Affordable Care Act’s expansion of Medicaid coverage, one form of Regulatory Arbitrage can now be found when businesses engage in “Medicaid Migration”, a maneuver by which qualifying employees who would typically be enrolled in company health plans elect to enroll in Medicaid instead. These programs that have similar characteristics as insurance products to the employee, but have radically different cost structures, resulting in significant expense reductions for employers.\n\nTelecom arbitrage companies allow phone users to make international calls for free through certain access numbers. Such services are offered in the United Kingdom; the telecommunication arbitrage companies get paid an interconnect charge by the UK mobile networks and then buy international routes at a lower cost. The calls are seen as free by the UK contract mobile phone customers since they are using up their allocated monthly minutes rather than paying for additional calls.\n\nSuch services were previously offered in the United States by companies such as FuturePhone.com. These services would operate in rural telephone exchanges, primarily in small towns in the state of Iowa. In these areas, the local telephone carriers are allowed to charge a high \"termination fee\" to the caller's carrier in order to fund the cost of providing service to the small and sparsely populated areas that they serve. However, FuturePhone (as well as other similar services) ceased operations upon legal challenges from AT&T and other service providers.\n\nStatistical arbitrage is an imbalance in expected nominal values. A casino has a statistical arbitrage in every game of chance that it offers—referred to as the house advantage, house edge, vigorish or house vigorish.\n\nLong-Term Capital Management (LTCM) lost 4.6 billion U.S. dollars in fixed income arbitrage in September 1998. LTCM had attempted to make money on the price difference between different bonds. For example, it would sell U.S. Treasury securities and buy Italian bond futures. The concept was that because Italian bond futures had a less liquid market, in the short term Italian bond futures would have a higher return than U.S. bonds, but in the long term, the prices would converge. Because the difference was small, a large amount of money had to be borrowed to make the buying and selling profitable.\n\nThe downfall in this system began on August 17, 1998, when Russia defaulted on its ruble debt and domestic dollar debt. Because the markets were already nervous due to the 1997 Asian financial crisis, investors began selling non-U.S. treasury debt and buying U.S. treasuries, which were considered a safe investment. As a result, the price on US treasuries began to increase and the return began decreasing because there were many buyers, and the return (yield) on other bonds began to increase because there were many sellers (i.e. the price of those bonds fell). This caused the difference between the prices of U.S. treasuries and other bonds to increase, rather than to decrease as LTCM was expecting. Eventually this caused LTCM to fold, and their creditors had to arrange a bail-out. More controversially, officials of the Federal Reserve assisted in the negotiations that led to this bail-out, on the grounds that so many companies and deals were intertwined with LTCM that if LTCM actually failed, they would as well, causing a collapse in confidence in the economic system. Thus LTCM failed as a fixed income arbitrage fund, although it is unclear what sort of profit was realized by the banks that bailed LTCM out.\n\n\n\n"}
{"id": "148635", "url": "https://en.wikipedia.org/wiki?curid=148635", "title": "Blood diamond", "text": "Blood diamond\n\nBlood diamonds (also called conflict diamonds, war diamonds, hot diamonds, or red diamonds) is a term used for a diamond mined in a war zone and sold to finance an insurgency, an invading army's war efforts, or a warlord's activity. The term is used to highlight the negative consequences of the diamond trade in certain areas, or to label an individual diamond as having come from such an area. Diamonds mined during the recent civil wars in Angola, Ivory Coast, Sierra Leone, Liberia, Guinea, and Guinea Bissau have been given the label. The term conflict resource refers to analogous situations involving other natural resources.\n\nReports estimated that as much as 20% of the total diamond production in the 1980s was being sold for illegal and unethical purposes and 19% was specifically \"conflict\" in nature. By 1999, the illegal diamond trade was estimated by the World Diamond Council to have been reduced to 4% of the world's diamond production. The World Diamond Council reported that by 2004 this percentage had fallen to approximately 1% and up to today the World Diamond Council refers to this illegal trade to be virtually eliminated, meaning that more than 99% of diamonds being sold have a legal background. \n\nDespite the UN Resolution, UNITA was able to continue to sell or trade some diamonds in order to finance its war effort. The UN set out to find how this remaining illicit trade was being conducted and appointed Canadian ambassador Robert Fowler to investigate. In 2000, he produced the Fowler Report, which named those countries, organizations and individuals involved in the trade. The report is credited with establishing the link between diamonds and third world conflicts, and led directly to United Nations Security Council Resolution 1295, as well as the Kimberley Process Certification Scheme. Still, after the report was published in 2013 smugglers from these African countries were selling blood diamonds through channels less sophisticated such as social media posts. And rhinestones from Angola, produced by UNITA were being traded to Cameroon to get them a Cameroonian certificate naturalization to then be sold as legitimate.\n\nIvory Coast began to develop a fledgling diamond mining industry in the early 1990s. A coup overthrew the government in 1999, starting a civil war. The country became a route for exporting diamonds from Liberia and war-torn Sierra Leone. Foreign investment began to withdraw from Ivory Coast. To curtail the illegal trade, the nation stopped all diamond mining and the UN Security Council banned all exports of diamonds from Ivory Coast in December 2005. This ban lasted about ten years but it was later lifted in April 2014 when members of the UN council voted to suspend the sanction. The Kimberley process officials also notified in November 2013 that Ivory Coast was right producing artisanal diamonds.\n\nDespite UN sanctions the illicit diamond trade still exists in Ivory Coast. Rough diamonds are exported out of the country to neighboring states and international trading centers through the northern Forces Nouvelles controlled section of the country, a group which is reported to be using these funds of chele to re-arm.\n\nThe Democratic Republic of the Congo (formerly Zaire) has suffered numerous looting wars in the 1990s, but has been a member of the Kimberley Process since 2003 and now exports about 8% of the world's diamonds.'However, even nowadays there is a warning concerning diamonds proceeding from this area since there have been multiple cases of fake Kimberley certificates accompanying the gems. Once one of De Beers most celebrated and priceless diamonds, the D-colour Millennium Star was discovered in the DRC and sold to De Beers, in open competition with other diamond buyers, between 1991 and 1992.\n\nFrom 1989 to 2003, Liberia was engaged in a civil war. In 2000, the UN accused Liberian president Charles G. Taylor of supporting the Revolutionary United Front (RUF) insurgency in neighboring Sierra Leone with weapons and training in exchange for diamonds. In 2001, the United Nations applied sanctions on the Liberian diamond trade. In August 2003, Taylor stepped down as president and, after being exiled to Nigeria, faced trial in The Hague. On July 21, 2006, he pleaded not guilty to crimes against humanity and war crimes, of which he was found guilty in April 2012. On May 30, 2012, he began a 50-year sentence in a high security prison in the United Kingdom.\n\nAround the time of the 1998 United States embassy bombings, al-Qaeda allegedly bought gems from Liberia as some of its other financial assets were frozen.\n\nHaving regained peace, Liberia is attempting to construct a legitimate diamond mining industry. The UN has lifted sanctions and Liberia is now a member of the Kimberley Process.\n\nIn December 2014 however, Liberian diamonds were reported to be partly produced using child labor according to the U.S. Department of Labor's \"List of Goods Produced by Child Labor or Forced Labor\".\n\nThe civil war started in 1991 and continued until 2002, costing at least 50,000 lives and causing local people to suffer killings, mutilation, rape, torture and abduction, mainly due to the brutal warfare waged by rebel group, the Revolutionary United Front (RUF). The Revolutionary United Front (RUF) claimed that they supported causes of justice and democracy in the beginning, but later on they started to control the villages and to prevent local people from voting for the new government by chopping off their limbs. Victims included children and infants. It created numerous examples of physical and psychological harm across Sierra Leone. \n\nMoreover, they also occupied the diamond mines in order to get access to funding and continued support of their actions. For example, during that time, RUF was mining up to $125 million of diamonds yearly. Since diamonds are used as a funding source, they also created opportunities for tax evasion and financial support of crime. Therefore, United Nations Security Council imposed diamond sanctions in 2000, which were then lifted in 2003. According to National Geographic News, all of these civil wars and conflicts created by rebel groups resulted in over four million deaths in the African population and injuries to over two million civilians. Another latest conflict diamond statistic from Statistic Brain, revealed that Sierra Leone has been listed as second highest in the production of conflict diamonds, which is shown as 1% of the world's production, after Angola, which produced 2.1% in 2016. 15% of Sierra Leone's diamond production are conflict diamonds. It shows that the production of conflict diamonds still exists in Sierra Leone.\n\nAccording to the 2005 Country Reports on Human Right Practices of Africa from the United States, serious human rights issues still exist in Sierra Leone, even though the 11-year civil conflict had officially ended by 2002. Sierra Leone remains in an unstable political situation, although the country has elected a new government. The huge consequences of blood diamonds still remains a mainstream issue in Sierra Leone. One of the biggest issues is people still being abused by the security forces, including rape and the use of excessive force on detainees, including teenagers. Child abuse and child labor are other serious issues which took place in Sierra Leone after the civil conflicts. As they need a huge number of workers, the security forces started kidnapping and forcing young adults to be their slaves; children were forced to join their army as soldiers, and women were raped. They even burned entire villages. Thousands of men, women, and children are used as slaves to collect diamonds, and they are forced to use their bare hands to dig in mud along river banks instead of digging with tools.\n\nBased on the report, The Truth About Diamonds: Conflict and Development from Global Witness, it mentioned that Sierra Leone is listed as second from the bottom of the United Nation Human Development Index. It also shows that Sierra Leone still makes slow progress, in 2016, in such different aspects as, for example, education, health, and human rights, since 1990, which is also the year that conflicts took place in Sierra Leone. It shows that it is a huge consequence of blood diamonds that it brought into Sierra Leone. Even though the war had ended in 2002 and the government tried to improve and adjust the cooperation of the diamond industry. Sierra Leone resulted in an increase of over US$140 million in 2005 and attempted a percentage return of export tax to diamond mining communities. However, it does not improve anything — the money is not reaching the public and it has not provided benefit to anyone in the communities. For instance, the Kono district in Sierra Leone has been mined for 70 years, but they still have no basic facilities, like electricity and repairing of roads. Houses are destroyed because of the civil wars. It also examines the unethical issues of how rebel groups treat those locals. They used brainwashing of inexperienced young children and forced them to be child soldiers as they lost their personal freedom and rights under command that included violence and intimidation. \n\nThe Republic of the Congo (Congo-Brazzaville) was expelled from the Kimberley Process in 2004 because, despite having no official diamond mining industry, the country was exporting large quantities of diamonds, the origin of which it could not detail. It was also accused of falsifying certificates of origin. The Republic of the Congo was readmitted in 2007.\n\nZimbabwe Diamonds are not considered conflict diamonds by the Kimberley Process Certification Scheme.\n\nIn July 2010, the Kimberley Process Certification Scheme agreed that diamonds from the country's disputed Marange Diamond Fields could be sold on the international market, after a report from the Scheme's monitor a month earlier described diamonds mined from the fields as conflict-free.\n\nGlobal Witness was one of the first organizations to pick up on the link between diamonds and conflicts in Africa in its 1998 report entitled \"A Rough Trade\". With the passing of United Nations Security Council Resolution 1173 in 1998, the United Nations too identified the conflict diamond issue as a funding for war. The Fowler Report in 2000 detailed in depth how UNITA was financing its war activities, and in May 2000, led directly to the passing of United Nations Security Council Resolution 1295 and the diamond producing countries of southern Africa meeting in Kimberley, South Africa to plan a method by which the trade in conflict diamonds could be halted, and buyers of diamonds could be assured that their diamonds have not contributed to violence. In this resolution the Security Council wrote:\n\nWelcomes the proposal that a meeting of experts be convened for the purpose of devising a system of controls to facilitate the implementation of the measures contained in Resolution 1173 (1998), including arrangements that would allow for increased transparency and accountability in the control of diamonds from their point of origin to the bourses, emphasizes that it is important that, in devising such controls, every effort be made to avoid inflicting collateral damage on the legitimate diamond trade, and welcomes the intention of the Republic of South Africa to host a relevant conference this year\".\n\nOn July 19, 2000, the World Diamond Congress at Antwerp adopted a resolution to strengthen the diamond industry's ability to block sales of conflict diamonds. The resolution called for an international certification system on the export and import of diamonds, legislation in all countries to accept only officially sealed packages of diamonds, for countries to impose criminal charges on anyone trafficking in conflict diamonds, and instituted a ban on any individual found trading in conflict diamonds from the diamond bourses of the World Federation of Diamond Bourses. The Kimberley Process was at the start led by South Africa and Canada as vice president and since then every year a new chair and vice chair country are elected to maintain the legitimacy of their practices. This system tracks diamonds from the mine to the market and regulates the policing surrounding the export, manufacture and sale of the products. Also in tourist countries like Dubai and the United Kingdom. All the Kimberley members are not allowed to trade with non members. Before a gemstone is allowed through the airports to other countries, the Kimberley Certification must be presented by the gem's owner or obtained from a renowned attorney. The certificate should also be requested by the customer when the gems have reached a retail store to ensure its precedence. \n\nOn January 17–18 of 2001, diamond industry figures convened and formed the new organization, the World Diamond Council. This new body set out to draft a new process, whereby all diamond rough could be certified as coming from a non-conflict source.\n\nThe KPCS was given approval by the UN on March 13, 2002, and in November, after two years of negotiation between governments, diamond producers, and Non-Government organizations, the Kimberley Process Certification Scheme was created.\n\nThe Kimberley Process attempted to curtail the flow of conflict diamonds, help stabilize fragile countries and support their development. As the Kimberley Process has made life harder for criminals, it has brought large volumes of diamonds onto the legal market that would not otherwise have made it there. This has increased the revenues of poor governments, and helped them to address their countries’ development challenges. For instance, around $125 million worth of diamonds were legally exported from Sierra Leone in 2006, compared to almost none at the end of the 1990s.\n\nThe Kimberley Process has ultimately failed to stem the flow of blood diamonds, leading key proponents such as Global Witness to abandon the scheme. In addition, there is no guarantee that diamonds with a Kimberley Process Certification are in fact conflict-free. This is due to the nature of the corrupt government officials in the leading diamond producing countries. It is common for these officials to be bribed with $50 to $100 a day in exchange for paperwork declaring that blood diamonds are Kimberley Process Certified.\n\nThe Kimberley system attempted to increase governments' transparency by forcing them to keep records of the diamonds they are exporting and importing and how much they are worth. In theory, this would show governments their finances so that they can be held accountable for how much they are spending for the benefit of the country's population. However non-compliance by countries such as Venezuela has led to the failure of accountability.\n\nThe company Materialytics claims that it can trace the origin of virtually any diamond using Laser-induced breakdown spectroscopy. However, there is no way to know whether a diamond purchased online is blood free or not.\n\nOn January 18, 2001, President Bill Clinton issued Executive Order 13194 which prohibited the importation of rough diamonds from Sierra Leone into the United States in accordance with the UN resolutions. On May 22, 2001, President George W. Bush issued Executive Order 13213 which banned rough diamond importation from Liberia into the United States. Liberia had been recognized by the United Nations as acting as a pipeline for conflict diamonds from Sierra Leone.\n\nUnited States enacted the Clean Diamond Trade Act (CDTA) on April 25, 2003, implemented on July 29, 2003, by Executive Order 13312. The CDTA installed the legislation to implement the KPCS in law in the United States. The implementation of this legislation was key to the success of the KPCS, as the United States is the largest consumer of diamonds. The CDTA states: 'As the consumer of a majority of the world's supply of diamonds, the United States has an obligation to help sever the link between diamonds and conflict and press for implementation of an effective solution.\n\nThe United States Department of State also maintains an office for a Special Adviser for Conflict Diamonds. As of October 14, 2015, the position is held by Ashley Orbach.\n\nDuring the 1990s diamond-rich areas were discovered in Northern Canada. Canada is one of the key players in the diamond industry. Partnership Africa Canada was created in 1986 to help with the crisis in Africa. This organization is also part of the Diamond Development Initiative. The Diamond Development Initiative helps improve and regulate the legal diamond industry.\n\nThe Kimberley Process was initiated in May 2000 by South Africa with Canada a major supporter of instituting the scheme. Canada has now passed several laws that help stop the trade of conflict diamonds. The laws deal with the export and import of rough diamonds, and also how they are transferred. In December 2002 the Export and Import of Rough Diamonds Act was passed by the Canadian government. This law acts as a system that helps control the importing, exporting and transporting of rough diamonds through Canada. The Export and Import of Rough Diamonds Act also states that the Kimberley Process is the minimum requirement of certifying rough diamonds and a certificate is also required for all shipments of diamonds. This certificate is called the Canadian Certificate, it gives permission for an officer to seize any shipment of diamonds that does not meet the requirements of the Export and Import of Rough Diamonds Act.\n\nThe Government of the Northwest Territories of Canada (GNWT) also has a unique certification program. They offer a Government certificate on all diamonds that are mined, cut, and polished in the Northwest Territories of Canada. Canadian diamonds are tracked from the mine, through the refining process, to the retail jeweler with a unique diamond identification number (DIN) laser inscribed on the diamond's girdle. To obtain this certificate one must cut and polish the diamond in the NWT.\n\nTechnical services have emerged that may act as a solution for tracking diamond movement across borders. A service was launched in July 2016 that allows managers to build systems using a blockchain database for tracking high-value or highly regulated items through a supply chain. Everledger is one using such a system to \"record the movement of diamonds from mines to jewelry stores\" and is one of the inaugural clients of a new blockchain-based tracking service from IBM.\n\n\n\n"}
{"id": "42610931", "url": "https://en.wikipedia.org/wiki?curid=42610931", "title": "Bullying and emotional intelligence", "text": "Bullying and emotional intelligence\n\nBullying is abusive social interaction between peers can include aggression, harassment, and violence. Bullying is typically repetitive and enacted by those who are in a position of power over the victim. A growing body of research illustrates a significant relationship between bullying and emotional intelligence. Emotional intelligence (EI) is a set of abilities related to the understanding, use and management of emotion as it relates to one's self and others. Mayer et al., (2008) defines the dimensions of overall EI as: \"accurately perceiving emotion, using emotions to facilitate thought, understanding emotion, and managing emotion\". The concept combines emotional and intellectual processes. Lower emotional intelligence appears to be related to involvement in bullying, as the bully and/or the victim of bullying. EI seems to play an important role in both bullying behavior and victimization in bullying; given that EI is illustrated to be malleable, EI education could greatly improve bullying prevention and intervention initiatives.\n\nBullying is the most prevalent form of violence in schools and has lasting consequences into adulthood. Increased concern regarding school bullying has been raised in part due to publicized suicides of childhood victims. Around 40% of middle school children are directly involved in bullying at least once a week according to the National Center of Education Statistics. Pre-adolescent research confirms such a negative relationship between trait EI and bullying behavior; bullying behavior is negatively associated with total empathy and more specifically, the EI dimension of cognitive empathy, which is the ability to understand or take on the emotional experiences and perspectives of others. It was found that adolescent bullying peer relations are also significantly negatively correlated with the dimension of EI that was conceptualized by Lomas et al. (2012) as Understanding the Emotions of Others. While the term naming the dimension varies within the research, the dimension of EI that appears to have the strongest inverse relationship with enacting bullying behavior throughout the literature is one’s ability to understand the emotional experience of other people. Because bullying behavior in school-aged children is related to lower levels of understanding of other’s emotions, one theory is that children who exhibit bullying behaviors are not able to fully understand the impact that they have on their victims. Indeed, when differentiating between the different components of empathy, it is the cognitive component that bullies seem to have the most deficit in. In addition to the inability to relate to the emotions of others, research also suggests that those who engage in bullying behavior may also lack proper skills in dealing with their own emotions, another aspect of EI often referred to as emotional facilitation or self-efficacy. The poor use of emotions is found to be significant in predicting problem behavior among adolescents, such as aggression, which can be characteristic in bullying behavior. In this way, the ability to understand and manage one’s own emotions may play an important role in preventing children from engaging in bullying behavior. For example, in a study among adolescent girls, it was found that better management of stress could prevent the perpetuation of aggression and violence.\n\nWorkplace bullying is reported to be far more prevalent than perhaps commonly thought. For some reason, workplace bullying seems to be particularly widespread in healthcare organizations; 80% of nurses report experiencing workplace bullying. Similar to the school environment for children, the work environment typically places groups of adult peers together in a shared space on a regular basis. In such a situation, social interactions and relationships are of great importance to the function of the organizational structure and in pursuing goals. The emotional consequences of bullying put an organization at risk of losing victimized employees. Bullying also contributes to a negative work environment, is not conducive to necessary cooperation and can lessen productivity at various levels. Bullying in the workplace is associated with negative responses to stress. The ability to manage emotions, especially emotional stress, seems to be a consistently important factor in different types of bullying. The workplace in general can be a stressful environment, so a negative way of coping with stress or an inability to do so can be particularly damning. Workplace bullies may have high social intelligence and low emotional intelligence. In this context, bullies tend to rank high on the social ladder and are adept at influencing others. The combination of high social intelligence and low empathy is conducive to manipulative behavior, such that Hutchinson (2013) describes workplace bullying to be. In working groups where employees have low EI, workers can be persuaded to engage in unethical behavior. With the bullies' persuasion, the work group is socialized in a way that rationalizes the behavior, and makes the group tolerant or supportive of the bullying. Hutchinson & Hurley (2013) make the case that EI and leadership skills are both necessary to bullying intervention in the workplace, and illustrates the relationship between EI, leadership and reductions in bullying. EI and ethical behavior among other members of the work team have been shown to have a significant impact on ethical behavior of nursing teams. Higher EI is linked to improvements in the work environment and is an important moderator between conflict and reactions to conflict in the workplace. The self-awareness and self-management dimensions of EI have both been illustrated to have strong positive correlations with effective leadership and the specific leadership ability to build healthy work environments and work culture.\n\nGiven lower emotional intelligence, it is also possible that many bullies are more malevolently creative. When original, the acts of aggression and abuse found in both childhood and adult bullying are considered examples of malevolent creativity (MC). Findings suggest that individuals lower in EI conceive more malevolently creative solutions, which theoretically leads to more malevolently creative behaviors. It is conjectured that people with lower emotional intelligence may not see the impropriety in malevolently creative ideas or disregard how others would perceive them, and thus they have less issue with disclosing such ideas. Given the hypothesis that more malevolently creative solutions should lead to more malevolently creative behaviors, this theory makes sense in light of the deficit in cognitive empathy found in bullying behavior.\n\nThere may also be a subtype of bully that is high in callous and unemotional traits (CU). CU traits include some of the discussed deficits in EI such as lack of empathy, as well as other traits such as a lack of guilt, shallow capacity for emotion and poor behavioral modulation when faced with punishment. Given that children who bully often have conduct problems, and CU traits are often co-occurring with conduct problems, Viding et al., (2009) investigated the relationship between CU and bullying behavior. Given that previous research suggests children with conduct problems fall into subtypes of those with high CU traits and those without, it was possible that this creates a distinction among bullies. Higher CU was independently correlated to direct bullying, which is associated with lack of empathy, while indirect bullying is not. When combined with conduct problems, CU increased the risk of direct and indirect bullying behaviors. Bullies high in CU traits will probably be resistant to many of the interventions successful with bullies who are not. Although a defining characteristic of CU is a lack of empathy, which overlaps with bullies deficits in empathy as highlighted above, the other characteristics of the concept would make bullies high in CU less malleable than those who simply have lower EI.\n\nBeing bullied can have a negative impact on the victim's life: Bullied children may go on to be maladjusted socially and emotionally, and worsen in behavior. Adults who are bullied in the workplace may have deteriorated self-esteem, suffer from isolation and become fearful and avoidant after being victimized. They may disengage and withdraw from their work community. Both child and adult victims are at greater risk of developing mental pathology. EI is found to be a significant predictor of variance in adolescent peer victimization in bullying and also has a negative correlation with adolescent bullying. Victim peer relations showed strong negative correlations with the emotional management and facilitation dimensions of EI conceptualized as Emotional Management and Control and Emotions Direct Cognitions respectively, both of which made significant semi-partial contributions to the overall model of Emotional Intelligence. These results indicate that victims may have less ability to handle their emotions or to use them to make decisions in response. The inability to manage one’s own emotions can lead to rejection, or further rejection, from peers which can help perpetuate victimization and further damage a victim’s social skills; peer relationships and support are influential on emotional adjustment. In workplace bullying the workgroup’s rejection isolates the victim and causes guilt and fear, causing withdrawal from the group and reducing opportunities for social support. In addition to self-efficacy, victimization is also found to be negatively correlated to cognitive and affective empathy.\n\nThere is a strong positive relationship between engaging in bullying behaviors and having been victimized by bullying behaviors. This is both a common finding in review of the research and is in tune with what is commonly observed during human adolescence; often victims of bullying go on to become bullies themselves. A history of victimization often leads to a perpetuation of similar behavior. Having low emotional intelligence increases the likelihood of being both a victim and a bully, which are apparently not mutually exclusive roles. This dual status is sometimes referred to as being a bully-victim. Bully-victims seem to be the most troubled. They tend to exhibit more emotional issues like low impulse control and self-esteem as well as social issues, such as the inability to interpret social cues or make friends. They may begin with pre-existing issues with behavior and emotion, and more often come from dysfunctional families. Childhood bully-victims also fare worse in adulthood than 'pure' bullies or victims. Across multiple areas, bully-victims had the greatest impairment in adult functioning and worse health outcomes including the diagnosis of a serious illness or psychiatric disorder. Bully-victims had similarly poor outcomes in educational achievement as bullies and also shared similar likelihood to some of the measured risk behaviors, all status groups showed impairment in some categories like wealth attainment and social relationships. When controlling for other factors pure bullies are no longer at an elevated risk across all these categories, which bullying is predictive of regardless of victim status, though bully-victims and victims remain at higher risk. While some dimensions of EI seem more predictive of one status or the other (that of the bully or the victim), there are dimensions of EI, such as empathy and self-efficacy, that have significant negative relationships with both. Additionally, EI as a whole is significant in predicting for victim status. Thus, victims may also be deficient in the dimensions of EI that correlate to becoming a bully, a risk that could be expected to be exacerbated by the damage to one's psycho-social health due to being a victim. Students who experience bullying often have a harder time adapting healthy relationships when they get older.\n\nThe most effective bullying interventions will likely be those that are dynamic and theory-driven in approach. Conventional intervention efforts have had small impact and mixed results in reducing bullying among children. These earlier models were based on descriptive data and focused on correcting the behavior of children who were already bullies or victims. Domino (2013) notes a theoretical shift from focus on deficit-based intervention to strength-based intervention.\n\nTake The Lead (TTL) is a curriculum for middle school students combining social emotional learning (SEL) and positive youth development (PYD). SEL is a process of building social competence and emotional intelligence through a set of pertinent skills. PYD is a SEL program that uses social end emotional learning to promote healthy outcomes for the children by developing, then applying, the learned individual and group skills. Meta-analysis of 213 studies linked SEL to significant improvement in interpersonal relationships, social skills, behavior issues, substance abuse and aggression. Positive contributions to the impact of SEL were found to be made by the development and application of social skills, social support and positive behavior reinforcement. Meta-analysis of 25 programs illustrated significant positive changes in interpersonal skills, self-control, problem-solving and both peer and adult relationships as a result of PYD as well as significant decrease in negative risk behaviors such as substance abuse, acts of aggression, truancy and risky sexual behavior. The most important elements of PYD for positive outcomes in the analysis were incorporation of emotional intelligence and self-efficacy, and the development of pro-social norms. Domino (2013) noted that prior research supported SEL and PYD being applied to youth risk behaviors and that their effectiveness was found to be positive and sustainable, however not much research had been done on a link between the constructs and reduction of bullying specifically. The study then investigates a model, TTL, that combines the SEL and PYD frameworks, strengthened by a social support system. TTL consists of 16 lessons, taught once a week for 16 weeks, during regular 45 minute class periods by teachers that are trained for a minimum of 6 hours.\nThe lessons are accompanied by a goal; for instance, the goal accompanying lesson 10 on Assertiveness is \"Differentiate between assertive, passive, and aggressive communication styles, and practice assertive and empathic interrelating.\". Every lesson includes knowledge, skill and application components so that students are able to practice the learned skills in their life outside of the classroom. A TTL training workshop is offered to the parents of participants and a letter is sent to the parents at the beginning of each segment with information about the lesson, goals and accompanying activities. Domino (2013) applied the TTL intervention to 7th grade students and measured changes in bullying and victim behavior using a quantitative pretest-posttest control group cohort design. Sum scores for bullying and victimization were obtained before and at the completion of the intervention using the PRQ, a self-report survey, completed anonymously.\n\nBeyond preventing bullying, it is also important to consider how interventions based on EI are important in the case that bullying will occur. Increasing EI may be an important step in trying to foster resilience among victims. When a person faces stress and adversity, especially of a repetitive nature, their ability to adapt is an important factor in whether they have a more positive or negative outcome. Resilient individuals are those who are considered to have positive developmental outcomes in light of their negative experiences, such as bullying. Sapouna & Wolke (2013) examined adolescents who illustrated resilience to bullying and found some interesting gendered differences, with higher behavioral resilience found among girls and higher emotional resilience found among boys. Despite these differences, they still implicated internal resources and negative emotionality in either encouraging or being negatively associated with resilience to bullying respectively and urged for the targeting of psychosocial skills as a form of intervention. Emotional Intelligence has been illustrated to promote resilience to stress and as mentioned previously the ability to manage stress and other negative emotions can be preventative of a victim going on to perpetuate aggression. One factor that is important in resilience is the regulation of one's own emotions. Schneider et al. (2013) found that emotional perception was significant in facilitating lower negative emotionality during stress and Emotional Understanding facilitated resilience and has a positive correlation with positive affect.\n\nBooks\n\nAcademic articles\n\n"}
{"id": "486965", "url": "https://en.wikipedia.org/wiki?curid=486965", "title": "Chinese bronze inscriptions", "text": "Chinese bronze inscriptions\n\nChinese bronze inscriptions, also commonly referred to as Bronze script or Bronzeware script, are writing in a variety of Chinese scripts on Chinese ritual bronzes such as \"zhōng\" bells and \"dǐng\" tripodal cauldrons from the Shang dynasty (2nd millennium BC) to the Zhou dynasty (11th – 3rd century BC) and even later. Early bronze inscriptions were almost always cast (that is, the writing was done with a stylus in the wet clay of the piece-mold from which the bronze was then cast), while later inscriptions were often engraved after the bronze was cast. The bronze inscriptions are one of the earliest scripts in the Chinese family of scripts, preceded by the oracle bone script.\n\nFor the early Western Zhou to early Warring States period, the bulk of writing which has been unearthed has been in the form of bronze inscriptions. As a result, it is common to refer to the variety of scripts of this period as \"bronze script\", even though there is no single such script. The term usually includes bronze inscriptions of the preceding Shang dynasty as well.\nHowever, there are great differences between the highly pictorial Shang emblem (aka \"identificational\") characters on bronzes (see \"ox\" clan insignia above), typical Shang bronze graphs, writing on bronzes from the middle of the Zhou dynasty, and that on late Zhou to Qin, Han and subsequent period bronzes. Furthermore, starting in the Spring and Autumn period, the writing in each region gradually evolved in different directions, such that the script styles in the Warring States of Chu, Qin and the eastern regions, for instance, were strikingly divergent. In addition, artistic scripts also emerged in the late Spring and Autumn to early Warring States, such as Bird Script (鳥書 \"niǎoshū\"), also called Bird Seal Script (\"niǎozhuàn\" 鳥篆 ), and Worm Script (\"chóngshū\" 蟲書).\n\nOf the abundant Chinese ritual bronze artifacts extant today, about 12,000 have inscriptions. These have been periodically unearthed ever since their creation, and have been systematically collected and studied since at least the Song dynasty. The inscriptions tend to grow in length over time, from only one to six or so characters for the earlier Shang examples, to forty or so characters in the longest, late-Shang case, and frequently a hundred or more on Zhou bronzes, with the longest up to around 500.\n\nIn general, characters on ancient Chinese bronze inscriptions were arranged in vertical columns, written top to bottom, in a fashion thought to have been influenced by bamboo books, which are believed to have been the main medium for writing in the Shang and Zhou dynasties. The very narrow, vertical bamboo slats of these books were not suitable for writing wide characters, and so a number of graphs were rotated 90 degrees; this style then carried over to the Shang and Zhou oracle bones and bronzes. Examples: \n\nOf the 12,000 inscribed bronzes extant today, roughly 3,000 date from the Shang dynasty, 6,000 from the Zhou dynasty, and the final 3,000 from the Qin and Han dynasties.\n\nInscriptions on Shang bronzes are of a fairly uniform style, making it possible to discuss a \"Shang bronze script\", although great differences still exist between typical characters and certain instances of clan names or emblems. Like early period oracle bone script, the structures and orientations of individual graphs varied greatly in the Shang bronze inscriptions, such that one may find a particular character written differently each time rather than in a standardized way (see the many examples of \"tiger\" graph to the lower left). As in the oracle bone script, characters could be written facing left or right, turned 90 degrees, and sometimes even flipped vertically, generally with no change in meaning. For instance, and both represent the modern character \"xū\" 戌 (the 11th Earthly Branch), while and are both \"hóu\" 侯 \"marquis\". This was true of normal as well as extra complex identificational graphs, such as the \"hǔ\" 虎 \"tiger\" clan emblem at right, which was turned 90 degrees clockwise on its bronze.\n\nThese inscriptions are almost all cast (as opposed to engraved), and are relatively short and simple. Some were mainly to identify the name of a clan or other name, while typical inscriptions include the maker's clan name and the posthumous title of the ancestor who is commemorated by the making and use of the vessel. These inscriptions, especially those late period examples identifying a name, are typically executed in a script of highly pictographic flavor, which preserves the formal, complex Shang writing as would have primarily been written on bamboo or wood books, as opposed to the concurrent simplified, linearized and more rectilinear form of writing as seen on the oracle bones.\n\nA few Shang inscriptions have been found which were brush-written on pottery, stone, jade or bone artifacts, and there are also some bone engravings on non-divination matters written in a complex, highly pictographic style; the structure and style of the bronze inscriptions is consistent with these.\n\nThe soft clay of the piece-molds used to produce the Shang to early Zhou bronzes was suitable for preserving most of the complexity of the brush-written characters on such books and other media, whereas the hard, bony surface of the oracle bones was difficult to engrave, spurring significant simplification and conversion to rectilinearity. Furthermore, some of the characters on the Shang bronzes may have been more complex than normal due to particularly conservative usage in this ritual medium, or when recording identificational inscriptions (clan or personal names); some scholars instead attribute this to purely decorative considerations. Shang bronze script may thus be considered a \"formal\" script, similar to but sometimes even more complex than the unattested daily Shang script on bamboo and wood books and other media, yet far more complex than the Shang script on the oracle bones.\n\nWestern Zhou dynasty characters (as exemplified by bronze inscriptions of that time) basically continue from the Shang writing system; that is, early W. Zhou forms resemble Shang bronze forms (both such as clan names, and typical writing), without any clear or sudden distinction. They are, like their Shang predecessors in all media, often irregular in shape and size, and the structures and details often vary from one piece of writing to the next, and even within the same piece. Although most are not pictographs in function, the early Western Zhou bronze inscriptions have been described as more pictographic in flavor than those of subsequent periods. During the Western Zhou, many graphs begin to show signs of simplification and linearization (the changing of rounded elements into squared ones, solid elements into short line segments, and thick, variable-width lines into thin ones of uniform width), with the result being a decrease in pictographic quality, as depicted in the chart below.\n\nSome flexibility in orientation of graphs (rotation and reversibility) continues in the Western Zhou, but this becomes increasingly scarce throughout the Zhou dynasty. The graphs start to become slightly more uniform in structure, size and arrangement by the time of the third Zhou sovereign, King Kāng, and after the ninth, King Yì, this trend becomes more obvious.\n\nSome have used the problematic term \"large seal\" (大篆 \"dàzhuàn\") to refer to the script of this period. This term dates back to the Han dynasty, when (small) seal script and clerical script were both in use. It thus became necessary to distinguish between the two, as well as any earlier script forms which were still accessible in the form of books and inscriptions, so the terms \"large seal\" (大篆 \"dàzhuàn\") and \"small seal\" (小篆 \"xiǎozhuàn\", aka 秦篆 \"Qín zhuàn\") came into being. However, since the term \"large seal\" is \"variously\" used to describe \"zhòuwén\" (籀文) examples from the ca. 800 BCE Shizhoupian compendium, \"or\" inscriptions on both late W. Zhou bronze inscriptions and the Stone Drums of Qin, \"or\" all forms (including oracle bone script) predating small seal, the term is best avoided entirely.\n\nBy the beginning of the Eastern Zhou, in the Spring and Autumn period, many graphs are fully linearized, as seen in the chart above; additionally, curved lines are straightened, and disconnected lines are often connected, with the result of greater convenience in writing, but a marked decrease in pictographic quality.\n\nIn the Eastern Zhou, the various states initially continued using the same forms as in the late Western Zhou. However, regional forms then began to diverge stylistically as early as the Spring and Autumn period, with the forms in the state of Qin remaining more conservative. At this time, seals and minted coins, both probably primarily of bronze, were already in use, according to traditional documents, but none of the extant seals have yet been indisputably dated to that period.\n\nBy the mid to late Spring and Autumn period, artistic derivative scripts with vertically elongated forms appeared on bronzes, especially in the eastern and southern states, and remained in use into the Warring States period (see detail of inscription from the Warring States Tomb of Marquis Yĭ of Zēng below left). In the same areas, in the late Spring and Autumn to early Warring States, scripts which embellished basic structures with decorative forms such as birds or worms also appeared. These are known as Bird Script (\"niǎoshū\" 鳥書) and Worm Script (\"chóngshū\" 蟲書), and collectively as Bird-worm scripts, (\"niǎochóngshū\" 鳥蟲書; see Bronze sword of King Gōujiàn to right); however, these were primarily decorative forms for inscriptions on bronzes and other items, and not scripts in daily use. Some bronzes of the period were incised in a rough, casual manner, with graph structures often differing somewhat from typical ones. It is thought that these reflected the popular (vulgar) writing of the time which coexisted with the formal script.\n\nSeals have been found from the Warring States period, mostly cast in bronze, and minted bronze coins from this period are also numerous. These form an additional, valuable resource for the study of Chinese bronze inscriptions. It is also from this period that the first surviving bamboo and silk manuscripts have been uncovered.\n\nIn the early Warring States period, typical bronze inscriptions were similar in content and length to those in the late Western Zhou to Spring and Autumn period. One of the most famous sets of bronzes ever discovered dates to the early Warring States: a large set of \"biānzhōng\" bells from the tomb of Marquis Yĭ of the state of Zēng, unearthed in 1978. The total length of the inscriptions on this set was almost 2,800 characters. \n\nIn the mid to late Warring States period, the average length of inscriptions decreased greatly. Many, especially on weapons, recorded only the date, maker and so on, in contrast with earlier narrative contents. Beginning at this time, such inscriptions were typically engraved onto the already cast bronzes, rather than being written into the wet clay of piece-molds as had been the earlier practice. The engraving was often roughly and hastily executed.\n\nIn Warring States period bronze inscriptions, trends from the late Spring and Autumn period continue, such as the use of artistically embellished scripts (e.g., Bird and Insect Scripts) on decorated bronze items. In daily writing, which was not embellished in this manner, the typical script continued evolving in different directions in various regions, and this divergence was accelerated by both a lack of central political control as well as the spread of writing outside of the nobility. In the state of Qin, which was somewhat culturally isolated from the other states, and which was positioned on the old Zhou homeland, the script became more uniform and stylistically symmetrical, rather than changing much structurally. Change in the script was slow, so it remained more similar to the typical late Western Zhou script as found on bronzes of that period and the Shĭ Zhoù Piān (史籀篇) compendium of ca. 800 BCE. As a result, it was not until around the middle of the Warring States period that popular (aka common or vulgar) writing gained momentum in Qin, and even then, the vulgar forms remained somewhat similar to traditional forms, changing primarily in terms of becoming more rectilinear. Traditional forms in Qin remained in use as well, so that two forms of writing coexisted. The traditional forms in Qin evolved slowly during the Eastern Zhou, gradually becoming what is now called (small) seal script during that period, without any clear dividing line (it is not the case, as is commonly believed, that small seal script was a sudden invention by Li Si in the Qin dynasty). Meanwhile, the Qin vulgar writing evolved into early clerical (or proto-clerical) in the late Warring States to Qin dynasty period, which would then evolve further into the clerical script used in the Han through the Wei-Jin periods.\n\nMeanwhile, in the eastern states, vulgar forms had become popular sooner; they also differed more radically from and more completely displaced the traditional forms. These eastern scripts, which also varied somewhat by state or region, were later misunderstood by Xu Shen, author of the Han dynasty etymological dictionary \"Shuowen Jiezi\", who thought they predated the Warring States Qin forms, and thus labeled them \"gǔwén\" (古文), or \"ancient script\".\n\nIt has been anticipated that Bronze script will some day be encoded in Unicode. Codepoints U+32000 to U+32FFF (Plane 3, Tertiary Ideographic Plane) have been tentatively allocated.\n\n\n"}
{"id": "54046997", "url": "https://en.wikipedia.org/wiki?curid=54046997", "title": "Circannual cycle", "text": "Circannual cycle\n\nA circannual cycle is a biological process that occurs in living creatures over the period of approximately one year. This cycle was first discovered by Ebo Gwinner and Canadian biologist Ted Pengelley. It is classified as an Infradian rhythm, which is biological process with a period longer than that of a circadian rhythm, less than one cycle per 28 hours. These processes continue even in artificial environments in which seasonal cues have been removed by scientists. The term circannual is Latin, circa meaning approximately and annual relating to one year. Chronobiology is the field of biology pertaining to periodic rhythms that occur in living organisms in response to external stimuli such as photoperiod.\n\nThe location of the physical circannual timer in organisms and how it works are almost entirely unknown.\n\nIn one study performed by Eberhard Gwinner, two species of birds were born in a controlled environment without ever being exposed to external stimuli. They were presented with a fixed Photoperiod of 10 hours of light and 14 hours of darkness each day. The birds were exposed to these conditions for eight years and consistently molted at the same time as they would have in the wild, indicating that this physiological cycle is innate rather than governed environmentally.\n\nResearchers Ted Pengelley and Ken Fisher studied the circannual clock in the golden-mantled ground squirrel. They exposed the squirrels to twelve hours of light and 12 hours of darkness and at a constant temperature for three years. Despite this constant cycle, they continued to hibernate once a year with each episode preceded by an increase in body weight and food consumption. During the first year, the squirrels began hibernation in late October. They started hibernating in mid August and early April respectively for the following two years, displaying a circannual rhythm with a period of about 10 months.\n\nA classic example in insects is the varied carpet beetle.\n\nGenerating biological rhythms internally helps organisms anticipate important changes in the environment before they occur, thus providing the organisms with time to prepare and survive. For example, some plants have a very strict time frame in regards to blooming and preparing for spring. If they begin their preparations too early or too late they risk not being pollinated, competing with different species, or other factors that might affect their survival rate. Having a circannual cycle may keep them from making this mistake if a particular geographic region experiences a false spring, where the weather becomes exceptionally warm early for a short period of time before returning to winter temperatures.\n\nSimilarly, bird plumage and mammal fur change with the approach of winter, and is triggered by the shortening photoperiod of autumn. The circannual cycle can also be useful for animals that Migrate or Hibernate. Many animals' reproductive organs change in response to changes in photoperiod. Male gonads will grow during the onset of spring to promote reproduction among the species. These enlarged gonads would be nearly impossible to keep year round and would be inefficient for the species. Many female animals will only produce eggs during certain times of the year.\n\nChanging climate may unravel ecosystems in which different organisms use different internal calendars. Warming temperatures may lead to earlier blooms of flora in spring. For instance, one study performed by Menzel et al., analyzed 125,000 phenological records of 542 plant species in 21 European countries from 1971 to 2000 and found that 78% of all plants studied advanced in flowering, leafing, and fruiting while only three percent were significantly delayed. They determined that the average advance of spring and summer was 2.5 days per decade in Europe. Meanwhile, fauna may breed or migrate based on the length of day, and thus might arrive too late for critical food supplies they co-evolved with.\n\nFor example, the Parus major closely times the hatching of their chicks to the emergence of the protein-rich winter moth caterpillar, which in turn hatches to meet the budding of oaks. These birds are a single-brood bird, meaning they breed once a year with about nine chicks per brood. If the birds and caterpillars and buds all emerge at the right time, the caterpillars eat the new oak leaves and their population increases dramatically, and this hopefully will coincide with the arrival of the new chicks, allowing them to eat. But if plants, insects, and birds respond differently to the advance of spring or other phenology changes, the relationship may be altered.\n\nAs another example, studies of the Pied Flycatcher (ficedula hypoleuca) have shown that their spring migration timing is triggered by an internal circannual clock that is fine tuned to day length. These particular birds overwinter in dry tropical forest in Western Africa and breed in temperate forests in Europe, over 4,500 km away. From 1980-2000, temperatures at the time of arrival and the start of breeding have warmed significantly. They have advanced their mean laying date by ten days, but have not advanced the spring arrival on their breeding grounds because their migration behavior is triggered by photoperiod rather than temperature.\n\nIn short, even if each individual species can easily live with elevated temperatures, disruptions of phenology timing at ecosystem level may still imperil them.\n\nOne reason for the paucity of research on circannual cycles is the duration of required efforts. The ratio of the period length of a circannual cycle to the length of the productive life of a scientist makes this branch of chronobiology difficult. It takes an entire year to get a time series which makes it difficult to see how these cycles adjust over the years. To put this into perspective, a two-week experiment for a circadian biologist would take fourteen years for a circannual researcher, in order to achieve the same level of data robustness for the conclusions.\n"}
{"id": "2760015", "url": "https://en.wikipedia.org/wiki?curid=2760015", "title": "Control Structure Diagram", "text": "Control Structure Diagram\n\nThe Control Structure Diagram automatically documents the program flow within the source code and adds indentation with graphical symbols. Thereby the source code becomes visibly structured without sacrificing space.\n\n\n"}
{"id": "21084005", "url": "https://en.wikipedia.org/wiki?curid=21084005", "title": "Control flow diagram", "text": "Control flow diagram\n\nA control flow diagram (CFD) is a diagram to describe the control flow of a business process, process or review\n\nControl flow diagrams were developed in the 1950s, and are widely used in multiple engineering disciplines. They are one of the classic business process modeling methodologies, along with flow charts, drakon-charts, data flow diagrams, functional flow block diagram, Gantt charts, PERT diagrams, and IDEF.\n\nA control flow diagram can consist of a subdivision to show sequential steps, with if-then-else conditions, repetition, and/or case conditions. Suitably annotated geometrical figures are used to represent operations, data, or equipment, and arrows are used to indicate the sequential flow from one to another.\n\nThere are several types of control flow diagrams, for example:\n\nIn software and systems development control flow diagrams can be used in control flow analysis, data flow analysis, algorithm analysis, and simulation. Control and data are most applicable for real time and data driven systems. These flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text. PERT, state transition, and transaction diagrams are examples of control flow diagrams.\n\nA flow diagram can be developed for the process control system for each critical activity. Process control is normally a closed cycle in which a sensor . The application determines if the sensor information is within the predetermined (or calculated) data parameters and constraints. The results of this comparison, which controls the critical component. This feedback may control the component electronically or may indicate the need for a manual action.\nThis closed-cycle process has many checks and balances to ensure that it stays safe. \nFurther, some process control systems may use prior generations of hardware and software, while others are state of the art\n\nThe figure presents an example of a performance seeking control flow diagram of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the Kalman filter estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded. \n\nIn addition to temperatures, pressures, and control positions, such estimated parameters as stall margins, thrust, and drag components were recorded. In the optimization phase, the operating condition constraints, optimal solution, and linear programming health status condition codes were recorded. Finally, the actual commands that were sent to the engine through the DEEC were recorded.\n\n"}
{"id": "55112880", "url": "https://en.wikipedia.org/wiki?curid=55112880", "title": "Cute aggression", "text": "Cute aggression\n\nCute aggression is a superficially aggressive behaviour or reaction caused by experiencing something cute, especially a human baby or young animal. People experiencing playful aggression may grit their teeth, clench their fists, or feel the urge to pinch and squeeze something they consider cute, while not actually causing or intending to cause any harm.\n\nThe term \"cute aggression\" was published widely in 2013, after Rebecca Dyer presented research on the topic at the annual meeting of the Society for Personality and Social Psychology on January 18. The term \"playful aggression\" was subsequently used in a 2015 paper by Oriana R. Aragón and colleagues, and defined as follows:\nPlayful aggression is in reference to the expressions that people show sometimes when interacting with babies. Sometimes we say things and appear to be more angry than happy, even though we are happy. For example some people grit their teeth, clench their hands, pinch cheeks, or say things like “I want to eat you up!” It would be difficult to ask about every possible behavior of playful aggression, so we ask generally about things of this kind—calling them playful aggressions.\nThe concept of playful aggression is also captured in several non-English terms. In Filipino, for example, the word \"gigil\" refers to \"the gritting of teeth and the urge to pinch or squeeze something that is unbearably cute.\"\n\nPlayful aggression is a type of dimorphous display, in which a positive experience elicits expressions usually associated with negative emotions. This behaviour occurs more commonly in individuals who experience dimorphous emotions across a range of situations, and may help to regulate emotions by balancing an overwhelmingly positive emotion with a negative response. \n"}
{"id": "49109026", "url": "https://en.wikipedia.org/wiki?curid=49109026", "title": "Cyborg art", "text": "Cyborg art\n\nCyborg art, also known as cyborgism, is an art movement that began in the mid-2000s in Britain. It is based on the creation and addition of new senses to the body via cybernetic implants and the creation of art works through new senses. Cyborg artworks are created by cyborg artists; artists whose senses have been voluntarily enhanced through cybernetic implants. Among the early artists shaping the cyborg art movement are Neil Harbisson, whose antenna implant allows him to perceive ultraviolet and infrared colours, and Moon Ribas whose implants in her elbows allow her to feel earthquakes and moonquakes. Other cyborg artists include: \n\nManel Muñoz, a Catalan photographer who developed and installed a barometric system in his ears that allows him to perceive atmospheric pressure changes.\n\nJoe Dekni, an artist who has developed and installed a radar system in his head. The sensory system includes two implants in his cheekbones.\n"}
{"id": "3461736", "url": "https://en.wikipedia.org/wiki?curid=3461736", "title": "Data visualization", "text": "Data visualization\n\nData visualization is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data.\n\nTo communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message. Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.\n\nData visualization is both an art and a science. It is viewed as a branch of descriptive statistics by some, but also as a grounded theory development tool by others. Increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as \"big data\" or Internet of things. Processing, analyzing and communicating this data present ethical and analytical challenges for data visualization. The field of data science and practitioners called data scientists help address this challenge.\n\nData visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in data analysis or data science. According to Friedman (2008) the \"main goal of data visualization is to communicate information clearly and effectively through graphical means. It doesn't mean that data visualization needs to look boring to be functional or extremely sophisticated to look beautiful. To convey ideas effectively, both aesthetic form and functionality need to go hand in hand, providing insights into a rather sparse and complex data set by communicating its key-aspects in a more intuitive way. Yet designers often fail to achieve a balance between form and function, creating gorgeous data visualizations which fail to serve their main purpose — to communicate information\".\n\nIndeed, Fernanda Viegas and Martin M. Wattenberg suggested that an ideal visualization should not only communicate clearly, but stimulate viewer engagement and attention.\n\nData visualization is closely related to information graphics, information visualization, scientific visualization, exploratory data analysis and statistical graphics. In the new millennium, data visualization has become an active area of research, teaching and development. According to Post et al. (2002), it has united scientific and information visualization.\n\nProfessor Edward Tufte explained that users of information displays are executing particular \"analytical tasks\" such as making comparisons. The \"design principle\" of the information graphic should support the analytical task. As William Cleveland and Robert McGill show, different graphical elements accomplish this more or less effectively. For example, dot plots and bar charts outperform pie charts.\n\nIn his 1983 book \"The Visual Display of Quantitative Information\", Edward Tufte defines 'graphical displays' and principles for effective graphical display in the following passage:\n\"Excellence in statistical graphics consists of complex ideas communicated with clarity, precision and efficiency. Graphical displays should:\nGraphics \"reveal\" data. Indeed graphics can be more precise and revealing than conventional statistical computations.\"\n\nFor example, the Minard diagram shows the losses suffered by Napoleon's army in the 1812–1813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, direction of movement, and temperature. The line width illustrates a comparison (size of the army at points in time) while the temperature axis suggests a cause of the change in army size. This multivariate display on a two dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility. Tufte wrote in 1983 that: \"It may well be the best statistical graphic ever drawn.\"\n\nNot applying these principles may result in misleading graphs, which distort the message or support an erroneous conclusion. According to Tufte, chartjunk refers to extraneous interior decoration of the graphic that does not enhance the message, or gratuitous three dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of \"administrative debris.\" The ratio of \"data to ink\" should be maximized, erasing non-data ink where feasible.\n\nThe Congressional Budget Office summarized several best practices for graphical displays in a June 2014 presentation. These included: a) Knowing your audience; b) Designing graphics that can stand alone outside the context of the report; and c) Designing graphics that communicate the key messages in the report.\n\nAuthor Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message:\n\nAnalysts reviewing a set of data may consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.\n\nA human can distinguish differences in line length, shape, orientation, and color (hue) readily without significant processing effort; these are referred to as \"pre-attentive attributes\". For example, it may require significant time and effort (\"attentive processing\") to identify the number of times the digit \"5\" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.\n\nEffective graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).\n\nAlmost all data visualizations are created for human consumption. Knowledge of human perception and cognition is necessary when designing intuitive visualizations. Cognition refers to processes in human beings like perception, attention, learning, memory, thought, concept formation, reading, and problem solving. Human visual processing is efficient in detecting changes and making comparisons between quantities, sizes, shapes and variations in lightness. When properties of symbolic data are mapped to visual properties, humans can browse through large amounts of data efficiently. It is estimated that 2/3 of the brain's neurons can be involved in visual processing. Proper visualization provides a different approach to show potential connections, relationships, etc. which are not as obvious in non-visualized quantitative data. Visualization can become a means of data exploration.\n\nThere is no comprehensive 'history' of data visualization. There are no accounts that span the entire development of visual thinking and the visual\nrepresentation of data, and which collate the contributions of disparate disciplines. Michael Friendly and Daniel J Denis of York University are engaged in a project that attempts to provide a comprehensive history of visualization. Contrary to general belief, data visualization is not a modern development. Stellar data, or information such as location of stars were visualized on the walls of caves (such as those found in Lascaux Cave in Southern France) since the Pleistocene era. Physical artefacts such as Mesopotamian clay tokens (5500 BC), Inca quipus (2600 BC) and Marshall Islands stick charts (n.d.) can also be considered as visualizing quantitative information.\n\nFirst documented data visualization can be tracked back to 1160 B.C. with Turin Papyrus Map which accurately illustrates the distribution of geological resources and provides information about quarrying of those resources. Such maps can be categorized as Thematic Cartography, which is a type of data visualization that presents and communicates specific data and information through a geographical illustration designed to show a particular theme connected with a specific geographic area. Earliest documented forms of data visualization were various thematic maps from different cultures and ideograms and hieroglyphs that provided and allowed interpretation of information illustrated. For example, Linear B tablets of Mycenae provided a visualization of information regarding Late Bronze Age era trades in the Mediterranean. The idea of coordinates was used by ancient Egyptian surveyors in laying out towns, earthly and heavenly positions were located by something akin to latitude and longitude at least by 200 BC, and the map projection of a spherical earth into latitude and longitude by Claudius Ptolemy [c.85–c. 165] in Alexandria would serve as reference standards until the 14th century.\n\nInvention of paper and parchment allowed further development of visualizations throughout history. Figure shows a graph from the 10th, possibly 11th century that is intended to be an illustration of the planetary movement, used in an appendix of a textbook in monastery schools. The graph apparently was meant to represent a plot of the inclinations of the planetary orbits as a function of the time. For this purpose the zone of the zodiac was represented on a plane with a horizontal line divided into thirty parts as the time or longitudinal axis. The vertical axis designates the width of the zodiac. The horizontal scale appears to have been chosen for each planet individually for the periods cannot be reconciled. The accompanying text refers only to the amplitudes. The curves are apparently not related in time. \n\nBy the 16th century, techniques and instruments for precise observation and measurement of physical quantities, and geographic and celestial position were well-developed (for example, a “wall quadrant” constructed by Tycho Brahe [1546–1601], covering an entire wall in his observatory). Particularly important were the development of triangulation and other methods to determine mapping locations accurately.\n\nFrench philosopher and mathematician René Descartes and Pierre de Fermat developed analytic geometry and two-dimensional coordinate system which heavily influenced the practical methods of displaying and calculating values. Fermat and Blaise Pascal's work on statistics and probability theory laid the groundwork for what we now conceptualize as data. According to the Interaction Design Foundation, these developments allowed and helped William Playfair, who saw potential for graphical communication of quantitative data, to generate and develop graphical methods of statistics. In the second half of the 20th century, Jacques Bertin used quantitative graphs to represent information \"intuitively, clearly, accurately, and efficiently\".\n\nJohn Tukey and Edward Tufte pushed the bounds of data visualization; Tukey with his new statistical approach of exploratory data analysis and Tufte with his book \"The Visual Display of Quantitative Information\" paved the way for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand drawn visualizations and evolving into more technical applications – including interactive designs leading to software visualization.\n\nPrograms like SAS, SOFA, R, Minitab, Cornerstone and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility. Private schools have also developed programs to meet the demand for learning data visualization and associated programming libraries, including free programs like The Data Incubator or paid programs like General Assembly.\n\nData visualization involves specific terminology, some of which is derived from statistics. For example, author Stephen Few defines two types of data, which are used in combination to support a meaningful analysis or visualization:\n\nTwo primary types of information displays are tables and graphs.\n\nEppler and Lengler have developed the \"Periodic Table of Visualization Methods,\" an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound.\n\nThere are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008). Friendly (2008) presumes two main parts of data visualization: statistical graphics, and thematic cartography. In this line the \"Data Visualization: Modern Approaches\" (2007) article gives an overview of seven subjects of data visualization:\nAll these subjects are closely related to graphic design and information representation.\nOn the other hand, from a computer science perspective, Frits H. Post in 2002 categorized the field into sub-fields:\n\nData presentation architecture (DPA) is a skill-set that seeks to identify, locate, manipulate, format and present data in such a way as to optimally communicate meaning and proper knowledge.\n\nHistorically, the term \"data presentation architecture\" is attributed to Kelly Lautt: \"Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of Business Intelligence. Data presentation architecture weds the science of numbers, data and statistics in discovering valuable information from data and making it usable, relevant and actionable with the arts of data visualization, communications, organizational psychology and change management in order to provide business intelligence solutions with the data scope, delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen. Data visualization skills are one element of DPA.\"\n\nDPA has two main objectives:\n\nWith the above objectives in mind, the actual work of data presentation architecture consists of:\n\nDPA work shares commonalities with several other fields, including:\n\n\n"}
{"id": "18638809", "url": "https://en.wikipedia.org/wiki?curid=18638809", "title": "Degenerate semiconductor", "text": "Degenerate semiconductor\n\nA degenerate semiconductor is a semiconductor with such a high level of doping that the material starts to act more like a metal than as a semiconductor.\n\nAt moderate doping levels the dopant atoms create individual doping levels that can often be considered as localized states that can donate electrons or holes by thermal promotion (or an optical transition) to the conduction or valence bands respectively. At high enough impurity concentrations the individual impurity atoms may become close enough neighbors that their doping levels merge into an impurity band and the behavior of such a system ceases to show the typical traits of a semiconductor, e.g. its increase in conductivity with temperature. On the other hand, a degenerate semiconductor still has far fewer charge carriers than a true metal so that its behavior is in many ways intermediary between semiconductor and metal.\n\nMany copper chalcogenides are degenerate p-type semiconductors with relatively large numbers of holes in their valence band. An example is the system LaCuOSSe with Mg doping. It is a wide gap p-type degenerate semiconductor. The hole concentration does not change with temperature, a typical trait of degenerate semiconductors.\n\nAnother well known example is indium tin oxide. Because its plasma frequency is in the IR-range it is a fairly good metallic conductor, but transparent in the visible range of the spectrum.\n"}
{"id": "19169866", "url": "https://en.wikipedia.org/wiki?curid=19169866", "title": "Disability etiquette", "text": "Disability etiquette\n\nDisability etiquette is a set of guidelines dealing specifically with how to approach a person with a disability.\n\nThere is no consensus on when this phrase first came into use, although it most likely grew out of the Disability Rights Movement that began in the early 1970s. The concept may have started as a cynical play on existing rule sheets, written for audiences without a disability, that were seen as patronizing by civil rights activists.\n\nMost disability etiquette guidelines seem to be predicated on a simple dictate: \"Do not assume ...\" They are written to address real and perceived shortcomings in how society as a whole treats disabled people.\n\nThese guidelines can be broken down into the several broad categories.\n\n\"Do not assume ...\":\n\nEach category encompasses specific \"rules.\" For example, the last two of these would include guidelines such as:\n\nPeople writing on specific disabilities have given rise to their own unique guidelines. Wheelchair users may, for example, include the rule, \"do not grab the push handles of a person's wheelchair without permission.\" Visually impaired people often list a request to, \"identify yourself when you enter a room.\"\n\nLike many other minority groups, disabled people do not always agree on what constitutes politically correct language. However, see the List of disability-related terms with negative connotations and people-first language.\n\n"}
{"id": "56061", "url": "https://en.wikipedia.org/wiki?curid=56061", "title": "Discrete space", "text": "Discrete space\n\nIn topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a \"discontinuous sequence\", meaning they are \"isolated\" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set, i.e., it defines all subsets as open sets. In particular, each singleton is an open set in the discrete topology.\n\nGiven a set \"X\":\nfor any formula_3. In this case formula_4 is called a discrete metric space or a space of isolated points.\n\nA metric space formula_16 is said to be \"uniformly discrete\" if there exists a \"packing radius\" formula_17 such that, for any formula_18, one has either formula_19 or formula_20. The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set {1, 1/2, 1/4, 1/8, ...} of real numbers.\nLet X = {1, 1/2, 1/4, 1/8, ...}, consider this set using the usual metric on the real numbers. Then, X is a discrete space, since for each point 1/2, we can surround it with the interval (1/2 - ɛ, 1/2 + ɛ), where ɛ = 1/2(1/2 - 1/2) = 1/2. The intersection (1/2 - ɛ, 1/2 + ɛ) ∩ {1/2} is just the singleton {1/2}. Since the intersection of two open sets is open, and singletons are open, it follows that X is a discrete space.\n\nHowever, X cannot be uniformly discrete. To see why, suppose there exists an r>0 such that d(x,y)>r whenever x≠y. It suffices to show that there are at least two points x and y in X that are closer to each other than r. Since the distance between adjacent points 1/2 and 1/2 is 1/2, we need to find an n that satisfies this inequality:\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nformula_26\n\nSince there is always an n bigger than any given real number, it follows that there will always be at least two points in X that are closer to each other than any positive r, therefore X is not uniformly discrete.\n\nThe underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.\nThus, the different notions of discrete space are compatible with one another.\nOn the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space \"X\" := {1/\"n\" : \"n\" = 1,2,3...} (with metric inherited from the real line and given by d(\"x\",\"y\") = |\"x\" − \"y\"|).\nObviously, this is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.\nNevertheless, it is discrete as a topological space.\nWe say that \"X\" is \"topologically discrete\" but not \"uniformly discrete\" or \"metrically discrete\".\n\nAdditionally:\n\nAny function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space \"X\" is free on the set \"X\" in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.\n\nWith metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.\n\nGoing the other direction, a function \"f\" from a topological space \"Y\" to a discrete space \"X\" is continuous if and only if it is \"locally constant\" in the sense that every point in \"Y\" has a neighborhood on which \"f\" is constant.\n\nA discrete structure is often used as the \"default structure\" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as \"extreme\" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as \"discrete groups\" . In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytical manifold) is nothing but a discrete topological space. We can therefore view any discrete group as a 0-dimensional Lie group.\n\nA product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space {0,1} is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.)\n\nIn the foundations of mathematics, the study of compactness properties of products of {0,1} is central to the topological approach to the ultrafilter principle, which is a weak form of choice.\n\nIn some ways, the opposite of the discrete topology is the trivial topology (also called the \"indiscrete topology\"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function \"from\" a topological space \"to\" an indiscrete space is continuous, etc.\n\n\n"}
{"id": "990632", "url": "https://en.wikipedia.org/wiki?curid=990632", "title": "Dynamical systems theory", "text": "Dynamical systems theory\n\nDynamical systems theory is an area of mathematics used to describe the behavior of the complex dynamical systems, usually by employing differential equations or difference equations. When differential equations are employed, the theory is called \"continuous dynamical systems\". From a physical point of view, continuous dynamical systems is a generalization of classical mechanics, a generalization where the equations of motion are postulated directly and are not constrained to be Euler–Lagrange equations of a least action principle. When difference equations are employed, the theory is called \"discrete dynamical systems\". When the time variable runs over a set that is discrete over some intervals and continuous over other intervals or is any arbitrary time-set such as a cantor set, one gets dynamic equations on time scales. Some situations may also be modeled by mixed operators, such as differential-difference equations.\n\nThis theory deals with the long-term qualitative behavior of dynamical systems, and studies the nature of, and when possible the solutions of, the equations of motion of systems that are often primarily mechanical or otherwise physical in nature, such as planetary orbits and the behaviour of electronic circuits, as well as systems that arise in biology, economics, and elsewhere. Much of modern research is focused on the study of chaotic systems.\n\nThis field of study is also called just \"dynamical systems\", \"mathematical dynamical systems theory\" or the \"mathematical theory of dynamical systems\".\nDynamical systems theory and chaos theory deal with the long-term qualitative behavior of dynamical systems. Here, the focus is not on finding precise solutions to the equations defining the dynamical system (which is often hopeless), but rather to answer questions like \"Will the system settle down to a steady state in the long term, and if so, what are the possible steady states?\", or \"Does the long-term behavior of the system depend on its initial condition?\"\n\nAn important goal is to describe the fixed points, or steady states of a given dynamical system; these are values of the variable that don't change over time. Some of these fixed points are \"attractive\", meaning that if the system starts out in a nearby state, it converges towards the fixed point.\n\nSimilarly, one is interested in \"periodic points\", states of the system that repeat after several timesteps. Periodic points can also be attractive. Sharkovskii's theorem is an interesting statement about the number of periodic points of a one-dimensional discrete dynamical system.\n\nEven simple nonlinear dynamical systems often exhibit seemingly random behavior that has been called \"chaos\". The branch of dynamical systems that deals with the clean definition and investigation of chaos is called \"chaos theory\".\n\nThe concept of dynamical systems theory has its origins in Newtonian mechanics. There, as in other natural sciences and engineering disciplines, the evolution rule of dynamical systems is given implicitly by a relation that gives the state of the system only a short time into the future.\n\nBefore the advent of fast computing machines, solving a dynamical system required sophisticated mathematical techniques and could only be accomplished for a small class of dynamical systems.\n\nSome excellent presentations of mathematical dynamic system theory include , , , and .\n\nThe dynamical system concept is a mathematical formalization for any fixed \"rule\" that describes the time dependence of a point's position in its ambient space. Examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, and the number of fish each spring in a lake.\n\nA dynamical system has a \"state\" determined by a collection of real numbers, or more generally by a set of points in an appropriate \"state space\". Small changes in the state of the system correspond to small changes in the numbers. The numbers are also the coordinates of a geometrical space—a manifold. The \"evolution rule\" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule may be deterministic (for a given time interval only one future state follows from the current state) or stochastic (the evolution of the state is subject to random shocks).\n\nDynamicism, also termed the \"dynamic hypothesis\" or the \"dynamic hypothesis in cognitive science\" or \"dynamic cognition\", is a new approach in cognitive science exemplified by the work of philosopher Tim van Gelder. It argues that differential equations are more suited to modelling cognition than more traditional computer models.\n\nIn mathematics, a nonlinear system is a system that is not linear—i.e., a system that does not satisfy the superposition principle. Less technically, a nonlinear system is any problem where the variable(s) to solve for cannot be written as a linear sum of independent components. A nonhomogeneous system, which is linear apart from the presence of a function of the independent variables, is nonlinear according to a strict definition, but such systems are usually studied alongside linear systems, because they can be transformed to a linear system as long as a particular solution is known.\n\nIn sports biomechanics, dynamical systems theory has emerged in the movement sciences as a viable framework for modeling athletic performance. From a dynamical systems perspective, the human movement system is a highly intricate network of co-dependent sub-systems (e.g. respiratory, circulatory, nervous, skeletomuscular, perceptual) that are composed of a large number of interacting components (e.g. blood cells, oxygen molecules, muscle tissue, metabolic enzymes, connective tissue and bone). In dynamical systems theory, movement patterns emerge through generic processes of self-organization found in physical and biological systems. There is no research validation of any of the claims associated to the conceptual application of this framework.\n\nDynamical system theory has been applied in the field of neuroscience and cognitive development, especially in the neo-Piagetian theories of cognitive development. It is the belief that cognitive development is best represented by physical theories rather than theories based on syntax and AI. It also believed that differential equations are the most appropriate tool for modeling human behavior. These equations are interpreted to represent an agent's cognitive trajectory through state space. In other words, dynamicists argue that psychology should be (or is) the description (via differential equations) of the cognitions and behaviors of an agent under certain environmental and internal pressures. The language of chaos theory is also frequently adopted.\n\nIn it, the learner's mind reaches a state of disequilibrium where old patterns have broken down. This is the phase transition of cognitive development. Self-organization (the spontaneous creation of coherent forms) sets in as activity levels link to each other. Newly formed macroscopic and microscopic structures support each other, speeding up the process. These links form the structure of a new state of order in the mind through a process called \"scalloping\" (the repeated building up and collapsing of complex performance.) This new, novel state is progressive, discrete, idiosyncratic and unpredictable.\n\nDynamic systems theory has recently been used to explain a long-unanswered problem in child development referred to as the A-not-B error.\n\nThe application of Dynamic Systems Theory to study second language acquisition is attributed to Diane Larsen-Freeman who published an article in 1997 in which she claimed that second language acquisition should be viewed as a developmental process which includes language attrition as well as language acquisition. In her article she claimed that language should be viewed as a dynamic system which is dynamic, complex, nonlinear, chaotic, unpredictable, sensitive to initial conditions, open, self-organizing, feedback sensitive, and adaptive.\n\n\n\n\n\n"}
{"id": "22319522", "url": "https://en.wikipedia.org/wiki?curid=22319522", "title": "EOTD", "text": "EOTD\n\neOTD is the acronym for the ECCMA Open Technical Dictionary. The dictionary is a language-independent database of concepts with associated terms, definitions and images used to unambiguously describe individuals, organizations, locations, goods, services, processes, rules, and regulations. The eOTD is maintained by the Electronic Commerce Code Management Association (ECCMA).\n\nThe eOTD was developed with the support of the Defense Logistics Information Service (DLIS) an agency of the US Defense Logistics Agency (DLA). The eOTD is the first dictionary to be compliant with ISO 22745 (open technical dictionaries).\n\nThe eOTD contains terms, definitions and images linked to concept identifiers. eOTD concept identifiers are used to create unambiguous language independent descriptions of individuals, organizations, locations, goods, services, processes, rules and regulations. The process of using concept identifiers from an external open technical dictionary is a form of semantic encoding compliant with the requirements of ISO 8000-110:2008, the international standard for the exchange of quality master data.\n\nThe eOTD concept identifiers are in the public domain. Using public domain identifiers as metadata creates portable data that can be legally separated from the software application that was used to create it. The dictionary contains concepts from international, national and industry standards including over 400,000 concepts of class (approved item name), property (attribute), units of measure, currency and common enumerated value (e.g., days of the week). The eOTD does not include a class hierarchy or class-property relationships.\n\nCompanies use the eOTD to create data requirement specifications as Identification Guides (IGs) or cataloging templates. These Identification Guides contain the class-property relationships and are used for cataloging, to measure data quality as well as to create requests for data or requests for data validation.\n\n\n\n"}
{"id": "188644", "url": "https://en.wikipedia.org/wiki?curid=188644", "title": "Environmental full-cost accounting", "text": "Environmental full-cost accounting\n\nEnvironmental full-cost accounting (EFCA) is a method of cost accounting that traces direct costs and allocates indirect costs by collecting and presenting information about the possible environmental, social and economical costs and benefits or advantagesin short, about the \"triple bottom line\"for each proposed alternative. It is also known as true-cost accounting (TCA), but, as definitions for \"true\" and \"full\" are inherently subjective, experts consider both terms problematical.\n\nSince costs and advantages are usually considered in terms of environmental, economic and social impacts, full or true cost efforts are collectively called the \"triple bottom line\". A large number of standards now exist in this area including Ecological Footprint, eco-labels, and the United Nations International Council for Local Environmental Initiatives approach to triple bottom line using the ecoBudget metric. The International Organization for Standardization (ISO) has several accredited standards useful in FCA or TCA including for greenhouse gases, the ISO 26000 series for corporate social responsibility coming in 2010, and the ISO 19011 standard for audits including all these.\n\nBecause of this evolution of terminology in the public sector use especially, the term full-cost accounting is now more commonly used in management accounting, e.g. infrastructure management and finance. Use of the terms FCA or TCA usually indicate relatively conservative extensions of current management practices, and incremental improvements to GAAP to deal with waste output or resource input.\n\nThese have the advantage of avoiding the more contentious questions of social cost.\n\nFull-cost accounting embodies several key concepts that distinguish it from standard accounting techniques. The following list highlights the basic tenets of FCA.\n\nAccounting for:\n\nExpenditure of cash to acquire or use a resource. A cost is the cash value of the resource as it is used. For example, an outlay is made when a vehicle is purchased, but the cost of the vehicle is incurred over its active life (e.g., 10 years). The cost of the vehicle must be allocated over a period of time because every year of its use contributes to the depreciation of the vehicle's value.\n\nThe value of goods and services is reflected as a cost even if no cash outlay is involved. One community might receive a grant from a state, for example, to purchase equipment. This equipment has value, even though the community did not pay for it in cash. The equipment, therefore, should be valued in an FCA analysis.\n\nGovernment subsidies in the energy and food production industries keep true costs low through artificially cheap product pricing. This price manipulation encourages unsustainable practices and further hides negative externalities endemic to fossil fuel production and modern mechanized agriculture.\n\nFCA accounts for all overhead and indirect costs, including those that are shared with other public agencies. Overhead and indirect costs might include legal services, administrative support, data processing, billing, and purchasing. Environmental costs as indirect costs include the full range of costs throughout the life-cycle of a product (Life cycle assessment), some of which even do not show up in the firm's bottom line. It also contains fixed overhead, fixed administration expense etc.\n\nPast and future cash outlays often do not appear on annual budgets under cash accounting systems. Past (or upfront) costs are initial investments necessary to implement services such as the acquisition of vehicles, equipment, or facilities. Future (or back-end) outlays are costs incurred to complete operations such as facility closure and postclosure care, equipment retirement, and post-employment health and retirement benefits.\n\nFor example, the State of Florida uses the term full-cost accounting for its solid waste management. In this instance, FCA is a systematic approach for identifying, summing, and reporting the actual costs of solid waste management. It takes into account past and future outlays, overhead (oversight and support services) costs, and operating costs.\n\nIntegrated solid waste management systems consist of a variety of municipal solid waste (MSW) activities and paths. Activities are the building blocks of the system, which may include waste collection, operation of transfer stations, transport to waste management facilities, waste processing and disposal, and sale of byproducts. Paths are the directions that MSW follows in the course of integrated solid waste management (i.e., the point of generation through processing and ultimate disposition) and include recycling, composting, waste-to-energy, and landfill disposal. The cost of some activities is shared between paths. Understanding the costs of MSW activities is often necessary for compiling the costs of the entire solid waste system, and helps municipalities evaluate whether to provide a service itself or contract out for it. However, in considering changes that affect how much MSW ends up being recycled, composted, converted to energy, or landfilled, the analyst should focus the costs of the different paths. Understanding the full costs of each MSW path is an essential first step in discussing whether to shift the flows of MSW one way another.\n\n\n\n\n\n\n\n\nVarious motives for adoption of FCA/TCA have been identified. The most significant of which tend to involve anticipating market or regulatory problems associated with ignoring the comprehensive outcome of the whole process or event accounted for. \"In green economics, this is the major concern and basis for critiques of such measures as GDP.\" The public sector has tended to move more towards longer term measures to avoid accusations of political favoritism towards specific solutions that seem to make financial or economic sense in the short term, but not longer term.\n\nCorporate decision makers sometimes call on FCA/TCA measures to decide whether to initiate recalls, practice voluntary product stewardship (a form of recall at the end of a product's useful life). This can be motivated as a hedge against future liabilities arising from those who are negatively affected by the waste a product becomes. Advanced theories of FCA, such as Natural Step, focus firmly on these. According to Ray Anderson, who instituted a form of FCA/TCA at Interface Carpet, used it to rule out decisions that increase Ecological Footprint and focus the company more clearly on a sustainable marketing strategy.\n\nThe urban ecology and industrial ecology approaches inherently advocate FCA — treating the built environment as a sort of ecosystem to minimize its own wastes.\n\n"}
{"id": "33685961", "url": "https://en.wikipedia.org/wiki?curid=33685961", "title": "Fairness dilemmas", "text": "Fairness dilemmas\n\nFairness dilemmas arise when groups are faced with making decisions about how to share their resources, rewards, or payoffs. Since resources are limited, groups need to decide on fair ways of apportioning them out to their members. These fairness judgments are determined by procedural and distributive forms of social justice. When payouts do not occur according to these norms, conflicts arise.\n\nProcedural justice deals with the methods used in making the decisions about the allocation of the group's resources. Issues arise in this area when groups do not stick to their agreed upon methods for spreading out these resources. Conflicts will arise when the methods used are not consistent, open, or agreed upon by the group. The question asked by group members in this type of social justice is \"did we make the decision in a fair way?\"\n\nDistributive justice deals with how rewards and costs are distributed across the members of the group. Issues might arise in this area when certain members are favored and consistently given premium rewards or apportioned more resources than other members. An analogy here is cutting up a cake into equal pieces, but someone feels that they have unjustly received a smaller piece. The question asked by group members in this type of social justice is \"did I get my fair share?\"\n\nThere are five types of distributive norms that help in maintaining distributive justice.\n\nThe distribution of resources (most often money) is a very common cause of conflict in groups. Individuals tend to push for equality distribution norms when they have not contributed as much to the group. Smaller groups also tend to adopt equality norms, as do women. On the other hand, larger groups, as well as groups that significantly depend on one person's specialized input, prefer equitable distributions for the group.\nNegative inequity is experienced by group members who feel that they are not receiving fair payment for what they have contributed. This often results in lower quality of work, a reduction in effort (perhaps caused by the sucker effect), or complete withdrawal from the group. In contrast, positive inequality is experienced when members receive too much for the amount of effort they've put forth. This can lead to the individual increasing their efforts to actually deserve what they are receiving, but more often it leads to conflict in the group.\nIndividuals tend to be motivated to maximize their personal rewards, so when they feel they are getting less than they deserve, they react negatively. On the flip side, when members feel like they are being rewarded for their input to the group, and they feel that the group is fairly allocating these rewards, they feel esteemed and proud. Members see a motive for group mates to give less rewards, while being less likely to believe they could be over-rewarded. The reaction in the former instance is shaped more by distributive justice, whereas the latter reaction is shaped more by procedural justice.\n\n"}
{"id": "47921", "url": "https://en.wikipedia.org/wiki?curid=47921", "title": "Free will", "text": "Free will\n\nFree will is the ability to choose between different possible courses of action unimpeded.\n\nFree will is closely linked to the concepts of responsibility, praise, guilt, sin, and other judgements which apply only to actions that are freely chosen. It is also connected with the concepts of advice, persuasion, deliberation, and prohibition. Traditionally, only actions that are freely willed are seen as deserving credit or blame. There are numerous different concerns about threats to the possibility of free will, varying by how exactly it is conceived, which is a matter of some debate.\n\nSome conceive free will to be the capacity to make choices in which the outcome has not been determined by past events. Determinism suggests that only one course of events is possible, which is inconsistent with the existence of free will thus conceived. This problem has been identified in ancient Greek philosophy and remains a major focus of philosophical debate. This view that conceives free will to be incompatible with determinism is called \"incompatibilism\" and encompasses both metaphysical libertarianism, the claim that determinism is false and thus free will is at least possible, and hard determinism, the claim that determinism is true and thus free will is not possible. It also encompasses hard incompatibilism, which holds not only determinism but also its negation to be incompatible with free will and thus free will to be impossible whatever the case may be regarding determinism.\n\nIn contrast, \"compatibilists\" hold that free will \"is\" compatible with determinism. Some compatibilists even hold that determinism is \"necessary\" for free will, arguing that choice involves preference for one course of action over another, requiring a sense of \"how\" choices will turn out. Compatibilists thus consider the debate between libertarians and hard determinists over free will vs determinism a false dilemma. Different compatibilists offer very different definitions of what \"free will\" even means and consequently find different types of constraints to be relevant to the issue. Classical compatibilists considered free will nothing more than freedom of action, considering one free of will simply if, \"had\" one counterfactually wanted to do otherwise, one \"could\" have done otherwise without physical impediment. Contemporary compatibilists instead identify free will as a psychological capacity, such as to direct one's behavior in a way responsive to reason, and there are still further different conceptions of free will, each with their own concerns, sharing only the common feature of not finding the possibility of determinism a threat to the possibility of free will.\n\nThe underlying questions are whether we have control over our actions, and if so, what sort of control, and to what extent. These questions predate the early Greek stoics (for example, Chrysippus), and some modern philosophers lament the lack of progress over all these centuries.\n\nOn one hand, humans have a strong sense of freedom, which leads us to believe that we have free will. On the other hand, an intuitive feeling of free will could be mistaken.\n\nIt is difficult to reconcile the intuitive evidence that conscious decisions are causally effective with the view that the physical world can be explained to operate perfectly by physical law. The conflict between intuitively felt freedom and natural law arises when either causal closure or physical determinism (nomological determinism) is asserted. With causal closure, no physical event has a cause outside the physical domain, and with physical determinism, the future is determined entirely by preceding events (cause and effect).\n\nThe puzzle of reconciling 'free will' with a deterministic universe is known as the \"problem of free will\" or sometimes referred to as the \"dilemma of determinism\". This dilemma leads to a moral dilemma as well: the question of how to assign responsibility for actions if they are caused entirely by past events.\n\nCompatibilists maintain that mental reality is not of itself causally effective. Classical compatibilists have addressed the dilemma of free will by arguing that free will holds as long as we are not externally constrained or coerced. Modern compatibilists make a distinction between freedom of will and freedom of \"action\", that is, separating freedom of choice from the freedom to enact it. Given that humans all experience a sense of free will, some modern compatibilists think it is necessary to accommodate this intuition. Compatibilists often associate freedom of will with the ability to make rational decisions.\n\nA different approach to the dilemma is that of incompatibilists, namely, that if the world is deterministic, then our feeling that we are free to choose an action is simply an illusion. Metaphysical libertarianism is the form of incompatibilism which posits that determinism is false and free will is possible (at least some people have free will). This view is associated with non-materialist constructions, including both traditional dualism, as well as models supporting more minimal criteria; such as the ability to consciously veto an action or competing desire. Yet even with physical indeterminism, arguments have been made against libertarianism in that it is difficult to assign \"Origination\" (responsibility for \"free\" indeterministic choices).\n\nFree will here is predominately treated with respect to physical determinism in the strict sense of nomological determinism, although other forms of determinism are also relevant to free will. For example, logical and theological determinism challenge metaphysical libertarianism with ideas of destiny and fate, and biological, cultural and psychological determinism feed the development of compatibilist models. Separate classes of compatibilism and incompatibilism may even be formed to represent these.\n\nBelow are the classic arguments bearing upon the dilemma and its underpinnings.\n\nIncompatibilism is the position that free will and determinism are logically incompatible, and that the major question regarding whether or not people have free will is thus whether or not their actions are determined. \"Hard determinists\", such as d'Holbach, are those incompatibilists who accept determinism and reject free will. In contrast, \"metaphysical libertarians\", such as Thomas Reid, Peter van Inwagen, and Robert Kane, are those incompatibilists who accept free will and deny determinism, holding the view that some form of indeterminism is true. Another view is that of hard incompatibilists, which state that free will is incompatible with both determinism and indeterminism.\n\nTraditional arguments for incompatibilism are based on an \"intuition pump\": if a person is like other mechanical things that are determined in their behavior such as a wind-up toy, a billiard ball, a puppet, or a robot, then people must not have free will. This argument has been rejected by compatibilists such as Daniel Dennett on the grounds that, even if humans have something in common with these things, it remains possible and plausible that we are different from such objects in important ways.\n\nAnother argument for incompatibilism is that of the \"causal chain\". Incompatibilism is key to the idealist theory of free will. Most incompatibilists reject the idea that freedom of action consists simply in \"voluntary\" behavior. They insist, rather, that free will means that man must be the \"ultimate\" or \"originating\" cause of his actions. He must be \"causa sui\", in the traditional phrase. Being responsible for one's choices is the first cause of those choices, where first cause means that there is no antecedent cause of that cause. The argument, then, is that if man has free will, then man is the ultimate cause of his actions. If determinism is true, then all of man's choices are caused by events and facts outside his control. So, if everything man does is caused by events and facts outside his control, then he cannot be the ultimate cause of his actions. Therefore, he cannot have free will. This argument has also been challenged by various compatibilist philosophers.\n\nA third argument for incompatibilism was formulated by Carl Ginet in the 1960s and has received much attention in the modern literature. The simplified argument runs along these lines: if determinism is true, then we have no control over the events of the past that determined our present state and no control over the laws of nature. Since we can have no control over these matters, we also can have no control over the \"consequences\" of them. Since our present choices and acts, under determinism, are the necessary consequences of the past and the laws of nature, then we have no control over them and, hence, no free will. This is called the \"consequence argument\". Peter van Inwagen remarks that C.D. Broad had a version of the consequence argument as early as the 1930s.\n\nThe difficulty of this argument for some compatibilists lies in the fact that it entails the impossibility that one could have chosen other than one has. For example, if Jane is a compatibilist and she has just sat down on the sofa, then she is committed to the claim that she could have remained standing, if she had so desired. But it follows from the consequence argument that, if Jane had remained standing, she would have either generated a contradiction, violated the laws of nature or changed the past. Hence, compatibilists are committed to the existence of \"incredible abilities\", according to Ginet and van Inwagen. One response to this argument is that it equivocates on the notions of abilities and necessities, or that the free will evoked to make any given choice is really an illusion and the choice had been made all along, oblivious to its \"decider\". David Lewis suggests that compatibilists are only committed to the ability to do something otherwise if \"different circumstances\" had actually obtained in the past.\n\nUsing \"T\", \"F\" for \"true\" and \"false\" and \"?\" for undecided, there are exactly nine positions regarding determinism/free will that consist of any two of these three possibilities:\n\n\"Incompatibilism\" may occupy any of the nine positions except (5), (8) or (3), which last corresponds to \"soft determinism\". Position (1) is \"hard determinism\", and position (2) is \"libertarianism\". The position (1) of hard determinism adds to the table the contention that \"D\" implies \"FW\" is untrue, and the position (2) of libertarianism adds the contention that \"FW\" implies \"D\" is untrue. Position (9) may be called \"hard incompatibilism\" if one interprets \"?\" as meaning both concepts are of dubious value. \"Compatibilism\" itself may occupy any of the nine positions, that is, there is no logical contradiction between determinism and free will, and either or both may be true or false in principle. However, the most common meaning attached to \"compatibilism\" is that some form of determinism is true and yet we have some form of free will, position (3).\nAlex Rosenberg makes an extrapolation of physical determinism as inferred on the macroscopic scale by the behaviour of a set of dominoes to neural activity in the brain where; \"If the brain is nothing but a complex physical object whose states are as much governed by physical laws as any other physical object, then what goes on in our heads is as fixed and determined by prior events as what goes on when one domino topples another in a long row of them.\" Physical determinism is currently disputed by prominent interpretations of quantum mechanics, and while not necessarily representative of intrinsic indeterminism in nature, fundamental limits of precision in measurement are inherent in the uncertainty principle. The relevance of such prospective indeterminate activity to free will is, however, contested, even when chaos theory is introduced to magnify the effects of such microscopic events.\n\nBelow these positions are examined in more detail.\n\nDeterminism can be divided into causal, logical and theological determinism. Corresponding to each of these different meanings, there arises a different problem for free will. Hard determinism is the claim that determinism is true, and that it is incompatible with free will, so free will does not exist. Although hard determinism generally refers to nomological determinism (see causal determinism below), it can include all forms of determinism that necessitate the future in its entirety. Relevant forms of determinism include:\n\nOther forms of determinism are more relevant to compatibilism, such as biological determinism, the idea that all behaviors, beliefs, and desires are fixed by our genetic endowment and our biochemical makeup, the latter of which is affected by both genes and environment, cultural determinism and psychological determinism. Combinations and syntheses of determinist theses, such as bio-environmental determinism, are even more common.\n\nSuggestions have been made that hard determinism need not maintain strict determinism, where something near to, like that informally known as adequate determinism, is perhaps more relevant. Despite this, hard determinism has grown less popular in present times, given scientific suggestions that determinism is false – yet the intention of their position is sustained by hard incompatibilism.\n\nMetaphysical libertarianism is one philosophical view point under that of incompatibilism. Libertarianism holds onto a concept of free will that requires that the agent be able to take more than one possible course of action under a given set of circumstances.\n\nAccounts of libertarianism subdivide into non-physical theories and physical or naturalistic theories. Non-physical theories hold that the events in the brain that lead to the performance of actions do not have an entirely physical explanation, which requires that the world is not closed under physics. This includes interactionist dualism, which claims that some non-physical mind, will, or soul overrides physical causality. Physical determinism implies there is only one possible future and is therefore not compatible with libertarian free will. As consequent of incompatibilism, metaphysical libertarian explanations that do not involve dispensing with physicalism require physical indeterminism, such as probabilistic subatomic particle behavior – theory unknown to many of the early writers on free will. Incompatibilist theories can be categorised based on the type of indeterminism they require; uncaused events, non-deterministically caused events, and agent/substance-caused events.\n\nNon-causal accounts of incompatibilist free will do not require a free action to be caused by either an agent or a physical event. They either rely upon a world that is not causally closed, or physical indeterminism. Non-causal accounts often claim that each intentional action requires a choice or volition – a willing, trying, or endeavoring on behalf of the agent (such as the cognitive component of lifting one's arm). Such intentional actions are interpreted as free actions. It has been suggested, however, that such acting cannot be said to exercise control over anything in particular. According to non-causal accounts, the causation by the agent cannot be analysed in terms of causation by mental states or events, including desire, belief, intention of something in particular, but rather is considered a matter of spontaneity and creativity. The exercise of intent in such intentional actions is not that which determines their freedom – intentional actions are rather self-generating. The \"actish feel\" of some intentional actions do not \"constitute that event's activeness, or the agent's exercise of active control\", rather they \"might be brought about by direct stimulation of someone's brain, in the absence of any relevant desire or intention on the part of that person\". Another question raised by such non-causal theory, is how an agent acts upon reason, if the said intentional actions are spontaneous.\n\nSome non-causal explanations involve invoking panpsychism, the theory that a quality of mind is associated with all particles, and pervades the entire universe, in both animate and inanimate entities.\n\nEvent-causal accounts of incompatibilist free will typically rely upon physicalist models of mind (like those of the compatibilist), yet they presuppose physical indeterminism, in which certain indeterministic events are said to be caused by the agent. A number of event-causal accounts of free will have been created, referenced here as \"deliberative indeterminism\", \"centred accounts\", and \"efforts of will theory\". The first two accounts do not require free will to be a fundamental constituent of the universe. Ordinary randomness is appealed to as supplying the \"elbow room\" that libertarians believe necessary. A first common objection to event-causal accounts is that the indeterminism could be destructive and could therefore diminish control by the agent rather than provide it (related to the problem of origination). A second common objection to these models is that it is questionable whether such indeterminism could add any value to deliberation over that which is already present in a deterministic world.\n\n\"Deliberative indeterminism\" asserts that the indeterminism is confined to an earlier stage in the decision process. This is intended to provide an indeterminate set of possibilities to choose from, while not risking the introduction of \"luck\" (random decision making). The selection process is deterministic, although it may be based on earlier preferences established by the same process. Deliberative indeterminism has been referenced by Daniel Dennett and John Martin Fischer. An obvious objection to such a view is that an agent cannot be assigned ownership over their decisions (or preferences used to make those decisions) to any greater degree than that of a compatibilist model.\n\n\"Centred accounts\" propose that for any given decision between two possibilities, the strength of reason will be considered for each option, yet there is still a probability the weaker candidate will be chosen. An obvious objection to such a view is that decisions are explicitly left up to chance, and origination or responsibility cannot be assigned for any given decision.\n\n\"Efforts of will theory\" is related to the role of will power in decision making. It suggests that the indeterminacy of agent volition processes could map to the indeterminacy of certain physical events – and the outcomes of these events could therefore be considered caused by the agent. Models of volition have been constructed in which it is seen as a particular kind of complex, high-level process with an element of physical indeterminism. An example of this approach is that of Robert Kane, where he hypothesizes that \"in each case, the indeterminism is functioning as a hindrance or obstacle to her realizing one of her purposes – a hindrance or obstacle in the form of resistance within her will which must be overcome by effort.\" According to Robert Kane such \"ultimate responsibility\" is a required condition for free will. An important factor in such a theory is that the agent cannot be reduced to physical neuronal events, but rather mental processes are said to provide an equally valid account of the determination of outcome as their physical processes (see non-reductive physicalism).\n\nAlthough at the time quantum mechanics (and physical indeterminism) was only in the initial stages of acceptance, in his book \"Miracles: A preliminary study\" C.S. Lewis stated the logical possibility that if the physical world were proved indeterministic this would provide an entry point to describe an action of a non-physical entity on physical reality. Indeterministic physical models (particularly those involving quantum indeterminacy) introduce random occurrences at an atomic or subatomic level. These events might affect brain activity, and could seemingly allow incompatibilist free will if the apparent indeterminacy of some mental processes (for instance, subjective perceptions of control in conscious volition) map to the underlying indeterminacy of the physical construct. This relationship, however, requires a causative role over probabilities that is questionable, and it is far from established that brain activity responsible for human action can be affected by such events. Secondarily, these incompatibilist models are dependent upon the relationship between action and conscious volition, as studied in the neuroscience of free will. It is evident that observation may disturb the outcome of the observation itself, rendering limited our ability to identify causality. Niels Bohr, one of the main architects of quantum theory, suggested, however, that no connection could be made between indeterminism of nature and freedom of will.\n\nAgent/substance-causal accounts of incompatibilist free will rely upon substance dualism in their description of mind. The agent is assumed power to intervene in the physical world.\nAgent (substance)-causal accounts have been suggested by both George Berkeley and Thomas Reid. It is required that what the agent causes is not causally determined by prior events. It is also required that the agent's causing of that event is not causally determined by prior events. A number of problems have been identified with this view. Firstly, it is difficult to establish the reason for any given choice by the agent, which suggests they may be random or determined by \"luck\" (without an underlying basis for the free will decision). Secondly, it has been questioned whether physical events can be caused by an external substance or mind – a common problem associated with interactionalist dualism.\n\nHard incompatibilism is the idea that free will cannot exist, whether the world is deterministic or not. Derk Pereboom has defended hard incompatibilism, identifying a variety of positions where free will is irrelevant to indeterminism/determinism, among them the following:\nPereboom calls positions 3 and 4 \"soft determinism\", position 1 a form of \"hard determinism\", position 6 a form of \"classical libertarianism\", and any position that includes having F as \"compatibilism\".\n\nJohn Locke denied that the phrase \"free will\" made any sense (compare with theological noncognitivism, a similar stance on the existence of God). He also took the view that the truth of determinism was irrelevant. He believed that the defining feature of voluntary behavior was that individuals have the ability to \"postpone\" a decision long enough to reflect or deliberate upon the consequences of a choice: \"... the will in truth, signifies nothing but a power, or ability, to prefer or choose\".\n\nThe contemporary philosopher Galen Strawson agrees with Locke that the truth or falsity of determinism is irrelevant to the\nproblem. He argues that the notion of free will leads to an infinite regress and is therefore senseless.\nAccording to Strawson, if one is responsible for what one does in a given situation, then one must be responsible for the way one is in certain mental respects. But it is impossible for one to be responsible for the way one is in any respect. This is because to be responsible in some situation \"S\", one must have been responsible for the way one was at \"S\". To be responsible for the way one was at \"S\", one must have been responsible for the way one was at \"S\", and so on. At some point in the chain, there must have been an act of origination of a new causal chain. But this is impossible. Man cannot create himself or his mental states \"ex nihilo\". This argument entails that free will itself is absurd, but not that it is incompatible with determinism. Strawson calls his own view \"pessimism\" but it can be classified as hard incompatibilism.\n\nCausal determinism is the concept that events within a given paradigm are bound by causality in such a way that any state (of an object or event) is completely determined by prior states. Causal determinism proposes that there is an unbroken chain of prior occurrences stretching back to the origin of the universe. Causal determinists believe that there is nothing uncaused or self-caused. The most common form of causal determinism is nomological determinism (or scientific determinism), the notion that the past and the present dictate the future entirely and necessarily by rigid natural laws, that every occurrence results inevitably from prior events. Quantum mechanics poses a serious challenge to this view.\n\nFundamental debate continues over whether the physical universe is likely to be deterministic. Although the scientific method cannot be used to rule out indeterminism with respect to violations of causal closure, it can be used to identify indeterminism in natural law. Interpretations of quantum mechanics at present are both deterministic and indeterministic, and are being constrained by ongoing experimentation.\n\nDestiny or fate is a predetermined course of events. It may be conceived as a predetermined future, whether in general or of an individual. It is a concept based on the belief that there is a fixed natural order to the cosmos.\n\nAlthough often used interchangeably, the words \"fate\" and \"destiny\" have distinct connotations.\n\nFate generally implies there is a set course that cannot be deviated from, and over which one has no control. Fate is related to determinism, but makes no specific claim of physical determinism. Even with physical indeterminism an event could still be fated externally (see for instance theological determinism). Destiny likewise is related to determinism, but makes no specific claim of physical determinism. Even with physical indeterminism an event could still be destined to occur.\n\nDestiny implies there is a set course that cannot be deviated from, but does not of itself make any claim with respect to the setting of that course (i.e., it does not necessarily conflict with incompatibilist free will). Free will if existent could be the mechanism by which that destined outcome is chosen (determined to represent destiny).\n\nDiscussion regarding destiny does not necessitate the existence of supernatural powers. Logical determinism or determinateness is the notion that all propositions, whether about the past, present, or future, are either true or false. This creates a unique problem for free will given that propositions about the future already have a truth value in the present (that is it is already determined as either true or false), and is referred to as the problem of future contingents.\n\nOmniscience is the capacity to know everything that there is to know (included in which are all future events), and is a property often attributed to a creator deity. Omniscience implies the existence of destiny. Some authors have claimed that free will cannot coexist with omniscience. One argument asserts that an omniscient creator not only implies destiny but a form of high level predeterminism such as hard theological determinism or predestination – that they have independently fixed all events and outcomes in the universe in advance. In such a case, even if an individual could have influence over their lower level physical system, their choices in regard to this cannot be their own, as is the case with libertarian free will. Omniscience features as an incompatible-properties argument for the existence of God, known as the argument from free will, and is closely related to other such arguments, for example the incompatibility of omnipotence with a good creator deity (i.e. if a deity knew what they were going to choose, then they are responsible for letting them choose it).\n\nPredeterminism is the idea that all events are determined in advance. Predeterminism is the philosophy that all events of history, past, present and future, have been decided or are known (by God, fate, or some other force), including human actions. Predeterminism is frequently taken to mean that human actions cannot interfere with (or have no bearing on) the outcomes of a pre-determined course of events, and that one's destiny was established externally (for example, exclusively by a creator deity). The concept of predeterminism is often argued by invoking causal determinism, implying that there is an unbroken chain of prior occurrences stretching back to the origin of the universe. In the case of predeterminism, this chain of events has been pre-established, and human actions cannot interfere with the outcomes of this pre-established chain. Predeterminism can be used to mean such pre-established causal determinism, in which case it is categorised as a specific type of determinism. It can also be used interchangeably with causal determinism – in the context of its capacity to determine future events. Despite this, predeterminism is often considered as independent of causal determinism. The term predeterminism is also frequently used in the context of biology and heredity, in which case it represents a form of biological determinism.\n\nThe term predeterminism suggests not just a determining of all events, but the prior and deliberately conscious determining of all events (therefore done, presumably, by a conscious being). While determinism usually refers to a naturalistically explainable causality of events, predeterminism seems by definition to suggest a person or a \"someone\" who is controlling or planning the causality of events before they occur and who then perhaps resides beyond the natural, causal universe. Predestination asserts that a supremely powerful being has indeed fixed all events and outcomes in the universe in advance, and is a famous doctrine of the Calvinists in Christian theology. Predestination is often considered a form of hard theological determinism.\n\nPredeterminism has therefore been compared to fatalism. Fatalism is the idea that everything is fated to happen, so that humans have no control over their future.\n\nTheological determinism is a form of determinism stating that all events that happen are pre-ordained, or predestined to happen, by a monotheistic deity, or that they are destined to occur given its omniscience. Two forms of theological determinism exist, here referenced as strong and weak theological determinism.\n\nThere exist slight variations on the above categorisation. Some claim that theological determinism requires predestination of all events and outcomes by the divinity (that is, they do not classify the weaker version as 'theological determinism' unless libertarian free will is assumed to be denied as a consequence), or that the weaker version does not constitute 'theological determinism' at all. Theological determinism can also be seen as a form of causal determinism, in which the antecedent conditions are the nature and will of God. With respect to free will and the classification of theological compatibilism/incompatibilism below, \"theological determinism is the thesis that God exists and has infallible knowledge of all true propositions including propositions about our future actions,\" more minimal criteria designed to encapsulate all forms of theological determinism.\nThere are various implications for metaphysical libertarian free will as consequent of theological determinism and its philosophical interpretation.\n\nThe basic argument for theological fatalism in the case of weak theological determinism is as follows:\n\nThis argument is very often accepted as a basis for theological incompatibilism: denying either libertarian free will or divine foreknowledge (omniscience) and therefore theological determinism. On the other hand, theological compatibilism must attempt to find problems with it. The formal version of the argument rests on a number of premises, many of which have received some degree of contention. Theological compatibilist responses have included:\n\nIn the definition of compatibilism and incompatibilism, the literature often fails to distinguish between physical determinism and higher level forms of determinism (predeterminism, theological determinism, etc.) As such, hard determinism with respect to theological determinism (or \"Hard Theological Determinism\" above) might be classified as hard incompatibilism with respect to physical determinism (if no claim was made regarding the internal causality or determinism of the universe), or even compatibilism (if freedom from the constraint of determinism was not considered necessary for free will), if not hard determinism itself. By the same principle, metaphysical libertarianism (a form of incompatibilism with respect to physical determinism) might be classified as compatibilism with respect to theological determinism (if it was assumed such free will events were pre-ordained and therefore were destined to occur, but of which whose outcomes were not \"predestined\" or determined by God). If hard theological determinism is accepted (if it was assumed instead that such outcomes were predestined by God), then metaphysical libertarianism is not, however, possible, and would require reclassification (as hard incompatibilism for example, given that the universe is still assumed to be indeterministic – although the classification of hard determinism is technically valid also).\n\nThe idea of \"free will\" is one aspect of the mind-body problem, that is, consideration of the relation between mind (for example, consciousness, memory, and judgment) and body (for example, the human brain and nervous system). Philosophical models of mind are divided into physical and non-physical expositions.\n\nCartesian dualism holds that the mind is a nonphysical substance, the seat of consciousness and intelligence, and is not identical with physical states of the brain or body. It is suggested that although the two worlds do interact, each retains some measure of autonomy. Under cartesian dualism external mind is responsible for bodily action, although unconscious brain activity is often caused by external events (for example, the instantaneous reaction to being burned). Cartesian dualism implies that the physical world is not deterministic – and in which external mind controls (at least some) physical events, providing an interpretation of incompatibilist free will. Stemming from Cartesian dualism, a formulation sometimes called \"interactionalist dualism\" suggests a two-way interaction, that some physical events cause some mental acts and some mental acts cause some physical events. One modern vision of the possible separation of mind and body is the \"three-world\" formulation of Popper. Cartesian dualism and Popper's three worlds are two forms of what is called epistemological pluralism, that is the notion that different epistemological methodologies are necessary to attain a full description of the world. Other forms of epistemological pluralist dualism include psychophysical parallelism and epiphenomenalism. Epistemological pluralism is one view in which the mind-body problem is \"not\" reducible to the concepts of the natural sciences.\n\nA contrasting approach is called physicalism. Physicalism is a philosophical theory holding that everything that exists is no more extensive than its physical properties; that is, that there are no non-physical substances (for example physically independent minds). Physicalism can be reductive or non-reductive. Reductive physicalism is grounded in the idea that everything in the world can actually be reduced analytically to its fundamental physical, or material, basis. Alternatively, non-reductive physicalism asserts that mental properties form a separate ontological class to physical properties: that mental states (such as qualia) are not ontologically reducible to physical states. Although one might suppose that mental states and neurological states are different in kind, that does not rule out the possibility that mental states are correlated with neurological states. In one such construction, anomalous monism, mental events \"supervene\" on physical events, describing the emergence of mental properties correlated with physical properties – implying causal reducibility. Non-reductive physicalism is therefore often categorised as property dualism rather than monism, yet other types of property dualism do not adhere to the causal reducibility of mental states (see epiphenomenalism).\n\nIncompatibilism requires a distinction between the mental and the physical, being a commentary on the incompatibility of (determined) physical reality and one's presumably distinct experience of will. Secondarily, metaphysical libertarian free will must assert influence on physical reality, and where mind is responsible for such influence (as opposed to ordinary system randomness), it must be distinct from body to accomplish this. Both substance and property dualism offer such a distinction, and those particular models thereof that are not causally inert with respect to the physical world provide a basis for illustrating incompatibilist free will (i.e. interactionalist dualism and non-reductive physicalism).\n\nIt has been noted that the laws of physics have yet to resolve the hard problem of consciousness: \"Solving the hard problem of consciousness involves determining how physiological processes such as ions flowing across the nerve membrane \"cause\" us to have experiences.\" According to some, \"Intricately related to the hard problem of consciousness, the hard problem of free will represents \"the\" core problem of conscious free will: Does conscious volition impact the material world?\" Others however argue that \"consciousness plays a far smaller role in human life than Western culture has tended to believe.\"\n\nCompatibilists maintain that determinism is compatible with free will. They believe freedom can be present or absent in a situation for reasons that have nothing to do with metaphysics. For instance, courts of law make judgments about whether individuals are acting under their own free will under certain circumstances without bringing in metaphysics. Similarly, political liberty is a non-metaphysical concept. Likewise, some compatibilists define free will as freedom to act according to one's determined motives without hindrance from other individuals. So for example Aristotle in his \"Nicomachean Ethics\", and the Stoic Chrysippus.\nIn contrast, the incompatibilist positions are concerned with a sort of \"metaphysically free will\", which compatibilists claim has never been coherently defined. Compatibilists argue that determinism does not matter; though they disagree among themselves about what, in turn, \"does\" matter. To be a compatibilist, one need not endorse any particular conception of free will, but only deny that determinism is at odds with free will.\n\nAlthough there are various impediments to exercising one's choices, free will does not imply freedom of action. Freedom of choice (freedom to select one's will) is logically separate from freedom to \"implement\" that choice (freedom to enact one's will), although not all writers observe this distinction. Nonetheless, some philosophers have defined free will as the absence of various impediments. Some \"modern compatibilists\", such as Harry Frankfurt and Daniel Dennett, argue free will is simply freely choosing to do what constraints allow one to do. In other words, a coerced agent's choices can still be free if such coercion coincides with the agent's personal intentions and desires.\n\nMost \"classical compatibilists\", such as Thomas Hobbes, claim that a person is acting on the person's own will only when it is the desire of that person to do the act, and also possible for the person to be able to do otherwise, \"if the person had decided to\". Hobbes sometimes attributes such compatibilist freedom to each individual and not to some abstract notion of \"will\", asserting, for example, that \"no liberty can be inferred to the will, desire, or inclination, but the liberty of the man; which consisteth in this, that he finds no stop, in doing what he has the will, desire, or inclination to doe.\" In articulating this crucial proviso, David Hume writes, \"this hypothetical liberty is universally allowed to belong to every one who is not a prisoner and in chains.\" Similarly, Voltaire, in his \"Dictionnaire philosophique\", claimed that \"Liberty then is only and can be only the power to do what one will.\" He asked, \"would you have everything at the pleasure of a million blind caprices?\" For him, free will or liberty is \"only the power of acting, what is this power? It is the effect of the constitution and present state of our organs.\"\n\nCompatibilism often regards the agent free as virtue of their reason. Some explanations of free will focus on the internal causality of the mind with respect to higher-order brain processing – the interaction between conscious and unconscious brain activity. Likewise, some modern compatibilists in psychology have tried to revive traditionally accepted struggles of free will with the formation of character. Compatibilist free will has also been attributed to our natural sense of agency, where one must believe they are an agent in order to function and develop a theory of mind.\n\nThe notion of levels of decision is presented in a different manner by Frankfurt. Frankfurt argues for a version of compatibilism called the \"hierarchical mesh\". The idea is that an individual can have conflicting desires at a first-order level and also have a desire about the various first-order desires (a second-order desire) to the effect that one of the desires prevails over the others. A person's will is identified with their effective first-order desire, that is, the one they act on, and this will is free if it was the desire the person wanted to act upon, that is, the person's second-order desire was effective. So, for example, there are \"wanton addicts\", \"unwilling addicts\" and \"willing addicts\". All three groups may have the conflicting first-order desires to want to take the drug they are addicted to and to not want to take it.\n\nThe first group, \"wanton addicts\", have no second-order desire not to take the drug. The second group, \"unwilling addicts\", have a second-order desire not to take the drug, while the third group, \"willing addicts\", have a second-order desire to take it. According to Frankfurt, the members of the first group are devoid of will and therefore are no longer persons. The members of the second group freely desire not to take the drug, but their will is overcome by the addiction. Finally, the members of the third group willingly take the drug they are addicted to. Frankfurt's theory can ramify to any number of levels. Critics of the theory point out that there is no certainty that conflicts will not arise even at the higher-order levels of desire and preference. Others argue that Frankfurt offers no adequate explanation of how the various levels in the hierarchy mesh together.\n\nIn \"Elbow Room\", Dennett presents an argument for a compatibilist theory of free will, which he further elaborated in the book \"Freedom Evolves\". The basic reasoning is that, if one excludes God, an infinitely powerful demon, and other such possibilities, then because of chaos and epistemic limits on the precision of our knowledge of the current state of the world, the future is ill-defined for all finite beings. The only well-defined things are \"expectations\". The ability to do \"otherwise\" only makes sense when dealing with these expectations, and not with some unknown and unknowable future.\n\nAccording to Dennett, because individuals have the ability to act differently from what anyone expects, free will can exist. Incompatibilists claim the problem with this idea is that we may be mere \"automata responding in predictable ways to stimuli in our environment\". Therefore, all of our actions are controlled by forces outside ourselves, or by random chance. More sophisticated analyses of compatibilist free will have been offered, as have other critiques.\n\nIn the philosophy of decision theory, a fundamental question is: From the standpoint of statistical outcomes, to what extent do the choices of a conscious being have the ability to influence the future? Newcomb's paradox and other philosophical problems pose questions about free will and predictable outcomes of choices.\n\nCompatibilist models of free will often consider deterministic relationships as discoverable in the physical world (including the brain). Cognitive naturalism is a physicalist approach to studying human cognition and consciousness in which the mind is simply part of nature, perhaps merely a feature of many very complex self-programming feedback systems (for example, neural networks and cognitive robots), and so must be studied by the methods of empirical science, such as the behavioral and cognitive sciences (\"i.e.\" neuroscience and cognitive psychology). Cognitive naturalism stresses the role of neurological sciences. Overall brain health, substance dependence, depression, and various personality disorders clearly influence mental activity, and their impact upon volition is also important. For example, an addict may experience a conscious desire to escape addiction, but be unable to do so. The \"will\" is disconnected from the freedom to act. This situation is related to an abnormal production and distribution of dopamine in the brain. The neuroscience of free will places restrictions on both compatibilist and incompatibilist free will conceptions.\n\nCompatibilist models adhere to models of mind in which mental activity (such as deliberation) can be reduced to physical activity without any change in physical outcome. Although compatibilism is generally aligned to (or is at least compatible with) physicalism, some compatibilist models describe the natural occurrences of deterministic deliberation in the brain in terms of the first person perspective of the conscious agent performing the deliberation. Such an approach has been considered a form of identity dualism. A description of \"how conscious experience might affect brains\" has been provided in which \"the experience of conscious free will is the first-person perspective of the neural correlates of choosing.\"\n\nRecently Claudio Costa developed a neocompatibilist theory based on the causal theory of action that is complementary to classical compatibilism. According to him, physical, psychological and rational restrictions can interfer at different levels of the causal chain that would naturally lead to action. Corrrespondingly, there can be physical restrictions to the body, psychological restrictions to the decision, and rational restrictions to the formation of reasons (desires plus beliefs) that should lead to what we would call a reasonable action. The last two are usually called \"restrictions of free will\". The restriction at the level of reasons is particularly important, since it can be motivated by external reasons that are insufficiently conscious to the agent. One example was the collective suicide led by Jim Jones. The suicidal agents were not conscious that their free will have been manipulated by external, even if ungrounded, reasons.\n\nSome philosophers' views are difficult to categorize as either compatibilist or incompatibilist, hard determinist or libertarian. For example, Ted Honderich holds the view that \"determinism is true, compatibilism and incompatibilism are both false\" and the real problem lies elsewhere. Honderich maintains that determinism is true because quantum phenomena are not events or things that can be located in space and time, but are abstract entities. Further, even if they were micro-level events, they do not seem to have any relevance to how the world is at the macroscopic level. He maintains that incompatibilism is false because, even if indeterminism is true, incompatibilists have not provided, and cannot provide, an adequate account of origination. He rejects compatibilism because it, like incompatibilism, assumes a single, fundamental notion of freedom. There are really two notions of freedom: voluntary action and origination. Both notions are required to explain freedom of will and responsibility. Both determinism and indeterminism are threats to such freedom. To abandon these notions of freedom would be to abandon moral responsibility. On the one side, we have our intuitions; on the other, the scientific facts. The \"new\" problem is how to resolve this conflict.\n\nDavid Hume discussed the possibility that the entire debate about free will is nothing more than a merely \"verbal\" issue. He suggested that it might be accounted for by \"a false sensation or seeming experience\" (a \"velleity\"), which is associated with many of our actions when we perform them. On reflection, we realize that they were necessary and determined all along.\nArthur Schopenhauer put the puzzle of free will and moral responsibility in these terms:\nEveryone believes himself, \"a priori\", perfectly free – even in his individual actions, and thinks that at every moment he can commence another manner of life. ... But \"a posteriori\", through experience, he finds to his astonishment that he is not free, but subjected to necessity, that in spite of all his resolutions and reflections he does not change his conduct, and that from the beginning of his life to the end of it, he must carry out the very character which he himself condemns...\nIn his essay \"On the Freedom of the Will\", Schopenhauer stated, \"You can do what you will, but in any given moment of your life you can \"will\" only one definite thing and absolutely nothing other than that one thing.\" According to Schopenhauer, phenomena do not have free will. However, will [urging, craving, striving, wanting, and desiring] as noumenon is free.\n\nRudolf Steiner, who collaborated in a complete edition of Arthur Schopenhauer's work, wrote \"The Philosophy of Freedom\", which focuses on the problem of free will. Steiner (1861–1925) initially divides this into the two aspects of freedom: \"freedom of thought\" and \"freedom of action\". The controllable and uncontrollable aspects of decision making thereby are made logically separable, as pointed out in the introduction. This separation of \"will\" from \"action\" has a very long history, going back at least as far as Stoicism and the teachings of Chrysippus (279–206 BCE), who separated external \"antecedent\" causes from the internal disposition receiving this cause.\n\nSteiner then argues that inner freedom is achieved when we integrate our sensory impressions, which reflect the outer appearance of the world, with our thoughts, which lend coherence to these impressions and thereby disclose to us an understandable world. Acknowledging the many influences on our choices, he nevertheless point out that they do not preclude freedom unless we fail to recognise them. Steiner argues that outer freedom is attained by permeating our deeds with \"moral imagination.” “Moral” in this case refers to action that is willed, while “imagination” refers to the mental capacity to envision conditions that do not already hold. Both of these functions are necessarily conditions for freedom. Steiner aims to show that these two aspects of inner and outer freedom are integral to one another, and that true freedom is only achieved when they are united.\n\nWilliam James' views were ambivalent. While he believed in free will on \"ethical grounds\", he did not believe that there was evidence for it on scientific grounds, nor did his own introspections support it. Ultimately he believed that the problem of free will was a metaphysical issue and, therefore, could not be settled by science. Moreover, he did not accept incompatibilism as formulated below; he did not believe that the indeterminism of human actions was a prerequisite of moral responsibility. In his work \"Pragmatism\", he wrote that \"instinct and utility between them can safely be trusted to carry on the social business of punishment and praise\" regardless of metaphysical theories. He did believe that indeterminism is important as a \"doctrine of relief\" – it allows for the view that, although the world may be in many respects a bad place, it may, through individuals' actions, become a better one. Determinism, he argued, undermines meliorism – the idea that progress is a real concept leading to improvement in the world.\n\nIn 1739, David Hume in his \"A Treatise of Human Nature\" approached free will via the notion of causality. It was his position that causality was a mental construct used to explain the repeated association of events, and that one must examine more closely the relation between things \"regularly succeeding\" one another (descriptions of regularity in nature) and things that \"result\" in other things (things that cause or necessitate other things). According to Hume, 'causation' is on weak grounds: \"Once we realise that 'A must bring about B' is tantamount merely to 'Due to their constant conjunction, we are psychologically certain that B will follow A,' then we are left with a very weak notion of necessity.\"\n\nThis empiricist view was often denied by trying to prove the so-called apriority of causal law (i.e. that it precedes all experience and is rooted in the construction of the perceivable world):\n\nIn the 1780s Immanuel Kant suggested at a minimum our decision processes with moral implications lie outside the reach of everyday causality, and lie outside the rules governing material objects. \"There is a sharp difference between moral judgments and judgments of fact... Moral judgments ... must be \"a priori\" judgments.\"\n\nFreeman introduces what he calls \"circular causality\" to \"allow for the contribution of self-organizing dynamics\", the \"formation of macroscopic population dynamics that shapes the patterns of activity of the contributing individuals\", applicable to \"interactions between neurons and neural masses ... and between the behaving animal and its environment\". In this view, mind and neurological functions are tightly coupled in a situation where feedback between collective actions (mind) and individual subsystems (for example, neurons and their synapses) jointly decide upon the behaviour of both.\n\nThirteenth century philosopher Thomas Aquinas viewed humans as pre-programmed (by virtue of being human) to seek certain goals, but able to choose between routes to achieve these goals (our Aristotelian telos). His view has been associated with both compatibilism and libertarianism.\n\nIn facing choices, he argued that humans are governed by \"intellect\", \"will\", and \"passions\". The will is \"the primary mover of all the powers of the soul ... and it is also the efficient cause of motion in the body.\" Choice falls into five stages: (i) intellectual consideration of whether an objective is desirable, (ii) intellectual consideration of means of attaining the objective, (iii) will arrives at an intent to pursue the objective, (iv) will and intellect jointly decide upon choice of means (v) will elects execution. Free will enters as follows: Free will is an \"appetitive power\", that is, not a cognitive power of intellect (the term \"appetite\" from Aquinas's definition \"includes all forms of internal inclination\"). He states that judgment \"concludes and terminates counsel. Now counsel is terminated, first, by the judgment of reason; secondly, by the acceptation of the appetite [that is, the free-will].\"\n\nA compatibilist interpretation of Aquinas's view is defended thus: \"Free-will is the cause of its own movement, because by his free-will man moves himself to act. But it does not of necessity belong to liberty that what is free should be the first cause of itself, as neither for one thing to be cause of another need it be the first cause. God, therefore, is the first cause, Who moves causes both natural and voluntary. And just as by moving natural causes He does not prevent their acts being natural, so by moving voluntary causes He does not deprive their actions of being voluntary: but rather is He the cause of this very thing in them; for He operates in each thing according to its own nature.\"\n\nHistorically, most of the philosophical effort invested in resolving the dilemma has taken the form of close examination of definitions and ambiguities in the concepts designated by \"free\", \"freedom\", \"will\", \"choice\" and so forth. Defining 'free will' often revolves around the meaning of phrases like \"ability to do otherwise\" or \"alternative possibilities\". This emphasis upon words has led some philosophers to claim the problem is merely verbal and thus a pseudo-problem. In response, others point out the complexity of decision making and the importance of nuances in the terminology.\n\nThe problem of free will has been identified in ancient Greek philosophical literature. The notion of compatibilist free will has been attributed to both Aristotle (fourth century BCE) and Epictetus (1st century CE); \"it was the fact that nothing hindered us from doing or choosing something that made us have control over them\". According to Susanne Bobzien, the notion of incompatibilist free will is perhaps first identified in the works of Alexander of Aphrodisias (third century CE); \"what makes us have control over things is the fact that we are causally undetermined in our decision and thus can freely decide between doing/choosing or not doing/choosing them\".\n\nThe term \"free will\" (\"liberum arbitrium\") was introduced by Christian philosophy (4th century CE). It has traditionally meant (until the Enlightenment proposed its own meanings) lack of necessity in human will, so that \"the will is free\" meant \"the will does not have to be such as it is\". This requirement was universally embraced by both incompatibilists and compatibilists.\n\nScience has contributed to the free will problem in at least three ways. First, physics has addressed the question whether nature is deterministic, which is viewed as crucial by incompatibilists (compatibilists, however, view it as irrelevant). Second, although free will can be defined in various ways, all of them involve aspects of the way people make decisions and initiate actions, which have been studied extensively by neuroscientists. Some of the experimental observations are widely viewed as implying that free will does not exist or is an illusion (but many philosophers see this as a misunderstanding). Third, psychologists have studied the beliefs that the majority of ordinary people hold about free will and its role in assigning moral responsibility.\n\nEarly scientific thought often portrayed the universe as deterministic – for example in the thought of Democritus or the Cārvākans – and some thinkers claimed that the simple process of gathering sufficient information would allow them to predict future events with perfect accuracy. Modern science, on the other hand, is a mixture of deterministic and stochastic theories. Quantum mechanics predicts events only in terms of probabilities, casting doubt on whether the universe is deterministic at all, although evolution of the universal state vector is completely deterministic. Current physical theories cannot resolve the question of whether determinism is true of the world, being very far from a potential Theory of Everything, and open to many different interpretations.\n\nAssuming that an indeterministic interpretation of quantum mechanics is correct, one may still object that such indeterminism is for all practical purposes confined to microscopic phenomena. This is not always the case: many macroscopic phenomena are based on quantum effects. For instance, some hardware random number generators work by amplifying quantum effects into practically usable signals. A more significant question is whether the indeterminism of quantum mechanics allows for the traditional idea of free will (based on a perception of free will). If a person's action is, however, only a result of complete quantum randomness, and mental processes as experienced have no influence on the probabilistic outcomes (such as volition), According to many interpretations, non-determinism enables free will to exist, while others assert the opposite (because the action was not controllable by the physical being who claims to possess the free will).\n\nLike physicists, biologists have frequently addressed questions related to free will. One of the most heated debates in biology is that of \"nature versus nurture\", concerning the relative importance of genetics and biology as compared to culture and environment in human behavior. The view of many researchers is that many human behaviors can be explained in terms of humans' brains, genes, and evolutionary histories. This point of view raises the fear that such attribution makes it impossible to hold others responsible for their actions. Steven Pinker's view is that fear of determinism in the context of \"genetics\" and \"evolution\" is a mistake, that it is \"a confusion of \"explanation\" with \"exculpation\"\". Responsibility does not require that behavior be uncaused, as long as behavior responds to praise and blame. Moreover, it is not certain that environmental determination is any less threatening to free will than genetic determination.\n\nIt has become possible to study the living brain, and researchers can now watch the brain's decision-making process at work. A seminal experiment in this field was conducted by Benjamin Libet in the 1980s, in which he asked each subject to choose a random moment to flick their wrist while he measured the associated activity in their brain; in particular, the build-up of electrical signal called the readiness potential (after German Bereitschaftspotential, which was discovered by Kornhuber & Deecke in 1965.) Although it was well known that the readiness potential reliably preceded the physical action, Libet asked whether it could be recorded before the conscious intention to move. To determine when subjects felt the intention to move, he asked them to watch the second hand of a clock. After making a movement, the volunteer reported the time on the clock when they first felt the conscious intention to move; this became known as Libet's W time.\n\nLibet found that the \"unconscious\" brain activity of the readiness potential leading up to subjects' movements began approximately half a second before the subject was aware of a conscious intention to move.\n\nThese studies of the timing between actions and the conscious decision bear upon the role of the brain in understanding free will. A subject's declaration of intention to move a finger appears \"after\" the brain has begun to implement the action, suggesting to some that unconsciously the brain has made the decision \"before\" the conscious mental act to do so. Some believe the implication is that free will was not involved in the decision and is an illusion. The first of these experiments reported the brain registered activity related to the move about 0.2 s before movement onset. However, these authors also found that awareness of action was \"anticipatory\" to activity in the muscle underlying the movement; the entire process resulting in action involves more steps than just the \"onset\" of brain activity. The bearing of these results upon notions of free will appears complex.\n\nSome argue that placing the question of free will in the context of motor control is too narrow. The objection is that the time scales involved in motor control are very short, and motor control involves a great deal of unconscious action, with much physical movement entirely unconscious. On that basis \"... free will cannot be squeezed into time frames of 150–350 ms; free will is a longer term phenomenon\" and free will is a higher level activity that \"cannot be captured in a description of neural activity or of muscle activation...\" The bearing of timing experiments upon free will is still under discussion.\n\nMore studies have since been conducted, including some that try to:\n\nBenjamin Libet's results are quoted in favor of epiphenomenalism, but he believes subjects still have a \"conscious veto\", since the readiness potential does not invariably lead to an action. In \"Freedom Evolves\", Daniel Dennett argues that a no-free-will conclusion is based on dubious assumptions about the location of consciousness, as well as questioning the accuracy and interpretation of Libet's results. Kornhuber & Deecke underlined that absence of conscious will during the early Bereitschaftspotential (termed BP1) is not a proof of the non-existence of free will, as also unconscious agendas may be free and non-deterministic. According to their suggestion, man has relative freedom, i.e. freedom in degrees, that can be in- or decreased through deliberate choices that involve both conscious and unconscious (panencephalic) processes.\n\nOthers have argued that data such as the Bereitschaftspotential undermine epiphenomenalism for the same reason, that such experiments rely on a subject reporting the point in time at which a conscious experience occurs, thus relying on the subject to be able to consciously perform an action. That ability would seem to be at odds with early epiphenomenalism, which according to Huxley is the broad claim that consciousness is \"completely without any power… as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery\".\n\nAdrian G. Guggisberg and Annaïs Mottaz have also challenged those findings.\n\nA study by Aaron Schurger and colleagues published in the Proceedings of the National Academy of Sciences (PNAS) challenged assumptions about the causal nature of the readiness potential itself (and the \"pre-movement buildup\" of neural activity in general), casting doubt on conclusions drawn from studies such as Libet's and Fried's.\n\nIt has been shown that in several brain-related conditions, individuals cannot entirely control their own actions, though the existence of such conditions does not directly refute the existence of free will. Neuroscientific studies are valuable tools in developing models of how humans experience free will.\n\nFor example, people with Tourette syndrome and related tic disorders make involuntary movements and utterances (called tics) despite the fact that they would prefer not to do so when it is socially inappropriate. Tics are described as semi-voluntary or \"unvoluntary\", because they are not strictly \"involuntary\": they may be experienced as a \"voluntary\" response to an unwanted, premonitory urge. Tics are experienced as irresistible and must eventually be expressed. People with Tourette syndrome are sometimes able to suppress their tics for limited periods, but doing so often results in an explosion of tics afterward. The control exerted (from seconds to hours at a time) may merely postpone and exacerbate the ultimate expression of the tic.\n\nIn alien hand syndrome, the afflicted individual's limb will produce unintentional movements without the will of the person. The affected limb effectively demonstrates 'a will of its own.' The sense of agency does not emerge in conjunction with the overt appearance of the purposeful act even though the sense of ownership in relationship to the body part is maintained. This phenomenon corresponds with an impairment in the premotor mechanism manifested temporally by the appearance of the readiness potential (see section on the Neuroscience of Free Will above) recordable on the scalp several hundred milliseconds before the overt appearance of a spontaneous willed movement. Using functional magnetic resonance imaging with specialized multivariate analyses to study the temporal dimension in the activation of the cortical network associated with voluntary movement in human subjects, an anterior-to-posterior sequential activation process beginning in the supplementary motor area on the medial surface of the frontal lobe and progressing to the primary motor cortex and then to parietal cortex has been observed. The sense of agency thus appears to normally emerge in conjunction with this orderly sequential network activation incorporating premotor association cortices together with primary motor cortex. In particular, the supplementary motor complex on the medial surface of the frontal lobe appears to activate prior to primary motor cortex presumably in associated with a preparatory pre-movement process. In a recent study using functional magnetic resonance imaging, alien movements were characterized by a relatively isolated activation of the primary motor cortex contralateral to the alien hand, while voluntary movements of the same body part included the natural activation of motor association cortex associated with the premotor process. The clinical definition requires \"feeling that one limb is foreign or has a \"will of its own,\" together with observable involuntary motor activity\" (emphasis in original). This syndrome is often a result of damage to the corpus callosum, either when it is severed to treat intractable epilepsy or due to a stroke. The standard neurological explanation is that the felt will reported by the speaking left hemisphere does not correspond with the actions performed by the non-speaking right hemisphere, thus suggesting that the two hemispheres may have independent senses of will.\n\nIn addition, one of the most important (\"first rank\") diagnostic symptoms of schizophrenia is the patient's delusion of being controlled by an external force. People with schizophrenia will sometimes report that, although they are acting in the world, they do not recall initiating the particular actions they performed. This is sometimes likened to being a robot controlled by someone else. Although the neural mechanisms of schizophrenia are not yet clear, one influential hypothesis is that there is a breakdown in brain systems that compare motor commands with the feedback received from the body (known as proprioception), leading to attendant hallucinations and delusions of control.\n\nExperimental psychology's contributions to the free will debate have come primarily through social psychologist Daniel Wegner's work on conscious will. In his book, \"The Illusion of Conscious Will\" Wegner summarizes what he believes is empirical evidence supporting the view that human perception of conscious control is an illusion. Wegner summarizes some empirical evidence that may suggest that the perception of conscious control is open to modification (or even manipulation). Wegner observes that one event is inferred to have caused a second event when two requirements are met:\nFor example, if a person hears an explosion and sees a tree fall down that person is likely to infer that the explosion caused the tree to fall over. However, if the explosion occurs after the tree falls down (that is, the first requirement is not met), or rather than an explosion, the person hears the ring of a telephone (that is, the second requirement is not met), then that person is not likely to infer that either noise caused the tree to fall down.\n\nWegner has applied this principle to the inferences people make about their own conscious will. People typically experience a thought that is consistent with a behavior, and then they observe themselves performing this behavior. As a result, people infer that their thoughts must have caused the observed behavior. However, Wegner has been able to manipulate people's thoughts and behaviors so as to conform to or violate the two requirements for causal inference. Through such work, Wegner has been able to show that people often experience conscious will over behaviors that they have not, in fact, caused – and conversely, that people can be led to experience a lack of will over behaviors they did cause. For instance, priming subjects with information about an effect increases the probability that a person falsely believes is the cause. The implication for such work is that the perception of conscious will (which he says might be more accurately labelled as 'the emotion of authorship') is not tethered to the execution of actual behaviors, but is inferred from various cues through an intricate mental process, \"authorship processing\". Although many interpret this work as a blow against the argument for free will, both psychologists and philosophers have criticized Wegner's theories.\n\nEmily Pronin has argued that the subjective experience of free will is supported by the introspection illusion. This is the tendency for people to trust the reliability of their own introspections while distrusting the introspections of other people. The theory implies that people will more readily attribute free will to themselves rather than others. This prediction has been confirmed by three of Pronin and Kugler's experiments. When college students were asked about personal decisions in their own and their roommate's lives, they regarded their own choices as less predictable. Staff at a restaurant described their co-workers' lives as more determined (having fewer future possibilities) than their own lives. When weighing up the influence of different factors on behavior, students gave desires and intentions the strongest weight for their own behavior, but rated personality traits as most predictive of other people.\n\nPsychologists have shown that reducing a person's belief in free will makes them less helpful and more aggressive. This may occur because the subject loses a sense of self-efficacy.\n\nCaveats have, however, been identified in studying a subject's awareness of mental events, in that the process of introspection itself may alter the experience.\n\nJ.B. Miles contradicts the idea that free will has prosocial benefits, recognizing that many distinguished minds have already brought up the negative effects that such a belief would ensure. The explanation for the commonality of this mix-up is a lack of knowledge regarding the free will debate in psychological research. Miles analyzed the methods of popular studies and concluded that such research purported to be examining associations between behavior and disbelief in free will are actually examining the associations between behavior and belief in fatalism. While evidence of the negative effects of a belief in fatalism is legitimate, the research fails to study the effects of belief on free will which they claim to discuss. This occurrence is due to an incorrect understanding and implication that fatalism accompanies determinism. Fatalism is distinguished by the idea that decisions lack effect on the future because everything is determined. Conversely, determinism is the belief that everything operates under cause and effect; every action determines a reaction. Determinism, therefore emphasizes the importance and responsibility one has in decision making as every choice will have an accompanying effect. Seeing this flaw throughout commonly cited research, Miles presents countering research which includes “evidence that the myth of free choice encourages immoral, unjust, prejudiced, and anti-intellectual behaviour.” Miles suggests that while both extremes of fatalism and belief in free will result in negative social outcomes, determinism serves to encourage intentional, prosocial decision making. Ultimately, the point of this research is to encourage accurate knowledge of the free will debate when conducting and evaluating such studies in experimental psychology.\n\nRegardless of the validity of, or benefit of, belief in free will, it may be beneficial to understand where the idea comes from. One contribution is randomness. While it is established that randomness is not the only factor in the perception of the free will, it has been shown that randomness can be mistaken as free will due to its indeterminacy. This misconception applies both when considering oneself and others. Another contribution is choice. It has been demonstrated that people's belief in free will increases if presented with a simple level of choice. The specificity of the amount of choice is important, as too little or too great a degree of choice may negatively influence belief. It is also likely that the associative relationship between level of choice and perception of free will is influentially bidirectional. It is also possible that one's desire for control, or other basic motivational patterns, act as a third variable.\n\nIn recent years, free will belief in individuals has been analysed with respect to traits in social behaviour. In general the concept of free will researched to date in this context has been that of the incompatibilist, or more specifically, the libertarian, that is freedom from determinism.\n\nWhether people naturally adhere to an incompatibilist model of free will has been questioned in the research. Eddy Nahmias has found that incompatibilism is not intuitive – it was not adhered to, in that determinism does not negate belief in moral responsibility (based on an empirical study of people's responses to moral dilemmas under a deterministic model of reality). Edward Cokely has found that incompatibilism is intuitive – it was naturally adhered to, in that determinism does indeed negate belief in moral responsibility in general. Joshua Knobe and Shaun Nichols have proposed that incompatibilism may or may not be intuitive, and that it is dependent to some large degree upon the circumstances; whether or not the crime incites an emotional response – for example if it involves harming another human being. They found that belief in free will is a cultural universal, and that the majority of participants said that (a) our universe is indeterministic and (b) moral responsibility is not compatible with determinism.\n\nStudies indicate that peoples' belief in free will is inconsistent. Emily Pronin and Matthew Kugler found that people believe they have more free will than others.\n\nStudies also reveal a correlation between the likelihood of accepting a deterministic model of mind and personality type. For example, Adam Feltz and Edward Cokely found that people of an extrovert personality type are more likely to dissociate belief in determinism from belief in moral responsibility.\n\nRoy Baumeister and colleagues reviewed literature on the psychological effects of a belief (or disbelief) in free will. The first part of their analysis (which the only relevant part to this section) was not meant to discover the types of free will that actually exist. The researchers instead sought to identify what people believe, how many people believed it, and the effects of those beliefs. Baumeister found that most people tend to believe in a sort of \"naive compatibilistic free will\".\n\nThe researchers also found that people consider acts more \"free\" when they involve a person opposing external forces, planning, or making random actions. Notably, the last behaviour, \"random\" actions, may not be possible; when participants attempt to perform tasks in a random manner (such as generating random numbers), their behaviour betrays many patterns.\n\nTaking into account all these personal variables existing in the debates over free will, Manuel Vargas has generally assumed that in fact “there is nearly always an unremarked upon elephant that lurks in rooms where philosophers discuss free will”, but he particularly pointed to the role of (i)religion in motivating and sustaining various accounts of free will. As he put,My guess is that we would learn that a disproportionate number—perhaps even most—libertarians are religious and, especially, Christian. I suspect we would also learn that the overwhelming majority of compatibilists are atheist or agnostic. (I do not have a guess about the religious predilections of the various stripes of skeptical incompatibilists.) Even so, it may not be obvious why the religious beliefs of particular philosophers should matter. We could think that the arguments of the various partisans in free will debates should be judged on their merits, irrespective of whether or not they have religious motivation. However, systematically ignoring the role of religion can remove important considerations from view, considerations that affect how we think about, argue, and evaluate the various philosophical possibilities.For example, by a detailed analysis of the writings and work of Arthur Eddington and Arthur Holy Compton in the English speaking world of the late 1920s and 1930s, one analysis clearly revealed not only a strong intrinsic religious motivation and specific conservative worldview concerns behind their own work on free will, but also similar personal, political and ideological factors, only of the opposite sign, in their opponents.\n\nA recent 2009 survey has shown that compatibilism is quite a popular stance among those who specialize in philosophy (59%). Belief in libertarianism amounted to 14%, while a lack of belief in free will equaled 12%. More than a half of surveyed people were US Americans.\n\n79 percent of evolutionary biologists said that they believe in free-will according to a survey conducted in 2007, only 14 percent chose no free will, and 7 percent did not answer the question.\n\nBaumeister and colleagues found that provoking disbelief in free will seems to cause various negative effects. The authors concluded, in their paper, that it is belief in determinism that causes those negative effects. This may not be a very justified conclusion, however. First of all, free will can at least refer to either libertarian (indeterministic) free will or compatibilistic (deterministic) free will. Having participants read articles that simply \"disprove free will\" is unlikely to increase their understanding of determinism, or the compatibilistic free will that it still permits.\n\nIn other words, \"provoking disbelief in free will\" probably causes a belief in fatalism. As discussed earlier in this article, compatibilistic free will is illustrated by statements like \"my choices have causes, and an effect – so I affect my future\", whereas fatalism is more like \"my choices have causes, but no effect – I am powerless\". Fatalism, then, may be what threatens people's sense of self-efficacy. Lay people should not confuse fatalism with determinism, and yet even professional philosophers occasionally confuse the two. It is thus likely that the negative consequences below can be accounted for by participants developing a belief in \"fatalism\" when experiments attack belief in \"free will\". To test the effects of belief in determinism, future studies would need to provide articles that do not simply \"attack free will\", but instead focus on explaining determinism and compatibilism. Some studies have been conducted indicating that people react strongly to the way in which mental determinism is described, when reconciling it with moral responsibility. Eddy Nahmias has noted that when people's actions are framed with respect to their beliefs and desires (rather than their neurological underpinnings), they are more likely to dissociate determinism from moral responsibility.\n\nVarious social behavioural traits have been correlated with the belief in deterministic models of mind, some of which involved the experimental subjection of individuals to libertarian and deterministic perspectives.\n\nAfter researchers provoked volunteers to disbelieve in free will, participants lied, cheated, and stole more. Kathleen Vohs has found that those whose belief in free will had been eroded were more likely to cheat. In a study conducted by Roy Baumeister, after participants read an article arguing against free will, they were more likely to lie about their performance on a test where they would be rewarded with cash. Provoking a rejection of free will has also been associated with increased aggression and less helpful behaviour as well as mindless conformity. Disbelief in free will can even cause people to feel less guilt about transgressions against others.\n\nBaumeister and colleagues also note that volunteers disbelieving in free will are less capable of counterfactual thinking. This is worrying because counterfactual thinking (\"If I had done something different...\") is an important part of learning from one's choices, including those that harmed others. Again, this cannot be taken to mean that belief in determinism is to blame; these are the results we would expect from increasing people's belief in fatalism.\n\nAlong similar lines, Tyler Stillman has found that belief in free will predicts better job performance.\n\nThe six orthodox (astika) schools of thought in Hindu philosophy do not agree with each other entirely on the question of free will. For the Samkhya, for instance, matter is without any freedom, and soul lacks any ability to control the unfolding of matter. The only real freedom (\"kaivalya\") consists in realizing the ultimate separateness of matter and self. For the Yoga school, only Ishvara is truly free, and its freedom is also distinct from all feelings, thoughts, actions, or wills, and is thus not at all a freedom of will. The metaphysics of the Nyaya and Vaisheshika schools strongly suggest a belief in determinism, but do not seem to make explicit claims about determinism or free will.\n\nA quotation from Swami Vivekananda, a Vedantist, offers a good example of the worry about free will in the Hindu tradition.\n\nHowever, the preceding quote has often been misinterpreted as Vivekananda implying that everything is predetermined. What Vivekananda actually meant by lack of free will was that the will was not \"free\" because it was heavily influenced by the law of cause and effect – \"The will is not free, it is a phenomenon bound by cause and effect, but there is something behind the will which is free.\" Vivekananda never said things were absolutely determined and placed emphasis on the power of conscious choice to alter one's past karma: \"It is the coward and the fool who says this is his fate. But it is the strong man who stands up and says I will make my own fate.\"\n\nBuddhism accepts both freedom and determinism (or something similar to it), but in spite of its focus towards the human agency, rejects the western concept of a total agent from external sources. According to the Buddha, \"There is free action, there is retribution, but I see no agent that passes out from one set of momentary elements into another one, except the [connection] of those elements.\" Buddhists believe in neither absolute free will, nor determinism. It preaches a middle doctrine, named \"pratitya-samutpada\" in Sanskrit, often translated as \"inter-dependent arising\". This theory is also called \"Conditioned Genesis\" or \"Dependent Origination\". It teaches that every volition is a conditioned action as a result of ignorance. In part, it states that free will is inherently conditioned and not \"free\" to begin with. It is also part of the theory of karma in Buddhism. The concept of karma in Buddhism is different from the notion of karma in Hinduism. In Buddhism, the idea of karma is much less deterministic. The Buddhist notion of karma is primarily focused on the cause and effect of moral actions in this life, while in Hinduism the concept of karma is more often connected with determining one's destiny in future lives.\n\nIn Buddhism it is taught that the idea of absolute freedom of choice (that is that any human being could be completely free to make any choice) is unwise, because it denies the reality of one's physical needs and circumstances. Equally incorrect is the idea that humans have no choice in life or that their lives are pre-determined. To deny freedom would be to deny the efforts of Buddhists to make moral progress (through our capacity to freely choose compassionate action). \"Pubbekatahetuvada\", the belief that all happiness and suffering arise from previous actions, is considered a wrong view according to Buddhist doctrines. Because Buddhists also reject agenthood, the traditional compatibilist strategies are closed to them as well. Instead, the Buddhist philosophical strategy is to examine the metaphysics of causality. Ancient India had many heated arguments about the nature of causality with Jains, Nyayists, Samkhyists, Cārvākans, and Buddhists all taking slightly different lines. In many ways, the Buddhist position is closer to a theory of \"conditionality\" than a theory of \"causality\", especially as it is expounded by Nagarjuna in the \"Mūlamadhyamakakārikā\".\n\nThe notions of free will and predestination are heavily debated among Christians. Free will in the Christian sense is the ability to choose between good or evil. Among Catholics, there are those holding to Thomism, adopted from what Thomas Aquinas put forth in the \"Summa Theologica.\" There are also some holding to Molinism which was put forth by Jesuit priest Luis de Molina. Among Protestants there is Arminianism, held primarily by Methodist and some Baptist, and formulated by Dutch theologian Jacobus Arminius; and there is also Calvinism held by most in the Reformed tradition which was formulated by the French Reformed theologian, John Calvin. John Calvin was heavily influenced by Augustine of Hippo views on predestination put forth in his work \"On the Predestination of the Saints.\" Martin Luther seems to hold views on predestination similar to Calvinism in his \"On the Bondage of the Will,\" thus rejecting free will. In condemnation of Calvin and Luther views, the Council of Trent declared that \"the free will of man, moved and excited by God, can by its consent co-operate with God, Who excites and invites its action; and that it can thereby dispose and prepare itself to obtain the grace of justification. The will can resist grace if it chooses. It is not like a lifeless thing, which remains purely passive. Weakened and diminished by Adam's fall, free will is yet not destroyed in the race (Sess. VI, cap. i and v).\"\n\nPaul the Apostle discusses Predestination in some of his Epistles.\n\n\"\"For whom He foreknew, He also predestined to become conformed to the image of His Son, that He might be the first-born among many brethren; and whom He predestined, these He also called; and whom He called, these He also justified; and whom He justified, these He also glorified.\"” —Romans 8:29–30\n\n“\"He predestined us to adoption as sons through Jesus Christ to Himself, according to the kind intention of His will.\"” —Ephesians 1:5\n\nThe exact meaning of these verses has been debated by Christian theologians throughout history.\n\nMaimonides reasoned that human beings have free will (at least in the context of choosing to do good or evil). Without free will, the demands of the prophets would have been meaningless, there would be no need for the Torah, and justice could not be administered. In Maimonides's view, human free will is granted by God as part of the universe's design.\n\nIn Islam the theological issue is not usually how to reconcile free will with God's foreknowledge, but with God's \"jabr\", or divine commanding power. al-Ash'ari developed an \"acquisition\" or \"dual-agency\" form of compatibilism, in which human free will and divine \"jabr\" were both asserted, and which became a cornerstone of the dominant Ash'ari position. In Shia Islam, Ash'aris understanding of a higher balance toward predestination is challenged by most theologians. Free will, according to Islamic doctrine is the main factor for man's accountability in his/her actions throughout life. Actions taken by people exercising free will are counted on the Day of Judgement because they are their own; however, the free will happens with the permission of God.\n\nThe philosopher Søren Kierkegaard claimed that divine omnipotence cannot be separated from divine goodness. As a truly omnipotent and good being, God could create beings with true freedom over God. Furthermore, God would voluntarily do so because \"the greatest good ... which can be done for a being, greater than anything else that one can do for it, is to be truly free.\" Alvin Plantinga's \"free will defense\" is a contemporary expansion of this theme, adding how God, free will, and evil are consistent.\n\nSome philosophers follow William of Ockham in holding that necessity and possibility are defined with respect to a given point in time and a given matrix of empirical circumstances, and so something that is merely possible from the perspective of one observer may be necessary from the perspective of an omniscient. Some philosophers follow Philo of Alexandria, a philosopher known for his homocentrism, in holding that free will is a feature of a human's soul, and thus that non-human animals lack free will.\n\n\n"}
{"id": "4079050", "url": "https://en.wikipedia.org/wiki?curid=4079050", "title": "Gifts in kind", "text": "Gifts in kind\n\nGifts in kind, also referred to as \"in-kind donations\", is a kind of charitable giving in which, instead of giving money to buy needed goods and services, the goods and services themselves are given. Gifts in kind are distinguished from gifts of cash or stock. Some types of gifts in kind are appropriate, others are not. Examples of in-kind gifts include goods like food, clothing, medicines, furniture, office equipment, and building materials. Performance of services, such as building an orphanage, providing office space or offering administrative support, may also be counted as in-kind gifts. \n\nWhile many attest to the benefits of in-kind over cash gifts, others have argued for their disadvantages, particularly in the context of disaster relief.\n\nMany donated goods are either second hand or otherwise surplus. If not donated to people who need them, they might otherwise end up in a landfill. Thus, it is argued that gifts in kind reduce resource use and pollution. This provides a means, particularly for corporations, of doing social good with things that would otherwise be a liability.\n\nDuring disasters and other humanitarian crises, companies and individuals often want to help with the disaster relief operations. Some people have argued that giving goods that are already at hand is more cost effective for the donor than giving money to buy these same goods, thus reducing the cost of buying the goods afresh, particularly in the face of shortages.\n\nHelping with longer term development in impoverished or otherwise distressed areas is a high priority for governments and large NGOs. It is argued that gifts in kind can be a significant component of a larger humanitarian development strategy.\n\nIt has been argued that donated goods are much less susceptible to becoming graft because physical goods are more tangible than money. \n\nHowever, the argument may be reversed in the modern context, now that there exist mobile phone-based payment mechanisms such as m-Pesa that have been used successfully for cash transfer programs, making cash transfers less dependent on intermediaries than the shipping of physical goods.\n\nGifts in kind supply a market efficiency which is difficult to attain by other means. For example, many charities that provide life-saving medications to people in impoverished nations could not afford to buy these drugs using their cash donations or grants alone. Donated drugs help these organizations to work most effectively at a much lower cost.\n\nSome products are simply not available, but are desperately needed nonetheless. An example is anti-malarial drugs. These drugs are unavailable in many areas of the world where they are most needed, and if they are available, the people who need them are not in a position to purchase them. They are not manufactured locally and the costs of setting up local manufacturing facilities would be prohibitive, given the regulations surrounding pharmaceuticals. There is a high likelihood of locally available drugs being counterfeit, with often fatal consequences.\n\nAs more and more companies continue to acknowledge their corporate social responsibility, they are also recognizing the benefits of participating in gifts in kind programs. In \" The Business Case for Product Philanthropy, \" a 63-page report published by the Indiana University School of Public and Environmental Affairs, authors Justin M. Ross and Kellie L. McGiverin-Bohan argue that businesses can do well by doing good through product philanthropy, as well as explore the advantages of donating goods over the liquidation and/or destruction of goods. In addition, with cash donations on the decrease over the past several years, offering donations of goods and services is a way for corporations to continue pursuing their philanthropic goals.\n\nOne of the chief criticisms of gifts in kind, particularly in the context of disaster relief but also in other contexts, is that the things that people are likely to gift may be poorly matched to the immediate needs of recipients, but rather be influenced by what donors happen to want to dispose of. Some of the possibilities are:\n\n\nIn addition to the argument that gifts in kind often do not meet the needs of recipients, it has also been argued that gifts in kind fail to empower the recipients because the recipients don't have as much flexibility on how to spend the gifts as they would with gifts of cash or of public goods that they actively solicit. Relatedly, it has been argued that sending gifts in kind without checking on what the recipients may actually need may be disrespectful to the recipients, and in some cases self-centered and narcissistic, being focused on the needs of the donor rather than the recipient.\n\nSome critics of gifts in kind argue that, like dumping, these have an artificial adverse impact on local industries producing similar goods.\n\nSome of the downsides of gifts in kind may be mitigated by allowing recipients to communicate their needs to donors, thus helping donors and recipients match up. This has been made possible with the advent of the Internet as it is now possible to create an online marketplace for in-kind donations. Gifts in Kind International operates a network called Good360 that aims to do exactly this. Occupy Sandy volunteers use a sort of gift registry for this purpose; families and businesses impacted by the storm make specific requests, which remote donors can purchase directly via a web site.\n\nThe majority of online giving marketplaces, including GlobalGiving and DonorsChoose, however, are focused on cash donations, though the nonprofits seeking these donations usually specify what types of things they intend to buy with a given donation quantity.\n\nGlobal Hand has published a series of standards for gifts in kind. The principles include:\n\n\nand many more.\n\nThe Tales from the Hood blog has argued that there are two preconditions for successful gifts in kind.:\n\n\nUnlike a disaster relief scenario, the needs of a charity shop are long-term and more flexible; any item that can be sold at a price higher than the cost of warehousing it could be worthwhile. Large non-profits, such as Goodwill Industries, are also able to make use of items that cannot be sold in their thrift stores, for example by bundling them and selling them as bulk material or scrap. These stores refuse donations that cost money to dispose of safely if unwanted, such as e-waste.\n\n\n"}
{"id": "39658069", "url": "https://en.wikipedia.org/wiki?curid=39658069", "title": "Infant clothing", "text": "Infant clothing\n\nInfant clothing or baby clothing is clothing for infants. Baby fashion is a social-cultural consumerist practice that encodes in children's fashion the representation of many social features and depicts a system characterized by differences in social class, richness, gender or ethnicity.\n\nInfant and toddler clothing size is typically based on age. These are usually \"preemie\" for a preterm birth baby, 0 to 3 months, 3 to 6 months, 6 to 9 months, 9 to 12 months, 12 months, 18 months, and 24 months, though there is no industry standard definition for those sizes. Most retailers provide sizing charts based on a child's weight, height, or both, and the child's weight and height percentile may also be used for properly sizing clothing for the infant.\n\nIn an article in the October 1945 issue of \"Ladies' Home Journal\", B. F. Skinner stated that clothing and bedding \"interfere with normal exercise and growth and keep the baby from taking comfortable postures or changing posture during sleep\". An infant may stretch, necessitating clothing that is sufficiently loose to allow movement.\n\nIn the past centuries Baby Fashion assumed distinctive features between social classes. Dresses had a powerful potential in displaying social distinction. In general Baby Fashion was exploited by the high classes, or the so-called elite, to traduce symbols of power, wealth, richness. Children's appearance was useful to represent the family's position in the society. While, on the other side, the working classes were not involved in this kind of practice, since clothes should have been practical and not expensive. It must be remembered that in the feudal society, as in the industrial society, children worked as well as adults. \nThe symbolic value of Baby Fashion between high classes and the nobility was not only a western peculiarity. For example, in some African or oriental countries colors and shapes took a particular importance, while Western elites concentrated on fabrics and precious materials. But probably Western European Fashion put a stronger stress on the representation of social position through clothing; in fact, this practice became customary already in the late 13th century. Family paintings and portraits were very common between the European high classes, so today we have plenty of examples of ancient Baby Fashion features. A particular characteristic of ancient Baby Fashion is the absence of marked gender distinctions between young children. After a certain age, girls were painted in big gowns, and boys in trousers, or commonly military uniforms. But before they reached ten years of age, usually, children were represented wearing gowns, no matter if they are boys or girls. The symbols of wealth and power are translated by these rich dresses, with huge gowns full of trims, ornaments, and embroidered details. This kind of style developed in the Spanish Court in the 14th century and became common also in other Catholic Countries as Italy or France. This rich style makes very difficult, almost for a modern observer, to recognize boys from girls. Many examples come from 17th and 18th century European Court, where family paintings where very important expressions of power. In France Elizabeth Vigée-Lebrun's paintings represented young Mary Antoinette's children, and the younger Queen's son is dressed in a white, soft, traditional gown and coif.\n\nIn the United States, before the 1890s children predominantly wore clothing made by their parents. By 1910, retailers had formed a \"publicity structure\" toward children for the sale of children's goods, which resulted in a significant increase in the sale of manufactured children's clothing, sportswear, candy, and baby clothing. By 1915, baby clothing had become one of the nation's largest industries.\n\nIn the 1980s, infant and toddler clothing fashion design became an increasing source of revenue for US designer labels and fashion design houses, such as Polo Ralph Lauren and Guess. Gap Inc. established \"Baby Gap\" in 1990, four years after it had introduced the \"Gap Kids\" line.\n\nThe age of first-time mothers has been increasing in Western cultures, from 21.5 years old in 1970 to over 25 years old in the early 2000s, and hence they have more disposable income to spend for infant goods, including clothing.\n\nInfant clothing is within the retail and wholesale trade categories of the North American Industry Classification System (NAICS). For the 2012 revision, wholesale infant clothing is in category 424330 (Women's, Children's, and Infants' Clothing and Accessories Merchant Wholesalers) and retail infant clothing is in category 448130 (Children's and Infants' Clothing Stores).\n\nThe imitating model has changed over years. In the past nobility owned what was perceived as an ideal style paradigm. While nowadays, the upper-middle class embodies the ideal fashion; especially, in today's pop culture, this role is covered by celebrities and the so-called V.I.P.\n\nClothes have long been used to hide sexual differences in its strong biological sense and, at the same time, to point up and signal it through assumptions concerning gender in clothing codes. The manner in which an infant is dressed \"affects behavior toward the infant\". Clothing may be sex-typed by colour (e.g. - pink or yellow for girls, blue or red for boys), or by style (ruffles and puffed sleeves for girls). If children's clothes, in the past, were used to differentiate those belonging to rich families from those coming from poor ones, today clothes are a symbol of gender differentiation.\n\nA 1985 study found that US parents were not \"bothered by strangers' mistaking the infant's sex\".\n\nGender is a way in which social practices are ordered. In gender differentiation process, the everyday conduct of life is organized in relation to biological differences, defined by the bodily structures and processes of human reproduction. Bodies are therefore both agents and objects of practice. Such body-reflexive practices that define the social structure are not internal to the individual, but they involve social relations and shared symbolism.\nThey may well involve large-scale social institutions. Within this body structured practices, particular versions of femininity and masculinity are materialized as meaningful bodies and embodied meanings. Through body-reflexive practices and through the biological division of human bodies into male and female, more than individual lives are formed: a social world is formed, modelled on the basis of gender stereotypes.\nBy gender stereotypes we mean a representation, imagery or classification of men, women, or gender relations, that presents a simplified, conventionalized and selective picture of men's and women's lives. This representation is pretty often spread up also by the exposure to TV contents, which has been associated with more stereotypical sexual attitudes (i.e., the view that men are stereotypically sex-driven, the notion according to which women are sexual objects to be valued for their looks).\nTherefore, stereotypes frequently become vehicles for norms of inequality. For instance, a persistent devaluing of women can have the effect of celebrating masculine bodily power, or of believing that women and men should be confined to narrow and segregated social roles.\nIn Baby Fashion, gender-differentiated consumption can go from toys to particular dressing accessories or objects of everyday life. This particular structured system becomes an important tool to maintain intact these constructed gender social identities.\nDespite the different gender studies that has been done during the last years, it seems that sex role theory, which is an old approach based on the power of custom and social conformity, seems to be correct about some still existing social constructions. Sex role theory explains gender patterns by appealing to the social customs that define proper behaviour for women and for men. People learn their roles, in the course of growing up, and then perform them under social pressures.\nAccording to this theory, children, since their first years of life, are distinguished into girls and boys. They are dressed with the respective gender identifiable colours, the typical pink and blue. The blue dressed children are supposed to behave differently than the ones dressed in pink: they should be ruder, aggressive, demanding and more powerful. On the contrary, the pink dressed children are supposed to behave in a passive way, to be obedient and even prettier. When the girls grew up they are dressed with cute dresses, they are given toys like dolls and make up accessories, and they are educated to always take care of their physical aspect, to be able to cook and to always be educate and gentle with others. On the other hand, when boys grew up they are taught how to drive cars, how to be competitive in the market in order to earn money and how to chase all those persons who were dressed in pink colours.\n\nSpeaking about baby fashion, it is important to stress the consumerism that is behind all of this. Buying infant clothing is becoming more and more a phenomenon of fashion so that, since they are mainly bought by parents, sometimes the purchasing action is brought to an upper level through the objectification of the child. In fact, it can happen that they are adopted as a means to demonstrate the capability of their family to follow most recent fashions. When clothes are used in a way that differs from the norm, this can attract attention and provoke reactions. This affirmation is supported by the many practices that define today’s society and that highlight a current phenomenon: the sexualization of the child.\n\nThe acceptable sexual connotations expressed by clothing depends on both the era and the age of the person wearing the clothes.\nHowever, clothes continuously witness a phenomenon of sexualisation, resulting from a background that affects adults as well as children. Indeed, the body is more visible today than it was in the 1800s and in the first half of the 1900s.\nClothes themselves are innocent, it is the way in which they are displayed that sexualizes them: this happens mostly because of the influence of various media (television, internet, music, social networks, advertising etc.), and the way in which children's clothes are disposed alongside the ones of adults. The automatic consequence is the association between the two types of clothes, summed-up in the common practice, carried out by manufacturers and retailers, of scaling-down adult version of fashion into a child one. In this way, instead of age-appropriate clothes, children wear those that in principle have been designed for grown-up people. This happens especially with young girls who, nowadays, can be easily seen wearing short skirts, high heels, very deep necklines, bikinis or padded bras, all available in fabrics and prints that most of the people would consider inadequate for them. In fact, fashion is seen as imposing oppressive forms of gender identity, embodying practices designed to objectify and limit little girls.\nAt the same time, it will be difficult to ignore the limitations given to boys too. They are pressured by expectations about proper masculine behaviour from parents, school, mass media and peer groups. Masculine behaviour's role models are provided by sportsmen, military heroes, etc. and the social sanctions, from mild disapproval to violence, are applied to boys and men who do not conform to the role norms. \nThis phenomenon is exasperated by the untimely sexual development of children that has been registered in recent years. As a matter of fact, it has been demonstrated that contemporary kids tend to reach a sexual maturity at an early age, accelerating therefore the mental, physical and emotional evolution and catching the possibility to wear daring dresses.\n\nExcessive thermal insulation has been associated with an increased incidence of sudden infant death syndrome (SIDS). The primary causes are an excess of bedding or clothing, soft sleep surfaces, and stuffed animals. The odds ratio of SIDS associated with thermal insulation at least two togs above the lower critical value (after adjusting for season and confounding factors) was 1.35 in a New Zealand study, which also found that SIDS had some correlation with too little thermal insulation. A 1984 study of 34 infant cot deaths found that for 2/3 excessive clothing and over-wrapping was a contributing cause.\n\nClothing was responsible for an increased incidence of congenital hip dislocation (CDH) in Japanese infants. By custom, a diaper and clothing had been applied to the infants \"with the legs in extension\". Before 1965, the incidence of CDH in infants was up to 3.5%, but a national campaign established in 1975 \"to avoid prolonged extension of the hips and knees of infants during the early postnatal period\" led to a reduction in incidence of CDH in infants to 0.2% by the early 1980s.\n\nClose-fitting nightwear is \"invariably safer than long, loose nightwear\".\n\nCanada prohibits the importation, sale, or advertising of classes of clothing and other consumer products that do not meet the minimum flammability standards. Standards for infant and children's sleepwear were defined in 1971 and amended in 1987 as part of the Hazardous Products Act. Any textile product must also satisfy textile labelling requirements specified in the Textile Labelling Act administered by the Competition Bureau of Industry Canada.\n\nIn the United States, textile flammability is subject to the U.S. Flammable Fabrics Act. A study found that children less than five years old had a higher incidence of sleepwear fires than other age groups, and that they had an \"unreasonable risk of death or injury from fire accidents involving sleepwear\". This led to the first flammability standard for infant and children's sleepwear. On 30 April 1996, the Consumer Product Safety Commission relaxed standards for children's sleepwear flammability, allowing retailers to sell \"tight-fitting children's sleepwear and sleepwear for infants aged 9 months or younger\" that does not meet the flammability criteria.\nInfants may have allergic reactions to certain materials, especially synthetic fibres such as polyester, rayon, and nylon, and natural fibres such as wool.\n\n\n\n"}
{"id": "17209272", "url": "https://en.wikipedia.org/wiki?curid=17209272", "title": "Intelligibility (philosophy)", "text": "Intelligibility (philosophy)\n\nIn philosophy, intelligibility is what can be comprehended by the human mind in contrast to sense perception. The intelligible method is thought thinking itself, or the human mind reflecting on itself. Plato referred to the intelligible realm of mathematics, forms, first principles, logical deduction, and the dialectical method. The intelligible realm of thought thinking about thought does not necessarily require any visual images, sensual impressions, and material causes for the contents of mind. Descartes referred to this method of thought thinking about itself, without the possible illusions of the senses. Kant made similar claims about a priori knowledge. A priori knowledge is claimed to be independent of the content of experience.\n\nThe objects or concepts that have intelligibility may be called intelligible. Some possible examples are numbers and the logical law of non-contradiction.\n\nThere may be a distinction between everything that is intelligible and everything that is visible, called the \"intelligible world\" and the \"visible world\" in e.g. the analogy of the divided line as written by Plato.\n\nThe Absolute is generally regarded as being only partially intelligible. The Absolute is the idea of an unconditional reality which transcends limited, conditional, everyday existence. It is sometimes used as a term for God or the Divine. Other similar concepts are The One and First Cause.\n\n"}
{"id": "4448094", "url": "https://en.wikipedia.org/wiki?curid=4448094", "title": "Midtown Exchange", "text": "Midtown Exchange\n\nThe Midtown Exchange is a large commercial building located in the Midtown Phillips neighborhood, in Minneapolis, Minnesota, United States. It is the second-largest building in Minnesota in terms of leasable space, after the Mall of America. It was built in 1928 as a retail and mail-order catalog facility for Sears, which occupied it until 1994. It lay vacant until 2005, when it was transformed into multipurpose commercial space.\n\nThe building is listed on the National Register of Historic Places as the Sears, Roebuck and Company Mail-Order Warehouse and Retail Store.\n\nThe first phase of the building, along Elliot Avenue and Lake Street, was built in 1928. It was expanded in 1929, 1964, and 1979, resulting in 1.2 million square feet (110,000 m²) of space. A central tower along Elliot Avenue rises 16 floors to 211 feet (64 m).\nAfter Sears closed the site in 1994, it laid vacant as development proposals came and went. The city of Minneapolis acquired the site in 2001 and sold the 1979 expansion portion in 2002 to be used by the neighboring Abbott Northwestern Hospital as a parking ramp. Two years later, Ryan Companies was given exclusive development rights to the site. The resulting plan divided the structure into a mixed-use site with about 300 residential units, plus office and retail space. In 2004, Allina Health (which owns Abbott Northwestern among other area hospitals) announced plans to move their corporate headquarters to the building, taking up most of the allotted office space. Much of the residential space is known as the Chicago Lofts located on floors 9-16 and Midtown Exchange Apartments located on floors 2-8. The building also includes the Midtown Global Market, which is home to a variety of small independently owned restaurants, cafes, and specialty grocers, and hosts community programs including music, dance, and children's activities. A prototype Sheraton Hotel was built in the former Sears parking lot. The building and hotel have direct access to the Midtown Greenway.\n\nMidtown Exchange has a sister building called the Landmark Center in Boston, Massachusetts. Both were built in the 1920s and their designs are nearly identical. Both are former Sears warehouses which have since been renovated into commercial space. \n\nOther adaptively reused Sears warehouses include those in Atlanta (1925), Chicago (1906), Dallas (1910), North Kansas City (1913), and Seattle (1912). Similar sites under construction include the 1927 Sears Mail Order Building in Los Angeles and the Crosstown Concourse (1927) in Memphis.\n\nSimilar Sears warehouses existed in Philadelphia (1919) and Kansas City (1925) but were demolished in 1994 and 1997.\n\n\n"}
{"id": "23080327", "url": "https://en.wikipedia.org/wiki?curid=23080327", "title": "Murray's system of needs", "text": "Murray's system of needs\n\nIn 1938, Henry Murray developed a system of needs as part of his theory of personality, which he called 'personology.' He argued that everyone had a set of universal basic needs, with individual differences on these needs leading to the uniqueness of personality through varying dispositional tendencies for each need; in other words, specific needs are more important to some than to others. In his theory, Murray argues that needs and presses (another component of the theory) acted together to create an internal state of disequilibrium; the individual is then driven to engage in some sort of behavior to reduce the tension. Murray believed that the study of personality should look at the entire person over the course of their lifespan – that people needed to be analysed in terms of complex interactions and whole systems rather than individual parts – and an individual's behaviors, needs and their levels, etc. are all part of that understanding. Murray also argued that there was a biological (specifically neurological) basis for personality and behavior.\n\nMurray defines a need as a drive that has the potential to prompt a behavior from the person. For example, the need for affiliation may drive a person to join social organization. Needs are often influenced by environmental stimulus or 'presses,' another component of Murray's theory.\n\nIndividual differences in levels of needs lead to the uniqueness of a person's personality; in other words, specific needs may be more important to some than to others. According to Murray, human needs are psychogenic in origin, function on an unconscious level, and can play a major role in defining personality. Frustration of these psychogenic needs plays a central role in the origin of psychological pain. He also believed that these needs could be measured by projective tests, specifically one he had developed, known as the thematic apperception test (TAT). Unlike Maslow's hierarchy of needs, Murray's needs are not based on a hierarchy; individuals may be high in one and low in the other, and multiple needs may be affected by a single action.\n\nMurray differentiated each need as unique, but recognized commonalities among them, codified at least partially in his categorization system. Behaviors may meet more than one need: for instance, performing a difficult task for your fraternity may meet the needs of both achievement and affiliation. While each need is important in and of itself, he also believed that needs can support or conflict with one another, and can be interrelated. He coined the term 'subsidation of needs' to describe when two or more needs are combined in order to satisfy a more powerful need, and the term 'fusion of needs' to describe when a single action satisfies more than one need. For example, the need for dominance may conflict the need with affiliation when overly controlling behavior drives away family, romantic partners, and friends. A need may be a purely internal state, but more often it is evoked by a press.\n\nMurray argued environmental factors play a role in how psychogenic needs are displayed in behavior. He used the term 'presses' to describe external influences on motivation that may influence an individual's level of a need as well as their subsequent behavior. The 'press' of an object is what it can do for or to the subject.\n\nAny stimulus with the potential to affect the individual in a positive or negative way is referred to as 'pressive,' and everything else is referred to as inert. 'Pressive Perception' is how the subject interprets a press as either a positive or negative stimulus. 'Pressive Apperception' refers to the subjects anticipation that the stimulus will be perceived as either positive or negative. Murray notes that both Pressive Perception and Apperception are largely unconscious. Presses may have positive or negative effects, may be mobile (affecting the subject if they do nothing) or immobile (affecting the subject if they take an action), and may be an alpha press (real effects) or a beta press (merely perceived).\n\nMurray divides needs into several binary categories; Manifest (Overt) or Latent (Covert), Conscious or Unconscious, and Primary (viscerogenic) and Secondary (psychogenic) needs. Manifest (sometimes called overt) needs are those that are allowed to be directly expressed, while latent (sometimes called covert) needs are not outwardly acted on. Conscious needs as those that a subject can self-report, while Unconscious needs are all others. This is distinct from manifest vs latent in that a person may directly express a need they are unaware of, or not express a need they are aware of. The categorization most commonly referred to is the division between Primary (viscerogenic) and Secondary (psychogenic) needs.\n\nPrimary (Viscerogenic) needs are defined by Murray as needs involving some biological process and arise in response to certain stimuli or events that drive the body towards a certain outcome ('positive' or 'negative').For example, dehydration would trigger a \"need for water,\" which in turn drives a person to seek out and intake water. The first six primary needs; Air, Water, Food, Sentience, Sex, and Lactation, are considered 'positive' needs, as they drive a person towards a certain object or action. The remaining seven; Expiration, Urination, Defecation, and the four avoidance needs (see 'Retraction' below), are considered to be 'negative' needs as they drive a person away from an object (or in some cases towards the expulsion of an object). \nSecondary (Psychogenic) needs emerge from or are influenced by primary needs. Murray identified 17 secondary needs, each belonging to one of eight need domains: Ambition, Materialism, Status, Power, Sado-Masochism, Social-Conformance, Affection, and Information. Needs in each domain have similar themes underpinning them; for instance, the Ambition domain contains all those needs which relate to achievement and recognition.\n\nMurray's system of needs has influenced the creation of personality testing, including both objective and subjective measures. A personality test is a questionnaire or other standardized instrument designed to reveal aspects of an individual's character or psychological makeup. Murray's system of needs directly influenced the development of a variety of personality measures, including the \"Personality Research Form\" and the \"Jackson Personality Inventory\".\n\nHenry Murray, along with Christiana Morgan, developed the thematic apperception test (TAT) as a tool to assess personality. The TAT is based on the assumption that human unconscious needs are directed towards an external stimulus. Murray and Morgan created the TAT to evaluate \"press\" and \"need,\" which Murray emphasized in his theory of personality. The TAT is administered by an assessor, who chooses a subset of cards (generally concerning a particular theme, or those that they feel best suit the subject) out of the 32 available; Murray recommended selecting 20. Each card features various ambiguous scenes which relate to interpersonal situations. The test-taker is asked to give a detailed explanation of what they see. For example, an explanation may include a narrative of what is happening and what may unfold, and what the characters in the scene are feeling or thinking. From this narrative, the assessor uses Murray's theoretical themes to infer personality characteristics.\n\nMurray's theory of personality was the basis for several areas of further psychological research. Three of the needs he identified–the need for power, the need for affiliation, and the need for achievement–were later the subject of substantial study and considered especially significant; used to develop theories such as Maslow's hierarchy of needs, David McClelland's 'Achievement Motivation Theory', aspects of Richard Boyatzis' competency-based models of management effectiveness, and more.\n\nMurray's concept of the 'press' and his emphasis on the importance of environmental events (and their subjective interpretation) were also highly significant to later psychological research. Behavioral psychology-pioneered by John B. Watson and B. F. Skinner-focused on environmental events, while cognitive psychology included a focus on subjective interpretation of events, based on another one of Murray's ideas (his categorization of presses as either Alpha or Beta).\n\nAlthough Murray's theory has had a substantial influence on personality testing and research, some critics say that his system of needs is too broad and rather subjective. One criticism of this hierarchy is that it lacks the objective criterion for needs. It can also be said that some of the needs can conflict with each other like achievement and nurturance, which deal with opposing ideas of having to obstacles with achievement being active and nurturance being passive. When evaluating the TAT, critics have claimed that the test has a low test-retest reliability and validity. This could possibly be due to contrasting instructions from the experimenters.\n\n\n\n"}
{"id": "54344713", "url": "https://en.wikipedia.org/wiki?curid=54344713", "title": "National Philanthropy Day", "text": "National Philanthropy Day\n\nNational Philanthropy Day (November 15) is an observance designated by the Association of Fundraising Professionals (AFP). It is a day to celebrate charitable activities, in the form of donated financial, in-kind and volunteering support. It is celebrated with blog postings by AFP highlighting outstanding charitable activities, as well as luncheons and awards throughout the USA by different AFP chapters. \n\nThe association has been celebrating National Philanthropy Day since 1986, when then US President Ronald Reagan signed a proclamation recognizing November 15 as National Philanthropy Day in the US. In 2013, the Canadian government signed into law a proclamation celebrating November 15 as National Philanthropy Day annually. \n\nNational Philanthropy Day is registered with the USPTO and the U.S. Department of Commerce. The official National Philanthropy Day song, \"Now More Than Ever,\" was written by Marvin Hamlisch.\n\nThe day has been celebrated by AFP chapters across the US, including San Diego, California, Toledo, Ohio , Chattanooga, Tennessee, Detroit, Michigan.\n\n"}
{"id": "32821130", "url": "https://en.wikipedia.org/wiki?curid=32821130", "title": "Nonviolence: The History of a Dangerous Idea", "text": "Nonviolence: The History of a Dangerous Idea\n\nNonviolence: The History of a Dangerous Idea, first published as Nonviolence: Twenty-Five Lessons from the History of a Dangerous Idea, is a book by Mark Kurlansky. It follows the history of nonviolence and nonviolent activism, focusing on religious and political ideals from early history to the present.\n\nKurlansky summarizes the Twenty-Five Lessons as follows: \n\nThis book was the 2007 non-fiction winner of the Dayton Literary Peace Prize.\n\n"}
{"id": "6084375", "url": "https://en.wikipedia.org/wiki?curid=6084375", "title": "Nullable type", "text": "Nullable type\n\nIn programming, nullable types are a feature of the type system of some programming languages which allow the value to be set to the special value NULL instead of the usual possible values of the data type. In statically-typed languages, a nullable type is an option type (in functional programming terms), while in dynamically-typed languages (where values have types, but variables do not), equivalent behavior is provided by having a single null value.\n\nPrimitive types such as integers and booleans cannot generally be null, but the corresponding nullable types (nullable integer and nullable boolean, respectively) can also assume the NULL value. NULL is frequently used to represent a missing value or invalid value, such as from a function that failed to return or a missing field in a database, as in NULL in SQL.\n\nAn integer variable may represent integers, but 0 (zero) is a special case because 0 in many programming languages can mean \"false\". Also this doesn't give us any notion of saying that the variable is empty, a need for which occurs in many circumstances. This need can be achieved with a nullable type. In programming languages like C# 2.0, a nullable integer, for example, can be declared by a question mark (int? x). In programming languages like C# 1.0, nullable types can be defined by an external library as new types (e.g. NullableInteger, NullableBoolean).\n\nA boolean variable makes the effect more clear. Its values can be either \"true\" or \"false\", while a nullable boolean may also contain a representation for \"undecided\". However, the interpretation or treatment of a logical operation involving such a variable depends on the language.\n\nIn contrast, object pointers can be set to NULL by default in most common languages, meaning that the pointer or reference points to nowhere, that no object is assigned (the variable does not point to any object).\nNullable references were invented by C. A. R. Hoare in 1965 as part of the Algol W language. Hoare later described their invention as a \"billion-dollar mistake\". This is because object pointers that can be NULL require the user to check the pointer before using it and require specific code to handle the case when the object pointer is NULL. \n\nJava has classes that correspond to scalar values, such as Integer, Boolean and Float. Combined with autoboxing (automatic usage-driven conversion between object and value), this effectively allows nullable variables for scalar values.\n\nNullable type implementations usually adhere to the null object pattern.\n\nThere is a more general and formal concept that extend the nullable type concept, it comes from option types, which enforce explicit handling of the exceptional case.\nOption type implementations usually adhere to the Special Case pattern.\n\nThe following programming languages support nullable types.\n\nStatically typed languages with native null support include:\n\nStatically typed languages with library null support include:\nDynamically-typed languages with null include:\n\n"}
{"id": "5870092", "url": "https://en.wikipedia.org/wiki?curid=5870092", "title": "One-Roll Engine", "text": "One-Roll Engine\n\nThe One-Roll Engine (or O.R.E.) is a generic role-playing game system developed by Greg Stolze for the alternate history superhero roleplaying game \"Godlike.\" The system was expanded upon in the modern-day sequel, \"Wild Talents,\" as well as the heroic fantasy game \"Reign\" and the free horror game \"Nemesis.\" A simpler version was used for \"Monsters and Other Childish Things\". The One-Roll Engine is notable for its unique dice rolling system in which matched values on ten-sided dice (d10s) determine all variables of a check in a single roll. This eliminates, for example, the separate initiative, hit location and damage rolls common during combat in other systems.\n\nThe One-Roll Engine uses a dice pool of d10s equal to the character's Stat and Skill similar to that used by Storyteller system. Since the dice are always d10s, a pool is written as the number of dice followed by a \"d\"; for example, a pool of six dice would be written \"6d\".\n\nWhile most dice pool systems count the number of dice which roll above a certain number to determine success, the O.R.E. system instead depends on matching dice, such as a pair of dice showing 8 or three dice showing 2. Matching dice are called a \"set\"; the number of matching dice in a set is called the \"Width\", while the face up number on the dice is the set's \"Height\". Shorthand notation for writing results is Width x Height, so a pair of 8s would be written 2x8 and three 2s would be written 3x2.\n\nA roll may have more than one set; the player can usually choose which one to use. If there are no matches, then the player may select a single die to act as a set with a Width of 1. In general, a set's Width determines the speed of an action, while the Height determines how successful the action was. In combat, Width and Height also determine the damage and hit location.\n\nPowers, such as those possessed by superhuman Talents in \"Godlike\", are modeled with special dice. \"Hard Dice\" are considered to always have a value of 10, while \"Wiggle Dice\" have a value assigned by the player \"after\" the rest of the pool is rolled. The shorthand notation for Hard Dice is \"hd\" and Wiggle Dice is \"wd\", so a Dice pool of six regular dice, two Hard Dice and one Wiggle Die would be noted 6d+2hd+1wd.\n\nNemesis introduces \"Expert Dice\", which may be used as normal dice, or the player can select their value \"before\" the roll, but no two Expert Dice can be selected to have the same value. Expert Dice are also used as buffer against die penalties, where each Expert Die counteracts one die penalty, but then must be rolled as a normal die. NEMESIS also uses Wiggle Dice, but renames them \"Trump Dice\". The shorthand notation for Expert Dice is \"ed\" and Trump Dice is \"td\".\n\nReign also uses \"Wiggle Dice\", but uses the name \"Master Dice\" (shorthand notation \"md\"). In this setting any dice pool just could contain one special die (\"md\" or \"ed\") and can never roll more than ten dice at once. (Excess allows offsetting penalties while still resulting in an effective pool of ten.)\n\nThe simplified O.R.E. used in \"Monsters and Other Childish Things\" only uses special dice for monsters. Normal dice in a monsters' pools may be sacrificed during character creation for special qualities, one of which, \"Awesome\", grants a die that works like an Expert die in \"Nemesis\". Taking the Awesome quality a second time makes the die function like a Wiggle Die. \"Monsters\" does not use Hard Dice, but other special qualities modify rolls in specific, more limited ways. For example, \"Gnarly\" adds one to damage in combat rolls, while \"Wicked Fast\" adds one to Width for the purposes of determining speed.\n\n"}
{"id": "2573068", "url": "https://en.wikipedia.org/wiki?curid=2573068", "title": "Palingenesis", "text": "Palingenesis\n\nPalingenesis (; also palingenesia) is a concept of rebirth or re-creation, used in various contexts in philosophy, theology, politics, and biology. Its meaning stems from Greek \"palin\", meaning \"again\", and \"genesis\", meaning \"birth\".\n\nIn biology, it is another word for recapitulation—the largely discredited hypothesis which talks of the phase in the development of an organism in which its form and structure pass through the changes undergone in the evolution of the species. In political theory, it is a central component of Roger Griffin's analysis of Fascism as a fundamentally modernist ideology. In theology, the word may refer to reincarnation or to Christian spiritual rebirth symbolized by baptism.\n\nThe word \"palingenesis\" or rather \"palingenesia\" () may be traced back to the Stoics, who used the term for the continual re-creation of the universe. Similarly Philo spoke of Noah and his sons as leaders of a renovation or rebirth of the earth, Plutarch of the transmigration of souls, and Cicero of his own return from exile.\n\nIn the Gospel of Matthew Jesus is quoted in Greek (although his historical utterance would most likely have been in Aramaic) using the word \"παλιγγενεσία\" (\"palingenesia\") to describe the Last Judgment foreshadowing the event of the regeneration of a new world. Palingenesia is thus as much the result of, or reason for, the Last Judgement as it is directly the Judgement itself.\n\nIn philosophy it denotes in its broadest sense the theory (e.g. of the Pythagoreans) that the human soul does not die with the body but is born again in new incarnations. It is thus the equivalent of metempsychosis. The term has a narrower and more specific use in the system of Schopenhauer, who applied it to his doctrine that the will does not die but manifests itself afresh in new individuals. He thus repudiates the primitive metempsychosis doctrine which maintains the reincarnation of the particular soul.\n\nRobert Burton, in \"The Anatomy of Melancholy\" (1628), writes, \"The Pythagoreans defend metempsychosis and palingenesia, that souls go from one body to another.\"\n\nThe 17th century English physician-philosopher Sir Thomas Browne in his \"Religio Medici\" (1643) declared a belief in palingenesis, stating,\n\nA plant or vegetable consumed to ashes, to a contemplative and school Philosopher seems utterly destroyed, and the form to have taken his leave for ever: But to a sensible Artist the forms are not perished, but withdrawn into their incombustible part, where they lie secure from the action of that devouring element. This is made good by experience, which can from the ashes of a plant revive the plant, and from its cinders recall it into its stalk and leaves again.'\nPalingenesis is the subject of the Argentine author Jorge Luis Borges's last ever short story, \"The Rose of Paracelsus\" (1983)\n\nIn Antiquities of the Jews (11.3.9) Josephus used the term \"palingenesis\" for the national restoration of the Jews in their homeland after the Babylonian exile. The term is commonly used in Modern Greek to refer to the rebirth of the Greek nation after the Greek Revolution. British political theorist Roger Griffin has coined the term \"palingenetic ultranationalism\" as a core tenet of fascism, stressing the notion of fascism as an ideology of rebirth of a state or empire in the image of that which came before it – its ancestral political underpinnings. Examples of this are Fascist Italy and Nazi Germany. Under Benito Mussolini, Italy purported to establish an empire as the second incarnation of the Roman Empire, while Adolf Hitler's regime was seen as being the third palingenetic incarnation of the German \"Reich\" - beginning first with the Holy Roman Empire (\"First Reich\"), followed by Bismarck's German Empire (\"Second Reich\") and then Nazi Germany (\"Third Reich\").\n\nMoreover, Griffin's work on palingenesis in fascism analysed the pre-war Fin De Siecle Western society. In doing so he built on Frank Kermode's work \"The Sense of an Ending\" which sought to understand the belief in the death of society at the end of the century. As part of this death and rebirth Fascism sought to target what it perceived as degenerative elements of society, notably decadence, materialism, rationalism and enlightenment ideology. Out of this death society would regenerate by returning to a more spiritual and emotional state, with the role of the individual core. This built on the philosophy of Friedrich Nietzsche, Gustave Le Bon and Henri Bergson's work on the relationship between the mass and the individual with the individual's actions necessary to achieving a state of regeneration.\n\nChilean dictator Augusto Pinochet expressed his post-coup project in government as a national rebirth inspired in Diego Portales, a figure of the early republic:\nIn modern biology (e.g. Haeckel and Fritz Müller), \"palingenesis\" has been used for the exact reproduction of ancestral features by inheritance, as opposed to kenogenesis, in which the inherited characteristics are modified by environment.\n\nIt was also applied to the quite different process supposed by Karl Beurlen to be the mechanism for his orthogenetic theory of evolution.\n"}
{"id": "4935061", "url": "https://en.wikipedia.org/wiki?curid=4935061", "title": "Pre-existing duty rule", "text": "Pre-existing duty rule\n\nThe pre-existing duty rule is an aspect of consideration within the law of contract. Originating in England (the world's quintessential common law jurisdiction) the concept of consideration has been adopted by other jurisdictions, including the US.\n\nIn essence, this rule declares that performance of a pre-existing duty does not amount to good consideration to support a valid contract; but there are exceptions to the rule.\n\nEnglish law recognises bargains supported by consideration, not bare promises. However, only simple contracts need consideration to be enforceable; special contracts do not require consideration. \n\n\"Currie v Misa\" (1875) declares that consideration may comprise any of these positive and negative matters: \nThe leading case is \"Stilk v Myrick\" (1809), where a captain promised 8 crew the wages of 2 deserters provided the remainers completed the voyage. The shipowner refused to honour the agreement; the court deemed the 8 crew were unable to enforce the deal as they had an existing obligation to sail the ship and meet \"ordinary foreseeable emergencies\".\n\nHowever, in two cases the courts held that a claimant provides good consideration provided they act \"above and beyond\" their contractual obligation:\n\nThe rule may be affected by issues of public policy, as in:\n\"Collins v Godefroy\" (1831) , \"England v Davidson\" (1840) and \"Williams v Williams\" [1957] \n\nThe court may also be happy to enforce an agreement provided that is gives benefit:\n\nAny contracting party cannot who wished to amend the agreement must provide new consideration. This situatiation typically usually arises in any of three different ways:\n\nIf one party has performed their part of the contract, but the other party refuses to pay unless the amount owed is reduced, the full amount remains payable: \"Pinnel's Case\" (1602). Any attempt to use promissory estoppel will fail if the debtor behaves inequitably: \"D&C Builders v Rees\"[1966]\n\nOne party refuses to perform her side of the contract unless a larger sum of money is paid. For example, Christine agrees to sell Julian a set of textbooks for $300. Julian wires $300 to his friend Jake, who is charged with picking up the textbooks and delivering the $300. After the money has been wired and delivery arrangements have been made, Christine calls Julian and states that she has changed the price to $350 and will not deliver the books to Jake unless Julian promises to pay an additional $50. The rule will apply so Julian could agree to pay the extra money but then not do so when the books are delivered. (If Julian actually paid the extra money, he could sue later under \"duress\" to recover the $50.)\n\nThe party seeking payment already has a public duty to perform the act. For example, a government employee polygraph expert might ask a criminal about an unrelated crime during the administration of a polygraph. If the criminal admits to the crime and the employee then seeks a reward for identifying the perpetrator, he would not be entitled to it under the legal duty rule because he already has a public duty to find out about crimes.\n\nThe legal duty rule does not apply if the parties mutually agree to change the terms of the contract. For example, the homeowner and contractor could agree to modify their contract to include a new window for the bathroom at an additional cost of $1000. Alternatively, the parties could agree not to perform part of the contract for a $500 reduction in the price. Both modifications to the original contract would be enforceable because there was consideration for each. The legal duty rule protects one party when the other is trying to unilaterally change the terms of the agreement.\n\nThere are ways around the legal duty rule, such as mutual rescission of the existing contract with a clear indication of such rescission (literally tearing up the old contract). Also, in some states, parties may renegotiate contracts to include additional benefits if, for example, the party performs unexpected or additional duties, the parties assent in good faith, or a new contract is agreed.\n\nIf contractual parties owe each other existing contractual obligations but a third party offers a promise contingent upon performance of the contract, that promise has sufficient consideration.\n\nConsideration will be found if a party promises to perform where there are unforeseen and/or unforeseeable circumstances sufficient to discharge the party from the obligation if any new or different consideration is promised (earlier payment or payment in stock), the promise is to ratify a voidable obligation (such as to go through despite fraud), the preexisting duty is owed to another person, and there is an honest dispute as to the duty.\n\nAlso, under the Uniform Commercial Code, modifications may be made free of the Common Law legal duty rule even without consideration provided that the modification is made in good faith. See UCC § 2-209. However, the Statute of Frauds must be complied with. Thus, a written contract is necessary if the contract as modified comes within the scope of that statute. For purposes of the UCC, a contract must be in writing if it is for the sale of goods where the price exceeds $500. UCC § 2-201.\n\nThe pre-existing duty rule has been abrogated under the \"Restatement, Second of Contracts § 89\", which does not require independent consideration if the parties mutually and voluntarily agree to the modification (see \"Angel v. Murray\" for an early application of the Restatement). The restatement, however, will not always be followed, as evidenced by the decision in \"Labriola v. Pollard Group, Inc.\".\n\nThe pre-existing duty rule plays a part in salvage which is a \"voluntary successful service to save maritime property in danger at sea\". The service must be \"voluntary\", which in this context means that the salvor must not have an existing duty towards the ship. Generally, a ship's crew cannot claim salvage unless (i) they have been ordered to abandon ship (so that their contract of employment has ended), \"The San Demetrio\" (1941); or (ii) the crew act over and beyond their normal duty to look after the safety of the ship, \"The Beaver\" (1800).\n\n"}
{"id": "22944030", "url": "https://en.wikipedia.org/wiki?curid=22944030", "title": "Prima facie right", "text": "Prima facie right\n\nA prima facie right is a right that can be outweighed by other considerations. It stands in contrast with absolute rights, which cannot be outweighed by anything.\n\n\n"}
{"id": "102191", "url": "https://en.wikipedia.org/wiki?curid=102191", "title": "Right of return", "text": "Right of return\n\nThe right of return is a principle in international law which guarantees everyone's right of voluntary return to or re-enter their country of origin or of citizenship. A right of return based on nationality, citizenship or ancestry may be enshrined in a country's constitution or law, and some countries deny a right of return in particular cases or in general.\n\nThe right is formulated in several modern treaties and conventions, most notably in the 1948 Universal Declaration of Human Rights, the 1966 International Covenant on Civil and Political Rights and the 1948 Fourth Geneva Convention. The Geneva Conventions, it has been argued, have passed into customary international law and that the right of return is binding on non-signatories to the conventions.\n\nThe right of return is often invoked by representatives of refugee groups to assert that they have a right to return to the country from which they were displaced.\n\nThe right to leave one's country and return to it are regarded as human rights and based in natural law. In antiquity, the right to return was connected by philosophers with the right of travel.\n\nThere are various recorded cases in ancient history where people who had been deported or uprooted from their city or homeland were allowed (or encouraged) to return, typically when the balance of military and political forces which caused their exile had changed. However, in these cases the exiled populations were granted the \"option\" to return, it was never recognized that they had an inherent \"right\" to return.\n\nA well-known example is the return to Zion, which King Cyrus the Great granted the Jews expelled from Judah to the Babylonian Exile to return to their ancestral homeland and rebuild Jerusalem. Recorded in the Hebrew Bible (Book of Ezra and Book of Nehemiah) this case is often cited as a precedent by modern Zionists and also inspired other groups seeking to pursue their own return.\n\nDuring the Peloponnesian War, Athens expelled and scattered the inhabitants of Melos, Aegina and other cities (some of them being sold into slavery). Following the victory of Sparta, the Spartan general Lysander in 405 BC made a concerted effort to gather these exiles and restore them to their original cities.\n\nThe first codified law guaranteeing a Right of Return can be found in the Magna Carta:\n\nAnother early example of national law recognizing the Right of Return was the French constitution of 1791, enacted on 15 December 1790: \n\nThe constitution put an end to the centuries-long persecution and discrimination of Huguenots (French Protestants).\n\nConcurrently with making all Protestants resident in France into full-fledged citizens, the law enacted on December 15, 1790 stated that: \n\nThe Revocation of the Edict of Nantes and expulsion of the Huguenots had taken place more than a century earlier, and there were extensive Huguenot diasporas in many countries, where they often intermarried with the population of the host country (see Edict of Potsdam). Therefore, the law potentially conferred French citizenship on numerous Britons, Germans, South Africans and others – though only a fraction actually took advantage of it. This option for Huguenot descendants to gain French citizenship remained open until 1945, when it was abolished - since after the Occupation of France, the French were unwilling to let Germans of Huguenot origin take advantage of it.\n\nIn the aftermath of the Second Schleswig War of 1864, the previously Danish-ruled territory of Schleswig became part of Imperial Germany. A significant number of inhabitants, known as \"optants\", chose to retain their Danish citizenship and refused to take up a German one. Consequently, they were expelled from the area by Prussian authorities. Half a century later, following the German defeat in the First World War, a plebiscite was held in 1920 to determine the future of the area. The Danish government asked the Allied Powers to let these expelled ethnic Danes and their descendants return to Schleswig and take part in the plebiscite. This was granted, though many of the optants had in the meantime emigrated to the United States, and most of these did not actually come back.\n\nThe right of return principle has been codified in a number of international instruments, including:\n\nHague Regulations (HR), article 20:\n\nIt has been argued that if the HR require the repatriation of prisoners, then it is \"obvious\" that civilians displaced during conflict must also be allowed to repatriate.\n\nUniversal Declaration of Human Rights (UDHR), article 13:\n\nInternational Covenant on Civil and Political Rights (ICCPR) article 12(4):\n\nFourth Geneva Convention, article 49:\n\nConvention on the Elimination of All Forms of Racial Discrimination, article 5d(ii):\n\nSome controversy exists among scholars on how these articles should be interpreted.\n\nThere is some disagreement as to what \"his own\" and \"his country\" means in the ICCPR and UDHR. According to the United Nations Human Rights Committee's authoritative interpretation:\n\nand\n\nThe landmark Nottebohm case of 1955 is often cited as staking out more criteria as to what \"ones country\" should be. Among them were \"a close and enduring connection\", \"tradition\",\n\"establishment\", \"interests\" and \"family ties\".\n\nRefugees who are resettled into third countries lose refugee status and acquire a new nationality. They therefore lose the right to return to their country of origin.\n\nSome disagreement exists on whether the right of return is applicable to situations in which whole ethnic groups have been displaced or not. Ruth Lapidoth from the Jerusalem Center for Public Affairs has argued, by citing Stig Jägerskiöld from his 1966 commentary of ICCPR, that the right was not intended to protect groups of displaced people:\n\n... [it] is intended to apply to individuals asserting an individual right. There was no intention here to address the claims of masses of people who have been displaced as a by-product of war or by political transfers of territory or population, such as the relocation of ethnic Germans from Eastern Europe during and after the Second World War, the flight of the Palestinians from what became Israel, or the movement of Jews from the Arab countries.\n\nHurst Hannum has made a similar argument:\n\nManfred Nowak has argued the opposite position, that the right of return applies \"even if masses of people are claiming this right\" Bracka has argued similarly:\n\nFew cases have dealt with the right of return principle. In 1996, the European Court of Human Rights (ECHR) ruled in a landmark case known as \"Loizidou v Turkey\". Mrs Titina Loizidou was a Greek-Cypriot refugee displaced from Northern Cyprus and prevented from returning by Turkey. The court ruled that Turkey had violated Mrs Loizidou's human rights, that she should be allowed to return to her home and that Turkey should pay damages to her.\n\nIn a similar case, petitioners for the Chagossians asked the ECHR in 2005 to rule about their removal from Diego Garcia by Great Britain in the 1960s. The court ruled in 2012 that their case was inadmissible and that by accepting compensation, the islanders had forfeited their claim:\n\nCircassians are an indigenous ethnic group originating from the northwestern Caucasus. Throughout the 19th century, the Russian Empire adopted a policy to eradicate Circassians from their ancestral homelands, pushing most surviving Circassians into the diaspora. Many Circassians have expressed an interest in returning to Circassia, particularly Circassians fleeing the conflict in Syria.\n\nDuring Abkhazia's war of secession in 1992-1993 and the second Abkhazia war in 1998, 200,000-250,000 Georgian civilians became internally displaced persons (IDPs) and refugees. Abkhazia, while formally agreeing to repatriation, has hindered the return of refugees both officially and unofficially for more than fifteen years.\n\nDuring the Turkish invasion of Cyprus, 40% of the Greek-Cypriot population as well as over half of the Turk-Cypriot population of the island were displaced. The island was divided along ethnic lines and most of the Greek-Cypriot displaced persons were not allowed to return to their homes in the northern Turk-Cypriot side and vice versa.\n\nPlans for a solution of the conflict has centered around bilateral agreements of population exchange, such as the Third Vienna Agreement reached in 1975 or the proposed Annan Plan of 2004. In these plans, the right of return was to be severely limited with respect to Greek-Cypriot internally displaced persons/refugees to districts such as Kyrenia, Morphou, Famagusta, and parts of Nicosia, despite judgements of the European Court of Human Rights in cases such as Loizidou v. Turkey, and numerous UN resolutions recognizing the right of return (such as SC 361 and GA 3212). Two referendums on the Annan Plan were held in April 2004, separately along ethnic lines. The Annan Plan was overwhelmingly rejected in the Greek-Cypriot referendum.\n\nThe right of return continues to remain a stumbling block to the settlement of the Cyprus problem.\n\nThe Chagossians, an ethnic group residing on the island of Diego Garcia in the Indian Ocean, were expelled to Mauritius in the 1960s, in connection with the erection of an American strategic military installation on the island. Ever since, the Chagossians have been conducting a persistent political and legal struggle to return to Diego Garcia. As of 2007, their right to return was recognised by several British courts but the UK government failed to actually implement it (see Chagossians, Depopulation of Diego Garcia, Order in Council#United Kingdom).\n\nPalestinian refugees argue that international law guarantees them a right to return to their former homes in Mandatory Palestine and a right to property they left behind in what is now Israel. Israel's Defense Minister, Avigdor Lieberman, was reported as saying in June 2017 that his government will not allow any Palestinians to settle in the lands claimed by them and currently part of Israeli territory.\n\nArticle 14 of the Constitution of the Republic of Armenia (1995) provides that \"[i]ndividuals of Armenian origin shall acquire citizenship of the Republic of Armenia through a simplified procedure.\" This provision is consistent with the \"Declaration on Independence of Armenia\", issued by the Supreme Soviet of the Republic of Armenia in 1989, which declared at article 4 that \"Armenians living abroad are entitled to the citizenship of the Republic of Armenia\".\n\nArticle 36 (3) of the Constitution of Estonia states that \"Every Estonian is entitled to settle in Estonia.\"\n\nIn October 1985, French President François Mitterrand issued a public apology to the descendants of Huguenots around the world. A right of return to France was ensured for Huguenot descendants all around the world.\n\nGerman law allows (1) persons descending from German nationals of any ethnicity or (2) persons of ethnic German descent and living in countries of the former Warsaw Pact (as well as Yugoslavia) the right to \"return\" to Germany and (\"re\")claim German citizenship (Aussiedler/Spätaussiedler \"late emigrants\"). After legislative changes in late 1992 this right is de facto restricted to ethnic Germans from the former Soviet Union. As with many legal implementations of the Right of Return, the \"return\" to Germany of individuals who may never have lived in Germany based on their ethnic origin or their descent from German nationals has been controversial. The law is codified in paragraph 1 of Article 116 of the Basic Law for the Federal Republic of Germany, which provides access to German citizenship for anyone \"who has been admitted to the territory of the German Reich within the boundaries of December 31, 1937 as a refugee or expellee of German ethnic origin or as the spouse or descendant of such person\". Those territories had a Polish minority, which also had German citizenship and after World War II lived in Poland. These Polish people are also \"Aussiedler\" or \"Spätaussiedler\" and came especially in the 1980s to Germany, see Emigration from Poland to Germany after World War II. For example Lukas Podolski and Eugen Polanski became German citizens by this law. Paragraph 2 of Article 116 also provides that \"Former German citizens who between 30 January 1933 and 8 May 1945 were deprived of their citizenship on political, racial or religious grounds, and their descendants, shall on application have their citizenship restored\".\n\nThe historic context for Article 116 was the eviction, following World War II, of an estimated 9 million foreign ethnic Germans from other countries in Central and Eastern Europe. Another 9 million German nationalsformer eastern German territories, over which Joseph Stalin and eastern neighbour states extended military hegemony in 1945, were expelled as well. These expellees and refugees, known as \"Heimatvertriebene\", were given refugee status and documents, and - as to foreign ethnic Germans - also the German citizenship (in 1949), and resettled in Germany. The Discussion of possible compensation continues; this, however, has been countered by possible claims for war compensation from Germany's eastern neighbours, pertaining to both Germany's unconditional surrender and the series of population transfers carried out under the instruments of Potsdam.\n\nGhana grants an indefinite right to stay in Ghana to members of the African diaspora with African ancestry.\n\nVarious phenomena throughout Greek history (the extensive colonization by classical Greek city states, the vast expansion of Greek culture in Hellenistic times, the large dominions at times held by the Greek-speaking Byzantine Empire, and the energetic trading activity by Greeks under the Ottomans) all tended to create Greek communities far beyond the boundaries of modern Greece.\n\nRecognizing this situation, Greece grants citizenship to broad categories of people of ethnic Greek ancestry who are members of the Greek diaspora, including individuals and families whose ancestors have been resident in diaspora communities outside the modern state of Greece for centuries or millennia.\n\n\"Foreign persons of Greek origin\", who neither live in Greece nor hold Greek citizenship nor were necessarily born there, may become Greek citizens by enlisting in Greece's military forces, under article 4 of the \"Code of Greek Citizenship\", as amended by the \"Acquisition of Greek Nationality by Aliens of Greek Origin Law\" (Law 2130/1993). Anyone wishing to do so must present a number of documents, including \"[a]vailable written records ... proving the Greek origin of the interested person and his ancestors.\"\n\nAlbania has demanded since the 1940s that Greece grant a Right of Return to the Muslim Cham Albanians, who were expelled from the Greek region of Epirus between 1944 and 1945, at the end of World War II – a demand firmly rejected by the Greeks (see Cham issue).\n\nIn 2010, Hungary passed a law granting citizenship and the right of return to descendants of Hungarians living mostly on the former territory of the Hungarian Kingdom and now residing in Hungary's neighbouring countries. Slovakia, which has 500,000 ethnic Magyar citizens (10% of its population) objected vociferously.\n\nThe Law of Return is legislation enacted by Israel in 1950, that gives all Jews, persons of Jewish ancestry up to at least one Jewish grandparent, and spouses of Jews the right to immigrate to and settle in Israel and obtain citizenship, and obliges the Israeli government to facilitate their immigration. Originally, the law applied to Jews only, until a 1970 amendment stated that the rights \"are also vested in a child and a grandchild of a Jew, the spouse of a Jew, the spouse of a child of a Jew and the spouse of a grandchild of a Jew\". This resulted in several hundreds of thousands of persons fitting the above criteria immigrating to Israel (mainly from the former Soviet Union) but not being recognized as Jews by the Israeli religious authorities, which on the basis of halakha recognize only the child of a Jewish mother as being Jewish, or a proselyte to Judaism. Moreover, some of these immigrants, though having a Jewish grandparent, are known to be practicing Christians. People who would be otherwise eligible for this law can be excluded if they can reasonably be considered to constitute a danger to the welfare of the state, have a criminal past, or are wanted fugitives in their countries with the exception of persecution victims. Jews who converted to another religion can also be denied the right of return. Since its inception in 1948, over three million Jews have immigrated to Israel.\n\nFrom the Constitution of Lithuania, Article 32(4): \"Every Lithuanian person may settle in Lithuania.\"\n\nFrom the Constitution of Poland, Article 52(5): \"Anyone whose Polish origin has been confirmed in accordance with statute may settle permanently in Poland.\"\n\nOn April 12, 2013, the Portuguese parliament unanimously approved a measure that allows the descendants of Jews expelled from Portugal in the 16th century to become Portuguese citizens.\n\nSephardi Jews were expelled from Spain in 1492. Despite the requirement by general rule for obtaining Spanish nationality after five years of residence in Spain, by Royal Decree on the 20th of December 1924, Sephardi Jews can obtain Spanish nationality with two years of residence in Spain. From 1924 until 2015 Sephardi Jews living abroad could also ask the Spanish Government for a conferment of Spanish nationality, but the Government enjoyed full discretion as to the decision whether to grant Spanish nationality. On June 24, 2015, the Spanish Parliament approved the 12/2015 Act, the Law Granting the Nationality to Sephardi Jews, that grants the Spanish nationality automatically to Sephardi Jews living abroad, provided they can prove that they are descendants of the Sephardi Jews expelled in 1492.\n\nIn 2007, the Spanish Parliament approved the 57/2007 Act, the Law of Historical Memory. The 57/2007 Act provides for the descendants of Spaniards living abroad that left Spain because of political persecution during the Civil War and Franco's dictatorship — that is the period between 1936-1975 — to obtain Spanish nationality.\n\nFinally, following the British conquest of Gibraltar in August 1704, the original Spanish population of Gibraltar was expelled and established in the surrounding area called \"Campo de Gibraltar\". The expelled population established the Gibraltarian institutions, the census and the archives in the City of San Roque, which officially is still \"Gibraltar in exile\". Despite claims to the right of return by the association of descendants of expelled Gibraltarians, neither the Spanish Government nor the British Government have recognised any right of return for the expelled Gibraltarians.\n\n\n"}
{"id": "24462958", "url": "https://en.wikipedia.org/wiki?curid=24462958", "title": "Risk", "text": "Risk\n\nRisk is the possibility of losing something of value. Values (such as physical health, social status, emotional well-being, or financial wealth) can be gained or lost when taking risk resulting from a given action or inaction, foreseen or unforeseen (planned or not planned). Risk can also be defined as the intentional interaction with uncertainty. Uncertainty is a potential, unpredictable, and uncontrollable outcome; risk is a consequence of action taken in spite of uncertainty.\n\nRisk perception is the subjective judgment people make about the severity and probability of a risk, and may vary person to person. Any human endeavour carries some risk, but some are much riskier than others.\n\nThe Oxford English Dictionary cites the earliest use of the word in English (in the spelling of \"risque\" from its from French original, 'risque' ) as of 1621, and the spelling as \"risk\" from 1655. It defines \"risk\" as:\n\n(Exposure to) the possibility of loss, injury, or other adverse or unwelcome circumstance; a chance or situation involving such a possibility.\n\n\nThe International Organization for Standardization publication ISO 31000 (2009) / ISO Guide 73:2002 definition of risk is the 'effect of uncertainty on objectives'. In this definition, uncertainties include events (which may or may not happen) and uncertainties caused by ambiguity or a lack of information. It also includes both negative and positive impacts on objectives. Many definitions of risk exist in common usage, however this definition was developed by an international committee representing over 30 countries and is based on the input of several thousand subject matter experts.\n\nVery different approaches to risk management are taken in different fields, e.g. \"Risk is the unwanted subset of a set of uncertain outcomes\" (Cornelius Keating).\n\nRisk is ubiquitous in all areas of life and risk management is something that we all must do, whether we are managing a major organisation or simply crossing the road. When describing risk however, it is convenient to consider that risk practitioners operate in some specific practice areas.\n\nEconomic risks can be manifested in lower incomes or higher expenditures than expected. The causes can be many, for instance, the hike in the price for raw materials, the lapsing of deadlines for construction of a new operating facility, disruptions in a production process, emergence of a serious competitor on the market, the loss of key personnel, the change of a political regime, or natural disasters.\n\nRisks in personal health may be reduced by primary prevention actions that decrease early causes of illness or by secondary prevention actions after a person has clearly measured clinical signs or symptoms recognised as risk factors. Tertiary prevention reduces the negative impact of an already established disease by restoring function and reducing disease-related complications. Ethical medical practice requires careful discussion of risk factors with individual patients to obtain informed consent for secondary and tertiary prevention efforts, whereas public health efforts in primary prevention require education of the entire population at risk. In each case, careful communication about risk factors, likely outcomes and certainty must distinguish between causal events that must be decreased and associated events that may be merely consequences rather than causes.\n\nIn epidemiology, the lifetime risk of an effect is the \"cumulative incidence\", also called \"incidence proportion\" over an entire lifetime.\n\nIn terms of occupational health & safety management, the term 'risk' may be defined as the most likely consequence of a hazard, combined with the likelihood or probability of it occurring.\n\nHealth, safety, and environment (HSE) are separate practice areas; however, they are often linked. The reason for this is typically to do with organizational management structures; however, there are strong links among these disciplines. One of the strongest links between these is that a single risk event may have impacts in all three areas, albeit over differing timescales. For example, the uncontrolled release of radiation or a toxic chemical may have immediate short-term safety consequences, more protracted health impacts, and much longer-term environmental impacts. Events such as Chernobyl, for example, caused immediate deaths, and in the longer term, deaths from cancers, and left a lasting environmental impact leading to birth defects, impacts on wildlife, etc.\n\nOver time, a form of risk analysis called environmental risk analysis has developed. Environmental risk analysis is a field of study that attempts to understand events and activities that bring risk to human health or the environment.\n\nHuman health and environmental risk is the likelihood of an adverse outcome (See adverse outcome pathway). As such, risk is a function of hazard and exposure. Hazard is the intrinsic danger or harm that is posed, e.g. the toxicity of a chemical compound. Exposure is the likely contact with that hazard. Therefore, the risk of even a very hazardous substance approaches zero as the exposure nears zero, given a person's (or other organism's) biological makeup, activities and location (See exposome). Another example of health risks are when certain behaviours, such as risky sexual behaviours, increase the likelihood of contracting HIV.\n\nInformation technology risk, or IT risk, IT-related risk, is a risk related to information technology. This relatively new term was developed as a result of an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.\n\nThe increasing dependencies of modern society on information and computers networks (both in private and public sectors, including military) has led to new terms like IT risk and Cyberwarfare.\nInformation security means protecting information and information systems from unauthorised access, use, disclosure, disruption, modification, perusal, inspection, recording or destruction. Information security grew out of practices and procedures of computer security.\nInformation security has grown to information assurance (IA) i.e. is the practice of managing risks related to the use, processing, storage, and transmission of information or data and the systems and processes used for those purposes. \nWhile focused dominantly on information in digital form, the full range of IA encompasses not only digital but also analogue or physical form. \nInformation assurance is interdisciplinary and draws from multiple fields, including accounting, fraud examination, forensic science, management science, systems engineering, security engineering, and criminology, in addition to computer science.\n\nSo, \"IT risk\" is narrowly focused on computer security, while \"information security\" extends to risks related to other forms of information (paper, microfilm). \"Information assurance\" risks include the ones related to the consistency of the business information stored in IT systems and the information stored by other means and the relevant business consequences.\n\nInsurance is a risk treatment option which involves risk sharing. It can be considered as a form of contingent capital and is akin to purchasing an option in which the buyer pays a small premium to be protected from a potential large loss.\n\nInsurance risk is often taken by insurance companies, who then bear a pool of risks including market risk, credit risk, operational risk, interest rate risk, mortality risk, longevity risks, etc.\n\nMeans of assessing risk vary widely between professions. Indeed, they may define these professions; for example, a doctor manages medical risk, while a civil engineer manages risk of structural failure. A professional code of ethics is usually focused on risk assessment and mitigation (by the professional on behalf of client, public, society or life in general).\n\nIn the workplace, incidental and inherent risks exist. Incidental risks are those that occur naturally in the business but are not part of the core of the business. Inherent risks have a negative effect on the operating profit of the business.\n\nThe experience of many people who rely on human services for support is that 'risk' is often used as a reason to prevent them from gaining further independence or fully accessing the community, and that these services are often unnecessarily risk averse. \"People's autonomy used to be compromised by institution walls, now it's too often our risk management practices\", according to John O'Brien. Michael Fischer and Ewan Ferlie (2013) find that contradictions between formal risk controls and the role of subjective factors in human services (such as the role of emotions and ideology) can undermine service values, so producing tensions and even intractable and 'heated' conflict.\n\nA high reliability organisation (HRO) is an organisation that has succeeded in avoiding catastrophes in an environment where normal accidents can be expected due to risk factors and complexity. Most studies of HROs involve areas such as nuclear aircraft carriers, air traffic control, aerospace and nuclear power stations. Organizations such as these share in common the ability to consistently operate safely in complex, interconnected environments where a single failure in one component could lead to catastrophe. Essentially, they are organisations which appear to operate 'in spite' of an enormous range of risks.\n\nSome of these industries manage risk in a highly quantified and enumerated way. These include the nuclear power and aircraft industries, where the possible failure of a complex series of engineered systems could result in highly undesirable outcomes. The usual measure of risk for a class of events is then: \"R\" = probability of the event × the severity of the consequence.\n\nThe total risk is then the sum of the individual class-risks; see below.\n\nIn the nuclear industry, consequence is often measured in terms of off-site radiological release, and this is often banded into five or six-decade-wide bands.\n\nThe risks are evaluated using fault tree/event tree techniques (see safety engineering). Where these risks are low, they are normally considered to be \"broadly acceptable\". A higher level of risk (typically up to 10 to 100 times what is considered broadly acceptable) has to be justified against the costs of reducing it further and the possible benefits that make it tolerable—these risks are described as \"Tolerable if ALARP\", where ALARP stands for \"as low as reasonably practicable\". Risks beyond this level are classified as \"intolerable\".\n\nThe level of risk deemed broadly acceptable has been considered by regulatory bodies in various countries—an early attempt by UK government regulator and academic F. R. Farmer used the example of hill-walking and similar activities, which have definable risks that people appear to find acceptable. This resulted in the so-called Farmer Curve of acceptable probability of an event versus its consequence.\n\nThe technique as a whole is usually referred to as probabilistic risk assessment (PRA) (or probabilistic safety assessment, PSA). See WASH-1400 for an example of this approach.\n\nIn finance, risk is the chance that the return achieved on an investment will be different from that expected, and also takes into account the size of the difference. This includes the possibility of losing some or all of the original investment. In a view advocated by Damodaran, risk includes not only \"downside risk\" but also \"upside risk\" (returns that exceed expectations). Some regard the standard deviation of the historical returns or average returns of a specific investment as providing some historical measure of risk; see modern portfolio theory. Financial risk may be market-dependent, determined by numerous market factors, or operational, resulting from fraudulent behaviour (e.g. Bernard Madoff).\n\nA fundamental idea in finance is the relationship between risk and return (see modern portfolio theory). The greater the potential return one might seek, the greater the risk that one generally assumes. A free market reflects this principle in the pricing of an instrument: strong demand for a safer instrument drives its price higher (and its return correspondingly lower) while weak demand for a riskier instrument drives its price lower (and its potential return thereby higher). For example, a US Treasury bond is considered to be one of the safest investments. In comparison to an investment or speculative grade corporate bond, US Treasury notes and bonds yield lower rates of return. The reason for this is that a corporation is more likely to default on debt than the US government. Because the risk of investing in a corporate bond is higher, investors are offered a correspondingly higher rate of return.\n\nA popular risk measure is Value-at-Risk (VaR).\n\nThere are different types of VaR: long term VaR, marginal VaR, factor VaR and shock VaR. The latter is used in measuring risk during the extreme market stress conditions.\n\nIn finance, \"risk\" has no single definition.\n\nArtzner et al. write \"we call risk the investor's future net worth\". In Novak \"risk is a possibility of an undesirable event\".\n\nIn financial markets, one may need to measure credit risk, information timing and source risk, probability model risk, operational risk and legal risk if there are regulatory or civil actions taken as a result of \"investor's regret\".\n\nWith the advent of automation in financial markets, the concept of \"real-time risk\" has gained a lot of attention. Aldridge and Krawciw define real-time risk as the probability of instantaneous or near-instantaneous loss, and can be due to flash crashes, other market crises, malicious activity by selected market participants and other events. A well-cited example of real-time risk was a US $440 million loss incurred within 30 minutes by Knight Capital Group (KCG) on 1 August 2012; the culprit was a poorly-tested runaway algorithm deployed by the firm. Regulators have taken notice of real-time risk as well. Basel III requires real-time risk management framework for bank stability.\n\nIt is not always obvious if financial instruments are \"hedging\" (purchasing/selling a financial instrument specifically to reduce or cancel out the risk in another investment) or \"speculation\" (increasing measurable risk and exposing the investor to catastrophic loss in pursuit of very high windfalls that increase expected value).\n\nSome people may be \"risk seeking\", i.e. their utility function's second derivative is positive. Such an individual willingly pays a premium to assume risk (e.g. buys a lottery ticket).\n\nThe financial audit risk model expresses the risk of an auditor providing an inappropriate opinion (or material misstatement) of a commercial entity's financial statements. It can be analytically expressed as \nwhere AR is \"audit risk\", IR is \"inherent risk\", CR is \"control risk\" and DR is \"detection risk\".\n\nNote: As defined, audit risk does not consider the impact of an auditor misstatement and so is stated as a simple probability. The impact of misstatement must be considered when determining an acceptable audit risk.\n\nSecurity risk management involves protection of assets from harm caused by deliberate acts. A more detailed definition is: \"A security risk is any event that could result in the compromise of organizational assets i.e. the unauthorized use, loss, damage, disclosure or modification of organizational assets for the profit, personal interest or political interests of individuals, groups or other entities constitutes a compromise of the asset, and includes the risk of harm to people. Compromise of organizational assets may adversely affect the enterprise, its business units and their clients. As such, consideration of security risk is a vital component of risk management.\"\n\nOne of the growing areas of focus in risk management is the field of human factors where behavioural and organizational psychology underpin our understanding of risk based decision making. This field considers questions such as \"how do we make risk based decisions?\", \"why are we irrationally more scared of sharks and terrorists than we are of motor vehicles and medications?\"\n\nIn decision theory, regret (and anticipation of regret) can play a significant part in decision-making, distinct from risk aversion(preferring the status quo in case one becomes worse off).\n\nFraming is a fundamental problem with all forms of risk assessment. In particular, because of bounded rationality (our brains get overloaded, so we take mental shortcuts), the risk of extreme events is discounted because the probability is too low to evaluate intuitively. As an example, one of the leading causes of death is road accidents caused by drunk driving – partly because any given driver frames the problem by largely or totally ignoring the risk of a serious or fatal accident.\n\nFor instance, an extremely disturbing event (an attack by hijacking, or moral hazards) may be ignored in analysis despite the fact it has occurred and has a nonzero probability. Or, an event that everyone agrees is inevitable may be ruled out of analysis due to greed or an unwillingness to admit that it is believed to be inevitable. These human tendencies for error and wishful thinking often affect even the most rigorous applications of the scientific method and are a major concern of the philosophy of science.\n\nAll decision-making under uncertainty must consider cognitive bias, cultural bias, and notational bias: No group of people assessing risk is immune to \"groupthink\": acceptance of obviously wrong answers simply because it is socially painful to disagree, where there are conflicts of interest.\n\nFraming involves other information that affects the outcome of a risky decision. The right prefrontal cortex has been shown to take a more global perspective while greater left prefrontal activity relates to local or focal processing.\n\nFrom the Theory of Leaky Modules McElroy and Seta proposed that they could predictably alter the framing effect by the selective manipulation of regional prefrontal activity with finger tapping or monaural listening. The result was as expected. Rightward tapping or listening had the effect of narrowing attention such that the frame was ignored. This is a practical way of manipulating regional cortical activation to affect risky decisions, especially because directed tapping or listening is easily done.\n\nA growing area of research has been to examine various psychological aspects of risk taking. Researchers typically run randomised experiments with a treatment and control group to ascertain the effect of different psychological factors that may be associated with risk taking. Thus, positive and negative feedback about past risk taking can affect future risk taking. In an experiment, people who were led to believe they are very competent at decision making saw more opportunities in a risky choice and took more risks, while those led to believe they were not very competent saw more threats and took fewer risks.\n\nThe concept of risk-based maintenance is an advanced form of Reliability centred maintenance. In case of chemical industries, apart from probability of failure, consequences of failure is also very important. Therefore, the selection of maintenance policies should be based on risk, instead of reliability. Risk-based maintenance methodology acts as a tool for maintenance planning and decision making to reduce the probability of failure and its consequences. In risk-based maintenance decision making, the maintenance resources can be used optimally based on the risk class (high, medium, or low) of equipment or machines, to achieve tolerable risk criteria.\n\nClosely related to information assurance and security risk, cybersecurity is the application of system security engineering in order to address the compromise of company cyber-assets required for business or mission purposes. In order to address cyber-risk, cybersecurity applies security to the supply chain, the design and production environment for a product or service, and the product itself in order to provide efficient and appropriate security commensurate with the value of the asset to the mission or business process.\n\nSince risk assessment and management is essential in security management, both are tightly related. Security assessment methodologies like CRAMM contain risk assessment modules as an important part of the first steps of the methodology. On the other hand, risk assessment methodologies like Mehari evolved to become security assessment methodologies.\nAn ISO standard on risk management (Principles and guidelines on implementation) was published under code ISO 31000 on 13 November 2009.\n\nThere are many formal methods used to \"measure\" risk.\n\nOften the probability of a negative event is estimated by using the frequency of past similar events. Probabilities for rare failures may be difficult to estimate. This makes risk assessment difficult in hazardous industries, for example nuclear energy, where the frequency of failures is rare, while harmful consequences of failure are severe.\n\nStatistical methods may also require the use of a cost function, which in turn may require the calculation of the cost of loss of a human life. This is a difficult problem. One approach is to ask what people are willing to pay to insure against death or radiological release (e.g. GBq of radio-iodine), but as the answers depend very strongly on the circumstances it is not clear that this approach is effective.\n\nRisk is often measured as the expected value of an undesirable outcome. This combines the probabilities of various possible events and some assessment of the corresponding harm into a single value. See also Expected utility. The simplest case is a binary possibility of \"Accident\" or \"No accident\". The associated formula for calculating risk is then:\n\nFor example, if performing activity \"X\" has a probability of 0.01 of suffering an accident of \"A\", with a loss of 1000, then total risk is a loss of 10, the product of 0.01 and 1000.\n\nSituations are sometimes more complex than the simple binary possibility case. In a situation with several possible accidents, total risk is the sum of the risks for each different accident, provided that the outcomes are comparable:\n\nFor example, if performing activity \"X\" has a probability of 0.01 of suffering an accident of \"A\", with a loss of 1000, and a probability of 0.000001 of suffering an accident of type \"B\", with a loss of 2,000,000, then total loss expectancy is 12, which is equal to a loss of 10 from an accident of type \"A\" and 2 from an accident of type \"B\".\n\nOne of the first major uses of this concept was for the planning of the Delta Works in 1953, a flood protection program in the Netherlands, with the aid of the mathematician David van Dantzig. The kind of risk analysis pioneered there has become common today in fields like nuclear power, aerospace and the chemical industry.\n\nIn statistical decision theory, the risk function is defined as the expected value of a given loss function as a function of the decision rule used to make decisions in the face of uncertainty.\n\nPeople may rely on their fear and hesitation to keep them out of the most profoundly unknown circumstances. Fear is a response to perceived danger. Risk could be said to be the way we collectively measure and share this \"true fear\"—a fusion of rational doubt, irrational fear, and a set of unquantified biases from our own experience.\n\nThe field of behavioural finance focuses on human risk-aversion, asymmetric regret, and other ways that human financial behaviour varies from what analysts call \"rational\". Risk in that case is the degree of uncertainty associated with a return on an asset. Recognizing and respecting the irrational influences on human decision making may do much to reduce disasters caused by naive risk assessments that presume rationality but in fact merely fuse many shared biases.\n\nAccording to one set of definitions, fear is a fleeting emotion ascribed to a particular object, while anxiety is a trait of fear (this is referring to \"trait anxiety\", as distinct from how the term \"anxiety\" is generally used) that lasts longer and is not attributed to a specific stimulus (these particular definitions are not used by all authors cited on this page). Some studies show a link between anxious behaviour and risk (the chance that an outcome will have an unfavorable result). Joseph Forgas introduced valence based research where emotions are grouped as either positive or negative (Lerner and Keltner, 2000). Positive emotions, such as happiness, are believed to have more optimistic risk assessments and negative emotions, such as anger, have pessimistic risk assessments. As an emotion with a negative valence, fear, and therefore anxiety, has long been associated with negative risk perceptions. Under the more recent appraisal tendency framework of Jennifer Lerner et al., which refutes Forgas' notion of valence and promotes the idea that specific emotions have distinctive influences on judgments, fear is still related to pessimistic expectations.\n\nPsychologists have demonstrated that increases in anxiety and increases in risk perception are related and people who are habituated to anxiety experience this awareness of risk more intensely than normal individuals. In decision-making, anxiety promotes the use of biases and quick thinking to evaluate risk. This is referred to as affect-as-information according to Clore, 1983. However, the accuracy of these risk perceptions when making choices is not known.\n\nExperimental studies show that brief surges in anxiety are correlated with surges in general risk perception. Anxiety exists when the presence of threat is perceived (Maner and Schmidt, 2006). As risk perception increases, it stays related to the particular source impacting the mood change as opposed to spreading to unrelated risk factors. This increased awareness of a threat is significantly more emphasised in people who are conditioned to anxiety. For example, anxious individuals who are predisposed to generating reasons for negative results tend to exhibit pessimism. Also, findings suggest that the perception of a lack of control and a lower inclination to participate in risky decision-making (across various behavioural circumstances) is associated with individuals experiencing relatively high levels of trait anxiety. In the previous instance, there is supporting clinical research that links emotional evaluation (of control), the anxiety that is felt and the option of risk avoidance.\n\nThere are various views presented that anxious/fearful emotions cause people to access involuntary responses and judgments when making decisions that involve risk. Joshua A. Hemmerich et al. probes deeper into anxiety and its impact on choices by exploring \"risk-as-feelings\" which are quick, automatic, and natural reactions to danger that are based on emotions. This notion is supported by an experiment that engages physicians in a simulated perilous surgical procedure. It was demonstrated that a measurable amount of the participants' anxiety about patient outcomes was related to previous (experimentally created) regret and worry and ultimately caused the physicians to be led by their feelings over any information or guidelines provided during the mock surgery. Additionally, their emotional levels, adjusted along with the simulated patient status, suggest that anxiety level and the respective decision made are correlated with the type of bad outcome that was experienced in the earlier part of the experiment. Similarly, another view of anxiety and decision-making is dispositional anxiety where emotional states, or moods, are cognitive and provide information about future pitfalls and rewards (Maner and Schmidt, 2006). When experiencing anxiety, individuals draw from personal judgments referred to as pessimistic outcome appraisals. These emotions promote biases for risk avoidance and promote risk tolerance in decision-making.\n\nIt is common for people to dread some risks but not others: They tend to be very afraid of epidemic diseases, nuclear power plant failures, and plane accidents but are relatively unconcerned about some highly frequent and deadly events, such as traffic crashes, household accidents, and medical errors. One key distinction of dreadful risks seems to be their potential for catastrophic consequences, threatening to kill a large number of people within a short period of time. For example, immediately after the 11 September attacks, many Americans were afraid to fly and took their car instead, a decision that led to a significant increase in the number of fatal crashes in the time period following the 9/11 event compared with the same time period before the attacks.\n\nDifferent hypotheses have been proposed to explain why people fear dread risks. First, the psychometric paradigm suggests that high lack of control, high catastrophic potential, and severe consequences account for the increased risk perception and anxiety associated with dread risks. Second, because people estimate the frequency of a risk by recalling instances of its occurrence from their social circle or the media, they may overvalue relatively rare but dramatic risks because of their overpresence and undervalue frequent, less dramatic risks. Third, according to the preparedness hypothesis, people are prone to fear events that have been particularly threatening to survival in human evolutionary history. Given that in most of human evolutionary history people lived in relatively small groups, rarely exceeding 100 people, a dread risk, which kills many people at once, could potentially wipe out one's whole group. Indeed, research found that people's fear peaks for risks killing around 100 people but does not increase if larger groups are killed. Fourth, fearing dread risks can be an ecologically rational strategy. Besides killing a large number of people at a single point in time, dread risks reduce the number of children and young adults who would have potentially produced offspring. Accordingly, people are more concerned about risks killing younger, and hence more fertile, groups.\n\nThe relationship between higher levels of risk perception and \"judgmental accuracy\" in anxious individuals remains unclear (Joseph I. Constans, 2001). There is a chance that \"judgmental accuracy\" is correlated with heightened anxiety. Constans conducted a study to examine how worry propensity (and current mood and trait anxiety) might influence college student's estimation of their performance on an upcoming exam, and the study found that worry propensity predicted subjective risk bias (errors in their risk assessments), even after variance attributable to current mood and trait anxiety had been removed. Another experiment suggests that trait anxiety is associated with pessimistic risk appraisals (heightened perceptions of the probability and degree of suffering associated with a negative experience), while controlling for depression.\n\nIn his seminal work \"Risk, Uncertainty, and Profit\", Frank Knight (1921) established the distinction between risk and uncertainty.\nThus, Knightian uncertainty is immeasurable, not possible to calculate, while in the Knightian sense risk is measurable.\n\nAnother distinction between risk and uncertainty is proposed by Douglas Hubbard:\n\nIn this sense, one may have uncertainty without risk but not risk without uncertainty. We can be uncertain about the winner of a contest, but unless we have some personal stake in it, we have no risk. If we bet money on the outcome of the contest, then we have a risk. In both cases there are more than one outcome. The measure of uncertainty refers only to the probabilities assigned to outcomes, while the measure of risk requires both probabilities for outcomes and losses quantified for outcomes.\n\nThe terms \"risk attitude\", \"appetite\", and \"tolerance\" are often used similarly to describe an organisation's or individual's attitude towards risk-taking. One's attitude may be described as \"risk-averse\", \"risk-neutral\", or \"risk-seeking\". Risk tolerance looks at acceptable/unacceptable deviations from what is expected. Risk appetite looks at how much risk one is willing to accept. There can still be deviations that are within a risk appetite. For example, recent research finds that insured individuals are significantly likely to divest from risky asset holdings in response to a decline in health, controlling for variables such as income, age, and out-of-pocket medical expenses.\n\nGambling is a risk-increasing investment, wherein money on hand is risked for a possible large return, but with the possibility of losing it all. Purchasing a lottery ticket is a very risky investment with a high chance of no return and a small chance of a very high return. In contrast, putting money in a bank at a defined rate of interest is a risk-averse action that gives a guaranteed return of a small gain and precludes other investments with possibly higher gain. The possibility of getting no return on an investment is also known as the rate of ruin.\n\nHubbard also argues that defining risk as the product of impact and probability presumes, unrealistically, that decision-makers are risk-neutral. A risk-neutral person's utility is proportional to the expected value of the payoff. For example, a risk-neutral person would consider 20% chance of winning $1 million exactly as desirable as getting a certain $200,000. However, most decision-makers are not actually risk-neutral and would not consider these equivalent choices. This gave rise to prospect theory and cumulative prospect theory. Hubbard proposes to instead describe risk as a vector quantity that distinguishes the probability and magnitude of a risk. Risks are simply described as a set or function of possible payoffs (gains or losses) with their associated probabilities. This array is collapsed into a scalar value according to a decision-maker's risk tolerance.\n\nThis is a list of books about risk issues.\n\n\n\n\n"}
{"id": "10736601", "url": "https://en.wikipedia.org/wiki?curid=10736601", "title": "Rothschild's giraffe", "text": "Rothschild's giraffe\n\nRothschild's giraffe (\"Giraffa camelopardalis rothschildi\") is a subspecies of the giraffe. It is one of the most endangered distinct populations of giraffe, with 1671 individuals estimated in the wild in 2016.\n\nThe IUCN currently recognizes only one species of giraffe with nine subspecies. \"Giraffa camelopardalis rothschildi\" was named after the Tring Museum's founder, Walter Rothschild, and is also known as the Baringo giraffe, after the Lake Baringo area of Kenya, or as the Ugandan giraffe. All of those living in the wild are in protected areas in Kenya and Uganda. In 2007, Rothschild's giraffe was proposed as a separate species from other giraffe. In 2016, Rothschild's giraffe was proposed to be conspecific with the Nubian giraffe, but that taxonomy has not been widely adopted.\n\nThe Rothschild's giraffe is easily distinguishable from other subspecies. The most obvious sign is in the coloring of the coat, or pelt. Whereas the reticulated giraffe has very clearly defined dark patches with bright-whitish channels between them, Rothschild's giraffe more closely resembles the Masai giraffe. However, when compared to the Masai giraffe, the Rothschild's ecotype is paler, the orange-brown patches are less jagged and sharp in shape, and the connective channel is of a creamier hue compared to that seen on the reticulated giraffe. In addition, Rothschild's giraffe displays no markings on the lower leg, giving it the impression of wearing white stockings.\n\nAnother distinguishing feature of Rothschild's giraffe, although harder to spot, is the number of ossicones on the head. This is the only \"Giraffa\" phenotype to be born with five ossicones. Two of these are the larger and more obvious ones at the top of the head, which are common to all giraffes. The third ossicone can often be seen in the center of the giraffe's forehead, and the other two are behind each ear. They are also taller than many other populations, measuring up to tall.\n\nMales are larger than females and their two largest ossicones are usually bald from sparring. They usually tend to be darker in colour than the females, although this is not a guaranteed sexing indicator.\n\nIsolated populations of Rothschild's giraffes live in savannahs, grasslands and open woodlands of Uganda and Kenya. They are possibly regionally extinct from South Sudan and northeastern Democratic Republic of the Congo.\n\nRothschild's giraffes mate at any time of the year and have a gestation period of 14 to 16 months, typically giving birth to a single calf. They live in small herds, with males and females (and their calves) living separately, only mixing for mating.\n\nGiraffes in general are classified as vulnerable. Very few locations are left where Rothschild's giraffe can be seen in the wild, with notable spots being Lake Nakuru National Park in Kenya and Murchison Falls National Park in northern Uganda.\n\nVarious captive breeding programmes are in place — notably at the Giraffe Centre in Nairobi, Kenya — which aim to expand the gene pool in the wild population of Rothschild's giraffe. , more than 450 are kept in ISIS (international species information system) registered zoos (which does not include the Nairobi Giraffe Centre), making both it and the reticulated giraffe the most commonly kept phenotypes of \"Giraffa\".\n\n"}
{"id": "52374403", "url": "https://en.wikipedia.org/wiki?curid=52374403", "title": "Semicompatibilism", "text": "Semicompatibilism\n\nSemicompatibilism is the view that causal determinism is compatible with moral responsibility, while making no assertions about the truth of determinism or free will. The term was coined by John Martin Fischer.\nProminent semicompatibilists include Sam Harris, Daniel Dennett, and Harry Frankfurt.\n\nCriticisms of this view include the principle of alternative possibilities.\n\n"}
{"id": "7803125", "url": "https://en.wikipedia.org/wiki?curid=7803125", "title": "Serfdom Patent (1781)", "text": "Serfdom Patent (1781)\n\nThe Serfdom Patent of 1 November 1781 aimed to abolish aspects of the traditional serfdom () system of the Habsburg lands through the establishment of basic civil liberties for the serfs.\n\nThe feudal system bound farmers to inherited pieces of land and subjected them to the absolute control of their landlord. The landlord was obligated to provide protection, in exchange for the serfs' labor and goods. The Serfdom Patent, issued by the enlightened absolutist Emperor Joseph II, diminished the long-established mastery of the landlords; thus allowing the serfs to independently choose marriage partners, pursue career choices, and move between estates.\n\nThe Holy Roman Emperor Joseph II ruled as co-regent of the Habsburg Monarchy with his mother, Maria Theresa, from 1765 to 1780. The queen's July Decree of 1770 granted the peasants the right to justice through royal officials rather than their lords' courts. The Patent of 1772 even granted them the right to appeal to the sovereign, and limited the \"robot\" (labour that lords could demand of their serfs) to three days a week and twelve hours a day. The October Decree of 1773 capped the price of letters of release, which serfs could buy from their lords to gain their freedom.\n\nFollowing her death in 1780, Joseph II pursued further liberal reforms. His policies included the 1781 Edict of Toleration, in which the Roman Catholic Emperor granted Protestant denominations more equality than in the past. This represented a tremendous change from the Catholic-centered policies of his mother. Joseph was an enlightened absolutist ruler, incorporating reason and Enlightenment ideals into his administration. Emperor Joseph’s enlightened despot contemporaries, Catherine the Great of Russia and Frederick the Great of Prussia, both claimed to detest feudalism yet chose to appease their noble classes by strengthening the serfdom system during their years in power. Author T. K. E. Blemming describes the rulers' compromises, arguing that \"in exchange for absolute power at a national level it was necessary to hand over to them [nobles] absolute power on their estates.\" Joseph refused to give in to the nobles’ demands, which would soon create difficulties in the implementation of his decrees.\n\nMuch of the Habsburg economy was based on agriculture in the 18th century. The nobles and clerics were traditionally exempt from taxes, and the burden fell mainly on the peasants. After paying dues to the landlord, the serfs were unable to create high tax revenues for Joseph's centralized state. The Emperor recognized that the abolishment of the feudal system would allow peasants to pay higher tax rates to the state. Joseph’s primary objection to feudalism was economic, but his moral objections also arose from witnessing the “inhumanity of serfdom”. He abolished beatings and hoped to allow serfs to appeal court rulings to the throne following a reorganization of the landlord judicial system.\n\nThe Patent was enforced differently amongst all of the various Habsburg lands. The nobility in Bohemia refused to enact its provisions, while the Transylvanian nobles simply refused to notify the peasants in their region about this emancipation document. The Hungarian estates claimed that their peasants were not serfs, but “tenants in fee simple, who were fully informed as to their rights and duties by precise contracts” and continued to restrict these “tenants”. In contrast, the peasants of the German-speaking provinces were actually aided by the Patent. The 1781 Serfdom Patent allowed the serfs legal rights in the Habsburg monarchy, but the document did not affect the financial dues and the physical corvée (unpaid labor) that the serfs legally owed to their landlords. Joseph II recognized the importance of these further reforms, continually attempting to destroy the economic subjugation through related laws, such as his Tax Decree of 1789. This new law would have finally realized Emperor Joseph II’s ambition to modernize Habsburg society, allowing for the end of corvée and the beginning of lesser tax obligations. Joseph’s latter reforms were withdrawn upon his death, but the personal freedom of serfs remained guaranteed through the first half of the nineteenth century due to the consequences of the 1781 Serfdom Patent.\n"}
{"id": "3119943", "url": "https://en.wikipedia.org/wiki?curid=3119943", "title": "Shusaku Arakawa", "text": "Shusaku Arakawa\n\nShusaku Arakawa, who spoke of himself as an \"eternal outsider\" and \"abstractionist of the distant future,\" first studied mathematics and medicine at the University of Tokyo, and art at the Musashino Art University. He was a member of Tokyo's Neo-Dadaism Organizers, a precursor to The Neo-Dada movement. Arakawa's early works were first displayed in the infamous Yomiuri Independent Exhibition, a watershed event for postwar Japanese avant-garde art.\n\nArakawa arrived in New York in 1961 with fourteen dollars in his pocket and a telephone number for Marcel Duchamp, whom he phoned from the airport and with whom he eventually formed a close friendship. He started using diagrams within his paintings as philosophical propositions. Jean-Francois Lyotard said of Arakawa's work that it \"makes us think through the eyes,\" and Hans-Georg Gadamer described it as transforming \"the usual constancies of orientation into a strange, enticing game—a game of continually thinking out.\" Quoting Paul Celan, Gadamer also wrote of the work: \"There are songs to sing beyond the human.\" Charles Bernstein and Susan Bee observe, \"Arakawa deals with the visual field as discourse, modal systems that constitute the world\nrather than being constituted by it.\" Arthur Danto found Arakawa to be \"the most philosophical of contemporary artists.\" For his part, Arakawa declared: \"Painting is only an exercise, never more than that.\"\n\nBeginning in 1963, he collaborated with fellow artist, architect and poet Madeline Gins on the research project \"The Mechanism of Meaning\" which was then completed by 1973. This research project and the architectural projects that stem from it, both built and unbuilt ones, formed the basis of the 1997 \"Arakawa + Gins: Reversible Destiny\" exhibition at the Guggenheim Museum SoHo. (The accompanying comprehensive book of the same title remains the most comprehensive collection of their work, incorporating the whole of the Arakawa/Gins book, \"The Mechanism of Meaning\".)\n\nThe panels appear as a constellation of views concerning the nature of meaning that made be characterized in broad stroke as \"holistic\" or as entailments of a holistic view concerning meaning. To date, two editions of \"The Mechanism of Meaning\" have been made and many of the panels incorporate collaged elements.\n\nArakawa and Madeline Gins are co-founders of the Reversible Destiny Foundation, an organization dedicated to the use of architecture to extend the human lifespan. They have co-authored books, including \"Reversible Destiny\", which is the catalogue of their Guggenheim exhibition, \"Architectural Body\" (University of Alabama Press, 2002) and \"Making Dying Illegal\" (New York: Roof Books, 2006), and have designed and built residences and parks, including the Reversible Destiny Lofts, Bioscleave House, and the Site of Reversible Destiny – Yoro.\n\nArakawa and Gins \"lost their life savings\" to the Bernie Madoff ponzi scheme.\n\nArakawa died on March 18, 2010, after a week of hospitalization. Gins would not state the cause of death. \"This mortality thing is bad news,\" she stated. She planned to redouble efforts to prove \"aging can be outlawed.\"\n\n\n\n\n"}
{"id": "49763666", "url": "https://en.wikipedia.org/wiki?curid=49763666", "title": "Soap shaker", "text": "Soap shaker\n\nA soap shaker is a box entirely made from wire metal mesh with a handle. The box may be opened so as to be able to place in this box a piece or pieces of bar soap. These may be pieces that have become too small to be used as hand soap.\n\nThe box may now be securely closed. Held by its handle the box may be vigorously shaken in a water filled bucket or other container. The shaking will move the water through the box. The result is that the water will become soapy, rich with suds to be used for all kinds of cleaning purposes. This way even small pieces of bar soap could be re-used and are not wasted.\n\nThe use of a soap shaker was common early to mid 20th century. The invention and sale of powdered or liquid soap diminished its use.\n\n"}
{"id": "412938", "url": "https://en.wikipedia.org/wiki?curid=412938", "title": "Steve King", "text": "Steve King\n\nSteven Arnold King (born May 28, 1949) is an American politician serving as the U.S. Representative for since 2013. A member of the Republican Party, he has served in Congress since 2003; his district, which was numbered the 5th congressional district until 2013, is in northwestern Iowa and includes Sioux City.\n\nKing is an opponent of immigration and multiculturalism. He has stirred controversy by making statements that have been described as racist or racially charged, specifically against Jews, African Americans Latinos and immigrants in general, as well as by supporting European right-wing populist and far-right politicians accused of racism and Islamophobia. A short time before the 2018 election, the National Republican Congressional Committee (NRCC) withdrew funding for King's reelection campaign and its Chairman, Steve Stivers, condemned King's conduct. The Anti-Defamation League, an organization founded to oppose antisemitism and other forms of bigotry, has denounced him and called for him to be censured. \"The Washington Post\" has described King as the \"Congressman most openly affiliated with white nationalism.\" He will be the sole House Republican from Iowa in the 116th Congress.\n\nKing was born on May 28, 1949, in Storm Lake, Iowa, the son of Mildred Lila (née Culler), a homemaker, and Emmett A. King, a state police dispatcher. His father has Irish and German ancestry, and his mother has Welsh roots, as well as American ancestry going back to the colonial era. His grandmother was a German immigrant. King graduated in 1967 from Denison Community High School. In 1972 he married Marilyn Kelly, with whom he has three children. Though raised Methodist, King attends his wife's Catholic church, having converted 17 years after marrying her. His son Jeff King (a consultant) has been active in his political campaigns.\n\nKing attended Northwest Missouri State University from 1967 to 1970, and was a member of the Alpha Kappa Lambda fraternity, majoring in math and biology, but did not graduate. While in college, King received \"2S\" deferments in 1967, 1968, and 1969.\n\nIn 1975, King founded King Construction, an earthmoving company. In the 1980s he founded the Kiron Business Association. King's involvement with the Iowa Land Improvement Contractors' Association led to regional and national offices in that organization and a growing interest in public policy.\n\nIn 1996 he was elected to Iowa's 6th Senate district, defeating incumbent Senator Wayne Bennett in the primary 68%–31% and Democrat Eileen Heiden in the general election 64%–35%. In 2000, he won reelection to a second term, defeating Democratic nominee Dennis Ryan 70%–30%.\n\nFrom 1996 to 2002, King served as an Iowa state senator, representing the 6th district. He assisted in eliminating the inheritance tax, authored and passed into law workplace drug testing, and worked for strengthening parental rights, passing tax cuts for working residents of Iowa, and passing a law that made English the official language in Iowa. In May 2001, King visited Cuba.\n\n\nIn 2002, after redistricting, King ran for the open seat in Iowa's 5th congressional district. The incumbent, fellow Republican Tom Latham, had his home drawn into the reconfigured 4th district. King finished first in the four-way Republican primary with 31% of the vote, less than the 35% voting threshold needed to win; subsequently, a nominating convention was held, at which he was nominated, defeating state house speaker Brent Siegrist 51%–47%. King won the general election, defeating Council Bluffs city councilman Paul Shomshor 62%–38%. He won all the counties in the predominantly Republican district except Pottawattamie.\n\n\nKing won reelection to a second term, defeating Democratic candidate Joyce Schulte, 63%–37%. He won all the counties in the district except Clarke.\n\n\nIn 2006, King won reelection to a third term, defeating Schulte again, 59%–36%. He won all the counties in the district except Clarke and Union.\n\n\nKing won reelection to a fourth term, defeating Democratic candidate Rob Hubler, 60%–37%. For the first time in his career he won all 32 counties in his district.\n\n\nKing won reelection to a fifth term, defeating Matt Campbell, 66%–32%. That was his highest percentage yet. King also won all 32 counties again.\n\n\nIowa lost a district as a result of the 2010 census. King's district was renumbered the 4th, and pushed well to the east, absorbing Mason City and Ames. This placed King and his predecessor, Latham, in the same district. Latham opted to move to the reconfigured 3rd District to challenge Democratic incumbent Leonard Boswell. The reconfigured district was, at least on paper, much more competitive than King's old district. The old 5th had a Cook Partisan Voting Index of R+9, while the new 4th has a PVI of R+4. The new 4th was also mostly new to him; he retained only 45% of his former territory. Geographically it was more Latham's district than King's; it closely resembled the territory that Latham had represented from 1995 to 2003.\n\nSoon afterward, former Iowa first lady Christie Vilsack, the wife of former governor and then current U.S. agriculture secretary Tom Vilsack, announced she was moving to the new 4th to challenge King. King received the endorsement of Mitt Romney, who said, \"I'm looking here at Steve King because this man needs to be your congressman again. I want him as my partner in Washington, D.C.\" King won reelection to a sixth term, defeating Vilsack, 53%–45%. King won all but seven counties, none of which he had previously represented: Webster, Boone, Story, Chickasaw, Floyd, Cerro Gordo, and Winnebago. King later said of his 2012 victory, \"I faced $7 million, the best of everything Democrats can throw at me, their dream candidate and everything that can come from the Obama machine, and prevailed through all of that with 55 percent of my district that was new.\"\n\n\nOn May 3, 2013, King announced that he would not run for the U.S. Senate in 2014.\n\nKing won reelection with 61.6% of the vote, defeating Democratic candidate Jim Mowrer.\n\n\nKing won reelection, receiving 61.2% of the vote to Democratic nominee Kim Weaver's 38.6%.\n\n\nKing is considered an outspoken fiscal and social conservative. After winning the 2002 Republican nomination, he said that he intended to use his seat in Congress to \"move the political center of gravity in Congress to the right.\"\n\nDuring the 110th Congress, King voted with the majority of the Republican Party 90.9% of the time. He has continuously voted for Iraq War legislation, supported surge efforts and opposed a time table for troop withdrawals. During the 112th United States Congress King was one of 40 \"staunch\" members of the Republican Study Committee who frequently voted against Republican party leadership and vocally expressed displeasure with House bills.\n\nIn August 2015, King was named the least effective member of Congress by InsideGov due to his persistent failures to get legislation out of committee.\n\nCommittee assignments\n\n\n\nCaucus memberships\n\n\nKing opposes abortion. He has a 100% rating from the National Right to Life Committee, indicating an anti-abortion voting record. King has also voted against allowing human embryonic stem cell research. He supports the No Taxpayer Funding for Abortion Act, which would ban federal funding of abortions except in cases of what the bill calls \"forcible rape\". This would remove the coverage from Medicaid that covers abortions for victims of statutory rape or incest.\n\nAfter Todd Akin made a controversial statement about \"legitimate rape\" on August 19, 2012, King came to his defense, characterizing the critical response as \"petty personal attacks\" and calling Akin a \"strong Christian man\". King said that Akin's voting record should be more important than his words. Six months later, King's defense of Akin (who lost his race) was seen as politically damaging by Steven J. Law of the Conservative Victory Project, a group including Karl Rove that was working to discourage conservative candidates they deemed unelectable, to enable more viable conservative candidates to gain office. Law said, \"We're concerned about Steve King's Todd Akin problem.\"\n\nKing sponsored legislation to ban abortion of a fetus that has a detectable heartbeat, which can in some cases occur as early as 6 weeks (before many women know they are pregnant). A physician who performs a prohibited abortion would be subject to a fine, up to five years in prison, or both. A woman who undergoes a prohibited abortion could not be prosecuted for violating the provisions of this bill.\n\nKing opposes stricter regulations on gun ownership. In 2017, King said that a bill to close the so-called \"gun show loophole\" and add background checks for individuals who bought guns at gun shows would ruin \"Christmas at the Kings'\" if it passed. In 2018, King criticized 18-year old Parkland shooting survivor Emma Gonzalez, attempting to tie her to Communist Cuba. In 2018, he said that guns should not be blamed for gun violence, but rather video games, cultural changes, lack of prayer in schools, gun-free zones, family break-ups, and Ritalin.\n\nIn February 2010, King tweeted about chasing and shooting a raccoon that had tried to enter his house during a blizzard, prompting criticism from animal rights groups. He defended his actions, saying the animal might have been rabid.\n\nIn July 2012, King opposed the McGovern Amendment (to the 2012 Farm Bill) to establish misdemeanor penalties for knowingly attending an organized animal fight and felony penalties for bringing a minor to such a fight. He was also one of 39 members of the House to vote against an upgrade of penalties for transporting fighting animals across state lines in 2007. King received a score of zero on the 2012 Humane Society Legislative Fund's \"Humane Scorecard\". Afterward, he put out a video clarifying his position, stating that it would be putting animals above humans if it were legal to watch humans fight but not animals.\n\nIn July 2012, King introduced an amendment to the House Farm Bill that would legalize previously banned animal agriculture practices such as tail-docking, putting arsenic in chicken feed, and keeping impregnated pigs in small crates. \"My language wipes out everything they've done with pork and veal,\" King said of his amendment. The Humane Society of the United States (HSUS) President Wayne Pacelle said the measure could nullify \"any laws to protect animals, and perhaps ... laws to protect the environment, workers, or public safety.\"\n\nIn May 2013, King introduced another amendment to the House Farm Bill, the Protect Interstate Commerce Act (PICA), saying, \"PICA blocks states from requiring 'free range' eggs or 'free range' pork.\" In 2014, the controversial provision was dropped.\n\nOn April 3, 2009, the Iowa Supreme Court ruled unanimously that a state ban on same-sex marriage violated Iowa's constitution. King soon commented that the justices \"should resign from their position\" and the state legislature \"must also enact marriage license residency requirements so that Iowa does not become the gay marriage Mecca.\" King, along with others, mounted a campaign against the three Iowa Supreme Court justices who were up for retention and had ruled on the gay marriage case. King bought $80,000 of radio advertising across the state calling for Iowans to vote against their retention. None of the three were retained.\n\nOn October 7, 2014, King was one of 19 members of Congress inducted into the LGBT civil rights advocacy group Human Rights Campaign's \"Hall of Shame\" for his opposition to LGBT equality.\n\nIn response to the Supreme Court's 2015 decision \"Obergefell v. Hodges\", in which the court ruled that same-sex marriage is a constitutionally protected right, King has called for a non-binding resolution saying that states may refuse to recognize the decision. King has also called for the abolition of civil marriage.\n\nKing is a staunch opponent of the Affordable Care Act (Obamacare) and has led attempts to repeal it. He fought against Medicare and Medicaid covering a number of medications such as Viagra, which he called \"recreational drugs\".\n\nIn January 2017, King said that in the wake of the 2016 presidential election \"it has become abundantly clear that the American people have overwhelmingly rejected Obamacare time and time again\" and called for congressional Republicans to \"take swift action to fulfill our promise to We the People and repeal this unconstitutional and egregious law passed by hook, crook and legislative shenanigan.\" In May 2017, King said he had moved from supporting the American Health Care Act, the Republican replacement to the Affordable Care Act, to being unsure as a result of benefits such as emergency services, hospitalization and prescription drugs that were added following his backing of the measure: \"Once they negotiated [essential health benefits] with the Freedom Caucus and Tuesday Group, it is hard for me to imagine they will bring that language in the Senate, or that it will be effective because they diluted this thing substantially.\" King added that he and Trump agreed on the need for the federal government to not have a role in health insurance and that Republicans would not have had difficulty repealing the Affordable Care Act had the party prioritized its replacement within the first week of the 115th Congress.\n\nKing also has voted against each stimulus bill in the U.S. House of Representatives, saying, \"Our economy will not recover because government spends more. It will recover because people produce more.\"\n\nKing gained prominence as one of 11 in Congress to vote against the $52 billion Hurricane Katrina aid package, claiming fiscal responsibility and a need for a comprehensive plan for spending aid money.\n\nOn February 26, 2010, King went to the House floor to protest Democrats' handling of health care reform and said, \"Lobbyists do a very effective and useful job on this Hill ... There's a credibility there in that arena that I think somebody needs to stand up for the lobby, and it is a matter of providing a lot of valuable information.\"\n\nKing has dismissed concern over global warming, calling it a \"religion\" and claiming efforts to address climate change are useless. A day after claiming that climate change was more \"a religion than a science,\" he reasserted that many scientists overreact when discussing the consequences of global warming, saying, \"Everything that might result from a warmer planet is always bad in [environmentalists'] analysis. There will be more photosynthesis going on if the Earth gets warmer ... And if sea levels go up 4 or 6 inches, I don't know if we'd know that. We don't know where sea level is even, let alone be able to say that it's going to come up an inch globally because some polar ice caps might melt because there's suspended in the atmosphere.\"\n\nKing strongly endorsed Ted Cruz during the 2016 Republican primaries for President of the United States. He endorsed and strongly supported Donald Trump after Trump won the Republican nomination.\n\n\"The Washington Post\" has described King as \"the U.S. congressman most openly affiliated with white nationalism\", while \"Vanity Fair\" has said his opinions in this direction are \"barely veiled\". David Leonhardt in an opinion piece for \"The New York Times\" has explicitly identified King as being a \"white nationalist\". King has stirred controversy and come to prominence by making statements that have been described as racist or racially charged comments. He is a staunch opponent of immigration and multiculturalism and has supported various far-right European politicians. According to \"The Guardian\", King \"has long been one of the most vociferously anti-immigration members of the House Republican caucus.\"\n\nIn October 2018, the chairman of the National Republican Congressional Committee, Steve Stivers, condemned King as a racist, saying that King's actions and comments were \"completely inappropriate\" and constituted \"white supremacy and hate.\" The NRCC said it would not help King in his 2018 re-election efforts. Representative Carlos Curbelo described King's comments and actions as \"disgusting\" and said that he would never vote for someone like King. Senator Ted Cruz called King's rhetoric \"divisive\" but stopped short of condemning him. Other Republicans, such as House Agriculture Committee Chairman Mike Conaway, dismissed the idea that King is racist.\n\nKing is a staunch opponent of immigration and multiculturalism.\n\nIn April 2006, when asked if \"the US economy simply couldn't function without\" the presence of illegal immigrants, King said that he rejected that position \"categorically\". He said the 77.5 million people between the ages of 16 and 65 in the United States who are not part of the workforce \"could be put to work and we could invent machines to replace the rest.\"\n\nIn 2006, King called for an electrified fence on the US border, noting that such fences were successful in containing livestock.\n\nIn July 2013, speaking about proposed immigration legislation, King said of undocumented immigrants: \"For every one who's a valedictorian, there's another 100 out there who weigh 130 pounds—and they've got calves the size of cantaloupes because they're hauling 75 pounds of marijuana across the desert.\" Despite strong rebukes from both Democrats and King's fellow Republicans, including House speaker John Boehner, who called his statements \"ignorant\" and \"hateful\", and House majority leader Eric Cantor, who called the comments \"inexcusable\", King defended his comments, saying he got the description from the border patrol.\n\nIn July 2015, referencing HUD secretary Julian Castro's remarks on how poorly the Republican Party was doing with Hispanic voters, King responded, \"What does Julian Castro know? Does he know that I'm as Hispanic and Latino as he?\" King is neither Hispanic nor Latino by either family history or ethnic definition.\n\nKing displayed the Confederate flag on his office desk in 2016, despite the fact that Iowa was part of the Union during the American Civil War. He removed it after a Confederate flag-waver shot two Iowa police officers.\n\nIn March 2017, King wrote \"culture and demographics are our destiny. We can't restore our civilization with somebody else's babies.\" When asked about his comments, King stood by them, saying: \"you need to teach your children your values\" and \"with the inter-marriage, I'd like to see an America that is just so homogenous that we look a lot the same\". King was rebuked by members of his own party, including Speaker Paul D. Ryan, but praised by white supremacist David Duke and \"The Daily Stormer\", a neo-Nazi website.\n\nIn July 2017, the House Appropriations Committee voted to fund the US-Mexico border wall, allocating $1.6 billion for it. King called for an additional $5 billion for the wall, to be paid for with federal dollars coming from Planned Parenthood, food stamps, and other federal welfare programs, saying, \"I would find half of a billion of dollars of that right out of Planned Parenthood's budget, and the rest of it could come out of food stamps and the entitlements that are being spread out for people who have not worked in three generations.\" In June 2018, he retweeted a comment by Mark Collett, a British neo-Nazi and self-described admirer of Hitler, about Europe \"waking up\" to mass immigration.\n\nOn November 5, 2018, King referred to Mexican immigrants as \"dirt\" while at a campaign stop. \"The Weekly Standard\" reported the comment and later released an audio recording of the exchange after King initially denied having said it.\n\nOn March 7, 2008, during his press engagements to announce his reelection campaign, King made remarks about then U.S. senator and Democratic presidential candidate Barack Obama and his middle name \"Hussein\", saying:\n\nI don't want to disparage anyone because of their race, their ethnicity, their name—whatever their religion their father might have been, I'll just say this: When you think about the optics of a Barack Obama potentially getting elected President of the United States – I mean, what does this look like to the rest of the world? What does it look like to the world of Islam? I will tell you that, if he is elected president, then the radical Islamists, the al-Qaida, the radical Islamists and their supporters, will be dancing in the streets in greater numbers than they did on September 11.\nOn March 10, King defended his comments to the Associated Press, saying \"[Obama will] certainly be viewed as a savior for them... That's why you will see them supporting him, encouraging him.\"\n\nObama said he did not take the comments too seriously, describing King as a person who thrives on making controversial statements to get media coverage. He said, \"I would hope Senator McCain would want to distance himself from that kind of inflammatory and offensive remarks.\" The McCain campaign disavowed King's comments, saying \"John McCain rejects the type of politics that degrades our civics… and obviously that extends to Congressman King's statement.\"\n\nIn mid-January 2009, King acknowledged that terrorists were not dancing in the streets, adding, \"They have made statements against Obama.\" But he also claimed that he found Obama's decision to use his middle name \"Hussein\" when he was to be sworn in as the 44th President of the United States on January 20, 2009, to be \"bizarre\" and \"a double standard\".\n\nIn 2010, King said that Obama \"has demonstrated that he has a default mechanism in him that breaks down the side of race, on the side that favors the black person.\"\n\nOn June 14, 2010, King said on the House floor that racial profiling is an important component of law enforcement: \"Some claim that the Arizona law will bring about racial discrimination profiling. First let me say, Mr. Speaker, that profiling has always been an important component of legitimate law enforcement. If you can't profile someone, you can't use those common sense indicators that are before your very eyes. Now, I think it's wrong to use racial profiling for the reasons of discriminating against people, but it's not wrong to use race or other indicators for the sake of identifying people that are violating the law.\" As an example of profiling, King described an instance when a taxi driver would stop for him before he had to hail a cab, just because he was in a business suit.\n\nThe same day, on G. Gordon Liddy's radio program, King said that Obama's policies favored black people: \"The president has demonstrated that he has a default mechanism in him that breaks down the side of race—on the side that favors the black person in the case of Professor Gates and Officer Crowley.\"\n\nOn July 18, 2016, King participated in a panel discussion on MSNBC, during which a panelist from \"Esquire\" magazine suggested that the 2016 convention could be the last in which \"old white people would command the Republican Party's attention\". King responded, \"This whole 'old white people' business does get a little tired, Charlie. I'd ask you to go back through history and figure out where are these contributions that have been made by these other categories of people that you are talking about? Where did any other subgroup of people contribute more to civilization?\" Panel moderator Chris Hayes later described King's comments as odious and preposterous. Panel member April Ryan described them as \"in-my-face racism\". That evening, King was asked about his comments during an interview with ABC News. King said he had meant to say that \"Western civilization,\" rather than \"white people,\" is the \"superior culture\": \"when you describe Western civilization, that can mean much of Western civilization happens to be Caucasians. But we should not apologize for our culture or our civilization. The contributions that were made by Western civilization itself, and by Americans, by Americans of all races, stand far above the rest of the world. The Western civilization and the American civilization are a superior culture.\"\n\nIn an interview with Breitbart News, King said he did not want Muslims working in meat-packing plants. In May 2014, King compared the torture and prisoner abuse at Abu Ghraib prison to \"hazing\".\n\nKing opposes affirmative action. He has said, \"There's been legislation that's been brought through this House that sets aside benefits for women and minorities. The only people that it excludes are white men... Pretty soon, white men are going to notice they are the ones being excluded.\"\n\nOn March 12, 2017, King expressed his support for Geert Wilders, a far-right Dutch politician known for his anti-Islam views, leading up to the election in the Netherlands, stating, \"Wilders understands that culture and demographics are our destiny\" and \"We can't restore our civilization with somebody else's babies,\" referring to his views on ending birthright citizenship and promoting \"an America that's just so homogenous that we look a lot [sic] the same.\" His statements received criticism from other politicians, including several Republicans, with Jeb Bush responding that \"America is a nation of immigrants\"; despite the backlash, King firmly defended his statements. Others noted that King's statements were well received among white nationalists, garnering support from prominent members of that community. The next day on CNN, King said he was referring to culture, not ethnicity, saying \"It's the culture, not the blood. If you can go anywhere in the world and adopt these babies and put them into households that were already assimilated in America, those babies will grow up as American as any other baby with as much patriotism and love of country as any other baby.\"\n\nKing supported French right-wing populist politician, leader of the Front National Marine Le Pen in the French 2017 presidential election. He sent her a message stating: \"Our shared civilization must be saved\".\n\nKing supported Hungarian Prime Minister Viktor Orbán, a right-wing populist and strong opponent of admitting migrants during the European migrant crisis. On December 8, 2017, King tweeted Orbán's quote that \"Diversity is not our strength. Hungarian Prime Minister Victor Orban, 'Mixing cultures will not lead to a higher quality of life but a lower one'.\" \"Assimilation has become a dirty word to the multiculturalist Left. Assimilation, not diversity, is our American strength,\" he tweeted.\n\nOn August 24, 2018, King was interviewed by the Austrian website \"Unzensuriert\" (\"Uncensored\"), which is connected to the country's Freedom Party, part of the Kurz government. He agreed with the interviewer that American financier George Soros is involved with the \"Great Replacement\", a far-right conspiracy theory that claims to have identified a plot to replace white Europeans with minorities and immigrants.\n\nKing has also endorsed Faith Goldy for mayor of Toronto. Goldy has participated in a Neo-Nazi podcast. In response to the Goldy endorsement and King’s other racially contentious remarks, Land O Lakes ended its support for his reelection.\n\n\"Vox\" has claimed that King subscribes to the white genocide conspiracy theory, demonstrating the existence of the view in the United States Congress. \"ThinkProgress\" has accused King of endorsing \"a slightly more genteel\" version of the conspiracy, while \"Mother Jones\" and other media have reported more generally on his belief in and promotion of it.\n\nIn late October 2018, after the Pittsburgh synagogue shooting, the Anti-Defamation League director Jonathan Greenblatt sent Paul Ryan an open letter calling upon him to censure King, citing King's relationship with far-right Freedom Party of Austria and other far-right groups in Europe. The letter began, \"After the events of this weekend, I knew that ADL could be silent no more\", and said King had engaged in antisemitic smearing of George Soros. It concluded, \"Rep. King has brought dishonor onto the House of Representatives. We strongly urge you and the congressional leadership to demonstrate your revulsion with Rep. King’s actions by stripping him of his subcommittee chairmanship and initiating proceedings to formally censure or otherwise discipline him.\"\n\nKing responded: “These attacks are orchestrated by nasty, desperate, and dishonest fake news. Their ultimate goal is to flip the House and impeach Donald Trump. Establishment Never-Trumpers are complicit.”\n\n"}
{"id": "36407722", "url": "https://en.wikipedia.org/wiki?curid=36407722", "title": "Stop the Killing KC", "text": "Stop the Killing KC\n\nStop The Killing KC is a community improvement organization in Kansas City, Missouri patterned after \"stop the violence/stop the killing\" movements in other large American cities, urged since 1985 by Minister Louis Farrakhan of the Nation of Islam movement, and drawing on lessons learned in past decades by many other community improvement organizations such as urban development programs in various U.S. Cities and evangelical or social outreach programs of various churches, religions or groups.\n\nThe Kansas City organization became unofficially established with a \"Declaration of Unity\" signed by Nation of Islam Student Minister Vincent Muhammad, Pastor Tony R. Caldwell of Community United KC, Kenneth \"Sasteh Meter\" Mosley of EMWOT(East Meets West of Troost), and more than three dozen other community improvement activists on September 17, 2011.\n\nAfter several meetings and discussions in ensuing weeks, Pastor Caldwell, Captain Muhammad, Sasteh Meter Mosley, and other community leaders met at the Bruce R. Watkins Cultural Heritage Center on Feb. 11, 2012 and called for all interested persons to meet the following Saturday, February 18, and create a working organization. On March 3, 2012, an organizational meeting elected officers, including a President and Vice-President, but in mid-May policy disagreements among officers resulted in that organizational structure being abandoned for a seven-member Advisory Council. Stop The Killing KC was formally incorporated on March 14, 2012, with a dedicatory \"open house\" April 7, 2012. In flyers publicizing the event, Stop the Killing KC lists more than seventy \"partners\" as being supportive in a \"Stop The Killing KC Coalition\":\n\nActivities the group has encouraged or sponsored include vigils and funerals for deceased victims of violence, crime investigation, distributing flyers at busy intersections, canvassing crime-hit neighborhoods, voter registration drives, daily and nightly classes, Study Groups, like- Literacy for youth and adults, Health and Wellness awareness and food program,feeding youth and children through the summer and after school, urban agriculture, vocational green career opportunities and seminars, participation in events such as Kansas City's Troost Festival on April 28, 2012; the annual Convoy of Hope festival on June 16, 2012 and so forth. On the afternoon of July 1, 2012 the STK-KC held a \"3 on 3\" basketball tournament, Missouri State Representative Brandon Ellington participated, and Kansas City's Chief of Police Darryl Forte visited, as well as other community leaders and basketball players and fans from around the city. On July 29, 2012, STK-KC hosted a second 3 on 3 tournament \"Shoot Baskets- Not Bullets!\", and scheduled two basketball events for every month throughout the summer of 2012.\n\nOn May 14, 2012 a Kansas City woman named Andrea Shields was murdered and her body then set on fire. Stop the Killing KC and other anti-violence organizations held vigils for and coordinated funeral arrangements for Shields. The funeral was scheduled for 2 p.m. Saturday May 26, 2012. Shortly after 4 a.m. that morning a drive-by shooter fired eight bullets toward Stop the Killing KC's headquarters building, seven rounds lodging in a vehicle parked in front of the building, one round chipping siding near the building's \"Stop the Killing KC\" sign. Allegedly, someone phoned Kansas City's \"Tips Hotline\" the day before, and demanded that Stop the Killing KC '\"back off from the Andrea Shields investigation\"'. Because Stop The Killing KC is a \"Crime Prevention\" and pro-Civil Rights organization, Federal Law Enforcement agencies are investigating the incident along with Kansas City police.\n"}
{"id": "17670991", "url": "https://en.wikipedia.org/wiki?curid=17670991", "title": "Time displacement", "text": "Time displacement\n\nTime displacement in sociology refers to the idea that new forms of activities may replace older ones. New activities that cause time displacement are usually technology-based, most common are the information and communication technologies such as Internet and television. Those technologies are seen as responsible for declines of previously more common activities such as in- and out-of-home socializing, work, and even personal care and sleep.\n\nFor example, Internet users may spend time online using it as a substitute of other activities that served similar function(s) (watching television, reading printed media, face to face interaction, etc.). Internet is not the first technology to result in time displacement. Earlier, television had a similar impact, as it shifted people's time from activities such as listening to radio, going to movie theaters or, talking in house, or spending time outside it.\n\n\n"}
{"id": "34375387", "url": "https://en.wikipedia.org/wiki?curid=34375387", "title": "Trade-off talking rational economic person", "text": "Trade-off talking rational economic person\n\nTrade-off talking rational economic person (TOTREP) is one term, among various, used to denote, in the field of choice analysis, the rational, human agent of economic decisions.\n\nThe term was first used notably in David M. Kreps' \"Notes on the Theory of Choice\" (1988). Kreps, in his preface, acknowledges Mike Harrison for first using the acronym Totrep. In his work on the theory of choice, Michael Allingham characterizes the notion \"denoted by TOTREP\" as something \"both to admire and worry about\". Studies of the concept of the Trade-off Talking Rational Economic Person are now routinely conducted in the related fields of theory of choice, the mathematics of decision making, etc.\n\nTotrep denotes an agent of economic decisions with strict preferences. The setting of the preference relations presumes that\n\nIn attempts to legitimatize economic theory as ethical, the question was asked about how to \"teach or preach to economists or ethicists how to become more ethical\". In this, social scientist Kjell Hausken posits the notion that, \"if acting virtuously contributes to a character or personality which subsequently and indirectly influences economic [agent]'s reputation beneficially in the long run, then this action is to be recommended if the long-term benefit of the beneficial reputation outweighs the short-term benefit of a more deceitful or vicious action.\" Hausken observes that, the \"ethical economic [agent]\" is actually \"a trade-off talking rational economic person\", who constantly carries out \"a cost–benefit analysis where reputation is a relevant input, constantly focusing on her real interests in a broad sense, and constantly thinking long term.\"\n\nEmpirical research, from the 1970s onwards (research that eventually established the field of behavioral economics), showed that the Totrep model of rationality cannot be safely considered as representative of real-life human affairs.\n\nKreps himself initially describes the models of choice presented in his book as both normative and descriptive but, in the final chapter of his book, where he briefly outlines the paradoxes of Allais (1953) and Ellsberg (1961), ultimately acknowledges that Totrep modelling should be presumed to be merely normative, since the \"evidence [shows] individuals are very poor intuitive statisticians\" and \"use heuristic procedures which sometimes bias their choices in ways that the standard models don't capture.\"\n"}
{"id": "379514", "url": "https://en.wikipedia.org/wiki?curid=379514", "title": "Transarmament", "text": "Transarmament\n\nTransarmament (closely related to civilian-based defense) is the partial or total replacement of armed forces with the physical and social infrastructure to support nonviolent resistance. According to an encyclopedia definition, transarmament is\n\nthe process of changeover from a military-based defense policy to a civilian-based defense policy. Transarmament always involves the replacement of one means to provide defense with another. It therefore differs from \"disarmament,\" which is the simple reduction or abandonment of military capacity. (p 534)\nThe term \"transarmament\" appears to have been introduced in 1937 in a pamphlet by Kenneth Boulding. It appears not to have been used again until the 1960s.\n\nAccording to Adam Roberts,\n\n\"Transarmament\" has later been used—for example, by Johan Galtung—as a technical term to describe a shift in military strategy. Galtung distinguishes between offensive and defense armaments and suggests transitioning to a defensive system of national defense. His use of the term does not imply nonviolence. Galtung advocates precise weapons with limited range and destructive effects.\n\n\n"}
{"id": "46941716", "url": "https://en.wikipedia.org/wiki?curid=46941716", "title": "Vine copula", "text": "Vine copula\n\nA vine is a graphical tool for labeling constraints in high-dimensional probability distributions. A regular vine is a special case for which all constraints are two-dimensional or conditional two-dimensional. Regular vines generalize trees, and are themselves specializations of Cantor trees. Combined with bivariate copulas, regular vines have proven to be a flexible tool in high-dimensional dependence modeling. Copulas\n\nare multivariate distributions with uniform univariate margins. Representing a joint distribution as univariate margins plus copulas allows the separation of the problems of estimating univariate distributions from the problems of estimating dependence. This is handy in as much as univariate distributions in many cases can be adequately estimated from data, whereas dependence information is rough known, involving summary indicators and judgment.\nAlthough the number of parametric multivariate copula families with flexible dependence is limited, there are many parametric families of bivariate copulas. Regular vines owe their increasing popularity to the fact that they leverage from bivariate copulas and enable extensions to arbitrary dimensions. Sampling theory and estimation theory for regular vines are well developed\n\nand model inference has left the post\n. Regular vines have proven useful in other problems such as (constrained) sampling of correlation matrices, building non-parametric continuous Bayesian networks. \n\nIn finance, vine copulas have been shown to effectively model tail risk in portfolio optimization applications.\n\nThe first regular vine, avant la lettre, was introduced by Harry Joe.\nThe motive was to extend parametric bivariate extreme value copula families to higher dimensions. To this end he introduced what would later be called the \"D-vine\". Joe \n\nwas interested in a class of n-variate distributions with given one dimensional margins, and \"n\"(\"n\" − 1) dependence parameters, whereby \"n\" − 1 parameters correspond to bivariate margins, and the others correspond to conditional bivariate margins. In the case of multivariate normal distributions, the parameters would be \"n\" − 1 correlations and (\"n\" − 1)(\"n\" − 2)/2 partial correlations, which were noted to be algebraically independent in (−1, 1).\n\nAn entirely different motivation underlay the first formal definition of vines in Cooke.\nUncertainty analyses of large risk models, such as those undertaken for the European Union and the US Nuclear Regulatory Commission for accidents at nuclear power plants, involve quantifying and propagating uncertainty over hundreds of variables.\n\nDependence information for such studies had been captured with \"Markov trees\",\n\nwhich are trees constructed with nodes as univariate random variables and edges as bivariate copulas. For \"n\" variables, there are at most \"n\" − 1 edges for which dependence can be specified. New techniques at that time involved obtaining uncertainty distributions on modeling parameters by eliciting experts' uncertainties on other variables which are predicted by the models. These uncertainty distributions are pulled back onto the model's parameters by a process known as probabilistic inversion.\nThe resulting distributions often displayed a dependence structure that could not be captured as a Markov tree.\n\nGraphical models called \"vines\" were introduced in\n\nA vine on \"n\" variables is a nested set of connected trees where the edges in the first tree are the nodes of the second tree, the edges of the second tree are the nodes of the third tree, etc. \nA \"regular vine\" or \"R-vine\" on \"n\" variables is a vine in which two edges in tree are joined by an edge in tree \"j\" + 1 only if these edges share a common node, \"j\" = 1, …, \"n\" − 2. The nodes in the first tree are univariate random variables. The edges are constraints or conditional constraints explained as follows.\n\nRecall that an edge in a tree is an unordered set of two nodes. Each edge in a vine is associated with a \"constraint set\", being the set of variables (nodes in first tree) reachable by the set membership relation. For each edge, the constraint set is the union of the constraint sets of the edge's two members called its component constraint sets (for an edge in the first tree, the component constraint sets are empty). The constraint associated with each edge is now the symmetric difference of its component constraint sets conditional on the intersection of its constraint sets. One can show that for a regular vine, the symmetric difference of the component constraint sets is always a doubleton and that each pair of variables occurs exactly once as constrained variables. In other words, all constraints are bivariate or conditional bivariate.\n\nThe degree of a node is the number of edges attaching to it. The simplest regular vines have the simplest degree structure; the D-Vine assigns every node degree 1 or 2, the C-Vine assigns one node in each tree the maximal degree. For large vines, it is clearer to draw each tree separately.\n\nThe number of regular vines on \"n\" variables grows rapidly in \"n\": there are 2 ways of extending a regular vine with one additional variable, and there are \"n\"(\"n\" − 1)(\"n\" − 2)!2/2 labeled regular vines on \"n\" variables\n\nThe constraints on a regular vine may be associated with \"partial correlations\" or with \"conditional bivariate copula\". In the former case, we speak of a \"partial correlation vine\", and in the latter case of a \"vine copula\".\n\nBedford and Cooke show that any assignment of values in the open interval (−1, 1) to the edges in any partial correlation vine is consistent, the assignments are algebraically independent, and there is a one-to-one relation between all such assignments and the set of correlation matrices. In other words, partial correlation vines provide an algebraically independent parametrization of the set of correlation matrices, whose terms have an intuitive interpretation. Moreover, the determinant of the correlation matrix is the product over the edges of (1 − \"ρ\") where \"ρ\" is the partial correlation assigned to the edge with conditioned variables \"i\",\"k\" and conditioning variables \"D\"(\"ik\"). A similar decomposition characterizes the mutual information, which generalizes the determinant of the correlation matrix. These features have been used in constrained sampling of correlation matrices, building non-parametric continuous Bayesian networks and addressing the problem of extending partially specified matrices to positive definite matrices\n\nUnder suitable differentiability conditions, any multivariate density \"f\" on \"n\" variables, with univariate densities \"f\",…,\"f\", may be represented in closed form as a product of univariate densities and (conditional) copula densities on any R-vine \n\nwhere edges with conditioning set are in the edge set of any regular vine . The conditional copula densities in this representation depend on the cumulative conditional distribution functions of the conditioned variables, , and, potentially, on the values of the conditioning variables. When the conditional copulas do not depend on the values of the conditioning variables, one speaks of the \"simplifying assumption\" of constant conditional copulas. Though most applications invoke this assumption, exploring the modelling freedom gained by discharging this assumption has begun\n. When bivariate Gaussian copulas are assigned to edges of a vine, then the resulting multivariate density is the Gaussian density parametrized by a partial correlation vine rather than by a correlation matrix.\n\nThe vine pair-copula construction, based on the sequential mixing of conditional distributions has been adapted to discrete variables and mixed discrete/continuous response\nAlso factor copulas, where latent variables have been added to the vine, have been proposed (e.g., \n\nVine researchers have developed algorithms for maximum likelihood estimation and simulation of vine copulas, finding truncated vines that summarize the dependence in the data, enumerating through vines, etc. Chapter 6 of \"Dependence Modeling with Copulas\" summarizes these algorithms in pseudocode.\n\nFor parametric vine copulas, with a bivariate copula family on each edge of a vine, algorithms and software are available for maximum likelihood estimation of copula parameters, assuming data have been transformed to uniform scores after fitting univariate margins. There are also available algorithms (e.g., \nfor choosing good truncated regular vines where edges of high-level trees are taken as conditional independence. These algorithms assign variables with strong dependence or strong conditional dependence to low order trees in order that higher order trees have weak conditional dependence or conditional independence. Hence parsimonious truncated vines are obtained for a large number of variables. Software with a user interface in R are available (e.g., \n\nA sampling order for variables is a sequence of conditional densities in which the first density is unconditional, and the densities for other variables are conditioned on the preceding variables in the ordering. A sampling order is \"implied by a regular-vine\" representation of the density if each conditional density can be written as a product of copula densities in the vine and one dimensional margins.\nAn implied sampling order is generated by a nested sequence of subvines where each sub-vine in the sequence contains one new variable not present in the preceding sub-vine. For any regular vine on variables there are implied sampling orders. Implied sampling orders are a small\nsubset of all orders but they greatly facilitate sampling. Conditionalizing a regular vine on values of an arbitrary subset of variables is a complex operation. However, conditionalizing on an initial sequence of an implied sampling order is trivial, one simply plugs in the initial conditional values and proceeds with the sampling. A general theory of conditionalization does not exist at present.\n\n\n"}
