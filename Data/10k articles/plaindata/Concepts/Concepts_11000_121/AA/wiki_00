{"id": "241342", "url": "https://en.wikipedia.org/wiki?curid=241342", "title": "24-hour clock", "text": "24-hour clock\n\nThe 24-hour clock is the convention of time keeping in which the day runs from midnight to midnight and is divided into 24 hours, indicated by the hours passed since midnight, from 0 to 23. This system is the most commonly used time notation in the world today, and is used by international standard ISO 8601.\n\nA limited number of countries, particularly English-speaking, use the 12-hour clock, or a mixture of the 24- and 12-hour time systems. In countries where the 12-hour clock is still dominant, some professions prefer to use the 24-hour clock. For example, in the practice of medicine the 24-hour clock is generally used in documentation of care as it prevents any ambiguity as to when events occurred in a patient's medical history. In the United States and a handful of other countries, it is popularly referred to as military time.\n\nA time of day is written in the 24-hour notation in the form hh:mm (for example 01:23) or hh:mm:ss (for example, 01:23:45), where hh (00 to 23) is the number of full hours that have passed since midnight, mm (00 to 59) is the number of full minutes that have passed since the last full hour, and ss (00 to 59) is the number of seconds since the last full minute. In the case of a leap second, the value of ss may extend to 60. A leading zero is added for numbers under 10, but it is optional for the hours. The leading zero is very commonly used in computer applications, and always used when a specifications require it (for example, ISO 8601).\n\nWhere subsecond resolution is required, the seconds can be a decimal fraction; that is, the fractional part follows a decimal dot or comma, as in 01:23:45.678. The most commonly used separator symbol between hours, minutes and seconds is the colon, which is also the symbol used in ISO 8601. In the past, some European countries used the dot on the line as a separator, but most national standards on time notation have since then been changed to the international standard colon. In some contexts (including the U.S. military and some computer protocols), no separator is used and times are written as, for example, \"2359\".\n\nIn the 24-hour time notation, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59. Where convenient, the notation 24:00 may also be used to refer to midnight at the end of a given datethat is, 24:00 of one day is the same time as 00:00 of the following day.\n\nThe notation 24:00 mainly serves to refer to the exact end of a day in a time interval. A typical usage is giving opening hours ending at midnight (e.g. \"00:00–24:00\", \"07:00–24:00\"). Similarly, some railway timetables show 00:00 as departure time and 24:00 as arrival time. Legal contracts often run from the start date at 00:00 until the end date at 24:00.\n\nWhile the 24-hour notation unambiguously distinguishes between midnight at the start (00:00) and end (24:00) of any given date, there is no commonly accepted distinction among users of the 12-hour notation. Style guides and military communication regulations in some English-speaking countries discourage the use of 24:00 even in the 24-hour notation, and recommend reporting times near midnight as 23:59 or 00:01 instead. Sometimes the use of 00:00 is also avoided. In variance with this, the correspondence manual for the U.S. Navy and U.S. Marine Corps formerly specified 0001 to 2400.<ref name=\"http://www.marforres.marines.mil/Portals/116/Docs/G-1/AAU/AAUDocuments/CORRESPONDENCE%20MANUAL.pdf\">SECNAV M-5216.5 Department of the Navy Correspondence Manual dated March 2010, Chapter 2, Section 5 Paragraph 15. Expressing Military Time.</ref> The manual was updated in June 2015 to use 0000 to 2359.\n\nTime-of-day notations beyond 24:00 (such as 24:01 or 25:00 instead of 00:01 or 01:00) are not commonly used and not covered by the relevant standards. However, they have been used occasionally in some special contexts in the UK, Japan, South Korea, Hong Kong and China where business hours extend beyond midnight, such as broadcast television production and scheduling.\n\nIn most countries, computers by default show the time in 24-hour notation. For example, Microsoft Windows and macOS activate the 12-hour notation by default only if a computer is in a handful of specific language and region settings. The 24-hour system is commonly used in text-based interfaces. Programs such as ls default to displaying timestamps in 24 hour format.\n\nIn American and Canadian English, the term \"military time\" is a synonym for the 24-hour clock. In these dialects, the time of day is customarily given almost exclusively using the 12-hour clock notation, which counts the hours of the day as 12, 1, ..., 11 with suffixes \"a.m.\" and \"p.m.\" distinguishing the two diurnal repetitions of this sequence. The 24-hour clock is commonly used there only in some specialist areas (military, aviation, navigation, tourism, meteorology, astronomy, computing, logistics, emergency services, hospitals), where the ambiguities of the 12-hour notation are deemed too inconvenient, cumbersome, or dangerous.\n\nMilitary usage, as agreed between the United States and allied English-speaking military forces, differs in some respects from other twenty-four-hour time systems:\n\nThe 24-hour time system has its origins in the Egyptian astronomical system of decans, and has been used for centuries by scientists, astronomers, navigators, and horologists. In East Asia, time notation was 24-hour before westernization in modern times. Western-made clocks were changed into 12 dual-hours style when they were shipped to China in the Qing dynasty. There are many surviving examples of clocks built using the 24-hour system, including the famous Orloj in Prague, and the Shepherd Gate Clock at Greenwich.\n\nThe first mechanical public clocks introduced in Italy were mechanical 24-hour clocks which counted the 24 hours of the day from one half hour after sundown to the evening of the following day. The 24th hour was the last hour of day time. However, striking clocks had to produce 300 strokes each day which required a lot of rope, and wore out the mechanism quickly, so some localities switched to ringing sequences of 1 to 12 twice (156 strokes), or even 1 to 6 repeated 4 times (84 strokes).\n\nAfter missing a train while travelling in Ireland in 1876 because a printed schedule listed p.m. instead of a.m., Sir Sandford Fleming proposed a single 24-hour clock for the entire world, located at the centre of the Earth, not linked to any surface meridian – a predecessor to Coordinated Universal Time. He was an early proponent of using the 24-hour clock as part of a programme to reform timekeeping, which also included establishing time zones and a standard prime meridian. The Canadian Pacific Railway was among the first organizations to adopt the 24-hour clock, at midsummer 1886.\n\nAt the International Meridian Conference in 1884, Lewis M. Rutherfurd proposed:\n\nThat this universal day is to be a mean solar day; is to begin for all the world at the moment of midnight of the initial meridian coinciding with the beginning of the civil day and date of that meridian, and is to be counted from zero up to twenty-four hours.\nThis resolution was adopted by the conference.\n\nA report by a government committee in the United Kingdom noted Italy as the first country among those mentioned to adopt 24-hour time nationally, in 1893. Other European countries followed: France adopted it in 1912 (the French army in 1909), followed by Denmark (1916), and Greece (1917). By 1920, Spain, Portugal, Belgium, and Switzerland had switched, followed by Turkey (1925), and Germany (1927). By the early 1920s, many countries in Latin America had also adopted the 24-hour clock. Some of the railways in India had switched before the outbreak of the war.\n\nDuring World War I, the British Royal Navy adopted the 24-hour clock in 1915, and the Allied armed forces followed soon after, with the British Army switching officially in 1918. The Canadian armed forces first started to use the 24-hour clock in late 1917. In 1920, the US Navy was the first US organization to adopt the system; the US Army, however, did not officially adopt the 24-hour clock until World War II, on July 1, 1942.\n\nThe use of the 24-hour clock in the United Kingdom has grown steadily since the beginning of the 20th century, although attempts to make the system official failed more than once. In 1934, the BBC switched to the 24-hour clock for broadcast announcements and programme listings. The experiment was halted after five months following a lack of enthusiasm from the public, and the BBC continued using the 12-hour clock. In the same year, the US airlines Pan American World Airways Corporation and Western Airlines both adopted the 24-hour clock. In modern times, the BBC uses a mixture of both the 12-hour and the 24-hour clock. British Rail and London Transport switched to the 24-hour clock for timetables in 1964. A mixture of the 12- and 24-hour clocks similarly prevails in other English-speaking Commonwealth countries: French speakers have adopted the 24-hour clock in Canada much more broadly than English speakers, and Australia also uses both systems.\n\n\n"}
{"id": "3192516", "url": "https://en.wikipedia.org/wiki?curid=3192516", "title": "ACM Computing Classification System", "text": "ACM Computing Classification System\n\nThe ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organise subjects by area.\n\nThe system has gone through seven revisions, the first version being published in 1964, and revised versions appearing in 1982, 1983, 1987, 1991, 1998, and the now current version in 2012.\n\nThe ACM Computing Classification System, version 2012, has a revolutionary change in some areas, for example, in \"Software\" that now is called \"Software and its engineering\" which has three main subjects:\n\nIt is hierarchically structured in four levels. Thus, for example, one branch of the hierarchy contains:\n\n\n\n"}
{"id": "5785587", "url": "https://en.wikipedia.org/wiki?curid=5785587", "title": "A Satire of the Three Estates", "text": "A Satire of the Three Estates\n\nA Satire of the Three Estates (Middle Scots: \"Ane Pleasant Satyre of the Thrie Estaitis\"), is a satirical morality play in Middle Scots, written by makar Sir David Lyndsay. The complete play was first performed outside in the playing field at Cupar, Fife in June 1552 during the Midsummer holiday, where the action took place under Castle Hill. It was subsequently performed in Edinburgh, also outdoors, in 1554. The full text was first printed in 1602 and extracts were copied into the \"Bannatyne Manuscript\". The \"Satire\" is an attack on the Three Estates represented in the Parliament of Scotland – the clergy, lords and burgh representatives, symbolised by the characters \"Spiritualitie\", \"Temporalitie\" and \"Merchant\". The clergy come in for the strongest criticism. The work portrays the social tensions present at this pivotal moment in Scottish history.\n\nA complete version of the play was printed by Robert Charteris as, \"Ane (Pleasant) Satyre of the Thrie Estaits, in Commendation of Vertew and Vituperation of Vyce\", Edinburgh (1602). In the first part there are 27 different characters. In the second part seven more are added. The key characters are; King Humanity, Divine Correction, Sensuality, Spirituality, Temporality, Gude Counsel and Chastity.\n\nThe play opens with Diligence delivering a sermon on good kingship. The main character, young King Humanity, then appears and is at first led astray by Sensuality and the Vices. His false counsellors introduce him to a mistress, Sensuality, which is the starting point of his disconnection from the moral way of life. He is then fooled by three disguised liars. Gude Counsel is sent to prison by the liars who already have taken control of King Humanitie's mind. With the beginning of his lecherous new life the king forgets about the moral virtues and can no longer judge properly. He consigns Charity and Verity to the stocks. In the course of the following scenes the audience sees how the three so called Vices (Discretion, Devotion and Sapience) try to get rid of everything and everyone who could be dangerous to them. For instance Lady Chastitie, who is homeless since the church in Scotland is not as it was meant to be, begs for shelter from Spiritualitie, Temporalitie and finally the People but is rejected in each case. In the end when Lady Chastitie is sent to prison by the Vices, Divine Correction enters the stage. This is the moment when the vices know that their time has come to an end and they flee and take away the king’s treasure box. Correction frees Gude Counsel, Chastitie and Vertie. He advises the young king to call a parliament and gives him advice regarding a successful reign.\n\nThe second part starts with an interruption. A member of the King's realm, known only as The Poor Man, emerges from the audience, establishes an alliance with John Commonweal to demand reform, and Diligence reappears to announce that the King will seek to improve his realm. Afterwards the Pardoner enters the scene and tries to sell pardoners. Poor Man hears that and buys pardoners worth ‘ane groat’. But Poor Man is not satisfied and gets angry and so they start to argue. In the following scene Diligence opens parliament and King Humanitie, Correction, the king’s courtiers and the virtues enter. The three estates greet the king and parliament is opened. John Commonweal stands up and talks to the King and Correction. He reveals all the failures of the estates. In the course of the following hearing Temporalitie gets punished but as this estate wants to cooperate this is just a short episode. Spiritualitie does not agree on what is said about their estate and fights back. But there are too many accusations against this estate and therefore they also have to give in. The three Vices are imprisoned and sentenced to be hanged. Flatterie tried to get away by betraying his fellows Falsehood and Deceit but this did not work. In the end of the second part the three vices Deceit, Falsehood and Flatterie are allowed to say something before they are hanged. After the execution of the vices and a rousing speech by Folie, Diligence closes the play and advises the audience to go their ways and enjoy their time.\n\nThe 1931 edition of Lindsay's works by Douglas Hamer hypothesized different forms of the play. The critic John MacQueen proposed the play might have composed by Lindsay as early as 1532 for the court of the young James V of Scotland. An early form of the play is recorded in the royal treasurer's accounts and an English agent's report to Thomas Cromwell. This short play or 'interlude' performed in January 1540 used characters who later appeared in the \"Satyre of the Thrie Estaitis\", and had the same themes.\n\nA letter written by the Englishman William Eure to Thomas Cromwell on 26 January 1540 gives a description of the interlude. Eure, a Border Warden and Privy Councillor, had spoken to Sir Thomas Bellenden at Coldstream, who described the performance at Linlithgow Palace before James, his wife Mary of Guise and his bishops and council on the feast of the Epiphany. As the play turned on the Reformation of the church, Eure obtained a more detailed description from a Scottish contact who saw the play at Linlithgow, and enclosed in his letter the synopsis written by his spy . This description corresponds with the expanded later text of Lindsay's play. A king was shown with his courtiers, Placebo, Picthanke, and Flatterye. A Poor Man made his complaint, and was answered by a Burgess, a Man at Arms and a Bishop, who represented the three estates of the Parliament of Scotland. The Poor Man mentioned the real events of James V executing both John Armstrong (of Staplegordon; in ballads the Laird o'Gilnockie), hanged in July 1530, and 'Sym the Laird,' who was hanged in February 1536. The role of the poor man was described in the spy's synopsis;\"After them come a poor Man, who did go up and down the scaffald, making a heavy complaint that he was harried (chased) through the Courtier's place, where through he hade strayled (lost) his house, his wife and children beggyng thair bread, and so of many thousand in Scotland, whiche would make the Kyng's Grace lose of men if his Grace stod neide (required), saying there was no remedy to be gotten, for though he would suite to the King's Grace, he was neither acquainted with Controller nor Treasurer, and without them might no man get no goodness of the King. And after, he spered (asked) for the King, and when he was shewed the Man that was King in the play, he answered and said he was no King, for there was but one King, which made all and governethe all, who is eternal, to whom he and all earthly Kings are but officers, of the which they must make reckoning. And so forth much more to that effect. And then he looked to the King, and said he was not the King of Scotland, for there was another King in Scotland that hanged John Armestrang with his fellowes, and Sym the Larde, and many other more, which had pacified the country, and stanched theft, but he had left one thing undone, which pertained as well to his charge as th'other. And when he was asked what that was, he made a long narration of the oppression of the poor, by the taking of the 'corse presaunte beists' (animals due as tithes at funerals), and of the harrying of poor men by Consistory law, and of many other abussions of the spiritualitie and Churche, with many long stories and authorities.\"\n\nEure said he had talked with Bellenden, a member of the council of James V of Scotland about the possibility of a Reformation of the 'spirituality' in Scotland. The play at Linlithgow had shown the 'naughtiness' of the church. Bellenden said after the play the King spoke to the churchmen in the audience asking them to reform their factions and manner of living, otherwise he would send six of them into England to his uncle, Henry VIII.\n\nThe performance at Cupar on 7 June 1552 was heralded by a short piece called the \"Cupar Banns\" announcing the play, presumably also written by Lindsay. This has three sections of comic drama as a foretaste of the \"Satire\"; the Cotter and his wife, Bessy and the Auld Man, and Fynlaw of the Foot Band, introduced by the 'Nuncius' and linked by the Fool. The characters of the three parts are supposed to be members of the \"Satire's\" audience. The \"Banns\" with some stage directions are found only in the \"Bannatyne Manuscript.\"\n\nSome preparations for the Edinburgh performance on Sunday 14 August 1554 were made by the Burgh Council. William MacDowall with six carpenters built a stage of boards, a seat for Mary of Guise and the French ambassador Henri Cleutin, and a 'Convoy House', at the Greenside playfield, with the gallows, 'jebbettis,' used in the final scene. The town council paid the wages of 12 minstrels, and after the play treated the actors to dinner.\n\nThe printer Henry Charteris mentioned the Edinburgh performance in his introduction to Lindsay's \"Warkis\" (1568), saying how the clergy were surprised by the play and considered taking revenge. Charteris gave this summary of the \"Satire\";\"In the play, playit beside Edinburgh, in the presence of the Quene Regent, and ane greit part of the nobilitie, with ane exceeding greit nowmber of pepill. lestand fra 9 houris afoir none till 6 houris at evin, quhair, amangis mony baith grave materis and merie trickis, he brocht in ane Bischop, ane Persone (Parson), ane Freir, and ane Nun, deckit up in their papisticall ornamentis and maner of raiment. And theirefter broicht in King Correction, quha reformand sindie deformities in his realme, passit to the tryall of the Clergie. And findand thame to be altogether Idiotis, unworthie of ony functioun ecclesiasticall, dicernit thame to be degradit of their dignateis, and spulzeit (deprived) of their offices, quhilk beand executit, thay war fund bot verray fulis, hypocrites, flatteris & nouchtie persones.\"\n\nThe \"Bannatyne Manuscript\" contains only selected \"merry interludes\" from the 1554 Greenside performance, the copyist George Bannatyne omitted the \"grave matter\" because the church had been reformed in reality in the 1560 Scottish Reformation Parliament, and he noted, \"the samyne abuse is weill reformit in Scotland.\" Stage directions in the \"Bannatyne Manuscript\" mention the settings of \"houses\", the \"King's seat\" and \"palyeoun\" tent, and props for the scene of the Poor Man and the Pardoner, \"Heir thay feight togeddir and the puir man sall cast doun the burd and cast the rillickis in the watter.\"\n\nThe play's first complete modern production occurred on August 24, 1948, at the Edinburgh Festival with a modernised text by Robert Kemp and directed by Tyrone Guthrie, featuring Stanley Baxter.\n\nSimon Callow and Fulton Mackay acted in a 1973 Edinburgh Festival production.\nMary McCluskey directed a performance by young people in July 1996 as part of Scottish Youth Theatre's Summer Festival. The script was translated into modern Scots by Fiona McGarry, and the play was performed in the round in The Cottier Theatre, Glasgow, with an original score.\n\nJohn McGrath adapted the play as a contemporary morality \"A Satire of the Four Estaites\", which was presented by Wildcat Theatre Company at the Edinburgh International Conference Centre as part of the Edinburgh International Festival, also in 1996. This production opened on 16 August 1996 and starred Sylvester McCoy.\n\nThe play was quoted at the opening of the new Scottish Parliament, a mark illustrating its importance to modern Scots.\n\nA new performance at Linlithgow Palace and Stirling Castle based on the story of the 1540 interlude took place in 2013 using a cast drawn from stage and screen. In Linlithgow an open-air stage was erected on the Peel looking out across the loch for the performance.\n\nThe \"Satire\" is notable for being one of the earliest recorded instances of \"fuck,\" predating any English language forms but preceded in the Scots language by the makar William Dunbar (Oxford English Dictionary entry.)\n\nA complete version of the play was printed in 1602, see external links for an edition of the text. In this extract Diligence meets the Pauper, who begins his complaint, including the practice of the parish priest claiming livestock at funerals which was mentioned in the 1540 interlude, (Lines 1954-2028);\n\nDILIGENCE:<br>Swyith begger bogill, haist the away,<br>\nThow art over pert to spill our play.\n\nPAUPER:<br>I wil not gif for al ȝour play worth an sowis fart,<br>\nFor thair is richt lytill play at my hungrie hart.\n\nDILIGENCE: Quhat Devill ails this cruckit carle?\n\nPAUPER:<br>Marie Meikill sorrow :<br>\nI can not get, thocht I gasp, to beg, nor to borrow\n\nDILIGENCE: Quhair deuill is this thou dwels or quhats thy intent?\n\nPAUPER: I dwell into Lawthiane ane myle fra Tranent.\n\nDILIGENCE: Quhair wald thou be, carle, the suth to me shaw?\n\nPAUPER: Sir, evin to Sanct-Androes for to seik law.\n\nDILIGENCE: For to seik law in Edinburgh was the neirest way.\n\nPAUPER:<br>Sir I socht law thair this monie deir day;<br>\nBot I culd get nane at sessioun nor Seinȝe :<br>\nThairfoir the mekill dum Deuill droun all the meinȝe.\n\nDILIGENCE:<br>Shaw me thy mater, man, with al the circumstances,<br>\nHow that thou hes happinit on thir vnhappie chances.\n\nPAUPER:<br>Gude-man will ȝe gif me ȝour Charitie,<br>\nAnd I sall declair how the black veritie.<br>\nMy father was ane auld man and ane hoir,<br>\nAnd was of age fourscoir of ȝeirs and moir;<br>\nAnd Mald, my mother was fourscoir and fyfteine :<br>\nAnd with my labour I did thame baith sustein.<br>\nWee had ane Meir, that caryit salt and coill,<br>\nAnd everie ilk ȝeir scho brocht vs hame ane foill.<br>\nWee had thrie ky that was baith fat and fair,<br>\nNane tydier into the toun of Air.<br>\nMy father was sa waik of blude and bane,<br>\nThat he deit, quhairfoir my mother maid great maine.<br>\nThen scho deit within ane day or two ;<br>\nAnd thair began my povertie and wo.<br>\nOur gude gray Meir was baittand on the feild,<br>\nAnd our Lands Laird tuike hir for his hyreild.<br>\nThe Vickar tuik the best Cow be the head,<br>\nIncontinent, quhen my father was deid.<br>\nAnd quhen the Vickar hard tel how that my mother<br>\nWas dead, fra-hand he tuke to him ane vther.<br>\nThen meg my wife did murne both evin & morrow<br>\nTill at the last scho deit for verrie sorow :<br>\nAnd quhen the Vickar hard tell my wyfe was dead,<br>\nThe third cow he cleikit be the head.<br>\nThair vmest clayis, that was of rapploch gray,<br>\nThe Vickar gart his Clark bear them away.<br>\nQuhen all was gaine, I micht mak na debeat,<br>\nBot with my bairns past for till beg my meat.<br>\nNow haue I tald ȝow the black veritie,<br>\nHow I am brocht into this miserie.\n\nDILIGENCE: How did the person, was he not thy gude friend?\n\nPAUPER:<br>The devil stick him, he curst me for my teind,<br>\nAnd halds me ȝit vnder that same proces,<br>\nThat gart me want the Sacrament at Pasche.<br>\nIn gude faith, sir, Thocht he wald cut my throt,<br>\nI haue na geir except ane Inglis grot,<br>\nQuhilk I purpois to gif ane man of law.\n\nDILIGENCE:<br>Thou art the daftest fuill that ever I saw.<br>\nTrows thou, man, be the law to get remeid<br>\nOf men of kirk? Na, nocht till thou be deid.\n\nPAUPER:\nSir, be quhat law tell me, quhairfoir, or quhy<br>\nThat ane Vickar sould tak fra me thrie ky?\n\nDILIGENCE:<br>Thay haue na law, exceptand consuetude,<br>Quhilk law to them is sufficient and gude.\n\nPAUPER:<br>Ane consuetude against the common weill<br>\nSould be na law I think be sweit Sanct Geill.<br>\nQuhair will ȝe find that law tell gif ȝe can<br>\nTo tak thrie ky fra ane pure husband man?<br>\nAne for my father, and for my wyfe ane vther,<br>\nAnd the third Cow he tuke for Mald my mother.\n\nDILIGENCE:<br>It is thair law all that thay haue in vse,<br>\nThocht it be Cow, Sow, Ganar, Gryce, or Guse.\n\nPAUPER:<br>Sir, I wald speir at ȝow ane questioun.<br>\nBehauld sum Prelats of this Regioun:<br>\nManifestlie during thair lustie lyvfis,<br>\nThay swyfe Ladies, Madinis and vther mens wyfis.<br>\nAnd sa thair cunts thay haue in consuetude.<br>\nQuhidder say ȝe that law is evill or gude?\n\n\n"}
{"id": "6679056", "url": "https://en.wikipedia.org/wiki?curid=6679056", "title": "A priori and a posteriori", "text": "A priori and a posteriori\n\nThe Latin phrases a priori ( \"from the earlier\") and a posteriori ( \"from the later\") are philosophical terms of art popularized by Immanuel Kant's \"Critique of Pure Reason\" (first published in 1781, second edition in 1787), one of the most influential works in the history of philosophy. However, in their Latin forms they appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nThese terms are used with respect to reasoning (epistemology) to distinguish \"necessary conclusions from first premises\" (i.e., what must come before sense observation) from \"conclusions based on sense observation\" which must follow it. Thus, the two kinds of knowledge, justification, or argument, may be glossed:\n\n\nThere are many points of view on these two types of knowledge, and their relationship gives rise to one of the oldest problems in modern philosophy.\n\nThe terms \"a priori\" and \"a posteriori\" are primarily used as adjectives to modify the noun \"knowledge\" (for example, \"\"a priori\" knowledge\"). However, \"a priori\" is sometimes used to modify other nouns, such as \"truth\". Philosophers also may use \"apriority\" and \"aprioricity\" as nouns to refer (approximately) to the quality of being \"a priori\".\n\nAlthough definitions and use of the terms have varied in the history of philosophy, they have consistently labeled two separate epistemological notions. See also the related distinctions: deductive/inductive, analytic/synthetic, necessary/contingent.\n\nThe intuitive distinction between \"a priori\" and \"a posteriori\" knowledge (or justification) is best seen via examples, as below:\n\n\nSeveral philosophers reacting to Kant sought to explain \"a priori\" knowledge without appealing to, as Paul Boghossian (MD) explains, \"a special faculty ... that has never been described in satisfactory terms.\" One theory, popular among the logical positivists of the early 20th century, is what Boghossian calls the \"analytic explanation of the a priori.\" The distinction between analytic and synthetic propositions was first introduced by Kant. While Kant's original distinction was primarily drawn in terms of conceptual containment, the contemporary version of the distinction primarily involves, as the American philosopher W. V. O. Quine put it, the notions of \"true by virtue of meanings and independently of fact.\" \"Analytic\" propositions are thought to be true in virtue of their meaning alone, while \"a posteriori analytic\" propositions are thought to be true in virtue of their meaning \"and\" certain facts about the world. According to the analytic explanation of the \"a priori\", all \"a priori\" knowledge is analytic; so \"a priori\" knowledge need not require a special faculty of pure intuition, since it can be accounted for simply by one's ability to understand the meaning of the proposition in question. In short, proponents of this explanation claimed to have reduced a dubious metaphysical faculty of pure reason to a legitimate linguistic notion of analyticity.\n\nHowever, the analytic explanation of \"a priori\" knowledge has undergone several criticisms. Most notably, Quine argued that the analytic–synthetic distinction is illegitimate. Quine states: \"But for all its a priori reasonableness, a boundary between analytic and synthetic statements simply has not been drawn. That there is such a distinction to be drawn at all is an unempirical dogma of empiricists, a metaphysical article of faith.\" While the soundness of Quine's critique is highly disputed, it had a powerful effect on the project of explaining the \"a priori\" in terms of the analytic.\n\nThe metaphysical distinction between necessary and contingent truths has also been related to \"a priori\" and \"a posteriori\" knowledge. A proposition that is \"necessarily true\" is one whose negation is self-contradictory (thus, it is said to be true in every possible world). Consider the proposition that all bachelors are unmarried. Its negation, the proposition that some bachelors are married, is incoherent, because the concept of being unmarried (or the meaning of the word \"unmarried\") is part of the concept of being a bachelor (or part of the definition of the word \"bachelor\"). To the extent that contradictions are impossible, self-contradictory propositions are necessarily false, because it is impossible for them to be true. Thus, the negation of a self-contradictory proposition is supposed to be necessarily true. By contrast, a proposition that is \"contingently true\" is one whose negation is not self-contradictory (thus, it is said that it is \"not\" true in every possible world). As Jason Baehr states, it seems plausible that all necessary propositions are known \"a priori\", because \"[s]ense experience can tell us only about the actual world and hence about what is the case; it can say nothing about what must or must not be the case.\"\n\nFollowing Kant, some philosophers have considered the relationship between aprioricity, analyticity, and necessity to be extremely close. According to Jerry Fodor, \"Positivism, in particular, took it for granted that \"a priori\" truths must be necessary...\" However, since Kant, the distinction between analytic and synthetic propositions had slightly changed. Analytic propositions were largely taken to be \"true by virtue of meanings and independently of fact\", while synthetic propositions were not—one must conduct some sort of empirical investigation, looking to the world, to determine the truth-value of synthetic propositions.\n\nAprioricity, analyticity, and necessity have since been more clearly separated from each other. The American philosopher Saul Kripke (1972), for example, provided strong arguments against this position. Kripke argued that there are necessary \"a posteriori\" truths, such as the proposition that water is HO (if it is true). According to Kripke, this statement is necessarily true (since water and HO are the same thing, they are identical in every possible world, and truths of identity are logically necessary) and \"a posteriori\" (since it is known only through empirical investigation). Following such considerations of Kripke and others (such as Hilary Putnam), philosophers tend to distinguish more clearly the notion of aprioricity from that of necessity and analyticity.\n\nKripke's definitions of these terms, however, diverge in subtle ways from those of Kant. Taking these differences into account, Kripke's controversial analysis of naming as contingent and \"a priori\" would, according to Stephen Palmquist, best fit into Kant's epistemological framework by calling it \"analytic a posteriori\". Aaron Sloman presented a brief defence of Kant's three distinctions (analytic/synthetic, apriori/empirical and necessary/contingent) in . It did not assume \"possible world semantics\" for the third distinction, merely that some part of \"this\" world might have been different.\n\nThus, the relationship between aprioricity, necessity, and analyticity is not easy to discern. However, most philosophers at least seem to agree that while the various distinctions may overlap, the notions are clearly not identical: the \"a priori\"/\"a posteriori\" distinction is epistemological, the analytic/synthetic distinction is linguistic, and the necessary/contingent distinction is metaphysical.\n\nThe phrases \"\"a priori\" and \"a posteriori\"\" are Latin for \"from what comes before\" and \"from what comes later\" (or, less literally, \"from first principles, before experience\" and \"after experience\"). They appear in Latin translations of Euclid's \"Elements\", of about 300 , a work widely considered during the early European modern period as the model for precise thinking.\n\nAn early philosophical use of what might be considered a notion of \"a priori\" knowledge (though not called by that name) is Plato's theory of recollection, related in the dialogue \"Meno\" (380 ), according to which something like \"a priori\" knowledge is knowledge inherent, intrinsic in the human mind.\n\nAlbert of Saxony, a 14th-century logician, wrote on both \"a priori\" and \"a posteriori\".\n\nG. W. Leibniz introduced a distinction between \"a priori\" and \"a posteriori\" criteria for the possibility of a notion in his (1684) short treatise \"Meditations on Knowledge, Truth, and Ideas\". \"A priori\" and \"a posteriori\" arguments for the existence of God appear in his \"Monadology\" (1714).\n\nGeorge Berkeley outlined the distinction in his 1710 work \"A Treatise Concerning the Principles of Human Knowledge\" (para. XXI).\n\nThe 18th-century German philosopher Immanuel Kant (1781) advocated a blend of rationalist and empiricist theories. Kant says, \"Although all our cognition begins with experience, it does not follow that it arises [is caused by] from experience\" According to Kant, \"a priori\" cognition is transcendental, or based on the \"form\" of all possible experience, while \"a posteriori\" cognition is empirical, based on the \"content\" of experience. Kant states, \"[…] it is quite possible that our empirical knowledge is a compound of that which we receive through impressions, and that which the faculty of cognition supplies from itself sensuous impressions [sense data] giving merely the \"occasion\" [opportunity for a cause to produce its effect].\" Contrary to contemporary usages of the term, Kant thinks that \"a priori\" knowledge is not entirely independent of the content of experience. And unlike the rationalists, Kant thinks that \"a priori\" cognition, in its pure form, that is without the admixture of any empirical content, is limited to the deduction of the conditions of possible experience. These \"a priori\", or transcendental conditions, are seated in one's cognitive faculties, and are not provided by experience in general or any experience in particular (although an argument exists that \"a priori\" intuitions can be \"triggered\" by experience). \n\nKant nominated and explored the possibility of a transcendental logic with which to consider the deduction of the \"a priori\" in its pure form. Space, time and causality are considered pure \"a priori\" intuitions. Kant reasoned that the pure \"a priori\" intuitions are established via his transcendental aesthetic and transcendental logic. He claimed that the human subject would not have the kind of experience that it has were these \"a priori\" forms not in some way constitutive of him as a human subject. For instance, a person would not experience the world as an orderly, rule-governed place unless time, space and causality were determinant functions in the form of perceptual faculties, i. e., there can be no experience in general without space, time or causality as particular determinants thereon. The claim is more formally known as Kant's transcendental deduction and it is the central argument of his major work, the \"Critique of Pure Reason\". The transcendental deduction argues that time, space and causality are ideal as much as real. In consideration of a possible logic of the \"a priori\", this most famous of Kant's deductions has made the successful attempt in the case for the fact of subjectivity, what constitutes subjectivity and what relation it holds with objectivity and the empirical.\n\nAfter Kant's death, a number of philosophers saw themselves as correcting and expanding his philosophy, leading to the various forms of German Idealism. One of these philosophers was Johann Fichte. His student (and critic), Arthur Schopenhauer, accused him of rejecting the distinction between \"a priori\" and \"a posteriori\" knowledge:\n\n\n\n\n"}
{"id": "555979", "url": "https://en.wikipedia.org/wiki?curid=555979", "title": "Auto-antonym", "text": "Auto-antonym\n\nAn auto-antonym or autantonym, also called a contronym or contranym, is a word with multiple meanings (senses) of which one is the reverse of another. For example, the word \"cleave\" can mean \"to cut apart\" or \"to bind together\". This phenomenon is called enantiosemy, enantionymy or antilogy (\"enantio-\" means \"opposite\"). An enantiosemic term is necessarily polysemic.\n\nThe terms \"autantonym\" and \"contronym\" were coined by Joseph Twadell Shipley in 1960 and Jack Herring in 1962, respectively. An auto-antonym is alternatively called an antagonym, Janus word (after the Roman god with two faces), enantiodrome, self-antonym, antilogy, or addad (Arabic, singular didd).\n\nSome pairs of contronyms are true homographs, i.e., distinct words with different etymology which happen to have the same form. For instance \"cleave\" \"separate\" is from Old English \"clēofan\", while \"cleave\" \"adhere\" is from Old English \"clifian\", which was pronounced differently. The King James Bible often uses \"let\" in the sense of \"forbid\", a meaning which is now uncommon, and which is derived from the Old English verb \"lettan\" 'hinder, delay, impede, oppress', as opposed to the meaning \"allow\", which is derived from the Old English verb \"lǣtan\" 'leave, allow, let on lease (etc.)'. Still, the alternate meaning of \"let\" can be found today in the legal phrase \"without let or hindrance\" and in ball games such as tennis, squash, table tennis, and racquetball.\n\nOther contronyms are a form of polysemy, but where a single word acquires different and ultimately opposite definitions. For example, \"sanction\"—\"permit\" or \"penalize\"; \"bolt\" (originally from crossbows)—\"leave quickly\" or \"fix/immobilize\"; \"fast\"—\"moving rapidly\" or \"unmoving\". Some English examples result from nouns being verbed in the patterns of \"add <noun> to\" and \"remove <noun> from\"; e.g. \"dust\", \"seed\", \"stone\". Denotations and connotations can drift or branch over centuries. An apocryphal story relates how Charles II (or sometimes Queen Anne) described St Paul's Cathedral (using contemporaneous English) as \"awful, pompous, and artificial,\" with the meaning (rendered in modern English) of \"awe-inspiring, majestic, and ingeniously designed.\" Negative words such as \"bad\" and \"sick\" sometimes acquire ironic senses referring to traits that are impressive and admired, if not necessarily positive (\"that outfit is bad as hell\"; \"lyrics full of sick burns\").\n\nSome contronyms result from differences in varieties of English. For example, to \"table\" a bill means \"to put it up for debate\" in British English, while it means \"to remove it from debate\" in American English (where British English would have \"shelve\"). To \"barrack\" for anyone in Australian English is to loudly demonstrate your support, while it expresses disapproval and contempt in American English and British English.\n\nSome words contain simultaneous opposing or competing meanings in the same context, rather than alternative meanings in different contexts; examples include blend words such as \"coopetition\" (meaning a murky blend of cooperation and competition), \"frenemy\" (meaning a murky blend of friend and enemy), \"interdependence\", \"glocalization\" etc. These are not usually classed as contronyms, but they share the theme of containing opposing meanings.\n\nAuto-antonyms exist in many languages, as the following examples show.\n\nIn Latin, \"sacer\" has the double meaning \"sacred, holy\" and \"accursed, infamous\". Greek δημιουργός gave Latin its \"demiurgus\", from which English got its \"demiurge\", which can refer either to God as the creator or to the devil, depending on philosophical context.\n\nIn many languages, a word stem associated with a single event may treat the action of that event as unitary, so it can refer to any of the doings or persons on either side of the transaction, that is, to the action of either the subject or the object, or to either the person who does something or the person to whom (or for whom) it is done. Other cues nail down the aspects of subject versus object. Thus there is a simple logic involved, despite that discussions of such words sometimes fixate on a superficial appearance of illogic (that is, \"how can one word mean both?!\"). Examples: \n\nContemporary English has some auto-antonym adjectives that are used for evaluating: \"sick\" can mean \"ill\" or \"extremely good\"; \"bad\" can mean \"bad\" or \"excellent\"; words like \"cool\" or \"in\" may be evaluate something that is \"warm\" or \"out\", respectively (\"This new fireplace is cool.\" or \"Outdoor activities are in, now.\"). We may hear evaluations like \"pretty ugly\", \"ridiculously serious\", \"insanely rational\", \"incredibly trustworthy\", \"impossibly doable\", \"super sub-par\".\n\nAmong profanities, adjective or article decides its meaning, for example:\n\nSeeming auto-antonyms can occur from translation. In Hawaiian, for example, \"aloha\" is translated both as “hello” and as “goodbye”, but the essential meaning of the word is \"love,\" whether used as a greeting or farewell. The Italian greeting \"ciao\" is translated as \"hello\" or \"goodbye\" depending on the context; however, the original meaning was “(I'm your) slave.\"\n\n\n\n"}
{"id": "9788270", "url": "https://en.wikipedia.org/wiki?curid=9788270", "title": "Automated species identification", "text": "Automated species identification\n\nAutomated species identification is a method of making the expertise of taxonomists available to ecologists, parataxonomists and others via computers and other digital technology through artificial intelligence.\n\nThe automated identification of biological objects such as insects (individuals) and/or groups (e.g., species, guilds, characters) has been a dream among systematists for centuries. The goal of some of the first multivariate biometric methods was to address the perennial problem of group discrimination and inter-group characterization. Despite much preliminary work in the 1950s and '60s, progress in designing and implementing practical systems for fully automated object biological identification has proven frustratingly slow. As recently as 2004 Dan Janzen \n\nupdated the dream for a new audience:\n\nThe spaceship lands. He steps out. He points it around. It says ‘friendly–unfriendly—edible–poisonous—safe– dangerous—living–inanimate’. On the next sweep it says ‘\"Quercus oleoides—Homo sapiens—Spondias mombin—Solanum nigrum—Crotalus durissus—Morpho peleides\"—serpentine’. This has been in my head since reading science fiction in ninth grade half a century ago.\n\nJanzen’s preferred solution to this classic problem involved building machines to identify species from their DNA. His predicted budget and proposed research team is “US$1 million and five bright people.” However, recent developments in computer architectures, as well as innovations in software design, have placed the tools needed to realize Janzen’s vision in the hands of the systematics and computer science community not in several years hence, but now; and not just for creating DNA barcodes, but also for identification based on digital images.\n\nA seminal survey published in 2004, studies why automated species identification had not become widely employed at this time and whether it would be a realistic option for the future. The authors found that \"a small but growing number of studies sought to develop automated species identification systems based on morphological characters\". An overview of 20 studies analyzing species’ structures, such as cells, pollen, wings, and genitalia, shows identification success rates between 40% and 100% on training sets with 1 to 72 species. However, they also identified four fundamental problems with these systems: (1) training sets—were too small (5-10 specimens per species) and their extension especially for rare species may be difficult, (2) errors in identification—are not sufficiently studied to handle them and to find systematics, (3) scaling—studies consider only small numbers of species (<200 species), and (4) novel species — systems are restricted to the species they have been trained for and will classify any novel observation as one of the known species.\n\nA survey published in 2017 systematically compares and discusses progress and findings towards automated plant species identification within the last decade (2005–2015). 120 primary studies have been published in high-quality venues within this time, mainly by authors with computer science background. These studies propose a wealth of computer vision approaches, i.e., features reducing the high-dimensionality of the pixel-based image data while preserving the characteristic information as well as classification methods. The vast majority of these studies analyzes leaves for identification, while only 13 studies propose methods for flower-based identification. The reasons being that leaves can easier be collected and imaged and are available for most of the year. Proposed features capture generic object characteristic, i.e., shape, texture, and color as well as leaf-specific characteristics, i.e., venation and margin. The majority of studies still used datasets for evaluation that contained no more than 250 species. However, there is progress in this regard, one study uses a dataset with >2k and another with >20k species.\n\nThese developments could not have come at a better time. As the taxonomic community already knows, the world is running out of specialists who can identify the very biodiversity whose preservation has become a global concern. In commenting on this problem in palaeontology as long ago as 1993, Roger Kaesler recognized:\n\n“… we are running out of systematic palaeontologists who have anything approaching synoptic knowledge of a major group of organisms … Palaeontologists of the next century are unlikely to have the luxury of dealing at length with taxonomic problems … Palaeontology will have to sustain its level of excitement without the aid of systematists, who have contributed so much to its success.”This expertise deficiency cuts as deeply into those commercial industries that rely on accurate identifications (e.g., agriculture, biostratigraphy) as it does into a wide range of pure and applied research programmes (e.g., conservation, biological oceanography, climatology, ecology). It is also commonly, though informally, acknowledged that the technical, taxonomic literature of all organismal groups is littered with examples of inconsistent and incorrect identifications. This is due to a variety of factors, including taxonomists being insufficiently trained and skilled in making identifications (e.g., using different rules-of-thumb in recognizing the boundaries between similar groups), insufficiently detailed original group descriptions and/or illustrations, inadequate access to current monographs and well-curated collections and, of course, taxonomists having different opinions regarding group concepts. Peer review only weeds out the most obvious errors of commission or omission in this area, and then only when an author provides adequate representations (e.g., illustrations, recordings, and gene sequences) of the specimens in question.\n\nSystematics too has much to gain, both practically and theoretically, from the further development and use of automated identification systems. It is now widely recognized that the days of systematics as a field populated by mildly eccentric individuals pursuing knowledge in splendid isolation from funding priorities and economic imperatives are rapidly drawing to a close. In order to attract both personnel and resources, systematics must transform itself into a “large, coordinated, international scientific enterprise” \nMany have identified use of the Internet— especially via the World Wide Web — as the medium through which this transformation can be made. While establishment of a virtual, GenBank-like system for accessing morphological data, audio clips, video files and so forth would be a significant step in the right direction, improved access to observational information and/or text-based descriptions alone will not address either the taxonomic impediment or low identification reproducibility issues successfully. Instead, the inevitable subjectivity associated with making critical decisions on the basis of qualitative criteria must be reduced or, at the very least, embedded within a more formally analytic context.\n\nProperly designed, flexible, and robust, automated identification systems, organized around distributed computing architectures and referenced to authoritatively identified collections of training set data (e.g., images, and gene sequences) can, in principle, provide all systematists with access to the electronic data archives and the necessary analytic tools to handle routine identifications of common taxa. Properly designed systems can also recognize when their algorithms cannot make a reliable identification and refer that image to a specialist (whose address can be accessed from another database). Such systems can also include elements of artificial intelligence and so improve their performance the more they are used. Most tantalizingly, once morphological (or molecular) models of a species have been developed and demonstrated to be accurate, these models can be queried to determine which aspects of the observed patterns of variation and variation limits are being used to achieve the identification, thus opening the way for the discovery of new and (potentially) more reliable taxonomic characters.\n\n\n\nHere are some links to the home pages of species identification systems. The SPIDA and DAISY system are essentially generic and capable of classifying any image material presented. The ABIS and DrawWing system are restricted to insects with membranous wings as they operate by matching a specific set of characters based on wing venation.\n"}
{"id": "10661427", "url": "https://en.wikipedia.org/wiki?curid=10661427", "title": "Blogger's Code of Conduct", "text": "Blogger's Code of Conduct\n\nThe Blogger's Code of Conduct is a proposal by Tim O'Reilly for bloggers to enforce civility on their blogs by being civil themselves and moderating comments on their blog. The code was proposed in 2007 due to threats made to blogger Kathy Sierra. The idea of the code was first reported by BBC News, who quoted O'Reilly saying, \"I do think we need some code of conduct around what is acceptable behaviour, I would hope that it doesn't come through any kind of regulation it would come through self-regulation.\".\n\nIn Ireland the proposal for a code was raised in an article in Sunday Business Post in 2009 by Simon Palmer, a radio presenter and PR consultant in Dublin, after false details in relation to a client had appeared on Irish blogs Time To Raise Above Blog Standard. After his comments he was subjected to sustained on line abuse from Irish bloggers and anonymous trolls and even received death threats.\n\nIn Nepal, 10 prominent bloggers signed a Code of Ethics for Bloggers, first proposed by Ujjwal Acharya and finalized after discussion among bloggers, on July 27, 2011.\n\nAccording to \"The New York Times\", O'Reilly and Jimmy Wales based their preliminary list on one developed by the BlogHer women's blogging support network and, working with others, came up with a list of seven proposed ideas:\n\nReaction to the proposal was internationally widespread among bloggers and media writers. According to the \"San Francisco Chronicle\", the blogosphere described it as \"excessive, unworkable and an open door to censorship.\" Author Bruce Brown approved of the code, reproducing in his book on blogging. TechCrunch founder Michael Arrington and entrepreneur and blogger Dave Winer were two notable Americans who wrote against the plan. Technology blogger Robert Scoble stated that the proposed rules “make me feel uncomfortable” and “As a writer, it makes me feel like I live in Iran.”\n\n"}
{"id": "550138", "url": "https://en.wikipedia.org/wiki?curid=550138", "title": "Braid group", "text": "Braid group\n\nIn mathematics, the braid group on strands (denoted ), also known as the Artin braid group, is the group whose elements are equivalence classes of -braids (e.g. under ambient isotopy), and whose group operation is composition of braids (see ). Example applications of braid groups include knot theory, where any knot may be represented as the closure of certain braids (a result known as Alexander's theorem); in mathematical physics where Artin's canonical presentation of the braid group corresponds to the Yang–Baxter equation (see ); and in monodromy invariants of algebraic geometry.\n\nIn this introduction let ; the generalization to other values of will be straightforward. Consider two sets of four items lying on a table, with the items in each set being arranged in a vertical line, and such that one set sits next to the other. (In the illustrations below, these are the black dots.) Using four strands, each item of the first set is connected with an item of the second set so that a one-to-one correspondence results. Such a connection is called a \"braid\". Often some strands will have to pass over or under others, and this is crucial: the following two connections are \"different\" braids:\n\nOn the other hand, two such connections which can be made to look the same by \"pulling the strands\" are considered \"the same\" braid:\n\nAll strands are required to move from left to right; knots like the following are \"not\" considered braids:\n\nAny two braids can be \"composed\" by drawing the first next to the second, identifying the four items in the middle, and connecting corresponding strands:\n\nAnother example:\n\nThe composition of the braids and is written as .\n\nThe set of all braids on four strands is denoted by . The above composition of braids is indeed a group operation. The identity element is the braid consisting of four parallel horizontal strands, and the inverse of a braid consists of that braid which \"undoes\" whatever the first braid did, which is obtained by flipping a diagram such as the ones above across a vertical line going through its centre. (The first two example braids above are inverses of each other.)\n\nTo put the above informal discussion of braid groups on firm ground, one needs to use the homotopy concept of algebraic topology, defining braid groups as fundamental groups of a configuration space. Alternatively, one can define the braid group purely algebraically via the braid relations, keeping the pictures in mind only to guide the intuition.\n\nBraid groups were introduced explicitly by Emil Artin in 1925, although (as Wilhelm Magnus pointed out in 1974) they were already implicit in Adolf Hurwitz's work on monodromy from 1891. In fact, as Magnus says, Hurwitz gave the interpretation of a braid group as the fundamental group of a configuration space (cf. braid theory), an interpretation that was lost from view until it was rediscovered by Ralph Fox and Lee Neuwirth in 1962.\n\nConsider the following three braids:\n\nEvery braid in formula_1 can be written as a composition of a number of these braids and their inverses. In other words, these three braids generate the group formula_1. To see this, an arbitrary braid is scanned from left to right for crossings; beginning at the top, whenever a crossing of strands formula_3 and formula_4 is encountered, formula_5 or formula_6 is written down, depending on whether strand formula_3 moves under or over strand formula_4. Upon reaching the right end, the braid has been written as a product of the σ's and their inverses.\n\nIt is clear that\n\nwhile the following two relations are not quite as obvious:\n\n(these relations can be appreciated best by drawing the braid on a piece of paper). It can be shown that all other relations among the braids formula_9, formula_10 and formula_11 already follow from these relations and the group axioms.\n\nGeneralising this example to formula_12 strands, the group formula_13 can be abstractly defined via the following presentation:\n\nwhere in the first group of relations and in the second group of relations, . This presentation leads to generalisations of braid groups called Artin groups. The cubic relations, known as the braid relations, play an important role in the theory of Yang–Baxter equation.\n\n\nBy forgetting how the strands twist and cross, every braid on strands determines a permutation on elements. This assignment is onto, compatible with composition, and therefore becomes a surjective group homomorphism from the braid group into the symmetric group. The image of the braid σ ∈ is the transposition . These transpositions generate the symmetric group, satisfy the braid group relations, and have order 2. This transforms the Artin presentation of the braid group into the Coxeter presentation of the symmetric group:\n\nThe kernel of the homomorphism is the subgroup of called the pure braid group on strands and denoted . In a pure braid, the beginning and the end of each strand are in the same position. Pure braid groups fit into a short exact sequence\n\nThis sequence splits and therefore pure braid groups are realized as iterated semi-direct products of free groups.\n\nThe braid group is the universal central extension of the modular group , with these sitting as lattices inside the (topological) universal covering group\n\nFurthermore, the modular group has trivial center, and thus the modular group is isomorphic to the quotient group of modulo its center, , and equivalently, to the group of inner automorphisms of .\n\nHere is a construction of this isomorphism. Define\n\nFrom the braid relations it follows that . Denoting this latter product as , one may verify from the braid relations that\n\nimplying that is in the center of . Let denote the subgroup of generated by , since , it is a normal subgroup and one may take the quotient group . We claim ; this isomorphism can be given an explicit form. The cosets and map to\n\nwhere and are the standard left and right moves on the Stern–Brocot tree; it is well known that these moves generate the modular group.\n\nAlternately, one common presentation for the modular group is\n\nwhere\n\nMapping to and to yields a surjective group homomorphism .\n\nThe center of is equal to , a consequence of the facts that is in the center, the modular group has trivial center, and the above surjective homomorphism has kernel .\n\nThe braid group can be shown to be isomorphic to the mapping class group of a punctured disk with punctures. This is most easily visualized by imagining each puncture as being connected by a string to the boundary of the disk; each mapping homomorphism that permutes two of the punctures can then be seen to be a homotopy of the strings, that is, a braiding of these strings.\n\nVia this mapping class group interpretation of braids, each braid may be classified as periodic, reducible or pseudo-Anosov.\n\nIf a braid is given and one connects the first left-hand item to the first right-hand item using a new string, the second left-hand item to the second right-hand item etc. (without creating any braids in the new strings), one obtains a link, and sometimes a knot. Alexander's theorem in braid theory states that the converse is true as well: every knot and every link arises in this fashion from at least one braid; such a braid can be obtained by cutting the link. Since braids can be concretely given as words in the generators , this is often the preferred method of entering knots into computer programs.\n\nThe word problem for the braid relations is efficiently solvable and there exists a normal form for elements of in terms of the generators . (In essence, computing the normal form of a braid is the algebraic analogue of \"pulling the strands\" as illustrated in our second set of images above.) The free GAP computer algebra system can carry out computations in if the elements are given in terms of these generators. There is also a package called \"CHEVIE\" for GAP3 with special support for braid groups. The word problem is also efficiently solved via the Lawrence–Krammer representation.\n\nIn addition to the word problem, there are several known hard computational problems that could implement braid groups, applications in cryptography have been suggested.\n\nIn analogy with the action of the symmetric group by permutations, in various mathematical settings there exists a natural action of the braid group on -tuples of objects or on the -folded tensor product that involves some \"twists\". Consider an arbitrary group and let be the set of all -tuples of elements of whose product is the identity element of . Then acts on in the following fashion:\n\nThus the elements and exchange places and, in addition, is twisted by the inner automorphism corresponding to — this ensures that the product of the components of remains the identity element. It may be checked that the braid group relations are satisfied and this formula indeed defines a group action of on . As another example, a braided monoidal category is a monoidal category with a braid group action. Such structures play an important role in modern mathematical physics and lead to quantum knot invariants.\n\nElements of the braid group can be represented more concretely by matrices. One classical such representation is Burau representation, where the matrix entries are single variable Laurent polynomials. It had been a long-standing question whether Burau representation was faithful, but the answer turned out to be negative for . More generally, it was a major open problem whether braid groups were linear. In 1990, Ruth Lawrence described a family of more general \"Lawrence representations\" depending on several parameters. In 1996, C. Nayak and Frank Wilczek posited that in analogy to projective representations of , the projective representations of the braid group have a physical meaning for certain quasiparticles in the fractional quantum hall effect. Around 2001 Stephen Bigelow and Daan Krammer independently proved that all braid groups are linear. Their work used the Lawrence–Krammer representation of dimension depending on the variables and . By suitably specializing these variables, the braid group may be realized as a subgroup of the general linear group over the complex numbers.\n\nThere are many ways to generalize this notion to an infinite number of strands. The simplest way is to take the direct limit of braid groups, where the attaching maps formula_24 send the formula_25 generators of formula_26 to the first formula_25 generators of formula_28 (i.e., by attaching a trivial strand). Fabel has shown that there are two topologies that can be imposed on the resulting group each of whose completion yields a different group. One is a very tame group and is isomorphic to the mapping class group of the infinitely punctured disk—a discrete set of punctures limiting to the boundary of the disk.\n\nThe second group can be thought of the same as with finite braid groups. Place a strand at each of the points formula_29 and the set of all braids—where a braid is defined to be a collection of paths from the points formula_30 to the points formula_31 so that the function yields a permutation on endpoints—is isomorphic to this wilder group. An interesting fact is that the pure braid group in this group is isomorphic to both the inverse limit of finite pure braid groups formula_32 and to the fundamental group of the Hilbert cube minus the set\n\nThe cohomology of a group formula_34 is defined as the cohomology of the corresponding Eilenberg–MacLane classifying space, formula_35, which is a CW complex uniquely determined by formula_34 up to homotopy. A classifying space for the braid group formula_13 is the unordered configuration space of formula_38, that is, the set of formula_12 distinct unordered points in the plane: \nSo by definition\n\nThe calculations for coefficients in formula_42 can be found in .\n\nSimilarly, a classifying space for the pure braid group formula_32 is formula_44, the \"ordered\" configuration space of formula_38. In 1968 Vladimir Arnold showed that the integral cohomology of the pure braid group formula_32 is the quotient of the exterior algebra generated by the collection of degree-one classes formula_47, subject to the relations\n\n\n\n"}
{"id": "44526218", "url": "https://en.wikipedia.org/wiki?curid=44526218", "title": "Circulus (theory)", "text": "Circulus (theory)\n\nCirculus was a socioeconomics doctrine devised by nineteenth-century French utopian socialist Pierre Leroux (1797-1871), who proposed that human excrement be collected by the state in the form of a tax and used as fertiliser, thereby increasing agricultural production sufficiently to prevent Malthusian catastrophe.\n\nAccording to Leroux, human excrement has remarkable fertilising properties, which society fails to harness. The human waste that is otherwise discarded would fertilise agricultural production sufficient to cover society's progressively increasing food consumption. Leroux posits the existence of a natural law dependent on the circulation of human excrement back through the agricultural process and which maintains a necessary balance between soil fertility and inexorable population growth. In the \"Revue de l'ordre social\" (No. 1, 1850, p. 6), Leroux responded to his critics, summing up his theory as follows: \"Si les hommes étaient croyants, savants, religieux, ajoute-t-il, au lieu de rire, comme ils le font, du socialisme, ils professeraient avec respect et vénération la doctrine du circulus. Chacun recueillerait \"religieusement\" son fumier pour le donner à l’état, c’est-à-dire au percepteur, en guise d’impôt ou de contribution personelle. La production agricole serait immédiatement doublée, et la misère disparaîtrait du globe\" (If men were believers, learned, religious, then, rather than laughing at socialism, as they do, they would profess the doctrine of circulus with respect and veneration. Each would religiously collect his dung to give it to the state, that is, to the tax-collector, by way of an impost or personal contribution. Agricultural production would instantly be doubled, and poverty would vanish from the globe).\n\nLeroux first put forward his theory of circulus in 1834, as part of an attempt to explain the failure of a Fourierist phalanstery. In response to Madame Baudet-Dulary, astonished at the failure of the utopian community established on her land, Fourier himself is supposed to have said: \"Madame, donnez-moi fumier\" (Madame, give me dung). Despite public ridicule, Leroux held firm to his theory in the decades that followed and, while living in exile in Jersey after the fall of the Second Republic, he carried out experiments using his own excrement as fertiliser in an attempt to persuade the local authorities to adopt the doctrine of circulus. However, Leroux's theory did gain the sympathetic ear of fellow exile Victor Hugo. In the digression on the sewers of Paris in \"Les Misérables\", Hugo would seem to have been influenced by Leroux's diagnosis of socio-economic ills when he declares that the present sewerage system is \"a misunderstanding\" (\"Un égout est un malentendu\"), a negligent waste (\"coulage\") of public wealth (\"la richesse publique\"), and that were the \"givens of a new social economy\" (\"les données d'une économie sociale nouvelle\") applied to human waste, then the problem of poverty would be attenuated. But in practical terms, rather than by Leroux's vision of individual members of society willingly paying a tax in their own excrement for the sake of the common weal, Hugo's collector sewers were sooner inspired by the ideas of English social reformer Edwin Chadwick, who proposed that human waste be pumped from cities to the countryside to provide fertiliser for crops to feed those very cities, thereby completing \"the circle and [realising] the Egyptian type of eternity by bringing as it were the serpent's tail into the serpent's mouth\".\n"}
{"id": "11517229", "url": "https://en.wikipedia.org/wiki?curid=11517229", "title": "Closed-ended question", "text": "Closed-ended question\n\nA closed-ended question refers to any question for which a researcher provides research participants with options from which to choose a response. Open-ended questions are sometimes phrased as a statement which requires a response.\n\nA closed-ended question contrasts with an open-ended question, which cannot easily be answered with specific information.\n\nExamples of close-ended questions which may elicit a \"yes\" or \"no\" response include:\n\nSimilarly, variants of the above close-ended questions which possess specific responses are:\n\nAt the same time, there are closed-ended questions which are sometimes impossible to answer correctly with a yes or no without confusion, for example: \"Have you stopped taking heroin?\" (if you never took it) or \"Who told you to take heroin?\"; see \"loaded question\".\n\nA study by the University of Cincinnati found 20 to 40 percent of Americans will provide an opinion when they do not have one because of social pressure, using context clues to select an answer they believe will please the questioner. A classic example of this phenomenon was the 1947 study of the fictional Metallic Metals Act.\n\nSome in the field of education argue that closed-ended questions are broadly speaking \"bad\" questions. They are questions that are often asked to obtain a specific answer and are therefore good for testing knowledge. It is often argued that open-ended questions (i.e. questions that elicit more than a yes/no answers) are preferable because they open up discussion and enquiry.\n\nPeter Worley argues that this is a false assumption. This is based on Worley’s central arguments that there are two different kinds of open and closed questions: grammatical and conceptual. He argues that educational practitioners should be aiming for questions that are \"grammatically closed, but conceptually open\". For example, in standard parlance, \"is it ever right to lie?\" would be regarded as a closed question: it elicits a yes–no response. Significantly, however, it is conceptually open. Any initial yes–no answer to it can be \"opened up\" by the questioner (\"why do you think that?\", \"Could there be an instance where that's not the case?), inviting elaboration and enquiry.\n\nThis grammatically closed but cognitively open style of questioning, Worley argues, \"gives [educators] the best of both worlds: the focus and specificity of a closed question (this, after all, is why teachers use them) and the inviting, elaborating character of an open question\". Closed questions, simply require \"opening up\" strategies to ensure that conceptually open questions can fulfil their educational potential.\n\nWorley's structural and semantic distinction between open and closed questions is integral to his pedagogical invention \"Open Questioning Mindset\" (OQM). OQM refers to the development, in educators, of an open attitude towards the process of learning and the questioning at the heart of that process. It is a mind-set that is applicable to all subject areas and all pedagogical environments. Teachers who develop an Open Questioning Mindset listen openly for the cognitive content of student's contributions and looks for ways to use what is given for learning opportunities, whether right, wrong, relevant or apparently irrelevant. OQM encourages a style of pedagogy that values genuine enquiry in the classroom. It provides teachers with the tools to move beyond what Worley calls \"guess what's in my head\" teaching, that relies on closed and leading questions.\n\n"}
{"id": "42714893", "url": "https://en.wikipedia.org/wiki?curid=42714893", "title": "Culture of Peace News Network", "text": "Culture of Peace News Network\n\nThe Culture of Peace News Network is a United Nations authorized interactive online network, committed to supporting the global movement for a culture of peace and nonviolence. The network commenced under the auspices of UNESCO, as part of the International Year for the Culture of Peace. The United Nations General Assembly in 2009 further endorsed the work of the network, in resolution A/RES/64/80.\n\nCPNN has grown in scope in recent years. As of 2018, the CPNN website is updated more or less daily (30-50 articles per month) with articles promoting at least one of the eight program areas of the culture of peace as defined in the United Nations Declaration and Programme of Action on a Culture of Peace : Education for Peace, Human Rights, Sustainable Development, Equality of Women, Democratic Participation, Free Flow of Information, Tolerance/Solidarity, Disarmament/Security. CPNN has English, French and Spanish/Portuguese sections. Articles in the French section or in the Spanish/Portuguese section are always paired with a translation in the English section so that all articles are available in English. \n\nAt the beginning of each month a bulletin, available on the CPNN website, summarizes the major developments for a culture of peace and is sent by email to mailing lists in English, French and Spanish.\n\nCPNN is an all-volunteer initiative; additional reporters are encouraged. \n\n"}
{"id": "1545612", "url": "https://en.wikipedia.org/wiki?curid=1545612", "title": "Culture of fear", "text": "Culture of fear\n\nPopularized by the American sociologist Barry Glassner, culture of fear (or climate of fear) is the concept that people may incite fear in the general public to achieve political or workplace goals through emotional bias.\n\nA largely unrelated concept in sociology is the \"fear culture\" on the Guilt-Shame-Fear spectrum of cultures.\n\nAshforth discussed potentially destructive sides of leadership and identified what he referred to as petty tyrants: leaders who exercise a tyrannical style of management, resulting in a climate of fear in the workplace. Partial or intermittent negative reinforcement can create an effective climate of fear and doubt. When employees get the sense that bullies are tolerated, a climate of fear may be the result. Several studies have confirmed a relationship between bullying, on one hand, and an autocratic leadership and an authoritarian way of settling conflicts or dealing with disagreements, on the other. An authoritarian style of leadership may create a climate of fear, with little or no room for dialogue and with complaining being considered futile.\n\nIn a study of public-sector union members, approximately one in five workers reported having considered leaving the workplace as a result of witnessing bullying taking place. Rayner explained the figures by pointing to the presence of a climate of fear in which employees considered reporting to be unsafe, where bullies had been tolerated previously despite management knowing of the presence of bullying.\n\nIndividual differences in sensitivity to reward, punishment and motivation have been studied under the premises of reinforcement sensitivity theory and have also been applied to workplace performance.\n\nA culture of fear at the workplace runs contrary to the \"key principles\" established by W. Edwards Deming for managers to transform business effectiveness. One of his fourteen principles is to drive out fear in order to allow everyone to work effectively for the company.\n\nNazi leader Hermann Göring explains how people can be made fearful and to support a war they otherwise would oppose:\n\nIn her book \"State and Opposition in Military Brazil,\" Maria Helena Moreira Alves found a \"culture of fear\" was implemented as part of political repression since 1964. She used the term to describe methods implemented by the national security apparatus of Brazil in its effort to equate political participation with risk of arrest and torture.\n\nCassação (English: cassation) is one such mechanism used to punish members of the military by legally declaring them dead. This enhanced the potential for political control through intensifying the culture of fear as a deterrent to opposition.\n\nAlves found the changes of the National Security Law of 1969, as beginning the use of \"economic exploitation, physical repression, political control, and strict censorship\" to establish a \"culture of fear\" in Brazil. The three psychological components of the culture of fear included silence through censorship, sense of isolation, and a \"generalized belief that all channels of opposition were closed.\" A \"feeling of complete hopelessness,\" prevailed, in addition to \"withdrawal from opposition activity.\"\n\nFormer US National Security Advisor Zbigniew Brzezinski argues that the use of the term War on Terror was intended to generate a culture of fear deliberately because it \"obscures reason, intensifies emotions and makes it easier for demagogic politicians to mobilize the public on behalf of the policies they want to pursue\".\n\nFrank Furedi, a former professor of Sociology and writer for \"Spiked\" magazine, says that today's culture of fear did not begin with the collapse of the World Trade Center. Long before September 11, he argues, public panics were widespread – on everything from GM crops to mobile phones, from global warming to foot-and-mouth disease. Like Durodié, Furedi argues that perceptions of risk, ideas about safety and controversies over health, the environment and technology have little to do with science or empirical evidence. Rather, they are shaped by cultural assumptions about human vulnerability. Furedi say that \"we need a grown-up discussion about our post-September 11 world, based on a reasoned evaluation of all the available evidence rather than on irrational fears for the future.\nBritish academics Gabe Mythen and Sandra Walklate argue that following terrorist attacks in New York, the Pentagon, Madrid, and London, government agencies developed a discourse of \"new terrorism\" in a cultural climate of fear and uncertainty. UK researchers argued that these processes reduced notions of public safety and created the simplistic image of a non-white \"terroristic other\" that has negative consequences for ethnic minority groups in the UK.\n\nIn his 2004 BBC documentary film series, \"The Power of Nightmares\", subtitled \"The Rise of the Politics of Fear\", the journalist Adam Curtis argues that politicians have used our fears to increase their power and control over society. Though he does not use the term \"culture of fear,\" what Curtis describes in his film is a reflection of this concept. He looks at the American neo-conservative movement and its depiction of the threat first from the Soviet Union and then from radical Islamists. Curtis insists there has been a largely illusory fear of terrorism in the west since the September 11 attacks and that politicians such as George W Bush and Tony Blair had stumbled on a new force to restore their power and authority; using the fear of an organised \"web of evil\" from which they could protect their people. Curtis's film castigated the media, security forces and the Bush administration for expanding their power in this way. The film features Bill Durodié, then Director of the International Centre for Security Analysis, and Senior Research Fellow in the International Policy Institute, King's College London, saying that to call this network an \"invention\" would be too strong a term, but he asserts that it probably does not exist and is largely a \"(projection) of our own worst fears, and that what we see is a fantasy that's been created.\"\n\nThe consumption of mass media has had a profound effect on instilling the fear of terrorism in the United States, though acts of terror are a rare phenomenon. Beginning in the 1960s, George Gerbner and his colleagues have accelerated the study of the relationship that exists between media consumption and the fear of crime. According to Gerbner, television and other forms of mass media create a worldview that is reflective of “recurrent media messages”, rather than one that is based on reality. Many Americans are exposed to some form of media on a daily basis, with television and social media platforms being the most used methods to receive both local and international news, and as such this is how most receive news and details that center around violent crime and acts of terror. With the rise in use of smartphones and social media, people are bombarded with constant news updates, and able to read stories related to terrorism, stories that come from all corners of the globe. Media fuels fear of terrorism and other threats to national security, all of which have negative psychological effects on the population, such as depression, anxiety, and insomnia. Politicians conduct interviews, televised or otherwise, and utilize their social media platforms immediately after violent crimes and terrorist acts, to further cement the fear of terrorism into the minds of their constituents.\n\nSorted upwards by date, most recent last.\n\n\n"}
{"id": "55526", "url": "https://en.wikipedia.org/wiki?curid=55526", "title": "Donkey", "text": "Donkey\n\nThe donkey or ass (\"Equus africanus asinus\") is a domesticated member of the horse family, Equidae. The wild ancestor of the donkey is the African wild ass, \"E. africanus\". The donkey has been used as a working animal for at least 5000 years. There are more than 40 million donkeys in the world, mostly in underdeveloped countries, where they are used principally as draught or pack animals. Working donkeys are often associated with those living at or below subsistence levels. Small numbers of donkeys are kept for breeding or as pets in developed countries.\n\nA male donkey or ass is called a jack, a female a jenny or jennet; a young donkey is a foal. Jack donkeys are often used to mate with female horses to produce mules; the biological \"reciprocal\" of a mule, from a stallion and jenny as its parents instead, is called a hinny.\n\nAsses were first domesticated around 3000 BC, probably in Egypt or Mesopotamia, and have spread around the world. They continue to fill important roles in many places today. While domesticated species are increasing in numbers, the African wild ass is an endangered species. As beasts of burden and companions, asses and donkeys have worked together with humans for millennia.\n\nTraditionally, the scientific name for the donkey is \"Equus asinus asinus\" based on the principle of priority used for scientific names of animals. However, the International Commission on Zoological Nomenclature ruled in 2003 that if the domestic species and the wild species are considered subspecies of one another, the scientific name of the wild species has priority, even when that subspecies was described after the domestic subspecies. This means that the proper scientific name for the donkey is \"Equus africanus asinus\" when it is considered a subspecies, and \"Equus asinus\" when it is considered a species.\n\nAt one time, the synonym \"ass\" was the more common term for the donkey. The first recorded use of \"donkey\" was in either 1784 or 1785. While the word \"ass\" has cognates in most other Indo-European languages, \"donkey\" is an etymologically obscure word for which no credible cognate has been identified. Hypotheses on its derivation include the following:\nFrom the 18th century, \"donkey\" gradually replaced \"ass\", and \"jenny\" replaced \"she-ass\", which is now considered archaic. The change may have come about through a tendency to avoid pejorative terms in speech, and be comparable to the substitution in North American English of \"rooster\" for \"cock\", or that of \"rabbit\" for \"coney\", which was formerly homophonic with \"cunny\". By the end of the 17th century, changes in pronunciation of both \"ass\" and \"arse\" had caused them to become homophones. Other words used for the ass in English from this time include \"cuddy\" in Scotland, \"neddy\" in southwest England and \"dicky\" in the southeast; \"moke\" is documented in the 19th century, and may be of Welsh or Gypsy origin.\n\n Donkeys vary considerably in size, depending on breed and management. The height at the withers ranges from , and the weight from . Working donkeys in the poorest countries have a life expectancy of 12 to 15 years; in more prosperous countries, they may have a lifespan of 30 to 50 years.\n\nDonkeys are adapted to marginal desert lands. Unlike wild and feral horses, wild donkeys in dry areas are solitary and do not form harems. Each adult donkey establishes a home range; breeding over a large area may be dominated by one jack. The loud call or bray of the donkey, which typically lasts for twenty seconds and can be heard for over three kilometres, may help keep in contact with other donkeys over the wide spaces of the desert. Donkeys have large ears, which may pick up more distant sounds, and may help cool the donkey's blood. Donkeys can defend themselves by biting, striking with the front hooves or kicking with the hind legs.\n\nA jenny is normally pregnant for about 12 months, though the gestation period varies from 11 to 14 months, and usually gives birth to a single foal. Births of twins are rare, though less so than in horses. About 1.7 percent of donkey pregnancies result in twins; both foals survive in about 14 percent of those. In general jennies have a conception rate that is lower than that of horses (\"i.e.\" less than the 60–65% rate for mares).\n\nAlthough jennies come into heat within 9 or 10 days of giving birth, their fertility remains low, and it is likely the reproductive tract has not returned to normal. Thus it is usual to wait one or two further oestrous cycles before rebreeding, unlike the practice with mares. Jennies are usually very protective of their foals, and some will not come into estrus while they have a foal at side. The time lapse involved in rebreeding, and the length of a jenny's gestation, means that a jenny will have fewer than one foal per year. Because of this and the longer gestation period, donkey breeders do not expect to obtain a foal every year, as horse breeders often do, but may plan for three foals in four years.\n\nDonkeys can interbreed with other members of the family Equidae, and are commonly interbred with horses. The hybrid between a jack and a mare is a mule, valued as a working and riding animal in many countries. Some large donkey breeds such as the Asino di Martina Franca, the Baudet de Poitou and the Mammoth Jack are raised only for mule production. The hybrid between a stallion and a jenny is a hinny, and is less common. Like other inter-species hybrids, mules and hinnies are usually sterile. Donkeys can also breed with zebras in which the offspring is called a zonkey (among other names).\n\nDonkeys have a notorious reputation for stubbornness, but this has been attributed to a much stronger sense of self-preservation than exhibited by horses. Likely based on a stronger prey instinct and a weaker connection with humans, it is considerably more difficult to force or frighten a donkey into doing something it perceives to be dangerous for whatever reason. Once a person has earned their confidence they can be willing and companionable partners and very dependable in work.\n\nAlthough formal studies of their behaviour and cognition are rather limited, donkeys appear to be quite intelligent, cautious, friendly, playful, and eager to learn.\n\nThe genus \"Equus\", which includes all extant equines, is believed to have evolved from \"Dinohippus\", via the intermediate form \"Plesippus\". One of the oldest species is \"Equus simplicidens\", described as zebra-like with a donkey-shaped head. The oldest fossil to date is ~3.5 million years old from Idaho, USA. The genus appears to have spread quickly into the Old World, with the similarly aged \"Equus livenzovensis\" documented from western Europe and Russia.\n\nMolecular phylogenies indicate the most recent common ancestor of all modern equids (members of the genus \"Equus\") lived ~5.6 (3.9–7.8) mya. Direct paleogenomic sequencing of a 700,000-year-old middle Pleistocene horse metapodial bone from Canada implies a more recent 4.07 Myr before present date for the most recent common ancestor (MRCA) within the range of 4.0 to 4.5 Myr BP. The oldest divergencies are the Asian hemiones (subgenus \"E. (Asinus)\", including the kulan, onager, and kiang), followed by the African zebras (subgenera \"E. (Dolichohippus)\", and \"E. (Hippotigris)\"). All other modern forms including the domesticated horse (and many fossil Pliocene and Pleistocene forms) belong to the subgenus \"E. (Equus)\" which diverged ~4.8 (3.2–6.5) million years ago.\n\nThe ancestors of the modern donkey are the Nubian and Somalian subspecies of African wild ass. \n\nBy the end of the fourth millennium BC, the donkey had spread to Southwest Asia, and the main breeding center had shifted to Mesopotamia by 1800 BC. The breeding of large, white riding asses made Damascus famous, while Syrian breeders developed at least three other breeds, including one preferred by women for its easy gait. The Muscat or Yemen ass was developed in Arabia. By the second millennium BC, the donkey was brought to Europe, possibly at the same time as viticulture was introduced, as the donkey is associated with the Syrian god of wine, Dionysus. Greeks spread both of these to many of their colonies, including those in what are now Italy, France and Spain; Romans dispersed them throughout their empire.\n\nThe first donkeys came to the Americas on ships of the Second Voyage of Christopher Columbus, and were landed at Hispaniola in 1495. The first to reach North America may have been two animals taken to Mexico by Juan de Zumárraga, the first bishop of Mexico, who arrived there on 6 December 1528, while the first donkeys to reach what is now the United States may have crossed the Rio Grande with Juan de Oñate in April 1598. From that time on they spread northward, finding use in missions and mines. Donkeys were documented as present in what today is Arizona in 1679. By the Gold Rush years of the 19th century, the burro was the beast of burden of choice of early prospectors in the western United States. With the end of the placer mining boom, many of them escaped or were abandoned, and a feral population established itself.\n\nAbout 41 million donkeys were reported worldwide in 2006. China had the most with 11 million, followed by Pakistan, Ethiopia and Mexico. As of 2017, however, the Chinese population was reported to have dropped to 3 million, with African populations under pressure as well, due to increasing trade and demand for donkey products in China. Some researchers believe the actual number may be somewhat higher since many donkeys go uncounted. The number of breeds and percentage of world population for each of the FAO's world regions was in 2006:\n\nIn 1997 the number of donkeys in the world was reported to be continuing to grow, as it had steadily done throughout most of history; factors cited as contributing to this were increasing human population, progress in economic development and social stability in some poorer nations, conversion of forests to farm and range land, rising prices of motor vehicles and fuel, and the popularity of donkeys as pets.\nSince then, the world population of donkeys is reported to be rapidly shrinking, falling from 43.7 million to 43.5 million between 1995 and 2000, and to only 41 million in 2006. The fall in population is pronounced in developed countries; in Europe, the total number of donkeys fell from 3 million in 1944 to just over 1 million in 1994.\n\nThe Domestic Animal Diversity Information System (DAD-IS) of the FAO listed 189 breeds of ass in June 2011. In 2000 the number of breeds of donkey recorded worldwide was 97, and in 1995 it was 77. The rapid increase is attributed to attention paid to identification and recognition of donkey breeds by the FAO's Animal Genetic Resources project. The rate of recognition of new breeds has been particularly high in some developed countries. In France, for example, only one breed, the Baudet de Poitou, was recognised prior to the early 1990s; by 2005, a further six donkey breeds had official recognition.\n\nIn prosperous countries, the welfare of donkeys both at home and abroad has become a concern, and a number of sanctuaries for retired and rescued donkeys have been set up. The largest is The Donkey Sanctuary near Sidmouth, England, which also supports donkey welfare projects in Egypt, Ethiopia, India, Kenya, and Mexico.\n\nThe donkey has been used as a working animal for at least 5000 years. Of the more than 40 million donkeys in the world, about 96% are in underdeveloped countries, where they are used principally as pack animals or for draught work in transport or agriculture. After human labour, the donkey is the cheapest form of agricultural power. They may also be ridden, or used for threshing, raising water, milling and other work. Working donkeys are often associated with those living at or below subsistence levels. Some cultures that prohibit women from working with oxen in agriculture do not extend this taboo to donkeys, allowing them to be used by both sexes.\n\nIn developed countries where their use as beasts of burden has disappeared, donkeys are used to sire mules, to guard sheep, for donkey rides for children or tourists, and as pets. Donkeys may be pastured or stabled with horses and ponies, and are thought to have a calming effect on nervous horses. If a donkey is introduced to a mare and foal, the foal may turn to the donkey for support after it has been weaned from its mother.\n\nA few donkeys are milked or raised for meat; in Italy, which has the highest consumption of equine meat in Europe and where donkey meat is the main ingredient of several regional dishes, about 1000 donkeys were slaughtered in 2010, yielding approximately 100 tonnes of meat. Asses' milk may command good prices: the average price in Italy in 2009 was €15 per litre, and a price of €6 per 100 ml was reported from Croatia in 2008; it is used for soaps and cosmetics as well as dietary purposes. The niche markets for both milk and meat are expanding. In the past, donkey skin was used in the production of parchment. In 2017, the UK based charity The Donkey Sanctuary estimated that 1.8 million skins were traded every year, but the demand could be as high as 10 million.\n\nIn China, donkey meat is considered a delicacy with some restaurants specializing in such dishes, and Guo Li Zhuang restaurants offer the genitals of donkeys in dishes. Donkey-hide gelatin is produced by soaking and stewing the hide to make a traditional Chinese medicine product. Ejiao, the gelatine produced by boiling donkey skins, can sell for up to $388 per kilo, at October 2017 prices.\n\nIn 2017, a drop in the number of Chinese donkeys, combined with the fact that they are slow to reproduce, meant that Chinese suppliers began to look to Africa. As a result of the increase in demand, and the price that could be charged, Kenya opened three donkey abattoirs. Concerns for donkeys' well-being, however, have resulted in a number of African countries (including Uganda, Tanzania, Botswana, Niger, Burkina Faso, Mali, and Senegal) banning China from buying their donkey products.\n\nDuring World War I John Simpson Kirkpatrick, a British stretcher bearer serving with the Australian and New Zealand Army Corps, and Richard Alexander \"Dick\" Henderson of the New Zealand Medical Corps used donkeys to rescue wounded soldiers from the battlefield at Gallipoli.\n\nAccording to British food writer Matthew Fort, donkeys were used in the Italian Army. The Mountain Fusiliers each had a donkey to carry their gear, and in extreme circumstances the animal could be eaten.\n\nDonkeys have also been used to carry explosives in conflicts that include the war in Afghanistan and others.\n\nDonkey hooves are more elastic than those of horses, and do not naturally wear down as fast. Regular clipping may be required; neglect can lead to permanent damage. Working donkeys may need to be shod. Donkey shoes are similar to horseshoes, but usually smaller and without toe-clips.\n\nIn their native arid and semi-arid climates, donkeys spend more than half of each day foraging and feeding, often on poor quality scrub. The donkey has a tough digestive system in which roughage is efficiently broken down by hind gut fermentation, microbial action in the caecum and large intestine. While there is no marked structural difference between the gastro-intestinal tract of a donkey and that of a horse, the digestion of the donkey is more efficient. It needs less food than a horse or pony of comparable height and weight, approximately 1.5 percent of body weight per day in dry matter, compared to the 2–2.5 percent consumption rate possible for a horse. Donkeys are also less prone to colic. The reasons for this difference are not fully understood; the donkey may have different intestinal flora to the horse, or a longer gut retention time.\nDonkeys obtain most of their energy from structural carbohydrates. Some suggest that a donkey needs to be fed only straw (preferably barley straw), supplemented with controlled grazing in the summer or hay in the winter, to get all the energy, protein, fat and vitamins it requires; others recommend some grain to be fed, particularly to working animals, and others advise against feeding straw. They do best when allowed to consume small amounts of food over long periods. They can meet their nutritional needs on 6 to 7 hours of grazing per day on average dryland pasture that is not stressed by drought. If they are worked long hours or do not have access to pasture, they require hay or a similar dried forage, with no more than a 1:4 ratio of legumes to grass. They also require salt and mineral supplements, and access to clean, fresh water. In temperate climates the forage available is often too abundant and too rich; over-feeding may cause weight gain and obesity, and lead to metabolic disorders such as founder (laminitis) and hyperlipaemia, or to gastric ulcers.\n\nThroughout the world, working donkeys are associated with the very poor, with those living at or below subsistence level. Few receive adequate food, and in general donkeys throughout the Third World are under-nourished and over-worked.\n\nIn the Iberian Peninsula and the Americas, a is a small donkey. The Domestic Animal Diversity Information System (DAD-IS) of the FAO lists the burro as a specific breed of ass. In Mexico, the donkey population is estimated at three million. There are also substantial \"burro\" populations in El Salvador, Guatemala, and Nicaragua.\n\n\"Burro\" is the Spanish and Portuguese word for donkey. In Spanish, burros may also be called ' ('Mexican donkey'), ' ('Criollo donkey'), or \"\". In the United States, \"burro\" is used as a loan word by English speakers to describe any small donkey used primarily as a pack animal, as well as to describe the feral donkeys that live in Arizona, California, Oregon, Utah, Texas and Nevada.\n\nAmong donkeys, burros tend to be on the small side. A study of working burros in central Mexico found a weight range of , with an average weight of for males and for females. Height at the withers varied from , with an average of approximately , and girth measurements ranged from , with an average of about . The average age of the burros in the study was 6.4 years; evaluated by their teeth, they ranged from 1 to 17 years old. They are gray in color. Mexican burros tend to be smaller than their counterparts in the USA, which are both larger and more robust. To strengthen their bloodstock, in May 2005, the state of Jalisco imported 11 male and female donkeys from Kentucky.\n\nIn some areas domestic donkeys have returned to the wild and established feral populations such as those of the Burro of North America and the Asinara donkey of Sardinia, Italy, both of which have protected status. Feral donkeys can also cause problems, notably in environments that have evolved free of any form of equid, such as Hawaii. In Australia, where there may be 5 million feral donkeys, they are regarded as an invasive pest and have a serious impact on the environment. They may compete with livestock and native animals for resources, spread weeds and diseases, foul or damage watering holes and cause erosion.\n\nFew species of ass exist in the wild. The African wild ass, \"Equus africanus\", has two subspecies, the Somali wild ass, \"Equus africanus somaliensis\", and the Nubian wild ass, \"Equus africanus africanus\", the principal ancestor of the domestic donkey. Both are critically endangered. Extinct species include the European ass, \"Equus hydruntinus\", which became extinct during the Neolithic, and the North African wild ass, \"Equus africanus atlanticus\", which became extinct in Roman times.\n\nThere are five subspecies of Asiatic wild ass or onager, \"Equus hemionus\", and three subspecies of the kiang, \"Equus kiang\", of the Himalayan upland.\n\nA male donkey (jack) can be crossed with a female horse to produce a mule. A male horse can be crossed with a female donkey to produce a hinny.\n\nHorse-donkey hybrids are almost always sterile because horses have 64 chromosomes whereas donkeys have 62, producing offspring with 63 chromosomes. Mules are much more common than hinnies. This is believed to be caused by two factors, the first being proven in cat hybrids, that when the chromosome count of the male is the higher, fertility rates drop (as in the case of stallion x jenney). The lower progesterone production of the jenny may also lead to early embryonic loss. In addition, there are reasons not directly related to reproductive biology. Due to different mating behavior, jacks are often more willing to cover mares than stallions are to breed jennys. Further, mares are usually larger than jennys and thus have more room for the ensuing foal to grow in the womb, resulting in a larger animal at birth. It is commonly believed that mules are more easily handled and also physically stronger than hinnies, making them more desirable for breeders to produce.\n\nThe offspring of a zebra-donkey cross is called a zonkey, zebroid, zebrass, or zedonk; \"zebra mule\" is an older term, but still used in some regions today. The foregoing terms generally refer to hybrids produced by breeding a male zebra to a female donkey. \"Zebra hinny, zebret\" and \"zebrinny\" all refer to the cross of a female zebra with a male donkey. Zebrinnies are rarer than zedonkies because female zebras in captivity are most valuable when used to produce full-blooded zebras. There are not enough female zebras breeding in captivity to spare them for hybridizing; there is no such limitation on the number of female donkeys breeding.\n\nThe long history of human donkey use has created a rich store of cultural references:\n\nDue to its widespread domestication and use, the donkey is referred to in myth and folklore around the world. In classical and ancient cultures, donkeys had a part. The donkey was the symbol of the Egyptian sun god Ra . In Greek myth, Silenus is pictured in Classical Antiquity and during the Renaissance (\"illustration, left\") drunken and riding a donkey, and Midas was given the ears of an ass after misjudging a musical competition.\n\nDonkeys (or asses) are mentioned many times in the Bible, beginning in the first book and continuing through both Old and New Testaments, so they became part of Judeo-Christian tradition. They are portrayed as work animals, used for agricultural purposes, transport and as beasts of burden, and terminology is used to differentiate age and gender. In contrast, horses were represented only in the context of war, ridden by cavalry or pulling chariots. Owners were protected by law from loss caused by the death or injury of a donkey, showing their value in that time period. Narrative turning points in the Bible (and other stories) are often marked through the use of donkeys — for instance, leading, saddling, or mounting/dismounting a donkey are used to show a change in focus or a decision having been made. They are used as a measure of wealth in Genesis 30:43, and in Genesis chapter 34, the prince of Shechem (the modern Nablus) is named Hamor (\"donkey\" in Hebrew).\nAccording to Old Testament prophecy, the Messiah is said to arrive on a donkey: \"Behold, your King is coming to you; He is just and having salvation, Lowly and riding on a donkey, A colt, the foal of a donkey!\" (Zechariah 9:9). According to the New Testament, this prophecy was fulfilled when Jesus entered Jerusalem riding on the animal (Matthew 21:4-7, John 12:14-15). Jesus appeared to be aware of this connection (Matthew 21:1-3, John 12:16).\n\nIn the Jewish religion, the donkey is not a kosher animal. In the Zohar, it is considered \"avi avot hatuma\" or the ultimate impure animal, and doubly \"impure\", as it is both non-ruminant and non-cloven hoofed. However, it is the only impure animal that falls under the mitzvah (commandment) of firstborn (\"bechor\") consecration that also applies to humans and pure animals (See Petter Chamor). In Jewish Oral Tradition (Talmud Bavli), the son of David was prophesied as riding on a donkey if the tribes of Israel are undeserving of redemption.\n\nIn contemporary Israel, the term \"Messiah's Donkey\" (Chamoro Shel Mashiach חמורו של משיח) stands at the center of a controversial religious-political doctrine, under which it was the Heavenly-imposed \"task\" of secular Zionists to build up a Jewish State, but once the state is established they are fated to give place to the Religious who are ordained to lead the state. The secularists in this analogy are \"The Donkey\" while the religious who are fated to supplant them are a collective \"Messiach\". A book on the subject, published in 1998 by the militant secularist Sefi Rechlevsky, aroused a major controversy in the Israeli public opinion.\n\nWith the rise of Christianity, some believers came to see the cross-shaped marking present on donkeys' backs and shoulders as a symbol of the animal's bearing Jesus into Jerusalem on Palm Sunday. During the Middle Ages, Europeans used hairs from this cross (or contact with a donkey) as folk remedies to treat illness, including measles and whooping cough. Around 1400 AD, one physician listed riding backwards on a donkey as a cure for scorpion stings.\nDonkeys are also referred to repeatedly in the writings and imagery of the Hinduism, where the goddess Kalaratri's vahana (vehicle) is a donkey. Donkeys also appear multiple times in Indian folklore as the subject of stories in both the Hitopadesha and the Panchatantra.\n\nIn Islam, eating the meat of a domestic donkey is not allowed.\n\nDonkeys hold a significant place in literature, especially in Western cultures. The original representations of donkeys in Western literature come mainly from the Bible and Ancient Greece. Donkeys were represented in a fairly negative form by the Greeks, but perceptions later changed, partially due to donkeys becoming increasingly symbolically connected to Christianity. Donkeys were found in the works of Homer, Aesop and Apuleius, where they were generally portrayed as stupid and stubborn, or servile at best, and generally represented the lower class. They were often contrasted with horses, which were seen as powerful and beautiful. Aesop's \"The Ass in the Lion's Skin\", representational of the almost 20 of his fables that portray donkeys, shows the donkey as a fool. Apuleius's \"The Golden Ass\" (160 AD), where the narrator is turned into a donkey, is also notable for its portrayal of donkeys as stubborn, foolish, wicked and lowly. This work had a large influence on the portrayal of donkeys in later cultures, including medieval and renaissance Europe. During this time, donkeys continued to be shown as stupid, clumsy and slow. Shakespeare popularized the use of the word \"ass\" as an insult meaning stupid or clownish in many of his plays, including Bottom's appearance in \"A Midsummer Night's Dream\" (1600). In contrast, a few years later, Cervantes' \"Don Quixote\" shows a more positive slant on the donkey, primarily as Sancho Panza's mount, portraying them as steady and loyal companions. This difference is possibly due to donkeys being an important aspect of many Spaniards' lives at this point in time.\n\nIn contrast to Grecian works, donkeys were portrayed in Biblical works as symbols of service, suffering, peace and humility. They are also associated with the theme of wisdom in the Old Testament story of Balaam's ass, and are seen in a positive light through the story of Jesus riding into Jerusalem on a donkey. By the 19th century, the donkey was portrayed with more positive attributes by popular authors. William Wordsworth portrayed the donkey as loyal and patient in his 1819 poem \"Peter Bell:A Tale\", using the donkey as a Christian symbol. Robert Louis Stevenson in \"Travels with a Donkey\" (1879), portrays the animal as a stubborn beast of burden. Sympathetic portrayals return in Juan Ramon Jimenez's \"Platero and I.\" The melancholy Eeyore in \"Winnie the Pooh\" (first published in 1926) is arguably the most famous donkey in Western literature.\n\nDonkeys were featured in literature during the 20th century, including in George Orwell's 1951 \"Animal Farm\", where Benjamin the donkey is portrayed as resilient and loyal. Puzzle is a well-meaning but easily manipulated donkey in C. S. Lewis's 1956 \"The Last Battle\". Brighty is the central character of the 1953 children's novel and 1967 film \"Brighty of the Grand Canyon\". Donkeys are portrayed in film including the 1940 Disney film \"Fantasia\", where the donkey is portrayed as a slapstick character who participates in a social faux pas with Bacchus and is punished by Zeus. A donkey is featured as the main figure in the 1966 film \"Au hasard Balthazar\" by Robert Bresson, and, is given a life path of Christian symbolism. Donkey, voiced by Eddie Murphy, is featured as a main character in the \"Shrek\" franchise of the 2000s.\n\nMany cultures have colloquialisms and proverbs that include donkeys or asses. British phrases include \"to talk the hind legs off a donkey\", used to describe someone talking excessively and generally persuasively. Donkeys are the animals featured most often in Greek proverbs, including such statements of fatalistic resignation as \"the donkey lets the rain soak him\". The French philosopher Jean Buridan constructed the paradox called Buridan's ass, in which a donkey, placed exactly midway between water and food, would die of hunger and thirst because he could not find a reason to choose one of the options over the other, and so would never make a decision. Italy has several phrases regarding donkeys, including \"put your money in the ass of a donkey and they'll call him sir\" (meaning, if you're rich, you'll get respect) and \"women, donkeys and goats all have heads\" (meaning, women are as stubborn as donkeys and goats). The United States developed its own expressions, including \"better a donkey that carries me than a horse that throws me\", \"a donkey looks beautiful to a donkey\", and \"a donkey is but a donkey though laden with gold\", among others. From Afghanistan, we find the Pashto proverb, \"Even if a donkey goes to Mecca, he is still a donkey.\" In Ethiopia, there are many Amharic proverbs that demean donkeys, such as, \"The heifer that spends time with a donkey learns to fart\" (Bad company corrupts good morals).\n\nThe words \"donkey\" and \"ass\" (or translations thereof) have come to have derogatory or insulting meaning in several languages, and are generally used to mean someone who is obstinate, stupid or silly, In football, especially in the United Kingdom, a player who is considered unskilful is often dubbed a \"donkey\", and the term has a similar connotation in poker. In the US, the slang terms \"dumbass\" and \"jackass\" are used to refer to someone considered stupid.\n\nIn keeping with their widespread cultural references, donkeys feature in political systems, symbols and terminology in many areas of the world. A \"donkey vote\" is a vote that simply writes down preferences in the order of the candidates (1 at the top, then 2, and so on), and is most often seen in countries with ranked voting systems and compulsory voting, such as Australia. The donkey is a common symbol of the Democratic Party of the United States, originating in a cartoon by Thomas Nast of \"Harper's Weekly\" in the nineteenth century.\n\nThe bray of the donkey may be used as a simile for loud and foolish speech in political mockery. For example,\nIn 1963, Party of Donkeys, a frivolous political party was founded in Iran.\n\nThe \"\"ruc català\" or \"burro català\"\" (Catalan donkey) has become a symbol of Catalonia in Spain. In 2003 some friends in Catalonia made bumper stickers featuring the \"burro català\" as a reaction against a national advertising campaign for \"Toro d'Osborne\", a brandy. The burro became popular as a nationalist symbol in Catalonia, whose residents wanted to assert their identity to resist Spanish centralism. Renewed attention to the regional burro helped start a breeding campaign for its preservation, and its numbers have increased.\n\nProshka, an ass owned by Russian populist nationalist liberal democratic politician Vladimir Zhirinovsky, became prominent during the 2012 Russian presidential election campaign, when he was filmed in an election advertisement video. In that controversial ad, Zhirinovsky appeared sitting in a sleigh harnessed with Proshka, then claiming that the \"little wretched ass\" is the symbol of Russia and that if he would become President a \"daring troika\" would return as a symbol of Russia instead of the ass; at the end, Zhirinovsky beat Proshka with a whip, made the ass move and had a ride on him through the snow-covered backyard of his dacha. International organisations People for the Ethical Treatment of Animals (PETA) and World Animal Protection have accused Zhirinovsky of cruelty to animals. Zhirinovsky replied to the assertions by stating that similar treatment is commonplace in the Arab world and claimed that his ass has been treated \"better than many people\".\n\n"}
{"id": "34606144", "url": "https://en.wikipedia.org/wiki?curid=34606144", "title": "Dvesha (Buddhism)", "text": "Dvesha (Buddhism)\n\nDvesha (Sanskrit, also \"dveṣa\"; Pali: \"dosa\"; Tibetan: \"zhe sdang\") - is a Buddhist term that is translated as \"hate, aversion\".\n\n\"Dvesha\" (hate, aversion) is the opposite of \"raga\" (lust, desire). Along with \"Raga\" and \"Moha\", \"Dvesha\" is one of the three character afflictions that, in part, cause \"Dukkha\". It is also one of the \"threefold fires\" in Buddhist Pali canon that must be quenched.\n\nDvesha (dosa) is identified in the following contexts within the Buddhist teachings:\n\nWalpola Rahula renders it as \"hatred\", as does Chogyam Trungpa.\n\n\n"}
{"id": "52991970", "url": "https://en.wikipedia.org/wiki?curid=52991970", "title": "Esoteric Buddhism (book)", "text": "Esoteric Buddhism (book)\n\nEsoteric Buddhism is a book originally published in 1883 in London; it was compiled by a member of the Theosophical Society, A. P. Sinnett. It was one of the first books written for the purpose explain of the theosophy for the wide range of readers, and was \"made up of the author's correspondence with an Indian mystic.\" This is the most significant theosophical work of the author. According to Goodrick-Clarke, it \"disseminated the basic teachings of Theosophy in its new Asian cast.\"\n\nThrough the mediation of Blavatsky Sinnett began a correspondence in 1880 with the two adepts, who sponsored the Theosophical Society, the mahatmas Kuthumi and Morya. Hammer noted that between 1880 and 1884 Sinnett received from the mahatmas circa one hundred twenty letters with explanation of \"occult cosmology\". From this material, he attempted to formulate in his new book \"the basis of a revised theosophy.\" By foundation of the book became \"Cosmological notes\" received from the Mahatma Morya together with a long series of answers to questions sent by mahatma Kuthumi during the summer of 1882.Subba Row received from his Master mahatma Morya the instruction to provide assistance to Sinnett in his work on the book, but, according to the memoirs of the author, he did it reluctantly, and what little help. The main help came from the mahatmas through Blavatsky in the form of answers to the questions referred to her by the author.\n\n\nIn preface to the original edition author says that exoteric Buddhism \"has remained in closer union with the esoteric doctrine\" than any other world religion. Thus, specification of the \"inner knowledge\" addressed to modern readers will be connected with the familiar features of the Buddhist teaching. Sinnett argues that esoteric teaching \"be most conveniently studied in its Buddhist aspect.\"\n\nAt the beginning of the first chapter the author makes the following statement:\n\"I am bringing to my readers knowledge which I have obtained by favour rather than by effort. It will not be found the less valuable on that account; I venture, on the contrary, to declare that it will be found of incalculably greater value, easily as I have obtained it, than any results in a similar direction which I could possibly have procured by ordinary methods of research.\"\nOn the question of the whereabouts of his teachers Sinnett says that for a long time in Tibet there is a \"certain secret region,\" hitherto unknown and inaccessible to ordinary people and for those living in the surrounding mountains as well as for visitors, \"in which adepts have always congregated. But the country generally was not in Buddha's time, as it has since become, the chosen habitation of the great brotherhood. Much more than they are at present, were the Mahatmas in former times, distributed about the world.\" But the development of civilization has led to the fact that many occultists gathered in Tibet. The system of rules and laws for them has been developed in the 14th century by Tsong-ka-pa.\n\nThe author argues that \"a complete, or perfect man\" is made up of seven elements:\n\nA French philosopher René Guénon stated that the central place of the Theosophical doctrine [which there is in Sinnett's book] is occupied the \"idea of evolution.\" He then wrote that, according to the Theosophical teaching, there are\n\"seven 'mother-races' succeed one another in the course of a 'world period', that is to say while the 'wave of life' sojourns on a given planet. Each 'race' includes seven 'sub-races', each of which is divided into seven 'branches'. On the other hand, the 'wave of life' successively runs through seven globes in a 'round', and this 'round' is repeated seven times in a same 'planetary chain', after which the 'wave of life' passes to another 'chain', composed likewise of seven planets which will be traversed seven times in their turn. Thus there are seven 'chains' in a 'planetary system', also called an 'enterprise of evolution'; and finally, our solar system is formed of ten 'planetary systems'... We are presently in the fifth 'race' of our 'world period', and in the fourth 'round' of the 'chain' of which the earth forms part and in which it occupies the fourth rank. This 'chain' is also the fourth of our 'planetary system'.\"\n\nIn the fifth chapter of his book, Sinnett explains the fate of man after death. Of the seven components that make up our personality, the three lower at the time of physical death, moving away from us. The four upper components move on Kama loca, and \"from there [soul] proceed to Devachan, a kind of theosophical version of heaven.\" (The analogy should not be carried overly; Sinnett argues that Devachan is a state, not a place.) Then these four components divide themselves, the law of karma specifies that it will happen to them—different souls receive different devachanic experience. Only after a long stay in this state, the soul reincarnates. New incarnations on the earth plane are actually rather rare, \"but re-birth in less than fifteen hundred years is spoken of as almost impossible.\"\n\nSinnett says that the mediums contact with the inhabitants of Devachan, but very rare, and in this time occurs the following:\n\"The spirit of the sensitive, getting odylized, so to say, by the aura of the spirit in the Devachan, \"becomes\" for a few minutes that departed personality, and writes in the handwriting of the latter, in his language and in his thoughts, as they were during his lifetime. The two spirits become blended in one, and the preponderance of one over the other during such phenomena determines the preponderance of personality in the characteristic exhibited. Thus it may incidentally be observed, what is called \"rapport,\" is, in plain fact, an identity of molecular vibration between the astral part of the incarnate medium and the astral part of the disincarnate personality.\"\n\nLavoie noted that in Sinnett's book there are the two major questions – \"the structure of the universe and spiritual evolution.\" He selected \"some key terms\" in the book.\n\nAvitchi is a state \"of punishment reached only in exceptional cases and by exceptional natures.\" The usual man will work his karma out in a new incarnation.\nDevachan is a state of greatest bliss where \"the levels of intensity and the duration of stay are based on the karma one produces in his/her lifetime.\"\nEighth sphere is a planet associated with our planetary chain that is more materialistic than the earth. The Soul in the mean of the fifth round \"can be sent to the eighth sphere for annihilation if it has developed a positive attraction to materialism and a repulsion of spirituality.\"\nKama loca is \"an unconscious state of gestation. It is here that the fourth principle (the animal soul) is separated from the others.\" The fourth component and some of the fifth component stays in Kama loca while \"the rest of the principles continue on in their spiritual evolution.\" The Ego's duration in Kama loca can last from few moments to years.\nManvantara is a period of activity or manifestation. There are three different manvantaras: 1) the mahamanvantara, 2) the solar manvantara, 3) the minor manvantara.\nMonad is upper triad of the seven principles of man (Atma-Buddhi-Manas).\nPralaya is a state of nonbeing. \"Pralaya is described by Sinnett as a type of sleep, rest, or a time of inactivity.\" There are three different pralayas: 1) the mahapralaya, 2) the solar pralaya, 3) the minor pralaya.\nRound is a full turnover via the seven globes. \"During each round there is a maximum of 120 incarnations for each monad in each race with the average of 8,000 years between incarnations.\"\n\nMan begins as a monad and dwells in seven major races on each of the seven planets. Each race takes circa one million years. Only 12,000 of those will be used for objective existence on the planets. The rest of that time will be used mainly in a subjective existence on the devachanic plane. \"This meant that out of one million years – 988,000 years are spent reaping the effects of karma.\" A branch race is one of seven belonging to a subrace, itself one of seven belonging to a main race. \"If each monad in each race incarnates once, the total number of incarnations in each globe would be 343 (7 branch races x 7 subraces x 7 root races); however, each monad incarnates typically at a minimum of two times and some even more frequently.\" In \"Mahatma Letters\" it is said that \"one life in each of the seven root-races; seven lives in each of the 49 sub-races – or 7 x 7 x 7 = 343 and add 7 more. And then a series of lives in offshoot and branchlet races; making the total incarnations of man in each station or planet 777.\"\n\nThe ninth chapter of Sinnett's book called \"Buddha\". It begins with the words:\n\"The historical Buddha, as known to the custodians of the esoteric doctrine, is a personage whose birth is not invested with the quaint marvels popular story has crowded round it. Nor was his progress to adeptship traced by the literal occurrence of the supernatural struggles depicted in symbolic legend. On the other hand, the incarnation, which may outwardly be described as the birth of Buddha, is certainly not regarded by occult science as an event like any other birth, nor the spiritual development through which Buddha passed during his earth-life a mere process of intellectual evolution, like the mental history of any other philosopher. The mistake which ordinary European writers make in dealing with a problem of this sort lies in their inclination to treat exoteric legend either as a record of a miracle about which no more need be said, or as pure myth, putting merely a fantastic decoration on a remarkable life.\"\nAccording to Lopez, author of \"Esoteric Buddhism\" \"has a broader view of the Buddha\" than that of Western Buddhologists and scholars of Oriental studies. Sinnett stated that the Buddha is simply one of a row \"of adepts who have appeared over the course of the centuries.\" Buddha's next incarnation happened approximately sixty years after his death. He appeared as Shankara, the well-known Vedantic philosopher. Sinnett noted that for the uninitiated it is known that date of Shankara's birth is one thousand years after Buddha's death, and that he was hostile to Buddhism. Sinnett wrote that the Buddha came as Shankara \"to fill up some gaps and repair certain errors in his own previous teaching.\" The Buddha had leaved \"from the practice of earlier adepts by opening the path\" to adeptship to men of all castes. \"Although well-intentioned, this led\" to a deterioration of occult knowledge when it was penetrated into ignominious hands. Sinnett wrote that to further appeared a need \"to take no candidates except from the class which, on the whole, by reason of its hereditary advantages, is likely to be the best nursery of fit candidates.\"Sinnett claimed that the Buddha's next incarnation was as the great Tibetan adept reformer of the 14th century Tsong-ka-pa.\n\nIn the tenth chapter Sinnett expresses (as well as the Mahatmas) his very negative attitude to religiosity of any kind. He argues:\n\"Nothing can produce more disastrous effects on human progress, as regards the destiny of individuals, than the very prevalent notion that one religion followed out in a pious spirit, is as good as another, and that if such and such doctrines are perhaps absurd when you look into them, the great majority of good people will never think of their absurdity, but will recite them in a blamelessly devoted attitude of mind.\"\n\nThe presence of a secret or esoteric teaching in Buddhism is \"not accepted by orthodox Buddhist.\" For example, Rhys Davids wrote:\n\"In this connection, I shall doubtless be expected to say a few words on Theosophy, if only because one of the books giving an account of that very curious and widely spread movement has been called \"Esoteric Buddhism.\" It has always been a point of wonder to me why the author should have chosen this particular title for his treatise. For if there is anything that can be said with absolute certainty about the book it is, that it is not esoteric, and not Buddhism. The original Buddhism was the very contrary of esoteric.\"\nGuénon's opinion on the subject was the same. He wrote that never was genuine \"esoteric Buddhism.\" The ancient Buddhism was substantially an exoteric teaching \"serving as theoretical support for a social movement with egalitarian tendencies.\" According to Guénon, Sinnett, who \"at the beginning probably contributed more than anybody else to make Theosophism known in Europe, was genuinely fooled by all of Mme Blavatsky's tricks.\"\n\nSome Theosophists did not share the views presented by Sinnett in his new work; for example, according to Kingsford, this book was very distant from the esoteric, and the main mistake of the author was that he thought about the symbols as reality.\n\nAfter its first publication in 1883 the book was reprinted several times: in the same 1883 came 2nd edition, in 1885 – 5th, 1898 – 8th. This work has been translated into several European languages: French, German, Italian, Spanish, Russian.\n\n\n\n"}
{"id": "28353321", "url": "https://en.wikipedia.org/wiki?curid=28353321", "title": "European Week for Waste Reduction", "text": "European Week for Waste Reduction\n\nThe European Week for Waste Reduction (EWWR) was launched as a 3-year project supported by the LIFE+ Programme of the European Commission until July 2012. It continues taking place in the following years. The 2012 edition of the EWWR took place from the 17 to 25 November 2012 under the patronage of Mr Janez Potočnik, European Commissioner for the Environment. \nIt aims to organize multiple actions during a single week, across Europe, that will raise awareness about waste reduction. Each year, the most outstanding actions are rewarded during an awards ceremony in Brussels at the heart of the European institutions. Since the beginning of the project, more than 20.000 awareness raising actions on waste prevention have been implemented in the framework of the EWWR.\n\nThe 2017 date is from November 18-27. \n\nThe five partners of this project are:\n\n\n\n\n\n"}
{"id": "2806736", "url": "https://en.wikipedia.org/wiki?curid=2806736", "title": "Extractive Industries Transparency Initiative", "text": "Extractive Industries Transparency Initiative\n\nThe Extractive Industries Transparency Initiative (EITI) is a global standard for the good governance of oil, gas and mineral resources. It seeks to address the key governance issues in the extractive sectors.\n\nThe EITI Standard requires information along the extractive industry value chain from the point of extraction, to how the revenue makes its way through the government, to how it contributes to the economy.\n\nThis includes how licenses and contracts are allocated and registered, who are the beneficial owners of those operations are, what are the fiscal and legal arrangements are, how much is produced, how much is paid, where are those revenues allocated, and what is the contribution to the economy, including employment.\n\nThe EITI Standard is implemented in 52 countries around the world. Each of these countries is required publish to an annual EITI Report to disclose information on: contracts and licenses, production, revenue collection, revenue allocation, and social and economic spending.\n\nEvery country goes through a quality-assurance mechanism, called Validation, at least every three years. Validation serves to assess performance towards meeting the EITI Standard and promote dialogue and learning at the country level. It also safeguards the integrity of the EITI by holding all EITI implementing countries to the same global standard.\n\nEach implementing country has its own national secretariat and multi-stakeholder group, made up of representatives from the country’s government, extractive companies and civil society. The multi-stakeholder group takes decisions on how the EITI process is carried out in the country.\n\nThe EITI Standard is developed and overseen by an international multi-stakeholder Board, consisting of representatives from governments, extractives companies, civil society organisations, financial institutions and international organisations.\n\nThe Chair of the EITI is Fredrik Reinfeldt, former Prime Minister of Sweden. The previous chairs have been the Clare Short (2011-2016), former UK Secretary of State for International Development and Peter Eigen (2009-2011). The EITI International Secretariat is located in Oslo, Norway and is headed by former Swedish diplomat Jonas Moberg.\n\nThe “Extractive Industries Transparency Initiative” (EITI) was first launched in September 2002 by UK Prime Minister Tony Blair at the World Summit on Sustainable Development in Johannesburg, following years of academic debate, as well as lobbying from civil society and companies, on the management of government revenues from the extractive industries. In particular, the EITI was established to be an answer to public discussions on the “Resource Curse” or the “Paradox of Plenty”. NGOs such as by Global Witness and “Publish What You Pay”, as well as companies such as BP pushed the UK government to working towards an international transparency norm.\n\nThe organisation was founded at a conference in London in 2003. The 140 delegates from government, companies and civil society agreed on twelve principles to increase transparency over payments and revenues in the extractive sector. A pilot phase of the EITI was launched in Nigeria, Azerbaijan, Ghana and the Kyrgyz Republic. The management of the Initiative continued to lay with the UK Department for International Development.\n\nThe second EITI Conference on 17 March 2005 in London established six criteria based on the principles. These set out the minimum requirements for transparency in the management of resources in the oil, gas and mining sectors, laying the foundation for a rule-based organisation. This conference also established an international advisory group (IAG) under the Chairmanship of Peter Eigen to further guide the work of how the EITI is to be set up and function. More countries, companies and civil-society organisations joined the initiative. The International Monetary Fund and the World Bank endorsed the EITI.\n\nThe report issued in June 2006 by the international advisory group recommended the establishment of a multi-stakeholder board and an independent secretariat, and these were set in place at the third EITI conference held in Oslo, Norway on 11 October 2006. Oslo was chosen as the new location for the secretariat.\n\nIn the following years the body further fleshed out the criteria, turning them into a set of 23 requirements, known as the EITI Rules in 2011.\n\nThe EITI Standard replaced the EITI Rules on 24 May 2013. The Standard was revised in February 2016.\n\nThe EITI is organised as a non-profit association under Norwegian law. It has three institutional bodies: The Members’ Meeting, the EITI Board, and the International Secretariat. The Members’ Meeting governs the EITI and convenes alongside the EITI global conferences, which are held every two to three years.\n\nThe board develops the Standard and assesses the progress of the countries. It is supported by the international secretariat, located in Oslo, Norway.\n\nThe EITI Board meets between two and four times a year and is composed of three groups: countries, companies and civil society. The membership of the Board reflects the multi-stakeholder nature of the EITI. The EITI Board has eight committees (audit, finance, governance and oversight, implementation, nominations, outreach and candidature, rapid response and Validation) to develop recommendations to the full board.\n\nThe funding of the EITI is two-fold. At the country level, implementation is funded by the governments. At the international level, the EITI is funded by implementing countries, supporting governments and companies.\n\nAny country with extractive industry sectors can adhere to the EITI Standard. Countries implementing the transparency standard include OECD states such as Norway, the United Kingdom and the United States as well as countries in Africa, Central and East Asia, Europe, and Latin America and the Caribbean.\n\nWhen a country intends to join the EITI Standard, it is required to undertake five sign-up steps before applying.\n\nThese steps relate to a clear commitment of the government, company and civil society engagement, the establishment of a multi-stakeholder group and agreement on a work plan, which sets out what the country wants to achieve within a certain time frame.\n\nOnce the application of the country has been accepted by the board, the country is called an “EITI candidate”. The candidate receives the deadlines for publishing information and undergoes “Validation” two and a half years later.\n\nThe result of Validation is a measure of how well the country is progressing to meet the requirements of the standard. It can make satisfactory, meaningful, inadequate or no progress. The EITI board will ask the country to improve aspects which Validation deemed insufficient to fulfil the standard. An overview of Validation results is available online.\n\nWhen a candidate country passes EITI Validation, it is declared “EITI compliant” by the Board.\n\nAs of November 2017, 51 countries are implementing the EITI:\nOther countries, such as Lebanon, France and Australia have shown interest in implementing the EITI.\n\nThe EITI has made significant contributions to improved governance of the extractive sector in several countries around the world. In countries like the Democratic Republic of the Congo, the EITI has been central to many reforms of the sector. At the international level, debates on transparency in the sector are unrecognisable from ten years ago, and the EITI is seen as being at the forefront of many frontier debates including beneficial ownership, commodity trading, and artisanal and small-scale mining.\n\nIt is also clear that the EITI process is one of the only \"functioning\" global mechanisms to inform and channel debate in resource-rich countries in a way that includes all stakeholders. In Peru, EITI Reports have highlighted that, only about 15% of revenues from the mining and hydrocarbon sector has been used for developmental spending, such as infrastructure or economic diversification. The rest has been spent on current expenditures such as salaries and servicing debts. Local citizens are using this information to engage with their regional authorities on alternative ways to spend these resources.\n\nEITI Reports make recommendations aimed at addressing weaknesses in government systems and improving extractive sector management. In Nigeria, President Buhari has initiated major reforms in the oil sector, starting with restructuring the national oil company, a review of oil contracts, an pause in the awarding of the notorious oil swap deals, and a review of subsidy arrangements. These were all recommendations from Nigeria EITI Reports.\n\nFurthermore, the EITI can lead to higher attractiveness for investments. There is “mounting evidence that information release supports greater competition around government contracting and that being an EITI signatory leads to greater inflows of both aid and foreign direct investment”.\n\nAround 80 companies involved in oil, gas, and mining support the EITI. Supporting companies publicly endorse the EITI and contribute to covering the cost of the international secretariat of the EITI.\n\nExtractive companies are involved on the national level in countries implementing the transparency standard. They are part of the stakeholders and are required to hand over numbers on payments as part of the reporting process under the EITI standard. Company advocacy has resulted in several countries beginning EITI implementation.\n\nCampaigning organisations have criticised the organisation for the lack of sanction possibilities. On the other hand, business representatives have commented that the EITI board is captured by civil society organisations. The EITI has been seen as insufficient to bring full transparency to payments in the extractive industries, since it does not cover countries active in commodity trading. This has since been addressed by new requirements of the EITI standard.\n\nThe body's credibility was questioned after it permitted an Ethiopian application for membership in 2014.\n\nEITI has also been criticised for ignoring the violations of human rights in Azerbaijan, and for not reacting sufficiently strongly to the harassment of Azerbaijani civil society groups that are part of EITI's multi-stakeholder approach. On the other hand, the EITI has been criticised by an international lending institution for shifting its mandate beyond the promition of transparency.\n\n"}
{"id": "2479826", "url": "https://en.wikipedia.org/wiki?curid=2479826", "title": "Fuck Off (art exhibition)", "text": "Fuck Off (art exhibition)\n\n\"Fuck Off\" () was a controversial contemporary art exhibition which ran alongside the Third Shanghai Biennale (2000) in Shanghai, China. The exhibition’s title translates as \"Uncooperative Attitude\" in Chinese, but the blunter English language sentiment was deemed preferable. The exhibition encompassed conceptual, performance, and protest art.\n\nThe exhibition was held in an Eastlink Gallery warehouse by Feng Boyi and the 43-year-old Ai Weiwei, and is revered by many young Chinese artists. Ai encapsulated \"Fuck Off\"'s artistic-curatorial attitude with one set of photos in which he gives the finger in turn to the White House, the Forbidden City and the viewer, and another in which he releases an ancient Han Dynasty Chinese vase which smashes at his feet. \n\nThe exhibition included works by 46 avant-garde artists, among them He Yunchang, who posed in a color photograph while bare-chested and suspended from a crane by his ankles over a rushing river into which he holds a blade, the same knife he later used to cut his own arm. Sun Yuan exhibited \"Solitary Animal\", a glass case containing an animal skeleton and—purportedly—enough poison gas to wipe out the show's entire audience. Wang Chuyu's performance consisted of a four-day fast. Zhu Ming floated down the Huangpu River in a plastic bubble wearing a diaper.\n\n\"Fuck Off\" has been one of the most famous contemporary art exhibitions in recent history. It took place in the Eastlink gallery as well as a warehouse at 1133 West Suzhou River Road. The show included both emerging and famous artists and totaled 48 in all. The work displayed in Fuck Off was raw and unedited, which was the polar opposite of the Shanghai Beinnale that was open at the same time. The entire purpose of the show was to truly show the Chinese government exactly how \"uncooperative\" these artists could be, and that is evident in the closing line of the exhibition catalogue: \"Perhaps there is nothing that exists 'on-site,' but what will last forever is the very uncooperativeness with any system of power discourse.\"\n\nIn an interview by Chin-Chin Yap, Ai Weiwei was asked if the actual exhibit of \"Fuck Off\" had a concept similar to the Black, White, and Gray Cover Books he published. He went on to say that after the books were finished, there were a lot of interesting art works happening, and the people in his life continually recommended creating an exhibit with the theme of the books. His opinion was not that the show was really that good because it was organized so quickly, and he knew that there was a possibility that it could be shut down by the police and having all the work confiscated. Luckily, he said that the artists involved were \"cooperative and interested and the attitude was there.\" In a very eye-opening statement, Ai goes on to say that \"maybe Fuck Off was most important because of what it represented.\" Those involved had a clear thought about the image they wanted to give to Chinese institutions and Western curators, institutions and dealers, and that thought was:\"We had to say something as individual artist to the outside world, and what we said was 'fuck off'.\"\n\nOne of the most famous examples from this exhibition was the performance of \"Eating People\" by Zhu Yu. It consisted of a series of photographs of him cooking and eating what is alleged to be a human fetus. One picture, circulated on the internet via e-mail in 2001, provoked investigations by both the FBI and Scotland Yard. The piece's cannibalistic theme was controversial in Britain when Zhu's work was featured on a Channel 4 documentary exploring Chinese modern art in 2003. In response to the public reaction, Zhu Yu stated, \"No religion forbids cannibalism. Nor can I find any law which prevents us from eating people. I took advantage of the space between morality and the law and based my work on it\". Zhu has claimed that he used an actual fetus which was stolen from a medical school. \n\nMany influential artists of the current Chinese art scene took part, many of whom have since been included in international exhibitions, catalogues and television documentaries. \n\nThe exhibition was closed by the Shanghai police before its scheduled closing date.\n\nA catalog of the exhibition has been published, a black book with the simple title \"FUCK OFF\" on its cover.\n\nCo-curator Feng Boyi felt as though Chinese artists were just working for foreigners because the early Chinese contemporary art shows were being held in foreign countries. In co-curating the exhibition, Feng said that \"We wanted to show the 'fuck off' style, not working for the government or in the style of western countries, but a third way.\"\n\nDescription by Ai Weiwei and Feng Boyi in October, 2000:\n\n\"'Fuck Off' is an event that is participated by both the organizers and artists. In today's art, the alternative is playing the role of revising and criticizing the power discourse and mass convention. In an uncooperative and uncompromisable way, it self-consciously resists the threat of assimilation and vulgarization. A cultural attitude that stands against the power and makes no compromise with vulgarization is, together with independent individual experiences, feelings and creations, is what extends the pursuit and desire of art for spiritual freedom – an everlasting theme. Such a cultural attitude is obviously exclusive and alienated. It aims at dealing with such themes as cultural power, art institution, art trends, communications between the East and West, exoticism, post-modernism and post colonialism, etc.\n\n'Fuck Off' emphasizes the independent and critical stance that is basic to art existence, and its status of independence, freedom and plurality in the situation of contradictions and conflicts. It tries to provoke artist's responsibility and self-discipline, search for the way in which art lives as \"wildlife\", and raise questions about some issues of contemporary Chinese art.\n\nAllegory, directing questioning, resistance, alienation, dissolution, endurance, boredom, bias, absurdity, cynicism and self-entertainment are aspects of culture as well as features of existence. Such issues are re-presented here by the artists with unprecedented frankness and intelligence, which leaves behind fresh and stimulating information and traces of existence.\n\nIn this exhibition, participants and their works are not objects of choice, identification and judgment. They have no quest for any kind of excuse. Group identification and inner difference are both so fully respected and encouraged that it may be doubted if there is the necessity for the presence of audience.\n\nAn on-site ambiguity and uncertainty forces one to seek meaning and satisfaction only in the form of proliferation and postpone. Perhaps there is nothing that exists 'on-site', but what will last forever is the very uncooperativeness with any system of power discourse.\"\n\nAccidental Dropping \n\nAi Weiwei is so concerned with Chinese history as a whole and the fact that people are not playing the proper role in learning about it or preserving it. By dropping a near-priceless artifact, Ai is essentially asking if the Chinese people really care about their history, and why not? It is taking something that is worth an immense amount of money and reducing it to shards. If he can do this with an artifact this important, it should suffice to raise awareness about the importance of all aspects of Chinese history.\n\nGolden Sunlight (Performance) \n\nHe Yuchang does a large amount of performance art and is known to push the limits in what he does. His goal is to \"seek enduring and fearless confrontation with reality and a poetic expression for this.\"\n\nChinese Landscape: Tattoo No. 2 \n\nHuang Yan sees landscape paintings as a way of expressing himself. The landscapes he paints show tranquility, and does so on a variety of things. These scenes are painted on his body, on pork, and even on cow bones.\n\nMouse \n\nJin Le experiments with a wide variety of materials in order to demonstrate what he thinks that people and animals would look like if scientists succeeded in combining them into one form.\n\nPeace series No. 19 \n\nLiang Yue uses Photoshop as his main form of expression. He attempts to Photoshop a variety of ads onto photos he takes and then post them absolutely everywhere in an attempt to require people to see them.\n\nParadise Lost No. 17 \n\nMeng Huang was born in Beijing, and says \"I grew up, knowing nothing. Now I live in Beijing and find that works of art stars are very much westernized.\" His works seem to be reminiscent of an almost unattainable land that was once a paradise, but is now sad and downtrodden.\n\nStamping on Water \n\nSong Dong notes \"I find more pleasure in doing 'art' as a matter because of the openness of artistic language. As I understand it, the time is over when artistic styles are defined by medium, method and paradigm. When I make use of these, the only thing that I have in mind is whether they fit my ideas.\" His work seems to show a lot of disparity in life and the fact that humans are a lot less important to the world around them.\n\nSkin Graft \n\nZhu Yu is a very outspoken artist who uses his work to make a statement. His work is aimed at making people think about the world around them. He says that \"We're not very afraid that we are not thinking what others are thinking since such an issue is taken care of by our spirit. What we are afraid of is that people are thinking what they are not supposed to think. So we need to re-think over what people initially take to be right, and abstract out everything that has nothing to do with reality.\"\n\nAi Wei Wei, Cao Fei, Chen Lingyang, Chen Shaoxiong, Chen Yunquan, Ding Yi, Feng Weidong, Gu Dexin, He An, He Yunchang, Huang Lei, Huang Yan, Jin Lei, Li Wen, Li Zhiwang, Liang Yue, Liang Yue, Lin Yilin, Lu Chunsheng, Lu Qing, Meng Huang, Peng Yu, Peng Donghui, Qin Ga, Rong Rong, Song Dong, Sun Yuan, Wang Bing, Wang Yin, Wang Chuyu, Wang Xingwei, Wu Ershan, Xiao Yu, Xu Tan, Xu Zhen, Yang Yong, Yang Fudong, Yang Maoyuan, Zhang Zhenzhong, Yang Zhichao, Zhang Dali, Zhang Shengquan, Zheng Guogu, Zhu Ming, and Zhu Yu. Chen Hao, Zheng Jishun, and Song Tao exhibited a video documenting their walk through the city while blood leaked from plastic tubes inserted into their veins.\n"}
{"id": "2318826", "url": "https://en.wikipedia.org/wiki?curid=2318826", "title": "Generality (psychology)", "text": "Generality (psychology)\n\nIn behavioral psychology, the assumption of generality is the assumption that the results of experiments involving schedules of reinforcement, conducted on non-human subjects (often pigeons), can be generalized to apply to humans. If the assumption holds, many aspects of daily human life can be understood in terms of these results. The naturalization of sunlight helps our bodies to stay awake and keep motivated. The darkness that comes with night tells our body to slow down for the day and get some rest. The ability to survive comes with generality. Experiments have been done to test inescapability and insolubility.\n\nFergus Lowe has questioned the generality of schedule effects in cases of fixed-interval performance among humans and non-humans.\n\nThe ability to generalize information from one situation to another is a function of several factors: the reliability of the original information; the paradigm's validity; one's understanding of the paradigm, the true determinants of the behavior, and the relevant details of the situations in question; and the similarity between the original source of the data and the situation to which it is to be applied.\n\nThere are both similarities and differences between the terms \"stimulus generalization\" and \"generality of a functional relationship.\" Stimulus generalization is the description of the fact that an organism behaves in a similar way to similar stimuli, and that the more different the stimuli, the more different the behavior. The generality of a finding refers to the degree to which a functional relationship obtained in one situation is able to predict the obtained relationship in a new situation.\n\n\"Generality\" refers more to functional relationships than individual events. That responses occur to X about the same as to Z is irrelevant; rather, that distributed practice helps in learning nonsense syllables and in learning other tasks.\n"}
{"id": "7096967", "url": "https://en.wikipedia.org/wiki?curid=7096967", "title": "Geothermal heat pump", "text": "Geothermal heat pump\n\nA geothermal heat pump or ground source heat pump (GSHP) is a central heating and/or cooling system that transfers heat to or from the ground.\n\nIt uses the earth all the time, without any intermittency, as a heat source (in the winter) or a heat sink (in the summer). This design takes advantage of the moderate temperatures in the ground to boost efficiency and reduce the operational costs of heating and cooling systems, and may be combined with solar heating to form a geosolar system with even greater efficiency. They are also known by other names, including geoexchange, earth-coupled, earth energy systems. The engineering and scientific communities prefer the terms \"\"geoexchange\" or \"ground source heat pumps\"\" to avoid confusion with traditional geothermal power, which uses a high temperature heat source to generate electricity. Ground source heat pumps harvest heat absorbed at the Earth's surface from solar energy. The temperature in the ground below is roughly equal to the mean annual air temperature at that latitude at the surface.\n\nDepending on latitude, the temperature beneath the upper of Earth's surface maintains a nearly constant temperature between 10 and 16 °C (50 and 60 °F), if the temperature is undisturbed by the presence of a heat pump. Like a refrigerator or air conditioner, these systems use a heat pump to force the transfer of heat from the ground. Heat pumps can transfer heat from a cool space to a warm space, against the natural direction of flow, or they can enhance the natural flow of heat from a warm area to a cool one. The core of the heat pump is a loop of refrigerant pumped through a vapor-compression refrigeration cycle that moves heat. Air-source heat pumps are typically more efficient at heating than pure electric heaters, even when extracting heat from cold winter air, although efficiencies begin dropping significantly as outside air temperatures drop below 5 °C (41 °F). A ground source heat pump exchanges heat with the ground. This is much more energy-efficient because underground temperatures are more stable than air temperatures through the year. Seasonal variations drop off with depth and disappear below to due to thermal inertia. Like a cave, the shallow ground temperature is warmer than the air above during the winter and cooler than the air in the summer. A ground source heat pump extracts ground heat in the winter (for heating) and transfers heat back into the ground in the summer (for cooling). Some systems are designed to operate in one mode only, heating or cooling, depending on climate.\n\nGeothermal pump systems reach fairly high coefficient of performance (CoP), 3 to 6, on the coldest of winter nights, compared to 1.75–2.5 for air-source heat pumps on cool days. Ground source heat pumps (GSHPs) are among the most energy-efficient technologies for providing HVAC and water heating.\n\nSetup costs are higher than for conventional systems, but the difference is usually returned in energy savings in 3 to 10 years, and even shorter lengths of time with federal, state and utility tax credits and incentives. Geothermal heat pump systems are reasonably warranted by manufacturers, and their working life is estimated at 25 years for inside components and 50+ years for the ground loop. As of 2004, there are over one million units installed worldwide providing 12 GW of thermal capacity, with an annual growth rate of 10%.\n\nSome confusion exists with regard to the terminology of heat pumps and the use of the term \"\"geothermal\". \"Geothermal\" derives from the Greek and means \"Earth heat\" – which geologists and many laymen understand as describing hot rocks, volcanic activity or heat derived from deep within the earth. Though some confusion arises when the term \"geothermal\" is also used to apply to temperatures within the first 100 metres of the surface, this is \"Earth heat\"\" all the same, though it is largely influenced by stored energy from the sun.\n\nThe heat pump was described by Lord Kelvin in 1853 and developed by Peter Ritter von Rittinger in 1855. After experimenting with a freezer, Robert C. Webber built the first direct exchange ground-source heat pump in the late 1940s. The first successful commercial project was installed in the Commonwealth Building (Portland, Oregon) in 1948, and has been designated a National Historic Mechanical Engineering Landmark by ASME. The technology became popular in Sweden in the 1970s, and has been growing slowly in worldwide acceptance since then. Open loop systems dominated the market until the development of polybutylene pipe in 1979 made closed loop systems economically viable. As of 2004, there are over a million units installed worldwide providing 12 GW of thermal capacity. Each year, about 80,000 units are installed in the US (geothermal energy is used in all 50 U.S. states today, with great potential for near-term market growth and savings) and 27,000 in Sweden. In Finland, a geothermal heat pump was the most common heating system choice for new detached houses between 2006 and 2011 with market share exceeding 40%.\n\nHeat pumps provide winter heating by extracting heat from a source and transferring it into a building. Heat can be extracted from any source, no matter how cold, but a warmer source allows higher efficiency. A ground source heat pump uses the top layer of the earth's crust as a source of heat, thus taking advantage of its seasonally moderated temperature.\n\nIn the summer, the process can be reversed so the heat pump extracts heat from the building and transfers it to the ground. Transferring heat to a cooler space takes less energy, so the cooling efficiency of the heat pump gains benefits from the lower ground temperature.\n\nGround source heat pumps employ a heat exchanger in contact with the ground or groundwater to extract or dissipate heat. This component accounts for anywhere from a fifth to half of the total system cost, and would be the most cumbersome part to repair or replace. Correctly sizing this component is necessary to assure long-term performance: the energy efficiency of the system improves with roughly 4% for every degree Celsius that is won through correct sizing, and the underground temperature balance must be maintained through proper design of the whole system. Incorrect design can result in the system freezing after a number of years or very inefficient system performance; thus accurate system design is critical to a successful system \n\nShallow horizontal heat exchangers experience seasonal temperature cycles due to solar gains and transmission losses to ambient air at ground level. These temperature cycles lag behind the seasons because of thermal inertia, so the heat exchanger will harvest heat deposited by the sun several months earlier, while being weighed down in late winter and spring, due to accumulated winter cold. Deep vertical systems deep rely on migration of heat from surrounding geology, unless they are recharged annually by solar recharge of the ground or exhaust heat from air conditioning systems.\n\nSeveral major design options are available for these, which are classified by fluid and layout. Direct exchange systems circulate refrigerant underground, closed loop systems use a mixture of anti-freeze and water, and open loop systems use natural groundwater.\n\nThe direct exchange geothermal heat pump (DX) is the oldest type of geothermal heat pump technology. The ground-coupling is achieved through a single loop, circulating refrigerant, in direct thermal contact with the ground (as opposed to a combination of a refrigerant loop and a water loop). The refrigerant leaves the heat pump cabinet, circulates through a loop of copper tube buried underground, and exchanges heat with the ground before returning to the pump. The name \"direct exchange\" refers to heat transfer between the refrigerant loop and the ground without the use of an intermediate fluid. There is no direct interaction between the fluid and the earth; only heat transfer through the pipe wall. Direct exchange heat pumps are not to be confused with \"water-source heat pumps\" or \"water loop heat pumps\" since there is no water in the ground loop. ASHRAE defines the term ground-coupled heat pump to encompass closed loop and direct exchange systems, while excluding open loops.\n\nDirect exchange systems are more efficient and have potentially lower installation costs than closed loop water systems. Copper's high thermal conductivity contributes to the higher efficiency of the system, but heat flow is predominantly limited by the thermal conductivity of the ground, not the pipe. The main reasons for the higher efficiency are the elimination of the water pump (which uses electricity), the elimination of the water-to-refrigerant heat exchanger (which is a source of heat losses), and most importantly, the latent heat phase change of the refrigerant in the ground itself.\n\nHowever, in case of leakage there is virtually no risk of contaminating the ground or the ground water. Contrary to water-source geothermal systems, direct exchange systems do not contain antifreeze. So, in case of a refrigerant leakage, the refrigerant currently used in most systems – R-410A – would immediately vaporize and seek the atmosphere. This is due to the low boiling point of R-410A: . R-410A refrigerant replaces larger volumes of antifreeze mixtures used in water-source geothermal systems and presents no threat to aquifers or to the ground itself.\n\nWhile they require more refrigerant and their tubing is more expensive per foot, a direct exchange earth loop is shorter than a closed water loop for a given capacity. A direct exchange system requires only 15 to 40% of the length of tubing and half the diameter of drilled holes, and the drilling or excavation costs are therefore lower. Refrigerant loops are less tolerant of leaks than water loops because gas can leak out through smaller imperfections. This dictates the use of brazed copper tubing, even though the pressures are similar to water loops. The copper loop must be protected from corrosion in acidic soil through the use of a sacrificial anode or other cathodic protection.\n\nThe U.S. Environmental Protection Agency conducted field monitoring of a direct geoexchange heat pump water heating system in a commercial application. The EPA reported that the system saved 75% of the electrical energy that would have been required by an electrical resistance water heating unit. According to the EPA, if the system is operated to capacity, it can avoid the emission of up to 7,100 pounds of CO and 15 pounds of NO each year per ton of compressor capacity (or 42,600 lbs. of CO and 90 lbs. of NO for a typical 6 ton system).\n\nIn Northern climates, although the earth temperature is cooler, so is the incoming water temperature, which enables the high efficiency systems to replace more energy than would otherwise be required of electric or fossil fuel fired systems. Any temperature above is sufficient to evaporate the refrigerant, and the direct exchange system can harvest energy through ice.\n\nIn extremely hot climates with dry soil, the addition of an auxiliary cooling module as a second condenser in line between the compressor and the earth loops increases efficiency and can further reduce the amount of earth loop to be installed.\n\nMost installed systems have two loops on the ground side: the primary refrigerant loop is contained in the appliance cabinet where it exchanges heat with a secondary water loop that is buried underground. The secondary loop is typically made of high-density polyethylene pipe and contains a mixture of water and anti-freeze (propylene glycol, denatured alcohol or methanol). Monopropylene glycol has the least damaging potential when it might leak into the ground, and is therefore the only allowed anti-freeze in ground sources in an increasing number of European countries. After leaving the internal heat exchanger, the water flows through the secondary loop outside the building to exchange heat with the ground before returning. The secondary loop is placed below the frost line where the temperature is more stable, or preferably submerged in a body of water if available. Systems in wet ground or in water are generally more efficient than drier ground loops since water conducts and stores heat better than solids in sand or soil. If the ground is naturally dry, soaker hoses may be buried with the ground loop to keep it wet.\nClosed loop systems need a heat exchanger between the refrigerant loop and the water loop, and pumps in both loops. Some manufacturers have a separate ground loop fluid pump pack, while some integrate the pumping and valving within the heat pump. Expansion tanks and pressure relief valves may be installed on the heated fluid side. Closed loop systems have lower efficiency than direct exchange systems, so they require longer and larger pipe to be placed in the ground, increasing excavation costs.\n\nClosed loop tubing can be installed horizontally as a loop field in trenches or vertically as a series of long U-shapes in wells (see below). The size of the loop field depends on the soil type and moisture content, the average ground temperature and the heat loss and or gain characteristics of the building being conditioned. A rough approximation of the initial soil temperature is the average daily temperature for the region.\n\nA vertical closed loop field is composed of pipes that run vertically in the ground. A hole is bored in the ground, typically deep. Pipe pairs in the hole are joined with a U-shaped cross connector at the bottom of the hole. The borehole is commonly filled with a bentonite grout surrounding the pipe to provide a thermal connection to the surrounding soil or rock to improve the heat transfer. Thermally enhanced grouts are available to improve this heat transfer. Grout also protects the ground water from contamination, and prevents artesian wells from flooding the property. Vertical loop fields are typically used when there is a limited area of land available. Bore holes are spaced at least 5–6 m apart and the depth depends on ground and building characteristics. For illustration, a detached house needing 10 kW (3 ton) of heating capacity might need three boreholes deep. (A ton of heat is 12,000 British thermal units per hour (BTU/h) or 3.5 kilowatts.) During the cooling season, the local temperature rise in the bore field is influenced most by the moisture travel in the soil. Reliable heat transfer models have been developed through sample bore holes as well as other tests.\n\nA horizontal closed loop field is composed of pipes that run horizontally in the ground. A long horizontal trench, deeper than the frost line, is dug and U-shaped or slinky coils are placed horizontally inside the same trench. Excavation for shallow horizontal loop fields is about half the cost of vertical drilling, so this is the most common layout used wherever there is adequate land available. For illustration, a detached house needing 10 kW (3 ton) of heating capacity might need three loops long of NPS 3/4 (DN 20) or NPS 1.25 (DN 32) polyethylene tubing at a depth of .\n\nThe depth at which the loops are placed significantly influences the energy consumption of the heat pump in two opposite ways: shallow loops tend to indirectly absorb more heat from the sun, which is helpful, especially when the ground is still cold after a long winter. On the other hand, shallow loops are also cooled down much more readily by weather changes, especially during long cold winters, when heating demand peaks. Often, the second effect is much greater than the first one, leading to higher costs of operation for the more shallow ground loops. This problem can be reduced by increasing both the depth and the length of piping, thereby significantly increasing costs of installation. However, such expenses might be deemed feasible, as they may result in lower operating costs. Recent studies show that utilization of a non-homogeneous soil profile with a layer of low conductive material above the ground pipes can help mitigate the adverse effects of shallow pipe burial depth. The intermediate blanket with lower conductivity than the surrounding soil profile demonstrated the potential to increase the energy extraction rates from the ground to as high as 17% for a cold climate and about 5–6% for a relatively moderate climate.\n\nA slinky (also called coiled) closed loop field is a type of horizontal closed loop where the pipes overlay each other (not a recommended method). The easiest way of picturing a slinky field is to imagine holding a slinky on the top and bottom with your hands and then moving your hands in opposite directions. A slinky loop field is used if there is not adequate room for a true horizontal system, but it still allows for an easy installation. Rather than using straight pipe, slinky coils use overlapped loops of piping laid out horizontally along the bottom of a wide trench. Depending on soil, climate and the heat pump's run fraction, slinky coil trenches can be up to two thirds shorter than traditional horizontal loop trenches. Slinky coil ground loops are essentially a more economical and space efficient version of a horizontal ground loop.\n\nAs an alternative to trenching, loops may be laid by mini horizontal directional drilling (mini-HDD). This technique can lay piping under yards, driveways, gardens or other structures without disturbing them, with a cost between those of trenching and vertical drilling. This system also differs from horizontal & vertical drilling as the loops are installed from one central chamber, further reducing the ground space needed. Radial drilling is often installed retroactively (after the property has been built) due to the small nature of the equipment used and the ability to bore beneath existing constructions.\n\nA closed pond loop is not common because it depends on proximity to a body of water, where an open loop system is usually preferable. A pond loop may be advantageous where poor water quality precludes an open loop, or where the system heat load is small. A pond loop consists of coils of pipe similar to a slinky loop attached to a frame and located at the bottom of an appropriately sized pond or water source.\n\nIn an open loop system (also called a groundwater heat pump), the secondary loop pumps natural water from a well or body of water into a heat exchanger inside the heat pump. ASHRAE calls open loop systems \"groundwater heat pumps\" or \"surface water heat pumps\", depending on the source. Heat is either extracted or added by the primary refrigerant loop, and the water is returned to a separate injection well, irrigation trench, tile field or body of water. The supply and return lines must be placed far enough apart to ensure thermal recharge of the source. Since the water chemistry is not controlled, the appliance may need to be protected from corrosion by using different metals in the heat exchanger and pump. Limescale may foul the system over time and require periodic acid cleaning. This is much more of a problem with cooling systems than heating systems. Also, as fouling decreases the flow of natural water, it becomes difficult for the heat pump to exchange building heat with the groundwater. If the water contains high levels of salt, minerals, iron bacteria or hydrogen sulfide, a closed loop system is usually preferable.\n\nDeep lake water cooling uses a similar process with an open loop for air conditioning and cooling. Open loop systems using ground water are usually more efficient than closed systems because they are better coupled with ground temperatures. Closed loop systems, in comparison, have to transfer heat across extra layers of pipe wall and dirt.\n\nA growing number of jurisdictions have outlawed open-loop systems that drain to the surface because these may drain aquifers or contaminate wells. This forces the use of more environmentally sound injection wells or a closed loop system.\n\nA standing column well system is a specialized type of open loop system. Water is drawn from the bottom of a deep rock well, passed through a heat pump, and returned to the top of the well, where traveling downwards it exchanges heat with the surrounding bedrock. The choice of a standing column well system is often dictated where there is near-surface bedrock and limited surface area is available. A standing column is typically not suitable in locations where the geology is mostly clay, silt, or sand. If bedrock is deeper than from the surface, the cost of casing to seal off the overburden may become prohibitive.\n\nA multiple standing column well system can support a large structure in an urban or rural application. The standing column well method is also popular in residential and small commercial applications. There are many successful applications of varying sizes and well quantities in the many boroughs of New York City, and is also the most common application in the New England states. This type of ground source system has some heat storage benefits, where heat is rejected from the building and the temperature of the well is raised, within reason, during the summer cooling months which can then be harvested for heating in the winter months, thereby increasing the efficiency of the heat pump system. As with closed loop systems, sizing of the standing column system is critical in reference to the heat loss and gain of the existing building. As the heat exchange is actually with the bedrock, using water as the transfer medium, a large amount of production capacity (water flow from the well) is not required for a standing column system to work. However, if there is adequate water production, then the thermal capacity of the well system can be enhanced by discharging a small percentage of system flow during the peak Summer and Winter months.\n\nSince this is essentially a water pumping system, standing column well design requires critical considerations to obtain peak operating efficiency. Should a standing column well design be misapplied, leaving out critical shut-off valves for example, the result could be an extreme loss in efficiency and thereby cause operational cost to be higher than anticipated.\n\nThe heat pump is the central unit that becomes the heating and cooling plant for the building. Some models may cover space heating, space cooling, (space heating via conditioned air, hydronic systems and / or radiant heating systems), domestic or pool water preheat (via the desuperheater function), demand hot water, and driveway ice melting all within one appliance with a variety of options with respect to controls, staging and zone control. The heat may be carried to its end use by circulating water or forced air. Almost all types of heat pumps are produced for commercial and residential applications.\n\n\"Liquid-to-air\" heat pumps (also called \"water-to-air\") output forced air, and are most commonly used to replace legacy forced air furnaces and central air conditioning systems. There are variations that allow for split systems, high-velocity systems, and ductless systems. Heat pumps cannot achieve as high a fluid temperature as a conventional furnace, so they require a higher volume flow rate of air to compensate. When retrofitting a residence, the existing duct work may have to be enlarged to reduce the noise from the higher air flow.\n\"Liquid-to-water\" heat pumps (also called \"water-to-water\") are hydronic systems that use water to carry heating or cooling through the building. Systems such as radiant underfloor heating, baseboard radiators, conventional cast iron radiators would use a liquid-to-water heat pump. These heat pumps are preferred for pool heating or domestic hot water pre-heat. Heat pumps can only heat water to about efficiently, whereas a boiler normally reaches . Legacy radiators designed for these higher temperatures may have to be doubled in numbers when retrofitting a home. A hot water tank will still be needed to raise water temperatures above the heat pump's maximum, but pre-heating will save 25–50% of hot water costs.\n\nGround source heat pumps are especially well matched to underfloor heating and baseboard radiator systems which only require warm temperatures 40 °C (104 °F) to work well. Thus they are ideal for open plan offices. Using large surfaces such as floors, as opposed to radiators, distributes the heat more uniformly and allows for a lower water temperature. Wood or carpet floor coverings dampen this effect because the thermal transfer efficiency of these materials is lower than that of masonry floors (tile, concrete). Underfloor piping, ceiling or wall radiators can also be used for cooling in dry climates, although the temperature of the circulating water must be above the dew point to ensure that atmospheric humidity does not condense on the radiator.\n\nCombination heat pumps are available that can produce forced air and circulating water simultaneously and individually. These systems are largely being used for houses that have a combination of air and liquid conditioning needs, for example central air conditioning and pool heating.\n\nThe efficiency of ground source heat pumps can be greatly improved by using seasonal thermal energy storage and interseasonal heat transfer. Heat captured and stored in thermal banks in the summer can be retrieved efficiently in the winter. Heat storage efficiency increases with scale, so this advantage is most significant in commercial or district heating systems.\n\nGeosolar combisystems have been used to heat and cool a greenhouse using an aquifer for thermal storage. In summer, the greenhouse is cooled with cold ground water. This heats the water in the aquifer which can become a warm source for heating in winter. The combination of cold and heat storage with heat pumps can be combined with water/humidity regulation. These principles are used to provide renewable heat and renewable cooling to all kinds of buildings.\n\nAlso the efficiency of existing small heat pump installations can be improved by adding large, cheap, water filled solar collectors. These may be integrated into a to-be-overhauled parking lot, or in walls or roof constructions by installing one-inch PE pipes into the outer layer.\n\nThe net thermal efficiency of a heat pump should take into account the efficiency of electricity generation and transmission, typically about 30%. Since a heat pump moves three to five times more heat energy than the electric energy it consumes, the total energy output is much greater than the electrical input. This results in net thermal efficiencies greater than 300% as compared to radiant electric heat being 100% efficient. Traditional combustion furnaces and electric heaters can never exceed 100% efficiency.\n\nGeothermal heat pumps can reduce energy consumption— and corresponding air pollution emissions—up to 44% compared to air source heat pumps and up to 72% compared to electric resistance heating with standard air-conditioning equipment.\n\nThe dependence of net thermal efficiency on the electricity infrastructure tends to be an unnecessary complication for consumers and is not applicable to hydroelectric power, so performance of heat pumps is usually expressed as the ratio of heating output or heat removal to electricity input. Cooling performance is typically expressed in units of BTU/hr/watt as the energy efficiency ratio (EER), while heating performance is typically reduced to dimensionless units as the coefficient of performance (COP). The conversion factor is 3.41 BTU/hr/watt. Performance is influenced by all components of the installed system, including the soil conditions, the ground-coupled heat exchanger, the heat pump appliance, and the building distribution, but is largely determined by the \"lift\" between the input temperature and the output temperature.\n\nFor the sake of comparing heat pump appliances to each other, independently from other system components, a few standard test conditions have been established by the American Refrigerant Institute (ARI) and more recently by the International Organization for Standardization. Standard ARI 330 ratings were intended for closed loop ground-source heat pumps, and assume secondary loop water temperatures of for air conditioning and for heating. These temperatures are typical of installations in the northern US. Standard ARI 325 ratings were intended for open loop ground-source heat pumps, and include two sets of ratings for groundwater temperatures of and . ARI 325 budgets more electricity for water pumping than ARI 330. Neither of these standards attempt to account for seasonal variations. Standard ARI 870 ratings are intended for direct exchange ground-source heat pumps. ASHRAE transitioned to ISO 13256-1 in 2001, which replaces ARI 320, 325 and 330. The new ISO standard produces slightly higher ratings because it no longer budgets any electricity for water pumps.\n\nEfficient compressors, variable speed compressors and larger heat exchangers all contribute to heat pump efficiency. Residential ground source heat pumps on the market today have standard COPs ranging from 2.4 to 5.0 and EERs ranging from 10.6 to 30. To qualify for an Energy Star label, heat pumps must meet certain minimum COP and EER ratings which depend on the ground heat exchanger type. For closed loop systems, the ISO 13256-1 heating COP must be 3.3 or greater and the cooling EER must be 14.1 or greater.\n\nActual installation conditions may produce better or worse efficiency than the standard test conditions. COP improves with a lower temperature difference between the input and output of the heat pump, so the stability of ground temperatures is important. If the loop field or water pump is undersized, the addition or removal of heat may push the ground temperature beyond standard test conditions, and performance will be degraded. Similarly, an undersized blower may allow the plenum coil to overheat and degrade performance.\n\nSoil without artificial heat addition or subtraction and at depths of several metres or more remains at a relatively constant temperature year round. This temperature equates roughly to the average annual air-temperature of the chosen location, usually at a depth of in the northern US. Because this temperature remains more constant than the air temperature throughout the seasons, geothermal heat pumps perform with far greater efficiency during extreme air temperatures than air conditioners and air-source heat pumps.\n\nStandards ARI 210 and 240 define Seasonal Energy Efficiency Ratio (SEER) and Heating Seasonal Performance Factors (HSPF) to account for the impact of seasonal variations on air source heat pumps. These numbers are normally not applicable and should not be compared to ground source heat pump ratings. However, Natural Resources Canada has adapted this approach to calculate typical seasonally adjusted HSPFs for ground-source heat pumps in Canada. The NRC HSPFs ranged from 8.7 to 12.8 BTU/hr/watt (2.6 to 3.8 in nondimensional factors, or 255% to 375% seasonal average electricity utilization efficiency) for the most populated regions of Canada. When combined with the thermal efficiency of electricity, this corresponds to net average thermal efficiencies of 100% to 150%.\n\nThe US Environmental Protection Agency (EPA) has called ground source heat pumps the most energy-efficient, environmentally clean, and cost-effective space conditioning systems available. Heat pumps offer significant emission reductions potential, particularly where they are used for both heating and cooling and where the electricity is produced from renewable resources.\n\nGSHPs have unsurpassed thermal efficiencies and produce zero emissions locally, but their electricity supply includes components with high greenhouse gas emissions, unless the owner has opted for a 100% renewable energy supply. Their environmental impact therefore depends on the characteristics of the electricity supply and the available alternatives. \n\nThe GHG emissions savings from a heat pump over a conventional furnace can be calculated based on the following formula:\n\nformula_1\n\nGround-source heat pumps always produce fewer greenhouse gases than air conditioners, oil furnaces, and electric heating, but natural gas furnaces may be competitive depending on the greenhouse gas intensity of the local electricity supply. In countries like Canada and Russia with low emitting electricity infrastructure, a residential heat pump may save 5 tons of carbon dioxide per year relative to an oil furnace, or about as much as taking an average passenger car off the road. But in cities like Beijing or Pittsburgh that are highly reliant on coal for electricity production, a heat pump may result in 1 or 2 tons more carbon dioxide emissions than a natural gas furnace. For areas not served by utility natural gas infrastructure, however, no better alternative exists.\n\nThe fluids used in closed loops may be designed to be biodegradable and non-toxic, but the refrigerant used in the heat pump cabinet and in direct exchange loops was, until recently, chlorodifluoromethane, which is an ozone depleting substance. Although harmless while contained, leaks and improper end-of-life disposal contribute to enlarging the ozone hole. For new construction, this refrigerant is being phased out in favor of the ozone-friendly but potent greenhouse gas R410A. The EcoCute water heater is an air-source heat pump that uses carbon dioxide as its working fluid instead of chlorofluorocarbons. Open loop systems (i.e. those that draw ground water as opposed to closed loop systems using a borehole heat exchanger) need to be balanced by reinjecting the spent water. This prevents aquifer depletion and the contamination of soil or surface water with brine or other compounds from underground.\n\nBefore drilling, the underground geology needs to be understood, and drillers need to be prepared to seal the borehole, including preventing penetration of water between strata. The unfortunate example is a geothermal heating project in Staufen im Breisgau, Germany which seems the cause of considerable damage to historical buildings there. In 2008, the city centre was reported to have risen 12 cm, after initially sinking a few millimeters. The boring tapped a naturally pressurized aquifer, and via the borehole this water entered a layer of anhydrite, which expands when wet as it forms gypsum. The swelling will stop when the anhydrite is fully reacted, and reconstruction of the city center \"is not expedient until the uplift ceases.\" By 2010 sealing of the borehole had not been accomplished. By 2010, some sections of town had risen by 30 cm.\n\nGround-source heat pump technology, like building orientation, is a natural building technique (bioclimatic building).\n\nGround source heat pumps are characterized by high capital costs and low operational costs compared to other HVAC systems. Their overall economic benefit depends primarily on the relative costs of electricity and fuels, which are highly variable over time and across the world. Based on recent prices, ground-source heat pumps currently have lower operational costs than any other conventional heating source almost everywhere in the world. Natural gas is the only fuel with competitive operational costs, and only in a handful of countries where it is exceptionally cheap, or where electricity is exceptionally expensive. In general, a homeowner may save anywhere from 20% to 60% annually on utilities by switching from an ordinary system to a ground-source system.\n\nCapital costs and system lifespan have received much less study until recently, and the return on investment is highly variable. The most recent data from an analysis of 2011–2012 incentive payments in the state of Maryland showed an average cost of residential systems of $1.90 per watt, or about $26,700 for a typical (4 ton) home system. An older study found the total installed cost for a system with 10 kW (3 ton) thermal capacity for a detached rural residence in the US averaged $8000–$9000 in 1995 US dollars. More recent studies found an average cost of $14,000 in 2008 US dollars for the same size system. The US Department of Energy estimates a price of $7500 on its website, last updated in 2008. One source in Canada placed prices in the range of $30,000–$34,000 Canadian dollars. The rapid escalation in system price has been accompanied by rapid improvements in efficiency and reliability. Capital costs are known to benefit from economies of scale, particularly for open loop systems, so they are more cost-effective for larger commercial buildings and harsher climates. The initial cost can be two to five times that of a conventional heating system in most residential applications, new construction or existing. In retrofits, the cost of installation is affected by the size of living area, the home's age, insulation characteristics, the geology of the area, and location of the property. Proper duct system design and mechanical air exchange should be considered in the initial system cost.\n\nCapital costs may be offset by government subsidies; for example, Ontario offered $7000 for residential systems installed in the 2009 fiscal year. Some electric companies offer special rates to customers who install a ground-source heat pump for heating or cooling their building. Where electrical plants have larger loads during summer months and idle capacity in the winter, this increases electrical sales during the winter months. Heat pumps also lower the load peak during the summer due to the increased efficiency of heat pumps, thereby avoiding costly construction of new power plants. For the same reasons, other utility companies have started to pay for the installation of ground-source heat pumps at customer residences. They lease the systems to their customers for a monthly fee, at a net overall saving to the customer.\n\nThe lifespan of the system is longer than conventional heating and cooling systems. Good data on system lifespan is not yet available because the technology is too recent, but many early systems are still operational today after 25–30 years with routine maintenance. Most loop fields have warranties for 25 to 50 years and are expected to last at least 50 to 200 years. Ground-source heat pumps use electricity for heating the house. The higher investment above conventional oil, propane or electric systems may be returned in energy savings in 2–10 years for residential systems in the US. If compared to natural gas systems, the payback period can be much longer or non-existent. The payback period for larger commercial systems in the US is 1–5 years, even when compared to natural gas. Additionally, because geothermal heat pumps usually have no outdoor compressors or cooling towers, the risk of vandalism is reduced or eliminated, potentially extending a system's lifespan.\n\nGround source heat pumps are recognized as one of the most efficient heating and cooling systems on the market. They are often the second-most cost effective solution in extreme climates (after co-generation), despite reductions in thermal efficiency due to ground temperature. (The ground source is warmer in climates that need strong air conditioning, and cooler in climates that need strong heating.) The financial viability of these systems depends on the adequate sizing of ground heat exchangers (GHEs), which generally contribute the most to the overall capital costs of GSHP systems.\n\nCommercial systems maintenance costs in the US have historically been between $0.11 to $0.22 per m per year in 1996 dollars, much less than the average $0.54 per m per year for conventional HVAC systems.\n\nGovernments that promote renewable energy will likely offer incentives for the consumer (residential), or industrial markets. For example, in the United States, incentives are offered both on the state and federal levels of government. In the United Kingdom the Renewable Heat Incentive provides a financial incentive for generation of renewable heat based on metered readings on an annual basis for 20 years for commercial buildings. The domestic Renewable Heat Incentive is due to be introduced in Spring 2014 for seven years and be based on deemed heat.\n\nBecause of the technical knowledge and equipment needed to design and size the system properly (and install the piping if heat fusion is required), a GSHP system installation requires a professional's services. Several installers have published real-time views of system performance in an online community of recent residential installations. The International Ground Source Heat Pump Association (IGSHPA), Geothermal Exchange Organization (GEO), the Canadian GeoExchange Coalition and the Ground Source Heat Pump Association maintain listings of qualified installers in the US, Canada and the UK. Furthermore, detailed analysis of Soil thermal conductivity for horizontal systems and formation thermal conductivity for vertical systems will generally result in more accurately design systems with a higher efficiency.\n\n\n"}
{"id": "11358882", "url": "https://en.wikipedia.org/wiki?curid=11358882", "title": "Government database", "text": "Government database\n\nA government database collects information for various reasons, including climate monitoring, securities law compliance, geological surveys, patent applications and grants, surveillance, national security, border control, law enforcement, public health, voter registration, vehicle registration, social security, and statistics.\n\n\n\n\n\n\n\n\nVarious government bodies maintain databases about citizens and residents of the United Kingdom. Under the Data Protection Act 1998 and the Protection of Freedoms Act 2012, legal provisions exist that control and restrict the collection, storage, retention, and use of information in government databases.\n\n\n\nNATGRID: India is setting up a national intelligence grid called NATGRID, which will become operational in 2013. NATGRID would allow access to each individual's data ranging from land records, Internet logs, air and rail PNR, phone records, gun records, driving license, property records, insurance, and income tax records in real time and with no oversight. With a UID from the Unique Identification Authority of India being given to every Indian from February 2011, the government would be able track people in real time. A national population registry of all citizens will be established by the 2011 census, during which fingerprints and iris scans would be taken along with GPS records of each household. Access to the combined data will be given to 11 agencies, including the Research and Analysis Wing, the Intelligence Bureau, the Enforcement Directorate, the National Investigation Agency, the Central Bureau of Investigation, the Directorate of Revenue Intelligence and the Narcotics Control Bureau.\n\n\n\n\n"}
{"id": "14590005", "url": "https://en.wikipedia.org/wiki?curid=14590005", "title": "Grace (given name)", "text": "Grace (given name)\n\nGrace is a feminine given name from the Latin \"gratia\". It is often given in reference to the Christian concept of divine grace and used as a virtue name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "58919635", "url": "https://en.wikipedia.org/wiki?curid=58919635", "title": "Gustave Hermite", "text": "Gustave Hermite\n\nGustave Hermite (June 11, 1863 - November 9, 1914) was a French aeronaut and physicist, pioneer with Georges Besançon of the weather balloon. He was the nephew of Charles Hermite, one of the fathers of modern mathematical analysis.\n\nGustave Hermite was born in Nancy, France, on June 11, 1863. Fond of sciences, he began studying chemistry in 1884 at the laboratory of the Academy of Neuchâtel in Switzerland, then turned to astronomy. His first communication to the French Academy of Sciences in 1884 deals with an astronomical telescope of its design. In 1885, he became a member of the Astronomical Society of France. He then embarked on developing inventions and in 1887, he designed and made a rangefinder, then in 1888 a small captive helicopter powered by an electric motor attached to a battery on the ground.\n\nIn 1889 he began experimenting with heavier than air aviation: small airplanes propelled by rockets or kites used as a means of traction, on water or on ice. On August 17, 1889, he made his first flight in an untethered balloon and, with his friend Georges Besançon, in 1889 built the \"Sirius\" in which they made a trip from Paris to Le Creusot in 16 hours with a stop in the Yonne Department.\n\nIn 1890-91, Hermite and Besançon planned a flight over the North Pole but the project did not materialize for lack of funding. The two men began then working on high altitude balloons for scientific purposes. In early 1892, they launched a series of small paper balloons filled with gas to become familiar with the technique. On September 17, the first documented weather balloon in history flew, with a barometer and a minimum thermometer. The ball fell to the ground thanks to a parachute and the instruments could be recovered.\n\nHaving demonstrated the feasibility of the balloon probe, which he gave the name of weather balloon, and thanks to the support of the Union Aerophile de France, they then launch a series of balloons capable of carrying more than 10,000 meters a nacelle weighing several kilograms containing recording devices which inaugurated a series of international scientific ascents. In 1898 Léon Teisserenc de Bort organized the beginnings of systematic sounding of the atmosphere at the Trappes Observatory of Dynamic Meteorology .\n\nGustave Hermite died on November 9, 1914 in Bois-Colombes, a suburb of Paris.\n"}
{"id": "11358084", "url": "https://en.wikipedia.org/wiki?curid=11358084", "title": "Idealization and devaluation", "text": "Idealization and devaluation\n\nIn psychoanalytic theory, when an individual is unable to integrate difficult feelings, specific defenses are mobilized to overcome what the individual perceives as an unbearable situation. The defense that helps in this process is called splitting. Splitting is the tendency to view events or people as either all bad or all good. When viewing people as all good, the individual is said to be using the defense mechanism idealization: a mental mechanism in which the person attributes exaggeratedly positive qualities to the self or others. When viewing people as all bad, the individual employs devaluation: attributing exaggeratedly negative qualities to the self or others.\n\nIn child development, idealization and devaluation are quite normal. During the childhood development stage, individuals become capable of perceiving others as complex structures, containing both good and bad components. If the development stage is interrupted (by early childhood trauma, for example), these defense mechanisms may persist into adulthood.\n\nThe term idealization first appeared in connection with Freud's definition of narcissism. Freud's vision was that all human infants pass through a phase of primary narcissism in which they assume they are the centre of their universe. To obtain the parents' love the child comes to do what he thinks the parents value. Internalising these values the child forms an ego ideal. This ego ideal contains rules for good behaviour and standards of excellence toward which the ego has to strive. When the child cannot bear ambivalence between the real self and the ego ideal and defenses are used too often, it is called pathologic. Freud called this situation secondary narcissism, because the ego itself is idealized. Explanations of the idealization of others besides the self are sought in drive theory as well as in object-relation theory. From the viewpoint of libidinal drives, idealization of other people is a \"flowing-over\" of narcissistic libido onto the object; from the viewpoint of self-object relations, the object representations (like that of the caregivers) were made more beautiful than they really were.\n\nAn extension of Freud's theory of narcissism came when Heinz Kohut presented the so-called \"self-object transferences\" of idealization and mirroring. To Kohut, idealization in childhood is a healthy mechanism. If the parents fail to provide appropriate opportunities for idealization (healthy narcissism) and mirroring (how to cope with reality), the child does not develop beyond a developmental stage in which he sees himself as grandiose but in which he also remains dependent on others to provide his self-esteem. Kohut stated that, with narcissistic patients, idealization of the self and the therapist should be allowed during therapy and then very gradually will diminish as a result of unavoidable optimal frustration.\n\nOtto Kernberg has provided an extensive discussion of idealization, both in its defensive and adaptive aspects. He conceptualised idealization as involving a denial of unwanted characteristics of an object, then enhancing the object by projecting one's own libido or omnipotence on it. He proposed a developmental line with one end of the continuum being a normal form of idealization and the other end a pathological form. In the latter, the individual has a problem with object constancy and sees others as all good or all bad, thus bolstering idealization and devaluation. At this stage idealization is associated with borderline pathology. At the other end of the continuum, idealization is said to be a necessary precursor for feelings of mature love.\n"}
{"id": "42979524", "url": "https://en.wikipedia.org/wiki?curid=42979524", "title": "Intertrial priming", "text": "Intertrial priming\n\nIn cognitive psychology, intertrial priming is an accumulation of the priming effect over multiple trials, where \"priming\" is the effect of the exposure to one stimulus on subsequently presented stimuli. Intertrial priming occurs when a target feature (the characteristic that distinguishes targets from non-targets) is repeated from one trial to the next, and typically results in speeded response times to the target. A target is the stimulus participants are required to search for. For example, intertrial priming occurs when the task is to respond to either a red or a green target, and the response time to a red target is faster if the preceding trial also has a red target.\n\nVisual attention is influenced by top down and bottom up attentional processes. Top-down attention is allocated based on an observer's current knowledge about the stimuli. Participants in an experiment might be instructed to search for, and respond to a target object presented in a display that is a different colour than the other objects presented simultaneously. Top down knowledge of the dimension of the target (i.e. colour) can speed response times to target identification.\n\nBottom-up attention is involuntarily and automatically directed towards salient features in the environment such as a bright colour among dull colours. In experimental settings, the more different a stimulus is from other stimuli in a visual display, the more salient it is. Bottom-up attention is typically not guided by observers' goals or knowledge, only by the physical properties of the stimuli. Many studies employ various methods involving intertrial priming to assess the contribution of top down versus bottom up processes in guiding attention in visual search tasks.\n\nThere are factors in visual search tasks that the top down versus bottom up dichotomy does not take into consideration. Not all selection biases can be explained by physical saliency (bottom up) or observer goals (top down). Studies that have found that stimuli that are equally salient and are connected with rewards and can draw a participants' attention, even if this choice doesn't match their selection goals. An alternative framework has been proposed where past selection history, current goals and physical salience are integrated in a model of attentional control.\n\nIntertrial priming is an important aspect to consider in designing an experiment as it can influence the results if it is not considered/controlled. Intertrial priming is often measured using a visual search task. A typical visual search task involves participants searching for, and responding to, a target amongst a group of non-target items. Intertrial priming performance is generally measured by recording participants' reaction times to identify a target and comparing these times across trials. Different trial designs and visual search tasks can be employed to measure intertrial priming.\n\nStudies often compare blocked and mixed visual search trials to measure intertrial priming. Blocked trials are multiple, successively presented visual search trials that include the same target, and mixed trials are a randomised series of trials, each trial consisting of different targets. For example, a blocked trial condition may include searching for a green circle in trial 1, and in multiple successive preceding trials, whereas a mixed trial condition may include searching a for a green circle in trial 1, but a red circle in the proceeding trials. Blocking trials can control for effects of variability in targets. When a target with the same defining feature is repeated across trials (blocked conditions), participants reaction times are faster than when the target is not the same across trials (mixed conditions). This repetition effect is also cumulative. As the number of target repetitions increases, up to a certain point, participants reaction times are faster each time they are exposed to the same target in repeated trials. In mixed and blocked trials there can be a disparity in intertrial priming that results in faster reaction times in the blocked trials. Reaction times may be faster in blocked trials because participants are required to respond to targets that differ in only one dimension from non-targets.\n\nA cue is a presentation of a stimulus prior to a trial to inform the participant of an upcoming target feature. For example, a blue circle may be shown before a trial to signify a blue circle will be the target in the upcoming trial. Target relevant cues may be presented to participants to decrease their reaction time to the target in the display. These cues may be valid or invalid. Valid cues correctly predict the target stimulus but invalid cues do not. For example, if the target in an upcoming trial is a blue circle, a blue circle presented as a cue would be valid, but if a red circle was presented as a cue it would be invalid, as it doesn't correctly predict the blue circle target stimulus. Reaction times to valid cues are typically faster than reaction times to invalid cues. This phenomenon is known as a cueing effect.\n\nWhen a valid cue has a low probability of correct target prediction, there can still be a reliable cueing effect for valid cues, and faster reaction times to valid cues than invalid cues. This suggests that the cueing effect is not affected by the predictive nature of the cue, and may not be due to top-down control. If top-down control is involved in the response selection then invalid trials should have a faster response than valid trials because participants are aware that the likelihood of being presented with a valid trial is very low.\n\nPop-out search tasks include a target that differs in one dimension from a group of homogeneous non-target items. A dimension is a categorical feature of a stimulus such as its colour (i.e. a red target among green non-targets), its shape (a square target among circle non-targets) or its orientation (a vertically presented target among horizontal non-targets). Response times in pop-out searches are generally faster to targets when the colour of both targets and distractors remains the same throughout trials, and slower when these colours are switched during trials.\n\nA conjunctive search involves non-target stimuli that have more than one dimension in common with the target stimulus. For example, when a target is a green circle in a conjunctive search, non-targets (distractors) could be red circles and green squares. The target will share one dimension in common with one set of non-targets (i.e. shape) and another dimension in common with the other non-target group (i.e. colour). If target and distractor features are the same over consecutive trials, response times are faster than when these dimensions are not repeated.\n\nThe \"dimension-weighting account\" of visual selection states that there is a limit to the attentional weight that can be allocated to a particular dimension of an object at any one time. The dimensions of stimuli perceived as important to an observer are allocated more attentional weight (i.e. a target in a visual search), resulting in faster detection times. If a target dimension is known in advance, this can increase the saliency signals of the target. On the other hand, if the target dimension is unknown, attentional weight has to be shifted to the target dimension. When target dimensions remain the same across trials there is no change in attentional weight required, resulting in faster reaction times (intertrial facilitation).\n\nThe priming of pop-out hypothesis suggests performance in a visual search task involving a pop-out target can be affected by the search history of specific target features in previous trials. If target and distractor features are repeated in subsequent trials, reaction times will be faster than if these features change across trials. The hypothesis proposes the repetition of a target used in a preceding trial makes its pop-out features more salient to an observer and therefore increases the likelihood the observer will attend to it.\n\nThe episodic retrieval model suggests reduced response times in intertrial priming are due to the observers' retrieval of episodic memories relevant to the task. The hypothesis states that visual search is composed of three successive stages of processing:\nThis hypothesis argues that when a target presented in a previous trial is presented again in the current trial, processing of the target is accelerated in the target decision stage of the model, so that after identification the target is verified to assess whether it matches the previous target stored in episodic memory.\n\nMany theories focus on the repetition of target features as dominant explanation for the repetition effects seen in intertrial priming. If target features are the same over consecutive trials but distractor features are changed, response times are not as fast as if both target and distractor features are kept constant over trials. This suggests that intertrial priming may mainly be due to distractor feature repetition, and target feature repetition influences this only slightly. This distractor-based priming may be due to faster perceptual grouping of distractors across trials. Perceptual grouping of distractors allows the target presence or absence to be distinguished more quickly. However, the repetition of target defining features cannot be excluded as a contributor to the priming effect found in conjunctive searches.\n\n"}
{"id": "161979", "url": "https://en.wikipedia.org/wiki?curid=161979", "title": "Judgement", "text": "Judgement\n\nJudgement (or judgment) is the evaluation of evidence to make a decision. The term has four distinct uses:\n\nAdditionally, judgement can mean:\n\n\n"}
{"id": "332770", "url": "https://en.wikipedia.org/wiki?curid=332770", "title": "Lila (Hinduism)", "text": "Lila (Hinduism)\n\nLila (, IAST ') or Leela\"' can be loosely translated as the \"divine play\". The concept of Lila is common to both non-dualist and dualist philosophical schools, but has a markedly different significance in each. Within non-dualism, Lila is a way of describing all reality, including the cosmos, as the outcome of creative play by the divine absolute (Brahman). In the dualistic schools of Vaishnavism, Lila refers to the activities of God and his devotee, as well as the macrocosmic actions of the manifest universe, as seen in the Vaishnava scripture \"Srimad Bhagavatam\", verse 3.26.4:\n\nsa eṣa prakṛtiḿ sūkṣmāḿ <br>daivīḿ guṇamayīḿ vibhuḥ <br>yadṛcchayaivopagatām <br>abhyapadyata līlayā<br><br>\"As His pastimes, that Supreme Personality of Godhead, the greatest of the great, accepted the subtle material energy, which is invested with three material modes of nature.\"\n\nHindu denominations differ on how a human should react to awareness of Lila. Karma Yoga allows a joyful embrace of all aspects of life (\"intentional acceptance\") while maintaining distinction from the Supreme, while Bhakti and Jnana Yoga advocate striving for oneness with the Supreme. Lila is an important idea in the traditional worship of Krishna (as prankster) and Shiva (as dancer), and has been used by modern writers like Stephen Nachmanovitch, Fritjof Capra, and Alan Watts.\n\nLila is comparable to the Western theological position of Pandeism, which describes the Universe as God taking a physical form in order to experience the interplay between the elements of the Universe.\n\n\n\n"}
{"id": "27986832", "url": "https://en.wikipedia.org/wiki?curid=27986832", "title": "Macroscopic traffic flow model", "text": "Macroscopic traffic flow model\n\nA Macroscopic traffic flow model is a mathematical traffic model that formulates the relationships among traffic flow characteristics like density, flow, mean speed of a traffic stream, etc.. Such models are conventionally arrived at by integrating microscopic traffic flow models and converting the single-entity level characteristics to comparable system level characteristics.\n\nThe method of modeling traffic flow at macroscopic level originated under an assumption that traffic streams as a whole are comparable to fluid streams. The first major step in macroscopic modeling of traffic was taken by Lighthill and Whitham in 1955, when they indexed the comparability of ‘traffic flow on long crowded roads’ with ‘flood movements in long rivers’. A year later, Richards (1956) complemented the idea with the introduction of ‘shock-waves on the highway’, completing the so-called LWR model. \nMacroscopic modeling may be primarily classified according to the type of traffic as homogeneous and heterogeneous, and further with respect to the order of the mathematical model.\n\n"}
{"id": "17886679", "url": "https://en.wikipedia.org/wiki?curid=17886679", "title": "Marginal return", "text": "Marginal return\n\nMarginal return is the rate of return for a marginal increase in investment; roughly, this is the additional output resulting from a one-unit increase in the use of a variable input, while other inputs are constant.\n\n"}
{"id": "45292214", "url": "https://en.wikipedia.org/wiki?curid=45292214", "title": "Mark C. Suchman", "text": "Mark C. Suchman\n\nMark C. Suchman (born 1960) is an American sociologist, Professor in Sociology at Brown University, known for his work on Institutional theory, and particularly on \"managing legitimacy.\"\n\nSuchman obtained his AB in sociology at Harvard University in 1983 and his MA in sociology in 1985 at Stanford University. At Stanford he also obtained his PhD in sociology in 1994 with the thesis entitled \"On Advice of Counsel: Law Firms and Venture Capital Funds as Information Intermediaries in the Structuration of Silicon Valley\", under the supervision of W. Richard Scott. In addition, he holds a JD from Yale Law School (1989).\n\nSuchman started his academic career as a research assistant under Harrison White at Harvard in 1982. At Stanford from 1985 to 1989, he was a teaching assistant, subsequently, of Morris Zelditch, Ann Swidler, Nancy Tuma and Lawrence Wu. In 1993, he started at the University of Wisconsin–Madison as assistant professor of sociology and law, and was promoted to associate professor in 1998, and full professor in 2003. In 2008, he moved to Brown University, Rhode Island, where he was appointed Professor of Sociology. From 2011 to 2012, he was program director for Social, Human and Organizational Factors and Resources in the National Science Foundation's Division of Advanced Cyberinfrastructure.\n\nSuchman was awarded a National Merit Scholarship in 1979, a Harvard College Scholarship in 1980 and a NSF Graduate Research Fellowship from 1984 to 1988. He was a Robert Wood Johnson Foundation Scholar in health policy research at Yale in 1999-2001, and a fellow in residence at the Center for Advanced Study in the Behavioral Sciences in 2001-03.\n\nHe has chaired the American Sociological Association's Sections on the Sociology of Law (2005-2006) and Organizations, Occupations, and Work (2016-2017), and has served on the Board of Trustees of the Law and Society Association (2005-2008).\n\nIn his article \"Managing Legitimacy: Strategic and Institutional Approaches'\", Suchman defines legitimacy as \"a generalized perception or assumption that the actions of an entity are desirable, proper, appropriate within some socially constructed system of norms, values, beliefs, and definitions.\"\n\nHe later adds to this definition, stating that because legitimacy is socially conferred, legitimacy is independent of individual participants, while dependent upon the collective constituency. In other words, an organization is legitimate when it enjoys public approval, even though the actions of an organization might deviate from particular individual interests.\n\nSuchman states three types of legitimacy: \n\nPragmatic legitimacy relies upon the self-interests of an organizations constituencies, in which the constituency scrutinizes actions and behaviors taken by the organization in order to determine their effects. This is further broken down into three sub-sections: \nSuchman defines exchange legitimacy as the support for organizational policies due to the policy's benefit to the constituencies.\n\nInfluence legitimacy is the support for the organization not due to the benefits that constituencies believe they will receive, but rather due to their belief that the organization will be responsive to their larger interests.\n\nDispositional legitimacy is defined as support for an organization due to the good attributes constituencies believe the organization has, such as trustworthy, decent, or wise. This is due to the fact that people typically personify organizations and characterize them as being autonomous.\n\nMoral legitimacy is dependent upon whether the actions of an organization or institution are judged to be moral. In other words, if the constituency believe the organization is breaking the rules of the political or economic system for immoral reasons, then this can threaten moral legitimacy. Suchman breaks moral legitimacy down into four sub-sections:\nConsequential legitimacy relates to what an organization has accomplished based on criteria that is specific to that organization. Procedural legitimacy can be obtained by an organization by adhering to socially formalized and accepted procedures (e.g. regulatory oversight). In the case of structural legitimacy, people view an organization as legitimate because its structural characteristics allow it to do specific kinds of work.\n\nSuchman refers to this organization as being the \"right organization for the job.\" Lastly, personal legitimacy refers to legitimacy that is derived from the charisma of individual leaders.\n\nCognitive legitimacy is created when an organization pursues goals that society deems to be proper and desirable.\nConstituency support for the organization is not due to self-interest, but rather due to its taken-for-granted character. When an organization has reached this taken-for-granted status, an organization is beyond dissent. While moral and pragmatic legitimacy deal with some form of evaluation, cognitive legitimacy does not. Instead, with cognitive legitimacy society accepts these organizations as being necessary or inevitable.\n\n\nArticles, a selection:\n\n"}
{"id": "38612089", "url": "https://en.wikipedia.org/wiki?curid=38612089", "title": "Milking the bull", "text": "Milking the bull\n\nMilking the bull is a proverb which uses the metaphor of milking a bull to indicate that an activity would be fruitless or futile.\n\nIn the 16th century, the German painter Hans Schäufelein illustrated the proverb on the eight of bells in a deck of playing cards.\nDr Johnson used the proverb to criticise the work of David Hume and other sceptical philosophers.\n\n"}
{"id": "407860", "url": "https://en.wikipedia.org/wiki?curid=407860", "title": "Notation", "text": "Notation\n\nIn linguistics and semiotics, a notation is a system of graphics or symbols, characters and abbreviated expressions, used (for example) in artistic and scientific disciplines to represent technical facts and quantities by convention. Therefore, a notation is a collection of related symbols that are each given an arbitrary meaning, created to facilitate structured communication within a domain knowledge or field of study.\n\nStandard notations refer to general agreements in the way things are written or denoted. The term is generally used in technical and scientific areas of study like mathematics, physics, chemistry and biology, but can also be seen in areas like business, economics and music.\n\n\n\n\n\nA variety of symbols are used to express logical ideas; see the List of logic symbols\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "53992656", "url": "https://en.wikipedia.org/wiki?curid=53992656", "title": "Open–closed political spectrum", "text": "Open–closed political spectrum\n\nThe open–closed political spectrum, or horseshoe theory is an alternative to the standard left–right system, especially used to describe the cleavage in political systems in Europe and North America in the 21st century. In this system, parties and voters are arranged on an axis from open (socially liberal and globalist) to closed (culturally conservative and protectionist). Each side draws from both traditionally left- and right-wing ideas and values: \"closed\" parties usually hold conventionally right-wing views on social issues but may support the left-wing policies of market intervention and redistribution of wealth, while open parties can hold left-wing or progressive opinions on many issues but be staunchly in favour of the traditionally more rightist policies of free trade. Depending on context, open–closed can be a replacement to left–right or a second axis on a political compass.\n\nA political realignment along these lines across the Western world has been described by political scientists in the wake of the financial crisis of 2007–2008, the Great Recession and the European migrant crisis, with mainstream left-wing and right-wing political parties shifting or falling behind populist parties and independents. Examples of votes described as having been fought on open-closed lines include the 2016 Brexit referendum, presidential elections in Austria in 2016, the United States in 2016 and France in 2017, and general elections in Poland in 2015 and the Netherlands in 2017.\n\nPrior to the development of socialism in the late 19th and early 20th centuries, the primary divide in British politics was between classical liberalism (Whiggism) and traditional conservatism (Toryism) as seen in debates about free trade and the Corn Laws, and James Kirkup, writing in the \"Daily Telegraph\", has suggested that the open-closed split marks a return to this era of politics. In the United States, the rise of the socially-liberal New Left and socially-conservative religious right in the 1970s, and the subsequent \"culture wars\", marked the beginning the open-closed cleavage. Fareed Zakaria, writing in the \"Washington Post\", described the Nordic model of free market social democracy as another early example of open politics. \n\nStephan Shakespeare, director of public opinion research at YouGov, identified the divide in an analysis of the party positions in the 2005 UK general election, but termed closed and open voters \"drawbridge up\" and \"drawbridge down\" respectively. The terms \"open\" and \"closed\" for this divide were first used by then-British prime minister Tony Blair in 2006, with reference to the horseshoe theory that far-left and far-right politics are similar in substance. Blair, who declared himself staunchly for \"open\", had overseen the development of the Third Way New Labour movement in 1990s and the schism in the Labour Party between \"open\" Blairites and \"closed\" traditional socialists has remained. Political analyst James Bloodworth criticised Blair's choice of terminology, describing \"closed\" as pejorative to those threatened by globalism and \"open\" as overly laudatory of the \"Davos Man\" type of globalist.\n\nFollowing the financial crisis and subsequent recession, plus the arrival of large numbers of refugees of the Syrian Civil War, populist political parties made significant gains across the European Union. In southern European countries, these tended to be anti-austerity left-wing parties such as Syriza in Greece and Podemos in Spain, while northern European countries saw anti-immigrant right-wing parties such as the UK Independence Party and the Alternative for Germany gain support. However all these parties shared a Eurosceptic and anti-elite viewpoint, and appealed to the \"left-behind\" who saw their livelihoods or communities as threatened by globalism and immigration. In 2015, Syriza became the largest party in the Parliament of Greece, while the right-wing populist Law and Justice became the first party since the fall of Communism to win an absolute majority in the Polish Sejm in elections the same year. At the 2016 UK EU membership referendum, the Leave option narrowly defeated the cross-party Remain option. The subsequent realignment of British politics saw the Conservatives, aligned with \"Leave\", and Liberal Democrats, aligned with \"Remain\", rise in the polls while Labour, which remained deeply divided between its open and closed wings, lost ground rapidly.\n\nSimilar political developments followed in the United States in the run-up to the 2016 presidential election. Democratic nominee Hillary Clinton, liberal and pro-globalism, faced two political opponents who both represented protectionist viewpoints from opposite ends of the left–right spectrum: Bernie Sanders, a democratic socialist who challenged her in the primaries, and the Republican nominee Donald Trump, running on a nativist \"America First\" platform. Trump eventually triumphed, winning several Rust Belt states from the \"Blue Wall\" of Democratic safe states which had suffered deindustrialisation and economic deprivation.\n\nWhile \"closed\" politicians scored several electoral victories, another realignment in the political centre began in Europe, as cosmopolitan pro-European voters abandoned traditional parties of government and clustered around new liberal parties and independent politicians. In the Netherlands, where the closed anti-Islam and anti-European Party for Freedom (PVV) under Geert Wilders led opinion polls for much of the previous parliament, the liberal pro-EU Democrats 66 and the green GroenLinks positioned themselves as the main open opponents of the PVV. At the 2017 elections, all three parties made significant gains at the expense of the established parties.\n\nAlexander Van der Bellen, an independent formerly of The Greens, defeated the anti-immigrant Norbert Hofer of the Freedom Party of Austria in the 2016 Austrian presidential election, while Emmanuel Macron, a former member of the Socialists who described himself as \"neither left nor right\" and who founded his own party En Marche, defeated the Front National candidate Marine Le Pen in the 2017 French presidential election. In both cases, neither of the mainstream parties of left and right reached the second round of voting – instead, each election saw an open centrist independent against a closed far-right party.\n\nAlthough they can come from all sides of the left–right spectrum, closed parties share common ground on many issues, as likewise do open parties.\n\nOpposition to immigration is one of the starkest divides between open and closed parties. The closed position supports restrictions on migration, and prefers that immigrants integrate into the national culture. By contrast, the open position takes a more liberal stance on migration, and tolerates or even celebrates multiculturalism. Both left-wing and right-wing parties can be closed on the topic of immigration: the conservative Polish Law and Justice accuses immigrants of undermining Polish values and bringing in terrorists, while the socialist German Left Party considers immigrants a drain on the welfare state. Conversely, in the United Kingdom for example the Adam Smith Institute (a neoliberal right-wing organisation) and Vince Cable (a left-wing social democrat and former business secretary) stand together as some of the strongest defenders of immigration.\n\nIssues of international trade and international cooperation are another significant line of division between the two camps. In the open ideology, free trade and globalism is a net good, strengthening the national economy and providing jobs while cutting prices. In the closed ideology, free trade is detrimental to the national economy and encourages companies to offshore industries, resulting in lower wages and higher unemployment, as well as potentially threatening national culture. The closed attitude to globalism manifests in opposition to trade agreements such as the Transatlantic Trade and Investment Partnership, to military alliances such as NATO, and to supranational unions such as the EU, which are generally supported by the open side.\n\nPro-closed politicians opposing free trade and immigration usually describe these as impositions from an existing political elite, either within the country or outside its borders, and campaign on an anti-establishment platform. In the most extreme case, this can cross over into conspiracism, alleging that shadowy figures are actively trying to harm the nation.\n\nIdentity politics plays a role in forming both the open and closed coalitions. Pro-closed voters tend to have a strong sense of national identity, while pro-open ones share a more cosmopolitan identity. This does not mean that all closed voters are racist, nor that all open voters are unpatriotic. Nationalist pride can be enhanced by the arrival of newcomers, if these assimilate to the national culture quickly. Conversely some open politicians, such as Jesse Klaver, leader of GroenLinks, and Emmanuel Macron, have succeeded in developing a patriotic alternative to nationalism in which tolerance and cooperation are themselves considered national values to be prized and protected.\n\nPolitical scientists have identified several lines of cleavage between supporters of open and closed parties. Educational attainment was the strongest predictor of voting preference in the Brexit referendum and the 2016/2017 US, Dutch and French elections: in each case, those with low educational achievement were more likely to vote for the closed or populist option. This was described by the BBC as \"one of the most remarkable features of the US election\", as the Republican party usually performs well among university graduates.\n\nIncome has been suggested as another dividing line: in the EU referendum, the bottom fifth of UK earners overwhelmingly voted for Brexit (68% Leave, 32% Remain) while a large majority of the top quarter voted against Brexit (39% Leave, 61% Remain). However, Trump performed better with middle class voters than with either the highest or lowest earners and, although a small wealth effect was seen among French voters, this disappeared when controlling for education.\n\nPessimism and optimism also appear to be important psychological factors in determining whether a voter chooses an open or a closed candidate. Le Pen only took 20% of the vote of those who believed the situation would improve for the next generation, but over 40% among those who believed it would deteriorate. A majority of Leave voters agreed life in Britain is worse than it was thirty years ago, while a majority of Remain voters disagreed.\n\n"}
{"id": "13302295", "url": "https://en.wikipedia.org/wiki?curid=13302295", "title": "Postmodern social construction of nature", "text": "Postmodern social construction of nature\n\nThe postmodern social construction of nature is a theorem or speculation of postmodernist continental philosophy that poses an alternative critique of previous mainstream, Promethean discourse about environmental sustainability and ecopolitics.\n\nWhereas traditional criticisms of environmentalism come from the more conservative \"right\" of politics, leftist critiques of nature pioneered by postmodernist constructionism highlight the need to recognise \"the other\". The implicit assumption made by theorists like Wapner refer to it as a new \"response to ecocriticism [which] would require critics to acknowledge the ways in which they themselves silence nature and then to respect the sheer otherness of the non-human world.\"\n\nCritics argue that, by capturing the nonhuman world within its own conceptual domain, postmodern exerts precisely the urge toward mastery that it criticizes in modernity. Thus, postmodern cultural criticism deepens the modernist urge toward mastery by eliminating the ontological weight of the nonhuman world. \"What else could it mean to assert that there is no such thing as nature?\". The issue becomes an existentialist query about whether nature can exist in a humanist critique, and whether we can discern the \"others'\" views in relation to our actions on their behalf. This is referred to as the Wapner Paradox.\n\nDavid Demeritt's typology of the social construction of nature looks at the idea from several standpoints. He seeks to clarify the meaning through exploring the extent of the different uses applied to the term.\n\nOther examinations of the social construction of Nature, from a postmodern perspective, include:\n\nMarshall, A, (2002) The Unity of Nature, Imperial College Press\n\nSoule. ME, et al., eds, (1995) Reinventing Nature: Responses To Postmodern Deconstruction, Island Press.\n\nWhite, DR. (1997) Postmodern Ecologies, SUNY Press.\n"}
{"id": "11325244", "url": "https://en.wikipedia.org/wiki?curid=11325244", "title": "Raising and lowering indices", "text": "Raising and lowering indices\n\nIn mathematics and mathematical physics, raising and lowering indices are operations on tensors which change their type. Raising and lowering indices are a form of index manipulation in tensor expressions.\n\nGiven a tensor field on a manifold , in the presence of a nonsingular form on (such as a Riemannian metric or Minkowski metric), one can raise or lower indices to change a type tensor to a tensor (raise index) or to a tensor (lower index), where the notation has been used to denote the tensor order with upper indices and lower indices.\n\nOne does this by multiplying by the covariant or contravariant metric tensor and then contracting indices, meaning two indices are set equal and then summing over the repeated indices (applying Einstein notation). See examples below.\n\nMultiplying by the \"contravariant\" metric tensor and contracting produces another tensor with an upper index:\n\nThe same base symbol is typically used to denote this new tensor, and repositioning the index is typically understood in this context to refer to this new tensor, and is called \"raising the index\", which would be written\n\nSimilarly, multiplying by the \"covariant\" metric tensor and contracting \"lowers\" an index (with the same understanding about the reuse of the base symbol):\n\nThe form need not be nonsingular to lower an index, but to get the inverse (and thus raise an index) it must be nonsingular.\n\nRaising and then lowering the same index (or conversely) are inverse operations, which is reflected in the covariant and contravariant metric tensors being inverse to each other:\n\nwhere is the Kronecker delta or identity matrix. Since there are different choices of metric with different metric signatures (signs along the diagonal elements, i.e. tensor components with equal indices), the name and signature is usually indicated to prevent confusion. Different authors use different metrics and signatures for different reasons.\n\nMnemonically (though \"incorrectly\"), one could think of indices \"cancelling\" between a metric and another tensor, and the metric stepping up or down the index. In the above examples, such \"cancellations\" and \"steps\" are like\n\nAgain, while a helpful guide, this is only mnemonical and not a property of tensors since the indices do not cancel like in equations, it is only a concept of the notation. The results are continued below, for higher order tensors (i.e. more indices).\n\nWhen raising indices of quantities in spacetime, it helps to decompose summations into \"timelike components\" (where indices are zero) and \"spacelike components\" (where indices are 1, 2, 3, represented conventionally by Latin letters).\n\nThe covariant 4-position is given by\n\nwith components:\n\n(where , are the usual Cartesian coordinates) and the Minkowski metric tensor with signature (− + + +) is defined as\n\nin components:\n\nTo raise the index, multiply by the tensor and contract:\n\nthen for :\n\nand for :\n\nSo the index-raised contravariant 4-position is:\n\nFor an order-2 tensor, twice multiplying by the contravariant metric tensor and contracting in different indices raises each index:\n\nand twice multiplying by the covariant metric tensor and contracting in different indices lowers each index:\n\nThe contravariant electromagnetic tensor in the signature is given by\n\nin components:\n\nTo obtain the covariant tensor , multiply by the metric tensor and contract:\n\nand since \"F\" = 0 and \"F\" = − \"F\", this reduces to\n\nNow for , :\n\nand by antisymmetry, for , :\n\nthen finally for , ;\n\nThe (covariant) lower indexed tensor is then:\n\nWhen a vector space is equipped with an inner product (or metric as it is often called in this context), there exist operations that convert a contravariant (upper) index into a covariant (lower) index and vice versa. A metric itself is a (symmetric) (0,2)-tensor, it is thus possible to contract an upper index of a tensor with one of the lower indices of the metric. This produces a new tensor with the same index structure as the previous, but with lower index in the position of the contracted upper index. This operation is quite graphically known as lowering an index.\nConversely, a metric has an inverse which is a (2,0)-tensor. This inverse metric can be contracted with a lower index to produce an upper index. This operation is called raising an index.\n\nFor a tensor of order , indices are raised by:\n\nand lowered by:\n\nand for a mixed tensor:\n\n"}
{"id": "60562", "url": "https://en.wikipedia.org/wiki?curid=60562", "title": "Redlining", "text": "Redlining\n\nIn the United States and Canada, redlining is the systematic denial of various services to residents of specific, often racially associated, neighborhoods or communities, either directly or through the selective raising of prices. While the best known examples of redlining have involved denial of financial services such as banking or insurance, other services such as health care or even supermarkets have been denied to residents. In the case of retail businesses like supermarkets, purposely locating impractically far away from said residents results in a redlining effect. Reverse redlining occurs when a lender or insurer targets particular neighborhoods that are predominantly nonwhite, not to deny residents loans or insurance, but rather to charge them more than in a non-redlined neighborhood where there is more competition.\n\nIn the 1960s, sociologist John McKnight coined the term \"redlining\" to describe the discriminatory practice of fencing off areas where banks would avoid investments based on community demographics. During the heyday of redlining, the areas most frequently discriminated against were black inner city neighborhoods. For example, in Atlanta in the 1980s, a Pulitzer Prize-winning series of articles by investigative reporter Bill Dedman showed that banks would often lend to lower-income whites but not to middle-income or upper-income blacks. The use of blacklists is a related mechanism also used by redliners to keep track of groups, areas, and people that the discriminating party feels should be denied business or aid or other transactions. In the academic literature, redlining falls under the broader category of credit rationing.\n\nAlthough informal discrimination and segregation had existed in the United States, the specific practice called \"redlining\" began with the National Housing Act of 1934, which established the Federal Housing Administration (FHA). Racial segregation and discrimination against minorities and minority communities pre-existed this policy. The implementation of this federal policy aggravated the decay of minority inner-city neighborhoods caused by the withholding of mortgage capital, and made it even more difficult for neighborhoods to attract and retain families able to purchase homes. The assumptions in redlining resulted in a large increase in residential racial segregation and urban decay in the United States.\n\nIn 1935, the Federal Home Loan Bank Board (FHLBB) asked Home Owners' Loan Corporation (HOLC) to look at 239 cities and create \"residential security maps\" to indicate the level of security for real-estate investments in each surveyed city. On the maps, the newest areas—those considered desirable for lending purposes—were outlined in green and known as \"Type A\". These were typically affluent suburbs on the outskirts of cities. \"Type B\" neighborhoods, outlined in blue, were considered \"Still Desirable\", whereas older \"Type C\" were labeled \"Declining\" and outlined in yellow.\n\"Type D\" neighborhoods were outlined in red and were considered the most risky for mortgage support. These neighborhoods tended to be the older districts in the center of cities; often they were also black neighborhoods. Urban planning historians theorize that the maps were used by private and public entities for years afterward to deny loans to people in black communities. But, recent research has indicated that the HOLC did not redline in its own lending activities and that the racist language reflected the bias of the private sector and experts hired to conduct the appraisals.\n\nSome redlined maps were also created by private organizations, such as J.M. Brewer's 1934 map of Philadelphia. Private organizations created maps designed to meet the requirements of the Federal Housing Administration's underwriting manual. The lenders had to consider FHA standards if they wanted to receive FHA insurance for their loans. FHA appraisal manuals instructed banks to steer clear of areas with \"inharmonious racial groups\", and recommended that municipalities enact racially restrictive zoning ordinances.\n\nFollowing a National Housing Conference in 1973, a group of Chicago community organizations led by The Northwest Community Organization (NCO) formed National People's Action (NPA), to broaden the fight against disinvestment and mortgage redlining in neighborhoods all over the country. This organization, led by Chicago housewife Gale Cincotta and Shel Trapp, a professional community organizer, targeted The Federal Home Loan Bank Board, the governing authority over federally chartered Savings & Loan institutions (S&L) that held at that time the bulk of the country's home mortgages. NPA embarked on an effort to build a national coalition of urban community organizations to pass a national disclosure regulation or law to require banks to reveal their lending patterns.\n\nFor many years, urban community organizations had battled neighborhood decay by attacking blockbusting, forcing landlords to maintain properties, and requiring cities to board up and tear down abandoned properties. These actions addressed the short-term issues of neighborhood decline. Neighborhood leaders began to learn that these issues and conditions were symptoms of a disinvestment that was the true, though hidden, underlying cause of these problems. They changed their strategy as more data was gathered.\n\nWith the help of NPA, a coalition of loosely affiliated community organizations began to form. At the Third Annual Housing Conference held in Chicago in 1974, eight hundred delegates representing 25 states and 35 cities attended. The strategy focused on the Federal Home Loan Bank Board (FHLBB), which oversaw S&L's in cities all over the country.\n\nIn 1974, Chicago's Metropolitan Area Housing Association (MAHA), made up of representatives of local organizations, succeeded in having the Illinois State Legislature pass laws mandating disclosure and outlawing redlining. In Massachusetts, organizers allied with NPA confronted a unique situation. Over 90% of home mortgages were held by state-chartered savings banks. A Jamaica Plain neighborhood organization pushed the disinvestment issue into the statewide gubernatorial race. The Jamaica Plain Banking & Mortgage Committee and its citywide affiliate, The Boston Anti-redlining Coalition (BARC), won a commitment from Democratic candidate Michael S. Dukakis to order statewide disclosure through the Massachusetts State Banking Commission. After Dukakis was elected, his new Banking Commissioner ordered banks to disclose mortgage-lending patterns by ZIP code. The suspected redlining was revealed.\n\nNPA and its affiliates achieved disclosure of lending practices with the passage of The Home Mortgage Disclosure Act of 1975. The required transparency and review of loan practices began to change lending practices. NPA began to work on reinvestment in areas that had been neglected. Their support helped gain passage in 1977 of the Community Reinvestment Act.\n\nAccording to blackpast.org contributor Brent Gaspaire:\n\nRedlining paralyzed the housing market, lowered property values in certain areas and encouraged landlord abandonment. As abandonment increased, the population density became lower. Abandoned buildings served as havens for drug dealing and other illegal activity, increasing social problems and reluctance of people to invest in these areas.\nBecause areas were redlined residents in them were unable to obtain loans to improve their homes or get loans to move to a different area. Obviously, the neighborhoods had zero investment while neighborhoods around them improved. When the GI Bill was created during World War II, veterans who once lived in redlined areas were unable to get zero interest loans to build new homes like the rest of the returning soldiers. This forced them to stay in the areas that were poor and uninvested in while the rest of America was growing and moving to the suburbs. Around the same time the GI Bill was created, the Federal Highway Act was also created. Because the areas that were redlined were so poor, many cities chose to destroy these areas to create the highways. The residents were displaced and forced to move into different uninvested neighborhoods while their homes and businesses were destroyed by the highways.\n\nA 2017 study by Federal Reserve Bank of Chicago economists found that the practice of redlining—the practice whereby banks discriminated against the inhabitants of certain neighborhoods—had a persistent adverse impact on the neighborhoods, with redlining affecting homeownership rates, home values and credit scores in 2010. Since many African-Americans could not access conventional home loans, they had to turn to predatory lenders (who charged high interest rates). Due to lower home ownership rates, slumlords were able to rent out apartments that would otherwise be owned.\n\nThe U.S. Department of Housing and Urban Development announced a $200 million settlement with Associated Bank over redlining in Chicago and Milwaukee in May 2015. The three-year HUD observation led to the complaint that the bank purposely rejected mortgage applications from black and Latino applicants. The final settlement required AB to open branches in non-white neighborhoods, just like HCSB.\n\nNew York Attorney General Eric Schneiderman announced a settlement with Evans Bank for $825,000 on September 10, 2015. An investigation had uncovered the erasure of black neighborhoods from mortgage lending maps. According to Schneiderman, of the over 1,100 mortgage applications the bank received between 2009 and 2012, only four were from African Americans. Following this investigation, the Buffalo News reported that more banks could be investigated for the same reasons in the near future. The most notable examples of such DOJ and HUD settlements have focused heavily on community banks in large metropolitan areas, but banks in other regions have been the subject of such orders as well, including First United Security Bank in Thomasville, Alabama, and Community State Bank in Saginaw, Michigan.\n\nThe United States Department of Justice announced a $33 million settlement with Hudson City Savings Bank, which services New Jersey, New York, and Pennsylvania, on September 24, 2015. The six-year DOJ investigation had proven that the company was intentionally avoiding granting mortgages to Latinos and African Americans and purposely avoided expanding into minority-majority communities. The Justice Department called it the \"largest residential mortgage redlining settlement in its history.\" As a part of the settlement agreement, HCSB was forced to open branches in non-white communities. As U.S. Attorney Paul Fishman explained to Emily Badger for The Washington Post, \"[i]f you lived in a majority-black or Hispanic neighborhood and you wanted to apply for a mortgage, Hudson City Savings Bank was not the place to go.\" The enforcement agencies cited additional evidence of discrimination Hudson City's broker selection practices, noting that the bank received 80 percent of its mortgage applications from mortgage brokers but that the brokers with whom the bank worked were not located in majority African-American and Hispanic areas.\n\nIn the United States, the Fair Housing Act of 1968 was passed to fight the practice. According to the Department of Housing and Urban Development \"The \"Fair Housing Act\" makes it unlawful to discriminate in the terms, conditions, or privileges of sale of a dwelling because of race or national origin. The Act also makes it unlawful for any person or other entity whose business includes residential real estate-related transactions to discriminate against any person in making available such a transaction, or in the terms or conditions of such a transaction, because of race or national origin.\" The Office of Fair Housing and Equal Opportunity was tasked with administering and enforcing this law. Anyone who suspects that their neighborhood has been redlined is able to file a housing discrimination complaint.\n\nThe Community Reinvestment Act passed by Congress in 1977 to reduce discriminatory credit practices against low-income neighborhoods further required banks to apply the same lending criteria in all communities. Although open redlining was made illegal in the 1970s through community reinvestment legislation, the practice may have continued in less overt ways. AIDS activists allege redlining of health insurance against the LGBT community in response to the AIDS crisis.\n\nShoreBank, a community-development bank in Chicago's South Shore neighborhood, was a part of the private-sector fight against redlining. Founded in 1973, ShoreBank sought to combat racist lending practices in Chicago's African-American communities by providing financial services, especially mortgage loans, to local residents. In a 1992 speech, then-Presidential candidate Bill Clinton called ShoreBank \"the most important bank in America.\" On August 20, 2010, the bank was declared insolvent, closed by regulators and most of its assets were acquired by Urban Partnership Bank.\n\nIn the mid-1970s, community organizations, under the banner of the NPA, worked to fight against redlining in South Austin, Illinois. One of these organisations was SACCC (South Austin Coalition Community Council), formed to restore South Austin's neighbourhood and to fight against financial institutions accused of propagating redlining. This got the attention of insurance regulators in the Illinois Department of Insurance, as well as federal officers enforcing anti-racial discrimination laws.\n\nRetail redlining is a spatially discriminatory practice among retailers. Taxicab services and delivery food may not serve certain areas, based on their ethnic-minority composition and assumptions about business (and perceived crime), rather than data and economic criteria, such as the potential profitability of operating in those areas. Consequently, consumers in these areas are vulnerable to prices set by fewer retailers. They may be exploited by retailers who charge higher prices and/or offer them inferior goods.\n\nA 2012 study by the \"Wall Street Journal\" found that Staples, Home Depot, Rosetta Stone and some other online retailers displayed different prices to customers in different locations (distinct from shipping prices). Staples based discounts on proximity to competitors like OfficeMax and Office Depot. This generally resulted in higher prices for customers in more rural areas, who were on average less wealthy than customers seeing lower prices.\n\nSome service providers target low-income neighborhoods for nuisance sales. When those services are believed to have adverse effects on a community, they may considered to be a form of \"reverse redlining.\" The term \"liquorlining\" is sometimes used to describe high densities of liquor stores in low income and/or minority communities relative to surrounding areas. High densities of liquor stores are associated with crime and public health issues, which may in turn drive away supermarkets, grocery stores, and other retail outlets, contributing to low levels of economic development. Controlled for income, nonwhites face higher concentrations of liquor stores than do whites.\n\nIn December 2007, a class action lawsuit was brought against student loan lending giant Sallie Mae in the United States District Court for the District of Connecticut. The class alleged that Sallie Mae discriminated against African American and Hispanic private student loan applicants.\n\nThe case alleged that the factors Sallie Mae used to underwrite private student loans caused a disparate impact on students attending schools with higher minority populations. The suit also alleged that Sallie Mae failed to properly disclose loan terms to private student loan borrowers.\n\nThe lawsuit was settled in 2011. The terms of the settlement included Sallie Mae agreeing to make a $500,000 donation to the United Negro College Fund and the attorneys for the plaintiffs receiving $1.8 million in attorneys' fees.\n\nCredit card redlining is a spatially discriminatory practice among credit card issuers, of providing different amounts of credit to different areas, based on their ethnic-minority composition, rather than on economic criteria, such as the potential profitability of operating in those areas. Scholars assess certain policies, such as credit card issuers reducing credit lines of individuals with a record of purchases at retailers frequented by so-called \"high-risk\" customers, to be akin to redlining.\n\nGregory D. Squires wrote in 2003 that data showed that race continues to affect the policies and practices of the insurance industry. Racial profiling or redlining has a long history in the property-insurance industry in the United States. From a review of industry underwriting and marketing materials, court documents, and research by government agencies, industry and community groups, and academics, it is clear that race has long affected and continues to affect the policies and practices of the insurance industry. Home-insurance agents may try to assess the ethnicity of a potential customer just by telephone, affecting what services they offer to inquiries about purchasing a home insurance policy. This type of discrimination is called linguistic profiling. There have also been concerns raised about redlining in the automotive insurance industry. Reviews of insurance scores based on credit are shown to have unequal results by ethnic group. The Ohio Department of Insurance in the early 21st century allows insurance providers to use maps and collection of demographic data by ZIP code in determining insurance rates. The FHEO Director of Investigations at the Department of Housing and Urban Development, Sara Pratt, wrote:\n\nLike other forms of discrimination, the history of insurance redlining began in conscious, overt racial discrimination practiced openly and with significant community support in communities throughout the country. There was documented overt discrimination in practices relating to residential housing—from the appraisal manuals which established an articulated \"policy\" of preferences based on race, religion and national origin. to lending practices which only made loans available in certain parts of town or to certain borrowers, to the decision-making process in loans and insurance which allowed the insertion of discriminatory assessments into final decisions about either.\n\nReverse redlining occurs when a lender or insurer particularly targets minority consumers, not to deny them loans or insurance, but to charge them more than would be charged to a similarly situated white consumer, specifically marketing the most expensive and onerous loan products. These communities had largely been ignored by most lenders just a couple of decades earlier. In the 2000s some financial institutions considered black communities as suitable for subprime mortgages. Wells Fargo partnered with churches in black communities, where the pastor would deliver \"wealth building\" seminars in their sermons, and the bank would make a donation to the church in return for every new mortgage application. Working-class blacks wanted a part of the nation's home-owning trend. Instead of contributing to homeownership and community progress, predatory lending practices through reverse redlining stripped the equity homeowners struggled to build and drained the wealth of those communities for the enrichment of financial firms. The growth of subprime lending (higher cost loans to borrowers with flaws on their credit records) prior to the 2008 financial crisis, coupled with growing law enforcement activity in those areas, clearly showed a surge in a range of manipulative practices. Not all subprime loans were predatory, but virtually all predatory loans were subprime. Some subprime loans certainly benefit high-risk borrowers who would not qualify for conventional, prime loans. Predatory loans, however, charge unreasonably higher rates and fees by compared to the risk, trapping homeowners in unaffordable debt and often costing them their homes and life savings.\n\nA survey of two districts of similar incomes, one being largely white and the other largely black, found that bank branches in the black community offered largely subprime loans and almost no prime loans. Studies found out that high-income blacks were almost twice as likely to end up with subprime home-purchase mortgages as did low-income whites. Some loan officers referred to blacks as \"mud people\" and to subprime lending as \"ghetto loans.\" A lower savings rate and a distrust of banks, stemming from a legacy of redlining, may help explain why there are fewer branches in minority neighborhoods. In the early 21st century, brokers and telemarketers actively pushed subprime mortgages. A majority of the loans were refinance transactions, allowing homeowners to take cash out of their appreciating property or pay off credit card and other debt.\n\nRedlining has helped preserve segregated living patterns for blacks and whites in the United States, as discrimination is often contingent on the racial composition of neighborhoods and the race of the applicant. Lending institutions such as Wells Fargo have been shown to treat black mortgage applicants differently when they are buying homes in white neighborhoods than when buying homes in black neighborhoods.\n\nDan Immergluck writes that in 2002 small businesses in black neighborhoods received fewer loans, even after accounting for business density, business size, industrial mix, neighborhood income, and the credit quality of local businesses.\n\nSeveral state attorneys general have begun investigating these practices, which may violate fair lending laws. The NAACP filed a class-action lawsuit charging systematic racial discrimination by more than a dozen banks.\n\nPolicies related to redlining and urban decay can also act as a form of environmental racism, which in turn affect public health. Urban minority communities may face environmental racism in the form of parks that are smaller, less accessible and of poorer quality than those in more affluent or white areas in some cities. This may have an indirect effect on health, since young people have fewer places to play, and adults have fewer opportunities for exercise.\n\nRobert Wallace writes that the pattern of the AIDS outbreak during the 80s was affected by the outcomes of a program of \"planned shrinkage\" directed at African-American and Hispanic communities. It was implemented through systematic denial of municipal services, particularly fire protection resources, essential to maintain urban levels of population density and ensure community stability. Institutionalized racism affects general health care as well as the quality of AIDS health intervention and services in minority communities. The over-representation of minorities in various disease categories, including AIDS, is partially related to environmental racism. The national response to the AIDS epidemic in minority communities was slow during the 80s and 90s, showing an insensitivity to ethnic diversity in prevention efforts and AIDS health services.\n\nWorkers living in American inner cities have more difficulty finding jobs than do suburban workers.\n\nFair Housing Act\n\nThe Fair Housing Act (also known as the Civil Rights Act) of 1968 was passed in order to help protect minority individuals from the discriminatory practices of financial institutions and agents.\n\nGuidelines\n\nThe Human Relations Commissions of Pennsylvania adopted guidelines regarding redlining. The guidelines demonstrate practices that are to be prohibited in the selling of property to people.\n\nThe film \"Revolution '67\" examines the practice of redlining that occurred in Newark, New Jersey in the 1960s.\n\n\n\nGreenwald, Carol S., \"Banks are Dangerous to your Wealth\", Prentice-Hall 1980.\n\nHallahan, Kirk. \"The Mortgage Redlining Controversy 1972–1975\" https://web.archive.org/web/20130809022759/http://lamar.colostate.edu/~pr/redlining.pdf\n\nWestgate, Michael and Ann Vick., \"Gale Force, The Battles For Disclosure and Community Reinvestment\", Harvard Book Store, 2nd edition, 2011. \n\nRothstein, Richard, \"The Color of Law: A Forgotten History of How Our Government Segregated America\", Liveright, 1st Ed. May 2017.\n\n"}
{"id": "4363670", "url": "https://en.wikipedia.org/wiki?curid=4363670", "title": "Relation algebra", "text": "Relation algebra\n\nIn mathematics and abstract algebra, a relation algebra is a residuated Boolean algebra expanded with an involution called converse, a unary operation. The motivating example of a relation algebra is the algebra 2 of all binary relations on a set \"X\", that is, subsets of the cartesian square \"X\", with \"R\"•\"S\" interpreted as the usual composition of binary relations \"R\" and \"S\", and with the converse of \"R\" as the converse relation.\n\nRelation algebra emerged in the 19th-century work of Augustus De Morgan and Charles Peirce, which culminated in the algebraic logic of Ernst Schröder. The equational form of relation algebra treated here was developed by Alfred Tarski and his students, starting in the 1940s. Tarski and Givant (1987) applied relation algebra to a variable-free treatment of axiomatic set theory, with the implication that mathematics founded on set theory could itself be conducted without variables.\n\nA relation algebra (\"L\", ∧, ∨, , 0, 1, •, I, ˘) is an algebraic structure equipped with the Boolean operations of conjunction \"x\"∧\"y\", disjunction \"x\"∨\"y\", and negation \"x\", the Boolean constants 0 and 1, the relational operations of composition \"x\"•\"y\" and converse \"x\"˘, and the relational constant I, such that these operations and constants satisfy certain equations constituting an axiomatization of a calculus of relations. Roughly, a relation algebra is to a system of binary relations on a set containing the empty (0), complete (1), and identity (I) relations and closed under these five operations as a group is to a system of permutations of a set containing the identity permutation and closed under composition and inverse. However, the first order theory of relation algebras is not complete for such systems of binary relations.\n\nFollowing Jónsson and Tsinakis (1993) it is convenient to define additional operations \"x\"◁\"y\" = \"x\"•\"y\"˘, and, dually, \"x\"▷\"y\" = \"x\"˘•\"y\" . Jónsson and Tsinakis showed that I◁\"x\" = \"x\"▷I, and that both were equal to \"x\"˘. Hence a relation algebra can equally well be defined as an algebraic structure (\"L\", ∧, ∨, , 0, 1, •, I, ◁, ▷). The advantage of this signature over the usual one is that a relation algebra can then be defined in full simply as a residuated Boolean algebra for which I◁\"x\" is an involution, that is, I◁(I◁\"x\") = \"x\" . The latter condition can be thought of as the relational counterpart of the equation 1/(1/\"x\") = \"x\" for ordinary arithmetic reciprocal, and some authors use reciprocal as a synonym for converse.\n\nSince residuated Boolean algebras are axiomatized with finitely many identities, so are relation algebras. Hence the latter form a variety, the variety RA of relation algebras. Expanding the above definition as equations yields the following finite axiomatization.\n\nThe axioms B1-B10 below are adapted from Givant (2006: 283), and were first set out by Tarski in 1948.\n\n\"L\" is a Boolean algebra under binary disjunction, ∨, and unary complementation ():\nThis axiomatization of Boolean algebra is due to Huntington (1933). Note that the meet of the implied Boolean algebra is \"not\" the • operator (even though it distributes over formula_1 like a meet does), nor is the 1 of the Boolean algebra the I constant.\n\n\"L\" is a monoid under binary composition (•) and nullary identity I:\n\nUnary converse ()˘ is an involution with respect to composition:\n\nAxiom B6 defines conversion as an involution, whereas B7 expresses the antidistributive property of conversion relative to composition.\n\nConverse and composition distribute over disjunction:\n\nB10 is Tarski's equational form of the fact, discovered by Augustus De Morgan, that \"A\"•\"B\" ≤ \"C\" \"A\"˘•\"C\" ≤ \"B\" \"C\"•\"B\"˘ ≤ \"A\".\n\nThese axioms are ZFC theorems; for the purely Boolean B1-B3, this fact is trivial. After each of the following axioms is shown the number of the corresponding theorem in Chapter 3 of Suppes (1960), an exposition of ZFC: B4 27, B5 45, B6 14, B7 26, B8 16, B9 23.\n\nThe following table shows how many of the usual properties of binary relations can be expressed as succinct RA equalities or inequalities. Below, an inequality of the form \"A\"≤\"B\" is shorthand for the Boolean equation \"A\"∨\"B\" = \"B\".\n\nThe most complete set of results of this nature is Chapter C of Carnap (1958), where the notation is rather distant from that of this entry. Chapter 3.2 of Suppes (1960) contains fewer results, presented as ZFC theorems and using a notation that more resembles that of this entry. Neither Carnap nor Suppes formulated their results using the RA of this entry, or in an equational manner.\nThe metamathematics of RA are discussed at length in Tarski and Givant (1987), and more briefly in Givant (2006).\n\nRA consists entirely of equations manipulated using nothing more than uniform replacement and the substitution of equals for equals. Both rules are wholly familiar from school mathematics and from abstract algebra generally. Hence RA proofs are carried out in a manner familiar to all mathematicians, unlike the case in mathematical logic generally.\n\nRA can express any (and up to logical equivalence, exactly the) first-order logic (FOL) formulas containing no more than three variables. (A given variable can be quantified multiple times and hence quantifiers can be nested arbitrarily deeply by \"reusing\" variables.) Surprisingly, this fragment of FOL suffices to express Peano arithmetic and almost all axiomatic set theories ever proposed. Hence RA is, in effect, a way of algebraizing nearly all mathematics, while dispensing with FOL and its connectives, quantifiers, turnstiles, and modus ponens. Because RA can express Peano arithmetic and set theory, Gödel's incompleteness theorems apply to it; RA is incomplete, incompletable, and undecidable. (N.B. The Boolean algebra fragment of RA is complete and decidable.)\n\nThe representable relation algebras, forming the class RRA, are those relation algebras isomorphic to some relation algebra consisting of binary relations on some set, and closed under the intended interpretation of the RA operations. It is easily shown, e.g. using the method of pseudoelementary classes, that RRA is a quasivariety, that is, axiomatizable by a universal Horn theory. In 1950, Roger Lyndon proved the existence of equations holding in RRA that did not hold in RA. Hence the variety generated by RRA is a proper subvariety of the variety RA. In 1955, Alfred Tarski showed that RRA is itself a variety. In 1964, Donald Monk showed that RRA has no finite axiomatization, unlike RA, which is finitely axiomatized by definition.\n\nAn RA is a Q-relation algebra (QRA) if, in addition to B1-B10, there exist some \"A\" and \"B\" such that (Tarski and Givant 1987: §8.4):\n\nEssentially these axioms imply that the universe has a (non-surjective) pairing relation whose projections are \"A\" and \"B\". It is a theorem that every QRA is a RRA (Proof by Maddux, see Tarski & Givant 1987: 8.4(iii) ).\n\nEvery QRA is representable (Tarski and Givant 1987). That not every relation algebra is representable is a fundamental way RA differs from QRA and Boolean algebras, which, by Stone's representation theorem for Boolean algebras, are always representable as sets of subsets of some set, closed under union, intersection, and complement.\n\n1. Any Boolean algebra can be turned into a RA by interpreting conjunction as composition (the monoid multiplication •), i.e. \"x\"•\"y\" is defined as \"x\"∧\"y\". This interpretation requires that converse interpret identity (\"ў\" = \"y\"), and that both residuals \"y\"\\\"x\" and \"x\"/\"y\" interpret the conditional \"y\"→\"x\" (i.e., ¬\"y\"∨\"x\").\n\n2. The motivating example of a relation algebra depends on the definition of a binary relation \"R\" on a set \"X\" as any subset \"R\" ⊆ \"X\"², where \"X\"² is the Cartesian square of \"X\". The power set 2 consisting of all binary relations on \"X\" is a Boolean algebra. While 2 can be made a relation algebra by taking \"R\"•\"S\" = \"R\"∧\"S\", as per example (1) above, the standard interpretation of • is instead \"x\"(\"R\"•\"S\")\"z\" = ∃\"y\":\"xRy.ySz\". That is, the ordered pair (\"x\",\"z\") belongs to the relation \"R\"•\"S\" just when there exists \"y\" ∈ \"X\" such that (\"x\",\"y\") ∈ \"R\" and (\"y\",\"z\") ∈ \"S\". This interpretation uniquely determines \"R\"\\\"S\" as consisting of all pairs (\"y\",\"z\") such that for all \"x\" ∈ \"X\", if \"xRy\" then \"xSz\". Dually, \"S\"/\"R\" consists of all pairs (\"x\",\"y\") such that for all \"z\" ∈ \"X\", if \"yRz\" then \"xSz\". The translation \"ў\" = ¬(y\\¬I) then establishes the converse \"R\"˘ of \"R\" as consisting of all pairs (\"y\",\"x\") such that (\"x\",\"y\") ∈ \"R\".\n\n3. An important generalization of the previous example is the power set 2 where \"E\" ⊆ \"X\"² is any equivalence relation on the set \"X\". This is a generalization because \"X\"² is itself an equivalence relation, namely the complete relation consisting of all pairs. While 2 is not a subalgebra of 2 when \"E\" ≠ \"X\"² (since in that case it does not contain the relation \"X\"², the top element 1 being \"E\" instead of \"X\"²), it is nevertheless turned into a relation algebra using the same definitions of the operations. Its importance resides in the definition of a \"representable relation algebra\" as any relation algebra isomorphic to a subalgebra of the relation algebra 2 for some equivalence relation \"E\" on some set. The previous section says more about the relevant metamathematics.\n\n4. Let formula_2 be group. Then the power set formula_3 is a relation algebra with the obvious boolean algebra operations, composition given by the product of group subsets, the converse by the inverse subset (formula_4), and the identity by the singleton subset formula_5. There is a relation algebra homomorphism embedding formula_3 in formula_7 which sends each subset formula_8 to the relation\nformula_9. The image of this homomorphism is the set of all right-invariant relations on formula_2. \n\n5. If group sum or product interprets composition, group inverse interprets converse, group identity interprets I, and if \"R\" is a one-to-one correspondence, so that \"R\"˘•\"R\" = \"R•R\"˘ = I, then \"L\" is a group as well as a monoid. B4-B7 become well-known theorems of group theory, so that RA becomes a proper extension of group theory as well as of Boolean algebra.\n\nDe Morgan founded RA in 1860, but C. S. Peirce took it much further and became fascinated with its philosophical power. The work of DeMorgan and Peirce came to be known mainly in the extended and definitive form Ernst Schröder gave it in Vol. 3 of his \"Vorlesungen\" (1890–1905). \"Principia Mathematica\" drew strongly on Schröder's RA, but acknowledged him only as the inventor of the notation. In 1912, Alwin Korselt proved that a particular formula in which the quantifiers were nested four deep had no RA equivalent. This fact led to a loss of interest in RA until Tarski (1941) began writing about it. His students have continued to develop RA down to the present day. Tarski returned to RA in the 1970s with the help of Steven Givant; this collaboration resulted in the monograph by Tarski and Givant (1987), the definitive reference for this subject. For more on the history of RA, see Maddux (1991, 2006).\n\n\n\n\n\n"}
{"id": "389401", "url": "https://en.wikipedia.org/wiki?curid=389401", "title": "Scientific management", "text": "Scientific management\n\nScientific management is a theory of management that analyzes and synthesizes workflows. Its main objective is improving economic efficiency, especially labour productivity. It was one of the earliest attempts to apply science to the engineering of processes and to management. Scientific management is sometimes known as Taylorism after its founder, Frederick Winslow Taylor.\n\nTaylor began the theory's development in the United States during the 1880s and '90s within manufacturing industries, especially steel. Its peak of influence came in the 1910s; Taylor died in 1915 and by the 1920s, scientific management was still influential but had entered into competition and syncretism with opposing or complementary ideas.\n\nAlthough scientific management as a distinct theory or school of thought was obsolete by the 1930s, most of its themes are still important parts of industrial engineering and management today. These include: analysis; synthesis; logic; rationality; empiricism; work ethic; efficiency and elimination of waste; standardization of best practices; disdain for tradition preserved merely for its own sake or to protect the social status of particular workers with particular skill sets; the transformation of craft production into mass production; and knowledge transfer between workers and from workers into tools, processes, and documentation.\n\nTaylor's own names for his approach initially included \"shop management\" and \"process management\". However, \"scientific management\" came to national attention in 1910 when crusading attorney Louis Brandeis (then not yet Supreme Court justice) popularized the term. Brandeis had sought a consensus term for the approach with the help of practitioners like Henry L. Gantt and Frank B. Gilbreth. Brandeis then used the consensus of \"scientific management\" when he argued before the Interstate Commerce Commission (ICC) that a proposed increase in railroad rates was unnecessary despite an increase in labor costs; he alleged scientific management would overcome railroad inefficiencies (The ICC ruled against the rate increase, but also dismissed as insufficiently substantiated that concept the railroads were necessarily inefficient.) Taylor recognized the nationally-known term \"scientific management\" as another good name for the concept, and adopted it in the title of his influential 1911 monograph.\n\nThe Midvale Steel Company, \"one of America's great armor plate making plants,\" was the birthplace of scientific management. In 1877, at age 22, Frederick W. Taylor started as a clerk in Midvale, but advanced to foreman in 1880. As foreman, Taylor was \"constantly impressed by the failure of his [team members] to produce more than about one-third of [what he deemed] a good day's work.\" Taylor determined to discover, by scientific methods, how long it should take men to perform each given piece of work; and it was in the fall of 1882 that he started to put the first features of scientific management into operation. \n\nHorace Bookwalter Drury, in his 1918 work, \"Scientific management: A History and Criticism\", identified seven other leaders in the movement, most of whom learned of and extended scientific management from Taylor's efforts:\n\nEmerson's testimony in late 1910 to the Interstate Commerce Commission brought the movement to national attention and instigated serious opposition. Emerson contended the railroads might save $1,000,000 a day by paying greater attention to efficiency of operation. By January 1911, a leading railroad journal began a series of articles denying they were inefficiently managed.\nWhen steps were taken to introduce scientific management at the government-owned Rock Island Arsenal in early 1911, it was opposed by Samuel Gompers, founder and President of the American Federation of Labor (an alliance of craft unions). When a subsequent attempt was made to introduce the bonus system into the government's Watertown Arsenal foundry during the summer of 1911, the entire force walked out for a few days. Congressional investigations followed, resulting in a ban on the use of time studies and pay premiums in Government service.\n\nTaylor's death in 1915 at age 59 left the movement without its original leader. In management literature today, the term \"scientific management\" mostly refers to the work of Taylor and his disciples (\"classical\", implying \"no longer current, but still respected for its seminal value\") in contrast to newer, improved iterations of efficiency-seeking methods. Today, task-oriented optimization of work tasks is nearly ubiquitous in industry.\n\nFlourishing in the late 19th and early 20th century, scientific management built on earlier pursuits of economic efficiency. While it was prefigured in the folk wisdom of thrift, it favored empirical methods to determine efficient procedures rather than perpetuating established traditions. Thus it was followed by a profusion of successors in applied science, including time and motion study, the Efficiency Movement (which was a broader cultural echo of scientific management's impact on business managers specifically), Fordism, operations management, operations research, industrial engineering, management science, manufacturing engineering, logistics, business process management, business process reengineering, lean manufacturing, and Six Sigma. There is a fluid continuum linking scientific management with the later fields, and the different approaches often display a high degree of compatibility.\n\nTaylor rejected the notion, which was universal in his day and still held today, that the trades, including manufacturing, were resistant to analysis and could only be performed by craft production methods. In the course of his empirical studies, Taylor examined various kinds of manual labor. For example, most bulk materials handling was manual at the time; material handling equipment as we know it today was mostly not developed yet. He looked at shoveling in the unloading of railroad cars full of ore; lifting and carrying in the moving of iron pigs at steel mills; the manual inspection of bearing balls; and others. He discovered many concepts that were not widely accepted at the time. For example, by observing workers, he decided that labor should include rest breaks so that the worker has time to recover from fatigue, either physical (as in shoveling or lifting) or mental (as in the ball inspection case). Workers were allowed to take more rests during work, and productivity increased as a result.\n\nSubsequent forms of scientific management were articulated by Taylor's disciples, such as Henry Gantt; other engineers and managers, such as Benjamin S. Graham; and other theorists, such as Max Weber. Taylor's work also contrasts with other efforts, including those of Henri Fayol and those of Frank Gilbreth, Sr. and Lillian Moller Gilbreth (whose views originally shared much with Taylor's but later diverged in response to Taylorism's inadequate handling of human relations).\n\nScientific management requires a high level of managerial control over employee work practices and entails a higher ratio of managerial workers to laborers than previous management methods. Such detail-oriented management may cause friction between workers and managers.\n\nTaylor observed that some workers were more talented than others, and that even smart ones were often unmotivated. He observed that most workers who are forced to perform repetitive tasks tend to work at the slowest rate that goes unpunished. This slow rate of work has been observed in many industries and many countries and has been called by various terms. Taylor used the term \"soldiering\", a term that reflects the way conscripts may approach following orders, and observed that, when paid the same amount, workers will tend to do the amount of work that the slowest among them does. Taylor describes soldiering as \"the greatest evil with which the working-people ... are now afflicted.\"\n\nThis reflects the idea that workers have a vested interest in their own well-being, and do not benefit from working above the defined rate of work when it will not increase their remuneration. He therefore proposed that the work practice that had been developed in most work environments was crafted, intentionally or unintentionally, to be very inefficient in its execution. He posited that time and motion studies combined with rational analysis and synthesis could uncover one best method for performing any particular task, and that prevailing methods were seldom equal to these best methods. Crucially, Taylor himself prominently acknowledged that if each employee's compensation was linked to their output, their productivity would go up. Thus his compensation plans usually included piece rates. In contrast, some later adopters of time and motion studies ignored this aspect and tried to get large productivity gains while passing little or no compensation gains to the workforce, which contributed to resentment against the system.\n\nScientific management evolved in an era when mechanization and automation were still in their infancy. The ideas and methods of scientific management extended the American system of manufacturing in the transformation from craft work (with humans as the only possible agents) to mechanization and automation, although proponents of scientific management did not predict the extensive removal of humans from the production process. Concerns over labor-displacing technologies rose with increasing mechanization and automation.\n\nBy factoring processes into discrete, unambiguous units, scientific management laid the groundwork for automation and offshoring, prefiguring industrial process control and numerical control in the absence of any machines that could carry it out. Taylor and his followers did not foresee this at the time; in their world, it was \"humans\" that would execute the optimized processes. (For example, although in their era the instruction \"open valve A whenever pressure gauge B reads over value X\" would be carried out by a human, the fact that it had been reduced to an algorithmic component paved the way for a machine to be the agent.) However, one of the common threads between their world and ours is that the agents of execution need not be \"smart\" to execute their tasks. In the case of computers, they are \"not able\" (yet) to be \"smart\" (in that sense of the word); in the case of human workers under scientific management, they were often \"able\" but were \"not allowed\". Once the time-and-motion men had completed their studies of a particular task, the workers had very little opportunity for further thinking, experimenting, or suggestion-making. They were forced to \"play dumb\" most of the time, which occasionally led to revolts.\n\nThe middle ground between the craft production of skilled workers and full automation is occupied by systems of extensive mechanization and partial automation operated by semiskilled and unskilled workers. Such systems depend on algorithmic workflows and knowledge transfer, which require substantial engineering to succeed. Although Taylor's intention for scientific management was simply to optimize work methods, the process engineering that he pioneered also tends to build the skill into the equipment and processes, removing most need for skill in the workers. Such engineering has governed most industrial engineering since then. It is also the essence of successful offshoring. The common theme in all these cases is that businesses engineer their way out of their need for large concentrations of skilled workers, and the high-wage environments that sustain them. This creates competitive advantage on the local level of individual firms, although the pressure it exerts systemically on employment and employability is an externality.\n\nOther thinkers soon offered more ideas on the roles that workers play in mature industrial systems. These included ideas on improvement of the individual worker with attention to the worker's needs, not just the needs of the whole. James Hartness published \"The Human Factor in Works Management\" in 1912, while Frank Gilbreth and Lillian Moller Gilbreth offered their own alternatives to Taylorism. The human relations school of management evolved in the 1930s to complement rather than replace scientific management, with Taylorism determining the organisation of the work process, and human relations helping to adapt the workers to the new procedures. Today's efficiency-seeking methods, such as lean manufacturing, include respect for workers and fulfillment of their needs as integral parts of the theory. (Workers slogging their way through workdays in the business world do encounter flawed implementations of these methods that make jobs unpleasant; but these implementations generally lack managerial competence in matching theory to execution.) Clearly, a syncretism has occurred since Taylor's day, although its implementation has been uneven, as lean management in capable hands has produced good results for both managers and workers, but in incompetent hands has damaged enterprises.\n\nWith the division of labor that became commonplace as Taylorism was implemented in manufacturing, workers lost their sense of connection to the production of goods. Workers began to feel disenfranchised with the monotonous and unfulfilling work they were doing in factories. Before scientific management, workers felt a sense of pride when completing their good, which went away when workers only completed one part of production. \"The further 'progress' of industrial development... increased the anomic or forced division of labor,\" the opposite of what Taylor thought would be the effect. Partial adoption of Taylor's principles by management seeking to boost efficiency, while ignoring principles such as fair pay and direct engagement by managers, led to further tensions and the rise of unions to represent workers needs.\n\nTaylor had a largely negative view of unions, and believed they only led to decreased productivity. Although he opposed them, his work with scientific management led disenfranchised workers to look to unions for support.\n\nUnder scientific management, the demands of work intensified. Workers became dissatisfied with the work environment and became angry. During one of Taylor's own implementations at the Watertown Arsenal in Massachusetts, a strike led to an investigation of Taylor's methods by a U.S. House of Representatives committee. The committee reported in 1912, concluding that scientific management did provide some useful techniques and offered valuable organizational suggestions, but that it also gave production managers a dangerously high level of uncontrolled power. After an attitude survey of the workers revealed a high level of resentment and hostility towards scientific management, the Senate banned Taylor's methods at the arsenal.\n\nScientific management lowered worker morale and exacerbated existing conflicts between labor and management. As a consequence, the method inadvertently strengthened labor unions and their bargaining power in labor disputes, thereby neutralizing most or all of the benefit of any productivity gains it had achieved. Thus its net benefit to owners and management ended up as small or negative. It took new efforts, borrowing some ideas from scientific management but mixing them with others, to produce more productive formula.\n\nScientific management may have exacerbated grievances among workers about oppressive or greedy management. It certainly strengthened developments that put workers at a disadvantage: the erosion of employment in developed economies via both offshoring and automation. Both were made possible by the deskilling of jobs, which was made possible by the knowledge transfer that scientific management achieved. Knowledge was transferred both to cheaper workers and from workers into tools. Jobs that once would have required craft work first transformed to semiskilled work, then unskilled. At this point the labor had been commoditized, and thus the competition between workers (and worker populations) moved closer to pure than it had been, depressing wages and job security. Jobs could be offshored (giving one human's tasks to others—which could be good for the new worker population but was bad for the old) or they could be rendered nonexistent through automation (giving a human's tasks to machines). Either way, the net result from the perspective of developed-economy workers was that jobs started to pay less, then disappear. The power of labor unions in the mid-twentieth century only led to a push on the part of management to accelerate the process of automation, hastening the onset of the later stages just described.\n\nIn a central assumption of scientific management, \"the worker was taken for granted as a cog in the machinery.\" While scientific management had made jobs unpleasant, its successors made them less remunerative, less secure, and finally nonexistent as a consequence of structural unemployment.\n\nIt is often assumed that Fordism derives from Taylor's work. Taylor apparently made this assumption himself when visiting the Ford Motor Company's Michigan plants not too long before he died, but it is likely that the methods at Ford were evolved independently, and that any influence from Taylor's work was indirect at best. Charles E. Sorensen, a principal of the company during its first four decades, disclaimed any connection at all. There was a belief at Ford, which remained dominant until Henry Ford II took over the company in 1945, that the world's experts were worthless, because if Ford had listened to them, it would have failed to attain its great successes. Henry Ford felt that he had succeeded \"in spite of\", not \"because of\", experts, who had tried to stop him in various ways (disagreeing about price points, production methods, car features, business financing, and other issues). Sorensen thus was dismissive of Taylor and lumped him into the category of useless experts. Sorensen held the New England machine tool vendor Walter Flanders in high esteem and credits him for the efficient floorplan layout at Ford, claiming that Flanders knew nothing about Taylor. Flanders may have been exposed to the spirit of Taylorism elsewhere, and may have been influenced by it, but he did not cite it when developing his production technique. Regardless, the Ford team apparently did independently invent modern mass production techniques in the period of 1905-1915, and they themselves were not aware of any borrowing from Taylorism. Perhaps it is only possible with hindsight to see the zeitgeist that (indirectly) connected the budding Fordism to the rest of the efficiency movement during the decade of 1905-1915.\n\nScientific management appealed to managers of planned economies because central economic planning relies on the idea that the expenses that go into economic production can be precisely predicted and can be optimized by design. The opposite theoretical pole would be laissez-faire thinking in which the invisible hand of free markets is the only possible \"designer\". In reality most economies today are somewhere in between. Another alternative for economic planning is workers' self-management.\n\nIn the Soviet Union, Taylorism was advocated by Aleksei Gastev and \"nauchnaia organizatsia truda\" (\"the movement for the scientific organisation of labor\"). It found support in both Vladimir Lenin and Leon Trotsky. Gastev continued to promote this system of labor management until his arrest and execution in 1939. In the 1920s and 1930s, the Soviet Union enthusiastically embraced Fordism and Taylorism, importing American experts in both fields as well as American engineering firms to build parts of its new industrial infrastructure. The concepts of the Five Year Plan and the centrally planned economy can be traced directly to the influence of Taylorism on Soviet thinking. As scientific management was believed to epitomize American efficiency, Joseph Stalin even claimed that \"the combination of the Russian revolutionary sweep with American efficiency is the essence of Leninism.\"\n\nSorensen was one of the consultants who brought American know-how to the USSR during this era, before the Cold War made such exchanges unthinkable. As the Soviet Union developed and grew in power, both sides, the Soviets and the Americans, chose to ignore or deny the contribution that American ideas and expertise had made: the Soviets because they wished to portray themselves as creators of their own destiny and not indebted to a rival, and the Americans because they did not wish to acknowledge their part in creating a powerful communist rival. Anti-communism had always enjoyed widespread popularity in America, and anti-capitalism in Russia, but after World War II, they precluded any admission by either side that technologies or ideas might be either freely shared or clandestinely stolen.\n\nBy the 1950s, scientific management had grown dated, but its goals and practices remained attractive and were also being adopted by the German Democratic Republic as it sought to increase efficiency in its industrial sectors. In the accompanying photograph from the German Federal Archives, workers discuss standards specifying how each task should be done and how long it should take. The workers are engaged in a state-planned instance of process improvement, but they are pursuing the same goals that were contemporaneously pursued in capitalist societies, as in the Toyota Production System.\n\nIn 1911, organized labor erupted with strong opposition to scientific management, spreading from Samuel Gompers, founder and president of the American Federal of Labor (AFL), in the US to far around the globe. By 1913 Vladimir Lenin wrote that the \"most widely discussed topic today in Europe, and to some extent in Russia, is the 'system' of the American engineer, Frederick Taylor\"; Lenin decried it as merely a \"'scientific' system of sweating\" more work from laborers. Again in 1914, Lenin derided Taylorism as \"man’s enslavement by the machine.\" However, after the Russian Revolutions brought him to power, Lenin wrote in 1918 that the \"Russian is a bad worker [who must] learn to work. The Taylor system... is a combination of the refined brutality of bourgeois exploitation and a number of the greatest scientific achievements in the field of analysing mechanical motions during work, the elimination of superfluous and awkward motions, the elaboration of correct methods of work, the introduction of the best system of accounting and control, etc. The Soviet Republic must at all costs adopt all that is valuable in the achievements of science and technology in this field.\" \n\nThe Watertown Arsenal in Massachusetts provides an example of the application and repeal of the Taylor system in the workplace, due to worker opposition. In the early 2001, neglect in the Watertown shops included overcrowding, dim-lighting, lack of tools and equipment, and questionable management strategies in the eyes of the workers. Frederick W. Taylor and Carl G. Barth visited Watertown in April 1909 and reported on their observations at the shops. Their conclusion was to apply the Taylor system of management to the shops to produce better results. Efforts to install the Taylor system began in June 1909. Over the years of time study and trying to improve the efficiency of workers, criticisms began to evolve. Workers complained of having to compete with one another, feeling strained and resentful, and feeling excessively tired after work. There is, however, no evidence that the times enforced were unreasonable. In June 1913, employees of the Watertown Arsenal petitioned to abolish the practice of scientific management there. A number of magazine writers inquiring into the effects of scientific management found that the \"conditions in shops investigated contrasted favorably with those in other plants\".\n\nCriticism of Taylor's principles of effective workmanship and the productivity of the workers continues today. Often, his theories are described as man-contemptuous and portrayed as now overhauled. In practice, however, the principles of Taylor are still being pursued by Kaizen and Six Sigma and similar methodologies, which are based on the development of working methods and courses based on systematic analysis rather than relying on tradition and rule of thumb.\n\nTaylorism is, according to Stephen P. Waring, considered very controversial, despite its popularity. It is often criticized for turning the worker into an \"automaton\" or \"machine\". Due to techniques employed with scientific management, employees claim to have become overworked and were hostile to the process. Criticisms commonly came from workers who were subjected to an accelerated work pace, lower standards of workmanship, lower product-quality, and lagging wages. Workers defied being reduced to such machines, and objected to the practices of Taylorism. Many workers formed unions, demanded higher pay, and went on strike to be free of control issues. This ignited class conflict, which Taylorism was initially meant to prevent. Efforts to resolve the conflicts included methods of scientific collectivism, making agreements with unions, and the personnel management movement.\n\nIn the middle of 1960 some counter-movements to Taylorism arose. Representatives of the so-called Human Relations movement urged humanization and democratization of the working world. The criticism of Taylorism supports the unilateral approach of labor. Strictly speaking, Taylorism is not a scientific theory. All theories of F. W. Taylor are based on experiments. On the basis of samples, conclusions were made, which were then generalized. There is no representativeness of the selected sample.\n\nAnother reason for criticizing Taylor's methods stemmed from Taylor's belief that the scientific method included the calculations of exactly how much time it takes a man to do a particular task, or his rate of work. However, the opposition to this argument is that such a calculation relies on certain arbitrary, non-scientific decisions such as what constituted the job, which men were timed, and under which conditions. Any of these factors are subject to change, and therefore can produce inconsistencies.\n\nSome dismiss so-called \"scientific management\"/Taylorism as pseudoscience.\n\nScientific management was one of the first attempts to systematically treat management and process improvement as a scientific problem. It may have been the first to do so in a \"bottom-up\" way and found a lineage of successors that have many elements in common. With the advancement of statistical methods, quality assurance and quality control began in the 1920s and 1930s. During the 1940s and 1950s, the body of knowledge for doing scientific management evolved into operations management, operations research, and management cybernetics. In the 1980s total quality management became widely popular, and in the 1990s \"re-engineering\" went from a simple word to a mystique. Today's Six Sigma and lean manufacturing could be seen as new kinds of scientific management, although their evolutionary distance from the original is so great that the comparison might be misleading. In particular, Shigeo Shingo, one of the originators of the Toyota Production System, believed that this system and Japanese management culture in general should be seen as a kind of scientific management.\n\nPeter Drucker saw Frederick Taylor as the creator of knowledge management, because the aim of scientific management was to produce knowledge about how to improve work processes. Although the typical application of scientific management was manufacturing, Taylor himself advocated scientific management for all sorts of work, including the management of universities and government. For example, Taylor believed scientific management could be extended to \"the work of our salesmen\". Shortly after his death, his acolyte Harlow S. Person began to lecture corporate audiences on the possibility of using Taylorism for \"sales engineering\" (Person was talking about what is now called sales process engineering—engineering the processes that salespeople use—not about what we call sales engineering today.) This was a watershed insight in the history of corporate marketing.\n\nGoogle's methods of increasing productivity and output can be seen to be influenced by Taylorism as well. The Silicon Valley company is a forerunner in applying behavioral science to increase knowledge worker productivity. In classic scientific management as well as approaches like lean management or business process reengineering leaders and experts develop and define standard. Leading high-tech companies use the concept of nudge management to increase productivity of employees. More and more business leaders start to make use of this new scientific management.\n\nToday's militaries employ all of the major goals and tactics of scientific management, if not under that name. Of the key points, all but wage incentives for increased output are used by modern military organizations. Wage incentives rather appear in the form of skill bonuses for enlistments.\n\nScientific management has had an important influence in sports, where stop watches and motion studies rule the day. (Taylor himself enjoyed sports, especially tennis and golf. He and a partner won a national championship in doubles tennis. He invented improved tennis racquets and improved golf clubs, although other players liked to tease him for his unorthodox designs, and they did not catch on as replacements for the mainstream implements).\n\nModern human resources can be seen to have begun in the scientific management era, most notably in the writings of Katherine M. H. Blackford, who was also a proponent of eugenics.\n\nPractices descended from scientific management are currently used in offices and in medicine (e.g. managed care) as well.\n\nIn the 21st century the tendency to overcome Taylorism is very great. The trend is moving away from assembly line work, since people are increasingly being replaced by machines in production plants and sub-processes are automated, so that human labor is not necessary in these cases. The desire for automated workflow in companies is intended to reduce costs and support the company at the operational level.\n\nFurthermore, it can be observed that many companies try to make the workplace as comfortable as possible for the employees. This is achieved by light flooded rooms, Feng Shui methods in the workplace or even by creative jobs. The efficiency and creativity of the employees is to be promoted by a pleasant atmosphere at the workplace. Approaches of the Scientific Management, in which attempts are also made to make the work environment pleasant, are partly recognizable here.\n\nIn the works of Gouldner and Crozier, the recognition of the plurality of industrial forms is being discussed. In the 21st century, we have a modern corporate management, where managers are given the available positions in companies and are given the right to take legal action.\n\nThe working world of the 21st century is mainly based on Total Quality Management. This is derived from quality control. In contrast to Taylorism, by which products are produced in the shortest possible time without any form of quality control and delivered to the end customer, the focus in the 21st century is on quality control at TQM. In order to avoid error rates, it is necessary to hire specialists to check all the products which have been manufactured before they are delivered to the end customer. The quality controls have improved over time, and incorrect partial processes can be detected in time and removed from the production process.\n\nTaylorism approaches are largely prevalent in companies where machines can not perform certain activities. Certain subprocesses are still to be carried out by humans, such as the sorting out of damaged fruit in the final process before the goods are packed by machines. It turns out that the quality control is ultimately to be verified by the individual man. Certain activities remain similar to the approach of Taylorism. There are no \"zero error programs\", employees have to be trained and thus reduce error rates.\n\nThrough the invention of the management one managed positions, which are equipped with disposition rights. The positions are occupied by paid employees and form the basis for the current, modern corporate management. In order to be able to perceive these positions, it was no longer necessary to bring in resources such as capital, but instead qualifications were necessary. Written rights are also passed on to employees, which means that the leaders of an organization tend to fall into the background and merely have a passive position.\n\nThe structure and size of a company must be distinguished. Depending on which dispositions are predominant, the size of the company, the sector, and the number of employees in an organization, one can examine whether approaches of Taylorism are prevalent. It is believed to be predominant in the automotive industry. In spite of the fact that a lot of activities have been replaced by machines during the production, it is ultimately the person who can check the quality of a product.\n\nTaylorism led to a performance increase in companies. All superfluous working steps are avoided. The company benefits from the productivity of the workers and this in turn from higher wages. Unused productivity resources were effectively exploited by Taylorism.\n\nToday's work environment in the 21st century benefits from the humanity of working conditions. Corporate strategies are increasingly focused on the flexibility of work. Flexible adaptation to demand should be possible. The qualifications of the employees, the work content as well as the work processes are determined by the competition situation on the market. The aim is to promote self-discipline and the motivation of employees in order to achieve their own tasks and at the same time to prevent monotonous work. Technical progress has led to more humane working conditions since inhumane work steps are done by the machines.\n\nTaylorism's approach is called inhuman. The increased wage alone is not a permanent incentive for the workers to carry out the same monotonous work. Worker-friendly work structures are required. People no longer want to be perceived merely as executive organ. The complete separation from manual and headwork leads to a lack of pleasure in the execution of the work steps.\n\nIn the 21st century the rising level of education leads to better trained workers, but the competitive pressure also rises. The interplay of economic as well as the pressure to innovate also lead to uncertainty among employees. The national diseases in the 21st century have become burn-out phenomena and depressions, often in conjunction with the stress and the increased performance pressure in the work.\n\n\n\n"}
{"id": "169531", "url": "https://en.wikipedia.org/wiki?curid=169531", "title": "Sedlec Ossuary", "text": "Sedlec Ossuary\n\nThe Sedlec Ossuary () is a small Roman Catholic chapel, located beneath the Cemetery Church of All Saints (Czech: \"Hřbitovní kostel Všech Svatých\"), part of the former Sedlec Abbey in Sedlec, a suburb of Kutná Hora in the Czech Republic. The ossuary is estimated to contain the skeletons of between 40,000 and 70,000 people, whose bones have, in many cases, been artistically arranged to form decorations and furnishings for the chapel. The ossuary is among the most visited tourist attractions of the Czech Republic - attracting over 200,000 visitors annually.\nFour enormous bell-shaped mounds occupy the corners of the chapel. An enormous chandelier of bones, which contains at least one of every bone in the human body, hangs from the center of the nave with garlands of skulls draping the vault. Other works include piers and monstrances flanking the altar, a coat of arms of the House of Schwarzenberg, and the signature of Rint, also executed in bone, on the wall near the entrance.\n\nIn 1278, Henry, the abbot of the Cistercian monastery in Sedlec, was sent to the Holy Land by King Otakar II of Bohemia. He returned with a small amount of earth he had removed from Golgotha and sprinkled it over the abbey cemetery. The word of this pious act soon spread and the cemetery in Sedlec became a desirable burial site throughout Central Europe.\n\nIn the mid 14th century, during the Black Death, and after the Hussite Wars in the early 15th century, many thousands were buried in the abbey cemetery, so it had to be greatly enlarged.\n\nAround 1400, a Gothic church was built in the center of the cemetery with a vaulted upper level and a lower chapel to be used as an ossuary for the mass graves unearthed during construction, or simply slated for demolition to make room for new burials.\n\nAfter 1511, the task of exhuming skeletons and stacking their bones in the chapel was given to a half-blind monk of the order.\n\nBetween 1703 and 1710, a new entrance was constructed to support the front wall, which was leaning outward, and the upper chapel was rebuilt. This work, in the Czech Baroque style, was designed by Jan Santini Aichel.\n\nIn 1870, František Rint, a woodcarver, was employed by the Schwarzenberg family to put the bone heaps into order, yielding a macabre result. The signature of Rint, also executed in bone, appears on the wall near the entrance to the chapel.\n\nIn 1970, the centenary of Rint's contributions, Czech filmmaker Jan Švankmajer was commissioned to document the ossuary. The result was a 10-minute-long frantic-cut film of skeletal images overdubbed with an actual tour-guide's neutral voice narration. This version was initially banned by the Czech Communist authorities for alleged subversion, and the soundtrack was replaced by a brief spoken introduction and a jazz arrangement by Zdeněk Liška of the poem \"Comment dessiner le portrait d'un oiseau\" (\"How to Draw the Portrait of a Bird\") by Jacques Prévert. Since the Velvet Revolution, the original tour guide soundtrack has been made available.\n\nIn the documentary \"Long Way Round\", Ewan McGregor and Charley Boorman stop to see this church. Dan Cruickshank also views the church in his \"Adventures in Architecture\".\n\nThe ossuary is a major plot device in the John Connolly novel \"The Black Angel\".\n\nThe ossuary is used as a location for the \"Dungeons & Dragons\" movie and the movie \"Blood & Chocolate\".\n\nThe ossuary was also featured in \"Ripley's Believe it or Not\" and is described by Cara Seymour in the final scene of the film \"Adaptation.\"\n\nThe ossuary was also the influence for Dr. Satan's lair in the Rob Zombie film \"House of 1000 Corpses\". \n\n\n"}
{"id": "14707473", "url": "https://en.wikipedia.org/wiki?curid=14707473", "title": "Segen", "text": "Segen\n\nSegen is a German word translating to \"blessing, benediction; charm; prayer; spell, incantation\".\n\nIt is in origin a loan from Latin \"signum\" \"sīgnāre\" \"to make a sign\", viz. the Sign of the Cross used to confer a Christian blessing,\nThe term is attested as Old High German \"seganōn\" from as early as c. AD 800, resulting in a modern \"segnen\" \"to bless\". The noun \"Segen\" \"blessing\" was derived from the verb at an early time, attested in the 9th century as \"segan\".\nOld English hat the corresponding \"sægnan\", which survives as the dialectal (esp. Scottish) \"sain\" (popularized by Scott, \"Heart of Mid-Lothian\" \"God sain us\").\nThe concept of \"Segen\", understood magically, was very productive in the folklore, folk religion and superstition of German-speaking Europe, studied in great detail by the German philologists and folklorists of the 19th century.\n\nThe medieval church used the \"Segen\" (the sign of the cross with a spoken formula) liberally, intended as an act with protective effect, putting the person or thing blessed under the protection of God. Nor was the action reserved for priests or clerics, but any Christian was permitted to make the sign of the cross and invoke the protection of God. Thus the \"Segen\" came to be seen as the inverse of the curse (\"Fluch\"), magical acts with the power to either protect or harm. \nThe concept of \"Segen\" thus became the continuant of the incantation formulas of the pre-Christian period (the only surviving samples of which are the Merseburg Incantations).\n\nUse of such formulas was partly encouraged by the Church, as they did superficially involve an expression of piety by the invocation of God, Christ or the Virgin Mary, but at the same time their magical use was viewed with scepticism and was sometimes repressed.\nBy the time of the Early Modern witch-hunts, the term \"segen\" had become ambiguous, and depending on context could refer to a harmless farewell, to a pious invocation of God, or to a Satanic or superstitious spell (\"pro incantamento et adjuratione magica\" Stieler 1669; e.g. \"Wolfssegen\" \"contra lupos\").\nThis early modern usage survives in dialectal variation throughout the rural parts of German-speaking Europe.\nFor German-speaking Switzerland, the \"Schweizerisches Idiotikon\" 7,444 \"Sëgeⁿ\" \nrecords some two dozen compounds in \"-sëgeⁿ\", in some of which \"Segen\" takes the meaning \"prayer\" and in others \"spell, charm\".\n\nA notable concept in Swiss folklore is the \"Alpsegen\" (\"Alpe(n)sëgeⁿ, Alpsëgeⁿ\" 7,451), a folk religious custom in Alpine Switzerland where every night \na prayer must be sung over each Alpine pasture. This combined the function of an apotropaic charm with a practical aspect of communicating between remote pastures; if the \"Alpsegen\" was not heard from a neighbouring site, it would be a sign that a misfortune or accident had befallen and the neighbours would come to aid.\n"}
{"id": "234273", "url": "https://en.wikipedia.org/wiki?curid=234273", "title": "Separation of concerns", "text": "Separation of concerns\n\nIn computer science, separation of concerns (SoC) is a design principle for separating a computer program into distinct sections, such that each section addresses a separate concern. A concern is a set of information that affects the code of a computer program. A concern can be as general as the details of the hardware the code is being optimized for, or as specific as the name of a class to instantiate. A program that embodies SoC well is called a modular program. Modularity, and hence separation of concerns, is achieved by encapsulating information inside a section of code that has a well-defined interface. Encapsulation is a means of information hiding. Layered designs in information systems are another embodiment of separation of concerns (e.g., presentation layer, business logic layer, data access layer, persistence layer).\n\nThe value of separation of concerns is simplifying development and maintenance of computer programs. When concerns are well-separated, individual sections can be reused, as well as developed and updated independently. Of special value is the ability to later improve or modify one section of code without having to know the details of other sections, and without having to make corresponding changes to those sections.\n\nThe mechanisms for modular or object-oriented programming that are provided by a programming language are mechanisms that allow developers to provide SoC. For example, object-oriented programming languages such as C#, C++, Delphi, and Java can separate concerns into objects, and architectural design patterns like MVC or MVP can separate content from presentation and the data-processing (model) from content.\nService-oriented design can separate concerns into services. Procedural programming languages such as C and Pascal can separate concerns into procedures or functions. Aspect-oriented programming languages can separate concerns into aspects and objects.\n\nSeparation of concerns is an important design principle in many other areas as well, such as urban planning, architecture and information design. The goal is to more effectively understand, design, and manage complex interdependent systems, so that functions can be reused, optimized independently of other functions, and insulated from the potential failure of other functions.\n\nCommon examples include separating a space into rooms, so that activity in one room does not affect people in other rooms, and keeping the stove on one circuit and the lights on another, so that overload by the stove does not turn the lights off. The example with rooms shows encapsulation, where information inside one room, such as how messy it is, is not available to the other rooms, except through the interface, which is the door. The example with circuits demonstrates that activity inside one module, which is a circuit with consumers of electricity attached, does not affect activity in a different module, so each module is not concerned with what happens in the other.\n\nThe term \"separation of concerns\" was probably coined by Edsger W. Dijkstra in his 1974 paper \"On the role of scientific thought\".\nFifteen years later, it was evident the term Separation of Concerns was becoming an accepted idea. In 1989, Chris Reade wrote a book titled \"Elements of Functional Programming\" that describes separation of concerns:\n\nReade continues to say,\n\nSeparation of concerns is crucial to the design of the Internet. In the Internet Protocol Suite, great efforts have been made to separate concerns into well-defined layers. This allows protocol designers to focus on the concerns in one layer, and ignore the other layers. The Application Layer protocol SMTP, for example, is concerned about all the details of conducting an email session over a reliable transport service (usually TCP), but not in the least concerned about how the transport service makes that service reliable. Similarly, TCP is not concerned about the routing of data packets, which is handled at the Internet Layer.\n\nHyperText Markup Language (HTML), Cascading Style Sheets (CSS), and JavaScript (JS) are complementary languages used in the development of webpages and websites. HTML is mainly used for organization of webpage content, CSS is used for definition of content presentation style, and JS defines how the content interacts and behaves with the user. Historically, this was not the case: prior to the introduction of CSS, HTML performed both duties of defining semantics and style.\n\nSubject-oriented programming allows separate concerns to be addressed as separate software constructs, each on an equal footing with the others. Each concern provides its own class-structure into which the objects in common are organized, and contributes state and methods to the composite result where they cut across one another. Correspondence rules describe how the classes and methods in the various concerns are related to each other at points where they interact, allowing composite behavior for a method to be derived from several concerns. Multi-dimensional Separation of Concerns allows the analysis and composition of concerns to be manipulated as a multi-dimensional \"matrix\" in which each concern provides a dimension in which different points of choice are enumerated, with the cells of the matrix occupied by the appropriate software artifacts.\n\nAspect-oriented programming allows cross-cutting concerns to be addressed as primary concerns. For example, most programs require some form of security and logging. Security and logging are often secondary concerns, whereas the primary concern is often on accomplishing business goals. However, when designing a program, its security must be built into the design from the beginning instead of being treated as a secondary concern. Applying security afterwards often results in an insufficient security model that leaves too many gaps for future attacks. This may be solved with aspect-oriented programming. For example, an aspect may be written to enforce that calls to a certain API are always logged, or that errors are always logged when an exception is thrown, regardless of whether the program's procedural code handles the exception or propagates it.\n\nMost project organization tasks are seen as secondary tasks. For example, build automation is an approach to automating the process of compiling source code into binary code. The primary goals in build automation are reducing the risk of human error and saving time.\n\nIn cognitive science and artificial intelligence, it is common to refer to David Marr's levels of analysis. At any given time, a researcher may be focusing on (1) what some aspect of intelligence needs to compute, (2) what algorithm it employs, or (3) how that algorithm is implemented in hardware. This separation of concerns is similar to the interface/implementation distinction in software and hardware engineering.\n\nIn Normalized Systems separation of concerns is one of the four guiding principles. Adhering to this principle is one of the tools that helps reduce the combinatorial effects that, over time, get introduced in software that is being maintained. In Normalized Systems separation of concerns is actively supported by the tools.\n\nSeparation of concerns can be implemented and enforced via partial classes.\n\n\n\n"}
{"id": "2874293", "url": "https://en.wikipedia.org/wiki?curid=2874293", "title": "Single-minute exchange of die", "text": "Single-minute exchange of die\n\nSingle-minute exchange of die (SMED) is one of the many lean production methods for reducing waste in a manufacturing process. It provides a rapid and efficient way of converting a manufacturing process from running the current product to running the next product. This rapid changeover is key to reducing production lot sizes and thereby improving flow (Mura), reducing production loss and output variability.\n\nThe phrase \"single minute\" does not mean that all changeovers and startups should take only \"one\" minute, but that they should take less than 10 minutes (in other words, \"single-digit minute\"). Closely associated is a yet more difficult concept, One-Touch Exchange of Die, (OTED), which says changeovers can and should take less than 100 seconds. A die is a tool used in manufacturing. However SMED's utility is not limited to manufacturing (see value stream mapping).\n\nFrederick Taylor analyzed non-value-adding parts of setups in his 1911 book, \"Shop Management\" (page 171). However, he did not create any method or structured approach around it.\n\nFrank Gilbreth studied and improved working processes in many different industries, from bricklaying to surgery. As part of his work, he also looked into changeovers. His book \"Motion Study\" (also from 1911) described approaches to reduce setup time.\n\nEven Henry Ford's factories were using some setup reduction techniques. In the 1915 publication \"Ford Methods and Ford Shops\", setup reduction approaches were clearly described. However, these approaches never became mainstream. For most parts during the 20th century, the economic order quantity was the gold standard for lot sizing.\n\nThe JIT workflow of Toyota had this problem of tools changeover took between two and eight hours, Toyota could neither afford the lost production time nor the enormous lot sizes suggested by the economic order quantity. Lot reduction and set up time reduction had actually been ongoing in TPS since 1945 when Taiichi Ohno became manager of the machine shops in Toyota. On a trip to the US in 1955, Taiichi Ohno observed Danly stamping presses with rapid die change capability. Subsequently, Toyota bought multiple Danly presses for the Motomachi plant. And Toyota started to work on improving the changeover time of their presses. This was known as \"Quick Die Change\", or \"QDC\" for short. They developed a structured approach based on a framework from the US World War II \"Training within Industry\" (TWI) program, called ECRS – Eliminate, Combine, Rearrange, and Simplify.\n\nOver time they reduced these changeover times from hours to fifteen minutes by the 1960s, three minutes by the 1970s and then just 180 seconds by 1990s.\n\nDuring the late 1970s, when Toyota's method was already well refined, Shigeo Shingo participated in one QDC workshop. After he started to publicize details of the Toyota Production System without permission, the business connection was terminated abruptly by Toyota. Shingo moved to the US and started to consult on lean manufacturing. Besides claiming to have invented this quick changeover method (among many other things), he renamed it \"Single Minute Exchange of Die\" or, in short, SMED. The \"Single Minute\" stands for a single digit minute (i.e., less than ten minutes). He promoted TPS and SMED in US.\n\nToyota found that the most difficult tools to change were the dies on the large transfer-stamping machines that produce car vehicle body parts. The dies – which must be changed for each model – weigh many tons, and must be assembled in the stamping machines with tolerances of less than a millimeter, otherwise the stamped metal will wrinkle, if not melt, under the intense heat and pressure.\n\nWhen Toyota engineers examined the change-over, they discovered that the established procedure was to stop the line, let down the dies by an overhead crane, position the dies in the machine by human eyesight, and then adjust their position with crowbars while making individual test stampings. The existing process took from twelve hours to almost three days to complete.\n\nToyota's first improvement was to place precision measurement devices on the transfer stamping machines, and record the necessary measurements for each model's die. Installing the die against these measurements, rather than by human eyesight, immediately cut the change-over to a mere hour and a half.\n\nFurther observations led to further improvements – scheduling the die changes in a standard sequence (as part of FRS) as a new model moved through the factory, dedicating tools to the die-change process so that all needed tools were nearby, and scheduling use of the overhead cranes so that the new die would be waiting as the old die was removed. Using these processes, Toyota engineers cut the change-over time to less than 10 minutes per die, and thereby reduced the economic lot size below one vehicle.\n\nThe success of this program contributed directly to just-in-time manufacturing which is part of the Toyota Production System. SMED makes Load balancing much more achievable by reducing economic lot size and thus stock levels.\n\nShigeo Shingo, who created the SMED approach, claims that in his data from between 1975 and 1985 that average setup times he has dealt with have reduced to 2.5% of the time originally required; a 40 times improvement.\n\nHowever, the power of SMED is that it has a lot of other effects which come from systematically looking at operations; these include:\n\n\nShigeo Shingo recognizes eight techniques that should be considered in implementing SMED.\n\nNB External setup can be done without the line being stopped whereas internal setup requires that the line be stopped.\n\nHe suggests that SMED improvement should pass through four conceptual stages:\n\nA) ensure that external setup actions are performed while the machine is still running,\nB) separate external and internal setup actions, ensure that the parts all function and implement efficient ways of transporting the die and other parts,\nC) convert internal setup actions to external,\nD) improve all setup actions.\n\nThere are seven basic steps to reducing changeover using the SMED system:\n\n\nThis diagram shows four successive runs with learning from each run and improvements applied before the next.\n\nThe SMED concept is credited to Shigeo Shingo, one of the main contributors to the consolidation of the Toyota Production System, along with Taiichi Ohno.\n\nLook for:\n\nRecord all necessary data\n\nParallel operations using multiple operators By taking the 'actual' operations and making them into a network which contains the dependencies it is possible to optimise task attribution and further optimize setup time. Issues of effective communication between the operators must be managed to ensure safety is assured where potentially noisy or visually obstructive conditions occur.\n\n"}
{"id": "464804", "url": "https://en.wikipedia.org/wiki?curid=464804", "title": "Social cognition", "text": "Social cognition\n\nSocial cognition is \"a sub-topic of social psychology that focuses on how people process, store, and apply information about other people and social situations. It focuses on the role that cognitive processes play in social interactions.\"\n\nMore technically, social cognition refers to how people deal with conspecifics (members of the same species) or even across species (such as pet) information, include four stages: encoding, storage, retrieval, and processing. In the area of social psychology, social cognition refers to a specific approach in which these processes are studied according to the methods of cognitive psychology and information processing theory. According to this view, social cognition is a level of analysis that aims to understand social psychological phenomena by investigating the cognitive processes that underlie them. The major concerns of the approach are the processes involved in the perception, judgment, and memory of social stimuli; the effects of social and affective factors on information processing; and the behavioral and interpersonal consequences of cognitive processes. This level of analysis may be applied to any content area within social psychology, including research on intrapersonal, interpersonal, intragroup, and intergroup processes.\n\nThe term \"social cognition\" has been used in multiple areas in psychology and cognitive neuroscience, most often to refer to various social abilities disrupted in autism, schizophrenia and other disorders. In cognitive neuroscience the biological basis of social cognition is investigated. Developmental psychologists study the development of social cognition abilities.\n\nSocial cognition came to prominence with the rise of cognitive psychology in the late 1960s and early 1970s and is now the dominant model and approach in mainstream social psychology. Common to social cognition theories is the idea that information is represented in the brain as \"cognitive elements\" such as schemas, attributions, or stereotypes. A focus on how these cognitive elements are processed is often employed. Social cognition therefore applies and extends many themes, theories, and paradigms from cognitive psychology that can be identified in reasoning (representativeness heuristic, base rate fallacy and confirmation bias), attention (automaticity and priming) and memory (schemas, primacy and recency). It is likely that social psychology has always had a more cognitive than general psychology approach, as it traditionally discussed internal mental states such as beliefs and desires when mainstream psychology was dominated by behaviorism.\n\nOne notable theory of social cognition is social schema theory, although it is not the basis of all social cognition studies (for example, see attribution theory). It has been suggested that other disciplines in social psychology such as social identity theory and social representations may be seeking to explain largely the same phenomena as social cognition, and that these different disciplines might be merged into a \"coherent integrated whole\". A parallel paradigm has arisen in the study of action, termed motor cognition, which is concerned with understanding the representation of action and the associated process.\n\nSocial schema theory builds on and uses terminology from schema theory in cognitive psychology, which describes how ideas or \"concepts\" are represented in the brain and how they are categorized. According to this view, when we see or think of a concept a mental representation or schema is \"activated\" bringing to mind other information which is linked to the original concept by association. This activation often happens unconsciously. As a result of activating such schemas, judgements are formed which go beyond the information actually available, since many of the associations the schema evokes extend outside the given information. This may influence social cognition and behaviour regardless of whether these judgements are accurate or not. For example, if an individual is introduced as a teacher, then a \"teacher schema\" may be activated. Subsequently, we might associate this person with wisdom or authority, or past experiences of teachers that we remember and consider important.\n\nWhen a schema is more accessible it can be more quickly activated and used in a particular situation. Two cognitive processes that increase accessibility of schemas are salience and priming. Salience is the degree to which a particular social object stands out relative to other social objects in a situation. The higher the salience of an object the more likely that schemas for that object will be made accessible. For example, if there is one female in a group of seven males, female gender schemas may be more accessible and influence the group's thinking and behavior toward the female group member. Priming refers to any experience immediately prior to a situation that causes a schema to be more accessible. For example, watching a scary movie late at night might increase the accessibility of frightening schemas, increasing the likelihood that a person will perceive shadows and background noises as potential threats.\n\nSocial cognition researchers are interested in how new information is integrated into pre-established schemas, especially when the information contrasts with the existing schema. For example, a student may have a pre-established schema that all teachers are assertive and bossy. After encountering a teacher who is timid and shy, a social cognition researcher might be interested in how the student will integrate this new information with his/her existing teacher schema. Pre-established schemas tend to guide attention to new information, as people selectively attend to information that is consistent with the schema and ignore information that is inconsistent. This is referred to as a confirmation bias. Sometimes inconsistent information is sub-categorized and stored away as a special case, leaving the original schema intact without any alterations. This is referred to as subtyping.\n\nSocial cognition researchers are also interested in the regulation of activated schemas. It is believed that the situational activation of schemas is automatic, meaning that it is outside individual conscious control. In many situations however, the schematic information that has been activated may be in conflict with the social norms of the situation in which case an individual is motivated to inhibit the influence of the schematic information on their thinking and social behavior. Whether a person will successfully regulate the application of the activated schemas is dependent on individual differences in self-regulatory ability and the presence of situational impairments to executive control. High self-regulatory ability and the lack of situational impairments on executive functioning increase the likelihood that individuals will successfully inhibit the influence of automatically activated schemas on their thinking and social behavior. When people stop suppressing the influence of the unwanted thoughts, a rebound effect can occur where the thought becomes hyper-accessible.\n\nSocial psychologists have become increasingly interested in the influence of culture on social cognition. Although people of all cultures use schemas to understand the world, the content of schemas has been found to differ for individuals based on their cultural upbringing. For example, one study interviewed a Scottish settler and a Bantu herdsman from Swaziland and compared their schemas about cattle. Because cattle are essential to the lifestyle of the Bantu people, the Bantu herdsmen's schemas for cattle were far more extensive than the schemas of the Scottish settler. The Bantu herdsmen was able to distinguish his cattle from dozens of others, while the Scottish settler was not.\n\nCultural influences have been found to shape some of the basic ways in which people automatically perceive and think about their environment. For example, a number of studies have found that people who grow up in East Asian cultures such as China and Japan tend to develop holistic thinking styles, whereas people brought up in Western cultures like Australia and the USA tend to develop analytic thinking styles. The typically Eastern holistic thinking style is a type of thinking in which people focus on the overall context and the ways in which objects relate to each other. For example, if an Easterner was asked to judge how a classmate is feeling then he/she might scan everyone's face in the class, and then use this information to judge how the individual is feeling. On the other hand, the typically Western analytic thinking style is a type of thinking style in which people focus on individual objects and neglect to consider the surrounding context. For example, if a Westerner was asked to judge how a classmate is feeling, then he or she might focus only on the classmate's face in order to make the judgment.\n\nNisbett (2003) suggested that cultural differences in social cognition may stem from the various philosophical traditions of the East (i.e. Confucianism and Buddhism) versus the Greek philosophical traditions (i.e. of Aristotle and Plato) of the West. However, recent research indicates that differences in social cognition may originate from physical differences in the environments of the two cultures. One study found that scenes from Japanese cities were 'busier' than those in the USA as they contain more objects which compete for attention. In this study, the Eastern holistic thinking style (and focus on the overall context) was attributed to the busier nature of the Japanese physical environment.\n\nEarly interest in the relationship between brain function and social cognition includes the case of Phineas Gage, whose behaviour was reported to have changed after an accident damaged one or both of his frontal lobes. More recent neuropsychological studies have shown that brain injuries disrupt social cognitive processes. For example, damage to the frontal lobes can affect emotional responses to social stimuli and performance on theory of mind tasks. In the temporal lobe, damage to the fusiform gyrus can lead to the inability to recognize faces.\n\nPeople with psychological disorders such as autism, psychosis, mood disorder, Williams syndrome, antisocial personality disorder, Fragile X and Turner's syndrome show differences in social behavior compared to their unaffected peers. Parents with posttraumatic stress disorder (PTSD) show disturbances in at least one aspect of social cognition: namely, joint attention with their young children only after a laboratory-induced relational stressor as compared to healthy parents without PTSD. However, whether social cognition is underpinned by domain-specific neural mechanisms is still an open issue. There is now an expanding research field examining how such conditions may bias cognitive processes involved in social interaction, or conversely, how such biases may lead to the symptoms associated with the condition.\n\nThe development of social cognitive processes in infants and children has also been researched extensively (see developmental psychology). For example, it has been suggested that some aspects of psychological processes that promote social behavior (such as facial recognition) may be innate. Consistent with this, very young babies recognize and selectively respond to social stimuli such as the voice, face and scent of their mother.\n\n"}
{"id": "949648", "url": "https://en.wikipedia.org/wiki?curid=949648", "title": "Synectics", "text": "Synectics\n\nSynectics is a problem solving methodology that stimulates thought processes of which the subject may be unaware. This method was developed by George M. Prince (April 5, 1918 – June 9, 2009) and William J.J. Gordon, originating in the Arthur D. Little Invention Design Unit in the 1950s.\n\nThe process was derived from tape-recording (initially audio, later video) meetings, analysis of the results and experiments with alternative ways of dealing with the obstacles to success in the meeting. \"Success\" was defined as getting a creative solution that the group was committed to implement.\n\nThe name Synectics comes from the Greek and means \"the joining together of different and apparently irrelevant elements.\"\n\nGordon and Prince named both their practice and their new company Synectics, which can cause confusion as people not part of the company are trained and use the practice. While the name was trademarked, it has become a standard word for describing creative problem solving in groups.\n\nSynectics is a way to approach creativity and problem-solving in a rational way. \"Traditionally, the creative process has been considered after the fact... The Synectics study has attempted to research creative process in vivo, while it is going on.\"\n\nAccording to Gordon, Synectics research has three main assumptions:\nWith these assumptions in mind, Synectics believes that people can be better at being creative if they understand how creativity works.\n\nOne important element in creativity is embracing the seemingly irrelevant. Emotion is emphasized over intellect and the irrational over the rational. Through understanding the emotional and irrational elements of a problem or idea, a group can be more successful at solving a problem.\n\nPrince emphasized the importance of creative behaviour in reducing inhibitions and releasing the inherent creativity of everyone. He and his colleagues developed specific practices and meeting structures which help people to ensure that their constructive intentions are experienced positively by one another. The use of the creative behaviour tools extends the application of Synectics to many situations beyond invention sessions (particularly constructive resolution of conflict).\n\nGordon emphasized the importance of metaphorical process' to make the familiar strange and the strange familiar\". He expressed his central principle as: \"Trust things that are alien, and alienate things that are trusted.\" This encourages, on the one hand, fundamental problem-analysis and, on the other hand, the alienation of the original problem through the creation of analogies. It is thus possible for new and surprising solutions to emerge.\n\nAs an invention tool, Synectics invented a technique called \"springboarding\" for getting creative beginning ideas. For the development of beginning ideas, the method incorporates brainstorming and deepens and widens it with metaphor; it also adds an important evaluation process for Idea Development, which takes embryonic new ideas that are attractive but not yet feasible and builds them into new courses of action which have the commitment of the people who will implement them.\n\nSynectics is more demanding of the subject than brainstorming, as the steps involved imply that the process is more complicated and requires more time and effort. The success of the Synectics methodology depends highly on the skill of a trained facilitator.\n\n\n"}
{"id": "21736936", "url": "https://en.wikipedia.org/wiki?curid=21736936", "title": "The Schizophrenia of Modern Ethical Theories", "text": "The Schizophrenia of Modern Ethical Theories\n\n\"The Schizophrenia of Modern Ethical Theories\" is a 1976 paper in ethics by Michael Stocker. Published in \"The Journal of Philosophy\", the central claim of the paper is that some modern ethical theories fail to account for motive in their theories, producing a sort of schizophrenia because the agent is unable to use his reasons or motives as a basis for his actions. According to Stocker, motive is important to ethics and should be considered as well, rather than only \"duty, rightness and obligation\" which he believes are the main focuses of current theories. Stocker believes that this focus is not compatible with the motives required for goods such as love and friendship.\n\nStocker uses the example of a friend visiting you in the hospital. It is nice at first, however he reveals that he chose to spend time with you not out of concern for you in particular, but because he felt it was his moral duty. In this case, we feel that there is something missing in this action—we would much prefer to be visited by someone who cares about us directly, not just his duty.\n\n"}
{"id": "639072", "url": "https://en.wikipedia.org/wiki?curid=639072", "title": "Theatrical property", "text": "Theatrical property\n\nA prop, formally known as (theatrical) property, is an object used on stage or on screen by actors during a performance or screen production. In practical terms, a prop is considered to be anything movable or portable on a stage or a set, distinct from the actors, scenery, costumes, and electrical equipment. Consumable food items appearing in the production are also considered props.\n\nThe earliest known use of the term \"properties\" in English to refer to stage accessories is in the 1425 CE morality play, \"The Castle of Perseverance\". The \"Oxford English Dictionary\" finds the first usage of \"props\" in 1841, while the singular form of \"prop\" appeared in 1911.\nDuring the Renaissance in Europe, small acting troupes functioned as cooperatives, pooling resources and dividing any income. Many performers provided their own costumes, but special items—stage weapons, furniture or other hand-held devices—were considered \"company property\"; hence the term \"property.\" Some experts however seem to think that the term comes from the idea that stage or screen objects \"belong\" to whoever uses them on stage.\n\nThere is no difference between props in different media, such as theatre, film, or television. Bland Wade, a properties director, says, \"A coffee cup onstage is a coffee cup on television, is a coffee cup on the big screen.\" He adds, \"There are definitely different responsibilities and different vocabulary.\"\n\nThe term \"theatrical property\" originated to describe an object used in a stage play and similar entertainments to further the action. Technically, a prop is any object that gives the scenery, actors, or performance space specific period, place, or character. The term comes from live-performance practice, especially theatrical methods, but its modern use extends beyond the traditional plays and musical, circus, novelty, comedy, and even public-speaking performances, to film, television, and electronic media.\n\nProps in a production originate from off stage unless they have been preset on the stage before the production begins. Props are stored on a prop table backstage near the actor's entrance during production then generally locked in a storage area between performances. The person in charge of handling the props is generally called the \"props master\". Other positions also include coordinators, production assistants and interns as may be needed for a specific project.\n\nThe term has readily transferred to television, motion picture and video game production, where they are commonly referred to by the phrase movie prop, film prop or simply prop. In recent years, the increasing popularity of movie memorabilia (a broader term that also includes costumes) has added new meaning to the term \"prop\", broadening its existence to include a valuable after-life as a prized collector's item. Typically not available until after a film's premiere, movie props appearing on-screen are called \"screen-used\", and can fetch thousands of dollars in online auctions and charity benefits.\n\nMany props are ordinary objects. However, a prop must \"read well\" from the house or on-screen, meaning it must look real to the audience. Many real objects are poorly adapted to the task of looking like themselves to an audience, due to their size, durability, or color under bright lights, so some props are specially designed to look more like the actual item than the real object would look. In some cases, a prop is designed to behave differently from how the real object would, often for the sake of safety.\n\nExamples of special props are:\n\n"}
{"id": "45413", "url": "https://en.wikipedia.org/wiki?curid=45413", "title": "Tribe", "text": "Tribe\n\nIn anthropology, a tribe is a human social group. Exact definitions of what constitutes a tribe vary among anthropologists, and the term is itself considered controversial in academic circles in part due to its association with colonialism. In general use, the term may refer to people perceived by a population to be primitive and may have negative connotations. The concept is often contrasted with other social groups concepts, such as nations, states, and forms of kinship.\n\nIn some places, such as India and North America, tribes are polities that have been granted legal recognition and limited autonomy by the national or federal government.\n\nThe word tribe first occurs in English in 12th-century Middle English-literature, in reference to the twelve tribes of Israel. The Middle English word is derived from Old French \"tribu\" and, in turn, from Latin \"tribus\" (plural \"tribūs\"), in reference to a supposed tripartite division of the original Roman state along ethnic lines, into \"tribūs\" known as the \"Ramnes\" (or \"Ramnenses\"), \"Tities\" (or \"Titienses\"), and \"Luceres\", corresponding, according to Marcus Terentius Varro, to the Latins, Sabines and Etruscans respectively. The \"Ramnes\" were named after Romulus, leader of the Latins, \"Tities\" after Titus Tatius, leader of the Sabines, and \"Luceres\" after Lucumo, leader of an Etruscan army that had assisted the Latins. In 242–240 BC, the Tribal Assembly (\"comitia tributa\") in the Roman Republic included 35 tribes (four \"urban tribes\" and 31 \"rural tribes\"). According to Livy, the three \"tribes\" were squadrons of cavalry, rather than ethnic divisions.\n\nThe term's ultimate etymology is uncertain, perhaps from the Proto-Indo-European roots \"tri-\" (\"three\") and \"bhew\" (\"to be\"). The classicist Gregory Nagy says, citing the linguist Émile Benveniste, that the Umbrian \"trifu\" (equivalent of the Latin \"tribus\") is apparently derived from a combination of \"*tri-\" and \"*bhu-\", where the second element is cognate with the Greek root \"phúō\" φύω “to bring forth” and the Greek \"phulē\" φυλή \"clan, race, people\" (plural \"phylai\" φυλαί). The Greek \"polis\" (\"state\" or \"city\") was, like the Roman state, divided into three \"phylai\".\n\nIn Europe during the late medieval era, the Bible was written mostly in New Latin and instead of \"tribus\" the word \"phyle\" was used, derived from the Greek \"phulē\". In the historical sense, \"tribe\", \"race\" and \"clan\" have often been used interchangeably.\n\nThe term and concept of a \"tribe\" is controversial among anthropologists and other academics active in the social sciences. The term \"tribe\" was in common use in the field of anthropology until the late 1950s and 1960s, during which time scholars began to reassess its utility. In 1970, anthropologist J. Clyde Mitchell writes:\nWriting in 2013, scholar Matthew Ortoleva notes that \"like the word \"Indian\", \"[t]ribe\" is a word that has connotations of colonialism.\"\n\nConsiderable debate has accompanied efforts to define and characterize tribes. When scholars use the term, they may perceive differences between pre-state tribes and contemporary tribes; there is also general controversy over cultural evolution and colonialism. In the popular imagination, tribes reflect a way of life that predates, and is more natural than that in modern states. Tribes also privilege primordial social ties and are clearly bounded, homogeneous, parochial, and stable. Tribes are an organization among families (including clans and lineages), which generates a social and ideological basis for solidarity that is in some way more limited than that of an \"ethnic group\" or of a \"nation\". Anthropological and ethnohistorical research has challenged all of these notions.\n\nAnthropologist Elman Service presented a system of classification for societies in all human cultures, based on the evolution of social inequality and the role of the state. This system of classification contains four categories:\n\n\nIn his 1975 study, \"The Notion of the Tribe\", anthropologist Morton H. Fried provided numerous examples of tribes that encompassed members who spoke different languages and practiced different rituals, or who shared languages and rituals with members of other tribes. Similarly, he provided examples of tribes in which people followed different political leaders, or followed the same leaders as members of other tribes. He concluded that tribes in general are characterized by fluid boundaries and heterogeneity, are not parochial, and are dynamic.\nFried proposed that most contemporary tribes do not have their origin in pre-state tribes, but rather in pre-state bands. Such \"secondary\" tribes, he suggested, developed as modern products of state expansion. Bands comprise small, mobile, and fluid social formations with weak leadership. They do not generate surpluses, pay no taxes, and support no standing army. Fried argued that secondary tribes develop in one of two ways. First, states could set them up as means to extend administrative and economic influence in their hinterland, where direct political control costs too much. States would encourage (or require) people on their frontiers to form more clearly bounded and centralized polities, because such polities could begin producing surpluses and taxes, and would have a leadership responsive to the needs of neighboring states (the so-called \"scheduled\" tribes of the United States or of British India provide good examples of this). Second, bands could form \"secondary\" tribes as a means to defend against state expansion. Members of bands would form more clearly bounded and centralized polities, because such polities could begin producing surpluses that could support a standing army that could fight against states, and they would have a leadership that could co-ordinate economic production and military activities.\n\nArchaeologists continue to explore the development of pre-state tribes. Current research suggests that tribal structures constituted one type of adaptation to situations providing plentiful yet unpredictable resources. Such structures proved flexible enough to coordinate production and distribution of food in times of scarcity, without limiting or constraining people during times of surplus.\n\n"}
{"id": "53654436", "url": "https://en.wikipedia.org/wiki?curid=53654436", "title": "UN Declaration on the Elimination of All Forms of Intolerance and of Discrimination Based on Religion or Belief", "text": "UN Declaration on the Elimination of All Forms of Intolerance and of Discrimination Based on Religion or Belief\n\nThe UN Declaration on the Elimination of All Forms of Intolerance and of Discrimination Based on Religion or Belief is a United Nations resolution, passed on November 25 1981. It outlined human rights regarding the freedom of religion.\n"}
{"id": "1796418", "url": "https://en.wikipedia.org/wiki?curid=1796418", "title": "Visibility", "text": "Visibility\n\nIn meteorology, visibility is a measure of the distance at which an object or light can be clearly discerned. It is reported within surface weather observations and METAR code either in meters or statute miles, depending upon the country. Visibility affects all forms of traffic: roads, sailing and aviation. Meteorological visibility refers to transparency of air: in dark, meteorological visibility is still the same as in daylight for the same air.\n\nICAO Annex 3 \"Meteorological Service for International Air Navigation\" contains the following definitions and note:\nAnnex 3 also defines Runway Visual Range (RVR) as:\n\nIn extremely clean air in Arctic or mountainous areas, the visibility can be up to 160 km (100 miles) where there are large markers such as mountains or high ridges. However, visibility is often reduced somewhat by air pollution and high humidity. Various weather stations report this as haze (dry) or mist (moist). Fog and smoke can reduce visibility to near zero, making driving extremely dangerous. The same can happen in a sandstorm in and near desert areas, or with forest fires. Heavy rain (such as from a thunderstorm) not only causes low visibility, but the inability to brake quickly due to hydroplaning. Blizzards and ground blizzards (blowing snow) are also defined in part by low visibility.\n\nTo define visibility the case of a perfectly black object being viewed against a perfectly white background is examined. The visual contrast, \"C\"(x), at a distance \"x\" from the black object is defined as the relative difference between the light intensity of the background and the object\n\nformula_1\n\nwhere \"F\"(x) and \"F\"(x) are the intensities of the background and the object, respectively. Because the object is assumed to be perfectly black, it must absorb all of the light incident on it. Thus when \"x\"=0 (at the object), \"F\"(0) = 0 and \"C\"(0) = 1.\n\nBetween the object and the observer, \"F\"(x) is affected by additional light that is scattered into the observer's line of sight and the absorption of light by gases and particles. Light scattered by particles outside of a particular beam may ultimately contribute to the irradiance at the target, a phenomenon known as multiple scattering. Unlike absorbed light, scattered light is not lost from a system. Rather, it can change directions and contribute to other directions. It is only lost from the original beam traveling in one particular direction. The multiple scattering's contribution to the irradiance at \"x\" is modified by the individual particle scattering coefficient, the number concentration of particles, and the depth of the beam. The intensity change \"dF\" is the result of these effects over a distance \"dx\". Because \"dx\" is a measure of the amount of suspended gases and particles, the fraction of \"F\" that is diminished is assumed to be proportional to the distance, \"dx\". The fractional reduction in \"F\" is\n\nformula_2\n\nwhere \"b\" is the attenuation coefficient. The scattering of background light into the observer's line of sight can increase \"F\" over the distance \"dx\". This increase is defined as \"b' F\"(\"x\") \"dx\", where \"b\"' is a constant. The overall change in intensity is expressed as\n\nformula_3\n\nSince \"F\" represents the background intensity, it is independent of \"x\" by definition. Therefore,\n\nformula_4\n\nIt is clear from this expression that \"b\"' must be equal to \"b\". Thus, the visual contrast, \"C\"(\"x\"), obeys the Beer–Lambert law\n\nformula_5\n\nwhich means that the contrast decreases exponentially with the distance from the object:\n\nformula_6\n\nLab experiments have determined that contrast ratios between 0.018 and 0.03 are perceptible under typical daylight viewing conditions. Usually, a contrast ratio of 2% (\"C\" = 0.02) is used to calculate visual range. Plugging this value into the above equation and solving for \"x\" produces the following visual range expression (the Koschmieder equation):\n\nformula_7\n\nwith \"x\" in units of length. At sea level, the Rayleigh atmosphere has an extinction coefficient of approximately 13.2 × 10 m at a wavelength of 520 nm. This means that in the cleanest possible atmosphere, visibility is limited to about 296 km.\n\nVisibility perception depends on several physical and visual factors. A realistic definition should consider the fact that the human visual system (HVS) is highly sensitive to spatial frequencies, and then to use the Fourier transform and the contrast sensitivity\nfunction of the HVS to assess visibility.\n\nThe international definition of fog is a visibility of less than ; mist is a visibility of between and and haze from to . Fog and mist are generally assumed to be composed principally of water droplets, haze and smoke can be of smaller particle size; this has implications for sensors such as Thermal Imagers (TI/FLIR) operating in the far-IR at wavelengths of about 10 μm which are better able to penetrate haze and some smokes because their particle size is smaller than the wavelength; the IR radiation is therefore not significantly deflected or absorbed by the particles. \n\nWith fog, occasional freezing drizzle and snow can occur. This usually occurs when temperatures are below 0° Celsius (32° Fahrenheit). Hazards can include slipping and falling on ice or slipping in your car on ice causing a potential lethal crash. Low visibility below 1000 yards is usually accompanied with this. Low lying stratus clouds are the culprit of most of these cold weather events.\n\nVisibility of less than is usually reported as zero. In these conditions, roads may be closed, or automatic warning lights and signs may be activated to warn drivers. These have been put in place in certain areas that are subject to repeatedly low visibility, particularly after traffic collisions or pile-ups involving multiple vehicles.\n\nIn addition, an advisory is often issued by a government weather agency for low visibility, such as a dense fog advisory from the U.S. National Weather Service. These generally advise motorists to avoid travel until the fog dissipates or other conditions improve. Airport travel is also often delayed by low visibility, sometimes causing long waits due to approach visibility minimums and the difficulty of safely moving aircraft on the ground in low visibility.\n\nA visibility reduction is probably the most apparent symptom of air pollution. Visibility degradation is caused by the absorption and scattering of light by particles and gases in the atmosphere. Absorption of electromagnetic radiation by gases and particles is sometimes the cause of discolorations in the atmosphere but usually does not contribute very significantly to visibility degradation.\n\nScattering by particulates impairs visibility much more readily. Visibility is reduced by significant scattering from particles between an observer and a distant object. The particles scatter light from the sun and the rest of the sky through the line of sight of the observer, thereby decreasing the contrast between the object and the background sky. Particles that are the most effective at reducing visibility (per unit aerosol mass) have diameters in the range of 0.1-1.0 µm. The effect of air molecules on visibility is minor for short visual ranges but must be taken into account for ranges above 30 km.\n\n"}
