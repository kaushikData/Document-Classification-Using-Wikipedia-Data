{"id": "56364466", "url": "https://en.wikipedia.org/wiki?curid=56364466", "title": "Allard Prize for International Integrity", "text": "Allard Prize for International Integrity\n\nThe Allard Prize for International Integrity is one of the world's largest prizes dedicated to the fight against corruption and the protection of human rights. The prize is awarded biennially to an individual, movement or organization that has \"demonstrated exceptional courage and leadership in combating corruption, especially through promoting transparency, accountability and the Rule of Law.\" The winner receives the Allard Prize Award, a uniquely crafted brass artwork, and CAD$100,000. Honourable mention recipients are awarded a unique nickel-plated artwork, and may also receive a cash award.\n\nAt the 2017 Award Ceremony, Pulitzer Prize-winning journalist Glenn Greenwald stated that the Allard Prize “is important … and isn't devoted just to honouring anti-corruption crusaders, but to constructing and fortifying a framework that really does protect them and enables the work to proceed much more safely,\" and \"this kind of courage can be very contagious.\" Past Allard Prize winners and others have said that the Allard Prize assists, supports and protects those working in anti-corruption and their work. Canada's National Observer calls The Allard Prize 'The Oscars of anti-corruption'.\n\nThe Allard Prize and Allard Prize Foundation, the charitable organization that supports the Allard Prize, were founded in 2011 and are funded by lawyer and businessman Peter A. Allard, Q.C.. The Allard Prize Committee is responsible for the oversight, organization and selection of the Prize winner and honourable mention recipients. The Prize is administered at the Peter A. Allard School of Law at the University of British Columbia, Vancouver, Canada.\n\nSome Allard Prize nominees and winners have previously been subjected to threats, violence, torture, imprisonment and other attacks associated with their anti-corruption and human rights activities. One honourable mention recipient, Russian lawyer Sergei Magnitsky, was nominated posthumously after being tortured to death in prison.\n\nThe Allard Prize winner and any honourable mention recipients are chosen through a comprehensive nomination and selection process involving the Allard Prize Committee, sub-committees and an Advisory Board. For the 2015 Allard Prize, over 140 people were nominated from 50 countries. In 2017, the number of nominations grew to 244 from 70 countries, with 42 nominations (17 percent) from North America.\n\nThere are no restrictions as to who may submit a nominee for consideration. Nominations may come from anywhere including from members of the Allard Prize Committee. Self-nominations are allowed for both individuals and organizations. Nominees range from neighbourhood activists to prominent world leaders, academics, social movements, political organizations and charitable groups.\n\nNominations are accepted on a rolling basis, with a new nomination cycle beginning with the announcement of finalists in the Prize year. Previously nominated candidates can be re-nominated, but new nomination forms must be submitted. There are no minimum or maximum age limits or any other limiting characteristics as to eligibility, with the exception that members of the Allard Prize Committee, Advisory Board and members of their immediate families may not be awarded the Allard Prize during their respective terms and for one year following the end of their terms.\n\nThe Allard Prize Committee reviews all nominations and is responsible for selecting the Prize winner and any honourable mention recipients. The selection process involves research, subcommittees and due diligence, two levels of short-listing, and a requirement to submit short-listed of nominees to the Allard Prize Advisory Board for review and comment prior to the selection of the winner and honourable mention recipients.\n\nThe 2015 and 2017 Allard Prize public awards ceremonies were held at the University of British Columbia's Old Auditorium and were live-streamed over the Internet.\n\nThe list of Allard Prize 'finalists' is released approximately one month prior to the Prize Ceremony. The public announcement identifying the Allard Prize winner is made at the Prize Ceremony, typically held in September or October of the award year. Finalists attend the ceremony in Vancouver, British Columbia, Canada, and become part of the growing Allard Prize anti-corruption network. Travel expenses are paid by the Allard Prize. Past award ceremony keynote speakers include Canadian diplomat Stephen Lewis (2013), Lieutenant-General Roméo Dallaire (2015) and Pulitzer Prize winning journalist Glenn Greenwald (2017).\n\nAs of 2017, the Allard Prize has been awarded to four individuals. Two organizations, one government office and three individuals (one posthumously) have received Honourable Mention.\n\nThe selection of Brazil's Car Wash Task Force (Operação Lava Jato) as a 2017 Allard Prize finalist triggered objections in Brazil and Canada. A group of Brazilian lawyers wrote to the Allard Prize Committee demanding the Committee rescind their finalist status and alleging \"numerous abuses, arbitrariness and legal violations\" by the Task Force. A group of University of British Columbia faculty and students supported the Brazilian lawyer group. During the September 28, 2017 Prize Ceremony, several audience members made vocal outbursts against the Car Wash Task Force.\n\nIn response, 2017 Allard Prize keynote speaker Glenn Greenwald said in his keynote address that while the Operation Car Wash Task Force had made some mistakes, powerful investigative work often creates controversy and enemies. In a post-award open discussion at the Allard School of Law, opponents confronted the Task Force's lead prosecutor Deltan Dallagnol with their criticisms, who responded during a recorded video session.\n\nShortly after the 2017 Allard Prize finalists were announced, journalist Russell Mokhiber noted in an article published in Corporate Crime Reporter that the Allard Prize has \"a bias in favour of anti-corruption fighters in the Third World\" rather than selecting nominees from Western developed countries. Mokhiber speculated that the bias might be due to a heavy representation of corporate lawyers, World Bank and Asian Development Bank personnel on the selection and advisory committees. In an interview with Mokhiber, Allard Prize Executive Director Nicole Barrett acknowledged that none of the previous finalists have been from North America. Noting that the majority of nominees are from the developing world, she said that the Allard Prize may consider developing a formal requirement of geographical balance to address this concern.\n\n"}
{"id": "43262117", "url": "https://en.wikipedia.org/wiki?curid=43262117", "title": "Aparoksha", "text": "Aparoksha\n\nAparoksha (Sanskrit: अपरोक्ष), a Sanskrit adjective meaning not invisible or perceptible, refers to direct intuitive knowledge which is one of the seven stages of knowledge or conditions of Chidabhasa, the first three being the sources of bondage and the rest four being the processes of liberation; and to the continuation of the deepening of conventional knowledge. It removes sorrows. According to Indian Philosophy, the three traditional kinds of knowledge are – \"pratyaksha\" (empirical), \"paroksha\" (conventional, universal) and \"aparoksha\" (transcendental). \"Aparoksha\" is the highest kind of knowledge which cannot be gained without the practice of morality that converts \"paroksha\" knowledge from which unity of existence is derived. This knowledge is gained by establishing a \"guru-shishya sambandha\" (Guru-shishya tradition) with a teacher who has already experienced that kind of knowledge (Aparoksanubhuti); the karma or acts required to be done, after gain of \"Aparoksha jnana\" is \"Vidya-karma\" which consists in \"sravana\" (hearing of srutis), \"manana\" (reflection) and \"nididhyasana\" (meditation on Brahman). \n\nAparoksha is \"savikalpa jnana\" (knowledge) when one re-recognizes the non-dual nature of the ever-realized Self (Tat Tvam Asi), it is the immediate knowledge gained through the \"pramanas\"; practice of \"Dhyana\" (meditation) removes all \"vikalpas\" (varied thoughts) and leads to \"nirvikalpa\" or the thoughtless state, which is the highest experience, the immediate realization of Truth. It is the method of cessation from individual and collective perception leading to the position of neutrality. \"Sarvam khalvidam Brahman\" (all this is Brahman) is \"paroksha\" knowledge, but the understanding that \"Aham Brahman Asmi\" (I am Brahman) is \"aparoksha\" knowledge.\n"}
{"id": "1781075", "url": "https://en.wikipedia.org/wiki?curid=1781075", "title": "Applied behavior analysis", "text": "Applied behavior analysis\n\nApplied behavior analysis (ABA) is a scientific discipline concerned with applying techniques based upon the principles of learning to change behavior of social significance. It is the applied form of behavior analysis; the other two forms are radical behaviorism (or the philosophy of the science) and the experimental analysis of behavior (or basic experimental research).\n\nThe name \"applied behavior analysis\" has replaced behavior modification because the latter approach suggested attempting to change behavior without clarifying the relevant behavior-environment interactions. In contrast, ABA tries to change behavior by first assessing the functional relationship between a targeted behavior and the environment. Further, the approach often seeks to develop socially acceptable alternatives for aberrant behaviors.\n\nABA has been brought to bear on a wide range of areas and behavioral problems. Examples include such things as early intensive behavioral interventions for children with an autism spectrum disorder (ASD), research on the principles influencing criminal behavior, as well as HIV prevention, conservation of natural resources, education, gerontology, health and exercise, industrial safety, language acquisition, littering, medical procedures, parenting, psychotherapy, seatbelt use, severe mental disorders, sports, substance abuse, phobias, pediatric feeding disorders, and zoo management and care of animals.\n\nABA is an applied science devoted to developing procedures which will produce observable changes in behavior. It is to be distinguished from the experimental analysis of behavior, which focuses on basic experimental research, but it uses principles developed by such research, in particular operant conditioning and classical conditioning. Behavior analysis adopts the viewpoint of radical behaviorism, treating thoughts, emotions, and other covert activity as behavior that is subject to the same rules as overt responses. This represents a shift away from methodological behaviorism, which restricts behavior-change procedures to behaviors that are overt, and was the conceptual underpinning of behavior modification.\n\nBehavior analysts also emphasize that the science of behavior must be a \"natural\" science as opposed to a \"social\" science. As such, behavior analysts focus on the observable relationship of behavior with the environment, including antecedents and consequences, without resort to \"hypothetical constructs\".\n\nThe beginnings of ABA can be traced back to the research of Teodoro Ayllon and Jack Michael with their study \"The psychiatric nurse as a behavioral engineer\" (1959) that they submitted to the \"Journal of the Experimental Analysis of Behavior\" (JEAB) as part of their doctoral dissertation at the University of Houston. The researchers were training the staff and nurses at a psychiatric hospital how to use a token economy based on the principles of operant conditioning and behavioral engineering—a synonym for ABA,\nalso then called behavior modification—with their patients, who were mostly adults with schizophrenia, but some were also mentally retarded children. This paper later led to the founding of the \"Journal of Applied Behavior Analysis\" (JABA), which publishes research in the application of behavior analysis to a wide array of socially relevant behavior, such as autism, pediatric feeding therapy, substance-use disorders, and classroom instruction with typically developing students.\n\nA group of faculty and researchers at the University of Washington, including Donald Baer, Sidney W. Bijou, Bill Hopkins, Jay Birnbrauer, Todd Risley, and Montrose Wolf, applied the principles of behavior analysis to instruct developmentally disabled children, provide behavior management strategies in juvenile detention centers, and organize employees who required proper structure and management in businesses, among other situations. In 1968, Baer, Bijou, Risley, Birnbrauer, Wolf, and James Sherman joined the Department of Human Development and Family Life at the University of Kansas, where they founded the \"Journal of Applied Behavior Analysis\".\n\nNotable graduate students from the University of Washington include Robert Wahler, James Sherman, and Ivar Lovaas. Lovaas established the UCLA Young Autism Project while teaching at the University of California, Los Angeles, and devoted nearly half a century to groundbreaking research and practice aimed at improving the lives of children with autism and their families. In 1965, Lovaas published a series of articles that outlined his system for coding observed behaviors, described a pioneering investigation of the antecedents and consequences that maintained a problem behavior, and relied upon the methods of errorless learning that was initially devised by Charles Ferster to teach nonverbal children to speak. Lovaas also described how to use social (secondary) reinforcers, teach children to imitate, and what interventions (including electric shocks) may be used to reduce aggression and life-threatening self-injury. \n\nIn 1987, Lovaas published the landmark study, \"Behavioral Treatment and Normal Educational and Intellectual Functioning in Young Autistic Children\". The experimental group in this study received up to 40-hours per week in a 1:1 teaching setting using errorless discrete trial training (DTT). The treatment is done at home with parents involved in every aspect of treatment, and the curriculum is highly individualized with a heavy emphasis on teaching eye contact and language. ABA principles are used to motivate learning and reduce non-desired behaviors. The outcome of this study indicated 47% of the experimental group (9/19) went on to lose their autism diagnosis and were described as indistinguishable from their typical adolescent peers. This included passing regular education without assistance, making and maintaining friends, and becoming self-sufficient as adults. These gains were maintained as reported in the 1993 study, \"Long-Term Outcome for Children With Autism Who Received Early Intensive Behavioral Treatment\". Lovaas’ work went on to be recognized by the US Surgeon General in 1999, and his research outcomes were replicated in university and private settings. The \"Lovaas Method\" went on to become known as early intensive behavioral intervention (EIBI), or DTT for 30 to 40 hours per week. \n\nIn addition to being a major contributor of ABA, Lovaas taught now prominent behaviorists, such as Robert Koegel, Laura Schreibman, Tristram Smith, John McEachin, Ron Leaf, Doreen Granpeesheh, Jacquie Wynn, and over 20,000 UCLA students who took his \"Behavior Modification\" course during his 50 years of teaching. He even cofounded what is today the Autism Society of America (ASA), published hundreds of research articles and books, received state and national awards, and forced school districts to adopt evidenced based teaching programs. His work influenced how autism was treated, and improved the lives of parents and children stricken with the autism diagnosis worldwide.\n\nOver the years, \"behavior analysis\" gradually superseded \"behavior modification\"; that is, from simply trying to alter problematic behavior, behavior analysts sought to understand the function of that behavior, what antecedents promote and maintain it, and how it can be replaced by successful behavior. This analysis is based on careful initial assessment of a behavior's function and a testing of methods that produce changes in behavior.\n\nWhile ABA seems to be intrinsically linked to autism intervention, it is also used for applied animal behavior and clinical behavior analysis, in behavioral medicine, behavioral neuroscience, and criminology, to increase job safety and performance, counteract phobias, and improve classroom management.\n\nBaer, Wolf, and Risley's 1968 article is still used as the standard description of ABA. It lists the following the seven characteristics of ABA.\n\nIn 2005, Heward et al. suggested that the following five characteristics should be added:\n\nBehavior refers to the movement of some part of an organism that changes some aspect of the environment. Often, the term behavior refers to a class of responses that share physical dimensions or functions, and in that case a \"response\" is a single instance of that behavior. If a group of responses have the same function, this group may be called a response class. \"Repertoire\" refers to the various responses available to an individual; the term may refer to responses that are relevant to a particular situation, or it may refer to everything a person can do.\n\nOperant behavior is the so-called \"voluntary\" behavior that is sensitive to, or controlled by its consequences. Specifically, operant conditioning refers to the three-term contingency that uses stimulus control, in particular an antecedent contingency called the discriminative stimulus (SD) that influences the strengthening or weakening of behavior through such consequences as reinforcement or punishment. The term is used quite generally, from reaching for a candy bar, to turning up the heat to escape an aversive chill, to studying for an exam to get good grades.\n\nRespondent (classical) conditioning is based on innate stimulus-response relationships called reflexes. In his famous experiments with dogs, Pavlov usually used the salivary reflex, namely salivation (unconditioned response) following the taste of food (unconditioned stimulus). Pairing a neutral stimulus, for example a bell (conditioned stimulus) with food caused the bell to elicit salivation (conditioned response). Thus, in classical conditioning, the conditioned stimulus becomes a signal for a biologically significant consequence. Note that in respondent conditioning, unlike operant conditioning, the response does not \"produce\" a reinforcer or punisher (e.g. the dog does not get food \"because\" it salivates).\n\nThe environment is the entire constellation of stimuli in which an organism exists. This includes events both inside and outside of an organism, but only real physical events are included. A stimulus is an \"energy change that affects an organism through its receptor cells\".\n\nA stimulus can be described:\n\nReinforcement is the key element in operant conditioning and in most behavior change programs. It is the process by which behavior is strengthened. If a behavior is followed closely in time by a stimulus and this results in an increase in the future frequency of that behavior, then the stimulus is a positive reinforcer. If the removal of an event serves as a reinforcer, this is termed negative reinforcement. There are multiple schedules of reinforcement that affect the future probability of behavior.\n\nPunishment is a process by which a consequence immediately follows a behavior which decreases the future frequency of that behavior. As with reinforcement, a stimulus can be added (positive punishment) or removed (negative punishment). Broadly, there are three types of punishment: presentation of aversive stimuli (e.g., pain), response cost (removal of desirable stimuli as in monetary fines), and restriction of freedom (as in a 'time out'). Punishment in practice can often result in unwanted side effects. Some other potential unwanted effects include resentment over being punished, attempts to escape the punishment, expression of pain and negative emotions associated with it, and recognition by the punished individual between the punishment and the person delivering it.\n\nExtinction is the technical term to describe the procedure of withholding/discontinuing reinforcement of a previously reinforced behavior, resulting in the decrease of that behavior. The behavior is then set to be extinguished (Cooper et al.). Extinction procedures are often preferred over punishment procedures, as many punishment procedures are deemed unethical and in many states prohibited. Nonetheless, extinction procedures must be implemented with utmost care by professionals, as they are generally associated with extinction bursts. An extinction burst is the temporary increase in the frequency, intensity, and/or duration of the behavior targeted for extinction. Other characteristics of an extinction burst include an extinction-produced aggression—the occurrence of an emotional response to an extinction procedure often manifested as aggression; and b) extinction-induced response variability—the occurrence of novel behaviors that did not typically occur prior to the extinction procedure. These novel behaviors are a core component of shaping procedures.\n\nIn addition to a relation being made between behavior and its consequences, operant conditioning also establishes relations between antecedent conditions and behaviors. This differs from the S–R formulations (If-A-then-B), and replaces it with an AB-because-of-C formulation. In other words, the relation between a behavior (B) and its context (A) is because of consequences (C), more specifically, this relationship between AB because of C indicates that the relationship is established by prior consequences that have occurred in similar contexts. This antecedent–behavior–consequence contingency is termed the three-term contingency. A behavior which occurs more frequently in the presence of an antecedent condition than in its absence is called a discriminated operant. The antecedent stimulus is called a discriminative stimulus (S). The fact that the discriminated operant occurs only in the presence of the discriminative stimulus is an illustration of stimulus control. More recently behavior analysts have been focusing on conditions that occur prior to the circumstances for the current behavior of concern that increased the likelihood of the behavior occurring or not occurring. These conditions have been referred to variously as \"Setting Event\", \"Establishing Operations\", and \"Motivating Operations\" by various researchers in their publications.\n\nB.F. Skinner's classification system of behavior analysis has been applied to treatment of a host of communication disorders. Skinner's system includes:\n\nFor assessment of verbal behavior from Skinner's system see Assessment of Basic Language and Learning Skills.\n\nWhen measuring behavior, there are both dimensions of behavior and quantifiable measures of behavior. In applied behavior analysis, the quantifiable measures are a derivative of the dimensions. These dimensions are repeatability, temporal extent, and temporal locus.\n\nResponse classes occur repeatedly throughout time—i.e., how many times the behavior occurs.\n\nThis dimension indicates that each instance of behavior occupies some amount of time—i.e., how long the behavior occurs.\n\nEach instance of behavior occurs at a specific point in time—i.e., when the behavior occurs.\n\nDerivative measures are unrelated to specific dimensions:\n\nIn applied behavior analysis, all experiments should include the following:\n\nPrior to the seminal article on functional analytic methodology for aberrant behaviors, behaviorists used the behavioral technology available to them at the time. Instead of treating the function of the disruptive behavior, behavioral psychologists would instead \"pre-assume\" consequences to alter disruptive behaviors. For example, in the past to decrease self-injurious behavior in an individual, behaviorists may have delivered an aversive stimulus contingent on the response, or assume a reinforcer without identifying the reinforcer that would be most motivating to the client (Iwata, 1988). This type of intervention was successful to the individual, but it was not uncommon to see other variations of aberrant behavior begin to appear. When applied behavior analysts let clients choose from a wide array of reinforcers (often determined through data collection and reinforcement assessments) in the mid-1980s, reinforcement was shown to be more effective than punishment contingencies. In general, applied behavior analysis as a field favors reinforcement based interventions over aversive contingencies, but at the time the behavioral technology was not advanced enough and the individuals needing intervention had a right to an effective treatment (Van Houten et al., 1988). Nevertheless, not all behavioral therapies involved the use of aversives prior to the mid-1980s. Some behaviorists (for instance, B.F. Skinner) always preferred reinforcement and extinction contingencies over punishment even during that time.\n\nIn 1977, Edward Carr published a paper on potential hypotheses for the occurrence and maintenance of self-injurious behaviors. This paper laid out the initial groundwork for a functional analysis of aberrant behaviors. In the paper, Carr described five potential causes for self-injurious behaviors that included (1) positive social reinforcement contingent on the response, (2) negative reinforcement in the form of removal of an aversive stimulus contingent on the response, (3) the response produced stimuli possessed reinforcing qualities (automatic reinforcement), (4) the behavior was a byproduct of an underlying psychological condition, and (5) psychodynamic hypothesis in which the behavior was an attempt to reduce guilt. Throughout the paper, Carr cited recent research to support the first three hypotheses, and disprove the latter two hypotheses, but no formal experiment was conducted to determine the controlling variables of the problem behavior.\n\nIn 1982, Iwata and colleagues conducted the first experimental analysis of the maintaining variables for self-injurious behavior. In the paper, the researchers alternated between specific conditions to examine whether or not the behavior occurred under specific environmental conditions. Through direct manipulation of the environment, the researchers could accurately identify the controlling variables of the aberrant behavior, and provide interventions that targeted the functional relationship between the behavior and the environment. Since this seminal article was published, a wide range of research has been published in the area of functional analyses of aberrant behaviors. The methodology has since become the gold standard in assessment and treatment of aberrant behaviors.\n\nFunctional assessment of behavior provides hypotheses about the relationships between specific environmental events and behaviors. Decades of research have established that both desirable and undesirable behaviors are learned and maintained through interactions with the social and physical environment. Functional behavior assessments are used to identify controlling variables for challenging behaviors as the basis for intervention efforts designed to decrease the occurrence of these behaviors.\n\nBehavior serves two major functions for an individual: (1) to obtain desired events, or (2) to escape/avoid undesired events. Put another way, individuals engage in behavior to get something or to get out of something. When trying to identify the function of a behavior, it is often helpful to think, \"What purpose is this behavior serving the individual?\" Described below are the common functions of behavior.\n\n\"Access to attention (positive reinforcement: social)\":\nThe individual engages in the behavior to obtain attention from another person. For example, a child throws a toy because it characteristically results in mom's attention. (If this behavior results in mom looking at child and giving him lots of attention—even if she's saying \"NO\"—he will be more likely to engage in the same behavior in the future to get mom's attention.) Common forms of attention include, but are not limited to, hugs, kisses, reprimands, frowns, smiles, etc.\n\n\"Access to tangibles (positive reinforcement: tangible/activity)\":\nThe individual engages in the behavior to obtain a specific item or engage in a specific activity from another person. For example, a child hits mom because s/he wants the toy mom is holding. (If this behavior results in mom giving the child the toy, s/he will be more likely to engage in the same behavior in the future to get mom's attention.) Common forms of tangible items include, but are not limited to, food, toys, movies, video games, etc.\n\n\"Automatic positive reinforcement\":\nThe individual engages in the behavior because the response-produced stimulation possesses reinforcing characteristics. In other words, engaging in the behavior produces reinforcing stimulation unique to the specific context. For example, a child hits his/her eyes because it produces the specific stimulation of various colors and effects. Another example includes a child spinning a bowl on a table to produce the specific auditory stimulation unique to that object. Common forms of automatic stimulation include, but are not limited to, auditory stimulation, visual stimulation, endorphin release, etc.\n\n\"Escape/removal of attention\":\nThe individual engages in the behavior to escape aversive socially mediated attention. Put another way, social situations that are aversive to the child are removed contingent on the behavior occurring. For example, a child hits the teacher to avoid talking in front of the class. Common forms of aversive social situations include, but are not limited to, smiles, hugs, frowns, corrections, group settings, etc.\n\n\"Escape/removal of tasks or activities\":\nThe individual engages in the behavior to escape aversive tasks or demands. For example, when a child is told to take a bath he begins to cry, and his mother tells him he no longer has to take a bath. Another example includes a teacher telling a student to complete a set of worksheets, to which the student flips the desk and is sent to the principal's office. Being sent to the principal's office reinforced the behavior of flipping the desk because it allowed the child to escape the aversive activity of completing the worksheets. Common forms of aversive demands/activities include, but are not limited to, difficult tasks, changes in routines, unpredictability, etc.\n\n\"Automatic negative reinforcement\":\nThe individual engages in the behavior because it produces a decrease in aversive stimulation. Put another way, something aversive is occurring in some location on the organism's body, and engaging in the behavior decreases the level of discomfort. For example, a child bangs his head against the wall to decrease the pain experienced from a toothache. Another example includes a child scratching his arm to decrease the level of itchiness experienced from a bug bite. Common forms of aversive stimulation abated by engaging in specific behaviors include sinus pain, itching, hunger, etc.\n\nAs previously stated function refers to the effect the behavior produces on the environment. The actual form of the behavior is referred to the topography. Different behaviors may serve the same function, thus describing one limitation of treating behaviors based on form alone. For example, a child may scream, hit, and cry to obtain attention from his mother. What the behavior looks like often reveals little useful information about the conditions that account for it. However, identifying the conditions that account for a behavior, suggests what conditions need to be altered to change the behavior. Therefore, assessment of function of a behavior can yield useful information with respect to intervention strategies that are likely to be effective.\n\nFBA methods can be classified into three types:\n\nThis method uses structured interviews, checklists, rating scales, or questionnaires to obtain information from persons who are familiar with the person exhibiting the behavior to identify possible conditions or events in the natural environment that correlate with the problem behavior. They are referred to as \"indirect\" because they do not involve direct observation of the behavior, but rather they solicit information based on others' recollections of the behavior. This form of assessment typically yields the least reliable information about the function of behavior, but can provide insight as to possible functions of the behavior to be tested in the future, the form of the behaviors (e.g. screaming, hitting, etc.), and environments in which the behavior typically occurs (e.g. school, home, etc.). This type of assessment should be performed as the initial step of any functional behavior assessment to gather relevant information to complete more direct assessments.\n\nUnlike the indirect methods of FBAs, descriptive functional behavior assessment employs direct observation of behavior. These observations occur in the environment in which the behavior naturally occurs (e.g. school, home, etc.) therefore there is no direct manipulation of the environment.\nThe most common form of descriptive assessment involves recording the antecedents and consequences that naturally occur when the individual emits the behavior. This is referred to as ABC data collection, in which (A) represents the common antecedent, (B) represents the behavior of interest, and (C) represents the immediate consequences that occur following the behavior. ABC data collection is used to identify the naturally occurring consequences delivered in the environment in which the behavior occurs. ABC data collection can be conducted by a wide array of individuals who have received appropriate training on how to record the data.\nAnother form of descriptive FBA is called a scatterplot. In this assessment, staff record the time and setting in which the behavior of interest occurs over a series of days. The data are plotted on a visual scale to indicate whether there are any patterns in the behavior (for example, if the behavior occurs more frequently during math instruction than it does during lunchtime). Although this assessment does not indicate the consequences maintaining the behavior, it can be used to identify some of the antecedent conditions that typically precede the behavior of interest.\n\nA functional analysis is the most direct form of functional behavior assessment, in which specific antecedents and consequences are systematically manipulated to test their separate effects on the behavior of interest. Each manipulation of the antecedent and consequence in a particular situation is referred to a condition. In a functional analysis, conditions are typically alternated between quite rapidly independent of responding to test the different functions of behavior. When data paths are elevated above the control condition (described below) it can be said that there is a functional relation between that condition and the behavior of interest. Complexity, time restraints, and setting restraints, are a few limitations to this particular method. When deciding to use this method, it should be noted that there is a chance of high-risk behavior and the possibility of low-rate behaviors.\n\nBelow, common examples of experimental conditions are described.\nA standard functional analysis normally has four conditions (three test conditions and one control).\n\nIn this condition, the experimenter gives the individual moderately preferred items and instructs them to go play. After that initial instruction, the experimenter pretends to act busy and ignores all bids for attention from the individual. If the individual engages in the behavior of interest, the experimenter provides the individual with attention (commonly in the form of a reprimand). Behaviors that occur more frequently in this condition can be said to be attention maintained.\n\nIn this condition, the experimenter instructs the individual that it is time to work. After the initial instruction, the experimenter delivers a series of demands that the individual is typically required to complete (e.g. math problems, cleaning up, etc.). If the individual engages in the behavior of interest, the demand is removed and the child is allowed to take a break. Behaviors that occur more frequently in this condition can be said to be escape maintained.\n\nIn this condition, the child is left alone with a variety of items to engage with. If the child engages in the behavior of interest, no programmed consequences are delivered. Behaviors that occur more frequently in this condition can be said to be automatically maintained.\n\nIn this condition, the child is allowed to engage with a variety of items during the session. No demands are placed on the child throughout the duration of the session. The experimenter provides attention to the individual throughout the session on any behavior that is not the target behavior. If the target behavior occurs, the experimenter removes attention until the behavior has subsided. This session is meant to act as a control condition, meaning that the environment is enriched for the purpose of the behavior not occurring. Said another way, by meeting environmental needs for all possible functions, the individual is not likely to engage in the behavior of interest. This condition is used as a comparison to the other conditions. Any condition that is elevated to a large degree form the control condition, shows a higher degree experimental control indicating the functional relationship between the specific environmental conditions and the behavior of interest.\n\nFunctional behavior assessments are rarely limited to only one of the methods described above. The most common, and most preferred, method for identifying the function of behavior can be seen as a four-part processes.\n\nTask analysis is a process in which a task is analyzed into its component parts so that those parts can be taught through the use of chaining: forward chaining, backward chaining and total task presentation. Task analysis has been used in organizational behavior management, a behavior analytic approach to changing the behaviors of members of an organization (e.g., factories, offices, or hospitals). Behavioral scripts often emerge from a task analysis. Bergan conducted a task analysis of the behavioral consultation relationship and Thomas Kratochwill developed a training program based on teaching Bergan's skills. A similar approach was used for the development of microskills training for counselors. Ivey would later call this \"behaviorist\" phase a very productive one and the skills-based approach came to dominate counselor training during 1970–90. Task analysis was also used in determining the skills needed to access a career. In education, Englemann (1968) used task analysis as part of the methods to design the Direct Instruction curriculum.\n\nThe skill to be learned is broken down into small units for easy learning. For example, a person learning to brush teeth independently may start with learning to unscrew the toothpaste cap. Once they have learned this, the next step may be squeezing the tube, etc.\n\nFor problem behavior, chains can also be analyzed and the chain can be disrupted to prevent the problem behavior. Some behavior therapies, such as dialectical behavior therapy, make extensive use of behavior chain analysis.\n\nA prompt is a cue that is used to encourage a desired response from an individual. Prompts are often categorized into a prompt hierarchy from most intrusive to least intrusive, although there is some controversy about what is considered most intrusive, those that are physically intrusive or those that are hardest prompt to fade (e.g.,verbal). In order to minimize errors and ensure a high level of success during learning, prompts are given in a most-to-least sequence and faded systematically. During this process, prompts are faded quickly as possible so that the learner does not come to depend on them and eventually behaves appropriately without prompting.\n\nTypes of prompts\nPrompters might use any or all of the following to suggest the desired response:\n\nThis is not an exhaustive list of prompts; the nature, number, and order of prompts are chosen to be the most effective for a particular individual.\n\nThe overall goal is for an individual to eventually not need prompts. As an individual gains mastery of a skill at a particular prompt level, the prompt is faded to a less intrusive prompt. This ensures that the individual does not become overly dependent on a particular prompt when learning a new behavior or skill.\n\nThinning is often confused with fading. Fading refers to a prompt being removed, where thinning refers to an increase in the time or number of responses required between reinforcements. Periodic thinning that produces a 30% decrease in reinforcement has been suggested as an efficient way to thin. Schedule thinning is often an important and neglected issue in contingency management and token economy systems, especially when these are developed by unqualified practitioners (see professional practice of behavior analysis).\n\nGeneralization is the expansion of a student's performance ability beyond the initial conditions set for acquisition of a skill. Generalization can occur across people, places, and materials used for teaching. For example, once a skill is learned in one setting, with a particular instructor, and with specific materials, the skill is taught in more general settings with more variation from the initial acquisition phase. For example, if a student has successfully mastered learning colors at the table, the teacher may take the student around the house or his school and then \"generalize\" the skill in these more natural environments with other materials. Behavior analysts have spent considerable amount of time studying factors that lead to generalization.\n\nShaping involves gradually modifying the existing behavior into the desired behavior. If the student engages with a dog by hitting it, then he or she could have their behavior shaped by reinforcing interactions in which he or she touches the dog more gently. Over many interactions, successful shaping would replace the hitting behavior with patting or other gentler behavior. Shaping is based on a behavior analyst's thorough knowledge of operant conditioning principles and extinction. Recent efforts to teach shaping have used simulated computer tasks.\n\nOne teaching technique found to be effective with some students, particularly children, is the use of video modeling (the use of taped sequences as exemplars of behavior). It can be used by therapists to assist in the acquisition of both verbal and motor responses, in some cases for long chains of behavior.\n\nCritical to behavior analytic interventions is the concept of a systematic behavioral case formulation with a functional behavioral assessment or analysis at the core. This approach should apply a behavior analytic theory of change (see Behavioral change theories). This formulation should include a thorough functional assessment, a skills assessment, a sequential analysis (behavior chain analysis), an ecological assessment, a look at existing evidenced-based behavioral models for the problem behavior (such as Fordyce's model of chronic pain) and then a treatment plan based on how environmental factors influence behavior. Some argue that behavior analytic case formulation can be improved with an assessment of rules and rule-governed behavior. Some of the interventions that result from this type of conceptualization involve training specific communication skills to replace the problem behaviors as well as specific setting, antecedent, behavior, and consequence strategies.\n\nCounterconditioning includes covert and overt conditioning.\n\nABA-based techniques are often used to change behaviors associated with autism, so much so that ABA itself is often mistakenly considered to be synonymous with therapy for autism. ABA for autism may be limited by diagnostic severity and IQ. The most influential and widely cited review of the literature regarding efficacy of treatments for autism is the National Research Council's book \"Educating Children with Autism\" (2001) which concluded that ABA was the best research supported and most effective treatment for the main characteristics of autism. Some critics claimed that the NRC's report was an inside job by behavior analysts but there were no board certified behavior analysts on the panel (which did include physicians, speech pathologists, educators, psychologists, and others). Other criticisms raised include the small sample sizes used in the published research to date. Medications have not been proven to correct the core deficits of ASDs and are not the primary treatment. ABA is the primary treatment according to the American Academy of Pediatrics. Recent reviews of the efficacy of ABA-based techniques in autism include:\n\nA 2009 systematic review and meta-analysis by Spreckley and Boyd of four small-n 2000–2007 studies (involving a total of 76 children) came to different conclusions than the aforementioned reviews. Spreckley and Boyd reported that applied behavior intervention (ABI), another name for EIBI, did not significantly improve outcomes compared with standard care of preschool children with ASD in the areas of cognitive outcome, expressive language, receptive language, and adaptive behavior. In a letter to the editor, however, authors of the four studies meta-analyzed claimed that Spreckley and Boyd had misinterpreted one study comparing two forms of ABI with each other as a comparison of ABI with standard care, which erroneously decreased the observed efficacy of ABI. Furthermore, the four studies' authors raised the possibility that Spreckley and Boyd had excluded some other studies unnecessarily, and that including such studies could have led to a more favorable evaluation of ABI. Spreckley, Boyd, and the four studies' authors did agree that large multi-site randomized trials are needed to improve the understanding of ABA's efficacy in autism. Some initial, theoretical work has been initiated to use applied behavioral analysis (ABA) as a foundation for agent-mediated, AI-based instructions for children with autism spectrum disorder.\n\nA United States District Court Judge ruled withholding ABA from children (0–21) with autism causes irreparable harm, finding Elizabeth Dudek of Florida's Agency for Healthcare Administration's position suggesting ABA was experimental was arbitrary and capricious, ordering AHCA enjoined from withholding ABA. Thereafter, CMS ordered its guidance that all states to cover ABA via Head Start and Early Head Start EPSDT programs.\n\nFurther research is clearly required, specifically to include larger and thus more representative samples.\n\nConversely, various major figures within the autistic community have written biographies detailing harm caused by the provision of ABA. Several of these people have since been diagnosed with PTSD and depression. Less scholarly reviews were provided by Elizabeth Devita-Raeburn in \"The Atlantic\" magazine and by an ex-practitioner.\n\nControversy regarding ABA persists in the autism community. A 2017 study, based on a survey of 'post-traumatic stress symptoms' completed by individuals with presumed, self-reported diagnosis of ASD and their caregivers, found that 46% of the individuals who also reported receiving ABA treatment met the author's \"diagnostic threshold for PTSD\" via their survey score, versus 28% of respondents not reporting receipt of ABA treatment. Individuals reported as receiving ABA were 86% more likely on average (41% for adults, 130% for children) to meet the author's \"diagnostic threshold\". However, the 2017 study was heavily criticized by peer reviewers who noted, among other things, that her methods for recruiting participants were faulty, she did not use appropriate operative definitions for PTSS versus PTSD, and there was detected bias baked into the hypothesis of the study.\n\nApplied behavior analysts publish in many journals. Some examples of \"core\" behavior analytic journals are:\n\n\n\n"}
{"id": "481678", "url": "https://en.wikipedia.org/wiki?curid=481678", "title": "Archimedean point", "text": "Archimedean point\n\nAn Archimedean point (or \"Punctum Archimedis\") is a hypothetical vantage point from which an observer can objectively perceive the subject of inquiry, with a view of totality. The ideal of \"removing oneself\" from the object of study so that one can see it in relation to all other things, but remain independent of them, is described by a view from an Archimedean point. For example, the philosopher John Rawls uses the heuristic device of the original position in an attempt to remove the particular biases of individual agents in an attempt to demonstrate how rational beings might arrive at an objective formulation of justice.\n\nThe expression comes from Archimedes, who supposedly claimed that he could lift the Earth off its foundation if he were given a place to stand, one solid point, and a long enough lever. This is also mentioned in Descartes' second meditation with regard to finding certainty, the 'unmovable point' Archimedes sought.\n\nSceptical and anti-realist philosophers criticise the possibility of an Archimedean point, claiming it is a form of scientism, for example;\n"}
{"id": "21782795", "url": "https://en.wikipedia.org/wiki?curid=21782795", "title": "Bio-energy with carbon capture and storage", "text": "Bio-energy with carbon capture and storage\n\nBio-energy with carbon capture and storage (BECCS) is a potential greenhouse gas mitigation technology which produces negative carbon dioxide emissions by combining bioenergy (energy from biomass) use with geologic carbon capture and storage. The concept of BECCS is drawn from the integration of trees and crops, which extract carbon dioxide (CO) from the atmosphere as they grow, the use of this biomass in processing industries or power plants, and the application of carbon capture and storage via CO injection into geological formations. There are other non-BECCS forms of carbon dioxide removal and storage that include technologies such as biochar, carbon dioxide air capture and biomass burial and enhanced weathering.\n\nAccording to a recent Biorecro report, there is 550 000 tonnes CO/year in total BECCS capacity currently operating, divided between three different facilities (as of January 2012).\n\nIn the IPCC Fourth Assessment Report by the Intergovernmental Panel on Climate Change (IPCC), BECCS was indicated as a key technology for reaching low carbon dioxide atmospheric concentration targets. The negative emissions that can be produced by BECCS has been estimated by the Royal Society to be equivalent to a 50 to 150 ppm decrease in global atmospheric carbon dioxide concentrations and according to the International Energy Agency, the BLUE map climate change mitigation scenario calls for more than 2 gigatonnes of negative CO emissions per year with BECCS in 2050. According to Stanford University, 10 gigatonnes is achievable by this date.\n\nThe Imperial College London, the UK Met Office Hadley Centre for Climate Prediction and Research, the Tyndall Centre for Climate Change Research, the Walker Institute for Climate System Research, and the Grantham Institute for Climate Change issued a joint report on carbon dioxide removal technologies as part of the \"AVOID: Avoiding dangerous climate change\" research program, stating that \"Overall, of the technologies studied in this report, BECCS has the greatest maturity and there are no major practical barriers to its introduction into today’s energy system. The presence of a primary product will support early deployment.\"\n\nAccording to the OECD, \"Achieving lower concentration targets (450 ppm) depends significantly on the use of BECCS\".\n\nThe main appeal of BECCS is in its ability to result in negative emissions of CO. The capture of carbon dioxide from bioenergy sources effectively removes CO from the atmosphere.\n\nBio-energy is derived from biomass which is a renewable energy source and serves as a carbon sink during its growth. During industrial processes, the biomass combusted or processed re-releases the CO into the atmosphere. The process thus results in a net zero emission of CO, though this may be positively or negatively altered depending on the carbon emissions associated with biomass growth, transport and processing, see below under environmental considerations. Carbon capture and storage (CCS) technology serves to intercept the release of CO into the atmosphere and redirect it into geological storage locations. CO with a biomass origin is not only released from biomass fuelled power plants, but also during the production of pulp used to make paper and in the production of biofuels such as biogas and bioethanol. The BECCS technology can also be employed on such industrial processes.\n\nIt is argued that through the BECCS technology, carbon dioxide is trapped in geologic formations for very long periods of time, whereas for example a tree only stores its carbon during its lifetime. In its report on the CCS technology, IPCC projects that more than 99% of carbon dioxide which is stored through geologic sequestration is likely to stay in place for more than 1000 years. While other types of carbon sinks such as the ocean, trees and soil may involve the risk of negative feedback loops at increased temperatures, BECCS technology is likely to provide a better permanence by storing CO in geological formations.\n\nThe amount of CO that has been released to date is believed to be too much to be able to be absorbed by conventional sinks such as trees and soil in order to reach low emission targets. In addition to the presently accumulated emissions, there will be significant additional emissions during this century, even in the most ambitious low-emission scenarios. BECCS has therefore been suggested as a technology to reverse the emission trend and create a global system of net negative emissions. This implies that the emissions would not only be zero, but negative, so that not only the emissions, but the absolute amount of CO in the atmosphere would be reduced.\n\nThe main technology for CO capture from biotic sources generally employs the same technology as carbon dioxide capture from conventional fossil fuel sources. Broadly, three different types of technologies exist: post-combustion, pre-combustion, and oxy-fuel combustion.\n\nThe sustainable technical potential for net negative emissions with BECCS has been estimated to 10 Gt of equivalent annually, with an economic potential of up to 3.5 Gt of equivalent annually at a cost of less than 50 €/tonne, and up to 3.9 Gt of equivalent annually at a cost of less than 100 €/tonne.\n\nCurrently, most schematic BECCS systems are not cost-efficient compared to normal CCS. The IPCC states that estimations for BECCS cost range from $60-$250 per ton of CO. On the other hand, \"normal\" CCS (from coal and natural gas processing) costs have been decreasing to less than $35 per ton. With limited large-scale testing, BECCS faces many challenges to be a financially viable alternative.\n\nBased on the current Kyoto Protocol agreement, carbon capture and storage projects are not applicable as an emission reduction tool to be used for the Clean Development Mechanism (CDM) or for Joint Implementation (JI) projects. Recognising CCS technologies as an emission reduction tool is vital for the implementation of such plants as there is no other financial motivation for the implementation of such systems. There has been growing support to have fossil CCS and BECCS included in the protocol. Accounting studies on how this can be implemented, including BECCS, have also been done.\n\nThe largest and most detailed techno-economic assessment of BECCS was carried out by cmcl innovations and the TESBiC group (Techno-Economic Study of Biomass to CCS) in 2012. This project recommended the most promising set of biomass fueled power generation technologies coupled with carbon capture and storage (CCS). The project outcomes lead to a detailed “biomass CCS roadmap” for the U.K..\n\nSome of the environmental considerations and other concerns about the widespread implementation of BECCS are similar to those of CCS. However, much of the critique towards CCS is that it may strengthen the dependency on depletable fossil fuels and environmentally invasive coal mining. This is not the case with BECCS, as it relies on renewable biomass. There are however other considerations which involve BECCS and these concerns are related to the possible increased use of biofuels.\n\nBiomass production is subject to a range of sustainability constraints, such as: scarcity of arable land and fresh water, loss of biodiversity, competition with food production, deforestation and scarcity of phosphorus. It is important to make sure that biomass is used in a way that maximizes both energy and climate benefits. There has been criticism to some suggested BECCS deployment scenarios, where there would be a very heavy reliance on increased biomass input.\n\nLarge areas of land would be required to operate BECCS on an industrial scale. To remove 10 billion tons of CO, upwards of 300 million hectares of land area (larger than India) would be required. As a result, BECCS risks using land that could be better suited to agriculture and food production, especially in developing countries.\n\nThese systems may have other negative side effects. There is however presently no need to expand the use of biofuels in energy or industry applications to allow for BECCS deployment. There is already today considerable emissions from point sources of biomass derived CO, which could be utilized for BECCS. Though, in possible future bio-energy system upscaling scenarios, this may be an important consideration.\n\nThe BECCS process allows CO to be collected and stored directly from the atmosphere, rather than from a fossil source. This implies that any eventual emissions from storage may be recollected and restored simply by reiterating the BECCS-process. This is not possible with CCS alone, as CO emitted to the atmosphere cannot be restored by burning more fossil fuel with CCS.\n\nMost CCS projects include adding capture to an existing power plant, usually coal or another fossil fuel. With complete capture, these processes would be carbon neutral. Decatur, Illinois in the United States has many corn plants run by Archer Daniels Midland (ADM), where corn is processed into syrups and ethanol. The plant emits high amounts of carbon dioxide as a byproduct of the process. With the CCS fitting, the plant ideally becomes carbon negative, since corn absorbs carbon dioxide when it grows, and all the carbon dioxide produced during processing is being captured and sequestered in the Mount Simon sandstone. The project cannot be completely carbon negative, as carbon dioxide is produced during the combustion of the ethanol that is being produced. The project is one of the only CCS projects in use to not be coupled with EOR. The Southern Illinois Basin is considered one of the best injection sites, due to its sandstone composition and depth (injection site is 2,000 meters below the surface), as well as its possible capacity (geologists project storage capacity of 27-109 Gt carbon dioxide).\n\n\n"}
{"id": "3994748", "url": "https://en.wikipedia.org/wiki?curid=3994748", "title": "Carrier-to-noise ratio", "text": "Carrier-to-noise ratio\n\nIn telecommunications, the carrier-to-noise ratio, often written CNR or C/N, is the signal-to-noise ratio (SNR) of a modulated signal. The term is used to distinguish the CNR of the radio frequency passband signal from the SNR of an analog base band message signal after demodulation, for example an audio frequency analog message signal. If this distinction is not necessary, the term SNR is often used instead of CNR, with the same definition.\n\nDigitally modulated signals (e.g. QAM or PSK) are basically made of two CW carriers (the I and Q components, which are out-of-phase carriers). In fact, the information (bits or symbols) is carried by given combinations of phase and/or amplitude of the I and Q components. It is for this reason that, in the context of digital modulations, digitally modulated signals are usually referred to as carriers. Therefore, the term carrier-to-noise-ratio (CNR), instead of signal-to-noise-ratio (SNR) is preferred to express the signal quality when the signal has been digitally modulated.\n\nHigh \"C/N\" ratios provide good quality of reception, for example low bit error rate (BER) of a digital message signal, or high SNR of an analog message signal.\n\nThe carrier-to-noise ratio is defined as the ratio of the received modulated carrier signal power \"C\" to the received noise power \"N\" after the receiver filters:\nWhen both carrier and noise are measured across the same impedance, this ratio can equivalently be given as:\nwhere formula_3 and formula_4 are the root mean square (RMS) voltage levels of the carrier signal and noise respectively.\n\"C\"/\"N\" ratios are often specified in decibels (dB):\nor in term of voltage:\n\nThe \"C/N\" ratio is measured in a manner similar to the way the signal-to-noise ratio (\"S/N\") is measured, and both specifications give an indication of the quality of a communications channel.\n\nIn the famous Shannon–Hartley theorem, the \"C/N\" ratio is equivalent to the \"S/N\" ratio. The \"C/N\" ratio resembles the carrier-to-interference ratio (\"C/I\", CIR), and the carrier-to-noise-and-interference ratio, \"C/(N+I)\" or CNIR.\n\"C/N\" estimators are needed to optimize the receiver performance. Typically, it is easier to measure the total\npower than the ratio of signal power to noise power (or noise power spectral density), and that is why CNR estimation techniques are timely and important.\n\n\n"}
{"id": "51386092", "url": "https://en.wikipedia.org/wiki?curid=51386092", "title": "Certifying algorithm", "text": "Certifying algorithm\n\nIn theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be \"efficient\" if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.\n\nThe proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.\n\nImplementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).\n\nMany examples of problems with checkable algorithms come from graph theory.\nFor instance, a classical algorithm for testing whether a graph is bipartite would simply output a Boolean value: true if the graph is bipartite, false otherwise. In contrast, a certifying algorithm might output a 2-coloring of the graph in the case that it is bipartite, or a cycle of odd length if it is not. Any graph is bipartite if and only if it can be 2-colored, and non-bipartite if and only if it contains an odd cycle. Both checking whether a 2-coloring is valid and checking whether a given odd-length sequence of vertices is a cycle may be performed more simply than testing bipartiteness.\n\nAnalogously, it is possible to test whether a given directed graph is acyclic by a certifying algorithm that outputs either a topological order or a directed cycle. It is possible to test whether an undirected graph is a chordal graph by a certifying algorithm that outputs either an elimination ordering (an ordering of all vertices such that, for every vertex, the neighbors that are later in the ordering form a clique) or a chordless cycle. And it is possible to test whether a graph is planar by a certifying algorithm that outputs either a planar embedding or a Kuratowski subgraph.\n\nThe extended Euclidean algorithm for the greatest common divisor of two integers and is certifying: it outputs three integers (the divisor), , and , such that . This equation can only be true of multiples of the greatest common divisor, so testing that is the greatest common divisor may be performed by checking that divides both and and that this equation is correct.\n\n"}
{"id": "48657510", "url": "https://en.wikipedia.org/wiki?curid=48657510", "title": "Chess diagram", "text": "Chess diagram\n\nA chess diagram is graphic representation and stylized of a specific moment in a chess game, showing the different positions occupied by chess pieces in given time during game development.\n\nThis graphical representation is done through symbols previously agreed for this purpose in order to facilitate reading the diagram.\n\nThe chess diagrams are a resource widely used not only in teaching game in the courses offered to people just getting into it; but they are also used in subsequent analysis, that amateurs and professionals players at this discipline can make of games played, especially in championship games.\n\nSimilarly they are used in the development of chess problems that are all these diagrams with composed positions and whose solution is to checkmate in a given number of moves and are often published in newspapers and magazines.\n\nIcons or symbols used represent the basic characteristics of chessmen, such as in the case of rook is used picture of a defensive tower made with bricks and with 3 battlements and 2 gunboats at top of its.\n\nIt is worth mentioning that at present most diagrams represent the bishop using a miter with two lappets by way of feet; but there are some variants of diagrams where this piece is represented by a helmet with visor.\n\n"}
{"id": "24161377", "url": "https://en.wikipedia.org/wiki?curid=24161377", "title": "Contemporary Chinese Thought and the Question of Modernity", "text": "Contemporary Chinese Thought and the Question of Modernity\n\n\"Contemporary Chinese Thought and the Question of Modernity\" (Chinese: 当代中国的思想状况与现代性问题) is an influential article of around 35,000 characters in length by Chinese intellectual historian and literary scholar Wang Hui, written in 1994 and published in left-wing literature journal \"Tianya\" (天涯) in 1997. An English translation by Rebecca E. Karl appeared in a volume of \"Social Text\" titled \"Intellectual Politics in Post-Tiananmen China\" (1998).\n\nThe article became the subject of intense debate and attention both for its methodology—an unusually socio-historical approach to intellectual history—and its expressed politics, which are critical of capitalist modernity. According to academic Yue Gang, it is \"a cornerstone in the transformation of contemporary Chinese thought\" and \"has become a benchmark for the New Left.\"\n\n"}
{"id": "3272235", "url": "https://en.wikipedia.org/wiki?curid=3272235", "title": "Deployment diagram", "text": "Deployment diagram\n\nA deployment diagram in the Unified Modeling Language models the \"physical\" deployment of artifacts on nodes. To describe a web site, for example, a deployment diagram would show what hardware components (\"nodes\") exist (e.g., a web server, an application server, and a database server), what software components (\"artifacts\") run on each node (e.g., web application, database), and how the different pieces are connected (e.g. JDBC, REST, RMI).\n\nThe nodes appear as boxes, and the artifacts allocated to each node appear as rectangles within the boxes. Nodes may have subnodes, which appear as nested boxes. A single node in a deployment diagram may conceptually represent multiple physical nodes, such as a cluster of database servers.\n\nThere are two types of Nodes:\n\nDevice nodes are physical computing resources with processing memory and services to execute software, such as typical computers or mobile phones. An execution environment node (EEN) is a software computing resource that runs within an outer node and which itself provides a service to host and execute other executable software elements.\n\n"}
{"id": "41303128", "url": "https://en.wikipedia.org/wiki?curid=41303128", "title": "Developmental regression", "text": "Developmental regression\n\nDevelopmental regression is when a child loses an acquired function or fails to progress beyond a prolonged plateau after a period of relatively normal development. Developmental regression could be due to metabolic disorders, progressive hydrocephalus, worsening of seizures, increased spasticity, worsening of movement disorders or parental misconception of acquired milestones. The timing of onset of developmental regression can be established by repeated medical evaluations, prior photographs and home movies. Whether the neurologic decline is predominantly affecting the gray matter or the white matter of the brain needs to be ascertained. Seizures or EEG changes, movement disorders, blindness with retinal changes, personality changes and dementia are features suggestive of grey matter involvement.\n\n"}
{"id": "55991275", "url": "https://en.wikipedia.org/wiki?curid=55991275", "title": "Dusky Peril", "text": "Dusky Peril\n\nDusky Peril is a term used by \"Puget Sound American\" a daily newspaper, published from Bellingham, Washington, USA, to describe the immigration of what it described as Hindus to the area, in its 16 September, 1906 issue by way of a feature article. It has been considered as an expression of xenophobia similar to the term Yellow Peril, that found practice in white and non-white countries across the globe in those times. The term is analysed to have both an ethnic and a religious dimension. The article is in response to the immigration of 17 individuals to the town. The article's headline is \"Have we a Dusky Peril: Hindu hordes invading the state\" with the byline going thus, \"Bellingham workmen are becoming excited over the arrival of East Indians in numbers across the Canadian border and fear that the dusky Asiatics with their turbans will prove a worse menance to the working classes than the \"Yellow Peril\" that has so long threatened the Pacific Coast.\" \n\nKaajal Ahuja writes that the graffiti \"Get Out\" written on the walls of a Hindu temple in Bothelo about an hour's drive south of Bellingham, brought back what she calls \"painful memories (of the incidents of the early decades of the 20th century such as the eponymous feature article)\" for the community. Shefali Chandan writing in Jano conjectures that the emotions behind the circumstances that led to the ethnic cleansing of Bellingham in 1907, in which white mobs went door to door to locate Indian immigrants and forced their expulsion, resulting in the entire community numbering about 200 leaving the town for good, found expression in the feature article written less than a year ago.\n"}
{"id": "1041878", "url": "https://en.wikipedia.org/wiki?curid=1041878", "title": "Edict of Expulsion", "text": "Edict of Expulsion\n\nThe Edict of Expulsion was a royal decree issued by King Edward I of England on 18 July 1290, expelling all Jews from the Kingdom of England. The expulsion edict remained in force for the rest of the Middle Ages. The edict was not an isolated incident, but the culmination of over 200 years of increased persecution. The edict was overturned during the Protectorate more than 350 years later, when Oliver Cromwell permitted Jews to return to England in 1657. King Edward I advised Sheriffs of all Counties he wanted all Jews expelled by no later than All Saints' Day (1st November) of the year the decree was issued.\n\nThe first Jewish communities of significant size came to England with William the Conqueror in 1066. After the conquest of England, William instituted a feudal system in the country, whereby all estates formally belonged to the Crown; the king then appointed lords over these vast estates, but they were subject to duties and obligations (financial and military) to the king. Under the lords were other subjects such as serfs, who were bound and obliged to their lords, and to their lords' obligations. Merchants had a special status in the system, as did Jews. Jews were declared to be direct subjects of the king, unlike the rest of the population. This was an ambivalent legal position for the Jewish population, in that they were not tied to any particular lord but were subject to the whims of the king, it could be either advantageous or disadvantageous. Every successive king formally reviewed a royal charter, granting Jews the right to remain in England. Jews did not enjoy any of the guarantees of the Magna Carta of 1215.\n\nEconomically, Jews played a key role in the country. The Church then strictly forbade the lending of money for profit, creating a vacuum in the economy of Europe that Jews filled because of extreme discrimination in every other economic area. Canon law was not considered applicable to Jews, and Judaism does not forbid loans with interest between Jews and non-Jews. Taking advantage of their unique status as his direct subjects, the King could appropriate Jewish assets in the form of taxation. He levied heavy taxes on Jews at will, without having to summon Parliament. \n\nThe reputation of Jews as extortionate money-lenders arose, which made them extremely unpopular with both the Church and the general public. While an anti-Jewish attitude was wide-spread in Europe, medieval England was particularly anti-Jewish. An image of the Jew as a diabolical figure who hated Christ started to become wide-spread, and myths such as the tale of the Wandering Jew and allegations of ritual murders originated and spread throughout England as well as in Scotland and Wales.\n\nIn frequent cases of blood libel, Jews were said to hunt for children to murder before Passover so that they could use their blood to make the unleavened matzah. Anti-Jewish attitudes sparked numerous riots in which many Jews were murdered, most notably in 1190, when over 100 Jews were massacred in York.\n\nThe situation only got worse for Jews as the 13th century progressed. In 1218, Henry III of England proclaimed the Edict of the Badge requiring Jews to wear a marking badge. Taxation grew increasingly intense. Between 1219–1272, 49 levies were imposed on Jews for a total of 200,000 marks, a vast sum of money. Henry III imposed greater segregation and reinforced the wearing of badges in the 1253 Statute of Jewry. He endorsed the myth of Jewish child murders. Meanwhile, his court and major Barons bought Jewish debts with the intention of securing lands of lesser nobles through defaults. The Second Barons' War in the 1260s brought a series of pogroms aimed at destroying the evidence of these debts and Jewish communities in major towns, including London, where 500 Jews died, Worcester, Canterbury and many other towns.\n\nThe first major step towards expulsion took place in 1275, with the Statute of the Jewry. The statute outlawed all lending at interest and gave Jews fifteen years to readjust.\n\nIn the duchy of Gascony in 1287, King Edward ordered the local Jews expelled. All their property was seized by the crown and all outstanding debts payable to Jews were transferred to the King’s name. By the time he returned to England in 1289, King Edward was deeply in debt. The next summer he summoned his knights to impose a steep tax. To make the tax more palatable, Edward, in exchange, essentially offered to expel all Jews. The heavy tax was passed, and three days later, on 18 July, the Edict of Expulsion was issued.\n\nOne official reason for the expulsion was that Jews had declined to follow the Statute of Jewry and continued to practice usury. This is quite likely, as it would have been extremely hard for many Jews to take up the \"respectable\" occupations demanded by the Statute. The edict of expulsion was widely popular and met with little resistance, and the expulsion was quickly carried out.\n\nThe Jewish population in England at the time was relatively small, perhaps 2,000 people, although estimates vary. The expulsion process appears to have been relatively non-violent, although there were some accounts to the contrary. One perhaps apocryphal story told of a captain taking a ship full of Jews to the Thames, en route to France, while the tide was low, and convincing them to go out for a walk with him. He then lost them and made it back to his ship quickly before the tide came back in, leaving them all to drown.\n\nMany Jews emigrated, to Scotland, France and the Netherlands, and as far as Poland, which, at that time, protected them (see Statute of Kalisz).\n\nBetween the expulsion of Jews in 1290 and their formal return in 1655, there are records of Jews in the Domus Conversorum up to 1551 and even later. An attempt was made to obtain a revocation of the edict of expulsion as early as 1310, but in vain. Notwithstanding, a certain number of Jews appeared to have returned; for complaints were made to the king in 1376 that some of those trading as Lombards were actually Jews.\n\nOccasionally permits were given to individuals to visit England, as in the case of Dr Elias Sabot (an eminent physician from Bologna summoned to attend Henry IV) in 1410, but it was not until the expulsion of the Jews from Spain in 1492 and Portugal in 1497 that any considerable number of Sephardic Jews found refuge in England. In 1542 many were arrested on the suspicion of being Jews, and throughout the sixteenth century a number of persons named Lopez, possibly all of the same family, took refuge in England, the best known of them being Rodrigo López, physician to Queen Elizabeth I, and who is said by some commentators to have been the inspiration for Shylock.\n\nEngland also saw converts like Immanuel Tremellius and Philip Ferdinand. Jewish visitors included Joachim Gaunse, who introduced new methods of mining into England and there are records of visits from Jews called Alonzo de Herrera and Simon Palache in 1614. The writings of John Weemes in the 1630s provided a positive view of the resettlement of Jews in England, effected in 1657.\n\n\n"}
{"id": "6913403", "url": "https://en.wikipedia.org/wiki?curid=6913403", "title": "Energy medicine", "text": "Energy medicine\n\nEnergy medicine, energy therapy, energy healing, psychic healing, spiritual medicine or spiritual healing are branches of alternative medicine based on a pseudo-scientific belief that healers can channel healing energy into a patient and effect positive results. This idea itself contains several methods: hands-on, hands-off, and distant (or absent) where the patient and healer are in different locations.\n\nMany schools of energy healing exist using many names, for example, \"biofield energy healing\", \"spiritual healing\", \"contact healing\", \"distant healing\", \"Qi Do\", \"therapeutic touch\", \"Reiki\" or \"Qigong.\"\n\n\"Spiritual healing\" occurs largely in non-denominational and ecumenical contexts. Practitioners do not see traditional religious faith as a prerequisite for effecting cures. \"Faith healing,\" by contrast, takes place within a traditional religious context.\n\nWhile early reviews of the scientific literature on energy healing were equivocal and recommended further research, more recent reviews have concluded that there is no evidence supporting clinical efficiency. The theoretical basis of healing has been criticised as implausible, research and reviews supportive of energy medicine have been faulted for containing methodological flaws and selection bias, and positive therapeutic results have been dismissed as resulting from known psychological mechanisms.\n\nEdzard Ernst, formerly Professor of Complementary and Alternative Medicine at the University of Exeter, has warned that \"healing continues to be promoted despite the absence of biological plausibility or convincing clinical evidence ... that these methods work therapeutically and plenty to demonstrate that they do not\". Some claims of those purveying \"energy medicine\" devices are known to be fraudulent and their marketing practices have drawn law-enforcement action in the US.\n\nThere is a history of association or exploitation of scientific inventions by individuals claiming that newly discovered science could help people to heal: In the 19th century, electricity and magnetism were in the \"borderlands\" of science and electrical quackery was rife. These concepts continue to inspire writers in the New Age movement. In the early 20th century health claims for radio-active materials put lives at risk, and recently quantum mechanics and grand unification theory have provided similar opportunities for commercial exploitation. Thousands of devices claiming to heal via putative or veritable energy are used worldwide. Many of them are illegal or dangerous and are marketed with false or unproven claims. Several of these devices have been banned.\nReliance on spiritual and energetic healings is associated with serious harm or death when medical treatment is delayed or foregone.\n\nThe term \"energy medicine\" has been in general use since the founding of the non-profit \"International Society for the Study of Subtle Energies and Energy Medicine\" in the 1980s. Guides are available for practitioners, and other books aim to provide a theoretical basis and evidence for the practice. Energy medicine often proposes that imbalances in the body's \"energy field\" result in illness, and that by re-balancing the body's energy-field health can be restored. Some modalities describe treatments as ridding the body of negative energies or blockages in 'mind'; illness or episodes of ill health after a treatment are referred to as a 'release' or letting go of a 'contraction' in the body-mind. Usually, a practitioner will then recommend further treatments for complete healing.\n\nThe US-based \"National Center for Complementary and Integrative Health\" (NCCIH) distinguishes between health care involving scientifically observable energy, which it calls \"Veritable Energy Medicine\", and health care methods that invoke physically undetectable or unverifiable \"energies\", which it calls \"Putative Energy Medicine\":\n\n\nPolarity therapy founded by Randolph Stone is a kind of energy medicine based on the belief that a person's health is subject to positive and negative charges in their electromagnetic field. It has been promoted as capable of curing a number of human ailments ranging from muscular tightness to cancer; however, according to the American Cancer Society \"available scientific evidence does not support claims that polarity therapy is effective in treating cancer or any other disease\".\n\nEnergy healing relies on a belief in the ability of a practitioner to channel healing energy into the person seeking help by different methods: hands-on, hands-off, and distant (or absent) where the patient and healer are in different locations. The \"Brockhampton Guide to Spiritual Healing\" describes contact healing in terms of \"transfer of ... healing energy\" and distant healing based on visualising the patient in perfect health. Practitioners say that this \"healing energy\" is sometimes perceived by the therapist as a feeling of heat.\n\nThere are various schools of energy healing, including \"biofield energy healing\", \"spiritual healing\", \"contact healing\", \"distant healing\", \"therapeutic touch\", \"Reiki,\" \"Qigong,\" and many others.\n\n\"Spiritual healing\" is largely non-denominational; traditional religious faith is not seen as a prerequisite for effecting a cure. \"Faith healing,\" by contrast, takes place within a religious context. The Buddha is often quoted by practitioners of energy medicine, but he did not practise \"hands on or off\" healing.\n\nEnergy healing techniques such as Therapeutic touch have found recognition in the nursing profession. In 2005–2006, the \"North American Nursing Diagnosis Association\" approved the diagnosis of \"energy field disturbance\" in patients, reflective of what has been variously called a \"postmodern\" or \"anti-scientific\" approach to nursing care. This approach has been strongly criticised.\n\nBelievers in these techniques have proposed quantum mystical invocations of non-locality to try to explain distant healing. They have also proposed that healers act as a channel passing on a kind of bioelectromagnetism which shares similarities to vitalistic pseudosciences such as orgone or qi. Drew Leder remarked in a paper in the \"Journal of Alternative and Complementary Medicine\" that such ideas were attempts to \"make sense of, interpret, and explore 'psi' and distant healing.\" and that \"such physics-based models are not presented as explanatory but rather as suggestive.\" Beverly Rubik, in an article in the same journal, justified her belief with references to biophysical systems theory, bioelectromagnetics, and chaos theory that provide her with a \"...scientific foundation for the biofield...\" Writing in the \"Journal of Bodywork and Movement Therapies\", James Oschman introduced the concept of healer-sourced electromagnetic fields which change in frequency. Oschman believes that \"healing energy\" derives from electromagnetic frequencies generated by a medical device, projected from the hands of the healer, or by electrons acting as antioxidants.\n\nPhysicists and sceptics roundly criticise these explanations as pseudophysics – a branch of pseudoscience which explains magical thinking by using irrelevant jargon from modern physics to exploit scientific illiteracy and to impress the unsophisticated.\nIndeed, even enthusiastic supporters of energy healing point out that \"there are only very tenuous theoretical foundations underlying [spiritual] healing.\"\n\nA systematic review of 23 trials of distant healing published in 2000 did not draw definitive conclusions because of the \"methodologic limitations of several studies\". In 2001 the lead author of that study, Edzard Ernst, published a primer on complementary therapies in cancer care in which he explained that though \"about half of these trials suggested that healing is effective\" he cautioned that the evidence was \"highly conflicting\" and that \"methodological shortcomings prevented firm conclusions.\" He concluded that \"as long as it is not used as an alternative to effective therapies, spiritual healing should be virtually devoid of risks.\" A 2001 randomised clinical trial by the same group found no statistically significant difference on chronic pain between distance healers and \"simulated healers\". A 2003 review by Ernst updating previous work concluded that more recent research had shifted the weight of evidence \"against the notion that distant healing is more than a placebo\" and that \"distant healing can be associated with adverse effects.\"\n\nA 2001 randomised clinical trial randomly assigned 120 patients with chronic pain to either healers or \"simulated healers\", but could not demonstrate efficacy for either distance or face-to-face healing. A systematic review in 2008 concluded that the evidence for a specific effect of spiritual healing on relieving neuropathic or neuralgic pain was not convincing. In their 2008 book \"Trick or Treatment\", Simon Singh and Edzard Ernst concluded that \"spiritual healing is biologically implausible and its effects rely on a placebo response. At best it may offer comfort; at worst it can result in charlatans taking money from patients with serious conditions who require urgent conventional medicine.\"\n\nAlternative medicine researcher Edzard Ernst has argued that although an initial review of pre-1999 distant healing trials had highlighted 57% of trials as showing positive results, later reviews of non-randomised and randomised clinical trials conducted between 2000 and 2002 led to the conclusion that \"the majority of the rigorous trials do not support the hypothesis that distant healing has specific therapeutic effects\". Ernst described the evidence base for healing practices to be \"increasingly negative\". Ernst also warned that many of the reviews were under suspicion for fabricated data, lack of transparency, and scientific misconduct. He concluded that \"[s]piritual healing continues to be promoted despite the absence of biological plausibility or convincing clinical evidence ... that these methods work therapeutically and plenty to demonstrate that they do not.\" A 2014 study of energy healing for colorectal cancer patients showed no improvement in quality of life, depressive symptoms, mood, or sleep quality.\n\nThere are several, primarily psychological, explanations for positive reports after energy therapy, including placebo effects, spontaneous remission, and cognitive dissonance. A 2009 review found that the \"small successes\" reported for two therapies collectively marketed as \"energy psychology\" (Emotional Freedom Techniques and Tapas Acupressure Technique) \"are potentially attributable to well-known cognitive and behavioral techniques that are included with the energy manipulation.\" The report concluded that \"[p]sychologists and researchers should be wary of using such techniques, and make efforts to inform the public about the ill effects of therapies that advertise miraculous claims.\"\n\nThere are primarily two explanations for anecdotes of cures or improvements, relieving any need to appeal to the supernatural. The first is \"post hoc ergo propter hoc\", meaning that a genuine improvement or spontaneous remission may have been experienced coincidental with but independent from anything the healer or patient did or said. These patients would have improved just as well even had they done nothing. The second is the placebo effect, through which a person may experience genuine pain relief and other symptomatic alleviation. In this case, the patient genuinely has been helped by the healer not through any mysterious or numinous function, but by the power of their own belief that they would be healed. In both cases the patient may experience a real reduction in symptoms, though in neither case has anything miraculous or inexplicable occurred. Both cases, are strictly limited to the body's natural abilities.\n\nPositive findings from research studies can also result from such psychological mechanisms, or as a result of experimenter bias, methodological flaws such as lack of blinding, or publication bias; positive reviews of the scientific literature may show selection bias, in that they omit key studies that do not agree with the author's position. All of these factors must be considered when evaluating claims.\n\nBioresonance therapy (including MORA therapy) is a pseudoscientific medical practice in which it is proposed that electromagnetic waves can be used to diagnose and treat human illness.\n\nBioresonance therapy was invented in (Germany) in 1977 by Franz Morell and his son-in-law, engineer Erich Rasche. Initially they marketed it as \"MORA-Therapie\", for MOrell and RAsche. Some of the machines contain an electronic circuit measuring skin-resistance, akin to the E-meter used by Scientology, which the bioresonance creators sought to improve; Franz Morell had links with Scientology.\n\nPractitioners claim to be able to detect a variety of diseases and addictions. Some practitioners also claim they can treat diseases using this therapy without drugs, by stimulating a change of \"bioresonance\" in the cells, and reversing the change caused by the disease. The devices would need to be able to isolate and pinpoint pathogens' responses from the mixture of responses the device receives via the electrodes. Transmitting these transformed signals over the same electrodes is claimed by practitioners to generate healing signals that have the curative effect.\n\nOne placebo-controlled study of the effects of bioresonance therapy showed that \"MORA bioresonance therapy can markedly improve non-organic gastro-intestinal complaints.\" This research study, however, was conducted with a relatively small sample size of 20 people and published by a journal that exclusively publishes \"alternative medicine\" procedures. The bioresonance diagnosis was complemented by the practitioner's dietary suggestions, making for an uncontrolled experiment with no analytical methods. The only measuring methods present were the subjective analysis of the practitioner pre and post diagnosis.\n\nLacking any scientific explanation of how bioresonance therapy might work, researchers have classified bioresonance therapy as pseudoscience.\nSome scientific studies did not show effects above that of the placebo effect.\n\nWebMD states: \"There is no reliable scientific evidence that bioresonance is an accurate indicator of medical conditions or disease or an effective treatment for any condition.\"\n\nProven cases of online fraud have occurred, with a practitioner making false claims that he had the ability to cure cancer, and that his clients did not need to follow the chemotherapy or surgery recommended by medical doctors, which can be life-saving. Ben Goldacre ridiculed the BBC when it reported as fact a clinic's claim that the treatment had the ability to stop 70% of clients smoking, a better result than any conventional therapy.\n\nIn the United States of America the U.S. Food and Drug Administration (FDA) classifies \"devices that use resistance measurements to diagnose and treat various diseases\" as Class III devices, which require FDA approval prior to marketing. The FDA has banned some of these devices from the US market, and has prosecuted many sellers of electrical devices for making false claims of health benefits.\n\nAccording to Quackwatch the therapy is completely nonsensical and the proposed mechanism of action impossible.\n\n\n"}
{"id": "15551070", "url": "https://en.wikipedia.org/wiki?curid=15551070", "title": "Evolutionary ideas of the Renaissance and Enlightenment", "text": "Evolutionary ideas of the Renaissance and Enlightenment\n\nEvolutionary ideas during the periods of the Renaissance and the Enlightenment developed over a time when natural history became more sophisticated during the 17th and 18th centuries, and as the scientific revolution and the rise of mechanical philosophy encouraged viewing the natural world as a machine with workings capable of analysis. But the evolutionary ideas of the early 18th century were of a religious and spiritural nature. In the second half of the 18th century more materialistic and explicit ideas about biological evolution began to emerge, adding further strands in the history of evolutionary thought.\n\nThe word \"evolution\" (from the Latin \"evolutio\", meaning \"to unroll like a scroll\") appeared in English in the 17th century, referring to an orderly sequence of events, particularly one in which the outcome was somehow contained within it from the start. Notably, in 1677 Sir Matthew Hale, attacking the atheistic atomism of Democritus and Epicurus, used the term \"evolution\" to describe his opponent's ideas that vibrations and collisions of atoms in the void — without divine intervention — had formed \"Primordial Seeds\" (semina) which were the \"immediate, primitive, productive Principles of Men, Animals, Birds and Fishes.\" For Hale, this mechanism was \"absurd\", because \"it must have potentially at least the whole Systeme of Humane Nature, or at least that Ideal Principle or Configuration thereof, in the evolution whereof the complement and formation of the Humane Nature must consist ... and all this drawn from a fortuitous coalition of senseless and dead Atoms.\"\n\nWhile Hale (ironically) first used the term evolution in arguing \"against\" the exact mechanistic view the word would come to symbolize, he also demonstrates that at least some evolutionist theories explored between 1650 and 1800 postulated that the universe, including life on earth, had developed mechanically, entirely without divine guidance. Around this time, the mechanical philosophy of Descartes, reinforced by the physics of Galileo and Newton, began to encourage the machine-like view of the universe which would come to characterise the scientific revolution. However, most contemporary theories of evolution, including those developed by the German idealist philosophers Schelling and Hegel (and mocked by Schopenhauer), held that evolution was a fundamentally \"spiritual\" process, with the entire course of natural and human evolution being \"a self-disclosing revelation of the Absolute\".\n\nTypical of these theorists, Gottfried Leibniz postulated in 1714 that \"monads\" inside objects caused motion by internal forces, and maintained that \"the 'germs' of all things have always existed ... [and] contain within themselves an internal principle of development which drives them on through a vast series of metamorphoses\" to become the geological formations, lifeforms, psychologies, and civilizations of the present. Leibniz clearly felt that evolution proceeded on divine principles — in his \"De rerum originatione radicali\" (1697), he wrote: \"A cumulative increase of the beauty and universal perfection of the works of God, a perpetual and unrestricted progress of the universe as a whole must be recognized, such that it advances to a higher state of cultivation.\" Others, such as J. G. von Herder, expressed similar ideas.\n\nBetween 1603 and 1613 Sir Walter Raleigh was a prisoner in the Tower of London awaiting execution; in this period he wrote a history of the world in five volumes where he described his American experiences and adventures, in it he wondered whether all the new species discovered in the new continent could have found their place on Noah's Ark. A very serious question at the time, he postulates that only animals from the old continent found place on the Ark; eventually, after the Flood, some of these animals would migrate to the new continent and, under environmental pressure, change their appearances to create new species. Fifty years later, Matthew Hale went even further, and said that only the prototypes of all animal species were welcomed on the Ark; these would eventually differentiate after their release. Many clergymen were happy with Raleigh and Hale ideas since they appeared to solve the problem of the Ark's tonnage.\n\nIn his \"Venus Physique\" in 1745, and \"System of Nature\" in 1751, Pierre Louis Maupertuis veered toward more materialist ground. He wrote of natural modifications occurring during reproduction and accumulating over the course of many generations, producing races and even new species. He also anticipated in general terms the idea of natural selection.\n\nVague and general ideas of evolution continued to proliferate among the mid-eighteenth century Enlightenment philosophers. G. L. L. Buffon suggested that what most people referred to as species were really just well-marked varieties. He thought that the members of what was then called a genus (which in terms of modern scientific classification would be considered a family) are all descended from a single, common ancestor. The ancestor of each family had arisen through spontaneous generation; environmental effects then caused them to diverge into different species. He speculated that the 200 or so species of mammals then known might have descended from as few as 38 original forms. Buffon's concept of evolution was strictly limited. He believed there were \"internal molds\" that shaped the spontaneous generation of each family and that the families themselves were entirely and eternally distinct. Thus, lions, tigers, leopards, pumas and house cats could all share a common ancestor, but dogs and house cats could not. Although Darwin's foreword to his 6th edition of \"Origin\" credited Aristotle with foreshadowing the concept of natural selection, he also wrote that \"the first author who in modern times has treated it in a scientific spirit was Buffon\".\n\nSome 18th-century writers speculated about human evolution. John Mitchell, a physician and cartographer, wrote a book in 1744 called \"An Essay upon the Causes of the Different Colours of People in Different Climates\" in which he claimed that the first race on earth had been a brown and reddish colour he said \"that an intermediate tawny colour found amongst Asiatics and Native Amerindians\" had been the “original complexion of mankind” and that others races came about by the original race spending generations in different climates. Between 1767 and 1792 James Burnett, Lord Monboddo included in his writings not only the concept that man had descended from other primates, but also that, in response to their environment, creatures had found methods of transforming their characteristics over long time intervals. He also produced research on the evolution of linguistics, which was cited by Erasmus Darwin in his poem \"Temple of Nature\". Jan-Andrew Henderson states that Monboddo was the first to articulate the theory of natural selection.\n\nIn 1792, the philosopher Immanuel Kant presented, in his \"Critique of Judgement\", what he referred to as “a daring venture of reason”, in which “one organic being [is] derived from another organic being, although from one which is specifically different; e.g., certain water-animals transform themselves gradually into marsh-animals and from these, after some generations, into land-animals.” Some 20th-century philosophers, such as Eric Voegelin, credit Kant with foreshadowing modern evolutionary theory.\nIn 1796, Erasmus Darwin published his \"Zoönomia\", which suggested \"that all warm-blooded animals have arisen from one living filament ... with the power of acquiring new parts\" in response to stimuli, with each round of improvements being inherited by successive generations. In his 1802 poem \"Temple of Nature\", he described the rise of life from minute organisms living in the mud to its modern diversity:\n\nFirst forms minute, unseen by spheric glass,<br>\nMove on the mud, or pierce the watery mass;<br>\nThese, as successive generations bloom,<br>\nNew powers acquire and larger limbs assume;<br>\nWhence countless groups of vegetation spring,<br>\nAnd breathing realms of fin and feet and wing.\n\n"}
{"id": "10174927", "url": "https://en.wikipedia.org/wiki?curid=10174927", "title": "Expulsion of Asians from Uganda", "text": "Expulsion of Asians from Uganda\n\nIn early August 1972, the President of Uganda, Idi Amin, ordered the expulsion of his country's South Asian minority, giving them 90 days to leave the country. At the time of the expulsion, there were approximately 80,000 individuals of South Asian descent (mostly Gujaratis) in Uganda, of whom 23,000 had had their applications for citizenship both processed and accepted. Although the latter were ultimately exempted from the expulsion, many chose to leave voluntarily. The expulsion took place against a backdrop of Indophobia in Uganda, with Amin accusing a minority of the Asian population of disloyalty, non-integration and commercial malpractice, claims Indian leaders disputed. Amin defended the expulsion by arguing that he was giving Uganda back to ethnic Ugandans.\n\nMany of the expellees were citizens of the United Kingdom and Colonies and 27,200 subsequently emigrated to the United Kingdom. Of the other refugees who were accounted for, 6,000 went to Canada, 4,500 refugees ended up in India and 2,500 went to nearby Kenya. In total, some 5,655 firms, ranches, farms, and agricultural estates were reallocated, along with cars, homes and other household goods.\n\nThe presence of South Asians in Uganda was the result of deliberate choices by the British administration (1894–1962). They were brought to the Uganda Protectorate by the British to \"serve as a buffer between Europeans and Africans in the middle rungs of commerce and administration\". In addition, in the 1890s, 32,000 labourers from British India were brought to Southeast Africa under indentured labour contracts to work on the construction of the Uganda Railway. Most of the surviving Indians returned home, but 6,724 individuals decided to remain in the African Great Lakes after the line's completion. At the time of the expulsion, there were approximately 80,000 individuals of South Asian descent in Uganda, of whom 23,000 had had their applications for citizenship both processed and accepted. A further 50,000 were British passport holders, though Amin himself used the apparently exaggerated figure of 80,000 British passport holders in his initial expulsion speech.\n\nThe British had invested in the education of the Asian minority, in preference to that of indigenous Ugandans. By the early 1970s, many Indians in Southeast Africa and Uganda were employed in the sartorial and banking businesses and Indophobia was already engrained by the start of Amin's rule in February 1971. While not all Ugandan Asians were well off, they were on average better off than the indigenous communities, constituting 1% of the population while receiving a fifth of the national income. Indians were stereotyped as \"merely traders\" and labelled as \"dukawallas\" (an occupational term that degenerated into an anti-Indian slur during Amin's time), who tried to cheat unsuspecting purchasers and looked out only for their own families. Conversely, it was \"not unusual to find that Indians possessed attitudes of superiority and negative pictures of the ability and efficiency of Africans\". Racial segregation was institutionalised. Gated ethnic communities served elite healthcare and schooling services. Additionally, the tariff system in Uganda had historically been oriented toward the economic interests of South Asian traders.\n\nMilton Obote's government had pursued a policy of \"Africanisation\" which included policies targeted at Ugandan Asians. The 1968 Committee on the \"Africanisation in Commerce and Industry\", for example, had made far-reaching Indophobic proposals and a system of work permits and trade licenses was introduced in 1969 to restrict the role of non-citizen Indians in economic and professional activities. Nevertheless, Amin's policies represented a significant acceleration. In August 1971, Amin announced a review of the citizenship status awarded to Uganda's Asian community, followed by the declaration of a census of Uganda's Asian population in October that year. In order to resolve the \"misunderstandings\" regarding the role of Uganda's Asian minority in society, he then convened an Indian 'conference' for 7–8 December. In a memorandum presented on the second day of the conference, he set out his hope that \"the wide gap\" between Ugandan Asians and Africans would narrow. While paying tribute to Indians' contribution to the economy and the professions, he accused a minority of the Asian population of disloyalty, non-integration and commercial malpractice, claims Indian leaders disputed. On the vexed question of citizenship, he said his government would recognise citizenship rights already granted, but all outstanding applications for citizenship (which by this point were thought to number more than 12,000) would be cancelled.\n\nThis expulsion of an ethnic minority was not the first in Uganda's history as the country's Kenyan minority, numbering approximately 30,000, had been expelled in 1969–70.\n\nOn 4 August 1972, Amin declared that Britain would need to take on responsibility for British subjects of Asian origin, accusing them of \"sabotaging Uganda's economy and encouraging corruption\". The deadline for British subjects to leave was confirmed as three months, which came to mean 8 November. On 9 August, the policy was expanded to include citizens of India, Pakistan and Bangladesh. The position of the 23,000 Asians who had been granted Ugandan citizenship (and in particular those who held no other citizenship) was less clear. Not originally included, on 19 August, they were seemingly added to the list, before being re-exempted three days later following international protest. Many chose to leave rather than endure further intimidation, with only 4,000 known to have stayed. Exemptions for certain professions were added, then later removed.\n\nThe precise motivation for the expulsion remains unclear. Some of his former supporters suggest that it followed a dream in which, he claimed, Allah had told him to expel them, as well as plot vengeance against the British government for refusing to provide him with arms to invade Tanzania. Amin defended the expulsion by arguing that he was giving Uganda back to the ethnic Ugandans:\n\nUgandan soldiers during this period engaged in theft and physical and sexual violence against the Asians with impunity. Restrictions were imposed on the sale or transfer of private businesses by Ugandan Asians and on 16 August Amin made it clear that after he was done with Indian-owned business, European-owned businesses would be next.\n\nAmin's decrees drew immediate condemnation, including from India, who severed diplomatic relations with Uganda. The Indian government warned Uganda of dire consequences, but took no action when Amin's government ignored the ultimatum. The United Kingdom froze a £10 million loan which had been arranged the previous year; Amin simply shrugged this off.\n\nMany of the Indians were citizens of the United Kingdom and Colonies and 27,200 refugees subsequently emigrated to the United Kingdom. Of the other refugees who were accounted for, 6,000 went to Canada, 4,500 ended up in India and 2,500 went to nearby Kenya. Malawi, Pakistan, West Germany and the United States took 1,000 refugees each, with smaller numbers emigrating to Australia, Austria, Sweden, Norway, Mauritius and New Zealand. About 20,000 refugees were unaccounted for. Only a few hundred remained behind. Reluctant to expand its newly introduced immigration quota, the British government had sought agreement from its British overseas territories to resettle them; however, only the Falkland Islands responded positively. Kenya and Tanzania similarly closed their borders with Uganda to prevent an influx of refugees.\n\nSome of those expelled were Nizari Ismaili Muslims. The Aga Khan, the Imam of Nizari Ismailis phoned his long-time friend Canadian Prime Minister Pierre Trudeau. Trudeau's government agreed to allow thousands of Nizari Ismailis to emigrate to Canada. The exodus of Ugandan Asians took on a new level of urgency on the September following a telegram from Amin to the UN Secretary General Kurt Waldheim, in which it appeared that Amin was sympathetic to Hitler's treatment of Jews and an airlift was organised. The UN dispatched the Executive Secretary of the Economic Commission for Africa, Robert K. A. Gardiner, who attempted in vain to convince Amin to reverse his decision.\n\nBefore the expulsion, Asians owned many large businesses in Uganda but the purge of Asians from Uganda's economy was virtually total. In total, some 5,655 firms, ranches, farms, and agricultural estates were reallocated, along with cars, homes and other household goods. For political reasons, most (5,443) were reallocated to individuals, with 176 going to government bodies, 33 being reallocated to semi-state organisations and 2 going to charities. Possibly the biggest winner was the state-owned Uganda Development Corporation, which gained control over some of the largest enterprises, though both the rapid nature of the growth and the sudden lack of experienced technicians and managers proved a challenge for the corporation, resulting in a restructuring of the sector in 1974–5. \"The Ugandan economy fell deep into a crisis under the strain of civil wars, the nationalization of certain industries and the expulsion of the Asians... By 1987, President Yoweri Museveni had inherited an economy that suffered the poorest growth rate in Africa.\"\n\nThousands of Gujaratis returned to Uganda after Yoweri Museveni, the subsequent head of state of Uganda, criticized Idi Amin's policies and invited them to return. According to Museveni, \"Gujaratis have played a lead role in Uganda's social and industrial development. I knew that this community can do wonders for my country and they have been doing it for last many decades.\" The Gujaratis have resurfaced in Uganda and helped rebuild the economy of East Africa, and are financially well settled.\n\n\n\n"}
{"id": "14551485", "url": "https://en.wikipedia.org/wiki?curid=14551485", "title": "Frankie Muse Freeman", "text": "Frankie Muse Freeman\n\nMarie Frankie Muse Freeman (née Muse; November 24, 1916 – January 12, 2018) was an American civil rights attorney, and the first woman to be appointed to the United States Commission on Civil Rights (1964–79), a federal fact-finding body that investigates complaints alleging discrimination. Freeman was instrumental in creating the Citizens' Commission on Civil Rights founded in 1982. She was a practicing attorney in State and Federal courts for nearly sixty years.\n\nIn 2007, Freeman was inducted in the International Civil Rights Walk of Fame at the Martin Luther King Jr. National Historic Site, Atlanta, Georgia, for her leadership role in the Civil Rights Movement.\n\nOn February 5, 2015, President Barack Obama appointed Freeman to serve as a Member of the Commission on Presidential Scholars.\n\nThe daughter of William Brown Muse and Maude Beatrice Smith Muse, Frankie came from a college-educated family. She was born and grew up in Danville, Virginia, where she attended Westmoreland School and learned to play the piano. At age sixteen, Muse enrolled in her mother's alma mater, Hampton Institute, which she attended between 1933 and 1936. In 1944, she was admitted to Howard University Law School and received a law degree in 1947. While a student at Howard Law, Freeman became a member of Epsilon Sigma Iota sorority, the first American legal sorority for women of color.\n\nIn 1948, after writing to several law firms and not hearing back from them, Muse decided to establish her own private practice. She began her practice with pro bono, divorce and criminal cases. After two years, Freeman began her work in civil rights when she became legal counsel to the NAACP legal team that filed suit against the St. Louis Board of Education in 1949. In 1954, Freeman was the lead attorney for the landmark NAACP case \"Davis et al. v. the St. Louis Housing Authority\", which ended legal racial discrimination in public housing with the city. Settling in St. Louis, Freeman worked as staff attorney for the St. Louis Land Clearance and Housing Authorities from 1956–70, first as associate general counsel and later as general counsel of the St. Louis Housing Authority.\n\nIn March 1964, she was nominated by President Lyndon Johnson as a member of the United States Commission on Civil Rights. On September 15, 1964, the Senate approved Freeman's nomination and she was officially appointed as the first black woman on the civil rights commission. Freeman was subsequently reappointed by presidents Richard Nixon, Gerald Ford and Jimmy Carter, and held the position until July 1979.\n\nShe was appointed as Inspector General for the Community Services Administration during Jimmy Carter's presidential administration in 1979. A year later, the Republican Ronald Reagan was elected president and demanded the resignation of Democratic inspectors general appointed by previous presidents.\n\nFreeman returned to St. Louis, where she practiced law. In 1982, Freeman joined 15 other former high federal officials who formed a bipartisan Citizens Commission on Civil Rights, a group committed to ending racial discrimination and devising remedies that would counteract its harmful effects.\n\nAt age 90, she was still practicing law with Montgomery Hollie & Associates, L.L.C. in St. Louis, a three-attorney firm. She had numerous volunteer activities, such as adult Sunday school classes at Washington Tabernacle Missionary Baptist Church. She was on the board of the World Affairs Councils of America, St. Louis, with the mission to promote understanding, engagement, relationships, and leadership in world affairs.\n\nIn 2003, she published her memoir, \"A Song of Faith and Hope\". She was the 14th National President of Delta Sigma Theta Sorority, Incorporated. She turned 100 in November 2016.\n\nFreeman was a Trustee Emeritus of the Board of Trustees of Howard University, past Chairman of the Board of Directors of the National Council on Aging, Inc. and the National Urban League of Metropolitan St. Louis. She was also a board member of the United Way of Greater St. Louis, the Metropolitan Zoological Park and Museum District, and the St. Louis Center for International Relations.\n\nSister Freeman had the honor of having a statue erected in downtown St. Louis in Kiener Plaza, at 500 Chestnut Street, with an unveiling date of November 21, 2017. The honor was presented by the NAACP and had many patrons to include Washington Tabernacle Missionary Baptist Church where Sister Freeman was an active member.\n\n\n"}
{"id": "18518450", "url": "https://en.wikipedia.org/wiki?curid=18518450", "title": "Functional analysis (psychology)", "text": "Functional analysis (psychology)\n\nFunctional analysis in behavioral psychology is the application of the laws of operant and respondent conditioning to establish the relationships between stimuli and responses. To establish the function of operant behavior, one typically examines the \"four-term contingency\": first by identifying the motivating operations (EO or AO), then identifying the antecedent or trigger of the behavior, identifying the behavior itself as it has been operationalized, and identifying the consequence of the behavior which continues to maintain it.\n\nFunctional assessment in behavior analysis employs principles derived from the natural science of behavior analysis to determine the \"reason\", purpose, or motivation for a behavior. The most robust form of functional assessment is functional analysis, which involves the direct manipulation, using some experimental design (e.g., a multielement design or a reversal design) of various antecedent and consequent events and measurement of their effects on the behavior of interest; this is the only method of functional assessment that allows for demonstration of clear cause of behavior. \n\nFunctional analysis and consequence analysis are commonly used in certain types of psychotherapy to better understand, and in some cases change, behavior. It is particularly common in behavioral therapies such as behavioral activation, although it is also part of Aaron Beck's cognitive therapy. In addition, functional analysis modified into a behavior chain analysis is often used in dialectical behavior therapy.\n\nThere are several advantages to using functional analysis over traditional assessment methods. Firstly, behavioral observation is more reliable than traditional self-report methods. This is because observing the individual from an objective stand point in their regular environment allows the observer to observe both the antecedent and the consequence of the problem behavior. Secondly, functional analysis is advantageous as it allows for the development of behavioral interventions, either antecedent control or consequence control, specifically designed to reduce a problem behavior. Thirdly, functional analysis is advantageous for interventions for young children or developmentally delayed children with problem behaviors, who may not be able to answer self-report questions about the reasons for their actions.\n\nDespite these benefits, functional analysis also has some disadvantages. The first that no standard methods for determining function have been determined and meta-analysis shows that different methodologies appear to bias results toward particular functions as well as not effective in improving outcomes. Second, Gresham and colleagues (2004) in a meta-analytic review of JABA articles found that functional assessment did not produce greater effect sizes compared to simple contingency management programs. However, Gresham et al. combined the three types of functional assessment, of which descriptive assessment and indirect assessment have been reliably found to produce results with limited validity Third, although functional assessment has been conducted with a variety host of populations (i.e.) much of the current functional assessment research has been limited to children with developmental disabilities.\n\nThe Association for Behavioral and Cognitive Therapies (ABCT) also has an interest group in behavior analysis, which focuses on the use of behavior analysis in the school setting including functional analysis.\n\nDoctoral level behavior analysts who are psychologists belong to the American Psychological Association's division 25 – Behavior analysis. APA offers a diplomate in behavioral psychology and school psychology both of which focus on the use of functional analysis in the school setting.\n\nThe World Association for Behavior Analysis offers a certification for clinical behavior therapy and behavioral consultation, which covers functional analysis.\n\nThe UK Society for Behaviour Analysis also provides a forum for behavior analysts for accreditation, professional development, continuing education and networking, and serves as an advocate body in public debate on issues relating to behavior analysis. The UK-SBA promotes the ethical and effective application of the principles of behavior and learning to a wide range of areas including education, rehabilitation and health care, business and the community and is committed to maintaining the availability of high-quality evidence-based professional behavior analysis practice in the UK. The society also promotes and supports the academic field of behavior analysis with in the UK both in terms of university-based training and research, and theoretical develop.\n\n"}
{"id": "1973470", "url": "https://en.wikipedia.org/wiki?curid=1973470", "title": "Fuzzy concept", "text": "Fuzzy concept\n\nA fuzzy concept is a concept of which the boundaries of application can vary considerably according to context or conditions, instead of being fixed once and for all. This means the concept is vague in some way, lacking a fixed, precise meaning, without however being unclear or meaningless altogether. It has a definite meaning, which can be made more precise only through further elaboration and specification - including a closer definition of the context in which the concept is used. The study of the characteristics of fuzzy concepts and fuzzy language is called \"fuzzy semantics\". The inverse of a \"fuzzy concept\" is a \"crisp concept\" (i.e. a precise concept). \n\nA fuzzy concept is understood by scientists as a concept which is \"to an extent applicable\" in a situation. That means the concept has \"gradations\" of significance or \"unsharp\" (variable) boundaries of application. A fuzzy statement is a statement which is true \"to some extent\", and that extent can often be represented by a scaled value. The best known example of a fuzzy concept around the world is an amber traffic light, and indeed fuzzy concepts are widely used in traffic control systems. The term is also used these days in a more general, popular sense - in contrast to its technical meaning - to refer to a concept which is \"rather vague\" for any kind of reason.\n\nIn the past, the very idea of reasoning with fuzzy concepts faced considerable resistance from academic elites. They did not want to endorse the use of imprecise concepts in research or argumentation. Yet although people might not be aware of it, the use of fuzzy concepts has risen gigantically in all walks of life from the 1970s onward. That is mainly due to advances in electronic engineering, fuzzy mathematics and digital computer programming. The new technology allows very complex inferences about \"variations on a theme\" to be anticipated and fixed in a program. \n\nThe new neuro-fuzzy computational methods make it possible, to identify, to measure and respond to fine gradations of significance, with great precision. It means that practically useful concepts can be coded and applied to all kinds of tasks, even if, ordinarily, these concepts are never precisely defined. Nowadays engineers, statisticians and programmers often represent fuzzy concepts mathematically, using fuzzy logic, fuzzy values, fuzzy variables and fuzzy sets.\n\nProblems of vagueness and fuzziness have probably always existed in human experience. The boundary between different things can appear blurry. Sometimes people have to think, when they are not in the best frame of mind to do it, or, they have to talk about something out there, which just isn't sharply defined. Across time, however, philosophers and scientists began to reflect about those kinds of problems, in much more systematic ways.\n\nThe ancient Sorites paradox first raised the logical problem of how we could exactly define the threshold at which a change in quantitative gradation turns into a qualitative or categorical difference. With some physical processes this threshold is relatively easy to identify. For example, water turns into steam at 100 °C or 212 °F (the boiling point depends partly on atmospheric pressure, which decreases at higher altitudes). \n\nWith many other processes and gradations, however, the point of change is much more difficult to locate, and remains somewhat vague. Thus, the boundaries between qualitatively different things may be \"unsharp\": we know that there are boundaries, but we cannot define them exactly. \n\nAccording to the modern idea of the continuum fallacy, the fact that a statement is to an extent vague, does not automatically mean that it is invalid. The problem then becomes one of how we could ascertain the kind of validity that the statement does have.\n\nThe Nordic myth of Loki's wager suggested that concepts that lack precise meanings or precise boundaries of application cannot be usefully discussed at all. However, the 20th century idea of \"fuzzy concepts\" proposes that \"somewhat vague terms\" can be operated with, since we can explicate and define the variability of their application, by assigning numbers to gradations of applicability. This idea sounds simple enough, but it had large implications.\n\nThe intellectual origins of the species of fuzzy concepts as a logical category have been traced back to a diversity of famous and less well-known thinkers, including (among many others) Eubulides, Plato, Cicero, Georg Wilhelm Friedrich Hegel, Karl Marx and Friedrich Engels, Friedrich Nietzsche, Hugh MacColl, Charles S. Peirce, Max Black, Jan Łukasiewicz, Emil Leon Post, Alfred Tarski, Georg Cantor, Nicolai A. Vasiliev, Kurt Gödel, Stanisław Jaśkowski and Donald Knuth. \n\nAcross at least two and a half millennia, all of them had something to say about graded concepts with unsharp boundaries. This suggests at least that the awareness of the existence of concepts with \"fuzzy\" characteristics, in one form or another, has a very long history in human thought. Quite a few logicians and philosophers have also tried to \"analyze\" the characteristics of fuzzy concepts as a recognized species, sometimes with the aid of some kind of many-valued logic or substructural logic.\n\nAn early attempt in the post-WW2 era to create a theory of sets where set membership is a matter of degree was made by Abraham Kaplan and Hermann Schott in 1951. They intended to apply the idea to empirical research. Kaplan and Schott measured the degree of membership of empirical classes using real numbers between 0 and 1, and they defined corresponding notions of intersection, union, complementation and subset. However, at the time, their idea \"fell on stony ground\". J. Barkley Rosser Sr. published a treatise on many-valued logics in 1952, anticipating \"many-valued sets\". Another treatise was published in 1963 by Aleksandr A. Zinov'ev and others\n\nIn 1964, the American philosopher William Alston introduced the term \"degree vagueness\" to describe vagueness in an idea that results from the absence of a definite cut-off point along an implied scale (in contrast to \"combinatory vagueness\" caused by a term that has a number of logically independent conditions of application). \n\nThe German mathematician Dieter Klaua published a German-language paper on fuzzy sets in 1965, but he used a different terminology (he referred to \"many-valued sets\", not \"fuzzy sets\").\n\nTwo popular introductions to many-valued logic in the late 1960s were by Robert J. Ackermann and Nicholas Rescher respectively. Rescher’s book includes a bibliography on fuzzy theory up to 1965, which was extended by Robert Wolf for 1966-1974. Haack provides references to significant works after 1974. Bergmann provides a more recent (2008) introduction to fuzzy reasoning.\n\nUsually the Iranian-born American computer scientist Lotfi A. Zadeh (1921-2017) is credited with inventing the specific idea of a \"fuzzy concept\" in his seminal 1965 paper on fuzzy sets, because he gave a formal mathematical presentation of the phenomenon that was widely accepted by scholars.<ref>Lotfi A. Zadeh, \"Fuzzy sets\". In: \"Information and Control\", Vol. 8, June 1965, pp. 338–353.\n\n"}
{"id": "223160", "url": "https://en.wikipedia.org/wiki?curid=223160", "title": "Gantt chart", "text": "Gantt chart\n\nA Gantt chart is a type of bar chart that illustrates a project schedule, named after its inventor, Henry Gantt (1861–1919), who designed such a chart around the years 1910–1915. Modern Gantt charts also show the dependency relationships between activities and current schedule status.\n\nA Gantt chart is a type of bar chart that illustrates a project schedule. This chart lists the tasks to be performed on the vertical axis, and time intervals on the horizontal axis. The width of the horizontal bars in the graph shows the duration of each activity. Gantt charts illustrate the start and finish dates of the terminal elements and summary elements of a project. Terminal elements and summary elements constitute the work breakdown structure of the project. Modern Gantt charts also show the dependency (i.e., precedence network) relationships between activities. Gantt charts can be used to show current schedule status using percent-complete shadings and a vertical \"TODAY\" line as shown here.\n\nGantt charts are sometimes equated with bar charts.\n\nGantt charts are usually created initially using an \"early start time approach\", where each task is scheduled to start immediately when its prerequisites are complete. This method maximizes the float time available for all tasks.\n\nAlthough now regarded as a common charting technique, Gantt charts were considered revolutionary when first introduced. The first known tool of this type was developed in 1896 by Karol Adamiecki, who called it a \"harmonogram\". Adamiecki did not publish his chart until 1931, however, and only in Polish, which limited both its adoption and recognition of his authorship.\n\nIn 1912, published what would be considered Gantt charts while discussing a construction project. It appears that Schürch's charts were not notable but rather routine in Germany at the time they were published. The prior development leading to Schürch's work is unknown. Unlike later Gantt charts, Schürch's charts did not display interdependencies, leaving them to be inferred by the reader. These were also static representations of a planned schedule.\n\nThe chart is named after Henry Gantt (1861–1919), who designed his chart around the years 1910–1915.\n\nOne of the first major applications of Gantt charts was by the United States during World War I, at the instigation of General William Crozier.\n\nThe earliest Gantt charts were drawn on paper and therefore had to be redrawn entirely in order to adjust to schedule changes. For many years, project managers used pieces of paper or blocks for Gantt chart bars so they could be adjusted as needed. Gantt's collaborator, Walter Polakov introduced Gantt charts to the Soviet Union in 1929 when he was working for the Supreme Soviet of the National Economy. They were used in developing the First Five Year Plan, supplying Russian translations to explain their use..\n\nIn the 1980s, personal computers allowed widespread creation of complex and elaborate Gantt charts. The first desktop applications were intended mainly for project managers and project schedulers. With the advent of the Internet and increased collaboration over networks at the end of the 1990s, Gantt charts became a common feature of web-based applications, including collaborative groupware. By 2012, almost all Gantt charts were made by software which can easily adjust to schedule changes.\n\nIn 1999, Gantt charts were identified as \"one of the most widely used management tools for project scheduling and control\".\n\nIn the following table there are seven tasks, labeled \"a\" through \"g\". Some tasks can be done concurrently (\"a\" and \"b\") while others cannot be done until their predecessor task is complete (\"c\" and \"d\" cannot begin until \"a\" is complete). Additionally, each task has three time estimates: the optimistic time estimate (\"O\"), the most likely or normal time estimate (\"M\"), and the pessimistic time estimate (\"P\"). The expected time (\"T\") is estimated using the for the time estimates, using the formula (\"O\" + 4\"M\" + \"P\") ÷ 6.\n\nOnce this step is complete, one can draw a Gantt chart or a network diagram.\n\nIn a progress Gantt chart, tasks are shaded in proportion to the degree of their completion: a task that is 60% complete would be 60% shaded, starting from the left. A vertical line is drawn at the time index when the progress Gantt chart is created, and this line can then be compared with shaded tasks. If everything is on schedule, all task portions left of the line will be shaded, and all task portions right of the line will not be shaded. This provides a visual representation of how the project and its tasks are ahead or behind schedule.\n\n\"Linked Gantt charts\" contain lines indicating the dependencies between tasks. However, linked Gantt charts quickly become cluttered in all but the simplest cases. Critical path network diagrams are superior to visually communicate the relationships between tasks. However, Gantt charts are often preferred over network diagrams because Gantt charts are easily interpreted without training, whereas critical path diagrams require training to interpret. Gantt chart software typically provides mechanisms to link task dependencies, although this data may or may not be visually represented. Gantt charts and network diagrams are often used for the same project, both being generated from the same data by a software application.\n\n\n"}
{"id": "9373204", "url": "https://en.wikipedia.org/wiki?curid=9373204", "title": "General set theory", "text": "General set theory\n\nGeneral set theory (GST) is George Boolos's (1998) name for a fragment of the axiomatic set theory Z. GST is sufficient for all mathematics not requiring infinite sets, and is the weakest known set theory whose theorems include the Peano axioms.\n\nThe ontology of GST is identical to that of ZFC, and hence is thoroughly canonical. GST features a single primitive ontological notion, that of set, and a single ontological assumption, namely that all individuals in the universe of discourse (hence all mathematical objects) are sets. There is a single primitive binary relation, set membership; that set \"a\" is a member of set \"b\" is written \"a\"∈\"b\" (usually read \"\"a\" is an element of \"b\"\").\n\nThe symbolic axioms below are from Boolos (1998: 196), and govern how sets behave and interact. The natural language versions of the axioms are intended to aid the intuition. The background logic is first order logic with identity.\n1) Axiom of Extensionality: The sets \"x\" and \"y\" are the same set if they have the same members.\nThe converse of this axiom follows from the substitution property of equality.\n\n2) Axiom Schema of Specification (or \"Separation\" or \"Restricted Comprehension\"): If \"z\" is a set and formula_2 is any property which may be satisfied by all, some, or no elements of \"z\", then there exists a subset \"y\" of \"z\" containing just those elements \"x\" in \"z\" which satisfy the property formula_2. The restriction to \"z\" is necessary to avoid Russell's paradox and its variants. More formally, let formula_4 be any formula in the language of GST in which \"x\" is free and \"y\" is not. Then all instances of the following schema are axioms:\n<br>\n3) Axiom of Adjunction: If \"x\" and \"y\" are sets, then there exists a set \"w\", the \"adjunction\" of \"x\" and \"y\", whose members are just \"y\" and the members of \"x\".\n\n\"Adjunction\" refers to an elementary operation on two sets, and has no bearing on the use of that term elsewhere in mathematics, including in category theory.\n\nGST is the fragment of Z obtained by omitting the axioms Union, Power Set, Elementary Sets (essentially Pairing) and Infinity, then taking Adjunction, a theorem of Z, as an axiom. The result is a first order theory.\n\nSetting φ(\"x\") in \"Separation\" to \"x\"≠\"x\", and assuming that the domain is nonempty, assures the existence of the empty set. \"Adjunction\" implies that if \"x\" is a set, then so is formula_7. Given \"Adjunction\", the usual construction of the successor ordinals from the empty set can proceed, one in which the natural numbers are defined as formula_8 (see Peano's axioms). More generally, given any model \"M\" of ZFC, the collection of hereditarily finite sets in \"M\" will satisfy the GST axioms. Therefore, GST cannot prove the existence of even a countable infinite set, that is, of a set whose cardinality is ℵ. Even if GST did afford a countably infinite set, GST could not prove the existence of a set whose cardinality is formula_9, because GST lacks the axiom of power set. Hence GST cannot ground analysis and geometry, and is too weak to serve as a foundation for mathematics.\n\nBoolos was interested in GST only as a fragment of Z that is just powerful enough to interpret Peano arithmetic. He never lingered over GST, only mentioning it briefly in several papers discussing the systems of Frege's \"Grundlagen\" and \"Grundgesetze\", and how they could be modified to eliminate Russell's paradox. The system Aξ'[δ] in Tarski and Givant (1987: 223) is essentially GST with an axiom schema of induction replacing Specification, and with the existence of an empty set explicitly assumed.\n\nGST is called STZ in Burgess (2005), p. 223. Burgess's theory ST is GST with Empty Set replacing the axiom schema of specification. That the letters \"ST\" also appear in \"GST\" is a coincidence.\n\nThe most remarkable fact about ST (and hence GST), is that these tiny fragments of set theory give rise to such rich metamathematics. While ST is a small fragment of the well-known canonical set theories ZFC and NBG, ST interprets Robinson arithmetic (Q), so that ST inherits the nontrivial metamathematics of Q. For example, ST is essentially undecidable because Q is, and every consistent theory whose theorems include the ST axioms is also essentially undecidable. This includes GST and every axiomatic set theory worth thinking about, assuming these are consistent. In fact, the undecidability of ST implies the undecidability of first-order logic with a single binary predicate letter.\n\nQ is also incomplete in the sense of Gödel's incompleteness theorem. Any axiomatizable theory, such as ST and GST, whose theorems include the Q axioms is likewise incomplete. Moreover, the consistency of GST cannot be proved within GST itself, unless GST is in fact inconsistent.\n\nGST is:\n\n\n"}
{"id": "9550415", "url": "https://en.wikipedia.org/wiki?curid=9550415", "title": "Generator (category theory)", "text": "Generator (category theory)\n\nIn category theory in mathematics a family of generators (or family of separators) of a category formula_1 is a collection formula_2 of objects, indexed by some set \"I\", such that for any two morphisms formula_3 in formula_1, if formula_5 then there is some \"i∈I\" and morphism formula_6, such that the compositions formula_7. If the family consists of a single object \"G\", we say it is a generator (or separator). \n\nGenerators are central to the definition of Grothendieck categories.\n\nThe dual concept is called a cogenerator or coseparator.\n\n\n"}
{"id": "23474670", "url": "https://en.wikipedia.org/wiki?curid=23474670", "title": "Geometric progression", "text": "Geometric progression\n\nIn mathematics, a geometric progression, also known as a geometric sequence, is a sequence of numbers where each term after the first is found by multiplying the previous one by a fixed, non-zero number called the \"common ratio\". For example, the sequence 2, 6, 18, 54, ... is a geometric progression with common ratio 3. Similarly 10, 5, 2.5, 1.25, ... is a geometric sequence with common ratio 1/2.\n\nExamples of a geometric sequence are powers \"r\" of a fixed number \"r\", such as 2 and 3. The general form of a geometric sequence is\n\nwhere \"r\" ≠ 0 is the common ratio and \"a\" is a scale factor, equal to the sequence's start value.\n\nThe \"n\"-th term of a geometric sequence with initial value \"a\" and common ratio \"r\" is given by\n\nSuch a geometric sequence also follows the recursive relation\n\nGenerally, to check whether a given sequence is geometric, one simply checks whether successive entries in the sequence all have the same ratio.\n\nThe common ratio of a geometric sequence may be negative, resulting in an alternating sequence, with numbers switching from positive to negative and back. For instance\nis a geometric sequence with common ratio −3.\n\nThe behaviour of a geometric sequence depends on the value of the common ratio.\nIf the common ratio is:\n\nGeometric sequences (with common ratio not equal to −1, 1 or 0) show exponential growth or exponential decay, as opposed to the linear growth (or decline) of an arithmetic progression such as 4, 15, 26, 37, 48, … (with common \"difference\" 11). This result was taken by T.R. Malthus as the mathematical foundation of his \"Principle of Population\".\nNote that the two kinds of progression are related: exponentiating each term of an arithmetic progression yields a geometric progression, while taking the logarithm of each term in a geometric progression with a positive common ratio yields an arithmetic progression.\n\nAn interesting result of the definition of a geometric progression is that for any value of the common ratio, any three consecutive terms \"a\", \"b\" and \"c\" will satisfy the following equation:\n\nwhere \"b\" is considered to be the \"geometric mean\" between \"a\" and \"c\".\n\nA geometric series is the sum of the numbers in a geometric progression. For example:\n\nLetting \"a\" be the first term (here 2), n be the number of terms (here 4), and \"r\" be the constant that each term is multiplied by to get the next term (here 5), the sum is given by:\n\nIn the example above, this gives:\n\nThe formula works for any real numbers \"a\" and \"r\" (except \"r\" = 1, which results in a division by zero). For example:\nSince the derivation (below) does not depend on \"a\" and \"r\" being real, it holds for complex numbers as well.\n\nTo derive this formula, first write a general geometric series as:\n\nWe can find a simpler formula for this sum by multiplying both sides\nof the above equation by 1 − \"r\", and we'll see that\n\nsince all the other terms cancel. If \"r\" ≠ 1, we can rearrange the above to get the convenient formula for a geometric series that computes the sum of n terms:\n\nIf one were to begin the sum not from k=1, but from a different value, say \"m\", then\n\nprovided formula_14 and formula_15 when formula_16.\n\nDifferentiating this formula with respect to \"r\" allows us to arrive at formulae for sums of the form\n\nFor example:\n\nFor a geometric series containing only even powers of \"r\" multiply by  1 − \"r\"  :\n\nThen\n\nEquivalently, take  \"r\"  as the common ratio and use the standard formulation.\n\nFor a series with only odd powers of \"r\"\n\nand\n\nAn exact formula for the generalized sum formula_23 when formula_24 is expanded by the Stirling numbers of the second kind as \n\nAn infinite geometric series is an infinite series whose successive terms have a common ratio. Such a series converges if and only if the absolute value of the common ratio is less than one ( < 1). Its value can then be computed from the finite sum formula\n\nSince:\n\nThen:\n\nFor a series containing only even powers of formula_29,\n\nand for odd powers only,\n\nIn cases where the sum does not start at \"k\" = 0,\n\nThe formulae given above are valid only for  < 1. The latter formula is valid in every Banach algebra, as long as the norm of \"r\" is less than one, and also in the field of \"p\"-adic numbers if  < 1. As in the case for a finite sum, we can differentiate to calculate formulae for related sums.\nFor example,\n\nThis formula only works for  < 1 as well. From this, it follows that, for  < 1,\n\nAlso, the infinite series 1/2 + 1/4 + 1/8 + 1/16 + ⋯ is an elementary example of a series that converges absolutely.\n\nIt is a geometric series whose first term is 1/2 and whose common ratio is 1/2, so its sum is\n\nThe inverse of the above series is 1/2 − 1/4 + 1/8 − 1/16 + ⋯ is a simple example of an alternating series that converges absolutely.\n\nIt is a geometric series whose first term is 1/2 and whose common ratio is −1/2, so its sum is\n\nThe summation formula for geometric series remains valid even when the common ratio is a complex number. In this case the condition that the absolute value of \"r\" be less than 1 becomes that the modulus of \"r\" be less than 1. It is possible to calculate the sums of some non-obvious geometric series. For example, consider the proposition\n\nThe proof of this comes from the fact that\nwhich is a consequence of Euler's formula. Substituting this into the original series gives\n\nThis is the difference of two geometric series, and so it is a straightforward application of the formula for infinite geometric series that completes the proof.\n\nThe product of a geometric progression is the product of all terms. If all terms are positive, then it can be quickly computed by taking the geometric mean of the progression's first and last term, and raising that mean to the power given by the number of terms. (This is very similar to the formula for the sum of terms of an arithmetic sequence: take the arithmetic mean of the first and last term and multiply with the number of terms.)\n\nProof:\n\nLet the product be represented by P:\n\nNow, carrying out the multiplications, we conclude that\n\nApplying the sum of arithmetic series, the expression will yield\n\nWe raise both sides to the second power:\n\nConsequently,\n\nwhich concludes the proof.\n\nBooks VIII and IX of Euclid's \"Elements\" analyzes geometric progressions (such as the powers of two, see the article for details) and give several of their properties.\n\n\n\n"}
{"id": "30562", "url": "https://en.wikipedia.org/wiki?curid=30562", "title": "Glossary of topology", "text": "Glossary of topology\n\nThis is a glossary of some terms used in the branch of mathematics known as topology. Although there is no absolute distinction between different areas of topology, the focus here is on general topology. The following definitions are also fundamental to algebraic topology, differential topology and geometric topology.\n\nAll spaces in this glossary are assumed to be topological spaces unless stated otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are some facts about submaximality as a property of topological spaces:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13291", "url": "https://en.wikipedia.org/wiki?curid=13291", "title": "Habitus (sociology)", "text": "Habitus (sociology)\n\nHabitus () is ingrained habits, skills, and dispositions. It is the way that individuals perceive the social world around them and react to it. These dispositions are usually shared by people with similar backgrounds (such as social class, religion, nationality, ethnicity, education, profession etc.). The habitus is acquired through imitation (\"mimesis\") and is the reality that individuals are socialized, which includes their individual experience and opportunities. Thus, the habitus represents the way group culture and personal history shape the body and the mind, and as a result, shape present social actions of an individual.\n\nPierre Bourdieu suggested that the habitus consists of both the \"hexis\" (the tendency to hold and use one's body in a certain way, such as posture and accent) and more abstract mental habits, schemes of perception, classification, appreciation, feeling, and action. These schemes are not mere habits: Bourdieu suggested they allow individuals to find new solutions to new situations without calculated deliberation, based on their gut feelings and intuitions, which Bourdieu believed were collective and socially shaped. These attitudes, mannerisms, tastes, moral intuitions and habits have influence on the individual's life chances, so the habitus not only is structured by an individual's objective past position in the social structure but also structures the individual's future life path. Pierre Bourdieu argued that the reproduction of the social structure results from the habitus of individuals (Bourdieu, 1987).\n\nThe notion of habitus is extremely influential (with 400,000 Google Scholar publications using it), yet it also evoked criticism for its alleged determinism, as Bourdieu compared social actors to \"automata\" (while relying on Leibniz's theory of Monads).\n\nThe concept of habitus has been used as early as Aristotle but in contemporary usage was introduced by Marcel Mauss and later Maurice Merleau-Ponty. However, it was Pierre Bourdieu who turned it into a cornerstone of his sociology, and used it to address the sociological problem of agency and structure: the habitus is shaped by structural position and generates action, thus when people act and demonstrate agency they simultaneously reflect and reproduce social structure. Bourdieu elaborated his theory of the habitus while borrowing ideas on cognitive and generative schemes from Noam Chomsky and Jean Piaget dependency on history and human memory. For instance, a certain behaviour or belief becomes part of a society's structure when the original purpose of that behaviour or belief can no longer be recalled and becomes socialized into individuals of that culture.\n\nLoïc Wacquant wrote that habitus is an old philosophical notion, originating in the thought of Aristotle, whose notion of \"hexis\" (\"state\") was translated into \"habitus\" by the Medieval Scholastics. Bourdieu first adapted the term in his 1967 postface to Erwin Panofsky's \"Gothic Architecture and Scholasticism\". The term was earlier used in sociology by Norbert Elias in \"The Civilizing Process\" (1939) and in Marcel Mauss's account of \"body techniques\" (techniques du corps). The concept is also present in the work of Max Weber, Gilles Deleuze, and Edmund Husserl.\n\nMauss defined habitus as those aspects of culture that are anchored in the body or daily practices of individuals, groups, societies, and nations. It includes the totality of learned habits, bodily skills, styles, tastes, and other non-discursive knowledges that might be said to \"go without saying\" for a specific group (Bourdieu 1990:66-67)—in that way it can be said to operate beneath the level of rational ideology.\n\nAccording to Bourdieu, habitus is composed of:\n\nThe term has also been adopted in literary criticism, adapting from Bourdieu's usage of the term. For example, Joe Moran's examination of authorial identities in \"Star Authors: Literary Celebrity in America\" uses the term in discussion of how authors develop a habitus formed around their own celebrity and status as authors, which manifests in their writing.\n\nBourdieu's principle of habitus is interwoven with the concept of structuralism in literary theory. Peter Barry explains, \"in the structuralist approach to literature there is a constant movement away from interpretation of the individual literary work and a parallel drive towards understanding the larger structures which contain them\" (2009, p. 39). There is therefore a strong desire to understand the larger influencing factors which makes an individual literary work. As Bourdieu explains, habitus \"are structured structures, generative principles of distinct and distinctive practices – what the worker eats, and especially the way he eats it, the sport he practices and the way he practices it, his political opinions and the way he expresses them are systematically different from the industrial proprietor's corresponding activities / habitus are also structuring structures, different classifying schemes classification principles, different principles of vision and division, different tastes. Habitus make different differences; they implement distinctions between what is good and what is bad, what is right and what is wrong, between what is distinguished and what is vulgar, and so on, but they are not the same. Thus, for instance, the same behaviour or even the same good can appear distinguished to one person, pretentious to someone else, and cheap or showy to yet another\" (Bourdieu, 1996). As a result, habitus may be employed in literary theory in order to understand those larger, external structures which influence individual theories and works of literature.\n\nBody habitus (or \"bodily habitus\") is the medical term for physique, and is categorized as either endomorphic (relatively short and stout), ectomorphic (relatively long and thin) or mesomorphic (average dimensions). In this sense, habitus has in the past been interpreted as the physical and constitutional characteristics of an individual, especially as related to the tendency to develop a certain disease. For example, \"Marfanoid bodily habitus\".\n\n\n"}
{"id": "306308", "url": "https://en.wikipedia.org/wiki?curid=306308", "title": "Honesty", "text": "Honesty\n\nHonesty is a facet of moral character that connotes positive and virtuous attributes such as integrity, truthfulness, straightforwardness, including straightforwardness of conduct, along with the absence of lying, cheating, theft, etc. Honesty also involves being trustworthy, loyal, fair, and sincere.\n\nHonesty is valued in many ethnic and religious cultures.\n\"Honesty is the best policy\" is a proverb of Benjamin Franklin, while the quote \"Honesty is the first chapter in the book of wisdom\" is attributed to Thomas Jefferson, as used in a letter to Nathaniel Macon. April 30 is national Honesty Day in the United States.\n\nWilliam Shakespeare famously describes honesty as an attribute people leave behind when he wrote that \"no legacy is so rich as honesty\" in act 3 scene 5 of \"All's Well that Ends Well.\"\n\nOthers have noted, however, that \"[t]oo much honesty might be seen as undisciplined openness\". For example, individuals may be perceived as being \"too honest\" if they honestly express the negative opinions of others, either without having been asked their opinion, or having been asked in a circumstance where the response would be trivial.\n\nMerriam-Webster defines honesty as \"fairness and straightforwardness of conduct\" or \"adherence to the facts.\"\n\nThe Oxford English Dictionary defines honesty as \"the quality of being honest.\" Honest is, in turn, defined as \"Free of deceit; truthful and sincere...Morally correct or virtuous...(attributive) Fairly earned, especially through hard work...(of an action) done with good intentions even if unsuccessful or misguided...(attributive) Simple, unpretentious, and unsophisticated.\n"}
{"id": "13033965", "url": "https://en.wikipedia.org/wiki?curid=13033965", "title": "Institutional logic", "text": "Institutional logic\n\nInstitutional logic is a core concept in sociological theory and organizational studies. It focuses on how broader belief systems shape the cognition and behavior of actors.\n\nFriedland and Alford (1991) defined Institutions as \"both supraorganizational patterns of activity by which individuals and organizations produce and reproduce their material subsistence and organize time and space. They are also symbolic systems, ways of ordering reality, thereby rendering experience of time and space meaningful\". Thornton and Ocasio (1999: 804) define institutional logics as \"the socially constructed, historical patterns of material practices, assumptions, values, beliefs, and rules by which individuals produce and reproduce their material subsistence, organize time and space, and provide meaning to their social reality\".\n\nFocusing on macro-societal phenomena, Friedland and Alford (1991: 232) identified several key Institutions: the Capitalist market, bureaucratic state, democracy, nuclear family, and Christianity that are each guided by a distinct institutional logic. Thornton (2004) revised Friedland and Alford’s (1991) inter-institutional scheme to six institutional orders, i.e., the market, the corporation, the professions, the state, the family, and religions. More recently, Thornton, Ocasio and Lounsbury (2012), in more fully fleshing out the institutional logic perspective, added community as another key institutional order. This revision to a theoretically abstract and analytically distinct set of ideal types makes it useful for studying multiple logics in conflict and consensus, the hybridization of logics, and institutions in other parts of society and the world. While building on Friedland and Alford’s scheme, the revision addresses the confusion created by conflating institutional sectors with ideology (democracy) and means of organization (bureaucracy), variables that can be characteristic several different institutional sectors. The institutional logic of Christianity leaves out other religions in the US and other religions that are dominant in other parts of the world. Thornton and Ocasio (2008) discuss the importance of not confusing the ideal types of the inter-institutional system with a description of the empirical observations in a study—that is to use the ideal types as meta theory and method of analysis.\n\nOrganizational theorists operating within the new institutionalism (see also institutional theory) have begun to develop the institutional logics concept by empirically testing it. One variant emphasizes how logics can focus the attention of key decision-makers on a particular set of issues and solutions (Ocasio, 1997), leading to logic-consistent decisions (Thornton, 2002). A fair amount of research on logics has focused on the importance of dominant logics and shifts from one logic to another (e.g., Lounsbury, 2002; Thornton, 2002; Suddaby & Greenwood, 2005). Haveman and Rao (1997) showed how the rise of Progressive thought enabled a shift in savings and loan organizational forms in the U.S. in the early 20th century. Scott et al. (2000) detailed how logic shifts in healthcare led to the valorization of different actors, behaviors and governance structures. Thornton and Ocasio (1999) analyzed how a change from professional to market logics in U.S. higher education publishing led to corollary changes in how executive succession was carried out.\n\nWhile much earlier work focused on ambiguity as a result of multiple and conflicting institutional logics, at the levels of analysis of society and individual roles, Friedland and Alford (1991:248-255) discussed in theory multiple and competing logics at the macro level of analysis. Recent empirical research, inspired by the work of Bourdieu, is developing a more pluralistic approach by focusing on multiple competing logics and contestation of meaning. By focusing on how some fields are composed of multiple logics, and thus, multiple forms of institutionally-based rationality, institutional analysts can provide new insight into practice variation and the dynamics of practice. Multiple logics can create diversity in practice by enabling variety in cognitive orientation and contestation over which practices are appropriate. As a result, such multiplicity can create enormous ambiguity, leading to logic blending, the creation of new logics, and the continued emergence of new practice variants. Thornton, Jones, and Kury (2005) showed how competing logics may never resolve but share the market space as in the case of architectural services.\n\nRecent research has also documented the co-existence or potential conflict of multiple logics within particular organizations. Zilber (2002), for example, described the organizational consequences of a shift from one logic to another within an Israeli rape crisis center, in which new organization members reshaped the center and its practices to reflect a new dominant logic that they have carried into the organization. Tilcsik (2010) documented a logic shift in a post-Communist government agency, describing a conflict between the agency's old guard (carriers of the logic of Communist state bureaucracies) and its new guard (carriers of a market logic). This study shows that, paradoxically, an intra-organizational group's efforts to resist a particular logic might in fact open the organization's door to carriers of that very logic. Almandoz (2012) examined the embeddedness of new local banks' founding teams in a community logic or a financial logic, linking institutional logics to new banking venture's establishment and entrepreneurial success. As these studies demonstrate, the institutional logics perspective offers valuable insights into important intra-organizational processes affecting organizational practices, change, and success. These studies represent an effort to understand institutional complexity due to conflicting or inconsistent logics within particular organizations, a situation that might results from the entry of new organization members or the layering (or \"sedimentation\") of new organizational imprints upon old ones over time.\n\n\n"}
{"id": "29306389", "url": "https://en.wikipedia.org/wiki?curid=29306389", "title": "Jon D. Williams Cotillions", "text": "Jon D. Williams Cotillions\n\nJon D. Williams Cotillions® is a division of Jon D. Williams Social Education Programs®. Jon D. Williams Social Education Programs® (JDWSEP®) are considered an authority in the field of social skills education which instructs over 10,000 students annually across the United States. JDWSEP® is a multi-divisional corporation that comprises four subsidiary branches. JDWSEP® instructs students in elementary and high school as well as college students, corporations, and their executives. JDWSEP® was founded in 1949 by Jon and Vivian Williams, it is headquartered in Denver, Colorado and has a full-time staff of 10 employees.\n\nThe JDW Cotillion program teaches elementary through high school students courtesy and consideration through the art of social dance. Students learn essential social skills, such as how to introduce themselves to adults and peers, table manners and etiquette, as well as how to treat each other with respect. Jon D. Williams Cotillions® offers a uniquely extensive educational program for children in over 50 cities across the United States.\n\nSurvival Etiquette Essentials® offers customized social skills programs designed for students from elementary school through high school. The education and structure of the program is specifically and uniquely tailored for schools, organizations, groups, families or individuals. The purpose of SEE is to provide students the opportunity to develop life skills that go beyond academics into areas of character, leadership, sense and civility. SEE challenges students to recognize that social skills build character and are essential in developing positive, successful relationships – \"wherever they go, whatever they do, whoever they meet\" (Jon D. Williams III).\n\nExecutive Social Presentation® was created to instruct individuals on how to effectively excel and succeed in business and personal relationships. Social skills represent an essential part of a person's education and play an instrumental role in establishing and nurturing relationships — which is the foundation for achieving success. Comprehensive programs combine instruction on social conduct and professional skills with the art of corporate etiquette. ESP®'s cadre of courses includes The Business of Courtesy, The Art of Effective Communication, Developing Successful Relationships, Customer & Employee Relations, Appearance & Dress, Interview Skills, Dining Etiquette & Entertaining, and Diversity Awareness & Cultural Competence.\n\nThe National Center for Social Education® offers certification in social skills education to entrepreneurs and others who wish to offer this type of training to others. NCSE® has instructed individuals who wish to become consultants and instruct social skills education classes. School teachers have benefited who wish to instruct classes in social skills education in schools, for after-school extracurricular activities, or on a private individual or group consulting basis. Corporate training from NCSE® is extremely valuable for businesses that wish to instruct social skills education internally for the benefit of other employees, or for the benefit of customer relations and sales.\n\nOver the last 65 years Jon D. Williams Cotillions® has redefined the term cotillion by rendering the traditional 'formal ball' to be applicable in the 21st century. In a world where kindness and civility are often overlooked it is the increasingly important responsibility of cotillion to be the forum where young people interact with one another. Jon D. Williams Cotillions® has effectively redefined cotillion \"as a group of young people assembled to develop, appreciate and respect their role in society through social skills and dance education\" (Jon D. Williams III).\n\nJon D. Williams Social Education Programs® are responsible for establishing a program for the cadets of the United States Air Force Academy, a program originally established in 1955 by Jon and Vivian Williams. Classes are currently hosted onsite for over 1,000 cadets a year at the United States Air Force Academy by a full-time resident who provides curriculum critical to a service members understanding that how they act is a direct reflection of the United States at home and abroad.\n\nJDWSEP® has a rich history behind its over 65 years in the field of social skills and dance education. The company’s founders, Jon and Vivian Williams, met in 1932. Jon Williams left medical school for the opportunity to study dance with Arthur Murray where he was advised to begin working with a talented blonde debutante named Vivian. Thus began the illustrious career of Jon and Vivian Williams as the number one national instructional team of Arthur Murray. Soon the perfect dance partners became partners in life and were married. After World War II, Jon was contacted by Fred Astaire and offered the position of setting up Fred Astaire Studios across the country. In 1949, now having a family and ready to settle down, the Williams decided to leave Astaire and establish their home and a new business in Colorado Springs, Colorado. Thus was born the first Jon D. Williams Cotillion® at the world-famous Broadmoor Hotel. JDW Social Education Programs® have established and incomparable reputation across the nation, with Jon D. Williams III, son of the founder and president of the organization, now leading the next generation.\n\n“How you act is not only a reflection of your character; it is also a demonstration of your education” – Jon D. Williams III\n\n“Wherever you go, whatever you do, whoever you meet, social skills are essential for success” – Jon D. Williams III\n\nJon D. Williams Cotillions donates over $100,000 annually in scholarship programs and community organizations to benefit cities where JDWC Signatory Cotillions® are hosted. Each of the six full-time instructors offer pro-bono programs for at risk children and contribute time to related charities.\n\nJDWSEP® has been a feature on numerous television, print and radio media sources. An abbreviated list includes; CBS Sunday Morning with Charles Osgood, ABC World Weekend News, WB2 News, FOX 31 News, The Denver Post, The Boston Globe, The Washington Post, Dallas Morning News, The Philadelphia Inquirer, The Greely Tribune, The Evergreen Canyon Courier, Kerrville Daily Times, 710 AM KNUS, 710 AM KURV, 100.3 FM KIMN, 102.3 FM Spirit, 560 AM KLZ – Mom’s the Word.\n\n\n"}
{"id": "51907322", "url": "https://en.wikipedia.org/wiki?curid=51907322", "title": "London Education Classification", "text": "London Education Classification\n\nThe London Education Classification is a library classification and indexing thesaurus used at the UCL Institute of Education. It was devised by D.J. Foskett and Joy Foskett. It was devised to address deficiencies in general classification schemes in dealing with education. It was originally devised in 1963, and revised in 1974. It is a faceted classification, inspired by the work of S.R. Ranganathan and of the Classification Research Group.\n\nFoiskett and Foskett observe: \"The basic idea of facets is separate grouping of each major division of a subject.\". Using the British Education Index and the British National Bibliography, they divided educational terms into groups that are mutually exclusive. This forms the foundation of the classification sequence.\nIn the LEC main facets are indicated by a capital letter, and terms in facets by lower case letters. Letters are arranged so that vowels and consonants alternate, so that the eventual notation will appear as a syllable. e.g.\n\nThus, a work on \"Wikipedia : the missing manual / John Broughton\" would be shelved at shelved at\n\nAs originally implemented, the shelf arrangement produced by the classification was supplemented by an indexed card catalogue, where index entries were created for each element in the classmark, in reverse order from the classification. The example Foskett and Foskett gave was: \n\"Each entry refers the reader to the part of the classified section, Rid, where all entries on Secondary modern school are filed.\". The 1974 second edition of the scheme was supplemented by thesaurus cross referencing between narrow terms, broad terms and related terms.\n"}
{"id": "48622944", "url": "https://en.wikipedia.org/wiki?curid=48622944", "title": "Margaret Crittendon Douglass", "text": "Margaret Crittendon Douglass\n\nMargaret Crittendon Douglass (born 1822; year of death unknown) was a Southern white woman who served one month in jail in 1854 for teaching free black children to read in Norfolk, Virginia. Refusing to hire a defense attorney, she defended herself in court and later published a book about her experiences. The case drew public attention to the highly restrictive laws against black literacy in the pre-Civil War American South.\n\nDouglass was born in 1822 in Washington, D.C., but moved in her childhood to Charleston, South Carolina, where she married. By age fourteen, she had given birth to a daughter, Hannah Rosa. She also birthed a son, who died. Nothing is known about her husband. In 1845, she and her daughter relocated to Norfolk, Virginia, where she established herself in a modest apartment in a tenement neighborhood and made ends meet by running her own business as a seamstress and vest maker. She chose not to participate in social activities with her white neighbors in the nearby tenements, who she felt were \"not of the most refined class\". She notes in her memoir that this made her unpopular in the neighborhood. She described her life as \"frugal and retiring\", and that it was \"necessary for me to labor incessantly\".\n\nIn 1853 she paid a business visit to the barber shop of a free black man named Robinson. Mr. Robinson was a man of influence in the community of more than 1500 free people of color who lived in Norfolk at that time. His two young sons were busy studying a reading primer, so Douglass asked Robinson if there wasn't a school in town where free black children could learn to read. Robinson said that there was only the Sunday school at Christ Church where, he said, \"they didn't learn much.\" Douglass returned home and asked her teenaged daughter, Hannah Rosa, if she would like to tutor the boys. Hannah Rosa said she would, and the boys began to receive free reading lessons at the Douglass home, using the books they received at church. Eventually, Robiinson took his sons back to the barber shop where he needed their help, and sent his two daughters instead. In her memoir, Douglass states that the two girls were intelligent, attentive, made rapid progress, and were \"a source of pleasure to us.\"\n\nA month later, Margaret and Hannah Rosa decided to open a small school for free black children in their home, and charge three dollars per student per quarter. Robinson spread word to the free black community, and the school was deluged with applications. For 11 months, they taught 25 children, who Douglass described as \"well-behaved\" and \"anxious to be taught\". When one of her female students grew ill, Douglass made regular visits to her home, and arranged for and participated in the girl's funeral when she died. This, according to Douglass, was not approved of by other whites in the community.\n\nOn the morning of May 9, 1853, while Hannah Rosa and the children were assembled at their desks, two police officers stationed themselves at the front and back door of her house. They informed Margaret and Hannah Rosa that teaching free black children was illegal under Virginia law, then took the two women and the twenty-five terrified children on foot to the mayor's office. This 1849 law, like similar ones throughout the South, was the result of Southern white panic following Nat Turner's Rebellion in 1831. Under this law, it was an 'unlawful assembly' for black people to gather together to learn to read and write, punishable by whipping, and for any white person to instruct them, punishable by up to 100 dollars and six months in jail.\n\n\"You have a very large family\", Mayor Stubbs commented when she and the 25 children arrived at his office. He then asked her if she was aware of the law. Douglass responded that she didn't know teaching free black children was illegal, but that if it were illegal for her to teach the children it should also be illegal for the church to do it, since she was using the same books. The mayor dismissed the matter and sent everyone home, after assuring Douglass that nothing would happen to the black children or their families.\n\nOutside the courtroom, Douglass noted that she was met by a group of free black adults, parents of the children and their friends, waiting to hear the decision and offering to pay for any penalties or bail she might incur. \nShe gathered the children back at the school, gave them back their books and slates, and said goodbye. \"It was a sad parting\", she wrote in her memoir.\n\nAfter her daughter Hannah Rosa left Norfolk for New York on June 29, Douglas was left alone, receiving however frequent gifts of flowers from her former students who stopped by to see her.\n\nOf the outpouring of support she received from the black community. Douglas writes, \"In my opinion, those who call the Southern Negroes ungrateful are only those who never do anything to call forth that emotion...May I ask what gratitude they owe to those who would degrade them? What gratitude does that child owe to his own father, who would coldly sell him as a slave?\"\n\nAlthough she thought the case was now behind her, on July 13, she was served legal papers for a Grand Jury Indictment for her and her 17-year-old daughter, which stated: \"Each of them did unlawfully assemble with diver negros, for the purpose of instructing them to read and to write, and did instruct them to read and to write, contrary to the act of the General Assembly, in such case made and provided, and against the peace and dignity of the Commonwealth of Virginia'.\n\nHaving little money and 'little affection for lawyers', Douglass decided to defend herself. She did not mention the upcoming trial to her daughter, who was still in New York, until September 1, when she instructed her to stay until further notice. On November 11, she entered the crowded courtroom alone, wearing a black velvet dress, white kid gloves and a plain straw bonnet. 'Everyone present seemed to be confused, except myself,' she noted.\n\nShe called three male members of Christ's Church as her witnesses, two of them also lawyers, one who had signed the summons against her. The excitement in the courtroom when these three witnesses arrived was 'most intense', she noted. In her memoir, she states that these men, who she describes as part of the 'aristocracy' of Norfolk, were either lecturers at the church's Sunday school, or had wives and daughters who were, and had given the free black children of the community the same books she was using to teach them to read. The witnesses either denied teaching altogether or said they had only provided moral and religious instruction, but did not teach the black children how to read and write.\n\nIn her closing speech to the jury, Douglass described herself as a Southern woman, a former slave-holder. She said she was not against slavery, and strongly opposed to northern abolitionist interference, though she believed their 'principles are based on a religious foundation'. She believed, she said, in 'the duty of every Southerner, morally and religiously to instruct his slaves, that they may know their duties to their masters and their common God. Let the masters first do their duty to them, for they are still our slaves and servants, whether bond or free, and can be nothing else in our community.' She also challenged the indifference of the white population towards the situation of the free and enslaved black population, where there was misery, starvation, and harsh laws in place to control them, noting that it was illegal for more than two or three of them to assemble in one place, for whatever reason.\n\n'When they are sick, or in want, on whom does the duty devolve to seek them out and administer to their necessities? Does it fall upon you, gentlemen? Oh no, it is not expected that gentlemen will take the trouble to seek out a negro hut for the purpose of alleviating the wretchedness he may find within it. Why then persecute your benevolent ladies for doing that which you yourselves have so long neglected? Shall we treat our slaves with less compassion than we do the cattle in the field?\"\n\nShe also told the jury that although they had nothing to fear from the 'true blooded' negroes, she felt that those who had 'white blood in their veins' were 'presumptuous, treacherous, and revengeful.' \"Ask how that white blood got beneath those tawny skins,\" she suggested, \"and let nature herself account for the exhibition of these instincts. Blame the authors of this devilish mischief, but not the victims of it.\"\n\nIn closing, she offered that she was willing to go to prison if she had to, but that the black anti-literacy law, she said, was 'one of the most inhuman and unjust laws that ever disgraced the statute book of a civilized community.' \n\nWhen she rested her case, the judge asked if anyone wanted to speak in her favor. No one came forward.\n\nDouglass successfully argued for dropping the charges against Hannah Rosa, since she was still a minor. At the close of the trial, the jury deliberated for two days, found her guilty, and fined her one dollar. She then left for New York to retrieve her daughter.\n\nOn January 10, 1854, she was called back before Judge Baker for further sentencing.\n\nThe Judge noted that some people in Norfolk were opposed to the law, and chided her for 'the indiscreet freedom with which you spoke of the colored race in general'. 'Such opinions I regard as manifestly mischievous', he said. The Church officials, he said, were allowed to instruct the children because religious instruction is necessary for black people. Black literacy, however, is dangerous. He described the law as a \"matter of self-defense against Northern anti-slavery agitators who clog the mail with anti-slavery pamphlets to be distributed among Southern Negroes to induce them to cut our throats,\" and who \"scattered pocket handkerchiefs around with anti-slavery engravings to work upon the feelings and ignorance of our negroes, who otherwise would remain comfortable and happy\". He also admonished her for speaking directly and honestly in her own defense instead of hiring an attorney, which would have allowed her case to have been presented in a 'far more favorable light'. Her 'bold and open opposition', he said, 'is a matter not to be slightly regarded' and 'something more substantial is required in this case'.\n\n'For these reasons', he continued, 'as an example to all others in like cases disposed to offend, and in vindication of the policy and justness of our laws, which every individual should be taught to respect, the judgment of the Court is, in addition to the proper fine and costs, that you be imprisoned for the period of one month in the jail of this city'.\n\nDouglass served out her sentence. Having no where else to stay when she was released, the jailer and his wife put her up for two days before she moved with her daughter to Philadelphia.\n\nThe case received substantial media coverage at the time, both for and against Douglass and the anti-literacy law. Under the headline \"Her Own Lawyer,\" an editor in the \"Petersburg Daily Express\" wrote of how the courtroom was 'filled with persons anxious to witness the novel spectacle' of a woman defending herself and compared her speech making abilities to renowned feminists like Lucy Stone. Despite her disavowal of abolitionism, she was hailed as a heroine by abolitionists. William Lloyd Garrison, in his abolitionist newspaper, \"The Liberator\", noted that a Quaker woman in Norfolk had delivered a sermon on Douglass' behalf while Douglass was in jail and stated, \"The women are a great trouble to our Norfolk neighbors\", he said. \"If they want peace, they will have to expel all Christian women... from the city.\" \n\nThe editor of the Virginia newspaper \"The Argus\" wrote that although the Norfolk community was reluctant to put a white woman in prison, 'there rose a righteous indignation towards a person who would throw contempt in the face of our laws, and brave the imprisonment for 'the cause of humanity' '.\n\n'Let her depart hence with only one wish, that her presence will never be intruded upon us again. Let her seek her associates at the North, and with them commingle, but let us put a check to such mischievous views as fell from her lips last November, sentiments unworthy of a resident of the State, and in direct rebellion against our Constitution\", he wrote.\n\nIn the religious publication \"Covenanter: Devoted to the Principles of the Reformed Presbyterian Church\", (Volume 9, 1853) David Smith called upon the Presbyterian General Assembly to intercede with the U.S. government to \"secure for all its citizens the rights of teaching human beings to read the language of their country...\" He noted that the United States government had interceded when Presbyterian women had been imprisoned in foreign Catholic countries for passing out Bibles, but would not intervene in Douglas's case.\n\nThe case, Smith said, has \"exposed the country to the contempt and hissing of every civilized country on earth\".\n\nIn 1854, Douglass published her memoir of the event, \"Educational Laws of Virginia: The Personal Narrative of Mrs. Margaret Douglass, a Southern Woman, who was Imprisoned for One Month in the Common Jail of Norfolk, Under the Laws of Virginia, for the Crime of Teaching Free Colored Children to Read.\" In it, she describes the trial as well as the events that led up to it, and includes the speech she made in her own self-defense. As she did in her courtroom speech, she criticizes the black literacy and assembly laws as well as white indifference to the situation of blacks, but identifies herself as a Southern supporter of slavery, and a white supremacist:\n\n\"I have been a slaveholder myself, and, if circumstances rendered it necessary or practicable, I might be such again\".\n\nPointing out that many of the black and mixed race children she was teaching were the children of the same white men who aided in the prosecution against her, she lays some of the blame for the condition of black people on Southern white men who sexually abused enslaved black women. This, she believed, is the root of the anti-literacy law. \"How important then for these Southern Sultans that the objects of their criminal passions should be kept in utter ignorance and degradation\", she writes. She speaks of the silent frustration of white women who know of their husbands' 'tawny mistresses' and the powerlessness of black women, who have \"parents, brothers, sisters, a lover perhaps, all of whom suffer through and with her, and in whose hearts spring up roots of bitterness which are destined to grow into trees whose branches will sooner or later overshadow the whole land.\" She ends her memoir calling upon her white \"Southern sisters\" to remedy this situation:\n\n\"I know my Southern sisters well enough to believe that they will not much longer rest tamely under the influence of this damning curse. I have told them plainly of the evil. The remedy is in their hands\".\n\nLittle is known about Margaret Douglass's life after her trial.\n\nIn June 1865 the members of the free black community of Norfolk, Virginia petitioned the federal government to abolish the restrictive literacy and assembly laws that were still in place in their community. The law was abolished in 1867.\n\nDocuments and publications about Douglass's trial and her subsequent memoir have helped shed light on the era she lived in for scholars of American history, race relations, women's history, religion and law. One legal scholar wrote that with the case of Margaret Douglass: \"The modern era of Anglo-American Law had arrived early, and the principal actor had been, appropriately, a woman. Her jury speech had, appropriately, employed the female metaphor of the family, in which all the children deserved equality of treatment. It was a metaphor, and an idea, that would come to dominate the legal discourse of America.\" \n\n"}
{"id": "5708400", "url": "https://en.wikipedia.org/wiki?curid=5708400", "title": "Marked graph", "text": "Marked graph\n\nA marked graph is a Petri net in which every place has exactly one incoming arc, and exactly one outgoing arc. This means, that there can \"not\" be \"conflict\", but there can be \"concurrency\". Mathematically: formula_1. Marked graphs are used mostly to mathematically represent concurrently running operations, such as a multiprocessor machine's internal process state. This class of Petri nets gets the name from a popular way of representing them: as a graph where each place is an edge and each transition is a node. \n\nMarked graphs are mainly used to mathematically represent concurrent mechanisms, in order to be able to mathematically derive certain characteristics of the design.\n\nThis example presents a Marked Graph, where a process is forked at transition T1 and synchronised at T4. In between, two operations take place in non-deterministic fashion, T2 and T3. In fact, Petri nets are so much non-deterministic, that they may not take place at all. But the reason for having this non-deterministic property is not this, but to mimic real-life experiences which shows that parallel computing always means that it is impossible to determine which process/thread will finish first i.e. which operation(s) will execute faster. This can be due to waiting for I/O in real world, or just the different parameters given to the processes/threads.\n"}
{"id": "17219130", "url": "https://en.wikipedia.org/wiki?curid=17219130", "title": "Media theory of composition", "text": "Media theory of composition\n\nCommonly called new media theory or media-centered theory of composition, media theory focuses on how writing is created, keeping in mind particularly the tools and mediums used in the composition process. New media refers to a range of digital modes of communication, often incorporating a multi-modal mix of the visual or oral in addition to traditional text. Stemming from the rise of computers as word processing tools, media theorists now also examine the rhetorical strengths and weakness of different media, and the implications these have for literacy, author, and reader.\n\nThe meaning of the term 'new media' can be confusing and debated over. At times extended to mean any sort of media that is not purely written-text-based, it generally refers to any medium that is technologically 'advanced' from pure text. The broadness of the term is useful in that it allows for the multiple modes that can be encompassed by this definition, instead of being focused on the technical aspect that the term 'digital' would invite. With this in mind, though, terms like 'digital', 'hyper-textual', 'interactive', 'simulated', 'virtual', and 'networked' can often be helpful when thinking about what constitutes new media. However, there is often a false dichotomy drawn between the 'analogue' media and the 'new' media; media theory invites re-mediation of texts, which often result in a mix of mediums. Gunther Kress remarks on the new responsibilities of writers:\n\"In the new theory of representation, in the present technological context of electronic, multimodal, multimedia textual production, the task of text-makers is that of complex orchestration. Further, individuals are now seen as remakers, transformers, of sets of representational resources... .\"\n\nNoted scholar in the field, Cynthia Selfe has frequently commented on the exigency of incorporating new media in the writing classroom, noting its ability to make students rhetorically aware of the arguments that they commonly take for granted based on medium. She states,\n\"Composition teachers, language arts teachers, and other literacy specialists need to recognize that the relevance of technology in the English studies disciplines is not simply a matter of helping students work effectively with communication software and hardware, but, rather, also a matter of helping them to understand and to be able to assess – to pay attention to – the social, economic, and pedagogical implications of new communication technologies and technological initiatives that affect their lives.\" \n\nRichard Ohmann extends this argument, saying, \"Adults ignorant of computers will soon be as restricted as those who today are unable to read. Software will become the language of the future, and the dominant intellectual asset of the human race, so that an understanding of software will be a primary component of literacy in the electronic age\".\nHolding intersections with rhetorical theory, new media theory focuses on how different mediums work rhetorically within various contexts. It subscribes to Marshall McLuhan's coined phrase \"the medium is the message\", or rather, that a message's content is just as important as the medium that delivers it in affecting how the message is received. A large amount of new media theory's focus is also on how different mediums generate different thinking; the theory proposes that new mediums produce specific (often unique) rhetorical moves like multi-threaded thought processes through the use of interactive texts. Related to this, patterns of organization and production are topics of emphasis for new media theory, especially when thinking about the commonly non-sequential or thinking and writing of new media, particularly through the use of hyperlinks. Gunther Kress says about the complexity of new media composition, \"But this one person now has to understand the semiotic potentials of each mode–sound, visual, speech–and orchestrate them to accord with his or her design. Multimedia production requires high levels of competence based on knowledge of the operation of different modes, and highly developed design abilities to produce complex semiotic \"texts.\"\nMedia theory focuses on the effects that can come from utilizing new media, like new textual experiences and new ways of representing the world. One effect is the changing relationship between subjects and technologies, especially in relation to identity and community.\n\nThe earliest research that can be considered part of media theory involved the use of computers as word processors. However, with the advent of applications like HyperCard for the Apple Macintosh, the focus in media research shifted to hypertext's implications in the writing classroom.\nMuch research has been done regarding technological literacy, and questions of literacy in general. Richard Ohmann, as early as 1985, questions the focus on technological literacy appearing in schools at the time. Insisting that technological literacy is tied to socioeconomic class, Ohmann, and others like him began to research the possibility for educational ostracism because of a lack of access to technological tools. This contrasts starkly with much of the utopian visions of the technological classroom that others exhibited.\nCurrent research is being conducted multimodal composition, broadening composition to encompass video, video games, music, and other interactive media Digital Media.\n\nIdentity and the construction of such is an area of particular focus for new media theory. Individuals can control how they are represented online through personalized avatars and profiles, and the rhetorical moves behind this are often explored. Identity is an important aspect of authorship in many new media writings in this manner, then, especially in situations like Internet forums.\nMany instances of new media have difficulty controlling authorship, though, and new media theory does not always view this as a bad thing. Digital medias are in a constant state of flux (versus the relative fixity of traditional text) because several transformations may occur. For instance, in the case of email, messages may be replied to or forwarded several times, during which times previous messages may be revised or edited by various parties. Technologies that allow for simultaneous authoring of texts also produce this effect.\nAuthorship is further complicated by the refashioning of analogue texts or the \"remixing\" of several new media texts. Jay Bolter says,\n\"In the new theory of representation, in the present technological context of electronic, multimodal, multimedia textual production, the task of text-makers is that of complex orchestration. Further, individuals are now seen as remakers, transformers, of sets of representational resources.\"\nThose involved in new media theories have often needed to redefine \"author\" in the context of new media. Some theorists draw a difference between the authors of analogue texts as people who produce texts that readers interpret; in contrast, those who produce new media texts are seen as being in better alignment with the term \"experience designers\", because they create spaces within which readers make their own paths. This is particularly true for interactive and immersive texts. This produces even more debate over who is seen as an author, because in many new media texts, the reader plays a dual role of consumer and producer.\n\nMedia theory focuses on the agency that composing in new media can give writers. It is important not to simply add in new media as a problematic addendum to analogue writing; instead, teachers can take advantage of these \"new\" means by calling students to become both consumers or critics of new media as well as producers, as technology's role in society becomes ever more prominent. Giving students the resources to take part in discourses around and through new media gives them the power and agency to act in this digital world.\nThe agency gained through new media composition also provides a basis for social change. Gunther Kress remarks on the potential for teaching design via new media composition in the classroom:\nDesign takes for granted competence in the use of resources, but beyond that it requires the orchestration and remaking of these resources in the services of frameworks and models that express the maker's intentions in shaping the social and cultural environment. While critique looks at the present through the means of past production, design shapes the future through deliberate deployment of representational resources in the designer's interest.\nTeachers cite the ease of which students pick up on the techniques of composition and analysis of new media. The visually saturated and technologically heavy society of today means that students and writers will need to question the communications and compositions around them as rhetorical moves integral to the culture students live in. Their familiarity with and knowledge of new media often results in positive, conscious appropriation of media, and opens the door for discussions of repurposing within culture. Jay Bolter and Richard Grusin say:\nNo medium today, and certainly no single media event, seems to do its cultural work in isolation from other social and economic forces. What is new about new media comes from the particular ways in which they refashion older media and the ways in which older media refashion themselves to answer the challenges of new media.\nOne potentially useful method of teaching composition with media theory in mind is through the use of online classrooms. Massive Open Online Courses (MOOCs) and Online Writing Labs (OWLs) are becoming increasingly popular, especially with distance learning. These educational formats also typically deal with analogue texts, but provide technologically based resources for this composition process via digital feedback and revision techniques.\nMedia theory has particular potential for teaching basic writers or students whose native language is not English. As a \"literacy\" that \"translates\" better across languages and demographics, new media can serve to engage students who were previously uninterested in composition, as well as allow these students to feel like they have more expertise or agency in the composing process. Furthermore, it allows for more creativity and exploration of rhetorical power in composition as students learn the semiotic power of various modes of discourse through technology.\n\nBecause media theory focuses so much on specific, often technical aspects of writing, it has much room for overlap, and facilitates other theoretical composition pedagogies.\n\nFor instance, Writing Across the Curriculum (commonly known as WAC) focuses on the different ideologies, paradigms, and standards between various disciplines, especially when it comes to writing and how these play out within writing. Thus, WAC recommends an approach to teaching writing that emphasizes these differences and the rhetorical awareness that is needed to write to varying audiences. Media theory works well with WAC because it, too, emphasizes different and multiple literacies. Not only does it emphasize a multi-modal approach to writing, but it also emphasizes the fact that the writer will have to be aware of different audience's familiarity with technology. Furthermore, WAC can easily be combined with media theory because technology and digital writing are becoming more popular in all disciplines, so understanding the differences that come with various fields could potentially interact with the implications of using digital composition instead of traditional print.\n\nAnother theory that media theory is conducive to in composition is collaborative learning theory. Typically, this theory focuses on the construction of knowledge as a social act. Media theory aligns well with collaborative learning because, with the advancement of technology, writing can easily accommodate multiple authors. This can be exemplified through something as simple as an online forum, in which writers converse through text to come to conclusions. Another example is Wikipedia, the online encyclopedia where authorship is relatively open to the public, so various writers may inform others of their knowledge and build on others' to create a constantly evolving definition and explanation of a certain topic.\n\nMedia theory also works well with critical pedagogy and feminist theories of composition. These theories challenge traditional notions of hierarchies in relation to certain social groups, like race or gender, and how this affects writing. When in practice, media theory can break down hierarchies in several ways. The first way is tied directly to writing as a product. Critical and feminist theories value texts written in non-traditional ways (for example, narrative essay as a \"feminine\" writing style versus thesis-driven essay as a \"masculine\" writing style) and texts written by minorities. Media theory breaks down the hegemony that \"pure text\" has over other modalities by utilizing \"non-traditional\" methods and modes of writing through the use of technology. The second way media theory in practice breaks down hierarchies is tied to writing as a process. There is a democratization to media theory because everyone is involved in the creation and consumption of a text because of common features like public access and interactivity. In terms of concrete pedagogy, Massively Open Online Classrooms (MOOCs) work to break down hierarchies by encouraging learning in an informal setting with partially anonymous users, which potentially allows for minorities to let their voices be heard without worrying about discrimination.\n\nA main critique of media theory in practice deals with time constraints. Many First-Year Composition teachers complain that there is not enough time in the course to teach digital writing in addition to \"regular\" writing. This often results in the addition of new media production as a last-minute addendum, \"as a strategy for adding relevance or interest to a required course. Only rarely does that call address students as producers as well as consumers or critics\".\nRelated to this, teachers are also frequently concerned that they do not have the ability or knowledge required to teach writing with new media and multi-modality to students because they are often unfamiliar with constantly evolving technologies.\nA separate criticism is that new media use allows for more prevalent plagiarism. As more information is available on the Internet, it is much easier to simply \"copy-paste\" data into a new format. Furthermore, the questionable authorship of many digital texts complicates things both in terms of writing and citing works. Multi-media works are especially subject to concern over criticism as appropriation and remixing are rampant and even encouraged in composition classrooms and as debates over SOPA occur.\nMany word processing programs have also been seen to reinforce norms and hierarchies. For instance, the commonly used spelling and grammar checkers of certain programs can be seen as possibly halting the natural progression of language and the creative use of it by encouraging writers to stay within the realm of standard vernaculars. Furthermore, some cite the highly class-based symbolic nature of many digital word processing systems.\nHierarchies are also potentially reaffirmed with implications of the word \"new\" in \"new media\". While media theory aims to expand writers' view of modality, a society caught up in modernism is prone to believe that \"newness\" will result in social progress, as fostered in this case by technological advances, by equating \"new\" and \"better\". Furthermore, class-based issues of access may serve to reinforce existing social structures; as those who are unable to access advanced technology fall behind in composition-based technological capabilities, they will be less able to compete in the job market and are likely to reaffirm the separation between classes. This is especially true in K–12 writing programs, as there are often economic differences between students and school districts. However, issues of access are disputed as technology becomes more available to the general public. Patricia Fitzsimmons-Hunter and Charles Moran, aware of these issues of access and their inevitable consequences, notably use technology that is friendlier to class-differences:\n\"We consider as our goal the integration of low-end, relatively affordable technology into the lives and work of those who are, or see themselves as being, left behind by the pace of the technological change: \"roadkill,\" to use a popular contemporary metaphor, on the information superhighway.\"\nOne of the most cited shortcomings of new media composition is that many teachers are unsure how to assess the writing. Oftentimes, teachers are only familiar with the standards that traditional text is held to. Texts written in new media encounter different rhetorical situations and contexts, and thus do not fit the standards that teachers are familiar with. Furthermore, these new texts often combine text with non-textual modes of communication, like the visual or auditory, making the situation for grading a \"written work\" even more complex.\n\n\n"}
{"id": "33768394", "url": "https://en.wikipedia.org/wiki?curid=33768394", "title": "National Council of Arts, Sciences and Professions", "text": "National Council of Arts, Sciences and Professions\n\nThe National Council of Arts, Sciences and Professions (NCASP or ASP) was a United States-based socialist organization of the 1950s.\n\nThe ASP sponsored the Cultural and Scientific Conference for World Peace, held at the Waldorf-Astoria hotel in New York City for 3 days in late March, 1949. It was a controversial conference, picketed by Catholic War Veterans. W. E. B. Du Bois gave an impassioned speech on the final night. The ASP asked Du Bois to represent them at the World Congress of the Partisans of Peace in Paris in April 1949. Du Bois also attended, on behalf of the ASP, the All-Soviet Peace Conference in August 1949. Ronald Reagan was a former member of the Hollywood chapter.\n\n\n"}
{"id": "25381785", "url": "https://en.wikipedia.org/wiki?curid=25381785", "title": "Philosophy of Baruch Spinoza", "text": "Philosophy of Baruch Spinoza\n\nSpinoza's philosophy encompasses nearly every area of philosophical discourse, including metaphysics, epistemology, political philosophy, ethics, philosophy of mind, and philosophy of science. It earned Spinoza an enduring reputation as one of the most important and original thinkers of the seventeenth century.\n\nSamuel Shirley, who translated Spinoza's complete works into English, summed up the significance of Spinoza's philosophy as follows:\n\nSpinoza's philosophy is largely contained in two books: the \"Theologico-Political Treatise\", and the \"Ethics\". The former was published during his lifetime, but the latter, which contains the entirety of his philosophical system in its most rigorous form, was not published until after his death in 1677. The rest of the writings we have from Spinoza are either earlier, or incomplete, works expressing thoughts that were crystallized in the two aforementioned books (\"e.g.\", the \"Short Treatise\" and the \"Treatise on the Emendation of the Intellect\"), or else they are not directly concerned with Spinoza's own philosophy (\"e.g.\", \"The Principles of Cartesian Philosophy\" and \"The Hebrew Grammar\"). He also left behind many letters that help to illuminate his ideas and provide some insight into what may have been motivating his views.\n\nSpinoza's philosophy of religion is largely contained in the \"Theologico-Political Treatise\". In that work he argues for the view that we should interpret scripture solely on its own terms by carefully studying it, not with any concepts or doctrines that cannot themselves be derived from the text. If we do this, he thought, it would turn out that many things we believe or are told by religious authorities about God and the universe could be shown to be false (\"e.g.\", miracles).\nSpinoza's view is exemplified in the following sentence from the Preface to the \"Theological Political Treatise\":\nIn Spinoza’s Ethics, he wrote a section titled “Treating of God and What Pertains to Him,” in which he discusses God’s existence and what God is. He starts off by saying: “whether there is a God, this, we say, can be proved”. His proof for God follows a similar structure as Descartes’ ontological argument. Descartes attempts to prove God’s existence by arguing that there “must be some one thing that is supremely good, through which all good things have their goodness”. Spinoza’s argument differs in that he does not move straight from the conceivability of the greatest being to the existence of God, but rather uses a deductive argument from the idea of God. Spinoza says that man’s ideas do not come from himself, but from some sort of external cause. Thus the things whose characteristics a man knows must have come from some prior source. So, if man has the idea of God, then God must exist before this thought, because man cannot create an idea of his own imagination. \n\n After stating his proof for God’s existence, Spinoza addresses who “God” is. Spinoza believed that God is “the sum of the natural and physical laws of the universe and certainly not an individual entity or creator”. Spinoza attempts to prove that God is just the substance of the universe by first stating that substances do not share attributes or essences, and then demonstrating that God is a “substance” with an infinite number of attributes, thus the attributes possessed by any other substances must also be possessed by God. Therefore, God is just the sum of all the substances of the universe. God is the only substance in the universe, and everything is a part of God. “Whatever is, is in God, and nothing can be or be conceived without God”.\n\nSpinoza's political philosophy is deeply influenced by both the turbulent time period in which he lived, and by the fact that he happened to live in a comparatively liberal place in Europe, which allowed him freedoms he wished to preserve and defend, as he says in the Preface to the Theological Political Treatise:\nSpinoza's political philosophy is scattered in three books, the \"Theologico-political Treatise\", the \"Ethics\" and the \"Political Treatise\". A first look at its main principles could bring the uninformed reader to believe that it is the same as Hobbes's. Yet both theories differ in their conclusions. Spinoza's political philosophy is also a philosophy of the \"conatus\", the individual tendency to exist, which cannot be brought to extinction even in the most powerful Leviathan, even in the worst of authoritarian regimes. Every individual, in Spinoza's opinion, has a natural right. This right includes everything that he desires and he is able to obtain. As a result, my own natural right is the equivalent of my individual strength or power. Hence, in Spinoza's political philosophy subjective rights (e.g. human rights) do not exist by nature, they are an institution of society, they only exist in the civil state. Moreover, according to Spinoza the notions of right and wrong have no meaning before society, since in the natural state there are no common norms, only individual desires (desires which can bring some people to dominate other weaker people).\n\nHow can civil society exist if people are only dominated by their own impulse to live? Through many ways. First, through the action of affections, the same ones that are described in the Ethics. Those affections, my feelings, will bring me to cluster, to gather with people similar to myself: this similarity reinforces the feeling or representation of my own existence. In a similar fashion, human needs will also play a role: society, through distribution and specialisation of each task, can provide more goods than I can generate myself and with less effort. This is why the sciences and the arts can only develop in societies, where there is time to attend to things other than one's own survival. This fear, the need to constantly look after danger and threats and to live in constant tension, is the third cause or root phenomenon of society. Society brings me protection and security. We see hence that Spinoza, while incorporating in his work Hobbesian arguments (the argument of fear), develops a distinct analysis that will bring him to different conclusions: the need of a free society.\n\nHere individuals never entirely renounce their individual right of nature. If in the Theologico-Political Treatise Spinoza refers to the notion of a pact that would be at the root of civil society, this notion disappears in the Political Treatise. People are not brought to form a society by their free will, but rather by their affections, or domination (a great number of individuals gathered through the authority of an unusually strong or charismatic man could also be a way to explain the birth of civil society). They are not passive subjects under the power of an absolute sovereign, but rather citizens that bring their own strength to the State. The power of the state exists in Spinoza's opinion only through the gathering of individual powers, powers which the society incorporates and can even develop if its political institutions are well designed. \"Well designed\" means that they must induce political leaders to act according to the rules, by their own will. In Spinoza's political philosophy, state is not opposed to the society but it is the apparatus that gives a certain form or existence to the society, to a gathering of human beings. It is not transcendent to it, as it is in Hobbes's philosophy.\n\nThese affirmations have some political implications. Here, individual rights exist only because we, as individuals, benefit from the power of our entire group. Members' rights are guaranteed by the strength of their political group (=State or imperium). Individual or subjective rights do not exist outside of a state, out of an organised society. But that doesn't mean that the government should have absolute power over us. To understand that well, we have to remember that according to Spinoza the government or society (there is no difference between them) are nothing else and do not exist without the individual conatuses of the individuals that are gathered in social entities. Individuals hold a part of their natural right in the civil state. They cannot restrain themselves from judging about the state of things as they wish, and any action that would go against this tendency can induce social unrest. It follows that the state must restrain itself from any action that could jeopardise its own integrity, as condemning determinate opinions can. In a broader perspective, a state that relies on fearsome and inhuman ways to preserve its power cannot survive for long, since those ways impede the development of its own strength, and reinforce the tendency of the \"multitudo\", the masses, to unrest or to disobedience: obedience is necessary to preserve social order and peace.\n\nThus, we can distinguish Hobbes and Spinoza through the way they see the normal operation of the state. For Hobbes, the object of the state is to preserve peace through security and fear if needed. According to Spinoza, that kind of peace would not be a true peace but only the absence of unrest. True peace implies a state of things where individuals can accomplish and realise their potentialities, where there is a minimum peace of mind. This is why Spinoza favors states that are organised so that citizens can participate in the elaboration of laws, as a way to improve their quality, and in the operation of the state.\n\nThe vocabulary of Spinoza shows a modification of the way philosophers see politics compared to the Antiquity. In Plato's and Aristotle's works good politics imply good government (defined as the way decisions are taken in a certain political community), in the sense that the different types of government can be ranked according to their virtues (aristocracy is better than democracy, which is better than oligarchy and tyranny according to Plato, and so on). Spinoza goes beyond this way of seeing things. There is not a better government in this sense: the better government is the government that the people of a certain country have been accustomed to, and there is no good in changing it: such a change alters the balance of power already in place and can bring unrest, conflict between opposed or entrenched interests. According to him, one should rather aim to design better institutions: for type of regime or government (Monarchy, Aristocracy, Democracy) Spinoza implements the outlines of what should be the good institutions for this regime. For example, in Monarchy there should be an official Council of the king, whose members are chosen formally, and whose opinions form a set of possible decisions for the king. This is a way of avoiding the issue of the king's secret counselors or ministers, who have a lot of influence on the king and often are the true decision takers. This system makes public and transparent through a formal process a matter of fact, the existence of a circle of advisers around the king.\n\nFor further reference, see Spinoza's Political Philosophy.\n\nSpinoza argues for a distinct conception of the human mind in Part Two of \"The Ethics\". He says the following:\n\nHe then argues that it follows that \"the human Mind is a part of the infinite intellect of God.\"(E2P11c) Further, Spinoza says: \"Whatever happens in the object of the idea constituting the human Mind must be perceived by the human Mind\"(E2P12) From this we get a clear rejection of Descartes' mind/body dualism: \"The object of the idea constituting the human Mind is the Body, or a certain mode of Extension which actually exists, and nothing else.\"(E2P13)\n\nOne thing which seems, on the surface, to distinguish Spinoza's view of the emotions from both Descartes' and Hume's pictures of them is that he takes the emotions to be cognitive in some important respect. Jonathan Bennett claims that \"Spinoza mainly saw emotions as caused by cognitions. [However] he did not say this clearly enough and sometimes lost sight of it entirely.\"\nSpinoza provides several demonstrations which purport to show truths about how human emotions work. The picture presented is, according to Bennett, \"unflattering, coloured as it is by universal egoism\"\nSpinoza's treatment of the emotions in Part Three of \"The Ethics\", \"On the Origin and Nature of the Affects\", utilizes a broad set of terminology, clearly intended to cover the whole of human experience. He tells us in the Preface:\nWhether there is any meaningful kind of freedom which humans may genuinely have is, in Spinoza's picture, at least contentious. He certainly claims that there is a kind of freedom, namely, that which is arrived at through adequate knowledge of God, or, what is the same: the universe. But in the last two propositions of Part Two of \"The Ethics\", P48 and P49, he explicitly rejects the traditional notion of free will. In E2P48, he claims:\n\nSo from this we get a strong sense of Spinoza's metaphysical naturalism, that is, that the natural and human orders are contiguous. With that being the case, human freedom of a kind which would extricate us from the order of physical causes is impossible. However, Spinoza argues, we still ought to strive to understand the world around us, and in doing so, gain a greater degree of \"power\", which will allow us to be more active than passive, and there is a sense in which this is a kind of freedom.\nFor more, see: Stanford.edu\n\nSpinoza's metaphysics consists of one thing, substance, and its modifications (modes). Early in \"The Ethics\" Spinoza argues that there is only one substance, which is absolutely infinite, self-caused, and eternal. He calls this substance \"God\", or \"Nature\". In fact, he takes these two terms to be synonymous (in the Latin the phrase he uses is \"Deus sive Natura\"). For Spinoza the whole of the natural universe is made of one substance, God, or, what's the same, Nature, and its modifications (modes).\nSpinoza defines \"substance\" as follows:\n\nThis means, essentially, that substance is just whatever can be thought of without relating it to any other idea or thing. For example, if one thinks of a particular object, one thinks of it as a kind of thing, \"e.g.\", \"x\" is a cat. Substance, on the other hand, is to be conceived of by itself, without understanding it as a particular kind of thing (because it isn't a particular thing at all).\n\nSpinoza defines \"attribute\" as follows:\n\nFrom this it can be seen that attributes are related to substance in some way. It is not clear, however, even from Spinoza's direct definition, whether, a) attributes are really the way(s) substance is, or b) attributes are simply ways to understand substance, but not necessarily the ways it really is.\nSpinoza thinks that there are an \"infinite\" number of attributes, but there are two attributes for which Spinoza thinks we can have knowledge. Namely, \"thought\" and \"extension\".\n\nThe attribute of thought is how substance can be understood to give rise to thoughts, or thinking things. When we understand a particular thing in the universe through the attribute of thought, we are understanding the mode as an \"idea\" of something (either another idea, or an object).\n\nThe attribute of extension is how substance can be understood to be physically extended in space. Particular things which have breadth and depth (that is, occupy space) are what is meant by \"extended\". It follows from this that if substance and God are identical, on Spinoza's view, and contrary to the traditional conception, God has extension as one of His attributes.\n\nModes are particular modifications of substance, \"i.e.\", particular things in the world. Spinoza gives the following definition:\nThe argument for there only being one substance in the universe occurs in the first fourteen propositions of \"The Ethics\". The following proposition expresses Spinoza's commitment to substance monism:\n\nSpinoza takes this proposition to follow directly from everything he says prior to it. Spinoza's monism is contrasted with Descartes' dualism and Leibniz's pluralism. It allows Spinoza to avoid the problem of interaction between mind and body, which troubled Descartes in his \"Meditations on First Philosophy\".\n\nThe issue of causality and modality (possibility and necessity) in Spinoza's philosophy is contentious. Spinoza's philosophy is, in one sense, thoroughly deterministic (or necessitarian). This can be seen directly from Axiom 3 of \"The Ethics\":\n\nYet Spinoza seems to make room for a kind of freedom, especially in the fifth and final section of \"The Ethics\", \"On the Power of the Intellect, or on Human Freedom\":\n\nSo Spinoza certainly has a use for the word 'freedom', but he equates \"Freedom of Mind\" with \"blessedness\", a notion which is not traditionally associated with freedom of the will at all.\n\nThough the PSR is most commonly associated with Gottfried Leibniz, it is arguably found in its strongest form in Spinoza's philosophy.\nWithin the context of Spinoza's philosophical system, the PSR can be understood to unify causation and explanation. What this means is that for Spinoza, questions regarding the \"reason\" why a given phenomenon is the way it is (or exists) are always answerable, and are always answerable in terms of the relevant cause(s). This constitutes a rejection of teleological, or final causation, except possibly in a more restricted sense for human beings. Given this, Spinoza's views regarding causality and modality begin to make much more sense.\n\nSpinoza's philosophy contains as a key proposition the notion that mental and physical (thought and extension) phenomena occur in parallel, but without causal interaction between them. He expresses this proposition as follows:\n\nHis proof of this proposition is that:\n\nThe reason Spinoza thinks the parallelism follows from this axiom is that since the idea we have of each thing requires knowledge of its cause, this cause must be understood under the same attribute. Further, there is only one substance, so whenever we understand some chain of ideas of things, we understand that the way the ideas are causally related must be the same as the way the things themselves are related, since the ideas and the things are the same modes understood under different attributes.\n\nSpinoza's epistemology is deeply rationalist. That is, unlike the empiricists who rejected knowledge of things as they are in themselves (in favour of knowledge merely of what appears to the senses), to think we can have a priori knowledge, knowledge of a world external from our sense perceptions, and, further, that this is tantamount to knowledge of God. The majority of Spinoza's epistemological claims come in Part Two of \"The Ethics\".\n\nSpinoza's notions of truth and falsity have to do with the relation between ideas and their objects. He thinks that:\n\nFrom this it is clear that the notions of adequate and inadequate ideas are important for understanding how Spinoza's view works. This may be explained in the following way. Spinoza argues that \"All ideas, insofar as they are related to God, are true.\"(E2P32) Since by \"God\", he means the one substance which exists necessarily and absolutely infinitely, it follows that an idea as it is with no reference to knowledge a particular person has, is necessarily true, since it just is a particular instance of God. (E2P32)\n\nOn the other hand, Spinoza argues: \"All ideas are in God; and, insofar as they are related to God, are true, and adequate. And so there are no inadequate or confused ideas except insofar as they are related to the singular Mind of someone.\"(E2P36d). That is, even though ideas considered objectively as elements of the universe are always adequate (meaning their relation to their object is total), when a particular individual has an idea of something, such an idea is necessarily incomplete, and therefore, inadequate. This is the source of falsehood.\n\nSpinoza discusses the three kinds of knowledge in E2P40s2.\n\nSpinoza thinks there are two ways we can have the first kind of knowledge:\nHe calls these two ways \"knowledge of the first kind, opinion or imagination.\"\n\nSpinoza argues that the second kind of knowledge arises:\n\nHe goes on to explain what this means in the propositions which immediately follow.\n\nThis can be referred to as \"Intuition\", but it means something rather technical for Spinoza. The third kind of knowledge is a particularly important part of Spinoza's philosophy because it is what he thinks allows us to have adequate knowledge, and therefore know things absolutely truly. As he says: \n\nSpinoza's ethical views are deeply tied to his metaphysical system. This is evident from the following claim:\n\nIt is also apparent from this that he is a kind of subjectivist about moral values. That is, he does not take good and evil to be real properties/facts in the objects we attribute them to, but rather, they are simply thoughts we have about the comparative value of one thing to another for a particular person.\n\nSpinoza gives the following definitions of \"Good\", and \"Evil\":\nFrom this it is clear that Spinoza's view of moral value is in some sense instrumental. That is, the goodness or badness of a particular object or action is measured not by some essential property. The emphasis on \"essential knowledge\" is important, given Spinoza's view of what epistemic certainty amounts to, i.e., adequate knowledge of God (a notion which is briefly elaborated on in this article).\n\nSpinoza's notion of blessedness figures centrally in his ethical philosophy.\nBlessedness (or salvation or freedom), Spinoza thinks,\n\nAnd this means, as Jonathan Bennett explains, that \"Spinoza wants \"blessedness\" to stand for the most elevated and desirable state one could possibly be in.\" Here, understanding what is meant by 'most elevated and desirable state' requires understanding Spinoza's notion of \"conatus\" (read: \"striving\", but not necessarily with any teleological baggage) and that \"perfection\" refers not to (moral) value, but to completeness. Given that individuals are identified as mere modifications of the infinite Substance, it follows that no individual can ever be \"fully\" complete, i.e., perfect, or blessed. Absolute perfection, is, as noted above, reserved solely for Substance. Nevertheless, mere modes can attain a lesser form of blessedness, namely, that of pure understanding of oneself as one really is, i.e., as a definite modification of Substance in a certain set of relationships with everything else in the universe. That this is what Spinoza has in mind can be seen at the end of the \"Ethics\", in E5P24 and E5P25, wherein Spinoza makes two final key moves, unifying the metaphysical, epistemological, and ethical propositions he has developed over the course of the work. In E5P24, he links the understanding of particular things to the understanding of God, or Substance; in E5P25, the \"conatus\" of the mind is linked to the third kind of knowledge (\"Intuition\"). From here, it is a short step to the connection of Blessedness with the \"amor dei intellectualis\" (\"intellectual love of God\").\n\n\n"}
{"id": "243627", "url": "https://en.wikipedia.org/wiki?curid=243627", "title": "Physical information", "text": "Physical information\n\nPhysical information is a form of information. In physics, it refers to the information of a physical system. Physical information is an important concept used in a number of fields of study in physics. For example, in quantum mechanics, the form of physical information known as quantum information is used in many descriptions of quantum phenomena, such as quantum observation, quantum entanglement and the causal relationship between quantum objects that carry out either or both close and long-range interactions with one another.\n\nIn a somewhat general sense, the information of a given entity can be interpreted as its identity. As such, its information can be perceived to be the representation of the specification of its existence and thus, to be serving as the full description of each of the properties (real or potentialized) that are responsible for the entity’s existence. This description, of course, is one that, in a sense, is completely divorced from both any and all forms of language.\n\nWhen clarifying the subject of information, care should be taken to distinguish between the following specific cases:\n\nAs the above usages are all conceptually distinct from each other, overloading the word \"information\" (by itself) to denote (or connote) several of these concepts simultaneously can lead to confusion. Accordingly, this article uses more detailed phrases, such as those shown in bold above, whenever the intended meaning is not made clear by the context.\n\nThe instance of information that is contained in a physical system is generally considered to specify\nthat system's \"true\" \"state\". (A realist would assert that a physical system \"always\" has a true state of some sort—whether classical or quantum—even though, in many practical situations, the system's true state may be largely unknown.)\n\nWhen discussing the information that is contained in physical systems according to modern quantum physics, we must distinguish between classical information and quantum information. Quantum information specifies the complete quantum state vector (or equivalently, wavefunction) of a system, whereas classical information, roughly speaking, only picks out a definite (pure) quantum state if we are already given a prespecified set of distinguishable (orthogonal) quantum states to choose from; such a set forms a basis for the vector space of all the possible pure quantum states (see pure state). Quantum information could thus be expressed by providing (1) a choice of a basis such that the actual quantum state is equal to one of the basis vectors, together with (2) the classical information specifying which of these basis vectors is the actual one. (However, the quantum information by itself does not include a specification of the basis, indeed, an uncountable number of different bases will include any given state vector.)\n\nNote that the amount of classical information in a quantum system gives the maximum amount of information that can actually be measured and extracted from that quantum system for use by external classical (decoherent) systems, since only basis states are operationally distinguishable from each other. The impossibility of differentiating between non-orthogonal states is a fundamental principle of quantum mechanics, equivalent to Heisenberg's uncertainty principle. Because of its more general utility, the remainder of this article will deal primarily with classical information, although quantum information theory does also have some potential applications (quantum computing, quantum cryptography, quantum teleportation) that are currently being actively explored by both theorists and experimentalists.\n\nAn amount of (classical) physical information may be quantified, as in information theory, as follows. For a system \"S\", defined abstractly in such a way that it has \"N\" distinguishable states (orthogonal quantum states) that are consistent with its description, the amount of information \"I\"(\"S\") contained in the system's state can be said to be log(\"N\"). The logarithm is selected for this definition since it has the advantage that this measure of information content is additive when concatenating independent, unrelated subsystems; e.g., if subsystem \"A\" has \"N\" distinguishable states (\"I\"(\"A\") = log(\"N\") information content) and an independent subsystem \"B\" has \"M\" distinguishable states (\"I\"(\"B\") = log(\"M\") information content), then the concatenated system has \"NM\" distinguishable states and an information content \"I\"(\"AB\") = log(\"NM\") = log(\"N\") + log(\"M\") = \"I\"(\"A\") + \"I\"(\"B\"). We expect information to be additive from our everyday associations with the meaning of the word, e.g., that two pages of a book can contain twice as much information as one page.\n\nThe base of the logarithm used in this definition is arbitrary, since it affects the result by only a multiplicative constant, which determines the unit of information that is implied. If the log is taken base 2, the unit of information is the binary digit or bit (so named by John Tukey); if we use a natural logarithm instead, we might call the resulting unit the \"nat.\" In magnitude, a nat is apparently identical to Boltzmann's constant \"k\" or the ideal gas constant \"R\", although these particular quantities are usually reserved to measure physical information that happens to be entropy, and that are expressed in physical units such as joules per kelvin, or kilocalories per mole-kelvin.\n\nAn easy way to understand the underlying unity between physical (as in thermodynamic) entropy and information-theoretic entropy is as follows: Entropy is simply that portion of the (classical) physical information contained in a system of interest (whether it is an entire physical system, or just a subsystem delineated by a set of possible messages) whose identity (as opposed to amount) is unknown (from the point of view of a particular knower).This informal characterization corresponds to both von Neumann's formal definition of the entropy of a mixed quantum state (which is just a statistical mixture of pure states; see von Neumann entropy), as well as Claude Shannon's definition of the entropy of a probability distribution over classical signal states or messages (see information entropy). Incidentally, the credit for Shannon's entropy formula (though not for its use in an information theory context) really belongs to Boltzmann, who derived it much earlier for use in his H-theorem of statistical mechanics. (Shannon himself references Boltzmann in his monograph.)\n\nFurthermore, even when the state of a system \"is\" known, we can say that the information in the system is still \"effectively\" entropy if that information is effectively incompressible, that is, if there are no known or feasibly determinable correlations or redundancies between different pieces of information within the system. Note that this definition of entropy can even be viewed as equivalent to the previous one (unknown information) if we take a meta-perspective, and say that for observer \"A\" to \"know\" the state of system \"B\" means simply that there is a definite correlation between the state of observer \"A\" and the state of system \"B\"; this correlation could thus be used by a meta-observer (that is, whoever is discussing the overall situation regarding A's state of knowledge about B) to compress his own description of the joint system \"AB\".\n\nDue to this connection with algorithmic information theory, entropy can be said to be that portion of a system's information capacity which is \"used up,\" that is, unavailable for storing new information (even if the existing information content were to be compressed). The rest of a system's information capacity (aside from its entropy) might be called \"extropy\", and it represents the part of the system's information capacity which is potentially still available for storing newly derived information. The fact that physical entropy is basically \"used-up storage capacity\" is a direct concern in the engineering of computing systems; e.g., a computer must first remove the entropy from a given physical subsystem (eventually expelling it to the environment, and emitting heat) in order for that subsystem to be used to store some newly computed information.\n\nIn a theory developed by B. Roy Frieden, \"physical information\" is defined as the loss of Fisher information that is incurred during the observation of a physical effect. Thus, if the effect has an intrinsic information level \"J\" but is observed at information level \"I\", the physical information is defined to be the difference \"I\" − \"J\". This defines an \"information Lagrangian\". Frieden's \"principle of extreme physical information\" or EPI states that extremalizing \"I\" − \"J\" by varying the system probability amplitudes gives the correct amplitudes for most or even all physical theories. The EPI principle was recently proven. It follows from a system of mathematical axioms of L. Hardy defining all known physics.\n\n\n"}
{"id": "18667798", "url": "https://en.wikipedia.org/wiki?curid=18667798", "title": "Pixel artist", "text": "Pixel artist\n\nA pixel artist is a graphic designer who specializes in computer art and can refer to a number of artistic and professional disciplines which focus on visual communication and presentation. Similar to chromoluminarism used in the pointillism style of painting, in which small distinct points of primary colors create the impression of a wide selection of secondary and intermediate colors, a pixel artist works with pixels, the smallest piece of information in an image. The technique relies on the perceptive ability of the eye and mind of the viewer to mix the color spots into a fuller range of tones. Pixel art is often utilitarian and anonymous. Pixel design can refer to both the process (designing) by which the communication is created and the products (designs) which are generated.\n\nCommon uses of pixel design include print and broadcast media, web design and games. For example, a product package might include a logo or other artwork, organized text and pure design elements such as shapes and color which unify the piece. Composition is one of the most important features of design especially when utilizing pre-existing materials or using diverse elements. Pixel artists can also be a specialist in computer animation such as Computer Animation Production System users in post production of animated films and rendering (computer graphics) images like raster graphics.\n\nIn the 2000s, pixel artists such as Tyler West, Stephane Martiniere and Daniel Dociu have gained international notoriety and artistic recognition, due in part to the popularity of computer and video games. For instance the E3 Media and Business Summit, an annual trade show for the computer and video games industry, has a concurrent juried art show, \"Into the Pixel\" starting in 2003. Jurist and Getty Research Institute curator Louis Marchesano noted that most of the works were concept pieces used in the development of games.\n\nPixel artists are also used in digital forensics, an emerging field, to both create and detect fraud in all forms of media including \"the courts, politics and scientific journals\". For instance, the Federal Office of Research Integrity has said that the percent of allegations of fraud they investigated involved contested images has risen from less than 3 in 1990 to 44.1 percent in 2006.\n\nComputer art started in 1960s and by its nature is evolutionary since changes in technology and software directly affect what is possible. The term \"pixel art\" was first published by Adele Goldberg and Robert Flegal of Xerox Palo Alto Research Center in 1982. Adobe Systems, founded in 1982, developed the Postscript language and digital fonts, making drawing painting and image manipulation software popular. Adobe Illustrator, a vector drawing program based on the Bezier curve, was introduced in 1987 and Adobe Photoshop followed in 1990. Adobe Flash, a popular set of multimedia software used to add animation and interactivity to web pages, was introduced in 1996.\n\nIn the 2000s, pixel artists have been employed increasingly in video games and, to a lesser extent in music videos. These artists have remained somewhat underground until the mid-2000s. In 2006 Röyksopp released \"Remind Me\", illustrated completely by pixel art, which the New York Times cited as amazing and hypnotic. (See video here .)\n\nA pixel artist is one of the new media artists that employs technology while also utilizing traditional media and art forms. They may have a fine arts background such as photography, painting or drawing but self-taught designers and artists are also able to accomplish this work. They are often required to employ imaging and a full range of artistic and technological skills including those of conceptual artists.\n\nIn digital imaging, a pixel (\"pict\"ure \"el\"ement) is the smallest piece of information in an image. \nThe word \"pixel\" is based on a contraction of \"pix\" (for \"pictures\") and \"el\" (for \"element\"); similar formations with \"el\" for \"element\" include voxel, luxel, and texel. Pixels are normally arranged in a regular 2-dimensional grid, and are often represented using dots, squares, or rectangles. Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel has typically three or four components such as red, green, and blue, or cyan, magenta, yellow, and black.\n\nNeuroplasticity is a key element of observing many pixel images. While two individuals will observe the same photons reflecting off a photorealistic image and hitting their retinas, someone whose mind has been primed with the theory of pointillism may \"see\" a very different image as the image is interpreted in the visual cortex.\n\nThe total number of pixels (\"image resolution\"), and the amount of information in each pixel (often called \"color depth\") determine the quality of an image. For example, an image that stores 24 bits of color-information per pixel (the standard for computer displays since around 1995) can represent smoother degrees of shading than one that only stores 16 bits per pixel, but not as smooth as one that stores 48 bits. Likewise, an image sampled at 640 x 480 pixels (and therefore containing 307,200 pixels) will look rough and blocky compared to one sampled at 1280 x 1024 (1,310,720 pixels). Because it takes a large amount of data to store a high-quality image, computer software often uses data compression techniques to reduce this size for images stored on disk. Some techniques sacrifice information, and therefore image quality, in order to achieve a smaller file-size. Computer scientists refer to compression techniques that lose information as lossy compression.\n\nModern computer-monitors typically display about 72 to 130 pixels per inch (PPI), and some modern consumer printers can resolve 2400 dots per inch (DPI) or more; determining the most appropriate image resolution for a given printer-resolution can pose difficulties, since printed output may have a greater level of detail than a viewer can discern on a monitor. Typically, a resolution of 150 to 300 pixel per inch works well for 4-color process (CMYK) printing.\nDrawings usually start with what is called the line art, which is the basic line that defines the item the artist intends to create. Line arts can be either traced over scanned drawings or hand drawn on the computer itself by the use of a mouse or a graphics tablet and are often shared among other pixel artists in diverse websites in order to receive some feedback. Other techniques, some resembling painting, also exist, such as knowledge of the color theory. The limited palette often implemented into pixel art usually promotes the use of dithering in order to achieve different shades and colors (when necessary); hand-made anti-aliasing is also used for smoother purposes. A pixel artist will exponentially increase the zoom of whatever they are working on to make adjustments as needed and then view the results until desired changes are achieved.\n\n\n"}
{"id": "53953041", "url": "https://en.wikipedia.org/wiki?curid=53953041", "title": "Predictive coding", "text": "Predictive coding\n\nPredictive coding models suggest that the brain is constantly generating and updating hypotheses that predict sensory input at varying levels of abstraction. This framework is in contrast to the view that the brain integrates exteroceptive information through a predominantly feedforward process, with feedback connections playing a more minor role in cortical processing.\n\nTheoretical ancestors to predictive coding date back as early as 1860 with Helmholz’s concept of unconscious inference (Clark, 2013). Unconscious inference refers to the idea that the human brain fills in visual information to make sense of a scene. For example, if something is relatively smaller than another object in the visual field, the brain uses that information as a likely cue of depth, such that the perceiver ultimately (and involuntarily) experiences depth. The understanding of perception as the interaction between sensory stimuli (bottom-up) and conceptual knowledge (top-down) continued to be established by Jerome Bruner (psychologist) who, starting in the 1940s, studied the ways in which needs, motivations and expectations influence perception, research that came to be known as 'New Look' psychology. In 1981, McClelland and Rumelhart in their seminal paper examined the interaction between processing features (lines and contours) which form letters, which in turn form words. While the features suggest the presence of a word, they found that when letters were situated in the context of a word, people were able to identify them faster than when they were situated in a non-word without semantic context. McClelland and Rumelhart’s parallel processing model describes perception as the meeting of top-down (conceptual) and bottom-up (sensory) elements.\n\nIn the late 1990s, the idea of top-down and bottom-up processing was translated into a computational model of vision by Rao and Ballard (1999). Their paper demonstrated that there could be a generative model of a scene (top-down processing), which would receive feedback via error signals (how much the visual input varied from the prediction), which would subsequently lead to updating the prediction. The computational model was able to replicate well-established receptive field effects, as well as less understood extra-classical receptive field effects such as end-stopping. Today, the fields of computer science and cognitive science incorporate these same concepts to create the multilayer generative models that underlie machine learning and neural nets (Hinton, 2010).\n\nMost of the research literature in the field has been about sensory perception, particularly vision, which is more easily conceptualized. However, the predictive coding framework could also be applied to different neural systems. Taking the sensory system as an example, the brain solves the seemingly intractable problem of modelling distal causes of sensory input through a version of Bayesian inference. It does this by modelling predictions of lower-level sensory inputs via backward connections from relatively higher levels in a cortical hierarchy (Clark, 2013). \nConstrained by the statistical regularities of the outside world (and certain evolutionarily prepared predictions), the brain encodes top-down generative models at various temporal and spatial scales in order to predict and effectively suppress sensory inputs rising up from lower levels. A comparison between predictions (priors) and sensory input (likelihood) yields a difference measure (e.g. prediction error, free energy, or surprise) which, if it is sufficiently large beyond the levels of expected statistical noise, will cause the generative model to update so that it better predicts sensory input in the future.\n\nIf, instead, the model accurately predicts driving sensory signals, activity at higher levels cancels out activity at lower levels, and the posterior probability of the model is increased. Thus, predictive coding inverts the conventional view of perception as a mostly bottom-up process, suggesting that it is largely constrained by prior predictions, where signals from the external world only shape perception to the extent that they are propagated up the cortical hierarchy in the form of prediction error.\n\nExpectations about the precision (or inverse variance) of incoming sensory input are crucial for effectively minimizing prediction error in that the expected precision of a given prediction error can inform confidence in that error, which influences the extent to which the error is weighted in updating predictions (Feldman & Friston, 2010). Given that the world we live in is loaded with statistical noise, precision expectations must be represented as part of the brain’s generative models, and they should be able to flexibly adapt to changing contexts. For instance, the expected precision of visual prediction errors likely varies between dawn and dusk, such that greater conditional confidence is assigned to errors in broad daylight than errors in prediction at nightfall (Hohwy, 2012). It has recently been proposed that such weighting of prediction errors in proportion to their estimated precision is, in essence, attention (Friston, 2009), and that the process of devoting attention may be neurobiologically accomplished by ascending reticular activating systems (ARAS) optimizing the “gain” of prediction error units.\n\nThe same principle of prediction error minimization has been used to provide an account of behavior in which motor actions are not commands but descending proprioceptive predictions. In this scheme of active inference, classical reflex arcs are coordinated so as to selectively sample sensory input in ways that better fulfill predictions, thereby minimizing proprioceptive prediction errors (Friston, 2009). Indeed, Adams et al. (2013) review evidence suggesting that this view of hierarchical predictive coding in the motor system provides a principled and neurally plausible framework for explaining the agranular organization of the motor cortex. This view suggests that “perceptual and motor systems should not be regarded as separate but instead as a single active inference machine that tries to predict its sensory input in all domains: visual, auditory, somatosensory, interoceptive and, in the case of the motor system, proprioceptive” (Adams, Shipp, & Friston, 2013).\n\nEvaluating the empirical evidence that suggests a neurologically plausible basis for predictive coding is a broad and varied task. For one thing, and according to the model, predictive coding occurs at every iterative step in the perceptual and cognitive processes; accordingly, manifestations of predictive coding in the brain include genetics, specific cytoarchitecture of cells, systemic networks of neurons, and whole brain analyses. Due to this range of specificity, different methods of investigating the neural mechanisms of predictive coding have been applied, where available; more generally, however, and at least as it relates to humans, there are significant methodological limitations to investigating the potential evidence and much of the work is based on computational modeling of microcircuits in the brain. Notwithstanding, there has been substantial (theoretical) work that has been applied to understanding predictive coding mechanisms in the brain. This section will focus on specific evidence as it relates to the predictive coding phenomenon, rather than analogues, such as homeostasis (which are, nonetheless, integral to our overall understanding of Bayesian inference but already supported heavily; see Clark, 2012 for a review).\n\nMuch of the early work that applied a predictive coding framework to neural mechanisms came from sensory neurons, particularly in the visual cortex (e.g., Rao and Ballard, 1999; Bolz & Gilbert, 1986).\n\nMore generally, however, what seems to be required by the theory are (at least) two types of neurons (at every level of the perceptual hierarchy): one set of neurons that encode incoming sensory input, so called feed-forward projections; one set of neurons that send down predictions, so called feed-backward projections. It is important to note that these neurons must also carry properties of error detection; which class of neurons has these properties is still up for debate (see Koster-Hale & Saxe, 2013; Seth, 2013). These sort of neurons have found support in superficial and non-superficial pyramidal neurons.\n\nAt a more whole-brain level, there is evidence that different cortical layers (aka laminae) may facilitate the integration of feedforward and feed-backward projections across hierarchies. These cortical layers, divided into granular, agranular, and dysgranular, which house the subpopulations of neurons mentioned above, are divided into 6 main layers. The cytoarchitecture within these layers are the same, but they differ across layers. For example, layer 4 of the granular cortex contain granule cells which are excitatory and distribute thalamocortical inputs to the rest of the cortex. According to one model: \n\n“...prediction neurons... in deep layers of agranular cortex drive active inference by sending sensory predictions via projections ...to supragranular layers of dysgranular and granular sensory cortices. Prediction-error neurons ….in the supragranular layers of granular cortex compute the difference between the predicted and received sensory signal, and send prediction-error signals via projections...back to the deep layers of agranular cortical regions. Precision cells … tune the gain on predictions and prediction error dynamically, thereby giving these signals reduced (or, in some cases, greater) weight depending on the relative confidence in the descending predictions or the reliability of incoming sensory signals.” (Barrett & Simmons, 2015)\n\nIn sum, the neural evidence is still in its infancy.\n\nThe empirical evidence for predictive coding is most robust for perceptual processing. As early as 1999, Rao and Ballard proposed a hierarchical visual processing model in which higher-order visual cortical area sends down predictions and the feedforward connections carry the residual errors between the predictions and the actual lower-level activities (Rao and Ballard, 1999). According to this model, each level in the hierarchical model network (except the lowest level, which represents the image) attempts to predict the responses at the next lower level via feedback connections, and the error signal is used to correct the estimate of the input signal at each level concurrently (Rao and Ballard, 1999). Emberson et al. established the top-down modulation in infants using a cross-modal audiovisual omission paradigm, determining that even infant brains have expectation about future sensory input that is carried downstream from visual cortices and are capable of expectation-based feedback (Emberson et al., 2015). Functional near-infrared spectroscopy (fNIRS) data showed that infant occipital cortex responded to unexpected visual omission (with no visual information input) but not to expected visual omission. These results establish that in a hierarchically organized perception system, higher-order neurons send down predictions to lower-order neurons, which in turn sends back up the prediction error signal.\n\nThere have been several competing models for the role of predictive coding in interoception.\n\nIn 2013, Anil Seth proposed that our subjective feeling states, otherwise known as emotions, are generated by predictive models that is built actively of causal interoceptive appraisals.\nIn relation to how we attribute internal states of others to causes, Sasha Ondobaka, James Kilner, and Karl Friston (2015) proposed that the free energy principle requires the brain to produce a continuous series of predictions with the goal of reducing the amount of prediction error that manifests as “free energy”. These errors are then used to model anticipatory information about what the state of the outside world will be and attributions of causes of that world state, including understanding of causes of others’ behavior. This is especially necessary because, to create these attributions, our multimodal sensory systems need interoceptive predictions to organize themselves. Therefore, Ondobaka posits that predictive coding is key to understanding other people’s internal states.\n\nIn 2015, Lisa Barrett and W. Kyle Simmons (2015) proposed the Embodied Predictive Interoception Coding model, a framework that unifies Bayesian active inference principles with a physiological framework of corticocortical connections. Using this model, they posited that agranular visceromotor cortices are responsible for generating predictions about interoception, thus, defining the experience of interoception.\n\nIn 2017, contrary to the inductive notion that emotion categories are biologically distinct, Barrett (2017) proposed the theory of constructed emotion, which is the account that a biological emotion category is constructed based on a conceptual category--the accumulation of instances sharing a goal. In a predictive coding model, Barrett hypothesizes that, in interoception, our brains regulate our bodies by activating \"embodied simulations\" (full-bodied representations of sensory experience) to anticipate what our brains predict that the external world will throw at us sensorially and how we will respond to it with action. These simulations are either preserved if, based on our brain's predictions, they prepare us well for what actually subsequently occurs in the external world, or they, and our predictions, are adjusted to compensate for the their error in comparison to what actually occurs in the external world and how well-prepared we were for it. Then, in a trial-error-adjust process, our bodies find similarities in goals among certain successful anticipatory simulations and group them together under conceptual categories. Every time a new experience arises, our brains use this past trial-error-adjust history to match the new experience to one of the categories of accumulated corrected simulations that is shares the most similarity with. Then, they apply the corrected simulation of that category to the new experience in the hopes of preparing our bodies for the rest of the experience. If it does not, the prediction, the simulation, and perhaps the boundaries of the conceptual category are revised in the hopes of higher accuracy next time, and the process continues. Barrett hypothesizes that, when prediction error for a certain category of simulations for x-like experiences is minimized, what results is a correction-informed simulation that the body will reenact for every x-like experience, resulting in a correction-informed full-bodied representation of sensory experience--an emotion. In this sense, Barrett proposes that we construct our emotions because the conceptual category framework our brains use to compare new experiences, and to pick the appropriate predictive sensory simulation to activate, is built on the go.\n\nAs a mechanistic theory, predictive coding has not been mapped out physiologically on the neuronal level. One of the biggest challenges to the theory has been the imprecision of exactly how prediction error minimization works (Kogo & Trengove, 2015). In some studies, the increase in BOLD signal has been interpreted as error signal while in others it indicates changes in the input representation (Kogo & Trengove, 2015). A crucial question that needs to be addressed is what exactly constitutes error signal and how it is computed at each level of information processing (Bastos et al., 2012). Another challenge that has been posed is predictive coding’s computational tractability. According to Kwisthout and Rooij, the subcomputation in each level of the predictive coding framework potentially hides a computationally intractable problem, which amounts to “intractable hurdles” that computational modelers have yet to overcome (Kwisthout & Rooij, 2013). Ransom and Fazelpour (2015) indicate \"Three Problems for the Predictive Coding Theory of Attention\".\n\nFuture research could focus on clarifying the neurophysiological mechanism and computational model of predictive coding.\n\nhttp://mindsonline.philosophyofbrains.com/2015/session4/three-problems-for-the-predictive-coding-theory-of-attention/\n"}
{"id": "58550280", "url": "https://en.wikipedia.org/wiki?curid=58550280", "title": "Property qualification", "text": "Property qualification\n\nA property qualification is a clause or rule by which those without property (land), or those without property of a set appraised value, or those without income of a set value, are not enfranchised to vote in elections, to stand for election, to hold office or from other activities. \n\nThe Eighteenth Century Militia of England and Wales did not sell commissions in the way that the British Army did at the time, but instead restricted them property owners and those with income of a set minimum value. \n\nIn jurisdictions where the political and economic spheres are dominated by one ethnic, religious or racial group, property qualifications have been used as a way to exclude members of other ethnic, religious or racial groups that may disproportionately lack the required resources. This was the case in Northern Ireland, where a property requirement was used to exclude indigenous Irish Catholics from voting in elections for seats in the Stormont Parliament until 1969. Prior to the 1921 Partition of Ireland, the Protestant Ascendancy had similarly barred most of the native Irish Catholics from voting for Irish seats in the Parliament of the United Kingdom of Great Britain and Ireland once the last bar to Catholics voting in the United Kingdom had been lifted by the Roman Catholic Relief Act 1829. The property qualification remained in place for United Kingdom elections until the passage of the Representation of the People Act 1918.\n"}
{"id": "9623828", "url": "https://en.wikipedia.org/wiki?curid=9623828", "title": "P–P plot", "text": "P–P plot\n\nIn statistics, a P–P plot (probability–probability plot or percent–percent plot or P value plot) is a probability plot for assessing how closely two data sets agree, which plots the two cumulative distribution functions against each other. P-P plots are vastly used to evaluate the skewness of a distribution.\n\nThe Q–Q plot is more widely used, but they are both referred to as \"the\" probability plot, and are potentially confused.\n\nA P–P plot plots two cumulative distribution functions (cdfs) against each other:\ngiven two probability distributions, with cdfs \"\"F\" and \"G\"\", it plots formula_1 as \"z\" ranges from formula_2 to formula_3 As a cdf has range [0,1], the domain of this parametric graph is formula_4 and the range is the unit square formula_5\n\nThus for input \"z\" the output is the pair of numbers giving what \"percentage\" of \"f\" and what \"percentage\" of \"g\" fall at or below \"z.\"\n\nThe comparison line is the 45° line from (0,0) to (1,1) – the distributions are equal if and only if the plot falls on this line – any deviation indicates a difference between the distributions.\n\nAs an example, if the two distributions do not overlap, say \"F\" is below \"G,\" then the P–P plot will move from left to right along the bottom of the square – as \"z\" moves through the support of \"F,\" the cdf of \"F\" goes from 0 to 1, while the cdf of \"G\" stays at 0 – and then moves up the right side of the square – the cdf of \"F\" is now 1, as all points of \"F\" lie below all points of \"G,\" and now the cdf of \"G\" moves from 0 to 1 as \"z\" moves through the support of \"G.\" (need a graph for this paragragh)\n\nAs the above example illustrates, if two distributions are separated in space, the P–P plot will give very little data – it is only useful for comparing probability distributions that have nearby or equal location. Notably, it will pass through the point (1/2, 1/2) if and only if the two distributions have the same median.\n\nP–P plots are sometimes limited to comparisons between two samples, rather than comparison of a sample to a theoretical model distribution. However, they are of general use, particularly where observations are not all modelled with the same distribution.\n\nHowever, it has found some use in comparing a sample distribution from a \"known\" theoretical distribution: given \"n\" samples, plotting the continuous theoretical cdf against the empirical cdf would yield a stairstep (a step as \"z\" hits a sample), and would hit the top of the square when the last data point was hit. Instead one only plots points, plotting the observed \"k\"th observed points (in order: formally the observed \"k\"th order statistic) against the \"k\"/(\"n\" + 1) quantile of the theoretical distribution. This choice of \"plotting position\" (choice of quantile of the theoretical distribution) has occasioned less controversy than the choice for Q–Q plots. The resulting goodness of fit of the 45° line gives a measure of the difference between a sample set and the theoretical distribution.\n\nA P–P plot can be used as a graphical adjunct to a tests of the fit of probability distributions, with additional lines being included on the plot to indicate either specific acceptance regions or the range of expected departure from the 1:1 line. An improved version of the P–P plot, called the SP or S–P plot, is available, which makes use of a variance-stabilizing transformation to create a plot on which the variations about the 1:1 line should be the same at all locations.\n\n"}
{"id": "893037", "url": "https://en.wikipedia.org/wiki?curid=893037", "title": "Road rage", "text": "Road rage\n\nRoad rage is aggressive or angry behavior exhibited by a driver of a road vehicle, which includes rude and offensive gestures, verbal insults, physical threats or dangerous driving methods targeted toward another driver or a pedestrian in an effort to intimidate or release frustration. Road rage can lead to altercations, assaults and collisions that result in serious physical injuries or even death. It can be referred to as an extreme case of aggressive driving.\n\nThe term originated in the United States in 1987–1988 from anchors at KTLA, a television station in Los Angeles, California, when a rash of freeway shootings occurred on the Interstate 405, 110, and 10 freeways in Los Angeles. These shooting sprees even spawned a response from the AAA Motor Club to its members on how to respond to drivers with road rage or aggressive maneuvers and gestures.\n\nAccording to a study by the AAA Foundation for Traffic Safety that examined police records nationally, there are more than 1,200 incidents of road rage on average reported per year in the United States, a number of which have ended with serious injuries or even fatalities. These rates rose yearly throughout the six years of the study. A number of studies have found that individuals with road rage were predominantly young (33 years old on average) and 96.6% male. In Germany, a gun-wielding truck driver was accused of firing at more than 762 vehicles and arrested in 2013, an exceptional case of road rage. According to authorities, the autobahn sniper was motivated by \"annoyance and frustration with traffic.\"\n\nIn some jurisdictions, there can be a legal difference between \"road rage\" and \"aggressive driving.\" In the U.S., only a few states have enacted special aggressive driving laws, where road rage cases are normally prosecuted as assault and battery (with or without a vehicle), or \"vehicular homicide.\" \n\nThe legal definition of road rage encompasses a group of behaviors expressed while driving, or stemming from traffic-related incidents. The U.S. National Highway Traffic Safety Administration defines road rage as when \"The operation of a motor vehicle in a manner that endangers or is likely to endanger persons or property.\" This definition makes the distinction that aggressive driving is a traffic violation and road rage is a criminal offense.\n\nA stressed driver’s behavior depends on that driver’s coping abilities. Generally, drivers who scored high on aggression tests used direct confrontation strategies when faced with stress while driving. Strategies include long horn honks, swerving, tailgating and attempting to fight the other driver. Many drivers who experience road rage have admitted that they believe they commit more traffic violations. Driving presents many stresses any time a person is behind the wheel because of high speeds and other drivers making different decisions. As stress increases, the likelihood of a person having road rage increases dramatically, and if a person has road rage, their stress levels increase. Typically, younger males are most susceptible to road rage.\n\nAccording to one study, people who customize their cars with stickers and other adornments are more prone to road rage. The number of territory markers predicted road rage better than vehicle value or condition. Furthermore, only the number of bumper stickers, and not their content, predicted road rage.\nRoad rage is not an official mental disorder recognized in the \"Diagnostic and Statistical Manual of Mental Disorders\" (\"DSM\"), although according to an article published by the Associated Press in June 2006, the behaviors typically associated with road rage can be the result of a disorder known as intermittent explosive disorder that is recognized in the DSM. This conclusion was drawn from surveys of some 9,200 adults in the United States between 2001 and 2003 and was funded by the National Institute of Mental Health.\n\nRoad rage is a relatively serious act: It may be seen as an endangerment of public safety. It is, however, not always possible to judge intent by external observation, so \"road ragers\" who are stopped by police may be charged with other offences such as careless or reckless driving, or may be fined. Road ragers may be considered as criminals.\n\nIn New South Wales, Australia, road rage is considered an extremely serious act. Any person who chases another motorist or shows intimidating and/or bullying towards another road user can be charged with \"predatory driving\", a serious offence that can leave the culprit in jail for up to 5 years. Offenders can also be fined A$100,000 and disqualified from driving, whether or not he or she intended to harm the victim physically. If the predatory driving results in a physical assault or harm, and/or the victim's car was intentionally damaged, penalties can be much more severe.\n\nAdditionally, most common-law countries prohibit common assault, which could apply to road rage where the personal safety of the victim is seen to be threatened. The common law regards assault as both a criminal and civil matter, leading to both public criminal penalties and private civil liabilities.\n\nRoad rage, insults and rude gestures in traffic can lead to fines and even prison sentences to drivers who shout insults or make offensive gestures while driving.\n\nIn New Zealand, Road Rage in itself is not an offence, but Drivers are usually charged with other offences committed during an act of road rage (usually assault or unlawful possession of an offensive weapon). Drivers have a legal duty to take reasonable care to avoid endangerment of human life when operating a vehicle (s 156 Crimes Act 1961); failure to discharge this duty, such as an act of aggressive driving, can give rise to liability in criminal nuisance (s 146 Crimes Act 1961). Ramming a vehicle constitutes intentional or reckless damage to property, a criminal offence, with a maximum penalty of 7 years imprisonment (s 269 Crimes Act 1961). New Zealand courts currently have no powers to disqualify drivers who physically assault another road user.\n\nRoad rage is a criminal offence in Singapore. When found guilty, the offender may be jailed for up to two years and/or fined up to $5,000 for causing damage.\n\nIn the UK, road rage can result in criminal penalties for assault or more serious offences against the person. The Public Order Act 1986 can also apply to road rage. Sections 4A and 5 of the 1986 Act prohibit public acts likely to cause harassment, alarm or distress. Section 4 also prohibits threatening, abusive or insulting words or behaviour with intent to cause a victim to believe that violence will be used against himself or another.\n\nIn some jurisdictions, such as the Commonwealth of Virginia, it is easier to prosecute road rage as reckless driving instead of aggressive driving simply because the burden of proof does not require \"intent\" to successfully convict.\n\nIt is likely that those causing serious injury or death during \"road rage\" incidents will suffer more serious penalties than those applicable to similar outcomes from simple negligence. In April 2007, a Colorado driver was convicted of first-degree murder for causing the deaths of two motorists in November 2005. He will serve a mandatory sentence of two consecutive life terms.\n\nFourteen U.S. states have passed laws against aggressive driving. Only one state, California, has turned \"road rage\" into a legal term of art by giving it a particular meaning. In Virginia, aggressive driving is punished as a lesser crime (Class 2 misdemeanor) than reckless driving (Class 1 misdemeanor).\n\nA 2007 study of the largest U.S. metropolitan areas concluded that the cities with the least courteous drivers (most road rage) are Miami, Phoenix, New York, Los Angeles, and Boston. The cities with the most courteous drivers (least road rage) are Minneapolis, Nashville, St. Louis, Seattle, and Atlanta. In 2009, New York, Dallas/Fort Worth, Detroit, Atlanta and Minneapolis/St. Paul were rated the top five \"Road Rage Capitals\" of the United States.\n\n\n\n"}
{"id": "244755", "url": "https://en.wikipedia.org/wiki?curid=244755", "title": "Sense and reference", "text": "Sense and reference\n\nIn the philosophy of language, the distinction between sense and reference was an innovation of the German philosopher and mathematician Gottlob Frege in 1892 (in his paper \"On Sense and Reference\"; German: \"Über Sinn und Bedeutung\"), reflecting the two ways he believed a singular term may have meaning.\n\nThe reference (or \"referent\"; \"Bedeutung\") of a proper name is the object it means or indicates (\"bedeuten\"), its sense (\"Sinn\") is what the name expresses. The reference of a sentence is its truth value, its sense is the thought that it expresses. Frege justified the distinction in a number of ways.\n\nMuch of analytic philosophy is traceable to Frege's philosophy of language. Frege's views on logic (i.e., his idea that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function) led to his views on a theory of reference.\n\nFrege developed his original theory of meaning in early works like \"Begriffsschrift\" ('concept script') of 1879 and \"Grundlagen\" ('foundations of arithmetic') of 1884. On this theory, the meaning of a complete sentence consists in its being true or false, and the meaning of each significant expression in the sentence is an extralinguistic entity which Frege called its \"Bedeutung\", literally 'meaning' or 'significance', but rendered by Frege's translators as 'reference', 'referent', \"'M\"eaning', 'nominatum', etc. Frege supposed that some parts of speech are complete by themselves, and are analogous to the arguments of a mathematical function, but that other parts are incomplete, and contain an empty place, by analogy with the function itself. Thus 'Caesar conquered Gaul' divides into the complete term 'Caesar', whose reference is Caesar himself, and the incomplete term '—conquered Gaul', whose reference is a Concept. Only when the empty place is filled by a proper name does the reference of the completed sentence – its truth value – appear. This early theory of meaning explains how the significance or reference of a sentence (its truth value) depends on the significance or reference of its parts.\n\nFrege introduced the notion of \"sense\" (German: \"Sinn\") to accommodate difficulties in his early theory of meaning.\n\nFirst, if the entire significance of a sentence consists of its truth value, it follows that the sentence will have the same significance if we replace a word of the sentence with one having an identical reference, as this will not change its truth value. The reference of the whole is determined by the reference of the parts. If \"the evening star\" has the same reference as \"the morning star\", it follows that \"the evening star is a body illuminated by the Sun\" has the same truth value as \"the morning star is a body illuminated by the Sun\". But it is possible for someone to think that the first sentence is true while also thinking that the second is false. Therefore, the thought corresponding to each sentence cannot be its reference, but something else, which Frege called its \"sense\".\n\nSecond, sentences that contain proper names with no reference cannot have a truth value at all. Yet the sentence 'Odysseus was set ashore at Ithaca while sound asleep' obviously has a sense, even though 'Odysseus' has no reference. The thought remains the same whether or not 'Odysseus' has a reference. Furthermore, a thought cannot contain the objects that it is about. For example, Mont Blanc, 'with its snowfields', cannot be a component of the thought that Mont Blanc is more than 4,000 metres high. Nor can a thought about Etna contain lumps of solidified lava.\n\nFrege's notion of sense is somewhat obscure, and neo-Fregeans have come up with different candidates for its role. Accounts based on the work of Carnap and Church treat sense as an intension, or a function from possible worlds to extensions. For example, the intension of ‘number of planets’ is a function that maps any possible world to the number of planets in that world. John McDowell supplies cognitive and reference-determining roles. Devitt treats senses as causal-historical chains connecting names to referents.\n\nIn his theory of descriptions, Bertrand Russell held the view that most proper names in ordinary language are in fact disguised definite descriptions. For example, 'Aristotle' can be understood as \"The pupil of Plato and teacher of Alexander,\" or by some other uniquely applying description. This is known as the descriptivist theory of names. Because Frege used definite descriptions in many of his examples, he is often taken to have endorsed the descriptivist theory. Thus Russell's theory of descriptions was conflated with Frege's theory of sense, and for most of the twentieth century this 'Frege-Russell' view was the orthodox view of proper name semantics. However, Saul Kripke argued compellingly against the descriptivist theory. According to Kripke, proper names are rigid designators which designate the same object in every possible world. Descriptions such as 'the President of the U.S. in 1970' do not designate the same in every possible world. For example, someone other than Richard Nixon, e.g. Hubert Humphrey, might have been the President in 1970. Hence a description (or cluster of descriptions) cannot be a rigid designator, and thus a proper name cannot \"mean\" the same as a description.\n\nHowever, the Russellian descriptivist reading of Frege has been rejected by many scholars, in particular by Gareth Evans in \"The Varieties of Reference\" and by John McDowell in \"The Sense and Reference of a Proper Name,\" following Michael Dummett, who argued that Frege's notion of sense should not be equated with a description. Evans further developed this line, arguing that a sense without a referent was not possible. He and McDowell both take the line that Frege's discussion of empty names, and of the idea of sense without reference, are inconsistent, and that his apparent endorsement of descriptivism rests only on a small number of imprecise and perhaps offhand remarks. And both point to the power that the sense-reference distinction \"does\" have (i.e., to solve at least the first two problems), even if it is not given a descriptivist reading.\n\nAs noted above, translators of Frege have rendered the German \"Bedeutung\" in various ways. The term 'reference' has been the most widely adopted, but this fails to capture the meaning of the original German ('meaning' or 'significance'), and does not reflect the decision to standardise key terms across different editions of Frege's works published by Blackwell. The decision was based on the principle of exegetical neutrality, namely that 'if at any point in a text there is a passage that raises for the native speaker legitimate questions of exegesis, then, if at all possible, a translator should strive to confront the reader of his version with the same questions of exegesis and not produce a version which in his mind resolves those questions'. The term 'meaning' best captures the standard German meaning of \"Bedeutung\", and Frege's own use of the term sounds as odd when translated into English as it does in German. Moreover, 'meaning' captures Frege's early use of \"Bedeutung\" well, and it would be problematic to translate Frege's early use as 'meaning' and his later use as 'reference', suggesting a change in terminology not evident in the original German.\n\nThe Greek philosopher Antisthenes, a pupil of Socrates, apparently distinguished \"a general object that can be aligned with the meaning of the utterance” from “a particular object of extensional reference.\" This \"suggests that he makes a distinction between sense and reference.\" \nThe principal basis of this claim is a quotation in Alexander of Aphrodisias's “Comments on Aristotle's 'Topics'” with a three-way distinction: \n\nThe sense-reference distinction is commonly confused with that between connotation and denotation, which originates with John Stuart Mill. According to Mill, a common term like 'white' \"denotes\" all white things, as snow, paper. But according to Frege, a common term does not refer to any individual white thing, but rather to an abstract Concept (\"Begriff\"). We must distinguish between the relation of reference, which holds between a proper name and the object it refers to, such as between the name 'Earth', and the planet Earth, and the relation of 'falling under', such as when the Earth falls under the concept \"planet\". The relation of a proper name to the object it designates is direct, whereas a word like 'planet' has no such direct relation at all to the Earth at all, but only to a concept that the Earth falls under. Moreover, judging \"of\" anything that it falls under this concept is not in any way part of our knowledge of what the word 'planet' means. The distinction between connotation and denotation is closer to that between Concept and Object, than to that between 'sense' and 'reference'.\n\n"}
{"id": "5711642", "url": "https://en.wikipedia.org/wiki?curid=5711642", "title": "Senseless violence", "text": "Senseless violence\n\nSenseless violence or zinloos geweld (Dutch) is a term frequently used by among others the media, politicians and NGOs to define the nature of several shocking events in Belgium and the Netherlands in recent years. The use of the term is politically charged and may not reflect any unique elements of any particular crime given that label.\n\nThe term expresses the perceived senselessness of the occurred acts of violence; the perpetrator and the victim do not know each other, the violence seems not to be motivated by greed or other common factors. The violence occurs suddenly and often under the influence of alcohol.\n\nThe term \"senseless violence\", in the meaning used in this article, was first used in 1997 by Cees Bangma, district chief of the Dutch police unit Midden-Friesland. Before 1997 the term did not carry the same moral connotation in Belgian and Dutch culture, and typically referred to overseas warzone violence. Bangma used it in a letter written to the Leeuwarder Courant, a Frisian newspaper in which he made an appeal to the Dutch population to have a minute of silence for Meindert Tjoelker, who was killed on 13 September 1997. This minute was necessary \"to make it clear to everyone that the Frisian society does not accept senseless violence\". A wave of reactions followed, which also included media hype. As a consequence, much more attention was spent to every similar case of violence, which led to the perception that violence was on the increase.\n\nA political group \"Landelijke Stichting Tegen Zinloos Geweld\" (national foundation against senseless violence) was created and people created the lady bug tile as a memorial or reminder against senseless violence. The group send guest teachers to schools and sport associations, but became rather inactive since ca. 2009.\n\nThe term is often criticized. Some people think that violence is never legitimate and consider the term to be a tautology. Others say that the term has never been clearly defined and does not help to understand a peticular phenomenon. Others point out that the term is or can be misused by politicians who strive for more repressive measures against crimes.\n\nIn 2016, the national newspaper NRC reported about a recent case of youngsters mistreating a homeless person. The newspaper quoted a spokesperson of the police who said that hardly any violence happens \"out of the blue\". Criminologists have started to use the term \"uitgaansgeweld\" instead, violence occurring \"going out\" in the evening to pubs and bars. Because of the local approach there are no recent national figures about the phenomenon, according to an expert.\n\nWhile there have been multiple isolated cases of fatal senseless violence in the decade since 1997, the number of deadly incidents surged since April in 2006. The following is a non-exhaustive list of deadly incidents since 1998:\n\n\n\n"}
{"id": "47570046", "url": "https://en.wikipedia.org/wiki?curid=47570046", "title": "Shaun King", "text": "Shaun King\n\nJeffery Shaun King (born September 17, 1979) is an American writer and civil rights activist. He is noted for his use of social media to promote social causes, including the Black Lives Matter movement. He is a columnist for \"The Intercept\". Previously, he was a contributing writer for Daily Kos and a political commentator for \"The Young Turks\". He co-founded the Real Justice PAC in February 2018, which supports progressive candidates running for district attorney offices in 2018.\n\nKing grew up in Versailles, Kentucky. He was raised by his white mother and white presumptive father, Jeffrey King. King grew up believing what his mother later confirmed to him: that his biological father was a light-skinned black man. According to a local police detective, those who knew him were aware of his biracial heritage: \"Anyone from around here who knew him knew he was mixed.\" King attended Huntertown Elementary School and Woodford County High School.\n\nKing attended Morehouse College, a private, historically black men's college in Atlanta, Georgia, where he majored in history. Midway through his education, he had to take a medical leave. Upon his return, he was named an Oprah Winfrey Scholar by Morehouse. Oprah scholars are given financial support and are required to maintain their grade point average and do community service. King fulfilled his community service requirement by tutoring and mentoring students at Franklin Lebby Stanton Elementary School in Atlanta. After graduation in 2002, King was a research assistant for Morehouse history professor Alton Hornsby Jr.\n\nAfter graduation, King was a high school civics teacher for about a year and then became a motivational speaker for Atlanta's juvenile justice system. He was then a pastor at Total Grace Christian Center in DeKalb County, Georgia. In 2008, King founded a church in Atlanta called \"Courageous Church\". He made use of social media to recruit new members and was known as the \"Facebook Pastor\".\n\nIn March 2010, while still a pastor, he founded aHomeinHaiti.org as a subsidiary of Courageous Church and used eBay and Twitter to raise $1.5 million to send tents to Haiti after the 2010 Haiti earthquake. \"Desperate Housewives\" star Eva Longoria was a spokesperson for the campaign. This inspired him to launch TwitChange.com, a charity auction site. TwitChange held Twitter charity auctions on eBay where celebrities offered to retweet winning bidders' tweets in exchange for support of a particular charity. One campaign raised funds to build an orphanage in Bonneau, Haiti. In 2010, TwitChange won the Mashable Award for \"Most Creative Social Good Campaign\".\n\nIn 2012, King resigned from the Courageous Church, citing personal stress and disillusionment. That same year he and web designer Chad Kellough founded HopeMob.org, a charity site that used voting to select a particular person's story and then raise money for that story until its goal was met. The money went to an organization which provided for the person's needs, not to the person individually. After one goal was met, the next story in line would then get funds raised. HopeMob initially raised funds to build their platform in January 2012 on the crowdfunding site Kickstarter. Their campaign raised about $125,000.\n\nIn 2014, he and two co-inventors, Ray Lee and Vincent Tuscano, were awarded U.S. patent 8,667,075, \"System and method for implementing a subscription-based social media platform\". This patent was filed by the startup he founded, @Upfront.\n\nIn 2015, he wrote the self-help book \"The Power of 100\".\n\nOn October 2, 2015, the \"New York Daily News\" announced that it was hiring King to the new position of senior justice writer, where he would focus on reporting and commentary on social justice, police brutality and race relations. He left the Daily News in August 2017.\n\nOn December 28, 2016, Cenk Uygur announced that King had been hired as a political commentator for \"The Young Turks\".\n\nKing has written extensively about incidents in the Black Lives Matter movement, gaining prominence during the events following the shooting of Michael Brown. King wrote an article analyzing the Brown crime scene, and argued that the evidence suggested that officer Darren Wilson's life was not in danger during the shooting.\n\nKing became a contributing blogger for the politically liberal website the Daily Kos in September 2014. His contributions to the website have focused on civil rights, violence in Ferguson, Missouri, and Charleston, South Carolina, as well as allegations of police brutality, especially toward the black community. In August 2015, he launched Justice Together, an organization to identify police brutality and lobby local politicians for change. To the surprise of many of the group's members, King unilaterally disbanded the organization in the fall of 2016. \n\nKing announced that he would leave the Democratic Party after the 2016 election due to allegations of corruption and lack of neutrality in the party during the primaries.\n\nIn September 2016, King proposed an Injustice Boycott for later that year in December.\n\nIn an October 11, 2017 article in \"The Washington Post\", Shaun King was credited with leading a successful months-long and far-reaching social media campaign which led to the identification and arrest of three of the men behind the August 12, 2017 assault on DeAndre Harris during the Unite the Right rally. 18-year-old Daniel P. Borden from Mason, Ohio; 33-year-old Alex Michael Ramos of Marietta, Georgia; and 22-year-old Jacob Scott Goodwin from Ward, Arkansas, were arrested for the parking garage beating. \"The Washington Post\" described how the attack on Harris became a \"symbol of the violence and racial enmity that engulfed Charlottesville when white supremacists, Klan members and neo-Nazis clashed with counterprotesters.\" Two were subsequently convicted while two others are awaiting trial.\n\nHarris was later served with an arrest warrant sought by 48-year-old Harold Crews, North Carolina's League of the South chairman and a real estate lawyer, who alleged that Harris had hit him with a flashlight during an altercation prior to the Market Street Garage brawl. Crews used a law by which alleged crime victims who have filed a police report can get a warrant if they can convince a local judge to sign it. In the interview with the \"Washington Post\", King responded, \"I am disgusted that the justice system bent over backwards to issue a warrant for one of the primary victims of that day, when I and others had to fight like hell to get that same justice system to prosecute people who were vicious in their attacks against Harris and others. Now, we're seeing white supremacists celebrate on social media, bragging about Harris's arrest. They're hailing this as a victory.\" Harris was later acquitted of misdemeanor assault by a local judge.\n\nOn May 20, 2018 King accused a white Texas state trooper of raping Sherita Dixon-Cole, an African-American human resources professional. The trooper arrested Dixon-Cole for drunk driving and King based his accusation on statements she and her family made to King and Philadelphia lawyer S. Lee Merritt. King's social media posts, which identified the trooper by name, went viral and threats were made against the arresting trooper as well as another trooper with the same last name. The Texas Department of Public Safety released nearly two hours of body cam footage on May 22 that exonerated the trooper. Merritt subsequently apologized for the false accusation and national attention he had brought to the case. King deleted his social media posts after the body cam video was released.\n\nKing has raised money for multiple causes including the Tamir Rice shooting, and various incidents where the Black Lives Matter movement has been involved. Through the fund-raising website, YouCaring.com, King raised $60,000 for Rice's family. Rice, a 12-year-old resident of Cleveland, Ohio, was killed in 2014 by two Cleveland city policemen after they responded to a complaint \"of a male black sitting on a swing and pointing a gun at people.\"\n\nAfter learning the child had not been buried as of five months after the shooting, and the child's mother had moved into a homeless shelter, he started the fund to assist the Rice family; however, family attorney Timothy Kucharski stated in May 2015 that neither he nor the Rice family had heard of King or the fundraiser, nor had they received any money. The money raised was then seized by the court and placed into Tamir Rice's estate instead of being freely available to the family. King and the Rice family's new legal counsel, Benjamin Crump, then started a second charity drive with the proceeds going directly to the family. An additional $25,000 was raised.\n\nHe is married with five children. Three of his children are biological with his wife and two are by custody and adoption. He has had foster children, nieces and nephews stay with him. He has written extensively about his experiences as a biracial person.\n\nOne of his experiences in high school was what he considered a hate crime assault. King stated a \"dozen rednecks\" had beaten him and the injuries caused him to miss a portion of two years of high school due to multiple spinal surgeries. A band teacher, two fellow students from King's high school, as well as King's wife, posted their recollection of the event to Facebook, backing King's account.\n\nThe detective who investigated the case in 1995 described King's injuries as \"minor\". The associated police report noted that the incident revolved around a fight involving only one other student who defended his girlfriend after being allegedly threatened by King. The report did not indicate the incident was racially motivated. There is no mention of a \"hate crime\" either with local police or with the FBI. Keith Broughton, the investigating detective, said he interviewed six witnesses put forth by the school's principal, including a teacher who broke up the fight. All of them described it as a one-on-one altercation.\n\nIn August 2015, Milo Yiannopoulos questioned King's biracial identity in an article for Breitbart News. Yiannopoulos reported that King's birth certificate lists Naomi Fleming and Jeffrey Wayne King (both of whom are white) as King's parents and that a police report cited King's race as \"white.\"\n\nKing said that the man listed on his birth certificate is his adoptive, not biological father, and that his mother has told him his biological father is a light-skinned black man. In various interviews with King's family and classmates conducted by the mainstream media, they stated that they understood King to be biracial growing up. \nAfter being contacted by reporters, the police officer who listed King's race as \"white\" was interviewed by the Independent Journal Review following Yiannopoulos's article for Breitbart. The officer recalled the case and stated that he believed King to be biracial, and that everyone who knew King presumed he was mixed. He went on to state that he had only listed King as white because he is light-skinned, and biracial was not an option on his form. King and his supporters expressed concern that such questions were an attempt to distract from the Black Lives Matter movement.\n\n"}
{"id": "38798417", "url": "https://en.wikipedia.org/wiki?curid=38798417", "title": "Technoself studies", "text": "Technoself studies\n\nTechnoself studies, commonly referred to as TSS, is an emerging, interdisciplinarity domain of scholarly research dealing with all aspects of human identity in a technological society focusing on the changing nature of relationships between the human and technology. As new and constantly changing experiences of human identity emerge due to constant technological change, technoself studies seeks to map and analyze these mutually influential developments with a focus on identity, rather than technical developments. Therefore, the self is a key concept of TSS. The term \"technoself\", advanced by Luppicini (2013), broadly denotes evolving human identity as a result of the adoption of new technology, while avoiding ideological or philosophical biases inherent in other related terms including cyborg, posthuman, transhuman, techno-human, beman (also known as bio-electric human), digital identity, avatar, and homotechnicus though Luppicini acknowledges that these categories \"capture important aspects of human identity\". Technoself is further elaborated and explored in Luppicini's \"Handbook of Research on Technoself: Identity in a Technological Environment\".\n\nTechnoself evolved from early groundwork in identity studies, philosophy of mind, and cognitive science. René Descartes is often credited as one of the first identity theorists of Modernity to question the material world and the certainty of knowledge from the self. Despite heavy criticism, the question he posed regarding the necessary relation between the mind and body is still considered a prevalent theme in contemporary discussions of identity and technology. Another major development in identity studies came from early social psychology, sociology and psychoanalysis. Beginning with Freud, the psychoanalytic tradition shed some light on the dynamics of identity and personality development. Erving Goffman expanded the inquiry of identity with his dramaturgical theory, which emphasized the centrality of the social realm and the notion of self-presentation to identity. Later, Foucault further expanded the area of inquiry by contemplating how technologies could facilitate the emergence of new ways of relating to oneself.\n\nThe most entrenched area of technoself studies is revolved around ontological considerations and conceptualizations of technoself. The effort to identify the essence of human being is frequent in philosophical circles and is entrenched within emerging theoretical scholarship on technoself. DeGrazia's (2005) examination on identify/numerical identity to shed light on the ethics of human enhancement. According to DeGrazia, human identity is divided into two parts: 1) numerical identity (concerns the continuity of an individual as the same object over time or across procedure), and 2) narrative identity (concerns the changes in self-perception experienced by an individual over time). By dividing human identity into two parts, DeGrazia is facilitating a discussion on the ethics of human enhancements. Meanwhile, Croon Fors(2012) research on the entanglement of the self and digitalization have helped frame ontological considerations related to the conceptualization of technoself studies. Furthermore, the changing nature of identity is a common theme within technoself studies. As a result, this has given way for scholars to analyze questions such as: How are advances in sensing technologies, biometrics, and genetics changing the way we define and recognize identity? How are technologies changing the way people define themselves and present themselves in society? These types of questions are being heavily analyzed as the conceptualization of identity is changing rapidly.\n\nCentral to the understanding of the development of technoself studies as a field of research is the idea that human identity is shaped by the adoption of new technologies and the relationship between humans and technology. Advancements in digital technology have recently forced researchers to consider the conception of the self in relation to the increasing reliance of society on the use of technologies (such as cellphones, tablets, and social media) in daily tasks in peoples' personal and professional lives. New technologies, particularly computer-mediated communication tools, have raised questions related to identity in relationship to privacy issues, virtual identity boundaries, online fraud, citizen surveillance, etc. These issues come as our perspective on technology shifts from one of functionality to one of interaction. According to John Lester, in the future, \"we won't simply enjoy using our tools, we will come to care for them\".\n\nThe noeme is a term coined in 2011 by biogerontologist Marios Kyriazis, and it denotes a \"combination of a distinct physical brain function and that of an outsourced virtual one\". A noeme is the intellectual \"networked presence\" of an individual within the global brain, a meaningful synergy between each individual human, their social interactions and artificial agents, globally connected to other noemes through digital communications technology. Kyriazis further clarifies that: \"\"...The noeme is structurally coupled with its medium, i.e. the computer/internet. It continuously generates its own organisation and specifies its operation and content. As a self-organising system it adjusts to external influences and reinvents itself in order to adapt to its environment i.e. it reproduces (self-replicates) horizontally in a process that can be termed 'noemic reproduction'. This digital intellectual manifestation of a person, if successful, will lead to others copying it, thus noemes are replicating... A noeme is not just a collection of single ideas or solitary intellectual achievements. It is the total sum of all individual cognitive efforts and active information-sharing accomplishments of a person, the intellectual standing of a person within the Global brain.\"\n\nA cyborg (cybernetic organism) is a term referring to individuals with \"both biological and artificial parts.\" Cyborgs are known as being half-human, half machine organisms, due to the fact that they are always connected with technology. This term, which was coined in 1960 by Manfred Clynes, refers to and acknowledges those beings whose abilities have been enhanced due to the presence and advancement of technology. The notion of cyborg has played a part in breaking down boundaries between humans and non-humans living within a technologically advanced society. For example, those who have installed pacemakers, hearing aids, artificial body parts, cochlear implants as well as other technologies that may aid in enhancing an organisms abilities and capacities to perform, either physically or mentally.\nHugh Herr, an American rock climber, engineer, and biophysicist, has successfully invented the next generation of cyborg (bionic limbs and robotic prosthetics). As the head of the Media Lab's Biomechatronics group in MIT, he shared his experience and presented the team achievement first time in a TED talk show.\n\nTranshuman is a concept that emerged as a result of the transhumanist movement which is centred around the notion of improving the abilities of human beings mainly through both 'scientific and technical means.' Unlike the posthuman concept, the notion of transhuman is based on human augmentation but does not commit itself to positing a new separate species. The philosophy of transhumanism was developed in the 1990s by British philosopher Max More who articulated the principles of transhumanism as a futurist philosophy. However, the transhuman philosophy has also been subject to scrutiny by prominent scholars such as Francis Fukuyama.\n\nPosthuman is a concept that aims towards signifying and characterizing a fresh and enhanced type of being. This organism is highly representative of a being that embraces drastic capabilities that exceed current human capabilities that are presently defining human beings. This posthuman state of identity has mainly resulted from the advancement of technological presence. According to Luppicini, posthuman capabilities \"suggest a new type of being over and above human. This compromises the neutrality needed for a clear conception of human identity in the face of human-technological integration.\" This concept aims towards enabling a brighter future concerned with gaining a better perception of the world through various viewpoints.\n\nHomo technicus is a term \"first coined by Galvin in 2003 to help refine the definition of human beings to more accurately reflect the evolving condition of human beings intertwined within advancing technological society\". It refers to the notion that human beings are technological by nature and evolve simultaneously with technology. Galvin states in his article titled \"On Technoethics\", \"mankind cannot do away with the technical dimension, going even to the\npoint of considering this part of its constitution: mankind is technical by nature. Technology is not an\naddition to man but is, in fact, one of the ways in which mankind distinguishes itself from animals.\" Luppicini builds upon the concept of homo technicus in his book \"Handbook of Research on Technoself: Identity in a Technological Society\". Luppicini feels that the notion of homo technicus contributes to the conception of humans as technoselves in two ways. First it helps to solidify the idea of technology as being a key component in defining humans and society and secondly it demonstrates the importance of technology as a human creation that aligns with human values. He further goes onto explain that human interactions with the material world around them helps to create meaning and this unique way of creating meaning has affected how humans have evolved as a species.\n\nNote: the term homo technicus was coined earlier than 2003. For instance, it was used by Russell E. Willis in his 1990 PhD dissertation for Emory University: Toward a Theological Ethics of Technology: An analysis in Dialogue with Jacques Ellul, James Gustafson, and the Philosophy of Technology. It was later used by Willis in \"Complex Responsibility in an Age of Technology,\" in Living Responsibly in Community, ed. Fredrick E. Glennon, et al. (University Press of America, 1997): 251ff. In both publications homo technicus is offered as a model for the responsible self in an age of pervasive technology.\n\nAvatars represent the individual, the individual's alter ego, or character(s) within virtual environments controlled by a human user. Avatars provide a unique opportunity to experiment with one's identity construction within virtual worlds (Turkle, 1995) and to do so with others\". Examples of avatars can include personas in online games or virtual life simulations such as Second Life.\n\nA new hybrid form of creature that results from an intertwinement between human and machine.\n\nA techno-sapien would be a slang term for a human being who is familiar and comfortable with technology. Someone who has the latest gadgets and electronic machinery would be techno-sapien.\n\nDigital identity is the data that uniquely describes a person or a thing and contains information about the subject's relationships. The social identity that an internet user establishes through digital identities in cyberspace is referred to as online identity.\n\nThe areas of focus in TSS are: philosophical inquiry and theoretical framing, digital identity and virtual life, human enhancement technologies, and their regulation. These areas of study have been influenced by extensive research, and input from an editorial advisory and review board over the course of several years.\n\nDigital identity and virtual life looks at how individuals explore, develop and represent their identities in online, virtual, or mediated environments. Research on virtual life and digital identities is concerned not only with how individuals relate to their own mediated identities, but also with how they relate to those of others. With the current popularity of social networking service sites, it is no surprise that TSS scholars have also begun studying the effects that such constant and mediated social connections have on identity. Topics that fall under this category have included intellectual disability, gender identity, and mass media in sport.\n\nCritical areas of research include: how individuals treat the identity of others in an online space; how people use media to develop and project their identity; and how digital representation can alter life meaning and identity (Luppicini, 2013). Such research examines the advantages and disadvantages of online life and digital identity construction.\n\nAreas of digital identity and virtual life have become quite popular, e.g. online avatars. Scholars are now focused on the role avatars play in identity exploration, priming behaviours, and self-presentation. Other research looks at the use of communication technologies by immigrant individuals as part of a digital diaspora. These scholars examine a trend in which diasporic immigrants who feel disconnected from their cultural identities have turned to digital technologies as a way to reconnect.\n\nThe term \"technoself\" is often used interchangeably with \"virtual self\". In this case, technoself is used to refer to a virtual manifestation of one's self. The ability to project one's self into a virtual world allows users to control their appearance and personality. Users are able to customize their virtual identity and craft a persona to their liking. The malleability of online identities allows users to not only create their own virtual self, but also to continually change and mold their online selves in ways impossible to do with their real identities. Users can edit and change their virtual selves' appearance and behavior to control other users' perception of them.\n\nThe ability to create and change your identity in this way, is due to anonymity. Anonymity is a paramount and dynamic feature of virtual social interaction within the online public sphere. As individuals are not required to reveal their real identity, they are able to explore new and undiscovered aspects of themselves. In this expansion of the self, anonymous individuals may try on various identities which break traditional social norms, without fear of retribution or judgment. This contributes to the creation of 'super-selves', through which individuals may amplify aspects of their projected identities in order to form an ideal expression of the self. The fact that the vast majority of virtual encounters are anonymous in nature allows a 'strangers on a train' phenomenon to\ntake place. Through invented and unknown personae, individuals are able to engage in self-disclosure, transvestism, and fantasies. However, this freedom may not be absolute, as there are many risks in participating in an online community, including identity theft and the potential linkage between anonymous and manifest identities. Anonymity also may have legal ramifications, making it difficult for law enforcement to maintain control over online communities. Tracking down online law-breakers is difficult when their identity is unknown. Anonymity also frees individuals so that they are able to behave in socially undesirable and harmful ways, which can result in forms of hate speech and cruel online behaviour. Lastly, anonymity also diminishes the integrity of information, and as a result, diminished the overall trust of online environment.\n\nMany online users choose to attempt anonymity through the use of avatars. Users associate themselves with avatars as digital representatives within a duplicated and simulated virtual community. The user's body is essentially plugged in within the avatar world, thereby creating the illusion of infinite \"space\" behind the computer screen. As a result, they provide the opportunity for users to manipulate their worlds and the spaces and objects with which they interact. Participation in online communities has resulted in the creation of a virtual economy based on the semantic value of digital products. This form of online consumerism is centered on the creation of avatars as extensions of the self. The purchase of symbolic goods for these avatars relates to the emotional and social value that the user holds for these items. These products may indicate roles or personality traits of players within a community and consist primarily of task oriented and nonfunctional items.\n\nLuppicini argues that the rise of online life creates serious questions on the advantages and disadvantages of online communities along with the challenges to online identity construction (Turkle, 1999). He notes the negative influence of the impersonality of virtual communities on offline interaction and the consequence of Internet addiction. Sherry Turkle states: \"We discovered the network – the world of connectivity – to be uniquely suited to the overworked and overscheduled life it makes possible. And now we look to the network to defend us against loneliness even as we want it to control the intensity of our connections\".\n\nComputer networking and smart technologies such as radio frequency identification (RFID), geographical information systems (GIS), and global positioning systems (GPS) are providing new tools and techniques for monitoring individuals and their behavior. The rise in these types of technologies has raised concerns over the invasion of privacy, and the misuse of information. That is because the networked identity of technoselves can be exploited by third parties who may want to gain access and control over personal information. Moreover, the implications of the sophisticated technologies for identifying and tracking people, the storage of this data, and the governmental use of surveillance to track suspicious types of people are significant issues in privacy/surveillance and TSS. The availability of related technologies (e.g. EyeTap, Memoto) to individuals (as opposed to governments or commercial interests) has also led to the phenomenon dubbed sousveillance, whereby individuals track or record authorities' activities either online or in real environments.\n\nLeading scholars in the study of surveillance include David Lyon and Mark Andrejevic. In addition to contributing to the advent of citizen journalism, the proliferation of sousveillance technologies has suggested a number of legal/regulatory, ethical, and social implications for democratic and consumer rights. A dramatic illustration of these concerns comes from University of Toronto Professor Steve Mann, a privacy rights advocate and pioneering engineer of such technologies. After being allegedly assaulted in a French McDonald's restaurant for wearing an augmented-reality digital eye glass device, Mann was, ironically, allegedly denied access to McDonald's own surveillance camera footage. This led to Mann's coinage of the term \"McVeillance\" for instances of surveillance/sousveillance double standards and to his contribution the proposal of the Mann-Wassell law in the New York legislature.\n\nHuman enhancement technology (HET) is the study of tools that would better and improve a human being's way of life. It seeks to advance and progress what humans already do within their normal lives. However, customarily it seeks to aid any illnesses and weaknesses in the body. Popular topics within this new area of study include, sex reassignment surgery, mood enhancers, genomics, and neuroenhancement. Enhancement within the workplace is a new topic of discussion, while the workplace should be adapting to the various types of human impairment, it seems that improving the workers is of more concern to corporations. Through the use of cyborg prosthetics one can assemble themselves in their own vision, any disfigurement or handicap can possibly disappear. Within the evolution of cyborg prosthetics a human is able to physically grasp things more easily, allowing more of the population to engage in whatever they choose. A large aspect of this technology stems from the ability to determine who may and may not benefit, as well as how access to these new technologies should be controlled.\n\nHuman bodies can now not only be improved upon through natural means, but through the effects of technology. This new form of enhancement is connected with what humans perceive of themselves, and as to how their own identity is created. A human operates based on their abilities; these capabilities are the factors and characteristics that create a personality. The augmentation of these aptitudes leads to a new human, who has a renewed sense of who they are. The term 'free to be me' is closely related to this new form of enhancement, wherein technological enhancements can be either cosmetic or reconstructive. Through the incorporation of medicine and technology \"...cosmetic surgery then becomes a technology through which the body is normalized and homogenized as much as enhanced\". A proper example relating to human enhancement and cyborgs could be the recently convicted Oscar Pistorius. In past years, Pistorius fought with the International Olympic Committee to have a place in the hurdles events of the 2012 Olympics in London. The controversy surrounding Pistorius extended to his artificial legs, and how they compared to the natural human anatomy; did Pistorius have an unfair advantage over his competitors? The ruling was left up to a scientific analysis of his legs and running stride which ultimately lead to his participation in the Olympic Games. Therefore, we've come to a time where decisions, and thus human panels, need to determine what is human, what is natural, and what is artificial.\n\nRights and privacy issues over human enhancement technology has given rise to challenging topics within technoself studies. For example, the consideration of ethical policies and guidelines in the deployment of HET is an emerging topic within TSS. Further, the question of access to HET, and where we draw the line between necessary therapeutic technologies, and frivolous human enhancement are being raised in TSS. Therefore, the emerging topic regarding the rights and privacy over HET is of great interest within TSS. Popular HET's topics in recent research academia include: Sex (re assignment) (Diamond & Sigmundson, 1997; Zucker, 2002), mood enhancers (Rabin, 2006), cognitive enhancers (Walker, 2008), genomics (Zwart, 2009), and neuroenhancement (Northoff,1996). A second line of inquiry explores social, legal, and ethical aspects of human enhancement and possible threats to human dignity that could arise from the implementation of human enhancements\n(Bostrom, 2005).\n\nSome critiques engage a discussion between the development of HET and the socio-economic environment. Francis Fukuyama, an American political scientist concerns about the future of HET might cause the extension of contradiction between the rich and poor within comparatively rich, industrialized nations because HET is likely to be a luxury product. At the moment, HET seems to be hard to be mainstream in public health services due to the price, which creates a deeper distinctions among those who can afford the technology and those who will remain disabled.\n\nTranshuman thought focuses on beliefs held that the fundamental transformation of the human condition will be through the development of various technology, which will eventually eliminate human aging and will enhance human capacities, both physical and mental. Believers in this theory think that the future of human development will see a new intelligent species that will be enhanced by the technological advances. They use these technological advances to approach various issues regarding the human experience, like morality and health issues. They see this convergence happening through the support of current technologies and the vision of technology in the future; using these advances to eventually make humans more than human, enhanced through this technology. Their central argument is that humans need to be able to choose whether or not this technology is used by them or not.\n\nThis theory expands on the notion of technoself, as transhumanism poses what to many who hold these beliefs is the natural evolution of the human condition. Many look to the history of technological advancements as proof that these future advancements are possible, at least in theory.\n\nAvatars are a visual representation of a user in an online environment. This representation may be an accurate physical representation of the user, or may be completely different. This online representation may affect the offline self. Pena and his colleagues explored a phenomenon known as the \"Proteus effect\" wherein \"avatars can prime negative attitudes and cognition in desktop virtual settings\". They conducted a study that demonstrated how the appearance and affiliations of an individual's online avatar can alter the individual's offscreen personality and attitudes. Pena's group used virtual group discussions to gauge the aggressiveness of individuals using avatars wearing black cloaks versus their control group counterparts wearing white and found more aggressive intentions and attitudes in the black cloak group.\n\nSimilar results were found in a second study that used Thematic Apperception Test studies to determine the differences between values and attitudes of a control group and a group using a Ku Klux Klan (KKK)-associated avatar. Individuals using the KKK-associated avatars were less affiliative and displayed more negative thoughts than the control group. Further support for Pena et al.'s work can be found in other studies that yielded similar results: \"Yee and Bailenson found that, in an immersive 3D environment, participants using avatars with more attractive faces walked closer and disclosed more information when compared to those using avatars with less attractive faces. In addition...participants using taller avatars tended to negotiate more forcefully in comparison to those using shorter avatars.\" A growing body of evidence supports how our online personas can affect our offline self; altering our attitudes and values.\n\nOnline anonymity is commonly described using the phrase \"On the Internet, nobody knows you're a dog\". Online anonymity allows users to present different versions of themselves in online environments. Unconstrained by physical limitations, users are free to choose and construct their virtual form(s) and identities. Virtual spaces which foster such freedom and anonymity therefore allow users to depart from the expectations, norms, and behaviours of their daily lives. It can be said that this unlimited freedom of anonymous expression allows for the transfer of real world suppressed emotions to the online domain. However, if one continually chooses to express their true self anonymously online as opposed to in the real world via face to face interaction, which realm would be more 'real'? As extreme as this scenario may seem, one could say the suppression of norms and natural expression would deem the physical self the avatar, and the online avatar the true self.\n\nA user's online identity is a social identity that represents the user in the online environment, allowing a user a high level of control over their identity in a way that differs from the offline world. Turkle found that the level of control over creating an online identity also extends to the intensity of connections made in such virtual spaces, as users may engage and disengage at will. Dervin and Abbas note that Turkle, in her early work was \"one of the first to show how anonymity 'provides ample room for individuals to express unexplored parts of themselves' more easily than in face-to-face interaction\". Within this notion of being free in online anonymity, technoself studies also looks at what the element of hiding does to us. Turkle suggests that, \"our networked life allows us to hide from each other, even as we are tethered to each other\". Technoself studies explores what these profiles do to the human unconscious. While people are \"exposing\" themselves, they question their level of exposure and sharing compared to extent in what they are truly hiding in reality.\nFurthermore, when creating online profiles, people risk others' perceptions of the information shared and if they receive the messages that the sender intended. Without verbal communication misperceptions, messages can alter identity or personal development.\n\nAvatars can be an important element of the online presentation of the user. In many cases, \"avatars in blogging were created to accurately reflect their owners' physical appearance, lifestyle and preferences. By contrast, participants in the dating and gaming treatments accentuated certain aspects of their avatar to reflect the tone and perceived expectations of the context\". In other words, individuals often emphasize or downplay certain characteristics depending upon the context of their online interactions. These inconsistencies tend to be trivial, however. For instance, men tend to mildly exaggerate their height, while women often underestimate their weight. This is typically not an attempt to mislead others but to be as honest as possible while still presenting themselves in the best light.\n\nAccording to Vasalou & Joinson, although various online forums may present people with the opportunity to create (an) alternate persona(s), they typically choose to create an avatar or represent themselves in a way that is consistent with reality: \"In having equal access to everyday artifacts and fantasy options, participants were inclined to draw on existing self-views rather than grasping the opportunity to explore other personas\". Furthermore, Vasalou and Joinson also claim that, in the context of online communication, high self-awareness (as demonstrated by an avatar largely consistent with an individual's offline persona), contributes to a higher rate of interpersonal communication.\n\nOne consequence of online anonymity and creating false identities is the ability to \"catfish\". Catfishing is a recent internet phenomenon, of manipulating, deceiving and luring people into relationships, through creating an online fictional persona. In many cases these deceptions are used to create romantic or intimate affairs. Since the affair happens entirely through technology, one is able to hide their true identity and carry on the relationship through their made up character. The majority of these incidences happen through social media sites, such as Facebook, and internet dating sites where people are already looking for love, and therefore can be easily manipulated by peoples personas and deceptions.\n\nNew directions and opportunities in technoself research involving personalized robots and social integration of artificial creatures is becoming an increasing reality. Considering the work of pioneering computer scientists and robotics experts such a Rodney Brooks and Hiroshi Ishiguro, human interaction with personal and social robots reached mainstream audiences beginning with the popularization of robotic dolls and pets for children. Research by Sherry Turkle examines many of the effects of these social robots on children, middle and elderly. There are also robots for adults aimed at therapeutic (technotherapy), personal, and social applications (Paro Phobot, Roxxxy, etc.). These types of therapeutic robots are used in nursing homes and hospitals, with the purpose of creating an environment where one can nurture and communicate with an animal. This allows people in a lonely or isolated environment the ability to have something to care for and interact with that is also able to respond and interact back. This has shown to provide happiness and a larger sense of purpose for the individuals, even if for a short period of time. With personalized robots and the social integration of artificial intelligence, technoself is developing in children through relationships with robotic pets and related robotic technologies based on animals, objects, or people (Tamagotchi, Furby, AIBO, etc.). Current areas of interest in this topic are reported in Melson (2012), which provide helpful insights into children's views about robot pets, children's relationship with robotic pets and, conceptualizations of self-identity within child-robot relationships. Other research is focusing more on personalized robots for adults. If the trend towards the personalization of robots and social integration of artificial creatures continues, it is expected that this research will become more prevalent. David Levy, the artificial intelligence researcher in University of Maastricht contains the forecast of robot and human relationship in his thesis, \"Intimate Relationships with Artificial Partners\". In his interview Forecast: Sex and Marriage with Robots by 2050 with \"LiveScience\", Levy says :\"My forecast is that around 2050, the state of Massachusetts will be the first jurisdiction to legalize marriages with robots\". are real life experience suggesting that humans can develop an psychological level relationship with artificial subjects, even if the subject itself is not in any physical shape. Judith Newman wrote an article on \"New York Times\" about the relationship between the Siri system and her 13-year-old son who has autism. Newman says his son develops a close relationship with the system and learning to show affection to it even though he knows Siri is not 'real'. Newman suggest that Siri could be a potential companion to those children who have a hard time to communicate with people. Duggan (2016) describes how users already form relationships with technology that share many of the features of relationships between humans. These relationships have important implications for the future of healthcare as interactive technology increasingly replaces roles traditionally filled by humans.\n\nHuman enhancement regulation, governance, and legal concerns has become another growing concern for the opportunity of TSS research. According to Saner and Geelen (2012), there is one framework to guide technoself governance which distinguishes six different approaches to which emerging technologies may affect human identity:\n\nLuppicini posits that this sort of model could \"prove invaluable for guiding future decision making directed at the framing of HET regulation debates, as well as leveraging strategic planning and decision making concerning HET adaption standards.\" Technoethics relates to the ethical considerations concerning technology in society.Human enhancement improves aspects of human function and may temporarily or permanently overcome the limitations of the human body through natural or artificial means. The consequences of such technological alterations implies ethical questions such as the unfair physical and mental categorization of certain individuals. Therefore, further consideration will need to be associated with ethical questions surrounding the evolution of technology. With growing trends of artificial intelligence and technological devices, such as Google Glass, stricter regulation will be necessary. Furthermore, Elon Musk recently stated that \"We need to be careful with AI (artificial intelligence). Potentially more dangerous than nukes\", meaning that there may be need to worry about the evolution of technology, and specifically how humans employ it to their benefit.\n\n"}
{"id": "14081394", "url": "https://en.wikipedia.org/wiki?curid=14081394", "title": "Triangular coordinates", "text": "Triangular coordinates\n\nThe term triangular coordinates may refer to any of at least three related systems of coordinates in the Euclidean plane:\n\n"}
{"id": "1026848", "url": "https://en.wikipedia.org/wiki?curid=1026848", "title": "Weyl tensor", "text": "Weyl tensor\n\nIn differential geometry, the Weyl curvature tensor, named after Hermann Weyl, is a measure of the curvature of spacetime or, more generally, a pseudo-Riemannian manifold. Like the Riemann curvature tensor, the Weyl tensor expresses the tidal force that a body feels when moving along a geodesic. The Weyl tensor differs from the Riemann curvature tensor in that it does not convey information on how the volume of the body changes, but rather only how the shape of the body is distorted by the tidal force. The Ricci curvature, or trace component of the Riemann tensor contains precisely the information about how volumes change in the presence of tidal forces, so the Weyl tensor is the traceless component of the Riemann tensor. It is a tensor that has the same symmetries as the Riemann tensor with the extra condition that it be trace-free: metric contraction on any pair of indices yields zero.\n\nIn general relativity, the Weyl curvature is the only part of the curvature that exists in free space—a solution of the vacuum Einstein equation—and it governs the propagation of gravitational waves through regions of space devoid of matter. More generally, the Weyl curvature is the only component of curvature for Ricci-flat manifolds and always governs the characteristics of the field equations of an Einstein manifold.\n\nIn dimensions 2 and 3 the Weyl curvature tensor vanishes identically. In dimensions ≥ 4, the Weyl curvature is generally nonzero. If the Weyl tensor vanishes in dimension ≥ 4, then the metric is locally conformally flat: there exists a local coordinate system in which the metric tensor is proportional to a constant tensor. This fact was a key component of Nordström's theory of gravitation, which was a precursor of general relativity.\n\nThe Weyl tensor can be obtained from the full curvature tensor by subtracting out various traces. This is most easily done by writing the Riemann tensor as a (0,4) valence tensor (by contracting with the metric). The (0,4) valence Weyl tensor is then \nformula_1\nwhere \"n\" is the dimension of the manifold, \"g\" is the metric, \"R\" is the Riemann tensor, \"Ric\" is the Ricci tensor, \"s\" is the scalar curvature, and formula_2 denotes the Kulkarni–Nomizu product of two symmetric (0,2) tensors:\n\nIn full tensor notation, this can be written as \nformula_3\n\nThe ordinary (1,3) valent Weyl tensor is then given by contracting the above with the inverse of the metric.\n\nThe decomposition () expresses the Riemann tensor as an orthogonal direct sum, in the sense that\nThis decomposition, known as the Ricci decomposition, expresses the Riemann curvature tensor into its irreducible components under the action of the orthogonal group . In dimension 4, the Weyl tensor further decomposes into invariant factors for the action of the special orthogonal group, the self-dual and antiself-dual parts \"C\" and \"C\".\n\nThe Weyl tensor can also be expressed using the Schouten tensor, which is a trace-adjusted multiple of the Ricci tensor,\nThen\n\nIn indices,\nwhere formula_8 is the Riemann tensor, formula_9 is the Ricci tensor, formula_10 is the Ricci scalar (the scalar curvature) and brackets around indices refers to the antisymmetric part. Equivalently,\nwhere \"S\" denotes the Schouten tensor.\n\nThe Weyl tensor has the special property that it is invariant under conformal changes to the metric. That is, if \"g\"′ = \"f g\" for some positive scalar function \"f\" then the (1,3) valent Weyl tensor satisfies \"C\"′ = \"C\". For this reason the Weyl tensor is also called the conformal tensor. It follows that a necessary condition for a Riemannian manifold to be conformally flat is that the Weyl tensor vanish. In dimensions ≥ 4 this condition is sufficient as well. In dimension 3 the vanishing of the Cotton tensor is a necessary and sufficient condition for the Riemannian manifold being conformally flat. Any 2-dimensional (smooth) Riemannian manifold is conformally flat, a consequence of the existence of isothermal coordinates.\n\nIndeed, the existence of a conformally flat scale amounts to solving the overdetermined partial differential equation\nIn dimension ≥ 4, the vanishing of the Weyl tensor is the only integrability condition for this equation; in dimension 3, it is the Cotton tensor instead.\n\nThe Weyl tensor has the same symmetries as the Riemann tensor. This includes:\nIn addition, of course, the Weyl tensor is trace free:\nfor all \"u\", \"v\". In indices these four conditions are\n\nTaking traces of the usual second Bianchi identity of the Riemann tensor eventually shows that\nwhere \"S\" is the Schouten tensor. The valence (0,3) tensor on the right-hand side is the Cotton tensor, apart from the initial factor.\n\n\n"}
{"id": "145555", "url": "https://en.wikipedia.org/wiki?curid=145555", "title": "XOR swap algorithm", "text": "XOR swap algorithm\n\nIn computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. \"Distinct\" means that the variables are stored at different, non-overlapping, memory addresses; the actual values of the variables do not have to be different.\n\nConventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:\nX := X XOR Y\nY := Y XOR X\nX := X XOR Y\nThe algorithm typically corresponds to three machine-code instructions. Since XOR is a commutative operation, X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:\n\nIn the above System/370 assembly code sample, R1 and R2 are distinct registers, and each XR operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and places the result of the operation in the first register.\n\nHowever, the algorithm fails if \"x\" and \"y\" use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be \"swapped with itself\". Note that this is \"not\" the same as if \"x\" and \"y\" have the same values. The trouble only comes when \"x\" and \"y\" use the same storage location, in which case their values must already be equal. That is, if \"x\" and \"y\" use the same storage location, then the line:\n\nX := X XOR Y\n\nsets \"x\" to zero (because \"x\" = \"y\" so X XOR Y is zero) \"and\" sets \"y\" to zero (since it uses the same storage location), causing \"x\" and \"y\" to lose their original values.\n\nThe binary operation XOR over bit strings of length formula_1 exhibits the following properties (where formula_2 denotes XOR):\n\n\nSuppose that we have two distinct registers codice_1 and codice_2 as in the table below, with initial values \"A\" and \"B\" respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.\n\nAs XOR can be interpreted as binary addition and a pair of values can be interpreted as a point in two-dimensional space, the steps in the algorithm can be interpreted as 2×2 matrices with binary values. For simplicity, assume initially that \"x\" and \"y\" are each single bits, not bit vectors.\n\nFor example, the step:\n\nX := X XOR Y\n\nwhich also has the implicit:\n\nY := Y\n\ncorresponds to the matrix formula_9 as\nThe sequence of operations is then expressed as:\n(working with binary values, so formula_12), which expresses the elementary matrix of switching two rows (or columns) in terms of the transvections (shears) of adding one element to the other.\n\nTo generalize to where X and Y are not single bits, but instead bit vectors of length \"n\", these 2×2 matrices are replaced by 2\"n\"×2\"n\" block matrices such as formula_13\n\nNote that these matrices are operating on \"values,\" not on \"variables\" (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.\n\nA C function that implements the XOR swap algorithm:\nNote that the code does not swap the integers passed immediately, but first checks if their addresses are distinct. This is because, if the addresses are equal, the algorithm will fold to a triple *x ^= *x resulting in zero.\n\nThe code below is an example of overly-concise C code; the behavior of the code is undefined. (Writing overly-concise C code has become unnecessary as modern optimizing compilers will eliminate intermediate results and temporary variables.)\nThe XOR swap algorithm can also be defined with a macro:\nIn most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:\n\n\nBecause these situations are rare, most optimizing compilers do not generate XOR swap code.\n\nMost modern compilers can optimize away the temporary variable in the native swap, in which case the native swap uses the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. The XOR swap is also much less readable and completely opaque to anyone unfamiliar with the technique.\n\nOn modern CPU architectures, the XOR technique can be slower than using a temporary variable to do swapping. One reason is that modern CPUs strive to execute instructions in parallel via instruction pipelines. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of instruction-level parallelism.\n\nA historical reason was that it used to be patented (US4197590). Even then, this was only for computer graphics.\n\nThe XOR swap is also complicated in practice by aliasing. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.\n\nSimilar problems occur with call by name, as in Jensen's Device, where swapping codice_3 and codice_4 via a temporary variable yields incorrect results due to the arguments being related: swapping via codice_5 changes the value for codice_3 in the second statement, which then results in the incorrect i value for codice_4 in the third statement.\n\nThe underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:\n\nUnlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as modular arithmetic or bignums to guarantee that the computation of codice_8 cannot cause an error due to integer overflow. Therefore, it is seen even more rarely in practice than the XOR swap.\n\nNote, however, that the implementation of codice_9 above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and subtraction of unsigned integers follow the rules of modular arithmetic, i. e. are done in the cyclic group formula_14 where formula_15 is the number of bits of codice_10. Indeed, the correctness of the algorithm follows from the fact that the formulas formula_16 and formula_17 hold in any abelian group. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group formula_18.\n\nPlease note that the above doesn't hold when dealing with the codice_11 type (the default for codice_12). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).\n\n"}
{"id": "201419", "url": "https://en.wikipedia.org/wiki?curid=201419", "title": "Śūnyatā", "text": "Śūnyatā\n\nŚūnyatā (; ) – pronounced ‘shoonyataa’, translated into English most often as \"emptiness\" and sometimes \"voidness\" – is a Buddhist concept which has multiple meanings depending on its doctrinal context. It is either an ontological feature of reality, a meditation state, or a phenomenological analysis of experience.\n\nIn Theravada Buddhism, suññatā often refers to the non-self (Pāli: \"anattā\", Sanskrit: \"anātman\") nature of the five aggregates of experience and the six sense spheres. Suññatā is also often used to refer to a meditative state or experience.\n\nIn Mahayana, \"Sunyata\" refers to the tenet that \"all things are empty of intrinsic existence and nature (\"svabhava\"),\" but may also refer to the Buddha-nature teachings and primordial or empty awareness, as in Dzogchen and Shentong.\n\n\"Śūnyatā\" (Sanskrit) is usually translated as \"devoidness,\" \"emptiness,\" \"hollow, hollowness,\" \"voidness.\" It is the noun form of the adjective \"śūnya\" or \"śhūnya\", plus \"-tā\": \n\nThe concept of Sunyata as \"emptiness\", states Sue Hamilton, is related to the concept of \"anatta\" in early Buddhism. Over time, many different philosophical schools or tenet-systems (Sanskrit: \"siddhānta\") have developed within Buddhism in an effort to explain the exact philosophical meaning of emptiness.\n\nAfter the Buddha, emptiness was further developed by the Abhidharma schools, Nāgārjuna and the Mādhyamaka school, an early Mahāyāna school. Emptiness (\"positively\" interpreted) is also an important element of the Buddha nature literature, which played a formative role in the evolution of subsequent Mahāyāna doctrine and practice.\n\nThe Pali canon uses the term emptiness in three ways: \"(1) as a meditative dwelling, (2) as an attribute of objects, and (3) as a type of awareness-release.\"\n\nThe \"Suñña Sutta\", part of the Pāli canon, relates that the monk Ānanda, Buddha's attendant asked, \nAccording to Thanissaro Bhikku:\n\nEmptiness as a meditative state is said to be reached when \"not attending to any themes, he [the bhikku] enters & remains in internal emptiness\" (MN 122). This meditative dwelling is developed through the \"four formless states\" of meditation or Arūpajhānas and then through \"themeless concentration of awareness.\"\n\nThe Cūlasuññata-sutta (MN III 104) and the Mahāsuññata-sutta (MN III 109) outline how a monk can \"dwell in emptiness\" through a gradual step by step mental cultivation process, they both stress the importance of the impermanence of mental states and the absence of a self.\n\nIn the Kāmabhu Sutta S IV.293, it is explained that a bhikkhu can experience a trancelike contemplation in which perception and feeling cease. When he emerges from this state, he recounts three types of \"contact\" (\"phasso\"):\nThe meaning of emptiness as contemplated here is explained at M I.297 and S IV.296-97 as the \"emancipation of the mind by emptiness\" (\"suññatā cetovimutti\") being consequent upon the realization that \"this world is empty of self or anything pertaining to self\" (\"suññam ida attena vā attaniyena vā\").\n\nThe term \"emptiness\" (\"suññatā\") is also used in two suttas in the \"Majjhima Nikāya\", in the context of a progression of mental states. The texts refer to each state's emptiness of the one below.\n\nSome of the Sarvāstivādin Agama sutras (extant in Chinese) which have emptiness as a theme include Samyukta Agama 335 - \"Paramārtha-śunyatā-sūtra\" (Sutra on ultimate emptiness) and Samyukta Agama 297 - \"Mahā-śunyatā-dharma-paryāya\" (Greater discourse on emptiness). These sutras have no parallel Pali suttas. These sutras associate emptiness with dependent origination, which shows that this relation of the two terms was already established in pre-Nagarjuna sources. The sutra on great emptiness states:\nThe phrase \"when this exists...\" is a common gloss on dependent origination. Sarvāstivādin Agamas also speak of a certain emptiness samadhi (\"śūnyatāsamādhi\") as well as stating that all dharmas are \"classified as conventional\".\n\nMun-Keat Choong and Yin Shun have both published studies on the various uses of emptiness in the Early Buddhist Texts (Pali Canon and Chinese Agamas). Choong has also published a collection of translations of Agama sutras from the Chinese on the topic of emptiness.\n\nMany of the early Buddhist schools featured sunyata as an important part of their teachings.\n\nThe Sarvastivadin school's Abhidharma texts like the Dharmaskandhapāda Śāstra, and the later Mahāvibhāṣa also take up the theme of emptiness vis a vis dependent origination as found in the Agamas.\n\nSchools such as the Mahāsāṃghika Prajñaptivādins as well as many of the Sthavira schools (except the Pudgalavada) held that all dharmas were empty (dharma śūnyatā). This can be seen in the early Theravada Abhidhamma texts such as the Patisambhidamagga which also speak of the emptiness of the five aggregates and of svabhava as being \"empty of essential nature\". The Theravada Kathavatthu also argues against the idea that emptiness is unconditioned.\n\nOne of the main themes of Harivarman's Tattvasiddhi-Śāstra (3rd-4th century) is \"dharma-śūnyatā\", the emptiness of phenomena.\n\nTheravada Buddhists generally take the view espoused in the Pali canon, that emptiness is merely the not-self nature of the five aggregates as well as a mode of perception which is \"empty of the presuppositions we usually add to experience to make sense of it\" - especially that of unchanging selfhood. Therefore, some Theravadan teachers like Thanissaro Bhikku hold that emptiness is not so much a metaphysical view, as it is a strategic mode of acting and of seeing the world which leads to liberation:\n\nThe idea of emptiness as lack of inherent existence has very little to do with what the Buddha himself said about emptiness. His teachings on emptiness — as reported in the earliest Buddhist texts, the Pali Canon — deal directly with actions and their results, with issues of pleasure and pain. To understand and experience emptiness in line with these teachings requires not philosophical sophistication, but a personal integrity willing to admit the actual motivations behind your actions and the actual benefits and harm they cause.\n\nSome Theravadins such as David Kalupahana, see Nagarjuna's view of emptiness as compatible with the Pali Canon. In his analysis of the Mulamadhyamikakarika, Kalupahana sees Nagarjuna's argument as rooted in the Kaccānagotta Sutta (which Nagarjuna cites by name). Kalupahana states that Nagarjuna's major goal was to discredit heterodox views of Svabhava (own-nature) held by the Sarvastivadins and establish the non-substantiality of all dharmas. According to Peter Harvey, the Abhidhamma theory of the Theravadins is not based on the kind of Svabhava that Nagarjuna was critiquing: \"They are dhammas because they are upheld by conditions or they are upheld according to their own nature' (Asl.39). Here 'own-nature' would mean characteristic nature, which is not something inherent in a dhamma as a separate ultimate reality, but arise due to the supporting conditions both of other dhammas and previous occurrences of that dhamma. This is of significance as it makes the Mahayana critique of the Sarvastivadin's notion of own-nature largely irrelevant to the Theravada.\"\n\nEmptiness as an approach to meditation is seen as a state in which one is \"empty of disturbance.\" This form of meditation is one in which the meditator becomes concentrated and focuses on the absence or presence of disturbances in their mind, if they find a disturbance they notice it and allow it drop away, this leads to deeper states of calmness. Emptiness is also seen as a way to look at sense experience that does not identify with the \"I-making\" and \"my-making\" process of the mind. As a form of meditation, this is developed by perceiving the six sense spheres and their objects as empty of any self, this leads to a formless jhana of nothingness and a state of equanimity.\n\nAccording to Gil Fronsdal: \"Emptiness is as important in the Theravada tradition as it is in the Mahayana. From the earliest times, Theravada Buddhism has viewed emptiness as one of the important doors to liberation.\" Mathew Kosuta sees the Abhidhamma teachings of the modern Thai teacher Ajaan Sujin Boriharnwanaket as being very similar to the Mahayana emptiness view.\n\nThe Prajna-paramita (Perfection of Wisdom) Sutras taught that all entities, including \"dharmas\", are only conceptual existents or constructs.\n\nThough we perceive a world of concrete and discrete objects, these objects are \"empty\" of the identity imputed by their designated labels. The Heart sutra, a text from the prajnaparamita-sutras, articulates this in the following saying in which the five skandhas are said to be \"empty\":\nMādhyamaka is a Mahāyāna Buddhist school of philosophy established by the Indian Buddhist philosopher Nagarjuna. In Madhyamaka, to say that an object is \"empty\" is synonymous with saying that it is dependently originated, which means, it's affected by the three marks of existence: it is fated to be transient, unsatisfactory, and without inherent existence.\n\nMadhyamaka states that impermanent collections of causes and conditions are designated by mere conceptual labels. This also applies to the principle of causality itself, since \"everything\" is dependently originated. If unaware of this, things may seem to arise as existents, remain for a time and then subsequently perish. In reality, dependently originated phenomena do not arise as having inherent existence in the first place. Thus both existence and nihilism are ruled out.\n\nMadhyamaka is retroactively seen as being founded by the monk Nāgārjuna. Nāgārjuna's goal was to refute the essentialism of certain Abhidharma schools. His best-known work is the Mūlamadhyamakakārikā, in which he used the \"reductio ad absurdum\" to show the non-substantiality of the perceived world.\n\nNāgārjuna equates emptiness with dependent origination:\nIn his analysis, any enduring essential nature would prevent the process of dependent origination, or any kind of origination at all. For things would simply always have been, and will always continue to be, without any change.\n\nIn doing so, he restores the Middle way of the Buddha, which had become influenced by absolute tendencies:\nIn Tibetan Buddhism, especially in the Gelugpa, a distinction is being made between Svatantrika and Prasaṅgika approaches to reasoning.\n\n\"Svātantrika\" is attributed primarily to the 6th century Indian scholar Bhavaviveka, who argued for the use of syllogistical statements in the propagation of Madhyamaka, in line with the developing Buddhist logic. It is used in contrast with Prāsangika Madhyamaka. According to Tsonghkhapa, for the Svatantrika conventional phenomena are understood to have a conventional essential existence, but without an ultimately existing essence.\n\nThe name \"Prasangika\" is derived from \"prasanga\", or \"reductio ad absurdum\" arguments, rather than \"svatantra-anumana\", or \"independent syllogisms\". Buddhapalita (470–550) are regarded as the main proponents of the Prasaṅgika, which argues that only arguments should be used which lay bare the logical inconstencies of their opponents.\n\nSome non-Buddhist and Buddhist writers state that the Sunyata concept in Madhyamaka philosophy is nihilistic. For example, Jackson writes:\nThis view has been challenged by other writers. Some scholars interpret emptiness as described by Nāgārjuna as a Buddhist transcendental absolute such as \"Tathagata\", while other scholars consider it a mistake. According to Jorge Ferrer, Nāgārjuna presents the middle way, one between nihilism and absolute eternalism, and sunyata as emptiness is the soteriological middle way.\n\nRandall Collins states that for Nagarjuna, ultimate reality was \"shunyata, emptiness\". In Nagarjuna's thesis, adds Collins, this emptiness is not a negation, but the premise that \"no concepts are intelligible\". David Kalupahana states that this topic has been debated by ancient and medieval Buddhist metaphysicians, with a divergence of views; emptiness is a view, adds Kalupahana, but \"holding up emptiness as an absolute or ultimate truth without reference to that which is empty is the last thing either the Buddha or Nāgārjuna would advocate\".\n\nAccording to Ferrer, Nāgārjuna criticized those whose mind held any \"positions and beliefs\", suggesting liberation is \"avoidance of all views\", and explaining emptiness as follows:\n\nYogacara explains \"emptiness\" in an analysis of the way we perceive \"things\". Everything we conceive of is the result of the working of the five skandhas: form, perception, feeling, volition and discrimination. The five skandhas together create consciousness. The \"things\" we are conscious of are \"mere concepts\", not 'das Ding an sich' or 'the thing in itself'.\n\nAn influential division of 1st-millennium CE Buddhist texts develop the notion of Buddha-nature. The \"Tathagatagarbha\" doctrine, at its earliest probably appeared about the later part of the 3rd century CE, and is verifiable in Chinese translations of 1st millennium CE.\n\nThe notion of Buddha-nature was opposed within the later Tibetan Gelugpa school, because they imply a \"self-like\" concept.\n\nThe Tathāgatagarbha is the topic of the \"Tathāgatagarbha sūtras\", where the title itself means a \"garbha\" (womb, matrix, seed) containing \"Tathagata\" (Buddha). In the \"Tathāgatagarbha sūtras\" sutras the perfection of the wisdom of not-self is stated to be the true self. The ultimate goal of the path is characterized using a range of positive language that had been used in Indian philosophy previously by essentialist philosophers, but which was now transmuted into a new Buddhist vocabulary to describe a being who has successfully completed the Buddhist path.\n\nThese Sutras suggest, states Paul Williams, that 'all sentient beings contain a Tathagata' as their 'essence, core or essential inner nature'. The \"Tathāgatagarbha\" sutras presents a further developed understanding of emptiness, wherein the Buddha Nature, the Buddha and Liberation are seen as transcending the realm of emptiness, i.e. of the conditioned and dependently originated phenomena.\n\nThe \"Śrīmālā Sūtra\" is one of the earliest texts on tathagata-garbha thought, composed in 3rd century in south India, according to Brian Brown. It asserted that everyone can potentially attain Buddhahood, and warns against the doctrine of Sunyata.\n\nThe \"Śrīmālā Sūtra\" posits that the Buddha-nature is ultimately identifiable as the supramundane nature of the Buddha, the \"garbha\" is the ground for Buddha-nature, this nature is unborn and undying, has ultimate existence, has no beginning nor end, is nondual, and permanent. The text also adds that the \"garbha\" has \"no self, soul or personality\" and \"incomprehensible to anyone distracted by sunyata (voidness)\"; rather it is the support for phenomenal existence.\n\nAnother seminal text, which was and is influential in Tibetan Buddhism, is the Ratnagotravibhāga sutra. It forms the basis of shentong, a further developed form of Madhyamaka, in which the realization of emptiness is a preliminary stage to realize the nature of mind, the self-reflexive nature of consciousness which shines through when it is freed from the defilements. Moderate shentong-views are still being taught in the Nyingma and Kagyu lineages, despite the fierce resistance and persecution by Gelugpas in previous centuries.\n\nWhile highly influential in Indian and east Asian Buddhism, for western scholars the \"Tathagatagarbha \" doctrine of an 'essential nature' in every living being appears to be confusing, since it seems to be equivalent to a 'Self', which seems to contradict the doctrines in a vast majority of Buddhist texts. Some scholars, however, view such teachings as metaphorical, not to be taken literally. Wayman and Wayman have also disagreed with this literalist view, and they state that the \"Tathagatagarbha\" is neither self nor sentient being, nor soul, nor personality.\n\nAccording to some scholars, the Buddha nature which these sutras discuss, does not represent a substantial self (\"ātman\"). Rather, it is a positive expression of emptiness, and represents the potentiality to realize Buddhahood through Buddhist practices. In this view, the intention of the teaching of Buddha nature is soteriological rather than theoretical. According to others, the potential of salvation depends on the ontological reality of a salvific, abiding core reality — the Buddha-nature, empty of all mutability and error, fully present within all beings.\n\nAccording to Matsumoto Shiro and Hakamaya Noriaki, the idea of an ontological reality of the Buddha-nature is an un-Buddhist idea: Their \"Critical Buddhism\" approach rejects what it calls \"dhatu-vada\" (substantialist Buddha nature doctrines)\n\nThe critical Buddhism approach has, in turn, recently been characterised as operating with a restricted definition of Buddhism. Paul Williams comments:\nTibetan Buddhism developed five main schools. The Madhyamika philosophy obtained a central position in all the schools, but with two distinct variations:\n\nThe Gelugpa school of Tibetan Buddhism is the most influential of the four Tibetan Buddhist schools. It was founded in the beginning of the 15th century by Tsongkhapa (1357–1419), who was \"strongly scholastic in orientation and encouraged the study of the great Indian masters of philosophy\".\n\nThe 14th Dalai Lama, who generally speaks from the Gelugpa version of the \"Mādhyamaka-Prasaṅgika\", states:\n\nIn Dzogchen, emptiness does not refer to the mere negation of inherent existence, but to profound emptiness, the Ground and nature of mind which is free from temporary characteristics.\n\nThe Tibetan Yungdrung Bon-tradition regards the Ma Gyu, or Mother Tantra, as the highest tantra. Its views are close to Dzogchen. It sees waking life as an illusion, from which we have to wake up, just as we recognize dreams to be illusions. Sunyata is the lack of inherent existence. The Mother Tantra uses ...\nThese \"examples, similes and metaphors\" ...\nThe Sakya school originated in the 11th century. It rose to power in the 13th century. Gorampa Sonam Senge (1429-1489), an important philosopher in the Sakya school of Tibetan Buddhism, established an understandings of Prasangika which differs from Tsongkhapa. According to Gorampa, emptiness is not just mere emptiness, the absence of inherent existence, but profound emptiness, the absence of the four extremes in phenomena. In Gorampa's approach, ultimate truth is a liberating insight, that is free from even grasping the mind.\n\nThe Jonang school originated in the 12th century. Tsongkhapa strongly opposed the Jonang school, whose views he \"deemed to be ... dharmically incorrect\".\n\nIn the Tibetan Jonang school, only the Buddha and the Buddha Nature are viewed as \"not\" intrinsically empty, but as truly real, unconditioned, and replete with eternal, changeless virtues. The Buddha Nature (tathagatagarbha) is only empty of what is impermanent and conditioned, not of its own self. The Buddha Nature is truly real, and primordially present in all beings.\n\nAn important Tibetan treatise on Emptiness and the Buddha Nature is found in the scholar-monk Dolpopa's voluminous study, \"Mountain Doctrine\". It... \nIn this vast \"Mountain Doctrine\", Dolpopa describes the Buddha Nature as ...\nThe Buddha-nature is filled with eternal powers and virtues:\nDolpopa also cites the \"Angulimaliya Sutra<nowiki>'</nowiki>s\" contrast between empty phenomena such as the moral and emotional afflictions (\"kleshas\"), which are like ephemeral hailstones, and the enduring, eternal Buddha, which is like a precious gem:\nThe Kagyu teacher Khenpo Tsultrim, in \"Progressive Stages of Meditation on Emptiness\", presents five strages of meditation, which he relates to five different schools or approaches:\n\nWhen Buddhism was introduced in China it was understood in terms of its own culture. Various sects struggled to attain an understanding of the Indian texts. The Tathāgatagarbha Sutras and the idea of the Buddha-nature were endorsed, because of the perceived similarities with the Tao, which was understood as a transcendental reality underlying the world of appearances. Sunyata at first was also understood as pointing to transcendental reality. It took Chinese Buddhism several centuries to realize that sunyata does not refer to an essential transcendental reality underneath or behind the world of appearances.\n\nThe influence of those various doctrinal and textual backgrounds is still discernable in Zen. Zen teachers still mention the Buddha-nature, but the Zen tradition also emphasizes that Buddha-nature is Sunyata, the absence of an independent and substantial \"self\".\n\nVarious western Buddhists note that \"sunyata\" refers to the emptiness of inherent existence, as in Madhyamaka; but also to the emptiness of mind or awareness, as open space and the \"ground of being,\" as in meditation-orientated traditions and approaches such as Dzogchen and Shentong.\n\nGaudapada is considered by some scholars to have been strongly influenced by Buddhism, as he developed his concept of \"ajāta\" from Nagajurna's Madhyamaka philosophy, which uses the term \"anutpāda\":\n\nTaken together \"anutpāda\" means \"having no origin\", \"not coming into existence\", \"not taking effect\", \"non-production\".\n\nAccording to Gaudapada, the Absolute is not subject to birth, change and death. The Absolute is \"aja\", the unborn eternal. The empirical world of appearances is considered Maya (unreal as it is transitory), and not absolutely existent. Thus, Gaudapada's concept of \"ajativada\" is similar to Buddhist term \"anutpāda\" for the absence of an origin or śūnyatā.\n\nBut Gaudapada's perspective is quite different from Nagarjuna. Gaudapada's perspective found in \"Mandukya Karika\" is based on the \"Mandukya Upanishad\". According to Gaudapada, the metaphysical absolute called Brahman never changes, while the phenomenal world changes continuously, so the phenomenal world cannot arise independently from Brahman. If the world cannot arise, yet is an empirical fact, than the perceived world has to be a transitory (unreal) appearance of Brahman. And if the phenomenal world is a transitory appearance, then there is no real origination or destruction, only apparent origination or destruction. From the level of ultimate truth (\"paramārthatā\") the phenomenal world is \"māyā\", \"illusion\", apparently existing but ultimately not metaphysically real.\n\nIn \"Gaudapada-Karika\", chapter III, verses 46-48, he states that Brahman never arises, is never born, is never unborn, it rests in itself:\nIn contrast to Renard's view, Karmarkar states the Ajativada of Gaudapada has nothing in common with the \"Śūnyatā\" concept in Buddhism. While the language of Gaudapada is undeniably similar to those found in Mahayana Buddhism, states Comans, their perspective is different because unlike Buddhism, Gaudapada is relying on the premise of \"Brahman, Atman or Turiya\" exist and are the nature of absolute reality.\n\n\"Sunya\" and \"Sunyatisunya\" are concepts which appear in some Shaiva texts, such as the Vijñāna Bhairava Tantra, which contains several verses mentioning voidness as a feature of ultimate reality - Shiva:\n\n\"The Absolute void is Bhairava who is beyond the senses and the mind, beyond all the categories of these instruments. From the point of view of the human mins, He is most void. from the point of view of Reality, He is most full, for He is the source of all manifestation.\"\n\n\"The yogi should concentrate intensely on the idea (and also feel) that this universe is totally void. In that void, his mind would become absorbed. Then he becomes highly qualified for absorption i.e. his mind is absorbed in the absolute void (sunyatisunya).\"\n\nIn a series of Kannada language texts of Lingayatism, a Shaivism tradition, \"shunya\" is equated to the concept of the Supreme. In particular, the \"Shunya Sampadane\" texts present the ideas of Allama Prabhu in a form of dialogue, where \"shunya\" is that void and distinctions which a spiritual journey seeks to fill and eliminate. It is the described as a state of union of one's soul with the infinite Shiva, the state of blissful moksha.\n\n\"Shunya Brahma\" is a concept found in certain texts of Vaishnavism, particularly in Odiya, such as the poetic \"Panchasakhas\". It explains the \"Nirguna Brahman\" idea of Vedanta, that is the eternal unchanging metaphysical reality as \"personified void\". Alternate names for this concept of Hinduism, include \"shunya purusha\" and \"Jagannatha\" (Vishnu) in certain text. However, both in Lingayatism and various flavors of Vaishnavism such as \"Mahima Dharma\", the idea of \"Shunya\" is closer to the Hindu concept of metaphysical \"Brahman\", rather than to the \"Śūnyatā\" concept of Buddhism. However, there is some overlap, such as in the works of Bhima Bhoi.\n\nIn the Vaishnavism of Orissa, the idea of Shunya Brahman or Shunya Purusha is found in the poetry of the Orissan Panchasakhas (Five Friends), such as in the compositions of 16th-century Acyutananda. Acyutananda's \"Shunya Samhita\" extols the nature of Shunya Brahman: \n\n\"nāhi tāhāra rūpa varṇa, adṛsha avarṇa tā cinha.\"\n\"tāhāku brahmā boli kahi, śūnya brahmhati se bolāi.\"\n\nIt has no shape, no colour, \nIt is invisible and without a name\nThis Brahman is called Shunya Brahman.\nThe Panchasakhas practiced a form of Bhakti called Jnana-mishrita Bhakti-marga, which saw the necessity of knowledge (Jnana) and devotion - Bhakti.\n\n\n\n"}
