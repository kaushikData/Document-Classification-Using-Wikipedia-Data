{"id": "30773484", "url": "https://en.wikipedia.org/wiki?curid=30773484", "title": "ABC FlowCharter", "text": "ABC FlowCharter\n\nABC FlowCharter is a flowchart program originally from Micrografx, Inc. The trademark for this software was filed on August 25, 1989, and registered January 1991. It is also known as Micrografx FlowCharter\nand iGrafx FlowCharter.\n\nVersion 4.0 of ABC FlowCharter was released in 1995. It integrated the ABC ToolKit software from Micrografx and included new diagramming tools and templates. The software added intelligent line routing, which automatically routed connecting lines around other shapes. This software was compatible with an IBM 386 computer with Windows 3.1, 5 Mb of RAM a VGA monitor and required 21 Mb of hard disk space.\n\nFollowing the acquisition of Micrografx by the Canadian company Corel in 2001, this software was then maintained by the separate business unit iGrafx, formed in 2003. The software package was renamed to \"iGrafx FlowCharter 2003\". , the currently available version of this software package is \"iGrafx FlowCharter 2013\". It is compatible with Windows XP, Windows Vista and Windows 7.\n"}
{"id": "40912579", "url": "https://en.wikipedia.org/wiki?curid=40912579", "title": "Accent perception", "text": "Accent perception\n\nAccents are the distinctive variations in the pronunciation of a language. They can be native or foreign, local or national and can provide information about a person’s geographical locality, socio-economic status and ethnicity. The perception of accents is normal within any given group of language users and involves the categorisation of speakers into social groups and entails judgments about the accented speaker, including their status and personality. Accents can significantly alter the perception of an individual or an entire group, which is an important fact considering that the frequency that people with different accents are encountering one another is increasing, partially due to inexpensive international travel and social media. As well as affecting judgments, accents also affect key cognitive processes (e.g., memory) that are involved in a myriad of daily activities. The development of accent perception occurs in early childhood. Consequently, from a young age accents influence our perception of other people, decisions we make about when and how to interact with others, and, in reciprocal fashion, how other people perceive us. A better understanding of the role accents play in our (often inaccurate) appraisal of individuals and groups, may facilitate greater acceptance of people different from ourselves and lessen discriminatory attitudes and behavior.\n\nSocial identity theory is a theory that describes intergroup behaviour based on group membership. Markers of group membership can be arbitrary, e.g., coloured vests, a flip of a coin, etc., or non-arbitrary, e.g., gender, language, race, etc. Accent is a non-arbitrary marker for group membership that is potentially more salient than most other non-arbitrary markers such as race and visual cues in general. One component of social identity theory states that members of the same group will treat and judge other members of their group (in-group members) preferentially compared to those who are not in their group (out-group members). This phenomenon is called in-group bias and when applied to accents is called the own-accent bias. There are many examples of the discrimination of out-groups based on language, e.g., the banning of the public speaking of German in the United States during World War I and the Al-Anfal Campaign, however, there are also examples of discrimination based on accent. Some of these instances date back many several millennia, for example, in the Bible in Judges 12:5-6 the following quote depicting the mass-killing of a people based on their accent appears:\n\nWhereas some are more recent, for example, in his play Pygmalion George Bernard Shaw famously recognised the disparities of accent (even in a native context) when he wrote:\n\nAccents function as markers of social group membership broadcasting information about individuals' in-group/out-group status. However, unlike other seemingly more conspicuous non-arbitrary markers (e.g., race), the accent an individual has is not outwardly obvious to a casual observer unless the individual speaks and is within hearing range of the observer. This begs the question of how such an easily hidden characteristic became a marker of group membership in the first place. One predominant account suggests an answer to this conundrum lies in evolutionary history. In modern societies people of many different racial backgrounds live together, which provides modern humans with the chance to experience a wide range of races and racial characteristics (e.g., different coloured skin). However, in early societies neighbouring communities could not travel far except by walking, thus they were likely to look similar. As such, a natural selection pressure may have existed that favoured social attention to accents, which functioned as an honesty signal (i.e., an honest signal of an individual's group membership), so individuals could easily identify in-group members from the potential threat of out-group members. In comparison, the selection pressure to socially attend to race was less relevant.\n\nThe \"own-accent bias\" is the inclination toward, and more positive judgement of, individuals with the same accent as yourself compared to those with a different accent. There are two main theories that attempt to explain this bias: affective processing and prototype representation.\n\nThe affective processing approach proposes that the positive-bias exhibited for others who speak with an own-accent is produced by a (potentially unconscious) emotional reaction. Put simply, people like others who have the same accent as themselves for that precise reason; they like it. This theory has developed, and draws support, from neuroscientific research investigating affective prosody (a key component underlying accent) and vocal emotion, which has found activation (predominantly in the right hemisphere) in important brain regions associated with the processing of emotion. These regions include:\nAdditional to the processing of memory and emotion, the amygdalae have important roles as “relevance detectors\" for the discernment of relevant social information. Therefore, these brain regions that deal with social relevance and vocal emotion are probable candidates for a neural network concerning accent-based group membership that would drive the affective processing of accents.\n\nThe prototype representation approach stems from theories developed in the linguistics and cognitive psychology domains. It proposes that there are “prototypes” (i.e., internal representations) stored in the brain, which incoming information from the senses is compared against to facilitate categorisation. Therefore, the own-accent bias is due to the fact that own-accents are similar to the prototype of \"accent\" hence are processed and categorised more easily than those other-accents that are dissimilar. This idea is supported by research showing that the further away a voice is from the average, (which is assumed to be a good representation of the internal prototype of accent) the more distinctive and less attractive it is rated, and the more activity is produced in the temporal voice areas (areas of the brain that deal with voice perception and accents).\n\nRecent research has investigated the effects of accent on earwitness memory (similar to eyewitness memory but based on what a person heard rather than saw). The study showed that ear-witnesses were more likely to mistake offenders with a different accent than an own-accent, and that their judgements were less confident in reporting other-accent offenders compared to those with their own-accent. The authors of the study present similarities between the own-accent bias and the own-race bias, which states that faces are more easily recognised by people of the same race (own-race) because those people have more experience (higher expertise) with them compared to faces of different races (other-race). This is similar to the prototype representation theory of the own-accent bias (see above). Another study investigated the effects of teacher-accent on student learning. This research found that students recalled more information from lectures with teachers who had their own-accent and rated the own-accent teachers more favourably compared to those with an other-accent. Additionally, research focussing on the development of the own-accent bias in infants and children has shown that children are not only consistently able to differentiate between foreign- and native-accents but that infants and children prefer individuals who have a native accent compared to a foreign one, leading them to change their behaviour based on a speakers accent (e.g., accepting a toy off a native-accented speaker rather than a foreign-accented speaker).\n\n\n"}
{"id": "909270", "url": "https://en.wikipedia.org/wiki?curid=909270", "title": "Agency (LDS Church)", "text": "Agency (LDS Church)\n\nAgency (also referred to as free agency or moral agency), in Latter-day Saint theology, is \"the privilege of choice which was introduced by God the Eternal Father to all of his spirit children in the premortal state\". Mortal life is viewed as a test of faith, where our choices are central to the plan of salvation in Mormon teaching. \"It was essential for their eternal progression that they be subjected to the influences of both good and evil\". Mormons believe that Lucifer presented an alternative plan, which resulted in a war in heaven, with Lucifer being cast out of heaven and becoming Satan.\n\nMormons believe that all individuals have the ability to differentiate between good and evil and that Satan and his followers are not able to tempt people beyond the point where they can resist. This implies that mortals can be held accountable for their actions; mortals will be judged by God based on a combination of one's faith and works (with salvation coming only through the power, mercy, and grace of Jesus Christ).\n\nA major difference between the beliefs of Latter-day Saints and many other Christians involves the belief of a life before mortality, referred to as the pre-earth life, pre-mortal life, or pre-existence. Latter-day Saints believe that before the earth was created, all mankind lived as spirit children of God. Here, God nurtured, taught and provided means for their development. This preparation would allow them to later become the men and women of Earth, to be further educated and tested in the schoolhouse of mortality in order to return to God's presence and become like him. Thus, the pre-existent life is believed to have been a period of unknown length of progression, and schooling. Mormons believe that there came a time when we could not progress further without being born into a body and experiencing earthly life.\n\nAccording to Mormon beliefs, God proposed a plan whereby further progression could take place, a plan of salvation. Because agency would allow all people to fall in sin, a savior was necessary to atone for the sins of each person so that they could return to live with their Father in Heaven. Jesus volunteered to follow the plan as outlined, which preserved agency, accountability for action, and the necessary result that some of Heavenly Father's children would never to return to Heavenly Father as a consequence of sin. The second volunteer, Lucifer, attempted to amend the plan by proposing that all mankind would return to Heavenly Father despite their sins—essentially defeating agency and the divine principle of accountability for action. God the Father chose the plan that he proposed with Jesus as the Savior. Lucifer and his followers rebelled against this plan and were eventually cast out of heaven. Lucifer became Satan.\n\nMormons believe that another aspect of agency occurred during the pre-mortal life. Some of the spirit children of God exercised their agency and conformed to God's law and became \"noble and great\". This doctrine is called \"foreordination\". God foreordained some of these spirits to particular stations in life in order to advance his plan to lead humanity back to his presence.\n\nMormons believe that these foreordinations were not unalterable decrees, but rather callings from God for man to perform specific missions in mortality. Even these who were foreordained for greatness could fall and transgress the laws of God. Therefore, mortality is simply a state wherein progression and testing is continued from what began in the pre-existence.\n\nMormon doctrine states that God's plan includes the foreordination of prophets and teachers who have gifts and callings among men to teach and re-teach correct principles so that agency can be used wisely. God's plan includes the important role of parents to teach their children the path of righteousness and happiness, and the blessing of the holy scriptures to give a foundation of gospel knowledge, including the knowledge of the saving role of Jesus Christ and the importance of ordinances and covenants of the gospel.\n\nIn essence, agency is the ability to make choices for oneself, as well as the ability to learn the difference between right and wrong and to make ethical and moral decisions.\n\nDavid O. McKay, a President of the Church, stated, \"It is the purpose of the Lord that man become like him. In order for man to achieve this it was necessary for the Creator first to make him free.\" Without agency, mortality would be useless. Men are ultimately responsible for their own destiny, through their faith and obedience to the commandments of God. \"Free agency\" therefore should not be interpreted to mean that actions are without consequences; \"free\" means that it is a gift from God and consequences must necessarily come as a result of choices made. Free agency and accountability are complementary and cannot be separated.\n\nThis principle holds that it is wrong to deny someone of his or her free agency unless they have abused it to infringe against the agency of another, as it would bind a person from their own choices. Such offenses logically include crimes such as murder, rape and slavery. Furthermore, a person who prevents an individual from doing what they have been commanded to do (e.g., force them to do something they believe is wrong) will be held responsible for any offense.\n\nThroughout the 20th century, church leaders often equated governments led by dictatorships as being under the influence and control of Satan. For example, apostle Ezra Taft Benson often spoke of the \"evil\" communist or socialist movements that threatened the free agency of mankind: \"it is realized that communism is turning out to be the earthly image of the plan which Satan presented in the preexistence. The whole program of socialistic communism is essentially a war against God and the plan of salvation—the very plan which we fought to uphold during 'the war in heaven.'\" Other \nleaders contradicted these statements, often pointing to the church's belief in the \"Law of Consecration\" which is a form of socialism practiced by early church members in which all property was held in common and distributed by church leaders based on the needs of each individual. The church is officially politically neutral and today church members around the world subscribe to a variety of political beliefs including communism and socialism.\n\nMormon doctrine teaches that many men and women since the beginning of mortal time have used their agency unwisely, limiting their own progress and their opportunity to receive light and knowledge. Beginning with Cain, some have used their agency to inflict harm, abuse, tyranny, slavery, or death upon others, contrary to the will and commandments of God.\n\nThe fact that God allows these actions does not mean that He condones them. LDS doctrine holds that agency is an eternal principle, and that God has provided the way through the atonement of Jesus Christ whereby men and women can repent of their wrongful acts of commission or of omission, and come back into the path of receiving further light and knowledge through making right choices. The atonement of Christ and the plan of compassion among men also provides a way whereby those who have been harmed by the sinful actions of others may be healed in a spiritual sense, although this may take great patience and long-suffering, and often requires the help of others.\n\nThe Pearl of Great Price, one of the scriptures of the LDS church, states that Satan, the great deceiver, sought during premortal life to destroy the agency of man, and that he continues to seek to enslave men, women and children in whatever ways that he can in this world, to \"lead them captive at his will.\" LDS doctrine teaches that whatever leads in this world to enslavement, addiction, or forced behavior is ultimately instigated by Satan. God allows these conditions because of the agency he has given to man, but He expects men to overcome evil by doing good among the society in which they live. God holds men and women responsible and accountable in relation to the light and knowledge they have. Every person born into the world is given the light of Christ, also called conscience, to guide each person in choosing good from evil.\n\nLDS doctrine also holds that whenever gospel knowledge has been lost or limited among portions of mankind, this has come about because of the unrighteousness of the people and their leaders, as described by the prophet Isaiah in the Bible.\n\nLDS leaders teach that family and societal relationships are a part of mortal life for many purposes, including the need to learn to show love, acceptance, and compassion in ways that continue to allow agency. They teach that unrighteous dominion is never acceptable to God, and that with the agency given to men is the expectation that when they marry, they will treat their wife and children with love, respect, tenderness, and material and emotional support. LDS leaders teach that men should treat women as equal partners in all decisions in the family.\n\nThe term free agency is commonly used, and has traditionally been interpreted as meaning that individuals have the ability to choose their actions freely. Many leaders of the LDS Church have pointed out that the term \"free agency\" should not be interpreted to mean that agency does not have consequences, but rather that agency is fraught with risk and choices (the result of the exercise of agency) determine eternal destination. Some church manuals avoid the term \"free agency\" and instead refer simply \"agency.\" Some church leaders favor the term \"moral agency\".\n\nIt is said that Adam and Eve were the first of God’s children to come to Earth. They were created in God’s image, with bodies of flesh and bones. God placed them in the Garden of Eden. Here they did not remember their former existence though they were still able to enjoy God's presence and could have lived forever.\n\nAs it is believed the Heavenly Father has blessed all of His children with the freedom to choose, Adam and Eve were given agency to make their own choices on the earth. He commanded them not to eat the forbidden fruit, or the fruit of the tree of knowledge of good and evil, warning that such would result in death. Obeying this commandment meant they could remain in the garden forever, but they could not progress by experiencing opposition in mortality and would remain innocent; they could not know joy because they could not experience sorrow and pain. Thus, as a part of the plan, Satan was allowed to tempt Adam and Eve to eat the forbidden fruit and they chose to do so. As a consequence, they were separated from God's presence physically and spiritually. Adam and Eve then became mortal; subject to sin and death, and were unable to return to Heavenly Father without his help. They could now experience disease and all types of suffering. They had moral agency, or the ability to choose between good and evil, which made it possible for them to learn and progress. It also made it possible for them to make wrong choices and to sin. In addition, they could now have children, so the rest of God’s spirit children could come to Earth, obtain physical bodies, and be proven. All this was in accordance with the plan of God. Only in this way could God's children progress and become like him.\n\n"}
{"id": "2007", "url": "https://en.wikipedia.org/wiki?curid=2007", "title": "Archery", "text": "Archery\n\nArchery is the art, sport, practice or skill of using a bow to shoot arrows. The word comes from the Latin \"arcus\". Historically, archery has been used for hunting and combat. In modern times, it is mainly a competitive sport and recreational activity. A person who participates in archery is typically called an archer or a \"bowman\", and a person who is fond of or an expert at archery is sometimes called a toxophilite.\n\nThe bow and arrow seems to have been invented in the later Paleolithic or early Mesolithic periods. The oldest signs of its use in Europe come from the in the north of Hamburg, Germany and dates from the late Paleolithic, about 10,000–9000 BC. The arrows were made of pine and consisted of a main shaft and a long fore shaft with a flint point. There are no definite earlier bows; previous pointed shafts are known, but may have been launched by spear-throwers rather than bows. The oldest bows known so far comes from the Holmegård swamp in Denmark. Bows eventually replaced the spear-thrower as the predominant means for launching shafted projectiles, on every continent except Australasia, though spear-throwers persisted alongside the bow in parts of the Americas, notably Mexico and among the Inuit.\n\nBows and arrows have been present in Egyptian & neighboring Nubian culture since its respective predynastic & Pre-Kerma origins. In the Levant, artifacts that could be arrow-shaft straighteners are known from the Natufian culture, (c. 10,800–8,300 BC) onwards. The Khiamian and PPN A shouldered Khiam-points may well be arrowheads.\n\nClassical civilizations, notably the Assyrians, Greeks, Armenians, Persians, Parthians, Indians, Koreans, Chinese, and Japanese fielded large numbers of archers in their armies. Akkadians were the first to use composite bows in war according to the victory stele of Naram-Sin of Akkad. Egyptians referred to Nubia as \"Ta-Seti,\" or \"The Land of the Bow,\" since the Nubians were known to be expert archers, and by the 16th Century BC Egyptians were using the composite bow in warfare . While the Bronze Age Aegean Cultures were able to deploy a number of state-owned specialized bow makers for warfare and hunting purposes already from the 15th century BC. The Welsh longbow proved its worth for the first time in Continental warfare at the Battle of Crécy. In the Americas archery was widespread at European contact.\n\nArchery was highly developed in Asia. The Sanskrit term for archery, dhanurveda, came to refer to martial arts in general. In East Asia, Goguryeo, one of the Three Kingdoms of Korea was well known for its regiments of exceptionally skilled archers.\n\nCentral Asian tribesmen (after the domestication of the horse) and American Plains Indians (after gaining access to horses) became extremely adept at archery on horseback. Lightly armoured, but highly mobile archers were excellently suited to warfare in the Central Asian steppes, and they formed a large part of armies that repeatedly conquered large areas of Eurasia. Shorter bows are more suited to use on horseback, and the composite bow enabled mounted archers to use powerful weapons. Empires throughout the Eurasian landmass often strongly associated their respective \"barbarian\" counterparts with the usage of the bow and arrow, to the point where powerful states like the Han Dynasty referred to their neighbours, the Xiong-nu, as \"Those Who Draw the Bow\". For example, Xiong-nu mounted bowmen made them more than a match for the Han military, and their threat was at least partially responsible for Chinese expansion into the Ordos region, to create a stronger, more powerful buffer zone against them. It is possible that \"barbarian\" peoples were responsible for introducing archery or certain types of bows to their \"civilized\" counterparts—the Xiong-nu and the Han being one example. Similarly, short bows seem to have been introduced to Japan by northeast Asian groups.\n\nThe development of firearms rendered bows obsolete in warfare, although efforts were sometimes made to preserve archery practice. In England and Wales, for example, the government tried to enforce practice with the longbow until the end of the 16th century. This was because it was recognised that the bow had been instrumental to military success during the Hundred Years' War. Despite the high social status, ongoing utility, and widespread pleasure of archery in Armenia, China, Egypt, England and Wales, America, India, Japan, Korea, Turkey and elsewhere, almost every culture that gained access to even early firearms used them widely, to the neglect of archery. Early firearms were inferior in rate-of-fire, and were very sensitive to wet weather. However, they had longer effective range and were tactically superior in the common situation of soldiers shooting at each other from behind obstructions. They also required significantly less training to use properly, in particular penetrating steel armour without any need to develop special musculature. Armies equipped with guns could thus provide superior firepower, and highly trained archers became obsolete on the battlefield. However, the bow and arrow is still an effective weapon, and archers have seen action in the 21st century. Traditional archery remains in use for sport, and for hunting in many areas.\n\nEarly recreational archery societies included the Finsbury Archers and the Ancient Society of Kilwinning Archers. The latter's annual Papingo event was first recorded in 1483. (In this event, archers shoot vertically from the base of an abbey tower to dislodge a wood pigeon placed approximately 30 meters above.) The Royal Company of Archers was formed in 1676 and is one of the oldest sporting bodies in the world. Archery remained a small and scattered pastime, however, until the late 18th century when it experienced a fashionable revival among the aristocracy. Sir Ashton Lever, an antiquarian and collector, formed the Toxophilite Society in London in 1781, with the patronage of George, the Prince of Wales.\n\nArchery societies were set up across the country, each with its own strict entry criteria and outlandish costumes. Recreational archery soon became extravagant social and ceremonial events for the nobility, complete with flags, music and 21 gun salutes for the competitors. The clubs were \"the drawing rooms of the great country houses placed outside\" and thus came to play an important role in the social networks of the local upper class. As well as its emphasis on display and status, the sport was notable for its popularity with females. Young women could not only compete in the contests but retain and show off their sexuality while doing so. Thus, archery came to act as a forum for introductions, flirtation and romance. It was often consciously styled in the manner of a Medieval tournament with titles and laurel wreaths being presented as a reward to the victor. General meetings were held from 1789, in which local lodges convened together to standardise the rules and ceremonies. Archery was also co-opted as a distinctively British tradition, dating back to the lore of Robin Hood and it served as a patriotic form of entertainment at a time of political tension in Europe. The societies were also elitist, and the new middle class bourgeoisie were excluded from the clubs due to their lack of social status.\n\nAfter the Napoleonic Wars, the sport became increasingly popular among all classes, and it was framed as a nostalgic reimagining of the preindustrial rural Britain. Particularly influential was Sir Walter Scott's 1819 novel, \"Ivanhoe\" that depicted the heroic character Lockseley winning an archery tournament.\n\nThe 1840s saw the second attempts at turning the recreation into a modern sport. The first Grand National Archery Society meeting was held in York in 1844 and over the next decade the extravagant and festive practices of the past were gradually whittled away and the rules were standardized as the 'York Round' - a series of shoots at 60, 80, and 100 yards. Horace A. Ford helped to improve archery standards and pioneered new archery techniques. He won the Grand National 11 times in a row and published a highly influential guide to the sport in 1856.\nTowards the end of the 19th century, the sport experienced declining participation as alternative sports such as croquet and tennis became more popular among the middle class. By 1889, just 50 archery clubs were left in Britain, but it was still included as a sport at the 1900 Paris Olympics.\n\nIn the United States, primitive archery was revived in the early 20th century. The last of the Yahi Indian tribe, a native known as Ishi, came out of hiding in California in 1911. His doctor, Saxton Pope, learned many of Ishi's traditional archery skills, and popularized them. \n\nFrom the 1920s, professional engineers took an interest in archery, previously the exclusive field of traditional craft experts. They led the commercial development of new forms of bow including the modern recurve and compound bow. These modern forms are now dominant in modern Western archery; traditional bows are in a minority. In the 1980s, the skills of traditional archery were revived by American enthusiasts, and combined with the new scientific understanding. Much of this expertise is available in the \"Traditional Bowyer's Bibles\" (see Further reading). Modern game archery owes much of its success to Fred Bear, an American bow hunter and bow manufacturer.\n\nDeities and heroes in several mythologies are described as archers, including the Greek Artemis and Apollo, the Roman Diana and Cupid, the Germanic Agilaz, continuing in legends like those of Wilhelm Tell, Palnetoke, or Robin Hood. Armenian Hayk and Babylonian Marduk, Indian Karna (also known as Radheya/son of Radha), Abhimanyu, Eklavya, Arjuna, Bhishma, Drona, Rama, and Shiva were known for their shooting skills. The famous archery competition of hitting the eye of a rotating fish while watching its reflection in the water bowl was one of the many archery skills depicted in the Mahabharata. \n\nPersian Arash was a famous archer. Earlier Greek representations of Heracles normally depict him as an archer.\n\nThe Nymphai Hyperboreioi (Νύμφαι Ὑπερβόρειοι) were worshipped on the Greek island of Delos as attendants of Artemis, presiding over aspects of archery; Hekaerge (Ἑκαέργη), represented distancing, Loxo (Λοξώ), trajectory, and Oupis (Οὖπις), aim.\n\nYi the archer and his apprentice Feng Meng appear in several early Chinese myths, and the historical character of Zhou Tong features in many fictional forms. Jumong, the first Taewang of the Goguryeo kingdom of the Three Kingdoms of Korea, is claimed by legend to have been a near-godlike archer. Archery features in the story of Oguz Khagan.\n\nIn West African Yoruba belief, Osoosi is one of several deities of the hunt who are identified with bow and arrow iconography and other insignia associated with archery.\n\nWhile there is great variety in the construction details of bows (both historic and modern), all bows consist of a string attached to elastic limbs that store mechanical energy imparted by the user drawing the string. Bows may be broadly split into two categories: those drawn by pulling the string directly and those that use a mechanism to pull the string.\n\nDirectly drawn bows may be further divided based upon differences in the method of limb construction, notable examples being self bows, laminated bows and composite bows. Bows can also be classified by the bow shape of the limbs when unstrung; in contrast to traditional European straight bows, a recurve bow and some types of longbow have tips that curve away from the archer when the bow is unstrung. The cross-section of the limb also varies; the classic longbow is a tall bow with narrow limbs that are D-shaped in cross section, and the flatbow has flat wide limbs that are approximately rectangular in cross-section. The classic D-shape comes from the use of the wood of the yew tree. The sap-wood is best suited to the tension on the back of the bow, and the heart-wood to the compression on the belly. Hence, a cross-section of a yew longbow shows the narrow, light-coloured sap-wood on the 'straight' part (riser) of the D, and the red/orange heartwood forms the curved part of the D, to balance the mechanical tension/compression stress. Cable-backed bows use cords as the back of the bow; the draw weight of the bow can be adjusted by changing the tension of the cable. They were widespread among Inuit who lacked easy access to good bow wood. One variety of cable-backed bow is the Penobscot bow or Wabenaki bow, invented by Frank Loring (Chief Big Thunder) about 1900. It consists of a small bow attached by cables on the back of a larger main bow.\n\nIn different cultures, the arrows are released from either the left or right side of the bow, and this affects the hand grip and position of the bow. In Arab archery, Turkish archery and Kyūdō, the arrows are released from the right hand side of the bow, and this affects construction of the bow. In western archery, the arrow is usually released from the left hand side of the bow for a right-handed archer.\n\nCompound bows are designed to reduce the force required to hold the string at full draw, hence allowing the archer more time to aim with less muscular stress. Most compound designs use cams or elliptical wheels on the ends of the limbs to achieve this. A typical let-off is anywhere from 65% to 80%. For example, a 60-pound bow with 80% let-off only requires 12 pounds of force to hold at full draw. Up to 99% let-off is possible. The compound bow was invented by Holless Wilbur Allen in the 1960s (a US patent was filed in 1966 and granted in 1969) and it has become the most widely used type of bow for all forms of archery in North America.\n\nMechanically drawn bows typically have a stock or other mounting, such as the crossbow. Crossbows typically have shorter draw lengths compared to compound bows. Because of this, heavier draw weights are required to achieve the same energy transfer to the arrow. These mechanically drawn bows also have devices to hold the tension when the bow is fully drawn. They are not limited by the strength of a single archer and larger varieties have been used as siege engines.\n\nThe most common form of arrow consists of a shaft, with an arrowhead at the front end, and fletchings and a nock at the other end. Arrows across time and history have normally been carried in a container known as a quiver, which can take many different forms. Shafts of arrows are typically composed of solid wood, bamboo, fiberglass, aluminium alloy, carbon fiber, or composite materials. Wooden arrows are prone to warping. Fiberglass arrows are brittle, but can be produced to uniform specifications easily. Aluminium shafts were a very popular high-performance choice in the latter half of the 20th century, due to their straightness, lighter weight, and subsequently higher speed and flatter trajectories. Carbon fiber arrows became popular in the 1990s because they are very light, flying even faster and flatter than aluminium arrows. Today, the most popular arrows at tournaments and Olympic events are made of composite materials, in particular the X10 and A/C/E, made by Easton,\n\nThe arrowhead is the primary functional component of the arrow. Some arrows may simply use a sharpened tip of the solid shaft, but separate arrowheads are far more common, usually made from metal, stone, or other hard materials. The most commonly used forms are target points, field points, and broadheads, although there are also other types, such as bodkin, judo, and blunt heads.\nFletching is traditionally made from bird feathers, but solid plastic vanes and thin sheet-like spin vanes are used. They are attached near the nock (rear) end of the arrow with thin double sided tape, glue, or, traditionally, sinew. The most common configuration in all cultures is three fletches, though as many as six have been used. Two makes the arrow unstable in flight. When the arrow is \"three-fletched\", the fletches are equally spaced around the shaft, with one placed such that it is perpendicular to the bow when nocked on the string, though variations are seen with modern equipment, especially when using the modern spin vanes. This fletch is called the \"index fletch\" or \"cock feather\" (also known as \"the odd vane out\" or \"the nocking vane\"), and the others are sometimes called the \"hen feathers\". Commonly, the cock feather is of a different color. However, if archers are using fletching made of feather or similar material, they may use same color vanes, as different dyes can give varying stiffness to vanes, resulting in less precision. When an arrow is \"four-fletched\", two opposing fletches are often cock feathers, and occasionally the fletches are not evenly spaced.\n\nThe fletching may be either \"parabolic\" cut (short feathers in a smooth parabolic curve) or \"shield\" cut (generally shaped like half of a narrow shield), and is often attached at an angle, known as \"helical\" fletching, to introduce a stabilizing spin to the arrow while in flight. Whether helicial or straight fletched, when natural fletching (bird feathers) is used it is critical that all feathers come from the same side of the bird. Oversized fletchings can be used to accentuate drag and thus limit the range of the arrow significantly; these arrows are called \"flu-flus\". Misplacement of fletchings can change the arrow's flight path dramatically.\n\nDacron and other modern materials offer high strength for their weight and are used on most modern bows. Linen and other traditional materials are still used on traditional bows. Several modern methods of making a bowstring exist, such as the 'endless loop' and 'Flemish twist'. Almost any fiber can be made into a bowstring. The author of \"Arab Archery\" suggests the hide of a young, emaciated camel. Njál's saga describes the refusal of a wife, Hallgerður, to cut her hair to make an emergency bowstring for her husband, Gunnar Hámundarson, who is then killed.\n\nMost archers wear a bracer (also known as an arm-guard) to protect the inside of the bow arm from being hit by the string and prevent clothing from catching the bowstring. The bracer does not brace the arm; the word comes from the armoury term \"brassard\", meaning an armoured sleeve or badge. The Navajo people have developed highly ornamented bracers as non-functional items of adornment. Some archers (nearly all female archers) wear protection on their chests, called chestguards or plastrons. The myth of the Amazons was that they had one breast removed to solve this problem. Roger Ascham mentions one archer, presumably with an unusual shooting style, who wore a leather guard for his face.\n\nThe drawing digits are normally protected by a leather tab, glove, or thumb ring. A simple tab of leather is commonly used, as is a skeleton glove. Medieval Europeans probably used a complete leather glove.\n\nEurasiatic archers who used the thumb or Mongolian draw protected their thumbs, usually with leather according to the author of \"Arab Archery\", but also with special rings of various hard materials. Many surviving Turkish and Chinese examples are works of considerable art. Some are so highly ornamented that the users could not have used them to loose an arrow. Possibly these were items of personal adornment, and hence value, remaining extant whilst leather had virtually no intrinsic value and would also deteriorate with time. In traditional Japanese archery a special glove is used that has a ridge to assist in drawing the string.\n\nA release aid is a mechanical device designed to give a crisp and precise loose of arrows from a compound bow. In the most commonly used, the string is released by a finger-operated trigger mechanism, held in the archer's hand or attached to their wrist. In another type, known as a back-tension release, the string is automatically released when drawn to a pre-determined tension.\n\nStabilizers are mounted at various points on the bow. Common with competitive archery equipment are special brackets that allow multiple stabilizers to be mounted at various angles to fine tune the bow's balance.\n\nStabilizers aid in aiming by improving the balance of the bow. Sights, quivers, rests, and design of the riser (the central, non-bending part of the bow) make one side of the bow heavier. One purpose of stabilizers are to offset these forces. A reflex riser design will cause the top limb to lean towards the shooter. In this case a heavier front stabilizer is desired to offset this action. A deflex riser design has the opposite effect and a lighter front stabilizer may be used.\n\nStabilizers can reduce noise and vibration. These energies are absorbed by viscoelastic polymers, gels, powders, and other materials used to build stabilizers.\n\nStabilizers improve the forgiveness and accuracy by increasing the moment of inertia of the bow to resist movement during the shooting process. Lightweight carbon stabilizers with weighted ends are desirable because they improve the moment of interia while minimizing the weight added.\n\nThe standard convention on teaching archery is to hold the bow depending upon eye dominance. (One exception is in modern Kyudo where all archers are trained to hold the bow in the left hand.) Therefore, if one is right-eye dominant, they would hold the bow in the left hand and draw the string with the right hand. However, not everyone agrees with this line of thought. A smoother, and more fluid release of the string will produce the most consistently repeatable shots, and therefore may provide greater accuracy of the arrow flight. Some believe that the hand with the greatest dexterity should therefore be the hand that draws and releases the string. Either eye can be used for aiming, and the less dominant eye can be trained over time to become more effective for use. To assist with this, an eye patch can be temporarily worn over the dominant eye.\n\nThe hand that holds the bow is referred to as the \"bow hand\" and its arm the \"bow arm\". The opposite hand is called the \"drawing hand\" or \"string hand\". Terms such as \"bow shoulder\" or \"string elbow\" follow the same convention.\n\nIf shooting according to eye dominance, right-eye-dominant archers shooting conventionally hold the bow with their left hand. If shooting according to hand dexterity, the archer draws the string with the hand that possesses the greatest dexterity, regardless of eye dominance.\n\nTo shoot an arrow, an archer first assumes the correct stance. The body should be at or nearly perpendicular to the target and the shooting line, with the feet placed shoulder-width apart. As an archer progresses from beginner to a more advanced level other stances such as the \"open stance\" or the \"closed stance\" may be used, although many choose to stick with a \"neutral stance\". Each archer has a particular preference, but mostly this term indicates that the leg furthest from the shooting line is a half to a whole foot-length from the other foot, on the ground.\n\nTo load, the bow is pointed toward the ground, tipped slightly clockwise of vertical (for a right handed shooter) and the shaft of the arrow is placed on the arrow rest or shelf. The back of the arrow is attached to the bowstring with the nock (a small locking groove located at the proximal end of the arrow). This step is called \"nocking the arrow\". Typical arrows with three vanes should be oriented such that a single vane, the \"cock feather\", is pointing away from the bow, to improve the clearance of the arrow as it passes the arrow rest.\n\nA compound bow is fitted with a special type of arrow rest, known as a launcher, and the arrow is usually loaded with the cock feather/vane pointed either up, or down, depending upon the type of launcher being used.\n\nThe bowstring and arrow are held with three fingers, or with a mechanical arrow release. Most commonly, for finger shooters, the index finger is placed above the arrow and the next two fingers below, although several other techniques have their adherents around the world, involving three fingers below the arrow, or an arrow pinching technique. \"Instinctive\" shooting is a technique eschewing sights and is often preferred by traditional archers (shooters of longbows and recurves). In either the split finger or three finger under case, the string is usually placed in the first or second joint, or else on the pads of the fingers. When using a mechanical release aid, the release is hooked onto the D-loop.\n\nAnother type of string hold, used on traditional bows, is the type favoured by the Mongol warriors, known as the \"thumb release\", style. This involves using the thumb to draw the string, with the fingers curling around the thumb to add some support. To release the string, the fingers are opened out and the thumb relaxes to allow the string to slide off the thumb. When using this type of release, the arrow should rest on the same side of the bow as the drawing hand i.e. Left hand draw = arrow on left side of bow.\n\nThe archer then raises the bow and draws the string, with varying alignments for vertical versus slightly canted bow positions. This is often one fluid motion for shooters of recurves and longbows, which tend to vary from archer to archer. Compound shooters often experience a slight jerk during the drawback, at around the last inch and a half, where the draw weight is at its maximum—before relaxing into a comfortable stable full draw position. The archer draws the string hand towards the face, where it should rest lightly at a fixed \"anchor point\". This point is consistent from shot to shot, and is usually at the corner of the mouth, on the chin, to the cheek, or to the ear, depending on preferred shooting style. The archer holds the bow arm outwards, toward the target. The elbow of this arm should be rotated so that the inner elbow is perpendicular to the ground, though archers with hyper extendable elbows tend to angle the inner elbow toward the ground, as exemplified by the Korean archer Jang Yong-Ho. This keeps the forearm out of the way of the bowstring.\n\nIn modern form, the archer stands erect, forming a \"T\". The archer's lower trapezius muscles are used to pull the arrow to the anchor point. Some modern recurve bows are equipped with a mechanical device, called a clicker, which produces a clicking sound when the archer reaches the correct draw length. In contrast, traditional English Longbow shooters step \"into the bow\", exerting force with both the bow arm and the string hand arm simultaneously, especially when using bows having draw weights from 100 lbs to over 175 lbs. Heavily stacked traditional bows (recurves, long bows, and the like) are released immediately upon reaching full draw at maximum weight, whereas compound bows reach their maximum weight around the last inch and a half, dropping holding weight significantly at full draw. Compound bows are often held at full draw for a short time to achieve maximum accuracy.\n\nThe arrow is typically released by relaxing the fingers of the drawing hand (see Bow draw), or triggering the mechanical release aid. Usually the release aims to keep the drawing arm rigid, the bow hand relaxed, and the arrow is moved back using the back muscles, as opposed to using just arm motions. An archer should also pay attention to the recoil or \"follow through\" of his or her body, as it may indicate problems with form (technique) that affect accuracy.\n\nThere are two main forms of aiming in archery: using a mechanical or fixed sight, or barebow.\n\nMechanical sights can be affixed to the bow to aid in aiming. They can be as simple as a pin, or may use optics with magnification. They usually also have a peep sight (rear sight) built into the string, which aids in a consistent anchor point. Modern compound bows automatically limit the draw length to give a consistent arrow velocity, while traditional bows allow great variation in draw length. Some bows use mechanical methods to make the draw length consistent. Barebow archers often use a sight picture, which includes the target, the bow, the hand, the arrow shaft and the arrow tip, as seen at the same time by the archer. With a fixed \"anchor point\" (where the string is brought to, or close to, the face), and a fully extended bow arm, successive shots taken with the sight picture in the same position fall on the same point. This lets the archer adjust aim with successive shots to achieve accuracy.\n\nModern archery equipment usually includes sights. Instinctive aiming is used by many archers who use traditional bows. The two most common forms of a non-mechanical release are split-finger and three-under. Split-finger aiming requires the archer to place the index finger above the nocked arrow, while the middle and ring fingers are both placed below. Three-under aiming places the index, middle, and ring fingers under the nocked arrow. This technique allows the archer to better look down the arrow since the back of the arrow is closer to the dominant eye, and is commonly called \"gun barreling\" (referring to common aiming techniques used with firearms).\n\nWhen using short bows or shooting from horseback, it is difficult to use the sight picture. The archer may look at the target, but without including the weapon in the field of accurate view. Aiming then involves hand-eye coordination—which includes proprioception and motor-muscle memory, similar to that used when throwing a ball. With sufficient practice, such archers can normally achieve good practical accuracy for hunting or for war. Aiming without a sight picture may allow more rapid shooting, not however increasing accuracy.\n\nInstinctive shooting is a style of shooting that includes the barebow aiming method that relies heavily upon the subconscious mind, proprioception, and motor/muscle memory to make aiming adjustments; the term used to refer to a general category of archers who did not use a mechanical or fixed sight.\n\nWhen a projectile is thrown by hand, the speed of the projectile is determined by the kinetic energy imparted by the thrower's muscles performing work. However, the energy must be imparted over a limited distance (determined by arm length) and therefore (because the projectile is accelerating) over a limited time, so the limiting factor is not work but rather power, which determined how much energy can be added in the limited time available. Power generated by muscles, however, is limited by force–velocity relationship, and even at the optimal contraction speed for power production, total work by the muscle is less than half of what it would be if the muscle contracted over the same distance at slow speeds, resulting in less than 1/4 the projectile launch velocity possible without the limitations of the force–velocity relationship.\n\nWhen a bow is used, the muscles are able to perform work much more slowly, resulting in greater force and greater work done. This work is stored in the bow as elastic potential energy, and when the bowstring is released, this stored energy is imparted to the arrow much more quickly than can be delivered by the muscles, resulting in much higher velocity and, hence, greater distance. This same process is employed by frogs, which use elastic tendons to increase jumping distance. In archery, some energy dissipates through elastic hysteresis, reducing the overall amount released when the bow is shot. Of the remaining energy, some is dampened both by the limbs of the bow and the bowstring. Depending on the arrow's elasticity, some of the energy is also absorbed by compressing the arrow, primarily because the release of the bowstring is rarely in line with the arrow shaft, causing it to flex out to one side. This is because the bowstring accelerates faster than the archer's fingers can open, and consequently some sideways motion is imparted to the string, and hence arrow nock, as the power and speed of the bow pulls the string off the opening fingers.\n\nEven with a release aid mechanism some of this effect is usually experienced, since the string always accelerates faster than the retaining part of the mechanism. This makes the arrow oscillate in flight—its center flexing to one side and then the other repeatedly, gradually reducing as the arrow's flight proceeds. This is clearly visible in high-speed photography of arrows at discharge. A direct effect of these energy transfers can clearly be seen when dry firing. Dry firing refers to releasing the bowstring without a nocked arrow. Because there is no arrow to receive the stored potential energy, almost all the energy stays in the bow. Some have suggested that dry firing may cause physical damage to the bow, such as cracks and fractures—and because most bows are not specifically made to handle the high amounts of energy dry firing produces, should never be done.\nModern arrows are made to a specified 'spine', or stiffness rating, to maintain matched flexing and hence accuracy of aim. This flexing can be a desirable feature, since, when the spine of the shaft is matched to the acceleration of the bow(string), the arrow bends or flexes around the bow and any arrow-rest, and consequently the arrow, and fletchings, have an un-impeded flight. This feature is known as the archer's paradox. It maintains accuracy, for if part of the arrow struck a glancing blow on discharge, some inconsistency would be present, and the excellent accuracy of modern equipment would not be achieved.\n\nThe accurate flight of an arrow is dependent on its fletching. The arrow's manufacturer (a \"fletcher\") can arrange fletching to cause the arrow to rotate along its axis. This improves accuracy by evening pressure buildups that would otherwise cause the arrow to \"plane\" on the air in a random direction after shooting. Even with a carefully made arrow, the slightest imperfection or air movement causes some unbalanced turbulence in air flow. Consequently, rotation creates an equalization of such turbulence, which, overall, maintains the intended direction of flight i.e. accuracy. This rotation is not to be confused with the rapid gyroscopic rotation of a rifle bullet. Fletching that is not arranged to induce rotation still improves accuracy by causing a restoring drag any time the arrow tilts from its intended direction of travel.\n\nThe innovative aspect of the invention of the bow and arrow was the amount of power delivered to an extremely small area by the arrow. The huge ratio of length vs. cross sectional area, coupled with velocity, made the arrow more powerful than any other hand held weapon until firearms were invented. Arrows can spread or concentrate force, depending on the application. Practice arrows, for instance, have a blunt tip that spreads the force over a wider area to reduce the risk of injury or limit penetration. Arrows designed to pierce armor in the Middle Ages used a very narrow and sharp tip (\"bodkinhead\") to concentrate the force. Arrows used for hunting used a narrow tip (\"broadhead\") that widens further, to facilitate both penetration and a large wound.\n\nUsing archery to take game animals is known as \"bow hunting\". Bow hunting differs markedly from hunting with firearms, as distance between hunter and prey must be much shorter to ensure a humane kill. The skills and practices of bow hunting therefore emphasize very close approach to the prey, whether by still hunting, stalking, or waiting in a blind or tree stand. In many countries, including much of the United States, bow hunting for large and small game is legal. Bow hunters generally enjoy longer seasons than are allowed with other forms of hunting such as black powder, shotgun, or rifle. Usually, compound bows are used for large game hunting due to the relatively short time it takes to master them as opposed to the longbow or recurve bow. These compound bows may feature fiber optic sights, stabilizers, and other accessories designed to increase accuracy at longer distances. Using a bow and arrow to take fish is known as \"bow fishing\".\n\nCompetitive archery involves shooting arrows at a target for accuracy from a set distance or distances. This is the most popular form of competitive archery worldwide and is called target archery. A form particularly popular in Europe and America is field archery, shot at targets generally set at various distances in a wooded setting. Competitive archery in the United States is governed by USA Archery and National Field Archery Association (NFAA), which also certifies instructors.\n\nPara-Archery is an adaptation of archery for athletes with a disability governed by the World Archery Federation (WA), and is one of the sports in the Summer Paralympic Games. There are also several other lesser-known and historical forms of archery, as well as archery novelty games and flight archery, where the aim is to shoot the greatest distance.\n\n\n"}
{"id": "37774663", "url": "https://en.wikipedia.org/wiki?curid=37774663", "title": "Autonomous sensory meridian response", "text": "Autonomous sensory meridian response\n\nAutonomous sensory meridian response (ASMR) is an experience characterized by a static-like or tingling sensation on the skin that typically begins on the scalp and moves down the back of the neck and upper spine. It has been compared with auditory-tactile synesthesia and may overlap with frisson.\n\nASMR signifies the subjective experience of \"low-grade euphoria\" characterized by \"a combination of positive feelings and a distinct static-like tingling sensation on the skin\". It is most commonly triggered by specific auditory or visual stimuli, and less commonly by intentional attention control.\n\nPrior to the subsequent social consensus that led to what is now the ubiquitous adoption of that term, other names were proposed and discussed at a number of locations including the Steady Health forum, the Society of Sensationalists Yahoo! group and the Unnamed Feeling Blog.\n\nProposed formal names included \"auditory induced head orgasm\", \"attention induced euphoria\" and \"attention induced observant euphoria\", while colloquial terms in usage included \"brain massage\", \"head tingle\", \"brain tingle\", \"spine tingle\" and \"brain orgasm\".\n\nWhile many colloquial and formal terms used and proposed between 2007 and 2010 included reference to orgasm, there was during that time a significant majority objection to its use among those active in online discussions, many of whom have continued to persist in differentiating the euphoric and relaxing nature of ASMR from sexual arousal. However, by 2015, a division had occurred within the ASMR community over the subject of sexual arousal, with some creating videos categorized as ASMRotica (ASMR erotica), which are deliberately designed to be sexually stimulating.\n\nThe initial consensus among the ASMR community was that the name should not pose a high risk of the phenomenon being perceived as sexual. Given that consensus, Jennifer Allen proposed \"autonomous sensory meridian response\". Allen chose the words intending or assuming them to have the following specific meanings:\nAllen verified in a 2016 interview that she purposely selected these terms because they were more objective, comfortable, and clinical than alternative terms for the sensation. Allen explained she selected the word meridian to replace the word orgasm due to its meaning of point or period of greatest prosperity. \n\nThe term \"autonomous sensory meridian response\" and its acronym ASMR were adopted by both the community of contributors to online discussions and those reporting and commentating on the phenomenon.\n\nThe subjective experience, sensation, and perceptual phenomenon now widely identified by the term 'autonomous sensory meridian response' is described by some of those susceptible to it as 'akin to a mild electrical current...or the carbonated bubbles in a glass of champagne'.\n\nASMR is usually precipitated by stimuli referred to as 'triggers'. ASMR triggers, which are most commonly auditory and visual, may be encountered through the interpersonal interactions of daily life. Additionally, ASMR is often triggered by exposure to specific audio and video. Such media may be specially made with the specific purpose of triggering ASMR or originally created for other purposes and later discovered to be effective as a trigger of the experience.\n\nStimuli that can trigger ASMR, as reported by those who experience it, include the following:\n\nWatching and listening to an audiovisual recording of a person performing or simulating the above actions and producing their consequent and accompanying sounds is sufficient to trigger ASMR for the majority of those who report susceptibility to the experience.\n\nPsychologists Nick Davis and Emma Barratt discovered that whispering was an effective trigger for 75% of the 475 subjects who took part in an experiment to investigate the nature of ASMR, and that statistic is reflected in the popularity of intentional ASMR videos that comprise someone speaking in a whispered voice.\n\nMany of those who experience ASMR report that some specific non-vocal ambient noises are also effective triggers of ASMR, including those like the sound of rain, fingers scratching or tapping a surface, the crushing of eggshells, the crinkling and crumpling of a flexible material such as paper, or writing. Many YouTube videos that are intended to trigger ASMR responses capture a single person performing these actions and the sounds that result.\n\nIn addition to the effectiveness of specific auditory stimuli, many subjects report that ASMR is triggered by the receipt of tender personal attention, often comprising combined physical touch and vocal expression, such as when having their hair cut, nails painted, ears cleaned, or back massaged, whilst the service provider speaks quietly to the recipient. Furthermore, many of those who have experienced ASMR during these and other comparable encounters with a service provider report that watching an \"ASMRtist\" simulate the provision of such personal attention, acting directly to the camera as if the viewer were the recipient of a simulated service, is sufficient to trigger it.\n\nPsychologists Nick Davis and Emma Barratt discovered that personal attention was an effective trigger for 69% of the 475 subjects who participated in a study conducted at Swansea University, second in popularity only to whispering.\n\nSome roleplays also incorporate fantasy or science fiction elements in a way that allows \"escape\" for the viewers. Some also incorporate legitimate stories into the roleplays in a way that could be considered entertainment in its own right, outside of the ASMR phenomenon.\n\nAmong the category of intentional ASMR videos that simulate the provision of personal attention is a subcategory of those specifically depicting the \"ASMRtist\" providing clinical or medical services, including routine general medical examinations. The creators of these videos make no claims to the reality of what is depicted, and the viewer is intended to be aware that they are watching and listening to a simulation, performed by an actor. Nonetheless, many subjects attribute therapeutic outcomes to these and other categories of intentional ASMR videos, and there are voluminous anecdotal reports of their effectiveness in inducing sleep for those susceptible to insomnia, and assuaging a range of symptoms including those associated with depression, anxiety, and panic attacks.\n\nIn the first peer-reviewed article on ASMR, published in \"Perspectives in Biology\" in summer 2013, Nitin Ahuja, who was at the time of publication a medical resident at the University of Virginia, invited conjecture on whether the receipt of simulated medical attention might have some tangible therapeutic value for the recipient, comparing the purported positive outcome of clinical role play ASMR videos with the themes of the novel \"Love in the Ruins\" by author and physician Walker Percy, published in 1971.\n\nThe story follows Tom More, a psychiatrist living in a dystopian future who develops a device called the Ontological Lapsometer that, when traced across the scalp of a patient, detects the neurochemical correlation to a range of disturbances. In the course of the novel, More admits that the 'mere application of his device' to a patient's body 'results in the partial relief of his symptoms'.\n\nAhuja alleges that through the character of Tom More, as depicted in \"Love in the Ruins\", Percy 'displays an intuitive understanding of the diagnostic act as a form of therapy unto itself'. Ahuja asks whether similarly, the receipt of simulated personal clinical attention by an actor in an ASMR video might afford the listener and viewer some relief.\n\nThe contemporary history of ASMR began on 19 October 2007 when a 21-year-old registered user of a discussion forum for health-related subjects at a website called 'Steady Health', with the username 'okaywhatever', submitted a post in which they described having experienced a specific sensation since childhood, comparable to that stimulated by tracing fingers along the skin, yet often triggered by seemingly random and unrelated non-haptic events, such as 'watching a puppet show' or 'being read a story'.\n\nReplies to this post, which indicated that a significant number of others experienced the sensation to which 'okaywhatever' referred, also in response to witnessing mundane events, precipitated the formation of a number of web-based locations intended to facilitate further discussion and analysis of the phenomenon for which there was plentiful anecdotal accounts, yet no consensus-agreed name nor any scientific data or explanation.\n\nAustrian writer Clemens J. Setz suggests that a passage from the novel \"Mrs. Dalloway\" authored by Virginia Woolf and published in 1925, describes something distinctly comparable. In the passage from \"Mrs. Dalloway\" cited by Setz, a nursemaid speaks to the man who is her patient 'deeply, softly, like a mellow organ, but with a roughness in her voice like a grasshopper's, which rasped his spine deliciously and sent running up into his brain waves of sound'.\n\nAccording to Setz, this citation generally alludes to the effectiveness of the human voice and soft or whispered vocal sounds specifically as a trigger of ASMR for many of those who experience it, as demonstrated by the responsive comments posted to YouTube videos that depict someone speaking softly or whispering, typically directly to camera.\n\nNothing can currently be definitively known about any evolutionary origins for ASMR since the perceptual phenomenon itself has yet to be clearly identified as having biological correlations. Even so, a significant majority of descriptions of ASMR by those who experience it compare the sensation to that precipitated by receipt of tender physical touch, providing examples such as having their hair cut or combed. This has led to the conjecture that ASMR might be related to the act of grooming.\n\nFor example, David Huron, Professor in the School of Music at Ohio State University, states: \"[The 'ASMR effect' is] clearly strongly related to the perception of non-threat and altruistic attention [and has a] strong similarity to physical grooming in primates [who] derive enormous pleasure (bordering on euphoria) when being groomed by a grooming partner...not to get clean, but rather to bond with each other.\"\n\nImaging subjects' brains with fMRI as they reported experiencing ASMR tingles suggests support for this hypothesis, because brain areas such as the medial prefrontal cortex (associated with social behaviors including grooming), and the secondary somatosensory cortex (associated with sensation of touch) were activated more strongly during tingle periods than control periods.\n\nWhile little scientific research has been conducted into potential neurobiological correlates to the perceptual phenomenon known as 'autonomous sensory meridian response' (ASMR), with a consequent dearth of data with which to either explain or refute its physical nature, there is voluminous anecdotal literature comprising personal commentary and intimate disclosure of subjective experiences distributed across forums, blogs, and YouTube comments by hundreds of thousands of people. Within this literature, in addition to the original consensus that ASMR is euphoric but non-sexual in nature, a further point of continued majority agreement within the community of those who experience it is that they fall into two broad categories of subjects.\n\nOne category depends upon external triggers in order to experience the localized sensation and its associated feelings, which typically originates in the head, often reaching down the neck and sometimes the upper back. The other category can intentionally augment the sensation and feelings through attentional control, without dependence upon external stimuli, or 'triggers', in a manner compared by some subjects to their experience of meditation.\n\nThe most popular source of stimuli reported by subjects to be effective in triggering ASMR is video. Videos reported being effective in triggering ASMR fall into two categories, identified and named by the community as 'Intentional' and 'Unintentional'. Intentional media is created by those known within the community as 'ASMRtists' with the purpose of triggering ASMR in viewers and listeners. Unintentional media is that made for other purposes, often before attention was drawn to the phenomenon in 2007, but which some subjects discover to be effective in triggering ASMR. One early unintentional example is the Art Bears song 'The Bath of Stars.' Another example of unintentional media several journalists have noted is of famed painter Bob Ross. In episodes of his popular television series \"The Joy of Painting\" both broadcast and on YouTube, his soft, gentle, altruistic speaking mannerisms and the sound of him painting and his tools trigger the effect on many of his viewers. The work of stop-motion filmmaker PES is also often noted.\n\nSome ASMR video creators use binaural recording techniques to simulate the acoustics of a three-dimensional environment, reported to elicit in viewers and listeners the experience of being in close proximity to actor and vocalist. Binaural recordings are made usually made using two microphones, just like stereo recordings. However, in binaural recordings the two microphones tend to be more specially designed to mimic ears on humans. In many cases, microphones are separated the same distance as ears are on humans, and microphones are surrounded by ear shaped cups to get similar reverb as human ears.\n\nViewing and hearing such ASMR videos that comprise ambient sound captured through binaural recording has been compared to the reported effect of listening to binaural beats, which are also alleged to precipitate pleasurable sensations and the subjective experience of calm and equanimity.\n\nBinaural recordings are made specifically to be heard through headphones rather than loudspeakers. When listening to sound through loudspeakers, the left and right ear can both hear the sound coming from both speakers. By distinction, when listening to sound through headphones, the sound from the left earpiece is audible only to the left ear, and the sound from the right ear piece is audible only to the right ear. When producing binaural media, the sound source is recorded by two separate microphones, placed at a distance comparable to that between two ears, and they are not mixed, but remain separate on the final medium, whether video or audio.\n\nListening to a binaural recording through headphones simulates the binaural hearing by which people listen to live sounds. For the listener, this experience is characterised by two perceptions. Firstly, the listener perceives being in close proximity to the performers and location of the sound source. Secondly, the listener perceives what is often reported as a three dimensional sound. This means the listener can perceive both the position and distance of the source of sound relative to them.\n\nSeveral peer-reviewed articles about ASMR have been published.\n\nThe first, by the physician Nitin Ahuja, is titled \"It Feels Good to Be Measured: clinical role-play, Walker Percy, and the tingles\". It was published in \"Perspectives in Biology and Medicine\" during 2013 and focused on a conjectural cultural and literary analysis.\n\nAnother article, published in the journal \"Television and New Media\" in November 2014, is by Joceline Andersen, a doctoral student in the Department of Art History and Communication Studies at McGill University, who suggested that ASMR videos comprising whispering 'create an intimate sonic space shared by the listener and the whisperer'. Andersen's article proposes that the pleasure jointly shared by both an ASMR video creator and its viewers might be perceived as a particular form of 'non-standard intimacy' by which consumers pursue a form of pleasure mediated by video media. Andersen suggests that such pursuit is private yet also public or publicized through the sharing of experiences via online communication with others within the 'whispering community'.\n\nAnother article, \"Autonomous Sensory Meridian Response (ASMR): a flow-like mental state\", by Nick Davis and Emma Barratt, lecturer and post-graduate researcher respectively in the Department of Psychology at Swansea University, was published in PeerJ. This article aimed to 'describe the sensations associated with ASMR, explore the ways in which it is typically induced in capable individuals ... to provide further thoughts on where this sensation may fit into current knowledge on atypical perceptual experiences ... and to explore the extent to which engagement with ASMR may ease symptoms of depression and chronic pain' The paper was based on a study of 245 men, 222 women, and 8 individuals of non-binary gender, aged from 18 to 54 years, all of whom had experienced ASMR, and regularly consumed ASMR media, from which the authors concluded and suggested that 'given the reported benefits of ASMR in improving mood and pain symptoms...ASMR warrants further investigation as a potential therapeutic measure similar to that of meditation and mindfulness.'\n\nAn article titled \"An examination of the default mode network in individuals with autonomous sensory meridian response (ASMR)\" by Stephen D. Smith, Beverley Katherine Fredborg, and Jennifer Kornelsen, looked at the default mode network (DMN) in individuals with ASMR. The study, which used functional magnetic resonance imaging (fMRI), concluded that there were significant differences in the DMN of individuals who have ASMR as compared to a control group without ASMR.\n\nThe first study to perform actual brain imaging (fMRI) on subjects currently experiencing ASMR tingles (as opposed to individuals who were merely able to experience the phenomenon) was published in \"BioImpacts\" in September 2018. Subjects viewed several ASMR videos with a screen and headphones while inside the MRI scanner. The study found a significant difference in brain activation between time periods when the subject reported tingling (communicated by pressing a button), as compared to time periods when they were watching a video but not reporting tingling (communicated by pressing a different button, to control for brain activation effects caused by merely pressing a button). They concluded that \"the brain regions found most active during the tingling sensations were the nucleus accumbens, mPFC, insula and secondary somatosensory cortex\", and suggested that these were similar to \"activation of brain regions previously observed during experiences like social bonding and musical frisson\".\n\nA number of scientists have published or made public their reaction to and opinions of ASMR.\n\nOn 12 March 2012, Steven Novella, Director of General Neurology at the Yale School of Medicine, published a post about ASMR on his blog \"Neurologica\". Regarding the question of whether ASMR is a real phenomeonon, Novella said \"in this case, I don't think there is a definitive answer, but I am inclined to believe that it is. There are a number of people who seem to have independently experienced and described\" it with \"fairly specific details. In this way it's similar to migraine headaches – we know they exist as a syndrome primarily because many different people report the same constellation of symptoms and natural history.\" Novella tentatively posited the possibilities that ASMR might be either a type of pleasurable seizure, or another way to activate the \"pleasure response\". However, Novella drew attention to the lack of scientific investigation into ASMR, suggesting that functional magnetic resonance imaging (fMRI) and transcranial magnetic stimulation technologies should be used to study the brains of people who experience ASMR in comparison to people who do not, as a way of beginning to seek scientific understanding and explanation of the phenomenon.\n\nFour months after Novella's blog post, Tom Stafford, a lecturer in psychology and cognitive sciences at the University of Sheffield, was reported to have said that ASMR \"might well be a real thing, but it's inherently difficult to research...something like this that you can't see or feel\" and \"doesn't happen for everyone\". Stafford compared the current status of ASMR with development of attitudes toward synesthesia, which he said \"for years...was a myth, then in the 1990s people came up with a reliable way of measuring it\".\n\nIntegral to the subjective experience of ASMR is a localized tingling sensation that many describe as similar to being gently touched, but which is stimulated by watching and listening to video media in the absence of any physical contact with another person.\n\nThese reports have precipitated comparison between ASMR and synesthesia – a condition characterised by the excitation of one sensory modality by stimuli that normally exclusively stimulates another, as when the hearing of a specific sound induces the visualization of a distinct color, a type of synesthesia called chromesthesia. Thereby, people with other types of synesthesia report for example 'seeing sounds' in the case of auditory-visual synesthesia, or 'tasting words' in the case of lexical-gustatory synesthesia.\n\nIn the case of ASMR, many report the perception of 'being touched' by the sights and sounds presented on a video recording, comparable to visual-tactile and auditory-tactile synesthesia.\n\nSome commentators and members of the ASMR community have sought to relate ASMR to misophonia, which literally means the 'hatred of sound', but manifests typically as 'automatic negative emotional reactions to particular sounds – the opposite of what can be observed in reactions to specific audio stimuli in ASMR'.\n\nFor example, those who suffer from misophonia often report that specific human sounds, including those made by breathing or whispering with any loudness can precipitate feelings of anger and disgust, in the absence of any previously learned associations that might otherwise explain those reactions.\n\nThere are plentiful anecdotal reports by those who claim to have both misophonia and ASMR at multiple web-based user-interaction and discussion locations. Common to these reports is the experience of ASMR to some sounds, and misophonia in response to others.\n\nThe tingling sensation that characterises ASMR has been compared and contrasted to 'frisson', which is a French word for 'shiver'.\n\nHowever, the English word 'shiver' signifies the rhythmic involuntary contraction of skeletal muscles which serves the function of generating heat in response to low temperatures, has variable duration, and is often reported subjectively as unpleasant. By distinction, the French word 'frisson', signifies a brief sensation usually reported as pleasurable and often expressed as an overwhelming emotional response to stimuli, such as a piece of music. Frisson often occurs simultaneously with piloerection, colloquially known as 'goosebumps', by which tiny muscles called arrector pili contract, causing body hair, particularly that on the limbs and back of the neck, to erect or 'stand on end'.\n\nFew legitimate studies have been done on ASMR, and even fewer have discussed the link between it and frisson specifically. At this time, much of the data on ASMR comes from primarily anecdotal sources. Although ASMR and frisson are \"interrelated in that they appear to arise through similar physiological mechanisms\", individuals who have experienced both describe them as qualitatively different, with different kinds of triggers. A 2018 fMRI study showed that the major brain regions already known to be activated in frisson are also activated in ASMR, and suggests that \"the similar pattern of activation of both ASMR and frisson could explain their subjective similarities, such as their short duration and tingling sensation\".\n\nThere have been persistent efforts by many of those who form the 'ASMR community' to distinguish the euphoric sensation that characterizes ASMR from sexual arousal, and to differentiate video media created with intent to trigger it from pornography.\n\nMeanwhile, some journalists and commentators have drawn attention to the way in which many videos made as triggers are susceptible to being perceived as sexually provocative in a number of ways. Firstly, the use of objects as acoustic instruments and points of visual focus, accompanied by a softly spoken voice has been described as fetishistic. Secondly, ASMR's potential appeal is further allegedly sexualized by their use of a whispered vocal expression and gentleness of simulated touch purportedly associated exclusively with intimacy. There are both popular male and female 'ASMRtists'.\n\nBritish artist Lucy Clout's single channel video 'Shrugging Offing', made for exhibition in March 2013, uses the model of online ASMR broadcasts as the basis for a work exploring the female body.\n\nThe first digital arts installation specifically inspired by ASMR was by the American artist Julie Weitz and called \"Touch Museum\", which opened at the Young Projects Gallery on 13 February 2015 and comprised video screenings distributed throughout seven rooms.\n\nThe music for Julie Weitz' \"Touch Museums\" digital arts installation was composed by Benjamin Wynn under his pseudonym 'Deru' and was the first musical composition specifically created for live ASMR arts event.\n\nSubsequently, artists Sophie Mallett and Marie Toseland created 'a live binaural sound work' composed of ASMR triggers, broadcast by Resonance FM, the listings for which advised the audience to 'listen with headphones for the full sensory effect'.\n\nOn 18 May 2015, contemporary composer Holly Herndon released an album called \"Platform\" which included a collaboration with artist Claire Tolan named \"Lonely At The Top\", intended to trigger ASMR.\n\nThe track \"Brush\" from Holly Pester's 2016 album and poetry collection \"Common Rest\" featured artist Claire Tolan, exploring ASMR and its relation to lullaby.\n\nThe hair-cutting scene of the film \"Battle of the Sexes\" deliberately included several ASMR triggers. Director Jonathan Dayton stated \"People work to make videos that elicit this response [...] and we were wondering, 'Could we get that response in a theater full of people?'\" \n\nThere have been three successfully crowdfunded projects, each based on proposals to make a film about ASMR: two documentaries and one fictional piece. None of these films are currently completed.\n\nOn 31 July 2015, the BBC panel show \"Would I Lie To You?\" featured an ASMR content maker as a guest as part of the \"This is my\" round, which resulted in the reveal of the person connected to comedian Joe Lycett.\n\nIn 2018, ASMR, along with a number of its adherents, was featured on Netflix's show \"Follow This\" for an episode titled: \"Internet Whisperers\"\n\nIn March 2013, the American weekly hour-long radio program \"This American Life\", broadcast the first short story on the subject of ASMR, called \"A Tribe Called Rest\", authored and read by American novelist and screenwriter Andrea Seigel.\n\nThere is currently one non-fiction book on ASMR, part of the Idiot's Guide series.\n\nIn addition to the information collected from the 475 subjects who participated in the scientific investigation conducted by Nick Davies and Emma Barratt, there have been two attempts to collate statistical data pertaining to the demographics, personal history, clinical conditions, and subjective experience of those who report susceptibility to ASMR.\n\nIn December 2012, Craig Richard – a blogger on the subject of ASMR – published the first results of a poll comprising 12 questions that had received 161 respondents, followed by second results in August 2015 by which time there were 477 responses.\n\nIn August 2014, Craig Richard, Jennifer Allen, and Karissa Burnett published a survey at SurveyMonkey that was reviewed by Shenandoah University Institutional Review Board, and the Fuller Theological Seminary School of Psychology Human Studies Review Committee. In September 2015, when the survey had received 13,000 responses, the publishers announced that they were analyzing the data with the intent to publish the results. No such publication or report is yet available.\n\nIn 2018, an fMRI scan experiment journal was written in by Bryson C. Lochte, Sean A. Guillory, Craig A. H. Richard, William M. Kelley. In short, the subjects that experienced ASMR showed a significant increase in activity in the regions of the brain associated with reward and emotional arousal.\n\n\n"}
{"id": "8112760", "url": "https://en.wikipedia.org/wiki?curid=8112760", "title": "Barker's notation", "text": "Barker's notation\n\nBarker's notation refers to the ERD notation developed by Richard Barker, Ian Palmer, Harry Ellis et al. whilst working at the British consulting firm CACI around 1981. The notation was adopted by Barker when he joined Oracle and is effectively defined in his book \"Entity Relationship Modelling\" as part of the CASE Method series of books. This notation was and still is used by the Oracle CASE modelling tools. It is a variation of the \"crows foot\" style of data modelling that was favoured by many over the original Chen style of ERD modelling because of its readability and efficient use of drawing space.\n\nThe notation has features that represent the properties of relationships including cardinality and optionality (the crows foot and dashing of lines), exclusion (the exclusion arc), recursion\n(looping structures) and use of abstraction (nested boxes).\n\n\n"}
{"id": "10938601", "url": "https://en.wikipedia.org/wiki?curid=10938601", "title": "Billy Apple", "text": "Billy Apple\n\nBilly Apple ONZM (born Barrie Bates in Auckland, New Zealand in 1935), is an artist whose work is associated with the New York and British schools of Pop Art in the 1960s and with the Conceptual Art movement in the 1970s. He collaborated with the likes of Andy Warhol and other pop artists. His work is part of the permanent collection of the Museum of New Zealand Te Papa Tongarewa (New Zealand), Auckland Art Gallery / Toi o Tamaki (New Zealand), the Christchurch Art Gallery / Te Puna o Waiwhetu (New Zealand), The University of Auckland (New Zealand) and the SMAK/Stedelijk Museum voor Actuele Kunst (Ghent, Belgium).\n\nBarrie Bates was born in Auckland, New Zealand in 1935. He left secondary school with no qualifications and took a job as an assistant to a paint manufacturer in 1951. Bates attended evening classes at Elam School of Fine Arts, where he met Robert Ellis, a graduate of the Royal College of Art in London.\n\nIn 1959, he left New Zealand on a National Art Gallery scholarship. He studied at the Royal College of Art, London, from 1959 until 1962. During his time at the Royal College of Art, Bates met several other artists who went on to become a new generation of pop artists; including David Hockney, Derek Boshier Frank Bowling and Pauline Boty. He exhibited frequently during his time at the College in the \"Young Contemporaries\" and \"Young Commonwealth Artists\" exhibitions along with Frank Bowling, Jonathan Kingdon, Bill Culbert, Jan Bensemann and Jerry Pethick.\n\nIn 1962, Bates conceived Billy Apple: he bleached his hair and eyebrows with Lady Clairol Instant Creme Whip and changed his name to Billy Apple. Apple had his first solo show in 1963 – \"Apple Sees Red: Live Stills\" – in London .\n\nApple moved to New York in 1964: he progressed his artistic career and also found work in various advertising agencies.\n\nA pivotal event was the 1964 exhibit \"The American Supermarket\", a show held in Paul Bianchini's Upper East Side gallery. The show was presented as a typical small supermarket environment, except that everything in it – the produce, canned goods, meat, posters on the wall, etc. – was created by six prominent pop artists of the time, including Billy Apple, Andy Warhol, Claes Oldenburg, Tom Wesselmann, Jasper Johns, Mary Inman, James Rosenquist and Robert Watts.\n\nApple was one of the artists who pioneered the use of neon in art works. This is seen in the 1965 exhibitions \"Apples to Xerox\" and \"Neon Rainbows\", both at The Bianchini Gallery. Then in 1967, the exhibition \"Unidentified Fluorescent Objects\" (\"UFOs\"), which showed a collection of neon light sculptures, was held at the Howard Wise Gallery, a fore-runner to the organisation Electronic Arts Intermix (EAI). One of Apple's \"UFO\"'s was included in a 2013 exhibition that reconsidered the influence of Howard Wise Gallery.\n\nIn 1969, the artist established \"Apple\", one of the first alternative exhibition spaces in New York at 161 West Twenty-third Street in order, as he stated, \"to provide an independent and experimental alternative space for the presentation of [his] own work and the work of others.\" Initially the exhibition space was part of his own studio. During its four years Apple produced 35 works in the venue and hosted work by other artists including Geoff Hendricks, Mac Adams, Davi Det Hompson, Larry Miller and Jerry Vis. The space was considered both an exhibition space and a forum for art and discourse.\n\nIn 1974, Apple's first major survey exhibition was held at the Serpentine Gallery in London: \"From Barrie Bates to Billy Apple\". In 1975 Apple returned to New Zealand for the first time in sixteen years. During the visit he embarked on a national exhibition tour with support from the Queen Elizabeth II Arts Council. Apple was then invited back by the Arts Council for a tour over the summer of 1979–1980. The exhibition he toured was called \"The Given as an Art Political Statement\". During each tour he exhibited in spaces throughout the country.\n\nDuring the 1980s, Apple's practice focused on the economics of the art world. The exhibition \"Art for Sale\" at Peter Webb gallery in 1980 was made up of a series of art works that were actual receipts for the payment given to the artist. This work progressed on to a series called \"Transactions\". Other important series of work that began in the 1980s include \"Golden Rectangle\" series, and \"From the Collection\". In 1983 he produced a solid gold apple for former Auckland Coin & Bullion Exchange Director, Ray Smith, valued at $(NZ)85,000 – the most expensive work made by a living New Zealander at the time and a significant precursor to Damien Hirst's 2007 diamond skull titled For the Love of God. The gold apple was later exhibited at Artspace, Auckland in 2004 as part of an installation developed with regular collaborator and writer, Wystan Curnow.\n\nHe returned to New Zealand, permanently in 1990 and currently lives in Auckland. In 1991 the Wellington City Art Gallery staged a decade survey of his work: \"As Good as Gold: Billy Apple Art Transactions 1981–1991\". Negotiations are underway between Saatchi & Saatchi and the New Zealand horticulture research centre to develop an apple that could be named \"Billy Apple\". In 2001 Apple created a company, \"Billy Apple Ltd\" in anticipation of securing licensing of the marketing rights over this new apple.\n\nThe artist has a long-standing interest and involvement in motor racing, which was acknowledged with the inclusion of two vehicles from his own collection in the 1991 \"As Good as Gold\" survey, as well as in the accompanying publication. This interest was brought to the fore with \"The Art Circuit\", a sound performance work incorporating famous bikes and riders staged on the Auckland Art Gallery forecourt in 2007. This was followed by the 2008 solo exhibition, \"The Bruce and Denny Show\", presented at Two Rooms in 2008 as a tribute to the McLaren brand, and particularly to the motoring triumphs of Bruce McLaren and Denny Hulme from 1967–1969. The exhibition included Hulme's $1.5 million McLaren M8A-2 racing car and text works that make reference to the tracks raced and the livery of the two drivers' cars.\n\nIn 2008, Apple was the subject of a feature-length documentary called \"Being Billy Apple\". Produced by Spacific Films and directed by award winning filmmaker, Leanne Pooley, the documentary tells the story of Billy Apple's life from his POP period through his involvement with the conceptual art movement in New York during the 1970s to his current \"horticultural/art\" Apple endeavours.\n\nIn 2009, the Adam Art Gallery, Wellington staged the survey exhibition \"Billy Apple: New York 1969–1973\", covering the activities undertaken by the artist in the not-for-profit gallery he ran from 161 West 23rd Street. Later in 2009 Witte de With Centre for Contemporary Art in Amsterdam presented a major exhibition in two parts, curated by Nicolaus Schafhausen; the first \"Billy Apple: A History of the Brand\", surveys the artist's entire practice from inception as his own brand to the present day; the second, \"Revealed/Concealed\", focuses on his works that critique the site of art through architectural interventions.\n\n\n\nWorks include\n"}
{"id": "22367851", "url": "https://en.wikipedia.org/wiki?curid=22367851", "title": "Biordered set", "text": "Biordered set\n\nA biordered set (\"boset\") is a mathematical object that occurs in the description of the structure of the set of idempotents in a semigroup. The concept and the terminology were developed by K S S Nambooripad in the early 1970s.\nThe defining properties of a biordered set are expressed in terms of two quasiorders defined on the set and hence the name biordered set. Patrick Jordan, while a master's student at University of Sydney, introduced in 2002 the term boset as an abbreviation of biordered set.\n\nAccording to Mohan S. Putcha, \"The axioms defining a biordered set are quite complicated. However, considering the general nature of semigroups, it is rather surprising that such a finite axiomatization is even possible.\" Since the publication of the original definition of the biordered set by Nambooripad, several variations in the definition have been proposed. David Easdown simplified the definition and formulated the axioms in a special arrow notation invented by him.\n\nThe set of idempotents in a semigroup is a biordered set and every biordered set is the set of idempotents of some semigroup.\nA regular biordered set is a biordered set with an additional property. The set of idempotents in a regular semigroup is a regular biordered set, and every regular biordered set is the set of idempotents of some regular semigroup. \nThe formal definition of a biordered set given by Nambooripad requires some preliminaries. \n\nIf \"X\" and \"Y\" be sets and ρ⊆ \"X\" × \"Y\", let ρ ( \"y\" ) = { \"x\" ∈ \"X\" : \"x\" ρ \"y\" }. \nLet \"E\" be a set in which a partial binary operation, indicated by juxtaposition, is defined. If \"D\" is the domain of the partial binary operation on \"E\" then \"D\" is a relation on \"E\" and (\"e\",\"f\") is in \"D\" if and only if the product \"ef\" exists in \"E\". The following relations can be defined in \"E\":\n\nIf \"T\" is any statement about \"E\" involving the partial binary operation and the above relations in \"E\", one can define the left-right dual of \"T\" denoted by \"T\"*. If \"D\" is symmetric then \"T\"* is meaningful whenever \"T\" is. \n\nThe set \"E\" is called a biordered set if the following axioms and their duals hold for arbitrary elements \"e\", \"f\", \"g\", etc. in \"E\".\n\nIn \"M\" ( \"e\", \"f\" ) = ω ( \"e\" ) ∩ ω ( \"f\" ) (the \"M\"-set of \"e\" and \"f\" in that order), define a relation formula_6 by \n\nThen the set\n\nis called the sandwich set of \"e\" and \"f\" in that order. \n\nWe say that a biordered set \"E\" is an \"M\"-biordered set if \"M\" ( \"e\", \"f\" ) ≠ ∅ for all \"e\" and \"f\" in \"E\". \nAlso, \"E\" is called a regular biordered set if \"S\" ( \"e\", \"f\" ) ≠ ∅ for all \"e\" and \"f\" in \"E\".\n\nIn 2012 Roman S. Gigoń gave a simple proof that \"M\"-biordered sets arise from \"E\"-inversive semigroups.\n\nA subset \"F\" of a biordered set \"E\" is a biordered subset (subboset) of \"E\" if \"F\" is a biordered set under the partial binary operation inherited from \"E\". \n\nFor any \"e\" in \"E\" the sets ω ( \"e\" ), ω ( \"e\" ) and ω ( \"e\" ) are biordered subsets of \"E\".\n\nA mapping φ : \"E\" → \"F\" between two biordered sets \"E\" and \"F\" is a biordered set homomorphism (also called a bimorphism) if for all ( \"e\", \"f\" ) in \"D\" we have ( \"e\"φ ) ( \"f\"φ ) = ( \"ef\" )φ.\n\nLet \"V\" be a vector space and \n\nwhere \"V\" = \"A\" ⊕ \"B\" means that \"A\" and \"B\" are subspaces of \"V\" and \"V\" is the internal direct sum of \"A\" and \"B\". \nThe partial binary operation ⋆ on E defined by \n\nmakes \"E\" a biordered set. The quasiorders in \"E\" are characterised as follows: \n\nThe set \"E\" of idempotents in a semigroup \"S\" becomes a biordered set if a partial binary operation is defined in \"E\" as follows: \"ef\" is defined in \"E\" if and only if \"ef\" = \"e\" or \"ef\"= \"f\" or \"fe\" = \"e\" or \"fe\" = \"f\" holds in \"S\". If \"S\" is a regular semigroup then \"E\" is a regular biordered set.\n\nAs a concrete example, let \"S\" be the semigroup of all mappings of \"X\" = { 1, 2, 3 } into itself. Let the symbol (\"abc\") denote the map for which 1 → \"a\", 2 → \"b\", and 3 → \"c\". The set \"E\" of idempotents in \"S\" contains the following elements:\n\nThe following table (taking composition of mappings in the diagram order) describes the partial binary operation in \"E\". An X in a cell indicates that the corresponding multiplication is not defined. \n"}
{"id": "35099585", "url": "https://en.wikipedia.org/wiki?curid=35099585", "title": "Cognitive bias mitigation", "text": "Cognitive bias mitigation\n\nCognitive bias mitigation is the prevention and reduction of the negative effects of cognitive biases – unconscious, automatic influences on human judgment and decision making that reliably produce reasoning errors.\n\nCoherent, comprehensive theories of cognitive bias mitigation are lacking. This article describes debiasing tools, methods, proposals and other initiatives, in academic and professional disciplines concerned with the efficacy of human reasoning, associated with the concept of cognitive bias mitigation; most address mitigation tacitly rather than explicitly.\n\nA long-standing debate regarding human decision making bears on the development of a theory and practice of bias mitigation. This debate contrasts the rational economic agent standard for decision making versus one grounded in human social needs and motivations. The debate also contrasts the methods used to analyze and predict human decision making, i.e. formal analysis emphasizing intellectual capacities versus heuristics emphasizing emotional states. This article identifies elements relevant to this debate.\n\nA large body of evidence has established that a defining characteristic of cognitive biases is that they manifest automatically and unconsciously over a wide range of human reasoning, so even those aware of the existence of the phenomenon are unable to detect, let alone mitigate, their manifestation via awareness only.\n\nThere are few studies explicitly linking cognitive biases to real-world incidents with highly negative outcomes. Examples:\n\nThere are numerous investigations of incidents determining that human error was central to highly negative potential or actual real-world outcomes, in which manifestation of cognitive biases is a plausible component. Examples:\n\nEach of the approximately 100 cognitive biases known to date can also produce negative outcomes in our everyday lives, though rarely as serious as in the examples above. An illustrative selection, recounted in multiple studies:\n\nAn increasing number of academic and professional disciplines are identifying means of cognitive bias mitigation. Notable examples in the field of debiasing, is a model by the NeuroLeadership Institute that categorizes over 150 known cognitive biases into a decision-making framework.\n\nWhat follows is a characterization of the assumptions, theories, methods and results, in disciplines concerned with the efficacy of human reasoning, that plausibly bear on a theory and/or practice of cognitive bias mitigation. In most cases this is based on explicit reference to cognitive biases or their mitigation, in others on unstated but self-evident applicability. This characterization is organized along lines reflecting historical segmentation of disciplines, though in practice there is a significant amount of overlap.\n\nDecision theory, a discipline with its roots grounded in neo-classical economics, is explicitly focused on human reasoning, judgment, choice and decision making, primarily in 'one-shot games' between two agents with or without perfect information. The theoretical underpinning of decision theory assumes that all decision makers are rational agents trying to maximize the economic expected value/utility of their choices, and that to accomplish this they utilize formal analytical methods such as mathematics, probability, statistics, and logic under cognitive resource constraints.\n\nNormative, or prescriptive, decision theory concerns itself with what people \"should\" do, given the goal of maximizing expected value/utility; in this approach there is no explicit representation in practitioners' models of unconscious factors such as cognitive biases, i.e. all factors are considered conscious choice parameters for all agents. Practitioners tend to treat deviations from what a rational agent would do as 'errors of irrationality', with the implication that cognitive bias mitigation can only be achieved by decision makers becoming more like rational agents, though no explicit measures for achieving this are proffered.\n\nPositive, or descriptive, decision theory concerns itself with what people \"actually\" do; practitioners tend to acknowledge the persistent existence of 'irrational' behavior, and while some mention human motivation and biases as possible contributors to such behavior, these factors are not made explicit in their models. Practitioners tend to treat deviations from what a rational agent would do as evidence of important, but as yet not understood, decision-making variables, and have as yet no explicit or implicit contributions to make to a theory and practice of cognitive bias mitigation.\n\nGame theory, a discipline with roots in economics and system dynamics, is a method of studying strategic decision making in situations involving multi-step interactions with multiple agents with or without perfect information. As with decision theory, the theoretical underpinning of game theory assumes that all decision makers are rational agents trying to maximize the economic expected value/utility of their choices, and that to accomplish this they utilize formal analytical methods such as mathematics, probability, statistics, and logic under cognitive resource constraints.\n\nOne major difference between decision theory and game theory is the notion of 'equilibrium', a situation in which all agents agree on a strategy because any deviation from this strategy punishes the deviating agent. Despite analytical proofs of the existence of at least one equilibrium in a wide range of scenarios, game theory predictions, like those in decision theory, often do not match actual human choices. As with decision theory, practitioners tend to view such deviations as 'irrational', and rather than attempt to model such behavior, by implication hold that cognitive bias mitigation can only be achieved by decision makers becoming more like rational agents.\n\nIn the full range of game theory models there are many that do not guarantee the existence of equilibria, i.e. there are conflict situations where there is no set of agents' strategies that all agents agree are in their best interests. However, even when theoretical equilibria exist, i.e. when optimal decision strategies are available for all agents, real-life decision-makers often do not find them; indeed they sometimes apparently do not even try to find them, suggesting that some agents are not consistently 'rational'. game theory does not appear to accommodate any kind of agent other than the rational agent.\n\nUnlike neo-classical economics and decision theory, behavioral economics and the related field, behavioral finance, explicitly consider the effects of social, cognitive and emotional factors on individuals' economic decisions. These disciplines combine insights from psychology and neo-classical economics to achieve this.\n\nProspect theory was an early inspiration for this discipline, and has been further developed by its practitioners. It is one of the earliest economic theories that explicitly acknowledge the notion of cognitive bias, though the model itself accounts for only a few, including loss aversion, anchoring and adjustment bias, endowment effect, and perhaps others. No mention is made in formal prospect theory of cognitive bias mitigation, and there is no evidence of peer-reviewed work on cognitive bias mitigation in other areas of this discipline.\n\nHowever, Daniel Kahneman and others have authored recent articles in business and trade magazines addressing the notion of cognitive bias mitigation in a limited form. These contributions assert that cognitive bias mitigation is necessary and offer general suggestions for how to achieve it, though the guidance is limited to only a few cognitive biases and is not self-evidently generalizable to others.\n\nNeuroeconomics is a discipline made possible by advances in brain activity imaging technologies. This discipline merges some of the ideas in experimental economics, behavioral economics, cognitive science and social science in an attempt to better understand the neural basis for human decision making.\n\nfMRI experiments suggest that the limbic system is consistently involved in resolving economic decision situations that have emotional valence, the inference being that this part of the human brain is implicated in creating the deviations from rational agent choices noted in emotionally valent economic decision making. Practitioners in this discipline have demonstrated correlations between brain activity in this part of the brain and prospection activity, and neuronal activation has been shown to have measurable, consistent effects on decision making. These results must be considered speculative and preliminary, but are nonetheless suggestive of the possibility of real-time identification of brain states associated with cognitive bias manifestation, and the possibility of purposeful interventions at the neuronal level to achieve cognitive bias mitigation.\n\nSeveral streams of investigation in this discipline are noteworthy for their possible relevance to a theory of cognitive bias mitigation.\n\nOne approach to mitigation originally suggested by Daniel Kahneman and Amos Tversky, expanded upon by others, and applied in real-life situations, is reference class forecasting. This approach involves three steps: with a specific project in mind, identify a number of past projects that share a large number of elements with the project under scrutiny; for this group of projects, establish a probability distribution of the parameter that is being forecast; and, compare the specific project with the group of similar projects, in order to establish the most likely value of the selected parameter for the specific project. This simply stated method masks potential complexity regarding application to real-life projects: few projects are characterizable by a single parameter; multiple parameters exponentially complicates the process; gathering sufficient data on which to build robust probability distributions is problematic; and, project outcomes are rarely unambiguous and their reportage is often skewed by stakeholders' interests. Nonetheless, this approach has merit as part of a cognitive bias mitigation protocol when the process is applied with a maximum of diligence, in situations where good data is available and all stakeholders can be expected to cooperate.\n\nA concept rooted in considerations of the actual machinery of human reasoning, bounded rationality is one that may inform significant advances in cognitive bias mitigation. Originally conceived of by Herbert A. Simon in the 1960s and leading to the concept of satisficing as opposed to optimizing, this idea found experimental expression in the work of Gerd Gigerenzer and others. One line of Gigerenzer's work led to the \"Fast and Frugal\" framing of the human reasoning mechanism, which focused on the primacy of 'recognition' in decision making, backed up by tie-resolving heuristics operating in a low cognitive resource environment. In a series of objective tests, models based on this approach outperformed models based on rational agents maximizing their utility using formal analytical methods. One contribution to a theory and practice of cognitive bias mitigation from this approach is that it addresses mitigation without explicitly targeting individual cognitive biases and focuses on the reasoning mechanism itself to avoid cognitive biases manifestation.\n\nIntensive situational training is capable of providing individuals with what appears to be cognitive bias mitigation in decision making, but amounts to a fixed strategy of selecting the single best response to recognized situations regardless of the 'noise' in the environment. Studies and anecdotes reported in popular-audience media of firefighter captains, military platoon leaders and others making correct, snap judgments under extreme duress suggest that these responses are likely not generalizable and may contribute to a theory and practice of cognitive bias mitigation only the general idea of domain-specific intensive training.\n\nSimilarly, expert-level training in such foundational disciplines as mathematics, statistics, probability, logic, etc. can be useful for cognitive bias mitigation when the expected standard of performance reflects such formal analytical methods. However, a study of software engineering professionals suggests that for the task of estimating software projects, despite the strong analytical aspect of this task, standards of performance focusing on workplace social context were much more dominant than formal analytical methods. This finding, if generalizable to other tasks and disciplines, would discount the potential of expert-level training as a cognitive bias mitigation approach, and could contribute a narrow but important idea to a theory and practice of cognitive bias mitigation.\n\nLaboratory experiments in which cognitive bias mitigation is an explicit goal are rare. One 1980 study explored the notion of reducing the optimism bias by showing subjects other subjects' outputs from a reasoning task, with the result that their subsequent decision-making was somewhat debiased.\n\nA recent research effort by Morewedge and colleagues (2015) found evidence for domain-general forms of debiasing. In two longitudinal experiments, debiasing training techniques featuring interactive games that elicited six cognitive biases (anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness), provided participants with individualized feedback, mitigating strategies, and practice, resulted in an immediate reduction of more than 30% in commission of the biases and a long term (2 to 3-month delay) reduction of more than 20%. The instructional videos were also effective, but were less effective than the games.\n\nThis discipline explicitly challenges the prevalent view that humans are rational agents maximizing expected value/utility, using formal analytical methods to do so. Practitioners such as Cosmides, Tooby, Haselton, Confer and others posit that cognitive biases are more properly referred to as cognitive heuristics, and should be viewed as a toolkit of cognitive shortcuts selected for by evolutionary pressure and thus are features rather than flaws, as assumed in the prevalent view. Theoretical models and analyses supporting this view are plentiful. This view suggests that negative reasoning outcomes arise primarily because the reasoning challenges faced by modern humans, and the social and political context within which these are presented, make demands on our ancient 'heuristic toolkit' that at best create confusion as to which heuristics to apply in a given situation, and at worst generate what adherents of the prevalent view call 'reasoning errors'.\n\nIn a similar vein, Mercier and Sperber describe a theory for confirmation bias, and possibly other cognitive biases, which is a radical departure from the prevalent view, which holds that human reasoning is intended to assist individual economic decisions. Their view suggests that it evolved as a social phenomenon and that the goal was argumentation, i.e. to convince others and to be careful when others try to convince us. It is too early to tell whether this idea applies more generally to other cognitive biases, but the point of view supporting the theory may be useful in the construction of a theory and practice of cognitive bias mitigation.\n\nThere is an emerging convergence between evolutionary psychology and the concept of our reasoning mechanism being segregated (approximately) into 'System 1' and 'System 2'. In this view, System 1 is the 'first line' of cognitive processing of all perceptions, including internally generated 'pseudo-perceptions', which automatically, subconsciously and near-instantaneously produces emotionally valenced judgments of their probable effect on the individual's well-being. By contrast, System 2 is responsible for 'executive control', taking System 1's judgments as advisories, making future predictions, via prospection, of their actualization and then choosing which advisories, if any, to act on. In this view, System 2 is slow, simple-minded and lazy, usually defaulting to System 1 advisories and overriding them only when intensively trained to do so or when cognitive dissonance would result. In this view, our 'heuristic toolkit' resides largely in System 1, conforming to the view of cognitive biases being unconscious, automatic and very difficult to detect and override. Evolutionary psychology practitioners emphasize that our heuristic toolkit, despite the apparent abundance of 'reasoning errors' attributed to it, actually performs exceptionally well, given the rate at which it must operate, the range of judgments it produces, and the stakes involved. The System 1/2 view of the human reasoning mechanism appears to have empirical plausibility (see Neuroscience, next) and thus may contribute to a theory and practice of cognitive bias mitigation.\n\nNeuroscience offers empirical support for the concept of segregating the human reasoning mechanism into System 1 and System 2, as described above, based on brain activity imaging experiments using fMRI technology. While this notion must remain speculative until further work is done, it appears to be a productive basis for conceiving options for constructing a theory and practice of cognitive bias mitigation.\n\nAnthropologists have provided generally accepted scenarios of how our progenitors lived and what was important in their lives. These scenarios of social, political, and economic organization are not uniform throughout history or geography, but there is a degree of stability throughout the Paleolithic era, and the Holocene in particular. This, along with the findings in Evolutionary psychology and Neuroscience above, suggests that our cognitive heuristics are at their best when operating in a social, political and economic environment most like that of the Paleolithic/Holocene. If this is true, then one possible means to achieve at least some cognitive bias mitigation is to mimic, as much as possible, Paleolithic/Holocene social, political and economic scenarios when one is performing a reasoning task that could attract negative cognitive bias effects.\n\nA number of paradigms, methods and tools for improving human performance reliability have been developed within the discipline of human reliability engineering. Although there is some attention paid to the human reasoning mechanism itself, the dominant approach is to anticipate problematic situations, constrain human operations through process mandates, and guide human decisions through fixed response protocols specific to the domain involved. While this approach can produce effective responses to critical situations under stress, the protocols involved must be viewed as having limited generalizability beyond the domain for which they were developed, with the implication that solutions in this discipline may provide only generic frameworks to a theory and practice of cognitive bias mitigation.\n\nMachine learning, a branch of artificial intelligence, has been used to investigate human learning and decision making.\n\nOne technique particularly applicable to cognitive bias mitigation is neural network learning and choice selection, an approach inspired by the imagined structure and function of actual biological neural networks in the human brain. The multilayer, cross-connected signal collection and propagation structure typical of neural network models, where weights govern the contribution of signals to each connection, allow very small models to perform rather complex decision-making tasks at high fidelity.\n\nIn principle, such models are capable of modeling decision making that takes account of human needs and motivations within social contexts, and suggest their consideration in a theory and practice of cognitive bias mitigation. Challenges to realizing this potential: accumulating the considerable amount of appropriate real world 'training sets' for the neural network portion of such models; characterizing real-life decision-making situations and outcomes so as to drive models effectively; and the lack of direct mapping from a neural network's internal structure to components of the human reasoning mechanism.\n\nThis discipline, though not focused on improving human reasoning outcomes as an end goal, is one in which the need for such improvement has been explicitly recognized, though the term \"cognitive bias mitigation\" is not universally used.\n\nOne study identifies specific steps to counter the effects of confirmation bias in certain phases of the software engineering lifecycle.\n\nAnother study takes a step back from focussing on cognitive biases and describes a framework for identifying \"Performance Norms\", criteria by which reasoning outcomes are judged correct or incorrect, so as to determine when cognitive bias mitigation is required, to guide identification of the biases that may be 'in play' in a real-world situation, and subsequently to prescribe their mitigations. This study refers to a broad research program with the goal of moving toward a theory and practice of cognitive bias mitigation.\n\nOther initiatives aimed directly at a theory and practice of cognitive bias mitigation may exist within other disciplines under different labels than employed here.\n\n\n"}
{"id": "46504539", "url": "https://en.wikipedia.org/wiki?curid=46504539", "title": "Corporate accountability for human rights violations", "text": "Corporate accountability for human rights violations\n\nHolding corporations accountable for either direct conduct or complicity for human rights violations has become an increasing area of attention in promoting human rights. Multinational corporations in particular have been singled out as important figures, for better or worse, in the maintenance of human rights given their economic status and international dimension. As it currently stands there is no mechanism at the international level which can hold corporations legally accountable. Reliance has instead been placed upon a number of soft law instruments the most important one of which is the United Nations Guiding Principles on Business and Human Rights. With the potential exception for redress under the Alien Tort Statute corporations are only legally accountable for human rights violations under the municipal law of the Nation in which the violation is alleged to have occurred or the company is based.\n\nMultinational corporations emerged in the 1990s as one of the most significant challenges to the dominance of states in the social and economic international order. In 2013 37 of the top 100 economies in the world were corporations, with Wal-Mart Stores annual revenue exceeding the GDP of all but the top 27 states in the world. As a result, there have been increasing concerns over the role multinationals have in promoting and protecting international human rights. Under an orthodox view of international law the obligation to protect human rights lies with the nation state. The power of multinationals has been recognized as diminishing the ability of weaker states to enforce human rights. Furthermore, concerns have been raised over the operation of multinationals in countries which have a poor human rights record. Numerous cases such as Doe v. Unocal Corp. alleging gross human rights violations carried out by multinationals or their associates in countries such as Burma (Myanmar) exist. Whilst concern has been raised over the role multinationals have played in human rights violations, recognition has also been given to the fact that multinationals have the potential to promote human rights in states with a poor track record beyond the ability of other nation states.\n\nUnder the International human rights framework corporations can only be held directly accountable for human rights violations which constitute international crimes as provided for in the Rome Statute of the International Criminal Court. Under the orthodox view States are the primary “subjects” of international law, and whilst International Organizations hold international legal personality, individuals and corporations hold limited standing There has been extensive debate on what status multinational corporations should hold at international law. Those arguing that multinationals should hold rights and duties do so on the ground that this is necessary given their size and influence. Those who argue against the imposition of international law upon multinationals contend that to do so would mean conveying rights which would inevitably enhance the influence and power they hold. Recent developments have seen the articulation of the human rights obligations of corporations but no enforceable regulatory mechanism exists at the international level.\n\nThe United Nations Guiding Principles on Business and Human Rights stand as the authoritative statement on the human rights standards corporations should adhere to. The Principles consist of a three pillar framework: protect, respect and remedy. The Principles do not create new law or impose obligations on corporations, instead they provide what should constitute best practice. The protect pillar retains the orthodox position that is the responsibility of states to protect human rights. The respect pillar holds that corporations should respect human rights, undertaking due diligence to ensure that their conduct does not infringe upon any rights. The remedy pillar calls for remedies both judicial and non-judicial to be provided for those whose human rights have been infringed by a corporation. The response to the Principles has been mixed. The OECD, European Commission, and the United Nations Global Compact have all endorsed the Principles as providing clarity and a point of reference for future developments. Of most significance the United Nations Human Rights Council formally endorsed the Principles shortly after their publication. The Principles not been met with universal favour. The International Federation for Human Rights, which represents numerous human rights groups, criticized the Principles for failing to provide an effective remedy for victims on an international level and failing to recognize the responsibility of states for the actions of their companies abroad. Although currently non-binding there has been speculation by scholars that the Principles could provide a platform for a binding instrument or solidify into customary international law therefore creating legally binding obligations.\n\nThe \"OECD Guidelines for Multinational Enterprises\" is a soft-law instrument containing information on the standards of conduct Multinational corporations should aim to meet. “They provide non-binding principles and standards for responsible business conduct in a global context consistent with applicable laws and internationally recognised standards.” Section IV of the updated 2011 version of the Guidelines deals directly with human rights and reiterates much of the content present in the United Nations Guiding Principles on Business and Human Rights. As such the Guidelines directly endorse the UN Guiding Principles. Section IV provides:\n\"States have the duty to protect human rights. Enterprises should, within the framework of internationally recognised human rights, the international human rights obligations of the countries in which they operate as well as relevant domestic laws and regulations:\nThe above six points mirror the three pillars of the UN Guiding Principles; State duty to protect, corporate responsibility to respect, and access to remedy. The Guidelines require participating states to establish National Contact Points (NCP). Allegations from individuals can be brought forward to the NCP. These are referred to as specific instances. The merits of the claim are then addressed by the NCP and if a case is found then an investigation will be undertaken. Mediation may take place between the company in question and the complainant as one of the final stages in the process. If a company is found to be in breach of the guidelines then a Final Statement will be produced outlining the violations of the Guidelines which occurred along with an impact assessment. Although remedies for any breaches of human rights are sought they cannot be enforced under the Guidelines. Amnesty International has commented that the Guidelines provide a \"lens for examining business conduct with regard to human rights.\" In this sense it has been argued that the OECD Guidelines have the potential to promote a degree of accountability. Amnesty International in its Briefing for the UK National Contact Point advised on the potential repercussions of a Final Statement for companies found to be non-compliant. Reputation, ability to operate, and value of shares may all be detrimentally affected by a finding of non-compliance. Amnesty believes that this may act as a deterrent, leveling the playing field for those companies who ensure they adhere to human rights standards. The rationale behind this being that the cost-benefit relationship is altered in favour of adherence to human rights.\n\nMany companies have taken it upon themselves to adopt their own codes of conduct which are self-imposed ethical standards. Most contain provisions on human rights. The Business and Human Rights Resource Centre maintains a list of companies which have a human rights policy in place. Whilst codes promote accountability, critics have argued that given their voluntary nature, and generally speaking lack of enforcement mechanisms, they are of limited value.\n\nSeveral organisations and commentators have recognised that many victims of human rights violations at the hands of multinational corporations will not have a means of redress in the country the violation has occurred. As such victims often rely on the exercise of extraterritorial jurisdiction. In Europe jurisdiction is granted to hear civil claims against via Resolution 44/2001 of the European Council but this requires that the corporation in question be domiciled in a Member State. The Alien Tort Statute is unique in that it in principle provides Federal courts in the United States of America with near universal jurisdiction to hear claims alleging serious violation of customary international law or the \"law of nations\", the only nexus required being that the defendant be present in the United States. However, the application of the statute to claims made against multinational corporations for human rights violations occurring abroad remains uncertain. Corporate liability for extraterritorial harms has been allowed by the Seventh, Ninth, Eleventh and D.C. Circuits but an increasing number of procedural limitations restricting access of foreign plaintiffs to redress have emerged. The recent decision of the Supreme Court in Kiobel v. Royal Dutch Petroleum Co. ruled that there is a presumption against the extraterritorial application of the Alien Tort Statue. The Business and Human Rights Resource Centre cites Kiobel as a turning point in the use of the Alien Tort Statue as a means for redress for human rights violations at the hands of corporations, and part of a wider trend globally in which avenues for extraterritorial claims are closing.\n\n\n"}
{"id": "15690807", "url": "https://en.wikipedia.org/wiki?curid=15690807", "title": "Count data", "text": "Count data\n\nIn statistics, count data is a statistical data type, a type of data in which the observations can take only the non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking. The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important.\n\nStatistical analyses involving count data includes simple counts, such as the number of occurrences of thunderstorms in a calendar year, and categorical data in which the counts represent the numbers of items falling into each of several categories.\n\nAn individual piece of count data is often termed a count variable. When such a variable is treated as a random variable, the Poisson, binomial and negative binomial distributions are commonly used to represent its distribution.\n\nGraphical examination of count data may be aided by the use of data transformations chosen to have the property of stabilising the sample variance. In particular, the square root transformation might be used when data can be approximated by a Poisson distribution (although other transformation have modestly improved properties), while an inverse sine transformation is available when a binomial distribution is preferred.\n\nHere the count variable would be treated as a dependent variable. Statistical methods such as least squares and analysis of variance are designed to deal with continuous dependent variables. These can be adapted to deal with count data by using data transformations such as the square root transformation, but such methods have several drawbacks; they are approximate at best and estimate parameters that are often hard to interpret.\n\nThe Poisson distribution can form the basis for some analyses of count data and in this case Poisson regression may be used. This is a special case of the class of generalized linear models which also contains specific forms of model capable of using the binomial distribution (binomial regression, logistic regression) or the negative binomial distribution where the assumptions of the Poisson model are violated, in particular when the range of count values is limited or when overdispersion is present.\n\n\n"}
{"id": "1481119", "url": "https://en.wikipedia.org/wiki?curid=1481119", "title": "Cournot competition", "text": "Cournot competition\n\nCournot competition is an economic model used to describe an industry structure in which companies compete on the amount of output they will produce, which they decide on independently of each other and at the same time. It is named after Antoine Augustin Cournot (1801–1877) who was inspired by observing competition in a spring water duopoly. It has the following features:\n\nAn essential assumption of this model is the \"not conjecture\" that each firm aims to maximize profits, based on the expectation that its own output decision will not have an effect on the decisions of its rivals.\nPrice is a commonly known decreasing function of total output. All firms know formula_1, the total number of firms in the market, and take the output of the others as given. Each firm has a cost function formula_2. Normally the cost functions are treated as common knowledge. The cost functions may be the same or different among firms. The market price is set at a level such that demand equals the total quantity produced by all firms.\nEach firm takes the quantity set by its competitors as a given, evaluates its residual demand, and then behaves as a monopoly.\n\nAntoine Augustin Cournot (1801-1877) first outlined his theory of competition in his 1838 volume \"Recherches sur les Principes Mathematiques de la Theorie des Richesses\" as a way of describing the competition with a market for spring water dominated by two suppliers (a duopoly). The model was one of a number that Cournot set out \"explicitly and with mathematical precision\" in the volume. Specifically, Cournot constructed profit functions for each firm, and then used partial differentiation to construct a function representing a firm's best response for given (exogenous) output levels of the other firm(s) in the market. He then showed that a stable equilibrium occurs where these functions intersect (i.e. the simultaneous solution of the best response functions of each firm).\n\nThe consequence of this is that in equilibrium, each firm's expectations of how other firms will act are shown to be correct; when all is revealed, no firm wants to change its output decision. This idea of stability was later taken up and built upon as a description of Nash equilibria, of which Cournot equilibria are a subset.\n\nThis section presents an analysis of the model with 2 firms and constant marginal cost.\n\nEquilibrium prices will be:\n\nThis implies that firm 1’s profit is given by formula_9\n\nIn very general terms, let the price function for the (duopoly) industry be formula_36 and firm formula_37 have the cost structure formula_38. To calculate the Nash equilibrium, the best response functions of the firms must first be calculated.\n\nThe profit of firm i is revenue minus cost. Revenue is the product of price and quantity and cost is given by the firm's cost function, so profit is (as described above):\nformula_39. The best response is to find the value of formula_40 that maximises formula_41 given formula_42, with formula_43, i.e. given some output of the opponent firm, the output that maximises profit is found. Hence, the maximum of formula_41 with respect to formula_40 is to be found. First take the derivative of formula_41 with respect to formula_40:\n\nSetting this to zero for maximization:\n\nThe values of formula_40 that satisfy this equation are the best responses. The Nash equilibria are where both formula_5 and formula_6 are best responses given those values of formula_5 and formula_6.\n\nSuppose the industry has the following price structure: formula_55 The profit of firm formula_37 (with cost structure formula_38 such that formula_58 and formula_59 for ease of computation) is:\n\nThe maximization problem resolves to (from the general case):\n\nWithout loss of generality, consider firm 1's problem:\n\nBy symmetry:\n\nThese are the firms' best response functions. For any value of formula_6, firm 1 responds best with any value of formula_5 that satisfies the above. In Nash equilibria, both firms will be playing best responses so solving the above equations simultaneously. Substituting for formula_6 in firm 1's best response:\n\nThe symmetric Nash equilibrium is at formula_72. (See Holt (2005, Chapter 13) for asymmetric examples.) Making suitable assumptions for the partial derivatives (for example, assuming each firm's cost is a linear function of quantity and thus using the slope of that function in the calculation), the equilibrium quantities can be substituted in the assumed industry price structure formula_55 to obtain the equilibrium market price.\n\nFor an arbitrary number of firms, formula_74, the quantities and price can be derived in a manner analogous to that given above. With linear demand and identical, constant marginal cost the equilibrium values are as follows:\n\nMarket demand; formula_75\n\nCost function; formula_76, for all i\n\nwhich is each individual firm's output\n\nwhich is total industry output\n\nwhich is the market clearing price, and\n\nThe Cournot Theorem then states that, in absence of fixed costs of production, as the number of firms in the market, \"N\", goes to infinity, market output, \"Nq\", goes to the competitive level and the price converges to marginal cost.\n\nHence with many firms a Cournot market approximates a perfectly competitive market. This result can be generalized to the case of firms with different cost structures (under appropriate restrictions) and non-linear demand.\n\nWhen the market is characterized by fixed costs of production, however, we can endogenize the number of competitors imagining that firms enter in the market until their profits are zero. In our linear example with formula_1 firms, when fixed costs for each firm are formula_83, we have the endogenous number of firms:\n\nand a production for each firm equal to:\n\nThis equilibrium is usually known as Cournot equilibrium with endogenous entry, or Marshall equilibrium.\n\n\nAlthough both models have similar assumptions, they have very different implications:\nHowever, as the number of firms increases towards infinity, the Cournot model gives the same result as in Bertrand model: The market price is pushed to marginal cost level.\n\n\n"}
{"id": "6625288", "url": "https://en.wikipedia.org/wiki?curid=6625288", "title": "Cradle-to-cradle design", "text": "Cradle-to-cradle design\n\nCradle-to-cradle design (also referred to as Cradle to Cradle, C2C, cradle 2 cradle, or regenerative design) is a biomimetic approach to the design of products and systems that models human industry on nature's processes viewing materials as nutrients circulating in healthy, safe metabolisms. The term itself is a play on the popular corporate phrase \"Cradle to Grave,\" implying that the C2C model is sustainable and considerate of life and future generations (i.e. from the birth, or \"cradle,\" of one generation to the next versus from birth to death, or \"grave,\" within the same generation.)\n\nC2C suggests that industry must protect and enrich ecosystems and nature's biological metabolism while also maintaining a safe, productive technical metabolism for the high-quality use and circulation of organic and technical nutrients. It is a holistic economic, industrial and social framework that seeks to create systems that are not only efficient but also essentially waste free. The model in its broadest sense is not limited to industrial design and manufacturing; it can be applied to many aspects of human civilization such as urban environments, buildings, economics and social systems.\n\nThe term Cradle to Cradle is a registered trademark of McDonough Braungart Design Chemistry (MBDC) consultants. Cradle to Cradle product certification began as a proprietary system; however, in 2012 MBDC turned the certification over to an independent non-profit called the Cradle to Cradle Products Innovation Institute. Independence, openness, and transparency are the Institute's first objectives for the certification protocols. The phrase \"cradle to cradle\" itself was coined by Walter R. Stahel in the 1970s. The current model is based on a system of \"lifecycle development\" initiated by Michael Braungart and colleagues at the \"Environmental Protection Encouragement Agency\" (EPEA) in the 1990s and explored through the publication \"A Technical Framework for Life-Cycle Assessment\".\n\nIn 2002, Braungart and William McDonough published a book called \"\", a manifesto for cradle to cradle design that gives specific details of how to achieve the model. The model has been implemented by a number of companies, organizations and governments around the world, predominantly in the European Union, China and the United States. Cradle to cradle has also been the subject of many documentary films, including the critically acclaimed \"Waste=Food\".\nIn the cradle to cradle model, all materials used in industrial or commercial processes—such as metals, fibers, dyes—fall into one of two categories: \"technical\" or \"biological\" nutrients. \"Technical nutrients\" are strictly limited to non-toxic, non-harmful synthetic materials that have no negative effects on the natural environment; they can be used in continuous cycles as the same product without losing their integrity or quality. In this manner these materials can be used over and over again instead of being \"downcycled\" into lesser products, ultimately becoming waste.\n\n\"Biological Nutrients\" are organic materials that, once used, can be disposed of in any natural environment and decompose into the soil, providing food for small life forms without affecting the natural environment. This is dependent on the ecology of the region; for example, organic material from one country or landmass may be harmful to the ecology of another country or landmass.\n\nThe two types of materials each follow their own cycle in the regenerative economy envisioned by Keunen and Huizing.\n\nInitially defined by McDonough and Braungart, the Cradle to Cradle Products Innovation Institute's five certification criteria are:\nThe certification is available at several levels: basic, silver, gold, platinum, with more stringent requirements at each. Prior to 2012, MBDC controlled the certification protocol.\n\nCurrently, many human beings come into contact or consume, directly or indirectly, many harmful materials and chemicals daily. In addition, countless other forms of plant and animal life are also exposed. C2C seeks to remove dangerous \"technical nutrients\" (synthetic materials such as mutagenic materials, heavy metals and other dangerous chemicals) from current life cycles. If the materials we come into contact with and are exposed to on a daily basis are not toxic and do not have long term health effects, then the health of the overall system can be better maintained. For example, a fabric factory can eliminate all harmful \"technical nutrients\" by carefully reconsidering what chemicals they use in their dyes to achieve the colours they need and attempt to do so with fewer base chemicals.\n\nThe use of a C2C model often lowers the financial cost of systems. For example, in the redesign of the Ford River Rouge Complex, the planting of Sedum (stonecrop) vegetation on assembly plant roofs retains and cleanses rain water. It also moderates the internal temperature of the building in order to save energy. The roof is part of an $18 million rainwater treatment system designed to clean of rainwater annually. This saved Ford $50 million that would otherwise have been spent on mechanical treatment facilities. If products are designed according to C2C design principles, they can be manufactured and sold for less than alternative designs. They eliminate the need for waste disposal such as landfills.\n\n\nThe question of how to deal with the countless existing \"technical nutrients\" (synthetic materials) that cannot be recycled or reintroduced to the natural environment is dealt with in C2C design. The materials that can be reused and retain their quality can be used within the technical nutrient cycles while other materials are far more difficult to deal with, such as plastics in the Pacific Ocean.\n\nOne effective example is a shoe that is designed and mass-produced using the C2C model. The sole might be made of \"biological nutrients\" while the upper parts might be made of \"technical nutrients\". The shoe is mass-produced at a manufacturing plant that utilises its waste material by putting it back into the cycle; an example of this is using off-cuts from the rubber soles to make more soles instead of merely disposing of them (this is dependent on the technical materials not losing their quality as they are reused). Once the shoes have been manufactured, they are distributed to retail outlets where the customer buys the shoe at a fraction of the price they would normally pay for a shoe of comparable aspects; the customer is only paying for the use of the materials in the shoe for the period of time that they will be using the shoe. When they outgrow the shoe or it is damaged, they return it to the manufacturer. When the manufacturer separates the sole from the upper parts (separating the technical and biological nutrients), the biological nutrients are returned to the natural environment while the technical nutrients are used to create the sole of another shoe.\n\nAnother example of C2C design is a disposable cup, bottle, or wrapper made entirely out of biological materials. When the user is finished with the item, it can be disposed of and returned to the natural environment; the cost of disposal of waste such as landfill and recycling is eliminated. The user could also potentially return the item for a refund so it can be used again.\n\nFord Model U is a design concept of a car, made completely from cradle-to-cradle materials. It also uses hydrogen propulsion.\n\n\nThe C2C model can be applied to almost any system in modern society: urban environments, buildings, manufacturing, social systems. 5 steps are outlined in \"Cradle to Cradle – Remaking the way we make things\":\n\nProducts that adhere to all steps can generally be granted a certification. Two certifications used for cradle-to-cradle products include Leadership in Energy and Environmental Design (LEED) and BRE Environmental Assessment Method (BREEAM).\n\nC2C principles were first applied to systems in the early 1990s by Braungart's Hamburger Umweltinstitut (HUI) and The Environmental Institute in Brazil for biomass nutrient recycling of effluent to produce agricultural products and clean water as a byproduct.\n\nIn 2005, William McDonough helped found the Center for Eco-Intelligent Management at Instituto de Empresa Business School. The center's research produced the Biosphere Rules, a set of five implementation principles that facilitate the adoption of closed loop production approaches with a minimum of disruption for established companies.\n\nIn 2007, MBDC and the EPEA formed a strategic partnership with global materials consultancy Material ConneXion to help promote and disseminate C2C design principles by providing greater global access to C2C material information, certification and product development.\n\nAs of January 2008, Material ConneXion's Materials Libraries in New York, Milan, Cologne, Bangkok and Daegu, Korea started to feature C2C assessed and certified materials and, in collaboration with MBDC and EPEA, the company now offers C2C Certification, and C2C product development.\n\nWhile the C2C model has influenced the construction or redevelopment of many smaller buildings, several large companies, organisations and governments have also implemented the C2C model and its ideas and concepts:\n\n\nThe Cradle to Cradle model can be viewed as a framework that considers systems as a whole or holistically. It can be applied to many aspects of human society, and is related to Life cycle assessment. See for instance the LCA based model of the Eco-costs, which has been designed to cope with analyses of recycle systems. The Cradle to Cradle model in some implementations is closely linked with the Car-free movement, such as in the case of large-scale building projects or the construction or redevelopment of urban environments. It is closely linked with passive solar design in the building industry and with permaculture in agriculture within or near urban environments. An earthship is a perfect example where different re-use models are used, cradle to cradle and permaculture.\n\nIn 2005, IE Business School in Madrid launched the Center for Eco-Intelligent Innovation in collaboration with William McDonough to study the implementation of Cradle to Cradle design approaches in pioneering businesses. The academic research of companies lead to the elaboration of the Biosphere Rules, a set of five principles derived from nature that guide the implementation of circular models in production. \n\nA major constraint in the optimal recycling of materials is that at civic amenity sites, products are not disassembled by hand and have each individual part sorted into a bin, but instead have the entire product sorted into a certain bin.\n\nThis makes the extraction of rare earth elements and other materials uneconomical (at recycling sites, products typically get crushed after which the materials are extracted by means of magnets, chemicals, special sorting methods, ...) and thus optimal recycling of, for example metals is impossible (an optimal recycling method for metals would require to sort all similar alloys together rather than mixing plain iron with alloys).\n\nObviously, disassembling products is not feasible at currently designed civic amenity sites, and a better method would be to send back the broken products to the manufacturer, so that the manufacturer can disassemble the product. These disassembled product can then be used for making new products or at least to have the components sent separately to recycling sites (for proper recycling, by the exact type of material). At present though, few laws are put in place in any country to oblige manufacturers to take back their products for disassembly, nor are there even such obligations for manufacturers of cradle-to-cradle products. One process where this is happening is in the EU with the Waste Electrical and Electronic Equipment Directive.\n\nCriticism has been advanced on the fact that McDonough and Braungart previously kept C2C consultancy and certification in their inner circle. Critics argued that this lack of competition prevented the model from fulfilling its potential. Many critics pleaded for a public-private partnership overseeing the C2C concept, thus enabling competition and growth of practical applications and services.\n\nMcDonough and Braungart responded to this criticism by giving control of the certification protocol to a non-profit, independent Institute called the Cradle to Cradle Products Innovation Institute. McDonough said the new institute \"will enable our protocol to become a public certification program and global standard.\" The new Institute announced the creation of a Certification Standards Board in June 2012. The new board, under the auspices of the Institute, will oversee the certification moving forward.\n\nExperts in the field of environment protection have questioned the practicability of the concept. Friedrich Schmidt-Bleek, head of the German Wuppertal Institute called his assertion, that the \"old\" environmental movement had hindered innovation with its pessimist approach \"pseudo-psychological humbug\".\n\"I can feel very nice on Michael's seat covers in the airplane. Nevertheless I am still waiting for a detailed proposal for a design of the other 99.99 percent of the Airbus 380 after his principles.\"\nIn 2009 Schmidt-Bleek stated that it is out of the question that the concept can be realized on a bigger scale.\n\nSome claim that C2C certification may not be entirely sufficient in all eco-design approaches. Quantitative methodologies (LCAs) and more adapted tools (regarding the product type which is considered) could be used in tandem. The C2C concept ignores the use phase of a product. According to the Variants of Life Cycle Assessment the entire life cycle of a product or service has to be evaluated, not only the material itself. For many goods e.g. in transport, the use phase has the most influence on the environmental footprint. E.g. the more lightweight a car or a plane the less fuel it consumes and consequently the less impact it has. Braungart fully ignores the use phase.\n\nIt is safe to say that every production step or resource-transformation step needs a certain amount of energy.\n\nThe C2C concept foresees an own certification of its analysis and therefore is in contradiction to international ISO standards 14040 and 14044 for Life Cycle Assessment whereas an independent and critical review is needed in order to obtain comparative and resilient results. Independent external review.\n\n"}
{"id": "2045310", "url": "https://en.wikipedia.org/wiki?curid=2045310", "title": "Cursive script (East Asia)", "text": "Cursive script (East Asia)\n\nCursive script (), often mistranslated as grass script, is a script style used in Chinese and East Asian calligraphy. Cursive script is faster to write than other styles, but difficult to read for those unfamiliar with it. It functions primarily as a kind of shorthand script or calligraphic style. People who can read standard or printed forms of Chinese may not be able to comprehend this script.\nThe character 書 (shū) means script in this context, and the character 草 (cǎo) means quick, rough or sloppy. Thus, the name of this script is literally \"rough script\" or \"sloppy script\". The same character 草 (cǎo) appears in this sense in the noun \"rough draft\" (草稿, cǎogǎo), and the verb \"to draft [a document or plan]\" (草擬, cǎonǐ). The other indirectly related meaning of the character 草 (cǎo) is grass, which has led to the mistranslation \"grass script\".\n\nCursive script originated in China during the Han dynasty through the Jin period, in two phases. First, an early form of cursive developed as a cursory way to write the popular and not yet mature clerical script. Faster ways to write characters developed through four mechanisms: omitting part of a graph, merging strokes together, replacing portions with abbreviated forms (such as one stroke to replace four dots), or modifying stroke styles. This evolution can best be seen on extant bamboo and wooden slats from the period, on which the use of early cursive and immature clerical forms is intermingled. This early form of cursive script, based on clerical script, is now called zhāngcǎo (章草), and variously also termed ancient cursive, draft cursive or clerical cursive in English, to differentiate it from modern cursive (今草 jīncǎo). Modern cursive evolved from this older cursive in the Wei Kingdom to Jin dynasty with influence from the semi-cursive and standard styles.\n\nBeside zhāngcǎo and the \"modern cursive\", there is the \"wild cursive\" (, Japanese \"kyōsō\") which is even more cursive and difficult to read. When it was developed by Zhang Xu and Huaisu in the Tang dynasty, they were called \"Dian Zhang Zui Su\" (crazy Zhang and drunk Su, 顛張醉素). Cursive, in this style, is no longer significant in legibility but rather in artistry.\n\nCursive scripts can be divided into the unconnected style (Chinese (S) and Japanese 独草, Chinese (T) 獨草, pinyin \"dúcǎo\", romaji \"dokusō\") where each character is separate, and the connected style (Chinese (S) 连绵, Chinese (T) 連綿, Japanese 連綿体, pinyin \"liánmián\", romaji \"renmentai\") where each character is connected to the succeeding one.\n\nMany of the simplified Chinese characters are modeled on the printed forms of the cursive forms of the corresponding characters ().\n\nCursive script forms of Chinese characters are also the origin of the Japanese hiragana script. Specifically, the hiragana characters developed from cursive forms of the man'yōgana script, called \"sōgana\" (草仮名). In Japan, the \"sōgana\" cursive script was considered to be suitable for women's writing, and thus came to be referred to as . This term was later applied to hiragana, as well. In contrast, kanji themselves were referred to as .\n\n\n\n"}
{"id": "5731493", "url": "https://en.wikipedia.org/wiki?curid=5731493", "title": "Domestic Violence, Crime and Victims Act 2004", "text": "Domestic Violence, Crime and Victims Act 2004\n\nThe Domestic Violence, Crime and Victims Act 2004 (c 28) is an Act of the Parliament of the United Kingdom. It is concerned with criminal justice and concentrates upon legal protection and assistance to victims of crime, particularly domestic violence. It also expands the provision for trials without a jury, brings in new rules for trials for causing the death of a child or vulnerable adult, and permits bailiffs to use force to enter homes.\n\nThe Act's provisions originated in several reports:\n\n\nNon-molestation orders under the Family Law Act 1996 were amended to provide a criminal sanction for non-compliance, with a maximum sentence of 5 years' imprisonment. The circumstances in which such orders could be imposed was extended to include same-sex couples and cohabiting couples on an equal footing with married couples. Former cohabitants are also included.\n\nRestraining orders (preventing the recipient from doing anything specified in the order) can be imposed upon acquitted defendants. They are imposed if the court \"considers it necessary to do so to protect a person from harassment by the defendant\". The Court of Appeal in allowing an appeal against conviction may also remit the matter to the Crown Court to consider a restraining order in respect of the otherwise successful appellant.\n\nThe Act deemed common assault an arrestable offence. The practical effect of this change was that the police could arrest a suspect at the scene without a warrant, rather than potentially be compelled to leave the suspected assailant with his or her alleged victim. Previously the police would have to allege assault occasioning actual bodily harm, which was arrestable, in order to detain the suspected assailant in borderline cases.\n\nHowever, the concept of \"arrestable offence\" was abolished on 1 January 2006. , police can effect an arrest, even in the case of suspected common assault, in order \"to prevent the person in question causing physical injury to himself or any other person.\"\n\nThe Act specified common assault as an alternative verdict to a count on an aggravated assault in the Crown Court, though it is not itself an indictable offence.\n\nJudges, not a specially empanelled jury, now decide if a defendant is fit to plead.\". The regime for dealing with defendants who are unfit to plead or not guilty by reason of insanity (that is, committed the physical acts constituting the offence but without the sane intent) has also been modified. The court, not the Home Secretary, makes the assessment (requiring medical evidence to do so) whether the defendant should be committed to a psychiatric hospital.\n\nTrials with a substantial number of charges can now be split into two phases: trial by jury of \"specimen counts\" and judge-only trial of the remaining counts. This further expands the circumstances in which trials can be heard without a jury (see the Criminal Justice Act 2003).\n\nThe prosecution must satisfy the court that three conditions are met:\n\nThe judge should take into account any ways that jury trial can be made easier, but no such measure should result in a trial where the defendant faces a lesser sentence than would be available with the new measures.\n\nAn intractable legal problem had arisen in relation to cases where a child or vulnerable adult cared for by two people dies as a result of ill-treatment. It is known that at least one of two people is responsible, but not which. This problem had been analysed in a number of cases. The Court of Appeal in \"Lane v Lane\" held that neither person could be convicted, nor the trial proceed past the end of the prosecution case, because there was no evidence specifically pointing to a certain defendant.\nLord Goddard earlier commented in \"Regina v Abbott\" \"Probably one or other must have committed it, but there was not evidence which, and although it is unfortunate that a guilty party cannot be brought to\njustice, it is far more important that there should not be a miscarriage of justice and that the law maintained that the prosecution should prove its case.\"\nThe Law Commission's report commented that this meant one or other parent were potentially \"getting away with murder\".\n\nThe Act deals with the problem in two ways: firstly by creating an offence of \"causing or allowing the death of a child or vulnerable adult\", and secondly by amending the rules of court procedure to require joint defendants to give their account of events in the witness box, effectively forcing them to incriminate the other if appropriate.\n\nThe offence of \"causing or allowing the death of a child or vulnerable adult\", now referred to as the \"new offence\", is committed under section 5 of the Act if the following four conditions apply:\n\nTherefore if it can be established that a child or vulnerable adult died as a result of an unlawful act, it need not be proved which of the two responsible members of the household either caused the death or allowed it to happen.\n\nIf there was no obvious history of violence, or any reason to suspect it, then the other members of the household would not be guilty of this offence, even in clear cases of homicide. Where there is no reason to suspect the victim is at risk, other members of the household cannot reasonably be expected to have taken steps to prevent the abuse.\n\nCourt procedure is amended to restrict the circumstances in which the trial can be stopped at the end of the prosecution case and before the defence case.\n\nThe ambit of the \"adverse inference\" (right of the jury to make assumptions about any part of the case, including the guilt of the defendant, based upon his or her failure to answer any question put in court) is extended to include an inference on a joint charge of homicide (murder and manslaughter) and the new offence; this means that if a person is charged with either (or both) homicide offences and this new offence, then silence in the witness box can imply guilt of homicide as well as the new offence. This is subject to the usual safeguard that a person cannot be convicted solely upon the basis of their silence.\n\nThe point at which a \"no case to answer\" submission (see definition) can be made has in certain circumstances been moved to the end of the whole case, not just the prosecution. Joint charges of homicide and the new offence can only be dismissed at the end of the whole case (if the new offence has survived past that stage as well). \n\nThe new offence will survive the \"no case to answer\" test as long as the fundamentals of the offence are demonstrated - the prosecution do not have to show whether the defendant caused or allowed the death to happen. The defendant will be under pressure to give evidence about what occurred - not to do so would result in the adverse inference being drawn.\n\nA number of issues have been pointed out by legal scholars with the current drafting. David Ormerod, writing in the criminal law textbook \"Smith and Hogan\", notes that the Act deliberately does not define what counts as a \"household\". Additionally, the Act does not adequately cover some classes of carers who do not live in a household residence but have regular contact—domestic nannies, for instance. The law also leaves unclear whether one carer is legally responsible for not taking steps to protect a vulnerable victim from the risky behaviour of another of his or her carers.\n\nThe Act permits bailiffs to use force to enter homes, overturning a centuries-old doctrine, confirmed by \"Semayne's case\" (1604), that \"an Englishman's home is his castle\". This had been described in the eighteenth century by William Blackstone, who wrote in Book 4, Chapter 16 of his \"Commentaries on the Laws of England\":\nIn 2009 charities providing advice to debtors said they were seeing bailiffs threatening to break in unless the debtor paid the full fine immediately, as well court and bailiff costs. Previously, charities had been able to advise debtors that bailiffs did not have the right to force entry, and the fine could be referred back to the courts and affordable payment schedules worked out.\n\nThe following orders have been made under this section:\n\n\n"}
{"id": "44785", "url": "https://en.wikipedia.org/wiki?curid=44785", "title": "Dream", "text": "Dream\n\nA dream is a succession of images, ideas, emotions, and sensations that usually occur involuntarily in the mind during certain stages of sleep. The content and purpose of dreams are not fully understood, although they have been a topic of scientific, philosophical and religious interest throughout recorded history. Dream interpretation is the attempt at drawing meaning from dreams and searching for an underlying message. The scientific study of dreams is called oneirology.\n\nDreams mainly occur in the rapid-eye movement (REM) stage of sleep—when brain activity is high and resembles that of being awake. REM sleep is revealed by continuous movements of the eyes during sleep. At times, dreams may occur during other stages of sleep. However, these dreams tend to be much less vivid or memorable. The length of a dream can vary; they may last for a few seconds, or approximately 20–30 minutes. People are more likely to remember the dream if they are awakened during the REM phase. The average person has three to five dreams per night, and some may have up to seven; however, most dreams are immediately or quickly forgotten. Dreams tend to last longer as the night progresses. During a full eight-hour night sleep, most dreams occur in the typical two hours of REM. Dreams related to waking-life experiences are associated with REM theta activity, which suggests that emotional memory processing takes place in REM sleep.\n\nOpinions about the meaning of dreams have varied and shifted through time and culture. Many endorse the Freudian theory of dreams – that dreams reveal insight into hidden desires and emotions. Other prominent theories include those suggesting that dreams assist in memory formation, problem solving, or simply are a product of random brain activation. \n\nSigmund Freud, who developed the psychological discipline of psychoanalysis, wrote extensively about dream theories and their interpretations in the early 1900s. He explained dreams as manifestations of one's deepest desires and anxieties, often relating to repressed childhood memories or obsessions. Furthermore, he believed that virtually every dream topic, regardless of its content, represented the release of sexual tension. In \"The Interpretation of Dreams\" (1899), Freud developed a psychological technique to interpret dreams and devised a series of guidelines to understand the symbols and motifs that appear in our dreams. In modern times, dreams have been seen as a connection to the unconscious mind. They range from normal and ordinary to overly surreal and bizarre. Dreams can have varying natures, such as being frightening, exciting, magical, melancholic, adventurous, or sexual. The events in dreams are generally outside the control of the dreamer, with the exception of lucid dreaming, where the dreamer is self-aware. Dreams can at times make a creative thought occur to the person or give a sense of inspiration.\n\nThe Dreaming is a common term within the animist creation narrative of indigenous Australians for a personal, or group, creation and for what may be understood as the \"timeless time\" of formative creation and perpetual creating.\n\nThe ancient Sumerians in Mesopotamia have left evidence of dream interpretation dating back to at least 3100 BC. Throughout Mesopotamian history, dreams were always held to be extremely important for divination and Mesopotamian kings paid close attention to them. Gudea, the king of the Sumerian city-state of Lagash (reigned 2144–2124 BC), rebuilt the temple of Ningirsu as the result of a dream in which he was told to do so. The standard Akkadian \"Epic of Gilgamesh\" contains numerous accounts of the prophetic power of dreams. First, Gilgamesh himself has two dreams foretelling the arrival of Enkidu. Later, Enkidu dreams about the heroes' encounter with the giant Humbaba. Dreams were also sometimes seen as a means of seeing into other worlds and it was thought that the soul, or some part of it, moved out of the body of the sleeping person and actually visited the places and persons the dreamer saw in his or her sleep. In Tablet VII of the epic, Enkidu recounts to Gilgamesh a dream in which he saw the gods Anu, Enlil, and Shamash condemn him to death. He also has a dream in which he visits the Underworld.\n\nThe Assyrian king Ashurnasirpal II (reigned 883–859 BC) built a temple to Mamu, possibly the god of dreams, at Imgur-Enlil, near Kalhu. The later Assyrian king Ashurbanipal (reigned 668– 627 BC) had a dream during a desperate military situation in which his divine patron, the goddess Ishtar, appeared to him and promised that she would lead him to victory. The Babylonians and Assyrians divided dreams into \"good,\" which were sent by the gods, and \"bad,\" sent by demons. A surviving collection of dream omens entitled \"Iškar Zaqīqu\" records various dream scenarios as well as prognostications of what will happen to the person who experiences each dream, apparently based on previous cases. Some list different possible outcomes, based on occasions in which people experienced similar dreams with different results. Dream scenarios mentioned include a variety of daily work events, journeys to different locations, family matters, sex acts, and encounters with human individuals, animals, and deities.\n\nIn ancient Egypt, as far back as 2000 BC, the Egyptians wrote down their dreams on papyrus. People with vivid and significant dreams were thought blessed and were considered special. Ancient Egyptians believed that dreams were like oracles, bringing messages from the gods. They thought that the best way to receive divine revelation was through dreaming and thus they would induce (or \"incubate\") dreams. Egyptians would go to sanctuaries and sleep on special \"dream beds\" in hope of receiving advice, comfort, or healing from the gods.\n\nIn Chinese history, people wrote of two vital aspects of the soul of which one is freed from the body during slumber to journey in a dream realm, while the other remained in the body, although this belief and dream interpretation had been questioned since early times, such as by the philosopher Wang Chong (27–97 AD). The Indian text \"Upanishads\", written between 900 and 500 BC, emphasize two meanings of dreams. The first says that dreams are merely expressions of inner desires. The second is the belief of the soul leaving the body and being guided until awakened.\n\nThe Greeks shared their beliefs with the Egyptians on how to interpret good and bad dreams, and the idea of incubating dreams. Morpheus, the Greek god of dreams, also sent warnings and prophecies to those who slept at shrines and temples. The earliest Greek beliefs about dreams were that their gods physically visited the dreamers, where they entered through a keyhole, exiting the same way after the divine message was given.\n\nAntiphon wrote the first known Greek book on dreams in the 5th century BC. In that century, other cultures influenced Greeks to develop the belief that souls left the sleeping body. Hippocrates (469–399 BC) had a simple dream theory: during the day, the soul receives images; during the night, it produces images. Greek philosopher Aristotle (384–322 BC) believed dreams caused physiological activity. He thought dreams could analyze illness and predict diseases. Marcus Tullius Cicero, for his part, believed that all dreams are produced by thoughts and conversations a dreamer had during the preceding days. Cicero's \"Somnium Scipionis\" described a lengthy dream vision, which in turn was commented on by Macrobius in his \"Commentarii in Somnium Scipionis\".\n\nHerodotus in his \"The Histories\", writes \"The visions that occur to us in dreams are, more often than not, the things we have been concerned about during the day.\"\nIn Welsh history, The Dream of Rhonabwy (Welsh: Breuddwyd Rhonabwy) is a Middle Welsh prose tale. Set during the reign of Madog ap Maredudd, prince of Powys (died 1160), it is dated to the late 12th or 13th century. It survives in only one manuscript, the Red Book of Hergest, and has been associated with the Mabinogion since its publication by Lady Charlotte Guest in the 19th century. The bulk of the narrative describes a dream vision experienced by its central character, Rhonabwy, a retainer of Madog, in which he visits the time of King Arthur.\n\nAlso in Welsh history, the tale 'The Dream of Macsen Wledig' is a romanticised story about the Roman emperor Magnus Maximus, called Macsen Wledig in Welsh. Born in Hispania, he became a legionary commander in Britain, assembled a Celtic army and assumed the title of Emperor of the Western Roman Empire in 383. He was defeated in battle in 385 and beheaded at the direction of the Eastern Roman emperor.\n\nIn Judaism, dreams are considered part of the experience of the world that can be interpreted and from which lessons can be garnered. It is discussed in the Talmud, Tractate Berachot 55–60.\n\nThe ancient Hebrews connected their dreams heavily with their religion, though the Hebrews were monotheistic and believed that dreams were the voice of one God alone. Hebrews also differentiated between good dreams (from God) and bad dreams (from evil spirits). The Hebrews, like many other ancient cultures, incubated dreams in order to receive divine revelation. For example, the Hebrew prophet Samuel would \"lie down and sleep in the temple at Shiloh before the Ark and receive the word of the Lord.\" Most of the dreams in the Bible are in the Book of Genesis.\n\nChristians mostly shared the beliefs of the Hebrews and thought that dreams were of a supernatural character because the Old Testament includes frequent stories of dreams with divine inspiration. The most famous of these dream stories was Jacob's dream of a ladder that stretches from Earth to Heaven. Many Christians preach that God can speak to people through their dreams.\n\nIain R. Edgar has researched the role of dreams in Islam. He has argued that dreams play an important role in the history of Islam and the lives of Muslims, since dream interpretation is the only way that Muslims can receive revelations from God since the death of the last prophet, Muhammad.\n\nIn the Mandukya Upanishad, part of the Veda scriptures of Indian Hinduism, a dream is one of three states that the soul experiences during its lifetime, the other two states being the waking state and the sleep state.\n\nIn Buddhism, ideas about dreams are similar to the classical and folk traditions in South Asia. The same dream is sometimes experienced by multiple people, as in the case of the Buddha-to-be leaving his home. It is described in the \"Mahāvastu\" that several of the Buddha's relatives had premonition-like dreams preceding this. Some dreams are also seen to transcend time: the Buddha-to-be has certain dreams that are the same as those of previous Buddhas, the \"Lalitavistara\" states. In Buddhist literature, dreams often function as a \"signpost\" motif to mark certain stages in the life of the main character.\n\nBuddhist views about dreams are expressed in the Pāli Commentaries and the Milinda Pañhā.\n\nSome philosophers have concluded that what we think of as the \"real world\" could be or is an illusion (an idea known as the skeptical hypothesis about ontology).\n\nThe first recorded mention of the idea was by Zhuangzi, and it is also discussed in Hinduism, which makes extensive use of the argument in its writings. It was formally introduced to Western philosophy by Descartes in the 17th century in his \"Meditations on First Philosophy\". Stimulus, usually an auditory one, becomes a part of a dream, eventually then awakening the dreamer.\n\nSome Indigenous American tribes and Mexican civilizations believe that dreams are a way of visiting and having contact with their ancestors. Some Native American tribes used vision quests as a rite of passage, fasting and praying until an anticipated guiding dream was received, to be shared with the rest of the tribe upon their return.\n\nThe Middle Ages brought a harsh interpretation of dreams. They were seen as evil, and the images as temptations from the devil. Many believed that during sleep, the devil could fill the human mind with corrupting and harmful thoughts. Martin Luther, founder of Protestantism, believed dreams were the work of the Devil. However, Catholics such as St. Augustine and St. Jerome claimed that the direction of their lives was heavily influenced by their dreams.\n\nThe depiction of dreams in Renaissance and Baroque art is often related to Biblical narrative. Examples are \"Joachim's Dream\" (1304–1306) from the Scrovegni Chapel fresco cycle by Giotto, and \"Jacob's Dream\" (1639) by Jusepe de Ribera. Dreams and dark imaginings are the theme of several notable works of the Romantic era, such as Goya's etching \"The Sleep of Reason Produces Monsters\" (c. 1799) and Henry Fuseli's painting \"The Nightmare\" (1781). Salvador Dalí's \"Dream Caused by the Flight of a Bee around a Pomegranate a Second Before Awakening\" (1944) also investigates this theme through absurd juxtapositions of a nude lady, tigers leaping out of a pomegranate, and a spider-like elephant walking in the background. Henri Rousseau's last painting was \"The Dream\". \"Le Rêve\" (\"The Dream\") is a 1932 painting by Pablo Picasso.\n\nDream frames were frequently used in medieval allegory to justify the narrative; \"The Book of the Duchess\" and \"The Vision Concerning Piers Plowman\" are two such dream visions. Even before them, in antiquity, the same device had been used by Cicero and Lucian of Samosata.\n\nThey have also featured in fantasy and speculative fiction since the 19th century. One of the best-known dream worlds is Wonderland from Lewis Carroll's \"Alice's Adventures in Wonderland\", as well as Looking-Glass Land from its sequel, \"Through the Looking-Glass\". Unlike many dream worlds, Carroll's logic is like that of actual dreams, with transitions and flexible causality.\n\nOther fictional dream worlds include the Dreamlands of H. P. Lovecraft's \"Dream Cycle\" and \"The Neverending Story\"s world of Fantasia, which includes places like the Desert of Lost Dreams, the Sea of Possibilities and the Swamps of Sadness. Dreamworlds, shared hallucinations and other alternate realities feature in a number of works by Philip K. Dick, such as \"The Three Stigmata of Palmer Eldritch\" and \"Ubik\". Similar themes were explored by Jorge Luis Borges, for instance in \"The Circular Ruins\".\n\nModern popular culture often conceives of dreams, like Freud, as expressions of the dreamer's deepest fears and desires. The film version of \"The Wizard of Oz\" (1939) depicts a full-color dream that causes Dorothy to perceive her black-and-white reality and those with whom she shares it in a new way. In films such as \"Spellbound\" (1945), \"The Manchurian Candidate\" (1962), \"Field of Dreams\" (1989), and \"Inception\" (2010), the protagonists must extract vital clues from surreal dreams.\n\nMost dreams in popular culture are, however, not symbolic, but straightforward and realistic depictions of their dreamer's fears and desires. Dream scenes may be indistinguishable from those set in the dreamer's real world, a narrative device that undermines the dreamer's and the audience's sense of security and allows horror film protagonists, such as those of \"Carrie\" (1976), \"Friday the 13th\" (1980) or \"An American Werewolf in London\" (1981) to be suddenly attacked by dark forces while resting in seemingly safe places.\n\nIn speculative fiction, the line between dreams and reality may be blurred even more in the service of the story. Dreams may be psychically invaded or manipulated (\"Dreamscape\", 1984; the \"Nightmare on Elm Street\" films, 1984–2010; \"Inception\", 2010) or even come literally true (as in \"The Lathe of Heaven\", 1971). In Ursula K. Le Guin's book, \"The Lathe of Heaven\" (1971), the protagonist finds that his \"effective\" dreams can retroactively change reality. Peter Weir's 1977 Australian film \"The Last Wave\" makes a simple and straightforward postulate about the premonitory nature of dreams (from one of his Aboriginal characters) that \"... dreams are the shadow of something real\". In Kyell Gold's novel \"Green Fairy\" from the Dangerous Spirits series, the protagonist, Sol, experiences the memories of a dancer who died 100 years before through Absinthe induced dreams and after each dream something from it materializes into his reality. Such stories play to audiences' experiences with their own dreams, which feel as real to them.\n\nIn the late 19th century, psychotherapist Sigmund Freud developed a theory that the content of dreams is driven by unconscious wish fulfillment. Freud called dreams the \"royal road to the unconscious.\" He theorized that the content of dreams reflects the dreamer's unconscious mind and specifically that dream content is shaped by unconscious wish fulfillment. He argued that important unconscious desires often relate to early childhood memories and experiences. Freud's theory describes dreams as having both manifest and latent content. Latent content relates to deep unconscious wishes or fantasies while manifest content is superficial and meaningless. Manifest content often masks or obscures latent content.\n\nIn his early work, Freud argued that the vast majority of latent dream content is sexual in nature, but he later moved away from this categorical position. In \"Beyond the Pleasure Principle\" he considered how trauma or aggression could influence dream content. He also discussed supernatural origins in \"Dreams and Occultism\", a lecture published in \"New Introductory Lectures on Psychoanalysis\".\n\nLate in life Freud acknowledged that \"It is impossible to classify as wish fulfillments\" the repetitive nightmares associated with post-traumatic stress disorder. Modern experimental studies weigh against many of Freud's theories regarding dreams. Freud's \"dream-work\" interpretation strategies have not been found to have empirical validity. His theory that dreams were the \"guardians\" of sleep, repressing and disguising bodily urges to ensure sleep continues, seems unlikely given studies of individuals who can sleep without dreaming. His assertions that repressed memory in infants re-surface decades later in adult dreams conflicts with modern research on memory. Freud's theory has difficulty explaining why young children have static and bland dreams, or why the emotions in most dreams are negative. On the plus side, modern researchers agree with Freud that dreams do have coherence, and that dream content connects to other psychological variables and often connect to recent waking thoughts (though not as often as Freud supposed). Despite the lack of scientific evidence, dream interpretation services based on Freudian or other systems remain popular.\n\nCarl Jung rejected many of Freud's theories. Jung expanded on Freud's idea that dream content relates to the dreamer's unconscious desires. He described dreams as messages to the dreamer and argued that dreamers should pay attention for their own good. He came to believe that dreams present the dreamer with revelations that can uncover and help to resolve emotional or religious problems and fears.\n\nJung wrote that recurring dreams show up repeatedly to demand attention, suggesting that the dreamer is neglecting an issue related to the dream. He called this \"compensation.\" The dream balances the conscious belief and attitudes with an alternative. Jung did not believe that the conscious attitude was wrong and that the dream provided the true belief. He argued that good work with dreams takes both into account and comes up with a balanced viewpoint. He believed that many of the symbols or images from these dreams return with each dream. Jung believed that memories formed throughout the day also play a role in dreaming. These memories leave impressions for the unconscious to deal with when the ego is at rest. The unconscious mind re-enacts these glimpses of the past in the form of a dream. Jung called this a \"day residue\". Jung also argued that dreaming is not a purely individual concern, that all dreams are part of \"one great web of psychological factors.\"\n\nFritz Perls presented his theory of dreams as part of the holistic nature of Gestalt therapy. Dreams are seen as projections of parts of the self that have been ignored, rejected, or suppressed. Jung argued that one could consider every person in the dream to represent an aspect of the dreamer, which he called the subjective approach to dreams. Perls expanded this point of view to say that even inanimate objects in the dream may represent aspects of the dreamer. The dreamer may, therefore, be asked to imagine being an object in the dream and to describe it, in order to bring into awareness the characteristics of the object that correspond with the dreamer's personality.\n\nAccumulated observation has shown that dreams are strongly associated with REM rapid eye movement sleep, during which an electroencephalogram (EEG) shows brain activity that, among sleep states, is most like wakefulness. Participant-remembered dreams during NREM sleep are normally more mundane in comparison. During a typical lifespan, a person spends a total of about six years dreaming (which is about two hours each night). Most dreams only last 5 to 20 minutes. It is unknown where in the brain dreams originate, if there is a single origin for dreams or if multiple portions of the brain are involved, or what the purpose of dreaming is for the body or mind.\n\nDuring REM sleep, the release of the neurotransmitters norepinephrine, serotonin and histamine is completely suppressed.\n\nDuring most dreams, the person dreaming is not aware that they are dreaming, no matter how absurd or eccentric the dream is. The reason for this may be that the prefrontal cortex, the region of the brain responsible for logic and planning, exhibits decreased activity during dreams. This allows the dreamer to more actively interact with the dream without thinking about what might happen, since things that would normally stand out in reality blend in with the dream scenery.\n\nWhen REM sleep episodes were timed for their duration and subjects were awakened to make reports before major editing or forgetting of their dreams could take place, subjects accurately reported the length of time they had been dreaming in an REM sleep state. Some researchers have speculated that \"time dilation\" effects only seem to be taking place upon reflection and do not truly occur within dreams. This close correlation of REM sleep and dream experience was the basis of the first series of reports describing the nature of dreaming: that it is a regular nightly rather than occasional phenomenon, and is correlated with high-frequency activity within each sleep period occurring at predictable intervals of approximately every 60–90 minutes in all humans throughout the lifespan.\n\nREM sleep episodes and the dreams that accompany them lengthen progressively through the night, with the first episode being shortest, of approximately 10–12 minutes duration, and the second and third episodes increasing to 15–20 minutes. Dreams at the end of the night may last as long as 15 minutes, although these may be experienced as several distinct episodes due to momentary arousals interrupting sleep as the night ends. Dream reports can be reported from normal subjects 50% of the time when they are awakened prior to the end of the first REM period. This rate of retrieval is increased to about 99% when awakenings are made from the last REM period of the night. The increase in the ability to recall dreams appears related to intensification across the night in the vividness of dream imagery, colors, and emotions. \n\nREM sleep and the ability to dream seem to be embedded in the biology of many animals in addition to humans. Scientific research suggests that all mammals experience REM. The range of REM can be seen across species: dolphins experience minimal REM, while humans are in the middle of the scale and the armadillo and the opossum (a marsupial) are among the most prolific dreamers, judging from their REM patterns.\n\nStudies have observed signs of dreaming in all mammals studied, including monkeys, dogs, cats, rats, elephants, and shrews. There have also been signs of dreaming in birds and reptiles. Sleeping and dreaming are intertwined. Scientific research results regarding the function of dreaming in animals remain disputable; however, the function of sleeping in living organisms is increasingly clear. For example, sleep deprivation experiments conducted on rats and other animals have resulted in the deterioration of physiological functioning and actual tissue damage.\n\nSome scientists argue that humans dream for the same reason other amniotes do. From a Darwinian perspective dreams would have to fulfill some kind of biological requirement, provide some benefit for natural selection to take place, or at least have no negative impact on fitness. In 2000 Antti Revonsuo, a professor at the University of Turku in Finland, claimed that centuries ago dreams would prepare humans for recognizing and avoiding danger by presenting a simulation of threatening events. The theory has therefore been called the threat-simulation theory. According to Tsoukalas (2012) dreaming is related to the reactive patterns elicited by encounters with predators, a fact that is still evident in the control mechanisms of REM sleep (see below).\n\nIn 1976 J. Allan Hobson and Robert McCarley proposed a new theory that changed dream research, challenging the previously held Freudian view of dreams as unconscious wishes to be interpreted. They assume that the same structures that induce REM sleep also generate sensory information. Hobson's 1976 research suggested that the signals interpreted as dreams originate in the brainstem during REM sleep. According to Hobson and other researchers, circuits in the brainstem are activated during REM sleep. Once these circuits are activated, areas of the limbic system involved in emotions, sensations, and memories, including the amygdala and hippocampus, become active. The brain synthesizes and interprets these activities; for example, changes in the physical environment such as temperature and humidity, or physical stimuli such as ejaculation, and attempts to create meaning from these signals, result in dreaming.\n\nHowever, research by Mark Solms suggests that dreams are generated in the forebrain, and that REM sleep and dreaming are not directly related. While working in the neurosurgery department at hospitals in Johannesburg and London, Solms had access to patients with various brain injuries. He began to question patients about their dreams and confirmed that patients with damage to the parietal lobe stopped dreaming; this finding was in line with Hobson's 1977 theory. However, Solms did not encounter cases of loss of dreaming with patients having brainstem damage. This observation forced him to question Hobson's prevailing theory, which marked the brainstem as the source of the signals interpreted as dreams.\n\nCombining Hobson's activation synthesis hypothesis with Solms' findings, the continual-activation theory of dreaming presented by Jie Zhang proposes that dreaming is a result of brain activation and synthesis; at the same time, dreaming and REM sleep are controlled by different brain mechanisms. Zhang hypothesizes that the function of sleep is to process, encode, and transfer the data from the temporary memory store to the long-term memory store. During NREM sleep the conscious-related memory (declarative memory) is processed, and during REM sleep the unconscious related memory (procedural memory) is processed.\n\nZhang assumes that during REM sleep the unconscious part of a brain is busy processing the procedural memory; meanwhile, the level of activation in the conscious part of the brain descends to a very low level as the inputs from the sensory systems are basically disconnected. This triggers the \"continual-activation\" mechanism to generate a data stream from the memory stores to flow through the conscious part of the brain. Zhang suggests that this pulse-like brain activation is the inducer of each dream. He proposes that, with the involvement of the brain associative thinking system, dreaming is, thereafter, self-maintained with the dreamer's own thinking until the next pulse of memory insertion. This explains why dreams have both characteristics of continuity (within a dream) and sudden changes (between two dreams). A detailed explanation of how a dream is synthesized is given in a later paper.\n\nAccording to Tsoukalas (2012) REM sleep is an evolutionary transformation of a well-known defensive mechanism, the tonic immobility reflex. This reflex, also known as animal hypnosis or death feigning, functions as the last line of defense against an attacking predator and consists of the total immobilization of the animal: the animal appears dead (cf. \"playing possum\"). Tsoukalas claims that the neurophysiology and phenomenology of this reaction shows striking similarities to REM sleep, a fact that suggests a deep evolutionary kinship. For example, both reactions exhibit brainstem control, paralysis, hippocampal theta and thermoregulatory changes. Tsoukalas claims that this theory integrates many earlier findings into a unified framework.\n\nEugen Tarnow suggests that dreams are ever-present excitations of long-term memory, even during waking life. The strangeness of dreams is due to the format of long-term memory, reminiscent of Penfield & Rasmussen's findings that electrical excitations of the cortex give rise to experiences similar to dreams. During waking life an executive function interprets long-term memory consistent with reality checking. Tarnow's theory is a reworking of Freud's theory of dreams in which Freud's unconscious is replaced with the long-term memory system and Freud's \"Dream Work\" describes the structure of long-term memory.\n\nA 2001 study showed evidence that illogical locations, characters, and dream flow may help the brain strengthen the linking and consolidation of semantic memories. These conditions may occur because, during REM sleep, the flow of information between the hippocampus and neocortex is reduced.\n\nIncreasing levels of the stress hormone cortisol late in sleep (often during REM sleep) causes this decreased communication. One stage of memory consolidation is the linking of distant but related memories. Payne and Nadal hypothesize these memories are then consolidated into a smooth narrative, similar to a process that happens when memories are created under stress.\nRobert (1886), a physician from Hamburg, was the first who suggested that dreams are a need and that they have the function to erase (a) sensory impressions that were not fully worked up, and (b) ideas that were not fully developed during the day. By the dream work, incomplete material is either removed (suppressed) or deepened and included into memory. Robert's ideas were cited repeatedly by Freud in his \"Die Traumdeutung\". Hughlings Jackson (1911) viewed that sleep serves to sweep away unnecessary memories and connections from the day.\n\nThis was revised in 1983 by Crick and Mitchison's \"reverse learning\" theory, which states that dreams are like the cleaning-up operations of computers when they are off-line, removing (suppressing) parasitic nodes and other \"junk\" from the mind during sleep. However, the opposite view that dreaming has an information handling, memory-consolidating function (Hennevin and Leconte, 1971) is also common.\n\nCoutts describes dreams as playing a central role in a two-phase sleep process that improves the mind's ability to meet human needs during wakefulness. During the accommodation phase, mental schemas self-modify by incorporating dream themes. During the emotional selection phase, dreams test prior schema accommodations. Those that appear adaptive are retained, while those that appear maladaptive are culled. The cycle maps to the sleep cycle, repeating several times during a typical night's sleep. Alfred Adler suggested that dreams are often emotional preparations for solving problems, intoxicating an individual away from common sense toward private logic. The residual dream feelings may either reinforce or inhibit contemplated action.\n\nNumerous theories state that dreaming is a random by-product of REM sleep physiology and that it does not serve any natural purpose. Flanagan claims that \"dreams are evolutionary epiphenomena\" and they have no adaptive function. \"Dreaming came along as a free ride on a system designed to think and to sleep.\" Hobson, for different reasons, also considers dreams epiphenomena. He believes that the substance of dreams have no significant influence on waking actions, and most people go about their daily lives perfectly well without remembering their dreams.\n\nIn 2005, however, Hobson published a book, \"Thirteen Dreams that Freud Never Had\", in which he analyzed his own dreams after having a stroke in 2001. The book is filled with the most forthright analyses of Hobson’s dreams, with attention to problems in bodily function and social relations. The book illustrates how dreams show our most compelling concerns and how they can be used to make sense of the most difficult life situations. \n\nHobson proposed the activation-synthesis theory, which states that \"there is a randomness of dream imagery and the randomness synthesizes dream-generated images to fit the patterns of internally generated stimulations\". This theory is based on the physiology of REM sleep, and Hobson believes dreams are the outcome of the forebrain reacting to random activity beginning at the brainstem. The activation-synthesis theory hypothesizes that the peculiar nature of dreams is attributed to certain parts of the brain trying to piece together a story out of what is essentially bizarre information.\n\nHowever, evolutionary psychologists believe dreams serve some adaptive function for survival. Deirdre Barrett describes dreaming as simply \"thinking in different biochemical state\" and believes people continue to work on all the same problems—personal and objective—in that state. Her research finds that anything—math, musical composition, business dilemmas—may get solved during dreaming. \n\nFinnish psychologist Antti Revonsuo posits that dreams have evolved for \"threat simulation\" exclusively. According to the Threat Simulation Theory he proposes, during much of human evolution physical and interpersonal threats were serious, giving reproductive advantage to those who survived them. Therefore, dreaming evolved to replicate these threats and continually practice dealing with them. In support of this theory, Revonsuo shows that contemporary dreams comprise much more threatening events than people meet in daily non-dream life, and the dreamer usually engages appropriately with them. It is suggested by this theory that dreams serve the purpose of allowing for the rehearsal of threatening scenarios in order to better prepare an individual for real-life threats.\n\nAccording to Tsoukalas (2012) the biology of dreaming is related to the reactive patterns elicited by predatorial encounters (especially the tonic immobility reflex), a fact that lends support to evolutionary theories claiming that dreams specialize in threat avoidance or emotional processing.\n\nThere are many other hypotheses about the function of dreams, including:\n\nFrom the 1940s to 1985, Calvin S. Hall collected more than 50,000 dream reports at Western Reserve University. In 1966 Hall and Van De Castle published \"The Content Analysis of Dreams\", in which they outlined a coding system to study 1,000 dream reports from college students. Results indicated that participants from varying parts of the world demonstrated similarity in their dream content. Hall's complete dream reports were made publicly available in the mid-1990s by Hall's protégé William Domhoff.\n\nThe visual nature of dreams is generally highly phantasmagoric; that is, different locations and objects continuously blend into each other. The visuals (including locations, characters/people, objects/artifacts) are generally reflective of a person's memories and experiences, but conversation can take on highly exaggerated and bizarre forms. Some dreams may even tell elaborate stories wherein the dreamer enters entirely new, complex worlds and awakes with ideas, thoughts and feelings never experienced prior to the dream.\n\nPeople who are blind from birth do not have visual dreams. Their dream contents are related to other senses like auditory, touch, smell and taste, whichever are present since birth.\n\nIn the Hall study, the most common emotion experienced in dreams was anxiety. Other emotions included abandonment, anger, fear, joy, and happiness. Negative emotions were much more common than positive ones.\n\nThe Hall data analysis shows that sexual dreams occur no more than 10% of the time and are more prevalent in young to mid-teens. Another study showed that 8% of both men and women's dreams have sexual content. In some cases, sexual dreams may result in orgasms or nocturnal emissions. These are colloquially known as \"wet dreams\".\n\nA small minority of people say that they dream only in black and white. A 2008 study by a researcher at the University of Dundee found that people who were only exposed to black and white television and film in childhood reported dreaming in black and white about 25% of the time.\n\nThere is evidence that certain medical conditions (normally only neurological conditions) can impact dreams. For instance, some people with synesthesia have never reported entirely black-and-white dreaming, and often have a difficult time imagining the idea of dreaming in only black and white.\n\nDream interpretation can be a result of subjective ideas and experiences. One study found that most people believe that \"their dreams reveal meaningful hidden truths\". In one study conducted in the United States, South Korea and India, they found that 74% of Indians, 65% of South Koreans and 56% of Americans believed their dream content provided them with meaningful insight into their unconscious beliefs and desires. This Freudian view of dreaming was endorsed significantly more than theories of dreaming that attribute dream content to memory consolidation, problem-solving, or random brain activity.\n\nIn the paper, Morewedge and Norton (2009) also found that people attribute more importance to dream content than to similar thought content that occurs while they are awake. In one study, Americans were more likely to report that they would miss their flight if they dreamt of their plane crashing than if they thought of their plane crashing the night before flying (while awake), and that they would be as likely to miss their flight if they dreamt of their plane crashing the night before their flight as if there was an actual plane crash on the route they intended to take. Not all dream content was considered equally important. Participants in their studies were more likely to perceive dreams to be meaningful when the content of dreams was in accordance with their beliefs and desires while awake. People were more likely to view a positive dream about a friend to be meaningful than a positive dream about someone they disliked, for example, and were more likely to view a negative dream about a person they disliked as meaningful than a negative dream about a person they liked.\n\nTherapy for recurring nightmares (often associated with posttraumatic stress disorder) can include imagining alternative scenarios that could begin at each step of the dream.\n\nDuring the night, many external stimuli may bombard the senses, but the brain often interprets the stimulus and makes it a part of a dream to ensure continued sleep. Dream incorporation is a phenomenon whereby an actual sensation, such as environmental sounds, is incorporated into dreams, such as hearing a phone ringing in a dream while it is ringing in reality or dreaming of urination while wetting the bed. The mind can, however, awaken an individual if they are in danger or if trained to respond to certain sounds, such as a baby crying.\n\nThe term \"dream incorporation\" is also used in research examining the degree to which preceding daytime events become elements of dreams. Recent studies suggest that events in the day immediately preceding, and those about a week before, have the most influence. Gary Alan Fine and Laura Fischer Leighton argue that “dreams are external to the individual mind” because “1) dreams are not willed by the individual self; 2) dreams reflect social reality; 3) dreams are public rhetoric; and 4) dreams are collectively interpretable.”\n\nAccording to surveys, it is common for people to feel their dreams are predicting subsequent life events. Psychologists have explained these experiences in terms of memory biases, namely a selective memory for accurate predictions and distorted memory so that dreams are retrospectively fitted onto life experiences. The multi-faceted nature of dreams makes it easy to find connections between dream content and real events. The term \"veridical dream\" has been used to indicate dreams that reveal or contain truths not yet known to the dreamer, whether future events or secrets.\n\nIn one experiment, subjects were asked to write down their dreams in a diary. This prevented the selective memory effect, and the dreams no longer seemed accurate about the future. Another experiment gave subjects a fake diary of a student with apparently precognitive dreams. This diary described events from the person's life, as well as some predictive dreams and some non-predictive dreams. When subjects were asked to recall the dreams they had read, they remembered more of the successful predictions than unsuccessful ones.\n\nLucid dreaming is the conscious perception of one's state while dreaming. In this state the dreamer may often have some degree of control over their own actions within the dream or even the characters and the environment of the dream. Dream control has been reported to improve with practiced deliberate lucid dreaming, but the ability to control aspects of the dream is not necessary for a dream to qualify as \"lucid\" — a lucid dream is any dream during which the dreamer knows they are dreaming. The occurrence of lucid dreaming has been scientifically verified.\n\n\"Oneironaut\" is a term sometimes used for those who lucidly dream.\n\nIn 1975, psychologist Keith Hearne successfully recorded a communication from a dreamer experiencing a lucid dream. On April 12, 1975, after agreeing to move his eyes left and right upon becoming lucid, the subject and Hearne's co-author on the resulting article, Alan Worsley, successfully carried out this task.\n\nYears later, psychophysiologist Stephen LaBerge conducted similar work including:\nCommunication between two dreamers has also been documented. The processes involved included EEG monitoring, ocular signaling, incorporation of reality in the form of red light stimuli and a coordinating website. The website tracked when both dreamers were dreaming and sent the stimulus to one of the dreamers where it was incorporated into the dream. This dreamer, upon becoming lucid, signaled with eye movements; this was detected by the website whereupon the stimulus was sent to the second dreamer, invoking incorporation into this dream.\n\nDreams of absent-minded transgression (DAMT) are dreams wherein the dreamer absentmindedly performs an action that he or she has been trying to stop (one classic example is of a quitting smoker having dreams of lighting a cigarette). Subjects who have had DAMT have reported waking with intense feelings of guilt. One study found a positive association between having these dreams and successfully stopping the behavior.\n\nThe recollection of dreams is extremely unreliable, though it is a skill that can be trained. Dreams can usually be recalled if a person is awakened while dreaming. Women tend to have more frequent dream recall than men. Dreams that are difficult to recall may be characterized by relatively little affect, and factors such as salience, arousal, and interference play a role in dream recall. Often, a dream may be recalled upon viewing or hearing a random trigger or stimulus. The \"salience hypothesis\" proposes that dream content that is salient, that is, novel, intense, or unusual, is more easily remembered. There is considerable evidence that vivid, intense, or unusual dream content is more frequently recalled. A dream journal can be used to assist dream recall, for personal interest or psychotherapy purposes.\n\nFor some people, sensations from the previous night's dreams are sometimes spontaneously experienced in falling asleep. However they are usually too slight and fleeting to allow dream recall. At least 95% of all dreams are not remembered. Certain brain chemicals necessary for converting short-term memories into long-term ones are suppressed during REM sleep. Unless a dream is particularly vivid and if one wakes during or immediately after it, the content of the dream is not remembered. Recording or reconstructing dreams may one day assist with dream recall. Using technologies such as functional magnetic resonance imaging (fMRI) and electromyography (EMG), researchers have been able to record basic dream imagery, dream speech activity and dream motor behavior (such as walking and hand movements).\n\nIn line with the salience hypothesis, there is considerable evidence that people who have more vivid, intense or unusual dreams show better recall. There is evidence that continuity of consciousness is related to recall. Specifically, people who have vivid and unusual experiences during the day tend to have more memorable dream content and hence better dream recall. People who score high on measures of personality traits associated with creativity, imagination, and fantasy, such as openness to experience, daydreaming, fantasy proneness, absorption, and hypnotic susceptibility, tend to show more frequent dream recall. There is also evidence for continuity between the bizarre aspects of dreaming and waking experience. That is, people who report more bizarre experiences during the day, such as people high in schizotypy (psychosis proneness) have more frequent dream recall and also report more frequent nightmares.\n\nOne theory of déjà vu attributes the feeling of having previously seen or experienced something to having dreamed about a similar situation or place, and forgetting about it until one seems to be mysteriously reminded of the situation or the place while awake.\n\nA daydream is a visionary fantasy, especially one of happy, pleasant thoughts, hopes or ambitions, imagined as coming to pass, and experienced while awake. There are many different types of daydreams, and there is no consistent definition amongst psychologists. The general public also uses the term for a broad variety of experiences. Research by Harvard psychologist Deirdre Barrett has found that people who experience vivid dream-like mental images reserve the word for these, whereas many other people refer to milder imagery, realistic future planning, review of past memories or just \"spacing out\"—i.e. one's mind going relatively blank—when they talk about \"daydreaming.\"\n\nWhile daydreaming has long been derided as a lazy, non-productive pastime, it is now commonly acknowledged that daydreaming can be constructive in some contexts. There are numerous examples of people in creative or artistic careers, such as composers, novelists and filmmakers, developing new ideas through daydreaming. Similarly, research scientists, mathematicians and physicists have developed new ideas by daydreaming about their subject areas.\n\nA hallucination, in the broadest sense of the word, is a perception in the absence of a stimulus. In a stricter sense, hallucinations are perceptions in a conscious and awake state, in the absence of external stimuli, and have qualities of real perception, in that they are vivid, substantial, and located in external objective space. The latter definition distinguishes hallucinations from the related phenomena of dreaming, which does not involve wakefulness.\n\nA nightmare is an unpleasant dream that can cause a strong negative emotional response from the mind, typically fear or horror, but also despair, anxiety and great sadness. The dream may contain situations of danger, discomfort, psychological or physical terror. Sufferers usually awaken in a state of distress and may be unable to return to sleep for a prolonged period of time.\n\nA night terror, also known as a sleep terror or \"pavor nocturnus\", is a parasomnia disorder that predominantly affects children, causing feelings of terror or dread. Night terrors should not be confused with nightmares, which are bad dreams that cause the feeling of horror or fear.\n\n\n"}
{"id": "43026", "url": "https://en.wikipedia.org/wiki?curid=43026", "title": "Endianness", "text": "Endianness\n\nEndianness refers to the sequential order in which bytes are arranged into larger numerical values when stored in memory or when transmitted over digital links. Endianness is of interest in computer science because two conflicting and incompatible formats are in common use: words may be represented in big-endian or little-endian format, depending on whether bits or bytes or other components are ordered from the big end (most significant bit) or the little end (least significant bit). \n\nIn big-endian format, whenever addressing memory or sending/storing words bytewise, the most significant byte—the byte containing the most significant bit—is stored first (has the lowest address) or sent first, then the following bytes are stored or sent in decreasing significance order, with the least significant byte—the one containing the least significant bit—stored last (having the highest address) or sent last.\n\nLittle-endian format reverses this order: the sequence addresses/sends/stores the least significant byte first (lowest address) and the most significant byte last (highest address). Most computer systems prefer a single format for all its data; using the system's native format is automatic. But when reading memory or receiving transmitted data from a different computer system, it is often required to process and translate data between the preferred native endianness format to the opposite format. \n\nThe order of bits within a byte or word can also have endianness (as discussed later); however, a byte is typically handled as a single numerical value or character symbol and so bit sequence order is obviated.\n\nBoth big and little forms of endianness are widely used in digital electronics. The choice of endianness for a new design is often arbitrary, but later technology revisions and updates perpetuate the existing endianness and many other design attributes to maintain backward compatibility. As examples, the IBM z/Architecture mainframes and the Motorola 68000 series use big-endian while the Intel x86 processors use little-endian. The designers of System/360, the ancestor of z/Architecture, chose its endianness in the 1960s; the designers of the Motorola 68000 and the Intel 8086, the first members of the 68000 and x86 families, chose their endianness in the 1970s.\n\nBig-endian is the most common format in data networking; fields in the protocols of the Internet protocol suite, such as IPv4, IPv6, TCP, and UDP, are transmitted in big-endian order. For this reason, big-endian byte order is also referred to as network byte order. Little-endian storage is popular for microprocessors, in part due to significant influence on microprocessor designs by Intel Corporation. Mixed forms also exist; for instance, in VAX floating point, the ordering of bytes in a 16-bit word differs from the ordering of 16-bit words within a 32-bit word. Such cases are sometimes referred to as mixed-endian or middle-endian. There are also some bi-endian processors that operate in either little-endian or big-endian mode.\n\nThe following two descriptive illustrations assume a normal reading and writing convention of \"left to right\", where the left-most digit or character therefore corresponds to data being \"sent\" (or \"received\") first, or being in the \"lowest address in memory,\" and the right-most digit or character corresponds to the data being sent or received last, or being in the \"highest\" address in memory.\n\nBig-endianness may be demonstrated by writing a decimal number, say one hundred twenty-three, on paper in the usual positional notation understood by a numerate reader: \"123\". The digits are written starting from the left and to the right, with the most significant digit, \"1\", written first. This is analogous to the lowest address of memory being used first. This is an example of a big-endian convention taken from daily life.\n\nThe little-endian way of writing the same number, one hundred twenty-three, would place the hundreds-digit \"1\" in the right-most position: \"321\". A person following conventional big-endian place-value order, who is not aware of this special ordering, would read a different number: three hundred and twenty one. Endianness in computing is similar, but it usually applies to the ordering of bytes, rather than of digits.\n\nThe illustrations to the right, where \"a\" is a memory address, show big-endian and little-endian storage in memory.\n\nDanny Cohen introduced the terms Little-Endian and Big-Endian for byte ordering in an article from 1980. \nIn this technical and political examination of byte ordering issues, the \"endian\" names were drawn from \nJonathan Swift's 1726 satire, \"\", in which civil war erupts over whether the big end or the little end of a boiled egg is the proper end to crack open, which is analogous to counting from the end that contains the most significant bit or the least significant bit.\n\nComputer memory consists of a sequence of storage cells. Each cell is identified in hardware and software by its memory address. If the total number of storage cells in memory is \"n\", then addresses are enumerated from \"0\" to \"n-1\". Computer programs often use data structures of fields that may consist of more data than is stored in one memory cell. For the purpose of this article where its use as an operand of an instruction is relevant, a field consists of a consecutive sequence of bytes and represents a simple data value. In addition to that, it has to be of numeric type in some positional number system (mostly base-10 or base-2 – or base-256 in case of 8-bit bytes). In such a number system the \"value\" of a digit is determined not only by its value as a single digit, but also by the position it holds in the complete number, its \"significance\". These positions can be mapped to memory mainly in two ways:\n\nWhile the little-endian Intel microprocessor product line became a popular architecture, many historical and extant processors use a big-endian memory representation, commonly referred to as network order, as used in the Internet protocol suite, either exclusively or as a design option; others use yet another scheme called \"middle-endian\", \"mixed-endian\" or \"PDP-11-endian\".\n\nThe IBM System/360 uses big-endian byte order, as do its successors System/370, ESA/390, and z/Architecture. The PDP-10 also uses big-endian addressing for byte-oriented instructions. The IBM Series/1 minicomputer also use big-endian byte order.\n\nDealing with data of different endianness is sometimes termed the \"NUXI problem\". This terminology alludes to the byte order conflicts encountered while adapting UNIX, which ran on the mixed-endian PDP-11, to a big-endian IBM Series/1 computer. Unix was one of the first systems to allow the same code to be compiled for platforms with different internal representations. One of the first programs converted was supposed to print out \"Unix\", but on the Series/1 it printed \"nUxi\" instead.\n\nThe Datapoint 2200 uses simple bit-serial logic with little-endian to facilitate carry propagation. When Intel developed the 8008 microprocessor for Datapoint, they used little-endian for compatibility. However, as Intel was unable to deliver the 8008 in time, Datapoint used a medium scale integration equivalent, but the little-endianness was retained in most Intel designs. Intel MCS-48 is also little-endian, as are the well-known DEC Alpha, Atmel AVR, VAX and many more.\n\nThe Motorola 6800 / 6801, the 6809 and the 68000 series of processors used the big-endian format, and for this reason, it is also known as the \"Motorola convention\".\n\nThe Intel 8051, contrary to other Intel processors, expects 16-bit addresses for LJMP and LCALL in big-endian format; however, xCALL instructions store the return address onto the stack in little-endian format.\n\nSPARC historically used big-endian until version 9, which is bi-endian; similarly early IBM POWER processors were big-endian, but now the PowerPC and Power Architecture descendants are bi-endian. The ARM architecture was little-endian before version 3 when it became bi-endian.\n\nOther well-known little-endian processor architectures include MOS Technology 6502 (including Western Design Center 65802 and 65C816), Zilog Z80 (including Z180 and eZ80) and Altera Nios II.\n\nThe Intel x86 and also AMD64 / x86-64 series of processors use the little-endian format, and for this reason, it is also known in the industry as the \"Intel convention\".\n\nSome current big-endian architectures include the IBM z/Architecture, Freescale ColdFire (which is Motorola 68000 series-based), Xilinx MicroBlaze, Atmel AVR32.\n\nAs a consequence of its original implementation on the Intel 8080 platform, the operating system-independent FAT file system is defined to use little-endian byte ordering, even on platforms using other endiannesses natively.\n\nSome architectures (including ARM versions 3 and above, PowerPC, Alpha, SPARC V9, MIPS, PA-RISC, SuperH SH-4 and IA-64) feature a setting which allows for switchable endianness in data fetches and stores, instruction fetches, or both. This feature can improve performance or simplify the logic of networking devices and software. The word \"bi-endian\", when said of hardware, denotes the capability of the machine to compute or pass data in either endian format.\n\nMany of these architectures can be switched via software to default to a specific endian format (usually done when the computer starts up); however, on some systems the default endianness is selected by hardware on the motherboard and cannot be changed via software (e.g. the Alpha, which runs only in big-endian mode on the Cray T3E).\n\nNote that the term \"bi-endian\" refers primarily to how a processor treats \"data\" accesses. \"Instruction\" accesses (fetches of instruction words) on a given processor may still assume a fixed endianness, even if \"data\" accesses are fully bi-endian, though this is not always the case, such as on Intel's IA-64-based Itanium CPU, which allows both.\n\nNote, too, that some nominally bi-endian CPUs require motherboard help to fully switch endianness. For instance, the 32-bit desktop-oriented PowerPC processors in little-endian mode act as little-endian from the point of view of the executing programs, but they require the motherboard to perform a 64-bit swap across all 8 byte lanes to ensure that the little-endian view of things will apply to I/O devices. In the absence of this unusual motherboard hardware, device driver software must write to different addresses to undo the incomplete transformation and also must perform a normal byte swap.\n\nSome CPUs, such as many PowerPC processors intended for embedded use and almost all SPARC processors, allow per-page choice of endianness.\n\nSPARC processors since the late 1990s (\"SPARC v9\" compliant processors) allow data endianness to be chosen with each individual instruction that loads from or stores to memory.\n\nMany processors have instructions to convert a word in a register to the opposite endianness, that is, they swap the order of the bytes in a 16-, 32- or 64-bit word. All the individual bits are not reversed though.\n\nRecent Intel x86 and x86-64 architecture CPUs have a MOVBE instruction (Intel Core since generation 4, after Atom), which fetches a big-endian format word from memory or writes a word into memory in big-endian format. These processors are otherwise thoroughly little-endian. They also already had a range of swap instructions to reverse the byte order of the contents of registers, such as when words have already been fetched from memory locations where they were in the 'wrong' endianness.\n\nZFS/OpenZFS combined file system and logical volume manager is known to provide adaptive endianness and to work with both big-endian and little-endian systems.\n\n<section begin=\"Floating-point\" />\nAlthough the ubiquitous x86 processors of today use little-endian storage for all types of data (integer, floating point, BCD), there are a number of hardware architectures where floating-point numbers are represented in big-endian form while integers are represented in little-endian form. There are ARM processors that have half little-endian, half big-endian floating-point representation for double-precision numbers: both 32-bit words are stored in little-endian like integer registers, but the most significant one first. Because there have been many floating-point formats with no \"network\" standard representation for them, the XDR standard uses big-endian IEEE 754 as its representation. It may therefore appear strange that the widespread IEEE 754 floating-point standard does not specify endianness. Theoretically, this means that even standard IEEE floating-point data written by one machine might not be readable by another. However, on modern standard computers (i.e., implementing IEEE 754), one may in practice safely assume that the endianness is the same for floating-point numbers as for integers, making the conversion straightforward regardless of data type. (Small embedded systems using special floating-point formats may be another matter however.)<section end=\"Floating-point\" />\n\nThe little-endian system has the property that the same value can be read from memory at different lengths without using different addresses (even when alignment restrictions are imposed). For example, a 32-bit memory location with content 4A 00 00 00 can be read at the same address as either 8-bit (value = 4A), 16-bit (004A), 24-bit (00004A), or 32-bit (0000004A), all of which retain the same numeric value. Although this little-endian property is rarely used directly by high-level programmers, it is often employed by code optimizers as well as by assembly language programmers.\nIn more concrete terms, such optimizations are the equivalent of the following C code returning true on most little-endian systems:\n\nWhile not allowed by C++, such Type punning code is allowed as \"implementation-defined\" by the C11 standard and commonly used in code interacting with hardware.\n\nOn the other hand, in some situations it may be useful to obtain an approximation of a multi-byte or multi-word value by reading only its most significant portion instead of the complete representation; a big-endian processor may read such an approximation using the same base-address that would be used for the full value.\n\nLittle-endian representation simplifies hardware in processors that add multi-byte integral values a byte at a time, such as small-scale byte-addressable processors and microcontrollers. As carry propagation must start at the least significant bit (and thus byte), multi-byte addition can then be carried out with a monotonically-incrementing address sequence, a simple operation already present in hardware. On a big-endian processor, its addressing unit has to be told how big the addition is going to be so that it can hop forward to the least significant byte, then count back down towards the most significant byte (MSB). On the other hand, arithmetic division is done starting from the MSB, so it is more natural for big-endian processors. However, high-performance processors usually fetch typical multi-byte operands from memory in the same amount of time they would have fetched a single byte, so the complexity of the hardware is not affected by the byte ordering.\n\nThe ARM architecture can also produce this format when writing a 32-bit word to an address 2 bytes from a 32-bit word alignment.\n\nSegment descriptors on Intel 80386 and compatible processors keep a 32-bit base address of the segment stored in little-endian order, but in four nonconsecutive bytes, at relative positions 2, 3, 4 and 7 of the descriptor start.\n\nEndianness is a problem when a binary file created on a computer is read on another computer with different endianness. Some CPU instruction sets provide native support for endian byte swapping, such as \"bswap\" (x86 - 486 and later), and \"rev\" (ARMv6 and later).\n\nSome compilers have built-in facilities to deal with data written in other formats. For example, the Intel Fortran compiler supports the non-standard codice_1 specifier, so a file can be opened as\nor\nSome compilers have options to generate code that globally enables the conversion for all file IO operations. This allows programmers to reuse code on a system with the opposite endianness without having to modify the code itself. If the compiler does not support such conversion, the programmer needs to swap the bytes via ad hoc code.\n\nFortran sequential unformatted files created with one endianness usually cannot be read on a system using the other endianness because Fortran usually implements a record (defined as the data written by a single Fortran statement) as data preceded and succeeded by count fields, which are integers equal to the number of bytes in the data. An attempt to read such file on a system of the other endianness then results in a run-time error, because the count fields are incorrect. This problem can be avoided by writing out sequential binary files as opposed to sequential unformatted.\n\nUnicode text can optionally start with a byte order mark (BOM) to signal the endianness of the file or stream. Its code point is U+FEFF. In UTF-32 for example, a big-endian file should start with codice_4; a little-endian should start with codice_5.\n\nApplication binary data formats, such as for example MATLAB .mat files, or the .BIL data format, used in topography, are usually endianness-independent. This is achieved by:\nWhen reading the file, the application converts the endianness, invisibly from the user. \nAn example of the first case is the binary XLS file format that is portable between Windows and Mac systems and always little endian, leaving the Mac application to swap the bytes on load and save when running on a big-endian Motorola 68K or PowerPC processor.\n\nTIFF image files are an example of the second strategy, whose header instructs the application about endianness of their internal binary integers. If a file starts with the signature \"codice_6\" it means that integers are represented as big-endian, while \"codice_7\" means little-endian. Those signatures need a single 16-bit word each, and they are palindromes (that is, they read the same forwards and backwards), so they are endianness independent. \"codice_8\" stands for Intel and \"codice_9\" stands for Motorola, the respective CPU providers of the IBM PC compatibles (Intel) and Apple Macintosh platforms (Motorola) in the 1980s. Intel CPUs are little-endian, while Motorola 680x0 CPUs are big-endian. This explicit signature allows a TIFF reader program to swap bytes if necessary when a given file was generated by a TIFF writer program running on a computer with a different endianness.\n\nSince the required byte swap depends on the size of the numbers stored in the file (two 2-byte integers require a different swap than one 4-byte integer), the file format must be known to perform endianness conversion.\n\nMany IETF RFCs use the term \"network order\", meaning the order of transmission for bits and bytes \"over the wire\" in network protocols. Among others, the historic RFC 1700 (also known as Internet standard STD 2) has defined the network order for protocols in the Internet protocol suite to be big-endian, hence the use of the term \"network byte order\" for big-endian byte order; however, not all protocols use big-endian byte order as the network order.\n\nThe Berkeley sockets API defines a set of functions to convert 16-bit and 32-bit integers to and from network byte order: the htons (host-to-network-short) and htonl (host-to-network-long) functions convert 16-bit and 32-bit values respectively from machine (\"host\") to network order; the ntohs and ntohl functions convert from network to host order. These functions may be a no-op on a big-endian system.\n\nIn CANopen, multi-byte parameters are always sent least significant byte first (little endian). The same is true for Ethernet Powerlink.\n\nWhile the high-level network protocols usually consider the byte (mostly meant as \"octet\") as their atomic unit, the lowest network protocols may deal with ordering of bits within a byte.\n\nBit numbering is a concept similar to endianness, but on a level of bits, not bytes. \"Bit endianness\" or \"bit-level endianness\" refers to the transmission order of bits over a serial medium. The bit-level analogue of little-endian (least significant bit goes first) is used in RS-232, HDLC, Ethernet, and USB. Some protocols use the opposite ordering (e.g. Teletext, I²C, SMBus, PMBus, and SONET and SDH). Usually, there exists a consistent view to the bits irrespective of their order in the byte, such that the latter becomes relevant only on a very low level. One exception is caused by the feature of some cyclic redundancy checks to detect \"all\" burst errors up to a known length, which would be spoiled if the bit order is different from the byte order on serial transmission.\n\nApart from serialization, the terms \"bit endianness\" and \"bit-level endianness\" are seldom used, as computer architectures where each individual bit has a unique address are rare. Individual bits or bit fields are accessed via their numerical value or, in high-level programming languages, assigned names, the effects of which, however, may be machine dependent or lack software portability. The natural numbering is that the arithmetic left shift 1«\"n\" yields a mask for the bit of position \"n\", a rule which exhibits the machine's (byte) endianness at least if e.g. if used for indexing a sufficiently large bit array. Other numberings do occur in various documentations.\n\n\n"}
{"id": "1204294", "url": "https://en.wikipedia.org/wiki?curid=1204294", "title": "Gluing axiom", "text": "Gluing axiom\n\nIn mathematics, the gluing axiom is introduced to define what a sheaf \"F\" on a topological space \"X\" must satisfy, given that it is a presheaf, which is by definition a contravariant functor\n\nto a category \"C\" which initially one takes to be the category of sets. Here \"O\"(\"X\") is the partial order of open sets of \"X\" ordered by inclusion maps; and considered as a category in the standard way, with a unique morphism\n\nif \"U\" is a subset of \"V\", and none otherwise.\n\nAs phrased in the sheaf article, there is a certain axiom that \"F\" must satisfy, for any open cover of an open set of \"X\". For example, given open sets \"U\" and \"V\" with union \"X\" and intersection \"W\", the required condition is that\n\nIn less formal language, a section \"s\" of \"F\" over \"X\" is equally well given by a pair of sections (\"s\"′,\"s\"′′) on \"U\" and \"V\" respectively, which 'agree' in the sense that \"s\"′ and \"s\"′′ have a common image in \"F\"(\"W\") under the respective restriction maps\n\nand\n\nThe first major hurdle in sheaf theory is to see that this \"gluing\" or \"patching\" axiom is a correct abstraction from the usual idea in geometric situations. For example, a vector field is a section of a tangent bundle on a smooth manifold; this says that a vector field on the union of two open sets is (no more and no less than) vector fields on the two sets that agree where they overlap.\n\nGiven this basic understanding, there are further issues in the theory, and some will be addressed here. A different direction is that of the Grothendieck topology, and yet another is the logical status of 'local existence' (see Kripke–Joyal semantics).\n\nTo rephrase this definition in a way that will work in any category \"C\" that has sufficient structure, we note that we can write the objects and morphisms involved in the definition above in a diagram which we will call (G), for \"gluing\":\n\nHere the first map is the product of the restriction maps\n\nand each pair of arrows represents the two restrictions\n\nand\n\nIt is worthwhile to note that these maps exhaust all of the possible restriction maps among \"U\", the \"U\", and the \"U\"∩\"U\".\n\nThe condition for \"F\" to be a sheaf is exactly that \"F\" is the limit of the diagram. This suggests the correct form of the gluing axiom:\n\nOne way of understanding the gluing axiom is to notice that \"un-applying\" \"F\" to (G) yields the following diagram:\n\nHere \"U\" is the colimit of this diagram. The gluing axiom says that \"F\" turns colimits of such diagrams into limits.\n\nIn some categories, it is possible to construct a sheaf by specifying only some of its sections. Specifically, let \"X\" be a topological space with basis {\"B\"}. We can define a category \"O\" ′(\"X\") to be the full subcategory of \"O\"(\"X\") whose objects are the {\"B\"}. A B-sheaf on \"X\" with values in C is a contravariant functor\n\nwhich satisfies the gluing axiom for sets in \"O\" ′(\"X\"). That is, on a selection of open sets of \"X\", \"F\" specifies all of the sections of a sheaf, and on the other open sets, it is undetermined.\n\nB-sheaves are equivalent to sheaves (that is, the category of sheaves is equivalent to the category of B-sheaves). Clearly a sheaf on \"X\" can be restricted to a B-sheaf. In the other direction, given a B-sheaf \"F\" we must determine the sections of \"F\" on the other objects of \"O\"(\"X\"). To do this, note that for each open set \"U\", we can find a collection {\"B\"} whose union is \"U\". Categorically speaking, this choice makes \"U\" the colimit of the full subcategory of \"O\" ′(\"X\") whose objects are {\"B\"}. Since \"F\" is contravariant, we define \"F\"′(\"U\") to be the limit of the {\"F\"(\"B\")} with respect to the restriction maps. (Here we must assume that this limit exists in C.) If \"U\" is a basic open set, then \"U\" is a terminal object of the above subcategory of \"O\" ′(\"X\"), and hence \"F\"′(\"U\") = \"F\"(\"U\"). Therefore, \"F\"′ extends \"F\" to a presheaf on \"X\". It can be verified that \"F\"′ is a sheaf, essentially because every element of every open cover of \"X\" is a union of basis elements (by the definition of a basis), and every pairwise intersection of elements in an open cover of \"X\" is a union of basis elements (again by the definition of a basis).\n\nThe first needs of sheaf theory were for sheaves of abelian groups; so taking the category \"C\" as the category of abelian groups was only natural. In applications to geometry, for example complex manifolds and algebraic geometry, the idea of a \"sheaf of local rings\" is central. This, however, is not quite the same thing; one speaks instead of a locally ringed space, because it is not true, except in trite cases, that such a sheaf is a functor into a category of local rings. It is the \"stalks\" of the sheaf that are local rings, not the collections of \"sections\" (which are rings, but in general are not close to being \"local\"). We can think of a locally ringed space \"X\" as a parametrised family of local rings, depending on \"x\" in \"X\".\n\nA more careful discussion dispels any mystery here. One can speak freely of a sheaf of abelian groups, or rings, because those are algebraic structures (defined, if one insists, by an explicit signature). Any category \"C\" having finite products supports the idea of a group object, which some prefer just to call a group \"in\" \"C\". In the case of this kind of purely algebraic structure, we can talk \"either\" of a sheaf having values in the category of abelian groups, or an \"abelian group in the category of sheaves of sets\"; it really doesn't matter.\n\nIn the local ring case, it does matter. At a foundational level we must use the second style of definition, to describe what a local ring means in a category. This is a logical matter: axioms for a local ring require use of existential quantification, in the form that for any \"r\" in the ring, one of \"r\" and 1 − \"r\" is invertible. This allows one to specify what a 'local ring in a category' should be, in the case that the category supports enough structure.\n\nTo turn a given presheaf \"P\" into a sheaf \"F\", there is a standard device called sheafification or sheaving. The rough intuition of what one should do, at least for a presheaf of sets, is to introduce an equivalence relation, which makes equivalent data given by different covers on the overlaps by refining the covers. One approach is therefore to go to the stalks and recover the sheaf space of the \"best possible\" sheaf \"F\" produced from \"P\".\n\nThis use of language strongly suggests that we are dealing here with adjoint functors. Therefore, it makes sense to observe that the sheaves on \"X\" form a full subcategory of the presheaves on \"X\". Implicit in that is the statement that a morphism of sheaves is nothing more than a natural transformation of the sheaves, considered as functors. Therefore, we get an abstract characterisation of sheafification as left adjoint to the inclusion. In some applications, naturally, one does need a description.\n\nIn more abstract language, the sheaves on \"X\" form a reflective subcategory of the presheaves (Mac Lane–Moerdijk \"Sheaves in Geometry and Logic\" p. 86). In topos theory, for a Lawvere–Tierney topology and its sheaves, there is an analogous result (ibid. p. 227).\n\nThe gluing axiom of sheaf theory is rather general. One can note that the Mayer–Vietoris axiom of homotopy theory, for example, is a special case.\n"}
{"id": "711728", "url": "https://en.wikipedia.org/wiki?curid=711728", "title": "Governmentality", "text": "Governmentality\n\nGovernmentality is a concept first developed by the French philosopher Michel Foucault in the later years of his life, roughly between 1977 and his death in 1984, particularly in his lectures at the Collège de France during this time.\n\nThe concept has been elaborated further from an \"Anglo-Neo Foucauldian\" perspective in the social sciences, especially by authors such as Peter Miller, Nikolas Rose, and Mitchell Dean. Governmentality can be understood as:\n\nGovernmentality may also be understood as:\n\nThis term was thought by some commentators to be made by the \"...linking of governing (\"gouverner\") and modes of thought (\"mentalité\")\". In fact, it was not coined by uniting words \"gouvernement\" and \"mentalité\", but simply by making \"gouvernement\" into \"gouvernementalité\" just like \"musical\" into \"musicalité\" [i.e. government + -al- \"adjective\" + -ité \"abstract noun\"] (see Michel Senellart's \"Course Context\" in Foucault's \"Security, territory, population\" lectures). To fully understand this concept, it is important to realize that in this case, Foucault does not only use the standard, strictly political definition of \"governing\" or government used today, but he also uses the broader definition of governing or government that was employed until the eighteenth century. That is to say, that in this case, for Foucault, \"...'government' also signified problems of self-control, guidance for the family and for children, management of the household, directing the soul, etc.\" In other words, for our purposes, government is \"...the conduct of conduct...\"\n\nIn his lectures at the Collège de France, Foucault often defines governmentality as the \"art of government\" in a wide sense, i.e. with an idea of \"government\" that is not limited to state politics alone, that includes a wide range of control techniques, and that applies to a wide variety of objects, from one's control of the self to the \"biopolitical\" control of populations. In the work of Foucault, this notion is indeed linked to other concepts such as biopolitics and power-knowledge. The genealogical exploration of the modern state as \"problem of government\" does not only deepen Foucault's analyses on sovereignty and biopolitics; it offers an analytics of government which refines both Foucault's theory of power and his understanding of freedom.\n\nThe concept of \"governmentality\" develops a new understanding of power. Foucault encourages us to think of power not only in terms of hierarchical, top-down power of the state. He widens our understanding of power to also include the forms of social control in disciplinary institutions (schools, hospitals, psychiatric institutions, etc.), as well as the forms of knowledge. Power can manifest itself positively by producing knowledge and certain discourses that get internalised by individuals and guide the behaviour of populations. This leads to more efficient forms of social control, as knowledge enables individuals to govern themselves.\n\n\"Governmentality\" applies to a variety of historical periods and to different specific power regimes. However, it is often used (by other scholars and by Foucault himself) in reference to \"neoliberal governmentality\", i.e. to a type of governmentality that characterizes advanced liberal democracies. In this case, the notion of governmentality refers to societies where power is de-centered and its members play an active role in their own self-government, e.g. as posited in neoliberalism. Because of its active role, individuals need to be regulated from 'inside'. A particular form of governmentality is characterized by a certain form of knowledge (\"savoir\" in French). In the case of neoliberal governmentality (a kind of governmentality based on the predominance of market mechanisms and of the restriction of the action of the state) the knowledge produced allows the construction of auto-regulated or auto-correcting selves.\n\nIn his lecture titled Governmentality, Foucault gives us a definition of governmentality:\nAs Foucault's explicit definition is rather broad, perhaps further examination of this definition would be useful.\n\nWe shall begin with a closer inspection of the first part of Foucault's definition of governmentality:\n\nThis strand of the three-part definition states that governmentality is, in other words, all of the components that make up a government that has as its end the maintenance of a well-ordered and happy society (population). The government's means to this end is its \"apparatuses of security,\" that is to say, the techniques it uses to provide this society a feeling of economic, political, and cultural well-being. The government achieves these ends by enacting \"political economy,\" and in this case, the meaning of economy is the older definition of the term, that is to say, \"economy at the level of the entire state, which means exercising towards its inhabitants, and the wealth and behavior of each and all, a form of surveillance and control as attentive as that of the head of a family over his household and his goods\". Thus, we see that this first part of the definition states that governmentality is a government with specific ends, means to these ends, and particular practices that should lead to these ends.\n\nThe second part of Foucault's definition (the \"resulting, on the one hand, in formation of a whole series of specific governmental apparatuses, and, on the other, in the development of a whole complex of savoirs\") presents governmentality as the long, slow development of Western governments which eventually took over from forms of governance like sovereignty and discipline into what it is today: bureaucracies and the typical methods by which they operate.\n\nThe next and last part of Foucault's definition of governmentality can be restated as the evolution from the Medieval state, which maintained its territory and an ordered society within this territory through a blunt practice of simply imposing its laws upon its subjects, to the early Renaissance state, which became more concerned with the \"disposing of things\", and so began to employ strategies and tactics to maintain a content and thus stable society, or in other words to \"render a society governable\".\n\nThus, if one takes these three definitions together, governmentality may be defined as the process through which a form of government with specific ends (a happy and stable society), means to these ends (\"apparatuses of security\"), and with a particular type of knowledge (\"political economy\"), to achieve these ends, evolved from a medieval state of justice to a modern administrative state with complex bureaucracies.\n\nThe concept of governmentality segues from Foucault's ethical, political and historical thoughts from the late 1970s to the early 1980s. His most widely known formulation of this notion is his lecture entitled \"Security, territory and population\" (1978). A deeper and richer reflection on the notion of governmentality is provided in Foucault's course on \"The Birth of Biopolitics\" at the Collège de France in 1978-1979. The course was first published in French in 2004 as \"Naissance de la biopolitique: Cours au Collège de France (1978-1979)\" (Paris: Gallimard & Seuil). This notion is also part of a wider analysis on the topic of disciplinary institutions, on neoliberalism and the \"Rule of Law\", the \"microphysics of power\" and also on what Foucault called biopolitics. In the second and third volumes of \"The History of Sexuality\", namely, \"The Use of Pleasure\" (1984) and \"The Care of the Self\" (1984), and in his lecture on \"Technologies of the Self\" (1982), Foucault elaborated a distinction between subjectivation and forms of subjectification by exploring how selves were fashioned and then lived in ways which were both heteronomously and autonomously determined. Also, in a series of lectures and articles, including \"The Birth of Biopolitics\" (1979), \"\"Omnes et Singulatim\": Towards a Criticism of Political Reason\" (1979), \"The Subject and Power\" (1982) and \"What is Enlightenment?\" (1984), he posed questions about the nature of contemporary social orders, the conceptualization of power, human freedom and the limits, possibilities and sources of human actions, etc. that were linked to his understanding of the notion of \"governmentality\".\n\nThe notion of governmentality (not to confuse with governance) gained attention in the English-speaking academic world mainly through the edited book \"The Foucault Effect\" (1991), which contained a series of essays on the notion of governmentality, together with a translation of Foucault's 1978 short text on \"gouvernementalité\".\n\nHunt and Wickham, in their work \"Foucault and Law\" [1994] begin the section on governmentality with a very basic definition derived from Foucault's work. They state, \"governmentality is the dramatic expansion in the scope of government, featuring an increase in the number and size of the governmental calculation mechanisms\" [1994:76]. In other words, governmentality describes the new form of governing that arose in the mid-eighteenth century that was closely allied with the creation and growth of the modern bureaucracies. In giving this definition, Hunt and Wickham conceive of the term as consisting of two parts 'governmental' and '–ity' - governmental meaning pertaining to the government of a country; and the suffix –ity meaning the study of. They acknowledge that this definition lacks some of Foucault's finer nuances and try to redress this by explaining some more of Foucault's ideas, including reason of state, the problem of population, modern political economy, liberal securitisation, and the emergence of the human sciences\" [1994:77].\n\nKerr's approach to the term is more complex. He conceives of the term as an abbreviation of \"governmental rationality\" [1999:174]. In other words, it is a way of thinking about the government and the practices of the government. To him it is not \"a zone of critical-revolutionary study, but one that conceptually reproduces capitalist rule\" [1999:197] by asserting that some form of government (and power) will always be necessary to control and constitute society. By defining governmentality only in terms of the state, Kerr fails to take account of other forms of governance and the idea of mentalities of government in this broader sense.\n\nDean's understanding of the term incorporates both other forms of governance and the idea of mentalities of government, as well as Hunt and Wickham's, and Kerr's approaches to the term. In line with Hunt and Wickham's approach, Dean acknowledges that in a very narrow sense, governmentality can be used to describe the emergence of a government that saw that the object of governing power was to optimise, use and foster living individuals as members of a population [1999:19]. He also includes the idea of government rationalities, seeing governmentality as one way of looking at the practices of government. In addition to the above, he sees government as anything to do with conducting oneself or others. This is evident in his description of the word in his glossary: \"Governmentality: How we think about governing others and ourselves in a wide variety of contexts...\" [1999:212]. This reflects that the term government to Foucault meant not so much the political or administrative structures of the modern state as the way in which the conduct of individuals or of groups may be directed. To analyse government is to analyse those mechanisms that try to shape, sculpt, mobilise and work through the choices, desires, aspirations, needs, wants and lifestyles of individuals and groups [Dean, 1999:12].\n\nDean's main contribution to the definition of the term, however, comes from the way he breaks the term up into 'govern' 'mentality', or mentalities of governing—mentality being a mental disposition or outlook. This means that the concept of governmentality is not just a tool for thinking about government and governing but also incorporates how and what people who are governed think about the way they are governed. He defines thinking as a \"collective activity\" [1999:16], that is, the sum of the knowledge, beliefs and opinions held by those who are governed. He also raises the point that a mentality is not usually \"examined by those who inhabit it\" [1999:16]. This raises the interesting point that those who are governed may not understand the unnaturalness of both the way they live and the fact that they take this way of life for granted—that the same activity in which they engage in \"can be regarded as a different form of practice depending on the mentalities that invest it\" [1999:17]. Dean highlights another important feature of the concept of governmentality—its reflexivity. He explains:\n\"On the one hand, we govern others and ourselves according to what we take to be true about who we are, what aspects of our existence should be worked upon, how, with what means, and to what ends. On the other hand, the ways in which we govern and conduct ourselves give rise to different ways of producing truth. [1999:18]\n\nBy drawing attention to the 'how and why', Dean connects \"technologies of power\" [Lemke, 2001:191] to the concept of governmentality. According to Dean any definition of governmentality should incorporate all of Foucault's intended ideas. A complete definition of the term governmentality must include not only government in terms of the state, but government in terms of any \"conduct of conduct\" [Dean, 1999:10]. It must incorporate the idea of mentalities and the associations that go with that concept: that it is an attitude towards something, and that it is not usually understood \"from within its own perspective\" [1999:16], and that these mentalities are collective and part of a society's culture. It must also include an understanding of the ways in which conduct is governed, not just by governments, but also by ourselves and others.\n\nThe semantic linking of governing and mentalities in governmentality indicates that it is not possible to study technologies of power without an analysis of the mentality of rule underpinning them. The practice of going to the gym, expounded below, is a useful example because it shows how our choices, desires, aspirations, needs, wants and lifestyles have been mobilised and shaped by various technologies of power.\n\nA mentality of rule is any relatively systematic way of thinking about government. It delineates a discursive field in which the exercise of power is 'rationalised' [Lemke, 2001:191]. Thus Neo-liberalism is a mentality of rule because it represents a method of rationalising the exercise of government, a rationalisation that obeys the internal rule of maximum economy [Foucault, 1997:74]. Fukuyama [in Rose, 1999: 63] writes \"a liberal State is ultimately a limited State, with governmental activity strictly bounded by the sphere of individual liberty\". However, only a certain type of liberty, a certain way of understanding and exercising freedom is compatible with Neo-liberalism. If Neo-liberalist government is to fully realize its goals, individuals must come to recognize and act upon themselves as both free and responsible [Rose, 1999:68]. Thus Neo-liberalism must work to create the social reality that it proposes already exists. For as Lemke states, a mentality of government \"is not pure, neutral knowledge that simply re-presents the governing reality\" [Lemke, 2001:191] instead, Neo-liberalism constitutes an attempt to link a reduction in state welfare services and security systems to the increasing call for subjects to become free, enterprising, autonomous individuals. It can then begin to govern its subjects, not through intrusive state bureaucracies backed with legal powers, the imposition of moral standards under a religious mandate, but through structuring the possible field of action in which they govern themselves, to govern them through their freedom. Through the transformation of subjects with duties and obligations, into individuals, with rights and freedoms, modern individuals are not merely 'free to choose' but obliged to be free, \"to understand and enact their lives in terms of choice\" [Rose, 1999:87]. This freedom is a different freedom to that offered in the past. It is a freedom to realize our potential and our dreams through reshaping the way in which we conduct our lives.\n\nCartographic mapping has historically been a key strategy of governmentality. Harley, drawing on Foucault, affirms that State-produced maps \"extend and reinforce the legal statutes, territorial imperatives, and values stemming from the exercise of political power\". Typically, State-led mapping conforms to Bentham's concept of a panopticon, in which 'the one views the many'. From a Foucauldian vantage point, this was the blueprint for disciplinary power.\n\nThrough processes of neoliberalism, the State has \"hollowed out\" some of its cartographic responsibilities and delegated power to individuals who are at a lower geographical scale. 'People's cartography' is believed to deliver a more democratic spatial governance than traditional top-down State-distribution of cartographic knowledge. Thus subverting Harley's theory that mapping is uniquely a source of power for the powerful. Joyce challenges Foucauldian notions of Panopticism, contending that neoliberal governmentality is more adequately conceptualised by an omniopticon - 'the many surveilling the many'. Collaborative mapping initiatives utilising GPS technology are arguably omniopticons, with the ability to reverse the panoptic gaze.\n\nThrough our freedom, particular self-governing capabilities can be installed in order to bring our own ways of conducting and evaluating ourselves into alignment with political objectives [Rose, 1996:155]. These capabilities are enterprise and autonomy. Enterprise here designates an array of rules for the conduct of one's everyday existence: energy, initiative, ambition, calculation, and personal responsibility. The enterprising self will make an enterprise of its life, seek to maximize its own human capital, project itself a future, and seek to shape life in order to become what it wishes to be. The enterprising self is thus both an active self and a calculating self, a self that calculates about itself and that acts upon itself in order to better itself [Rose, 1996:154]. Autonomy is about taking control of our undertakings, defining our goals, and planning to achieve our needs through our own powers [Rose, 1996:159]. The autonomy of the self is thus not the eternal antithesis of political power, but one of the objectives and instruments of modern mentalities for the conduct of conduct [Rose, 1996:155].\n\nThese three qualities: freedom, enterprise and autonomy are embodied in the practice of going to the gym. It is our choice to go the gym, our choice which gym to go to. By going to the gym we are working on ourselves, on our body shape and our physical fitness. We are giving ourselves qualities to help us perform better than others in life, whether to attract a better mate than others, or to be able to work more efficiently, more effectively and for longer without running out of steam to give us an advantage over our competitors. When we go to the gym, we go through our own discipline, on our own timetable, to reach our own goals. We design and act out our routine by ourselves. We do not need the ideas or support of a team, it is our self that makes it possible. The practice of going to the gym, of being free, enterprising, autonomous, is imbued with particular technologies of power.\n\nTechnologies of power are those \"technologies imbued with aspirations for the shaping of conduct in the hope of producing certain desired effects and averting certain undesired ones\" [Rose, 1999:52]. The two main groups of technologies of power are technologies of the self, and technologies of the market. Foucault defined technologies of the self as techniques that allow individuals to effect by their own means a certain number of operations on their own bodies, minds, souls, and lifestyle, so as to transform themselves in order to attain a certain state of happiness, and quality of life. Technologies of the market are those technologies based around the buying and selling of goods that enable us to define who we are, or want to be. These two technologies are not always completely distinct, as both borrow bits of each other from time to time.\n\nTechnologies of the self refer to the practices and strategies by which individuals represent to themselves their own ethical self-understanding. One of the main features of technologies of self is that of expertise. Expertise has three important aspects. First, its grounding of authority in a claim to scientificity and objectivity creates distance between self-regulation and the state that is necessary with liberal democracies. Second, expertise can \"mobilise and be mobilised within political argument in distinctive ways, producing a new relationship between knowledge and government. Expertise comes to be accorded a particular role in the formulation of programs of government and in the technologies that seek to give them effect\" [Rose, 1996:156]. Third, expertise operates through a relationship with the self-regulating abilities of individuals. The plausibility inherent in a claim to scientificity binds \"subjectivity to truth and subjects to experts\" [Rose, 1996:156]. Expertise works through a logic of choice, through a transformation of the ways in which individuals constitute themselves, through \"inculcating desires for self-development that expertise itself can guide and through claims to be able to allay the anxieties generated when the actuality of life fails to live up to its image [Rose, 1999:88].\n\nThe technologies of the self involved in the practice of, for example, going to the gym are the: technology of responsibilisation, technology of healthism, technology of normalisation and technology of self-esteem.\n\nIn line with its desire to reduce the scope of government (e.g. welfare) Neo-liberalism characteristically develops indirect techniques for leading and controlling individuals without being responsible for them. The main mechanism is through the technology of responsibilisation. This entails subjects becoming responsibilised by making them see social risks such as illness, unemployment, poverty, etc. not as the responsibility of the state, but actually lying in the domain for which the individual is responsible and transforming it into a problem of 'self-care' [Lemke, 2001:201] and of 'consumption'. The practice of going to the gym can be seen as a result of responsibilisation, our responsibility to remain free of illness so as to be able to work and to care for our dependants (children, elderly parents etc.) This technology somewhat overlaps with the technology of healthism.\n\nHealthism links the \"public objectives for the good health and good order of the social body with the desire of individuals for health and well-being\" [Rose, 1999:74]. Healthy bodies and hygienic homes may still be objectives of the state, but it no longer seeks to discipline, instruct, moralise or threaten us into compliance. Rather \"individuals are addressed on the assumption that they want to be healthy and enjoined to freely seek out the ways of living most likely to promote their own health\" [Rose, 1999:86-87] such as going to the gym. However while the technology of responsibilisation may be argued to be a calculated technique of the state, the wave of Healthism is less likely to be a consequence of state planning, but arising out of the newer social sciences such as nutrition and human movement. Healthism assigns, as do most technologies of the self, a key role to experts. For it is experts who can tell us how to conduct ourselves in terms of safe, precise techniques to improve cardiovascular fitness, muscle strength, and overall health. The borrowing from technologies of the market by technologies of the self can be clearly seen in the area of healthism. The idea of health, the goal of being healthy, the joys brought by good health and the ways of achieving it are advertised to us in the same manner as goods and services are marketed by sales people. By adhering to the principles of healthism, our personal goals are aligned with political goals and we are thus rendered governable.\n\nAnother technology of power arising from the social sciences is that of normalisation. The technology of norms was given a push by the new methods of measuring population. A norm is that \"which is socially worthy, statistically average, scientifically healthy and personally desirable\". The important aspect of normality, is that while the norm is natural, those who wish to achieve normality will do so by working on themselves, controlling their impulses in everyday conduct and habits, and inculcating norms of conduct into their children, under the guidance of others. Norms are enforced through the calculated administration of shame. Shame entails an anxiety over the exterior behaviour and appearance of the self, linked to an injunction to care for oneself in the name of achieving quality of life [Rose, 1999:73]. Norms are usually aligned with political goals, thus the norm would be fit, virile, energetic individuals, able to work, earn money, and spend it and thus sustain the economy. For instance, the practice of going to the gym allows one to achieve this 'normality'. Through shame we are governed into conforming with the goals of Neo-liberalism.\n\nSelf-esteem is a practical and productive technology linked to the technology of norms, which produces of certain kinds of selves. Self-esteem is a technology in the sense that it is a specialised knowledge of how to esteem ourselves to estimate, calculate, measure, evaluate, discipline, and to judge our selves. The 'self-esteem' approach considers a wide variety of social problems to have their source in a lack of self-esteem on the part of the persons concerned. 'Self-esteem' thus has much more to do with self-assessment than with self-respect, as the self continuously has to be measured, judged and disciplined in order to gear personal 'empowerment' to collective yardsticks. These collective yardsticks are determined by the norms previously discussed. Self-esteem is a technology of self for \"evaluating and acting upon ourselves so that the police, the guards and the doctors do not have to do so\". By taking up the goal of self-esteem, we allow ourselves to be governable from a distance. The technology of self-esteem and other similar psychological technologies also borrow from technologies of the market, namely consumption. A huge variety of self-help books, tapes, videos and other paraphernalia are available for purchase by the individual.\n\nThe technologies of the market that underlie the practice of going to the gym can be described as the technology of desire, and the technology of identity through consumption. The technology of desire is a mechanism that induces in us desires that we work to satisfy. Marketers create wants and artificial needs in us through advertising goods, experiences and lifestyles that are tempting to us. These advertisements seek to convey the sense of individual satisfaction brought about by the purchase or use of this product. We come to desire these things and thus act in a manner that allows us to achieve these things, whether by working harder and earning more money or by employing technologies of the self to shape our lifestyle to the manner we desire . The borrowing of technologies of the self by technologies of the market extends even further in this case. Marketers use the knowledge created by psyche- discourses, especially psychological characteristics as the basis of their market segmentation. This allows them to appeal more effectively to each individual. Thus we are governed into purchasing commodities through our desire.\n\nThe technology of identity through consumption utilises the power of goods to shape identities. Each commodity is imbued with a particular meaning, which is reflected upon those who purchase it, illuminating the kind of person they are, or want to be. Consumption is portrayed as placing an individual within a certain form of life. The technology of identity through consumption can be seen in the choices that face the gym attendee. To go to an expensive gym because it demonstrates wealth/success or to go to a moderately priced gym so as to appear economical. The range of gym wear is extensive. Brand name to portray the abilities portrayed in its advertising, expensive to portray commitment, or cheap to portray your unconcern for other people's opinions. All of these choices of consumption are used to communicate our identity to others, and thus we are governed by marketers into choosing those products that identify with our identity.\n\nThese technologies of the market and of the self are the particular mechanisms whereby individuals are induced into becoming free, enterprising individuals who govern themselves and thus need only limited direct governance by the state. The implementation of these technologies is greatly assisted by experts from the social sciences. These experts operate a regime of the self, where success in life depends on our continual exercise of freedom, and where our life is understood, not in terms of fate or social status, but in terms of our success or failure in acquiring the skills and making the choices to actualise ourself. If we engage in the practice of going to the gym, we are undertaking an exercise in self-government. We do so by drawing upon certain forms of knowledge and expertise provided by gym instructors, health professionals, of the purveyors of the latest fitness fad. Depending on why we go to the gym, we may calculate number of calories burned, heart-rate, or muscle size. In all cases, we attend the gym for a specific set of reasons underpinned by the various technologies of the self and the market. The part of ourselves we seek to work upon, the means by which we do so, and who we hope to become, all vary according to the nature of the technology of power by which we are motivated [Dean, 1999:17]. All of these various reasons and technologies are underpinned by the mentality of government that seeks to transform us into a free, enterprising, autonomous individual: Neo-liberalism. Furthermore, Neo-liberalism seeks to create and disseminate definitions of freedom, autonomy and what it means to be enterprising that re-create forms of behavior amenable to neo-liberal goals.\n\nEcogovernmentality (or eco-governmentality) is the application of Foucault's concepts of biopower and governmentality to the analysis of the regulation of social interactions with the natural world. Timothy W. Luke theorized this as environmentality and green governmentality. Ecogovernmentality began in the mid-1990s with a small body of theorists (Luke, Darier, and Rutherford) the literature on ecogovernmentality grew as a response to the perceived lack of Foucauldian analysis of environmentalism and in environmental studies.\n\nFollowing Michel Foucault, writing on ecogovernmentality focuses on how government agencies, in combination with producers of expert knowledge, construct \"The Environment.\" This construction is viewed both in terms of the creation of an object of knowledge and a sphere within which certain types of intervention and management are created and deployed to further the government's larger aim of managing the lives of its constituents. This governmental management is dependent on the dissemination and internalization of knowledge/power among individual actors. This creates a decentered network of self-regulating elements whose interests become integrated with those of the State.\n\nAccording to Foucault, there are several instances where the Western, \"liberal art of government\" enters into a period of crisis, where the logic of ensuring freedom (which was defined against the background of risk or danger) necessitates actions \"which potentially risk producing exactly the opposite.\"\n\nThe inherently contradictory logics that lead to such contradictions are identified by Foucault as:\n\n1. Liberalism depends on the socialization of individuals to fear the constant presence of danger, e.g., public campaigns advocating savings banks, public hygiene, and disease prevention, the development of detective novels as a genre and of news stories of crime, and sexual anxieties surrounding \"degeneration.\"\n\n2. Liberal freedom requires disciplinary techniques that manage the individual's behaviour and everyday life so as to ensure productivity and the increase in profit through efficient labour, e.g., Bentham's Panopticon surveillance system. Liberalism claims to supervise the natural mechanisms of behaviour and production, but must intervene when it notices \"irregularities.\"\n\n3. Liberalism must force individuals to be free: control and intervention becomes the entire basis of freedom. Freedom must ultimately be manufactured by control rather than simply \"counterweighted\" by it.\n\nExamples of this contradictory logic which Foucault cites are the policies of the Keynesian welfare state under F.D. Roosevelt, the thought of the German liberals in the Freiburg school, and the thought of American libertarian economists such as the Chicago School which attempt to free individuals from the lack of freedom perceived to exist under socialism and fascism, but did so by using state interventionist models.\n\nThese governmental crises may be triggered by phenomena such as a discursive concern with increasing economic capital costs for the exercise of freedom, e.g., prices for purchasing resources, the need for excessive state coercion and interventionism to protect market freedoms, e.g., anti-trust and anti-monopoly legislation that leads to a \"legal strait-jacket\" for the state, local protests rejecting the disciplinary mechanisms of the market society and state. and finally, the destructive and wasteful effects of ineffective mechanisms for producing freedom.\n\nScholars have recently suggested that the concept of governmentality may be useful in explaining the operation of evidence-based health care and the internalization of clinical guidelines relating to best practice for patient populations, such as those developed by the American Agency for Health Care Research and Quality and the British National Institute for Health and Clinical Excellence (NICE). Recent research by Fischer and colleagues at the University of Oxford has renewed interest in Foucault's exploration of potential resistance to governmentality, and its application to health care, drawing on Foucault's recently published final lectures at the College de France.\n\nJeffreys and Sigley (2009) highlight that governmentality studies have focused on advanced liberal democracies, and preclude considerations of non-liberal forms of governmentality in both western and non-western contexts. Recent studies have broken new ground by applying Foucault's concept of governmentality to non-western and non-liberal settings, such as China. Jeffreys (2009) for example provides a collection of essay on China's approach to governance, development, education, the environment, community, religion, and sexual health where the notion of 'Chinese governmentally' is based not on the notion of 'freedom and liberty' as in the western tradition but rather, on a distinct rational approach to planning and administration. Such new studies thus use Foucault's Governmentalities to outline the nature of shifts in governance and contribute to emerging studies of governmentality in non-western contexts.\n\n\nhttp://www.inderscience.com/info/inarticle.php?artid=67421\nhttps://eurasianpublications.com/Eurasian-Journal-of-Economics-and-Finance/Vol.4-No.2-2016.aspx\nhttps://eurasianpublications.com/Eurasian-Journal-of-Economics-and-Finance/Vol.4-No.2-2016.aspx\n"}
{"id": "175996", "url": "https://en.wikipedia.org/wiki?curid=175996", "title": "Grey goo", "text": "Grey goo\n\nGrey goo (also spelled gray goo) is a hypothetical end-of-the-world scenario involving molecular nanotechnology in which out-of-control self-replicating robots consume all biomass on Earth while building more of themselves, a scenario that has been called \"ecophagy\" (\"eating the environment\", more literally \"eating the habitation\"). The original idea assumed machines were designed to have this capability, while popularizations have assumed that machines might somehow gain this capability by accident.\n\nSelf-replicating machines of the macroscopic variety were originally described by mathematician John von Neumann, and are sometimes referred to as von Neumann machines or clanking replicators.\nThe term \"gray goo\" was coined by nanotechnology pioneer Eric Drexler in his 1986 book \"Engines of Creation\". In 2004 he stated, \"I wish I had never used the term 'gray goo'.\" \"Engines of Creation\" mentions \"gray goo\" in two paragraphs and a note, while the popularized idea of gray goo was first publicized in a mass-circulation magazine, \"Omni\", in November 1986.\n\nThe term was first used by molecular nanotechnology pioneer Eric Drexler in his book \"Engines of Creation\" (1986). In Chapter 4, \"Engines Of Abundance\", Drexler illustrates both exponential growth and inherent limits (not gray goo) by describing nanomachines that can function only if given special raw materials:\n\nImagine such a replicator floating in a bottle of chemicals, making copies of itself...the first replicator assembles a copy in one thousand seconds, the two replicators then build two more in the next thousand seconds, the four build another four, and the eight build another eight. At the end of ten hours, there are not thirty-six new replicators, but over 68 billion. In less than a day, they would weigh a ton; in less than two days, they would outweigh the Earth; in another four hours, they would exceed the mass of the Sun and all the planets combined — if the bottle of chemicals hadn't run dry long before.\n\nAccording to Drexler, the term was popularized by an article in science fiction magazine \"Omni\", which also popularized the term \"nanotechnology\" in the same issue. Drexler says arms control is a far greater issue than grey goo \"nanobugs\".\n\nIn a History Channel broadcast, a contrasting idea (a kind of gray goo) is referred to in a futuristic doomsday scenario:\n\"In a common practice, billions of nanobots are released to clean up an oil spill off the coast of Louisiana. However, due to a programming error, the nanobots devour all carbon based objects, instead of just the hydrocarbons of the oil. The nanobots destroy everything, all the while, replicating themselves. Within days, the planet is turned to dust.\"\n\nDrexler describes gray goo in Chapter 11 of \"Engines of Creation\":\n\nEarly assembler-based replicators could beat the most advanced modern organisms. 'Plants' with 'leaves' no more efficient than today's solar cells could out-compete real plants, crowding the biosphere with an inedible foliage. Tough, omnivorous 'bacteria' could out-compete real bacteria: they could spread like blowing pollen, replicate swiftly, and reduce the biosphere to dust in a matter of days. Dangerous replicators could easily be too tough, small, and rapidly spreading to stop — at least if we made no preparation. We have trouble enough controlling viruses and fruit flies.\n\nDrexler notes that the geometric growth made possible by self-replication is inherently limited by the availability of suitable raw materials.\n\nDrexler used the term \"gray goo\" not to indicate color or texture, but to emphasize the difference between \"superiority\" in terms of human values and \"superiority\" in terms of competitive success:\n\nThough masses of uncontrolled replicators need not be grey or gooey, the term \"grey goo\" emphasizes that replicators able to obliterate life might be less inspiring than a single species of crabgrass. They might be \"superior\" in an evolutionary sense, but this need not make them valuable.\nBill Joy, one of the founders of Sun Microsystems, discussed some of the problems with pursuing this technology in his now-famous 2000 article in \"Wired\" magazine, titled \"Why the Future Doesn't Need Us\". In direct response to Joy's concerns, the first quantitative technical analysis of the ecophagy scenario was published in 2000 by nanomedicine pioneer Robert Freitas.\n\nDrexler more recently conceded that there is no need to build anything that even resembles a potential runaway replicator. This would avoid the problem entirely. In a paper in the journal \"Nanotechnology\", he argues that self-replicating machines are needlessly complex and inefficient. His 1992 technical book on advanced nanotechnologies \"Nanosystems: Molecular Machinery, Manufacturing, and Computation\" describes manufacturing systems that are desktop-scale factories with specialized machines in fixed locations and conveyor belts to move parts from place to place. None of these measures would prevent a party from creating a weaponized grey goo, were such a thing possible.\n\nPrince Charles called upon the British Royal Society to investigate the \"enormous environmental and social risks\" of nanotechnology in a planned report, leading to much media commentary on grey goo. The Royal Society's report on nanoscience was released on 29 July 2004, and declared the possibility of self-replicating machines to lie too far in the future to be of concern to regulators.\n\nMore recent analysis in the paper titled \"Safe Exponential Manufacturing\" from the Institute of Physics (co-written by Chris Phoenix, Director of Research of the Center for Responsible Nanotechnology, and Eric Drexler), shows that the danger of grey goo is far less likely than originally thought. However, other long-term major risks to society and the environment from nanotechnology have been identified. Drexler has made a somewhat public effort to retract his grey goo hypothesis, in an effort to focus the debate on more realistic threats associated with knowledge-enabled nanoterrorism and other misuses.\n\nIn \"Safe Exponential Manufacturing\", which was published in a 2004 issue of \"Nanotechnology\", it was suggested that creating manufacturing systems with the ability to self-replicate by the use of their own energy sources would not be needed. The Foresight Institute also recommended embedding controls in the molecular machines. These controls would be able to prevent anyone from purposely abusing nanotechnology, and therefore avoid the grey goo scenario.\n\nGrey goo is a useful construct for considering low-probability, high-impact outcomes from emerging technologies. Thus, it is a useful tool in the ethics of technology. Daniel A. Vallero applied it as a worst-case scenario thought experiment for technologists contemplating possible risks from advancing a technology. This requires that a decision tree or event tree include even extremely low probability events if such events may have an extremely negative and irreversible consequence, i.e. application of the precautionary principle. Dianne Irving admonishes that \"any error in science will have a rippling effect...\" Vallero adapted this reference to chaos theory to emerging technologies, wherein slight permutations of initial conditions can lead to unforeseen and profoundly negative downstream effects, for which the technologist and the new technology's proponents must be held accountable.\n\n\n"}
{"id": "98099", "url": "https://en.wikipedia.org/wiki?curid=98099", "title": "Hieratic", "text": "Hieratic\n\nHieratic (; ) is a cursive writing system used for Ancient Egyptian, and the principal script used to write that language from its development in the 3rd millennium BCE until the rise of Demotic in the mid 1st millennium BCE. It was primarily written in ink with a reed pen on papyrus.\n\nIn the second century, the term \"hieratic\" was first used by Clement of Alexandria. It derives from the Greek for \"priestly writing\" (), as at that time, hieratic was used only for religious texts and literature, as had been the case for the previous eight and a half centuries.\n\n\"Hieratic\" can also be an adjective meaning \"[o]f or associated with sacred persons or offices; sacerdotal.\"\n\nHieratic developed as a cursive form of hieroglyphic script in the Naqada III period, roughly 3200–3000 BCE. Although handwritten printed hieroglyphs continued to be used in some formal situations, such as manuscripts of the Egyptian Book of the Dead, noncursive hieroglyphic script became largely restricted to monumental inscriptions. \n\nHieratic was used into the Hellenistic period. Around 660 BCE, the even more-cursive Demotic script arose in northern Egypt and replaced hieratic and the southern shorthand known as abnormal hieratic for most mundane writing, such as personal letters and mercantile documents. Hieratic continued to be used by the priestly class for religious texts and literature into the third century BCE.\n\nThrough most of its long history, hieratic was used for writing administrative documents, accounts, legal texts, and letters, as well as mathematical, medical, literary, and religious texts. During the Græco-Roman period, when Demotic (and later Greek) had become the chief administrative script, hieratic was limited primarily to religious texts. In general, hieratic was much more important than hieroglyphs throughout Egypt's history, being the script used in daily life. It was also the writing system first taught to students, knowledge of hieroglyphs being limited to a small minority who were given additional training. In fact, it is often possible to detect errors in hieroglyphic texts that came about due to a misunderstanding of an original hieratic text.\n\nMost often, hieratic script was written in ink with a reed brush on papyrus, wood, stone or pottery ostraca. Thousands of limestone ostraca have been found at the site of Deir al-Madinah, revealing an intimate picture of the lives of common Egyptian workmen. Besides papyrus, stone, ceramic shards, and wood, there are hieratic texts on leather rolls, though few have survived. There are also hieratic texts written on cloth, especially on linen used in mummification. There are some hieratic texts inscribed on stone, a variety known as lapidary hieratic; these are particularly common on stelae from the 22nd Dynasty.\n\nDuring the late 6th Dynasty, hieratic was sometimes incised into mud tablets with a stylus, similar to cuneiform. About five hundred of these tablets have been discovered in the governor's palace at Ayn Asil (Balat), and a single example was discovered from the site of Ayn al-Gazzarin, both in the Dakhla Oasis. At the time the tablets were made, Dakhla was located far from centers of papyrus production. These tablets record inventories, name lists, accounts, and approximately fifty letters. Of the letters, many are internal letters that were circulated within the palace and the local settlement, but others were sent from other villages in the oasis to the governor.\n\nHieratic script, unlike inscriptional and manuscript hieroglyphs, reads from right to left. Initially, hieratic could be written in either columns or horizontal lines, but after the 12th Dynasty (specifically during the reign of Amenemhat III), horizontal writing became the standard.\n\nHieratic is noted for its cursive nature and use of ligatures for a number of characters. Hieratic script also uses a much more standardized orthography than hieroglyphs; texts written in the latter often had to take into account extra-textual concerns, such as decorative uses and religious concerns that were not present in, say, a tax receipt. There are also some signs that are unique to hieratic, though Egyptologists have invented equivalent hieroglyphic forms for hieroglyphic transcriptions and typesetting. Several hieratic characters have diacritical additions so that similar signs could easily be distinguished. \n\nHieratic is often present in any given period in two forms, a highly ligatured, cursive script used for administrative documents, and a broad uncial bookhand used for literary, scientific, and religious texts. These two forms can often be significantly different from one another. Letters, in particular, used very cursive forms for quick writing, often with large numbers of abbreviations for formulaic phrases, similar to shorthand.\n\nA highly cursive form of hieratic known as \"Abnormal Hieratic\" was used in the Theban area from the second half of the 20th dynasty until the beginning of the 26th Dynasty. It derives from the script of Upper Egyptian administrative documents and was used primarily for legal texts, land leases, letters, and other texts. This type of writing was superseded by Demotic—a Lower Egyptian scribal tradition—during the 26th Dynasty, when Demotic was established as a standard administrative script throughout a re-unified Egypt.\n\nHieratic has had influence on a number of other writing systems. The most obvious is that on Demotic, its direct descendant. Related to this are the Demotic signs of the Meroitic script and the borrowed Demotic characters used in the Coptic alphabet and Old Nubian.\n\nOutside of the Nile Valley, many of the signs used in the Byblos syllabary were apparently borrowed from Old Kingdom hieratic signs. It is also known that early Hebrew used hieratic numerals.\n\nThe Unicode standard considers hieratic characters' font variants of the Egyptian hieroglyphs, and the two scripts have been unified. Hieroglyphs themselves were added to the Unicode Standard in October 2009 with the release of version 5.2. To date, there is no known Unicode font with hieratic.\n\n\n"}
{"id": "20148816", "url": "https://en.wikipedia.org/wiki?curid=20148816", "title": "Higher-order singular value decomposition", "text": "Higher-order singular value decomposition\n\nIn multilinear algebra, the higher-order singular value decomposition (HOSVD) of a tensor is a specific orthogonal Tucker decomposition. It may be regarded as one generalization of the matrix singular value decomposition. The HOSVD has applications in computer graphics, machine learning, scientific computing, and signal processing. Some key ingredients of the HOSVD can be traced as far back as F. L. Hitchcock in 1928, but it was L. R. Tucker who developed for third-order tensors the general Tucker decomposition in the 1960s, including the HOSVD. The HOSVD as decomposition in its own right was further advocated by L. De Lathauwer \"et al.\" in 2000.\n\nAs the HOSVD was studied in many scientific fields, it is sometimes historically referred to as \"multilinear singular value decomposition\", \"m-mode SVD\", or \"cube SVD,\" and sometimes it is incorrectly identified with a Tucker decomposition.\n\nFor the purpose of this article, the abstract tensor formula_1 is assumed to be given in coordinates with respect to some basis as a multidimensional array, also denoted by formula_1, in formula_3, where \"d\" is the order of the tensor and formula_4 is either formula_5 or formula_6.\n\nLet formula_7be a unitary matrix containing a basis of the left singular vectors of the standard factor-\"k\" flattening formula_8 of formula_1 such that the \"j\"th column formula_10 of formula_11 corresponds to the \"j\"th largest singular value of formula_8. Observe that the factor matrix formula_11 does not depend on the particular freedom of choice in the definition of the standard factor-\"k\" flattening. By the properties of the multilinear multiplication, we haveformula_14where formula_15 denotes the conjugate transpose. The second equality is because the formula_11's are unitary matrices. Define now the core tensorformula_17Then, the HOSVD of formula_1 is the decompositionformula_19The above construction shows that every tensor has a HOSVD.\n\nAs in the case of the compact singular value decomposition of a matrix, it is also possible to consider a compact HOSVD, which is very useful in applications.\n\nAssume that formula_20 is a matrix with unitary columns containing a basis of the left singular vectors corresponding to the nonzero singular values of the standard factor-\"k\" flattening formula_8 of formula_1. Let the columns of formula_11 be sorted such that the \"j\"th column formula_10 of formula_11 corresponds to the \"j\"th largest nonzero singular value of formula_8. Since the columns of formula_11 form a basis for the image of formula_8, we haveformula_29where the first equality is due to the properties of orthogonal projections (in the Hermitian inner product) and the last equality is due to the properties of multilinear multiplication. As flattenings are bijective maps and the above formula is valid for all formula_30, we find as before thatformula_31where the core tensor formula_32 is now of size formula_33.\n\nThe tuple formula_34 where formula_35 is nowadays called the multilinear rank of formula_1. By definition, every tensor has a unique multilinear rank, and its components are bounded by formula_37. Not all tuples in formula_38 are multilinear ranks. In particular, it is known that formula_39 must hold.\n\nThe compact HOSVD is a rank-revealing factorization in the sense that the dimensions of its core tensor correspond with the components of the multilinear rank of the tensor.\n\nThe following geometric interpretation is valid for both the full and compact HOSVD. Let formula_40 be the multilinear rank of the tensor formula_1. Since formula_42 is a multidimensional array, we can expand it as followsformula_43where formula_44 is the \"j\"th standard basis vector of formula_45. By definition of the multilinear multiplication, it holds thatformula_46where the formula_10 are the columns of formula_20. It is easy to verify that formula_49 is an orthonormal set of tensors. This means that the HOSVD can be interpreted as a way to express the tensor formula_1 with respect to a specifically chosen orthonormal basis formula_51 with the coefficients given as the multidimensional array formula_32.\n\nLet formula_53, where formula_4 is either formula_5 or formula_6, be a tensor of multilinear rank formula_40.\n\nThe classic strategy for computing a compact HOSVD was introduced in 1966 by L. R. Tucker and further advocated by L. De Lathauwer \"et al.\"; it is based on the definition of the decomposition. The next three steps are to be performed:\n\n\nA strategy that is significantly faster when some or all formula_64 consists of interlacing the computation of the core tensor and the factor matrices, as follows:\n\n\nIn applications, such as those mentioned below, a common problem consists of approximating a given tensor formula_72 by one of low multilinear rank. Formally, if formula_73 denotes the multilinear rank of formula_74, then the nonlinear non-convex formula_75-optimization problem isformula_76where formula_77 with formula_78, is a target multilinear rank that is assumed to be given, and where the norm is the Frobenius norm.\n\nBased on the foregoing algorithms for computing a compact HOSVD, a natural idea for trying to solve this optimization problem is to truncate the (compact) SVD in step 2 of either the classic or the interlaced computation. A classically truncated HOSVD is obtained by replacing step 2 in the classic computation by\nwhile a sequentially truncated HOSVD (or successively truncated HOSVD) is obtained by replacing step 2 in the interlaced computation by\nUnfortunately, neither of these strategies results in an optimal solution of the best low multilinear rank optimization problem, contrary to the matrix case where the Eckart-Young theorem holds. However, both the classically and sequentially truncated HOSVD result in a quasi-optimal solution: if formula_87 denotes the classically or sequentially truncated HOSVD and formula_88 denotes the optimal solution to the best low multilinear rank approximation problem, thenformula_89in practice this means that if there exists an optimal solution with a small error, then a truncated HOSVD will for many intended purposes also yield a sufficiently good solution.\n\nThe HOSVD is most commonly applied to the extraction of relevant information from multi-way arrays.\n\nCirca 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the result of several causal factors of data formation, and are well suited for multi-modal data tensor analysis. The power of the tensor framework was showcased in a visually and mathematically compelling manner by decomposing and representing an image in terms of its causal factors of data formation, in the context of Human Motion Signatures, face recognition—TensorFaces and computer graphics—TensorTextures.\n\nThe HOSVD has been successfully applied to signal processing and big data, e.g., in genomic signal processing. These applications also inspired a higher-order GSVD (HO GSVD) and a tensor GSVD.\n\nA combination of HOSVD and SVD also has been applied for real-time event detection from complex data streams (multivariate data with space and time dimensions) in disease surveillance.\n\nIt is also used in tensor product model transformation-based controller design. In multilinear subspace learning, it was applied to modeling tensor objects for gait recognition.\n\nThe concept of HOSVD was carried over to functions by Baranyi and Yam via the TP model transformation. This extension led to the definition of the HOSVD-based canonical form of tensor product functions and Linear Parameter Varying system models and to convex hull manipulation based control optimization theory, see TP model transformation in control theories.\n\nHOSVD was proposed to be applied to multi-view data analysis and was successfully applied to in silico drug discovery from gene expression.\n"}
{"id": "9132580", "url": "https://en.wikipedia.org/wiki?curid=9132580", "title": "Institute of All Nations for Advanced Studies", "text": "Institute of All Nations for Advanced Studies\n\nThe Institute of All Nations for Advanced Studies, Inc. (IAN) was established by Dr. Rama C. Mohanty and others in 1964. Mohanty, IAN’s General Secretary, is a Professor of Physics at Southern University in Baton Rouge, Louisiana. He moved to found the organization after being deeply affected by the brutal murder of thousands of innocent people, including children and women, as a result of religious and communal rioting in his native India.\n\nConvinced that the promotion of inner peace within each individual is vital to the establishment of meaningful world peace, IAN seeks to become:\n\n\nPermanently chartered as a private, nonprofit educational 501(c)(3) tax-exempt corporation in New York, the Institute began under the co-chairmanship of former Supreme Court of the United States Justice Arthur (Joseph) Goldberg, and former United Nations Ambassador Arthur S. Lall's niece, Dr. Anurita Kapur, M.D. a neurosurgeon in New York. Since 1998, IAM has promoted World Peace Day internationally on 1 October to raise mass awareness against violence at home and abroad as well as honor those working to promote peace.\n"}
{"id": "4974325", "url": "https://en.wikipedia.org/wiki?curid=4974325", "title": "International Decade for a Culture of Peace and Non-Violence for the Children of the World", "text": "International Decade for a Culture of Peace and Non-Violence for the Children of the World\n\nThe United Nations General Assembly proclaimed the first decade of the 21st century and the third millennium, the years 2001 to 2010, as the International Decade for a Culture of Peace and Non-Violence for the Children of the World. This followed resolutions about the International Year for the Culture of Peace and the International Day of Peace.\n\nThe action plan for the Decade proposes eight spheres of activities in which to work for the promotion of the Culture of Peace:\n\nOn 29 November 2000, the General Assembly decided to entrust UNESCO with the coordination \"of the activities of the organizations of the United Nations system to promote a culture of peace, as well as liaison with other organizations concerned in this matter.\". In 2009, the United Nations General Assembly, in a resolution on the International Decade for the Promotion of a Culture of Peace and Non-violence for the Children of the World, endorsed the ongoing work of the Culture of Peace News Network as an important part of the ongoing commitment of the UN to a culture of peace.\n\nNGOs and other civil society organizations were urged to contribute to a global movement for a culture of peace according to the Declaration and Programme of Action for a Culture of Peace, adopted by the UN General Assembly in 1999. In response, a mid-term report, submitted to the United Nations in 2005, included information from 700 organizations. In 2010 at the conclusion of the Decade, the Report from the Civil Society was submitted to the United Nations with detailed information and photographs about the actions of 1,054 organizations from over 100 countries. This information is the tip of an even larger iceberg, as indicated by the many partnerships listed by participating organizations which number in the many thousands. \n\nSome national NGOs coalitions promoting the Decade were established in several countries, including Austria, France, Italy and the Netherlands. These national coalitions along with international organizations decided to found the International Coalition for the Decade in June 2003.\n\nRelated to this UN Decade, in December 1998, the World Council of Churches adopted, at its 8th Assembly, in Harare (Zimbabwe), the Decade to Overcome Violence as one of its programmes for the decade 2000-2009.\n\nSue Gilmurray from the Anglican Pacifist Fellowship composed and recorded a cycle of songs to commemorate the decade. The songs tackle a variety of subjects, and they include a critique of the use of child soldiers and an attack on the promotion and marketing of violent films and games towards children.\n\n\n"}
{"id": "15045125", "url": "https://en.wikipedia.org/wiki?curid=15045125", "title": "Khom script", "text": "Khom script\n\nThere are two scripts in Southeast Asia called Khom script. This article describes the obscure script from Laos that Sidwell (2008) and Jacq (2001) have described under the name \"Khom script\". \nThis \"Khom\" script is unique in the way it divides syllables. It has one set of symbols to represent initial consonants. Then it uses another set of symbols to represent the final vowel and consonant. That is, unlike a syllabary (which has unitary symbols for consonants followed by vowels (CV), with maybe a vowelless consonant following), Khom script divides the syllable as C-VC, rather than CV-C (Sidwell 2008). This is unique, not readily accommodated in any of the existing typologies of scripts. \n\nThe script was invented by Ong Kommadam, a leader in the rebellion against the French colonizers. He began using the script as early as 1924, but its use did not continue after his death in 1936. Ong Kommadam claimed supernatural titles, including “King of the Khom”, “God of the Khom”, “Sky God of the Khom” (Sidwell 2008:17). The script was linked to his divine claims, messages written in this script carried mystical power as well as meaning.\n\nThe script was revealed to outsiders by old, dying insiders on two separate occasions (Sidwell 2008). Some of the symbols resemble those from nearby scripts, but many are original. The script contains more than 300 characters, in order to fully encompass the rich morphology of the Bahnaric languages.\n\nThe term \"Khom\" is also used to refer to the Ancient Khmer lettering used in Thailand's Buddhist temples to inscribe sacred Buddhist mantras and prayers, but that is an entirely different script.\n\nThe script could be used to write various Bahnaric languages, including the Laven dialects, especially Jru', and languages like Alak, Ong Kommadam's native language.\n\n"}
{"id": "246043", "url": "https://en.wikipedia.org/wiki?curid=246043", "title": "Kindness", "text": "Kindness\n\nKindness is a behavior marked by ethical characteristics, a pleasant disposition, and concern and consideration for others. It is considered a virtue, and is recognized as a value in many cultures and religions (see ethics in religion). \n\nIn Book II of \"Rhetoric\", Aristotle defines kindness as \"helpfulness towards someone in need, not in return for anything, nor for the advantage of the helper himself, but for that of the person helped\". Nietzsche considered kindness and love to be the \"most curative herbs and agents in human intercourse\". Kindness is considered to be one of the Knightly Virtues. In Meher Baba's teachings, God is synonymous with kindness: \"God is so kind that it is impossible to imagine His unbounded kindness!\"\n\nIn human mating choice, studies suggest that both men and women value kindness and intelligence in their prospective mates, along with physical appearance, attractiveness, social status, and age.\n\nA \"nice guy\" is an informal and usually stereotypical term for an (often young) adult male who portrays himself as gentle, compassionate, sensitive, and/or vulnerable. The term is used both positively and negatively. When used positively, and particularly when used as a preference or description by someone else, it is intended to imply a male who puts the needs of others before his own, avoids confrontations, does favors, gives emotional support, tries to stay out of trouble, and generally acts nicely towards others. In the context of a relationship, it may also refer to traits of honesty, loyalty, romanticism, courtesy and respect. When used negatively, a nice guy implies a male who is unassertive, does not express his true feelings and, in the context of dating (in which the term is often used), uses acts of friendship with the unstated aim of progressing to a romantic or sexual relationship.\n\nBased on experiments at Yale University using games with babies, some studies concluded that kindness is inherent to human beings. There are similar studies about the root of empathy in babies.\n\n\nBased on the novel of the same name written in 1999 by author Catherine Ryan Hyde, the motion picture \"Pay it Forward\", which starred Kevin Spacey, Helen Hunt, Haley Joel Osment and Jon Bon Jovi, illustrates the power one person can have to impact a chain reaction of kind deeds. The philosophy of Pay It Forward is that through acts of kindness among strangers, we all foster a more caring society. In the book and film, Reuben St. Clair, a social studies teacher in Atascadero, California, challenges his students to \"change the world\". One of his students, Trevor, takes the challenge to heart. He starts by showing kindness to a stranger which ripples further than he could have ever imagined.\n\nIn October 2011, Life Vest Inside posted a video called \"Kindness Boomerang\".It shows how one act of kindness passes seamlessly from one person to the next and boomerangs back to the person who set it into motion. Orly Wahba, Life Vest Inside Founder and Director of Kindness Boomerang explains that each scene was based on real life experiences she personally went through; moments of kindness that left a lasting impression on her life. Within several months after its release, Kindness Boomerang went viral; reaching over 20 million people globally and eventually landing Wahba spot on the TED2013 stage to speak about the power of kindness.\n\n\n"}
{"id": "51184343", "url": "https://en.wikipedia.org/wiki?curid=51184343", "title": "Knowledge inertia", "text": "Knowledge inertia\n\nKnowledge inertia (KI) is a concept in knowledge management. The term initially proposed by Liao (2002) constitutes a two dimensional model of knowledge inertia which incorporates experience inertia and learning inertia. Later, another dimension—the dimension of thinking inertia has been added based on the theoretical exploration of the existing concepts of experience inertia and learning inertia. One of the central problems in knowledge management related to organizational learning is to deal with “inertia”. Besides, individuals may also exhibit a natural tendency of inertia when facing problems during utilization of knowledge. Inertia in technical jargon means inactivity or torpor. Inertia in organizational learning context may be referred to as a slowdown in organizational learning-related activities. In fact, there are many other kinds of organizational inertia; e.g., innovation inertia, workforce inertia, productivity inertia, decision inertia, emotional inertia besides others that have different meanings in their own individual contexts. Some organization theorists have adopted the definition proposed by Liao (2002) to extend its further use in organizational learning studies.\n\nKnowledge inertia (KI) may be defined as a problem solving strategy using old, redundant, stagnant knowledge and past experience without recourse to new knowledge and experience. Inertia is a concept in physics that is used to explain the state of an object either remaining in stationary or uniform motion. Organizational theorists adopted this concept of inertia and applied it to different contexts which resulted in the emergence of diverse concepts—such as, for example, organizational inertia, consumer inertia, outsourcing inertia, and cognitive inertia. Some organization theorists have adopted the definition proposed by Liao (2002) to extend its further use in organizational learning studies. Not every instances of knowledge inertia result in gloomy of negative outcome: one study suggested that knowledge inertia could positively affect a firm's product innovation.\n\nKnowledge inertia stems from the use of routine problem solving procedures that involves the utilization of redundant, stagnant knowledge and past experience without any recourse to new knowledge and thinking processes. Different methodologies exist for diverse types of knowledge that could be applied to manage knowledge efficiently. Since KI is a component of knowledge management, it is essential to consider the circulation of various knowledge types in avoiding inertia. The theory of KI supposedly studies the extent to which an organization's ability on problem solving is inhibited. Numerous factors could be attributed as enablers or inhibitors of the abilities on problem solving of an individual or an organization. Knowledge inertia applicable in the context of problem solving, therefore, may require inputs from all these diverse knowledge types, or it may require learning, new thinking, and experience. Emergence of new ideas to supplement the existing knowledge and assimilation of the same could be of help in avoiding the use of stagnant, outdated information while attempting to solve problems.\n\n"}
{"id": "36701390", "url": "https://en.wikipedia.org/wiki?curid=36701390", "title": "Modular vehicle", "text": "Modular vehicle\n\nA modular vehicle is one in which substantial components of the vehicle are interchangeable. This modularity is intended to make repairs and maintenance easier, or to allow the vehicle to be reconfigured to suit different functions.\n\nAnother application of modular vehicle design is to enable the exchange of batteries in an electric vehicle.\n\nIn a modular electric vehicle, the power system, wheels and suspension can be contained in a single module or chassis. When the batteries need recharging, the vehicle's body is lifted off and placed onto a fresh power module. By using this \"Modular Vehicle\" system, the vehicle's batteries do not have to be removed or reinstalled, and their connections remain intact.\n\nThe world's first road-licensed quick-change modular electric vehicle, based on a patent awarded to Dr Gordon E Dower in 2000, was shown at the World Electric Vehicle Association 2003 Electric Vehicle Symposium EVS-20 in Long Beach, California, USA. \n\nDower described the vehicle's two parts as its motorized deck, shortened to \"Modek\", and its \"containing module\" or \"Ridon\". When attached to each other, the vehicle thus formed was dubbed the \"Ridek\". Mechanical connections between the modules for braking and steering automatically engage when the body is lowered on to the chassis.\nIn 2004, General Motors attempted to patent a modular vehicle called \"Autonomy\" but the attempt was unsuccessful because Dower’s patent already existed. \n\nA team at GM did, however, continue to work on Autonomy, which was intended to be powered by a hydrogen fuel cell. They unveiled a non-drivable version of their modular vehicle in January 2002 at the Detroit Auto Show. GM unveiled a drivable prototype, called Hy-wire at the Paris Auto Show in September, 2002. The name referred to the Hydrogen fuel and the \"drive-by-wire\" system that electronically connected the vehicle modules for steering, braking and controlling the 4 wheel motors. Hy-wire did not go into production.\n\nModular vehicles make it possible to use different types of bodies, e.g. sedan, sports car or pickup truck, on one standardized chassis.\n\nAlso, the modular chassis, with its batteries and motor, are relatively easy to work on, since there is no vehicle body to impede access.\n\n\n"}
{"id": "175918", "url": "https://en.wikipedia.org/wiki?curid=175918", "title": "Moksha", "text": "Moksha\n\nMoksha (; , \"\"), also called vimoksha, vimukti and mukti, is a term in Hinduism, Buddhism, Jainism and Sikhism which refers to various forms of emancipation, enlightenment, liberation, and release. In its soteriological and eschatological senses, it refers to freedom from \"saṃsāra\", the cycle of death and rebirth. In its epistemological and psychological senses, \"moksha\" refers to freedom from ignorance: self-realization, self-actualization and self-knowledge.\n\nIn Hindu traditions, \"moksha\" is a central concept and the utmost aim to be attained through three paths during human life; these three paths are \"dharma\" (virtuous, proper, moral life), \"artha\" (material prosperity, income security, means of life), and \"kama\" (pleasure, sensuality, emotional fulfillment). Together, these four concepts are called Puruṣārtha in Hinduism.\n\nIn some schools of Indian religions, \"moksha\" is considered equivalent to and used interchangeably with other terms such as \"vimoksha\", \"vimukti\", \"kaivalya\", \"apavarga\", \"mukti\", \"nihsreyasa\" and \"nirvana\". However, terms such as \"moksha\" and \"nirvana\" differ and mean different states between various schools of Hinduism, Buddhism and Jainism. The term nirvana is more common in Buddhism, while \"moksha\" is more prevalent in Hinduism.\n\n\"Moksha\" is derived from the root , ', which means free, let go, release, liberate. In Vedas and early Upanishads, the word , ' appears, which means to be set free or release, such as of a horse from its harness.\n\nThe definition and meaning of \"moksha\" varies between various schools of Indian religions. \"Moksha\" means freedom, liberation; from what and how is where the schools differ. \"Moksha\" is also a concept that means liberation from rebirth or \"saṃsāra\". This liberation can be attained while one is on earth (\"jivanmukti\"), or eschatologically (\"karmamukti\", \"videhamukti\"). Some Indian traditions have emphasized liberation on concrete, ethical action within the world. This liberation is an epistemological transformation that permits one to see the truth and reality behind the fog of ignorance.\n\n\"Moksha\" has been defined not merely as absence of suffering and release from bondage to \"saṃsāra\", various schools of Hinduism also explain the concept as presence of the state of \"paripurna-brahmanubhava\" (the experience of oneness with Brahman, the One Supreme Self), a state of knowledge, peace and bliss. For example, Vivekachudamani - an ancient book on \"moksha\", explains one of many meditative steps on the path to \"moksha\", as:\n\nMoksha has been defined in the book \"Sulightenment by Fathor\" \"Moksha is the State of Universe that having the liberation from the physical aspect. You liberate from the material aspects of life that you are in.\"\n\n\"Moksha\" is a concept associated with \"saṃsāra\" (birth-rebirth cycle). \"Samsara\" originated with religious movements in the first millennium BCE. These movements such as Buddhism, Jainism and new schools within Hinduism, saw human life as bondage to a repeated process of rebirth. This bondage to repeated rebirth and life, each life subject to injury, disease and aging, was seen as a cycle of suffering. By release from this cycle, the suffering involved in this cycle also ended. This release was called \"moksha\", \"nirvana\", \"kaivalya\", \"mukti\" and other terms in various Indian religious traditions.\n\nEschatological ideas evolved in Hinduism. In earliest Vedic literature, heaven and hell sufficed soteriological curiosities. Over time, the ancient scholars observed that people vary in the quality of virtuous or sinful life they lead, and began questioning how differences in each person's \"puṇya\" (merit, good deeds) or \"pāp\" (demerit, sin) as human beings affected their afterlife. This question led to the conception of an afterlife where the person stayed in heaven or hell, in proportion to their merit or demerit, then returned to earth and were reborn, the cycle continuing indefinitely. The rebirth idea ultimately flowered into the ideas of \"saṃsāra\", or transmigration - where one's balance sheet of \"karma\" determined one's rebirth. Along with this idea of \"saṃsāra\", the ancient scholars developed the concept of \"moksha\", as a state that released a person from the \"saṃsāra\" cycle. \"Moksha\" release in eschatological sense in these ancient literature of Hinduism, suggests van Buitenen, comes from self-knowledge and consciousness of oneness of supreme soul.\n\nScholars provide various explanations of the meaning of \"moksha\" in epistemological and psychological senses. For example, Deutsche sees \"moksha\" as transcendental consciousness, the perfect state of being, of self-realization, of freedom and of \"realizing the whole universe as the Self\".\n\n\"Moksha\" in Hinduism, suggests Klaus Klostermaier, implies a setting-free of hitherto fettered faculties, a removing of obstacles to an unrestricted life, permitting a person to be more truly a person in the full sense; the concept presumes an unused human potential of creativity, compassion and understanding which had been blocked and shut out. \"Moksha\" is more than liberation from a life-rebirth cycle of suffering (\"samsara\"); the Vedantic school separates this into two: \"jivanmukti\" (liberation in this life) and \"videhamukti\" (liberation after death). \"Moksha\" in this life includes psychological liberation from \"adhyasa\" (fears besetting one's life) and \"avidya\" (ignorance or anything that is not true knowledge).\n\nMany schools of Hinduism according to Daniel Ingalls, see \"moksha\" as a state of perfection. The concept was seen as a natural goal beyond \"dharma\". \"Moksha\", in the epics and ancient literature of Hinduism, is seen as achievable by the same techniques necessary to practice \"dharma\". Self-discipline is the path to \"dharma\", \"moksha\" is self-discipline that is so perfect that it becomes unconscious, second nature. \"Dharma\" is thus a means to \"moksha\".\n\nThe Samkhya school of Hinduism, for example, suggests that one of the paths to \"moksha\" is to magnify one's \"sattvam\". To magnify one's \"sattvam\", one must develop oneself where one's \"sattvam\" becomes one's instinctive nature. Many schools of Hinduism thus understood \"dharma\" and \"moksha\" as two points of a single journey of life, a journey for which the \"viaticum\" was discipline and self-training. Over time, these ideas about \"moksha\" were challenged.\n\n\"Dharma\" and \"moksha\", suggested Nagarjuna in the 2nd century, cannot be goals on the same journey. He pointed to the differences between the world we live in, and the freedom implied in the concept of \"moksha\". They are so different that \"dharma\" and \"moksha\" could not be intellectually related. \"Dharma\" requires worldly thought, \"moksha\" is unworldly understanding, a state of bliss. How can the worldly thought-process lead to unworldly understanding? asked Nagarjuna. Karl Potter explains the answer to this challenge as one of context and framework, the emergence of broader general principles of understanding from thought processes that are limited in one framework.\n\nAdi Shankara in the 8th century AD, like Nagarjuna earlier, examined the difference between the world one lives in and \"moksha\", a state of freedom and release one hopes for. Unlike Nagarjuna, Shankara considers the characteristics between the two. The world one lives in requires action as well as thought; our world, he suggests, is impossible without \"vyavahara\" (action and plurality). The world is interconnected, one object works on another, input is transformed into output, change is continuous and everywhere. \"Moksha\", suggests Shankara, is that final perfect, blissful state where there can be no change, where there can be no plurality of states. It has to be a state of thought and consciousness that excludes action. How can action-oriented techniques by which we attain the first three goals of man (\"kama\", \"artha\" and \"dharma\") be useful to attain the last goal, namely \"moksha\"?\n\nScholars suggest Shankara's challenge to the concept of \"moksha\" parallels those of Plotinus against the Gnostics, with one important difference: Plotinus accused the Gnostics of exchanging an anthropocentric set of virtues with a theocentric set in pursuit of salvation; Shankara challenged that the concept of \"moksha\" implied an exchange of anthropocentric set of virtues (\"dharma\") with a blissful state that has no need for values. Shankara goes on to suggest that anthropocentric virtues suffice.\n\nVaishnavism, one of the \"bhakti\" schools of Hinduism, is devoted to the worship of God, sings his name, anoints his image or idol, and has many sub-schools. Vaishnavas (followers of Vaishnavism) suggest that \"dharma\" and \"moksha\" cannot be two different or sequential goals or states of life.\nInstead, they suggest God should be kept in mind constantly to simultaneously achieve \"dharma\" and \"moksha\", so constantly that one comes to feel one cannot live without God's loving presence. This school emphasized love and adoration of God as the path to \"moksha\" (salvation and release), rather than works and knowledge. Their focus became divine virtues, rather than anthropocentric virtues. Daniel Ingalls regards Vaishnavas' position on \"moksha\" as similar to the Christian position on salvation, and Vaishnavism as the school whose views on \"dharma\", \"karma\" and \"moksha\" dominated the initial impressions and colonial-era literature on Hinduism, through the works of Thibaut, Max Müller and others.\n\nThe concept of \"moksha\" appears much later in ancient Indian literature than the concept of \"dharma\". The proto-concept that first appears in the ancient Sanskrit verses and early Upanishads is \"mucyate\", which means freed or released. It is the middle and later Upanishads, such as the Svetasvatara and Maitri, where the word \"moksha\" appears and begins becoming an important concept.\n\nKathaka Upanishad, a middle Upanishadic era script dated to be about 2500 years old, is among the earliest expositions about \"saṃsāra\" and \"moksha\". In Book I, Section III, the legend of boy Naciketa queries Yama, the lord of death to explain what causes \"saṃsāra\" and what leads to liberation. Naciketa inquires: what causes sorrow? Yama explains that suffering and \"saṃsāra\" results from a life that is lived absent-mindedly, with impurity, with neither the use of intelligence nor self-examination, where neither mind nor senses are guided by one's \"atma\" (soul, self). Liberation comes from a life lived with inner purity, alert mind, led by \"buddhi\" (reason, intelligence), realization of the Supreme Self (\"purusha\") who dwells in all beings. Kathaka Upanishad asserts knowledge liberates, knowledge is freedom. Kathaka Upanishad also explains the role of yoga in personal liberation, \"moksha\".\n\nSvetasvatara Upanishad, another middle era Upanishad written after Kathaka Upanishad, begins with questions such as why is man born, what is the primal cause behind the universe, what causes joy and sorrow in life? It then examines the various theories, that were then existing, about saṃsāra and release from bondage. Svetasvatara claims bondage results from ignorance, illusion or delusion; deliverance comes from knowledge. The Supreme Being dwells in every being, he is the primal cause, he is the eternal law, he is the essence of everything, he is nature, he is not a separate entity. Liberation comes to those who know Supreme Being is present as the Universal Spirit and Principle, just as they know butter is present in milk. Such realization, claims Svetasvatara, come from self-knowledge and self-discipline; and this knowledge and realization is liberation from transmigration, the final goal of the Upanishad.\nStarting with the middle Upanishad era, \"moksha\" - or equivalent terms such as \"mukti\" and \"kaivalya\" - is a major theme in many Upanishads. For example, Sarasvati Rahasya Upanishad, one of several Upanishads of the bhakti school of Hinduism, starts out with prayers to Goddess Sarasvati. She is the Hindu goddess of knowledge, learning and creative arts; her name is a compound word of ‘‘sara’’ and ‘‘sva’’, meaning \"essence of self\". After the prayer verses, the Upanishad inquires about the secret to freedom and liberation (mukti). Sarasvati's reply in the Upanishad is:\n\nThe concept of \"moksha\", according to Daniel Ingalls, represented one of many expansions in Hindu Vedic ideas of life and afterlife. In the Vedas, there were three stages of life: studentship, householdship and retirement. During the Upanishadic era, Hinduism expanded this to include a fourth stage of life: complete abandonment. In Vedic literature, there are three modes of experience: waking, dream and deep sleep. The Upanishadic era expanded it to include \"turiyam\" - the stage beyond deep sleep. The Vedas suggest three goals of man: kama, artha and dharma. To these, the Upanishadic era added \"moksha\".\n\nThe acceptance of the concept of \"moksha\" in some schools of Hindu philosophy was slow. These refused to recognize \"moksha\" for centuries, considering it irrelevant. The Mimamsa school, for example, denied the goal and relevance of \"moksha\" well into the 8th century AD, until the arrival of a Mimamsa scholar named Kumarila. Instead of \"moksha\", Mimamsa school of Hinduism considered the concept of heaven as sufficient to answer the question: what lay beyond this world after death. Other schools of Hinduism, over time, accepted the \"moksha\" concept and refined it over time.\n\nIt is unclear when the core ideas of samsara and \"moksha\" were developed in ancient India. Patrick Olivelle suggests these ideas likely originated with new religious movements in the first millennium BCE. \"Mukti\" and \"moksha\" ideas, suggests J. A. B. van Buitenen, seem traceable to yogis in Hinduism, with long hair, who chose to live on the fringes of society, given to self-induced states of intoxication and ecstasy, possibly accepted as medicine men and \"sadhus\" by the ancient Indian society. \"Moksha\" to these early concept developers, was the abandonment of the established order, not in favor of anarchy, but in favor of self-realization, to achieve release from this world.\n\nIn its historical development, the concept of \"moksha\" appears in three forms: Vedic, yogic and bhakti. In the Vedic period, \"moksha\" was ritualistic. Mokṣa was claimed to result from properly completed rituals such as those before \"Agni\" - the fire deity. The significance of these rituals was to reproduce and recite the cosmic creation event described in the Vedas; the description of knowledge on different levels - \"adhilokam\", \"adhibhutam\", \"adhiyajnam\", \"adhyatmam\" - helped the individual transcend to moksa. Knowledge was the means, the ritual its application. By the middle to late Upanishadic period, the emphasis shifted to knowledge, and ritual activities were considered irrelevant to the attainment of \"moksha\". Yogic \"moksha\" replaced Vedic rituals with personal development and meditation, with hierarchical creation of the ultimate knowledge in self as the path to \"moksha\". Yogic \"moksha\" principles were accepted in many other schools of Hinduism, albeit with differences. For example, Adi Shankara in his book on \"moksha\" suggests:\n\nBhakti \"moksha\" created the third historical path, where neither rituals nor meditative self-development were the way, rather it was inspired by constant love and contemplation of God, which over time results in a perfect union with God. Some Bhakti schools evolved their ideas where God became the means and the end, transcending \"moksha\"; the fruit of bhakti is bhakti itself. In the history of Indian religious traditions, additional ideas and paths to \"moksha\" beyond these three, appeared over time.\n\nThe words \"moksha\", nirvana (\"nibbana\") and kaivalya are sometimes used synonymously, because they all refer to the state that liberates a person from all causes of sorrow and suffering. However, in modern era literature, these concepts have different premises in different religions. Nirvana, a concept common in Buddhism, is a state of realization that there is no self (no soul) and Emptiness; while \"moksha\", a concept common in many schools of Hinduism, is acceptance of Self (soul), realization of liberating knowledge, the consciousness of Oneness with Brahman, all existence and understanding the whole universe as the Self. Nirvana starts with the premise that there is no Self, \"moksha\" on the other hand, starts with the premise that everything is the Self; there is no consciousness in the state of nirvana, but everything is One unified consciousness in the state of \"moksha\".\n\nKaivalya, a concept akin to \"moksha\", rather than nirvana, is found in some schools of Hinduism such as the Yoga school. Kaivalya is the realization of aloofness with liberating knowledge of one's self and union with the spiritual universe. For example, Patanjali’s Yoga Sutra suggests:\nNirvana and \"moksha\", in all traditions, represents a state of being in ultimate reality and perfection, but described in a very different way. Some scholars, states Jayatilleke, assert that the Nirvana of Buddhism is same as the Brahman in Hinduism, a view other scholars and he disagree with. Buddhism rejects the idea of Brahman, and the metaphysical ideas about soul (atman) are also rejected by Buddhism, while those ideas are essential to \"moksha\" in Hinduism. In Buddhism, nirvana is 'blowing out' or 'extinction'. In Hinduism, \"moksha\" is 'identity or oneness with Brahman'. Realization of \"anatta\" (anatman) is essential to Buddhist nirvana. Realization of \"atman\" (atta) is essential to Hindu \"moksha\".\n\nAncient literature of different schools of Hinduism sometimes use different phrases for \"moksha\". For example, \"Keval jnana\" or \"kaivalya\" (\"state of Absolute\"), \"Apavarga\", \"Nihsreyasa\", \"Paramapada\", \"Brahmabhava\", \"Brahmajnana\" and \"Brahmi sthiti\". Modern literature additionally uses the Buddhist term nirvana interchangeably with \"moksha\" of Hinduism. There is difference between these ideas, as explained elsewhere in this article, but they are all soteriological concepts of various Indian religious traditions.\n\nThe six major orthodox schools of Hinduism have had a historic debate, and disagree over whether \"moksha\" can be achieved in this life, or only after this life. Many of the 108 Upanishads discuss amongst other things \"moksha\". These discussions show the differences between the schools of Hinduism, a lack of consensus, with a few attempting to conflate the contrasting perspectives between various schools. For example, freedom and deliverance from birth-rebirth, argues Maitrayana Upanishad, comes neither from the Vedanta school's doctrine (the knowledge of one's own Self as the Supreme Soul) nor from the Samkhya school's doctrine (distinction of the Purusha from what one is not), but from Vedic studies, observance of the \"Svadharma\" (personal duties), sticking to \"Asramas\" (stages of life).\n\nThe six major orthodox schools of Hindu philosophy offer the following views on \"moksha\", each for their own reasons: the Nyaya, Vaisesika and Mimamsa schools of Hinduism consider \"moksha\" as possible only after death. Samkhya and Yoga schools consider \"moksha\" as possible in this life. In Vedanta school, the Advaita sub-school concludes \"moksha\" is possible in this life, while Dvaita and Visistadvaita sub-schools of Vedanta tradition believes that \"moksha\" is a continuous event, one assisted by loving devotion to God, that extends from this life to post-mortem. Beyond these six orthodox schools, some heterodox schools of Hindu tradition, such as Carvaka, deny there is a soul or after life \"moksha\".\n\nBoth Sāmkhya and Yoga systems of religious thought are \"mokshaśāstras\", suggests , they are systems of salvific liberation and release. Sāmkhya is a system of interpretation, primarily a theory about the world. Yoga is both a theory and a practice. Yoga gained wide acceptance in ancient India, its ideas and practices became part of many religious schools in Hinduism, including those that were very different from Sāmkhya. The eight limbs of yoga can be interpreted as a way to liberation (\"moksha\").\n\nIn Sāmkhya literature, liberation is commonly referred to as \"kaivalya\". In this school, kaivalya means the realization of \"purusa\", the principle of consciousness, as independent from mind and body, as different from \"prakrti\". Like many schools of Hinduism, in Sāmkhya and Yoga schools, the emphasis is on the attainment of knowledge, \"vidyā\" or \"jñāna\", as necessary for salvific liberation, \"moksha\". Yoga's purpose is then seen as a means to remove the \"avidyā\" - that is, ignorance or misleading/incorrect knowledge about one self and the universe. It seeks to end ordinary reflexive awareness (\"cittavrtti nirodhah\") with deeper, purer and holistic awareness (\"asamprājñāta samādhi\"). Yoga, during the pursuit of \"moksha\", encourages practice (\"abhyāsa\") with detachment (\"vairāgya\"), which over time leads to deep concentration (\"samādhi\"). Detachment means withdrawal from outer world and calming of mind, while practice means the application of effort over time. Such steps are claimed by Yoga school as leading to samādhi, a state of deep awareness, release and bliss called \"kaivalya\".\nYoga, or \"mārga\", in Hinduism is widely classified into four spiritual practices. The first mārga is Jñāna Yoga, the way of knowledge. The second mārga is Bhakti Yoga, the way of loving devotion to God. The third mārga is Karma Yoga, the way of works. The fourth mārga is Rāja Yoga, the way of contemplation and meditation. These mārgas are part of different schools in Hinduism, and their definition and methods to \"moksha\". For example, the Advaita Vedanta school relies on Jñāna Yoga in its teachings of \"moksha\".\n\nThe three main sub-schools in Vedanta school of Hinduism - Advaita Vedanta, Vishistadvaita and Dvaita - each have their own views about \"moksha\".\n\nThe Vedantic school of Hinduism suggests the first step towards mokṣa begins with \"mumuksutva\", that is desire of liberation. This takes the form of questions about self, what is true, why do things or events make us happy or cause suffering, and so on. This longing for liberating knowledge is assisted by, claims Adi Shankara of Advaita Vedanta, guru (teacher), study of historical knowledge and viveka (critical thinking). Shankara cautions that the guru and historic knowledge may be distorted, so traditions and historical assumptions must be questioned by the individual seeking \"moksha\". Those who are on their path to \"moksha\" (samnyasin), suggests Klaus Klostermaier, are quintessentially free individuals, without craving for anything in the worldly life, thus are neither dominated by, nor dominating anyone else.\n\nVivekachudamani, which literally means \"Crown Jewel of Discriminatory Reasoning\", is a book devoted to moksa in Vedanta philosophy. It explains what behaviors and pursuits lead to \"moksha\", as well what actions and assumptions hinder \"moksha\". The four essential conditions, according to Vivekachudamani, before one can commence on the path of \"moksha\" include (1) \"vivekah\" (discrimination, critical reasoning) between everlasting principles and fleeting world; (2) \"viragah\" (indifference, lack of craving) for material rewards; (3) \"samah\" (calmness of mind), and (4) \"damah\" (self restraint, temperance). The \"Brahmasutrabhasya\" adds to the above four requirements, the following: \"uparati\" (lack of bias, dispassion), \"titiksa\" (endurance, patience), \"sraddha\" (faith) and \"samadhana\" (intentness, commitment).\n\nThe Advaita tradition considers \"moksha\" achievable by removing avidya (ignorance). \"Moksha\" is seen as a final release from illusion, and through knowledge (\"anubhava\") of one's own fundamental nature, which is Satcitananda. Advaita holds there is no being/non-being distinction between \"Atman\", \"Brahman\", and \"Paramatman\". The knowledge of Brahman leads to \"moksha\", where Brahman is described as that which is the origin and end of all things, the universal principle behind and at source of everything that exists, consciousness that pervades everything and everyone. Advaita Vedanta emphasizes Jnana Yoga as the means of achieving \"moksha\". Bliss, claims this school, is the fruit of knowledge (vidya) and work (karma).\n\nThe Dvaita (dualism) traditions define \"moksha\" as the loving, eternal union with God (Vishnu) and considered the highest perfection of existence. Dvaita schools suggest every soul encounters liberation differently. Dualist schools (e.g. Vaishnava) see God as the object of love, for example, a personified monotheistic conception of Shiva or Vishnu. By immersing oneself in the love of God, one's karmas slough off, one's illusions decay, and truth is lived. Both the worshiped and worshiper gradually lose their illusory sense of separation and only One beyond all names remains. This is salvation to dualist schools of Hinduism. Dvaita Vedanta emphasizes Bhakti Yoga as the means of achieving \"moksha\".\n\nThe Vishistadvaita tradition, led by Ramanuja, defines avidya and \"moksha\" differently from the Advaita tradition. To Ramanuja, avidya is a focus on the self, and vidya is a focus on a loving god. The Vishistadvaita school argues that other schools of Hinduism create a false sense of agency in individuals, which makes the individual think oneself as potential or self-realized god. Such ideas, claims Ramanuja, decay to materialism, hedonism and self worship. Individuals forget \"Ishvara\" (God). Mukti, to Vishistadvaita school, is release from such avidya, towards the intuition and eternal union with God (Vishnu).\n\nAmong the Samkhya, Yoga and Vedanta schools of Hinduism, liberation and freedom reached within one's life is referred to as \"jivanmukti\", and the individual who has experienced this state is called \"jivanmukta\" (self-realized person). Dozens of Upanishads, including those from middle Upanishadic period, mention or describe the state of liberation, \"jivanmukti\". Some contrast \"jivanmukti\" with \"videhamukti\" (\"moksha\" from samsara after death). Jivanmukti is a state that transforms the nature, attributes and behaviors of an individual, claim these ancient texts of Hindu philosophy. For example, according to Naradaparivrajaka Upanishad, the liberated individual shows attributes such as:\n\nWhen a Jivanmukta dies he achieves Paramukti and becomes a Paramukta. Jivanmukta experience enlightenment and liberation while alive and also after death i.e., after becoming paramukta, while Videhmukta experiences enlightenment and liberation only after death.\n\nBalinese Hinduism incorporates \"moksha\" as one of five tattwas. The other four are: \"brahman\" (the one supreme god head, not to be confused with Brahmin), \"atma\" (soul or spirit), karma (actions and reciprocity, causality), \"samsara\" (principle of rebirth, reincarnation). \"Moksha\", in Balinese Hindu belief, is the possibility of unity with the divine; it is sometimes referred to as nirwana.\n\nIn Buddhism the term \"moksha\" is uncommon, but an equivalent term is \"vimutti\", \"release\". In the suttas two forms of release are mentioned, namely \"ceto-vimutti\", \"deliverance of mind,\" and \"panna-vimutti\", \"deliverance through wisdom\" (insight). \"Ceto-vimutti\" is related to the practice of dhyana, while \"panna-vimutti\" is related to the development of insight. According to Gombrich, the distinction may be a later development, which resulted in a change of doctrine, regarding the practice of dhyana to be insufficient for final liberation.\n\nWith release comes Nirvana (Pali: Nibbana), “blowing out”, \"quenching\", or “becoming extinguished” of the fires of the passions and of self-view. It is a \"timeless state\" in which there is no more becoming.\n\nNirvana ends the cycle of \"Dukkha\" and rebirth in the six realms of Saṃsāra (Buddhism). It is part of the Four Noble Truths doctrine of Buddhism, which plays an essential role in Theravada Buddhism. Nirvana has been described in Buddhist texts in a manner similar to other Indian religions, as the state of complete liberation, enlightenment, highest happiness, bliss, fearless, freedom, dukkha-less, permanence, non-dependent origination, unfathomable, indescribable. It has also been described as a state of release marked by \"emptiness\" and realization of \"non-Self\". Such descriptions, states Peter Harvey, are contested by scholars because nirvana in Buddhism is ultimately described as a state of \"stopped consciousness (blown out), but one that is not non-existent\", and \"it seems impossible to imagine what awareness devoid of any object would be like\".\n\nIn Jainism, \"moksha\" and \"nirvana\" are one and the same. Jaina texts sometimes use the term \"Kevalya\", and call the liberated soul as \"Kevalin\". As with all Indian religions, \"moksha\" is the ultimate spiritual goal in Jainism. It defines \"moksha\" as the spiritual release from all karma.\n\nJainism is a Sramanic non-theistic philosophy, that like Hinduism and unlike Buddhism, believes in a metaphysical permanent self or soul often termed \"jiva\". Jaina believe that this soul is what transmigrates from one being to another at the time of death. The \"moksa\" state is attained when a soul (\"atman\") is liberated from the cycles of deaths and rebirths (\"saṃsāra\"), is at the apex, is omniscient, remains there eternally, and is known as a \"siddha\". In Jainism, it is believed to be a stage beyond enlightenment and ethical perfection, states Paul Dundas, because they can perform physical and mental activities such as teach, without accruing karma that leads to rebirth.\n\nJaina traditions believe that there exist \"Abhavya\" (incapable), or a class of souls that can never attain \"moksha\" (liberation). The \"Abhavya\" state of soul is entered after an intentional and shockingly evil act, but Jaina texts also polemically applied \"Abhavya\" condition to those who belonged to a competing ancient Indian tradition called \"Ājīvika\". A male human being is considered closest to the apex of \"moksha\", with the potential to achieve liberation, particularly through asceticism. The ability of women to attain \"moksha\" has been historically debated, and the subtraditions with Jainism have disagreed. In the Digambara tradition of Jainism, women must live an ethical life and gain karmic merit to be reborn as a man, because only males can achieve spiritual liberation. In contrast, the Śvētāmbara tradition has believed that women too can attain \"moksha\" just like men.\n\nThe Sikh concept of mukti (\"moksha\") is similar to other Indian religions, and refers to spiritual liberation. It is described in Sikhism as the state that breaks the cycle of rebirths. Mukti is obtained according to Sikhism, states Singha, through \"God's grace\". According to the Guru Granth Sahib, the devotion to God is viewed as more important than the desire for \"Mukti\".\n\nSikhism recommends Naam Simran as the way to mukti, which is meditating and repeating the \"Naam\" (names of God).\n\n\n"}
{"id": "3005707", "url": "https://en.wikipedia.org/wiki?curid=3005707", "title": "Nazi plunder", "text": "Nazi plunder\n\nNazi plunder refers to art theft and other items stolen as a result of the organized looting of European countries during the time of the Third Reich by agents acting on behalf of the ruling Nazi Party of Germany. Plundering occurred from 1933 until the end of World War II, particularly by military units known as the Kunstschutz, although most plunder was acquired during the war. In addition to gold, silver and currency, cultural items of great significance were stolen, including paintings, ceramics, books, and religious treasures. Although most of these items were recovered by agents of the Monuments, Fine Arts, and Archives program (MFAA, also known as the Monuments Men), on behalf of the Allies immediately following the war, many are still missing. There is an international effort underway to identify Nazi plunder that still remains unaccounted for, with the aim of ultimately returning the items to the rightful owners, their families or their respective countries.\n\nAdolf Hitler was an unsuccessful artist who was denied admission to the Vienna Academy of Fine Arts. Nonetheless, he thought of himself as a connoisseur of the arts, and in \"Mein Kampf\" he ferociously attacked modern art as degenerate, including Cubism, Futurism, and Dadaism, all of which he considered the product of a decadent twentieth century society. In 1933 when Hitler became Chancellor of Germany, he enforced his aesthetic ideal on the nation. The types of art that were favored amongst the Nazi party were classical portraits and landscapes by Old Masters, particularly those of Germanic origin. Modern art that did not match this was dubbed degenerate art by the Third Reich, and all that was found in Germany's state museums was to be sold or destroyed. With the sums raised, the Führer's objective was to establish the European Art Museum in Linz. Other Nazi dignitaries, like Reichsmarschall Hermann Göring and Foreign Affairs minister von Ribbentrop, were also intent on taking advantage of German military conquests to increase their private art collections.\n\nArt dealers Hildebrand Gurlitt, Karl Buchholz, Ferdinand Moeller and Bernhard Boehmer set up shop in Schloss Niederschonhausen, just outside Berlin, to sell a cache of near-16,000 paintings and sculptures which Hitler and Göring removed from the walls of German museums in 1937-38. They were first put on display in the Haus der Kunst in Munich on 19 July 1937, with the Nazi leaders inviting public mockery by two million visitors who came to view the condemned modern art in the \"Degenerate art\" exhibition. Propagandist Joseph Goebbels in a radio broadcast called Germany's degenerate artists \"garbage\". Hitler opened the Haus der Kunst exhibition with a speech. In it he described German art as suffering \"a great and fatal illness\".\n\nHildebrand Gurlitt and his colleagues did not have much success with their sales, mainly because art labelled \"rubbish\" had small appeal. So on 20 March 1939 they set fire to 1,004 paintings and sculptures and 3,825 watercolours, drawings and prints in the courtyard of the Berlin Fire Department, an act of infamy similar to their earlier well-known book burnings. The propaganda act raised the attention they hoped. The Basel Museum in Switzerland arrived with 50,000 Swiss francs to spend. Shocked art lovers came to buy. What is unknown after these sales is how many paintings were kept by Gurlitt, Buchholz, Moeller and Boehmer and sold by them to Switzerland and America - ships crossed the Atlantic from Lisbon - for personal gain.\n\nThe most infamous auction of Nazi looted art was the \"degenerate art' auction organized by Theodor Fischer (auctioneer) in Lucerne, Switzerland, 30 June 1939 at the Grand Hotel National. The artworks on offer had been \"de-accessioned\" from German museums by the Nazis, yet many well known art dealers participated as well as proxies for major collectors and museums. Public auctions were only the visible tip of the iceberg, as many sales operated by art dealers were private. The Commission for Art Recovery has characterized Switzerland as \"a magnet\" for assets from the rise of Hitler until the end of World War II. Researching and documenting Switzerland's role \"as an art-dealing centre and conduit for cultural assets in the Nazi period and in the immediate post-war period\" was one of the missions of the Bergier Commission, under the directorship of Professor Georg Kreis. \n\nWhile the Nazis were in power, they plundered cultural property from every territory they occupied. This was conducted in a systematic manner with organizations specifically created to determine which public and private collections were most valuable to the Nazi Regime. Some of the objects were earmarked for Hitler's never realized Führermuseum, some objects went to other high-ranking officials such as Hermann Göring, while other objects were traded to fund Nazi activities.\n\nIn 1940, an organization known as the \"Einsatzstab Reichsleiter Rosenberg für die Besetzten Gebiete\" (The Reichsleiter Rosenberg Institute for the Occupied Territories), or ERR, was formed, headed for Alfred Rosenberg by . The first operating unit, the western branch for France, Belgium and the Netherlands, called the \"Dienststelle Westen\" (Western Agency), was located in Paris. The chief of this Dienststelle was . Its original purpose was to collect Jewish and Freemasonic books and documents, either for destruction, or for removal to Germany for further \"study\". However, late in 1940, Hermann Göring, who in fact controlled the ERR, issued an order that effectively changed the mission of the ERR, mandating it to seize \"Jewish\" art collections and other objects. The war loot had to be collected in a central place in Paris, the Museum Jeu de Paume. At this collection point worked art historians and other personnel who inventoried the loot before sending it to Germany. Göring also commanded that the loot would first be divided between Hitler and himself. Hitler later ordered that all confiscated works of art were to be made directly available to him. From the end of 1940 to the end of 1942 Göring traveled twenty times to Paris. In the Museum Jeu de Paume, art dealer Bruno Lohse staged 20 expositions of the newly looted art objects, especially for Göring, from which Göring selected at least 594 pieces for his own collection. Göring made Lohse his liaison-officer and installed him in the ERR in March 1941 as the deputy leader of this unit. Items which Hitler and Göring did not want were made available to other Nazi leaders. Under Rosenberg and Göring's leadership, the ERR seized 21,903 art objects from German-occupied countries.\n\nOther Nazi looting organizations included the , the organization run by the art historian Hans Posse, which was particularly in charge of assembling the works for the Führermuseum, the Dienststelle Mühlmann, operated by Kajetan Mühlmann, which Göring also controlled and operated primarily in the Netherlands, Belgium, and a Sonderkommando Kuensberg connected to the minister of foreign affairs Joachim von Ribbentrop, which operated first in France, then in Russia and North Africa. In Western Europe, with the advancing German troops, were elements of the 'von Ribbentrop Battalion', named after Joachim von Ribbentrop. These men were responsible for entering private and institutional libraries in the occupied countries and removing any materials of interest to the Germans, especially items of scientific, technical or other informational value.\n\nArt collections from prominent Jewish families, including the Rothschilds, the Rosenbergs, the Wildensteins and the Schloss Family were the targets of confiscations because of their significant value. Also Jewish art dealers sold art to German organizations - often under duress, e.g. the art dealerships of Jacques Goudstikker, Benjamin and Nathan Katz and Kurt Walter Bachstitz. Also non-Jewish art dealers sold art to the Germans, e.g. the art dealers De Boer and Hoogendijk in the Netherlands.\n\nBy the end of the war, the Third Reich amassed hundreds of thousands of cultural objects.\n\nOn November 21, 1944, at the request of Owen Roberts, William J. Donovan created the Art Looting Investigation Unit (ALIU) within the OSS to collect information on the looting, confiscation and transfer of cultural objects by Nazi Germany, its allies and the various individuals and organizations involved; to prosecute war criminals and to restitute property.\n\nThe ALIU compiled information on individuals believed to have participated in art looting, identifying a group of key suspects for capture and interrogation about their roles in carrying out Nazi policy. Interviews were conducted in Bad Aussee, Austria.\n\nThe ALIU Reports detail the networks of Nazi officials, art dealers and individuals involved in the Hitler's policy of spoliation of Jews in Nazi-occupied Europe. The ALIU's final report included 175 pages divided into three parts: Detailed Interrogation Reports (DIRs), which focused individuals who played pivotal roles in German spoliation. Consolidated Interrogations Reports (CIRs), and a \"Red Flag list\" of people involved in Nazi spoilation. The ALIU Reports form one of the key records in the US Government Archives of Nazi Era Assets\n\nDetailed Intelligence Reports (DIR)\nThe first group of reports detailing the networks and relations between art dealers and other agents employed by Hitler, Göring and Rosenberg are organized by name: Heinrich Hoffmann, Ernst Buchner, Gustav Rochlitz, Gunter Schiedlausky, Bruno Lohse, Gisela Limberger, Walter Andreas Hofer, Karl Kress, Walter Bornheim, Hermann Voss and Karl Haberstock.\n\nConsolidated Interrogation Reports (CIR)\nA second set of reports detail the art looting activities of Göring (The Goering Collection), the art looting activities of the Einsatzstab Reichsleiter Rosenberg (ERR), and Hitler's Linz Museum.\n\nALIU List of Red Flag Names\n\nThe Art Looting Intelligence Unit published a list of \"Red Flag Names\", organizing them by country: Germany, France, Switzerland, The Netherlands, Belgium, Italy, Spain, Portugal, Sweden, and Luxembourg. Each name is followed by a description of the person's activities, their relations with other people in the spoliation network and, in many cases, information concerning their arrest or imprisonment by Allied forces.\n\nTo investigate and estimate Nazi plunder in the USSR during 1941 through 1945, the Soviet State Extraordinary Commission for Ascertaining and Investigating the Crimes Committed by the German-Fascist Invaders and Their Accomplices was formed on 2 November 1942. During the Great Patriotic War and afterwards, until 1991, the Commission collected materials on Nazi crimes in the USSR, including incidents of plunder. Immediately following the war, the Commission outlined damage in detail to sixty-four of the most valuable Soviet museums, out of 427 damaged ones. In the Russian SFSR, 173 museums were found to have been plundered by the Nazis, with looted items numbering in the hundreds of thousands.\n\nAfter the dissolution of the USSR, the Government of the Russian Federation formed the State Commission for the Restitution of Cultural Valuables to replace the Soviet Commission. Experts from this Russian institution originally consulted the work of the Soviet Commission, yet continue to catalogue artworks lost during the war museum by museum. , lost artworks of 14 museums and the libraries of Voronezh Oblast, Kursk Oblast, Pskov Oblast, Rostov Oblast, Smolensk Oblast, Northern Caucasus, Gatchina, Peterhof Palace, Tsarskoye Selo (Pushkin), Novgorod and Novgorod Oblast, as well as the bodies of the Russian State Archives and CPSU Archives, were catalogued in 15 volumes, all of which were made available online. They contain detailed information on 1,148,908 items of lost artworks. The total number of lost items is unknown so far, because cataloguing work for other damaged Russian museums is ongoing.\n\nAlfred Rosenberg commanded the so-called Einsatzstab Reichsleiter Rosenberg [ERR] für die Besetzten Gebiete, which was responsible for collecting art, books, and cultural objects from invaded countries, and also transferred their captured library collections back to Berlin during the retreat from Russia. \"In their search for 'research materials' ERR teams and the Wehrmacht visited 375 archival institutions, 402 museums, 531 institutes, and 957 libraries in Eastern Europe alone\". The ERR also operated in the early days of the blitzkrieg of the Low Countries. This caused some confusion about authority, priority, and the chain of command among the German Army, the von Rippentropp Battalion and the Gestapo, and as a result of personal looting among the Army officers and troops. These ERR teams were, however, very effective. One account estimates that from the Soviet Union alone: \"one hundred thousand geographical maps were taken on ideological grounds, for academic research, as means for political, geographical and economic information on Soviet cities and regions, or as collector's items\".\n\nAfter the occupation of Poland by German forces in September 1939, the Nazi regime attempted to exterminate its upper classes as well as its culture. Thousands of art objects were looted, as the Nazis systematically carried out a plan of looting prepared even before the start of hostilities. 25 museums and many other facilities were destroyed. The total cost of German Nazi theft and destruction of Polish art is estimated at 20 billion dollars, or an estimated 43% of Polish cultural heritage; over 516,000 individual art pieces were looted, including 2,800 paintings by European painters; 11,000 paintings by Polish painters; 1,400 sculptures; 75,000 manuscripts; 25,000 maps; 90,000 books, including over 20,000 printed before 1800; and hundreds of thousands of other items of artistic and historical value. Germany still has much Polish material looted during World War II. For decades there have been mostly futile negotiations between Poland and Germany concerning the return of the looted property.\n\nAfter Hitler became Chancellor, he made plans to transform his home city of Linz, Austria into the Third Reich's capital city for the arts. Hitler hired architects to work from his own designs to build several galleries and museums, which would collectively be known as the Führermuseum. Hitler wanted to fill his museum with the greatest art treasures in the world, and believed that most of the world's finest art belonged to Germany after having been looted during the Napoleonic and First World wars.\n\nThe Hermann Göring collection, a personal collection of Reichsmarschall Hermann Göring, was another large collection including confiscated property, consisted of approximately 50 percent of works of art confiscated from the enemies of the Reich. Assembled in large measure by art dealer Bruno Lohse, Göring's adviser and ERR representative in Paris, in 1945 the collection included over 2,000 individual pieces including more than 300 paintings. The U.S. National Archives and Records Administration's Consolidated Interrogation Report No. 2 states that Göring never crudely looted, instead he always managed \"to find a way of giving at least the appearance of honesty, by a token payment or promise thereof to the confiscation authorities. Although he and his agents never had an official connection with the German confiscation organizations, they nevertheless used them to the fullest extent possible.\"\n\nThe Third Reich amassed hundreds of thousands of objects from occupied nations and stored them in several key locations, such as Musée Jeu de Paume in Paris and the Nazi headquarters in Munich. As the Allied forces gained advantage in the war and bombed Germany's cities and historic institutions, Germany \"began storing the artworks in salt mines and caves for protection from Allied bombing raids. These mines and caves offered the appropriate humidity and temperature conditions for artworks.\" Well known repositories of this kind were mines in Merkers, Altaussee and Siegen. These mines were not only used for the storage of looted art but also of art that had been in Germany and Austria before the beginning of the Nazi rule. \nDegenerate art was legally banned by the Nazis from entering Germany, and so ones designated were held in what was called the Martyr's Room at the Jeu de Paume. Much of Paul Rosenberg's professional dealership and personal collection were so subsequently designated by the Nazis. Following Joseph Goebels earlier private decree to sell these degenerate works for foreign currency to fund the building of the Führermuseum and the wider war effort, Hermann Göring personally appointed a series of ERR approved dealers to liquidate these assets and then pass the funds to swell his personal art collection, including Hildebrand Gurlitt. With the looted degenerate art sold onwards via Switzerland, Rosenberg's collection was scattered across Europe. Today, some 70 of his paintings are missing, including: the large Picasso watercolor \"Naked Woman on the Beach\", painted in Provence in 1923; seven works by Matisse; and the \"Portrait of Gabrielle Diot\" by Degas.\n\nThe Allies created special commissions, such as the Monuments, Fine Arts and Archives (MFAA) organization to help protect famous European monuments from destruction, and after the war, to travel to formerly Nazi-occupied territories to find Nazi art repositories. In 1944 and 1945 one of the greatest challenges for the \"Monuments Men\" was to keep Allied forces from plundering and \"taking artworks and sending them home to friends and family\"; When \"off-limits\" warning signs failed to protect the artworks the \"Monuments Men\" started to mark the storage places with white tape, which was used by Allied troops as a warning sign for unexploded mines. They recovered thousands of objects, many of which were pillaged by the Nazis.\n\nThe Allies found these artworks in over 1,050 repositories in Germany and Austria at the end of World War II. In summer 1945, Capt. Walter Farmer became the collecting point's first director. The first shipment of artworks arriving at Wiesbaden Collection Point included cases of antiquities, Egyptian art, Islamic artefacts, and paintings from the Kaiser Friedrich Museum. The collecting point also received materials from the Reichsbank and Nazi-looted, Polish, liturgical collections. At its height, Wiesbaden stored, identified, and restituted approximately 700,000 individual objects including paintings and sculptures, mainly to keep them away from the Soviet Army and wartime reparations.\n\nThe Allies collected the artworks and stored them in collecting points, in particular the Central Collection Point in Munich until they could be returned. The identifiable works of art, that had been acquired by the Germans during the Nazi rule, were returned to the countries from which they were taken. It was up to the governments of each nation if and under which circumstances they would return the objects to the original owners.\n\nWhen the Munich collection point was closed, the owners of many of the objects had not been found. Nations were also unable to find all of the owners or to verify that they were dead. There are many organizations put in place to help return the stolen items taken from the Jewish people. For example: Project Heart, the World Jewish Restitution Organization, and The Claims Conference. Depending on the circumstances these organizations may receive the art works in lieu of the heirs.\n\n\"Further reading:\" United States restitution to the Soviet Union\n\nAlthough most of the stolen artworks and antiques were documented, found or recovered \"by the victorious Allied armies ... principally hidden away in salt mines, tunnels, and secluded castles\", many artworks have never been returned to their rightful owners. Art dealers, galleries and museums worldwide have been compelled to research their collection's provenance in order to investigate claims that some of the work was acquired after it had been stolen from its original owners. Already in 1985, years before American museums recognized the issue and before the international conference on Nazi-looted assets of Holocaust victims, European countries released inventory lists of works of art, coins and medals \"that were confiscated from Jews by the Nazis during World War II, and announced the details of a process for returning the works to their owners and rightful heirs.\" In 1998 an Austrian advisory panel recommended the return of 6,292 objets d'art to their legal owners (most of whom are Jews), under the terms of a 1998 restitution law.\nNazi concentration camp and death camp victims had to strip completely before their murder, and all their personal belongings were stolen. The very valuable items such as gold coins, rings, spectacles, jewellry and other precious metal items were sent to the Reichsbank for conversion to bullion. The value was then credited to SS accounts.\n\nPieces of art looted by the Nazis can still be found in Russian/Soviet and American institutions: the Metropolitan Museum of Art revealed a list of 393 paintings that have gaps in their provenance during the Nazi Era, the Art Institute of Chicago has posted a listing of more than 500 works \"for which links in the chain of ownership for the years 1933–1945 are still unclear or not yet fully determined.\" The San Diego Museum of Art and the Los Angeles County Museum of Art provide lists on the internet to determine if art items within their collection were stolen by the Nazis.\n\nStuart Eizenstat, the Under Secretary of State and head of the U.S. delegation sponsoring the 1998 international conference on Nazi-looted assets of Holocaust victims in Washington conference stated that \"From now on, ... the sale, purchase, exchange and display of art from this period will be addressed with greater sensitivity and a higher international standard of responsibility.\" The conference was attended by more than forty countries and thirteen different private entities, and the goal was to come to a federal consensus on how to handle Nazi-Era Looted Art. The conference was built on the foundation of the Nazi Gold Conference held in London in 1997. The U.S. Department of State hosted the conference with the U.S. Holocaust Memorial Museum from November 30 to December 3, 1998.\n\nAfter the conference the Association of Art Museum Directors developed guidelines which require museums to review the provenance or history of their collections, focusing especially on art looted by the Nazis. The National Gallery of Art in Washington identified more than 400 European paintings with gaps in their provenance during World War II era. One particular piece of art, \"Still Life with Fruit and Game\" by the 16th-century Flemish painter Frans Snyders, was sold by Karl Haberstock, whom the World Jewish Congress describes as \"one of the most notorious Nazi art dealers.\" In 2000 the New York City's Museum of Modern Art still told Congress that they were \"not aware of a single Nazi-tainted work of art in our collection, of the more than 100,000\" they held.\n\nIn 1979 two paintings, a Renoir, Tête de jeune fille, and a Pissarro, Rue de village, appeared on Interpol's \"12 Most Wanted List,\" but to date no one knows their whereabouts (ATA Newsletter, Nov. '79, vol. 1, no. 9, p. 1. '78, 326.1-2) The New Jersey owner has asked IFAR to republish information about the theft, with the hope that someone will recognize the paintings. The owner wrote IFAR that when his parents emigrated from Berlin in 1938, two of their paintings \"mysteriously disappeared.\" All of their other possessions were shipped from Germany to the U.S. via the Netherlands, and everything except the box containing these two paintings arrived intact. After World War II the owner's father made a considerable effort to locate the paintings, but was unsuccessful. Over the years numerous efforts have been made to recover them, articles have been published, and an advertisement appeared in the German magazine, Die Weltkunst, May 15, 1959. A considerable reward has been offered, subject to usual conditions, but there has been no response. Anyone with information about these two paintings is asked to contact IFAR.\n\nHowever, restitution efforts initiated by German politicians have not been free of controversy, either. As the German law for restitution \"applies to \"cultural assets lost as a result of Nazi persecution, \"which includes paintings that Jews who emigrated from Germany sold to support themselves\", pretty much any trade involving Jews in that era is affected, and the benefit of the doubt is given to claimants. German leftist politicians Klaus Wowereit (SPD, mayor of Berlin) and Thomas Flierl (Linkspartei) were sued in 2006 for being overly willing to give away the 1913 painting \"\" of expressionist Ernst Ludwig Kirchner, which was in Berlin's Brücke Museum. On display in Cologne in 1937, it had been sold for 3,000 Reichsmark by a Jewish family residing in Switzerland to a German collector. This sum is considered by experts to have been well over the market price. The museum, which obtained the painting in 1980 after several ownership changes, could not prove that the family actually received the money. It was restituted to the heiress of the former owners, and she had it auctioned off for $38.1 Million.\n\nIn 2010, as work began to extend an underground line from Alexanderplatz through the historic city centre to the Brandenburg Gate, a number of sculptures from the degenerate art exhibition were unearthed in the cellar of a private house close to the \"Rote Rathaus\". These included, for example, the bronze cubist style statue of a female dancer by the artist Marg Moll, and are now on display at the Neues Museum.\n\nFrom 2013 up to 2015 a committee researched the collection of the Dutch Royal family. The committee focussed on all objects acquired by the family since 1933 and which were made prior to 1945. In total 1300 artworks were studied. Dutch musea had already researched their collection in order to find objects stolen by the Nazis. It appeared that one painting of the forest near Huis ten Bosch by the Dutch painter Joris van der Haagen came from a Jewish collector. He was forced to hand the painting over to the former Jewish bank Lippmann, Rosenthal & Co in Amsterdam, which collected money and other possessions of the Jews in Amsterdam. The painting was bought by Queen Juliana in 1960. The family plans to return the painting to the heirs of the owner in 1942, a Jewish collector.\n\nApproximately 20% of the art in Europe was looted by the Nazis, and there are well over 100,000 items that have not been returned to their rightful owners. The majority of what is still missing includes everyday objects such as china, crystal or silver.\n\nSome objects of great cultural significance remain missing, though how much has yet to be determined. This is a major issue for the art market, since legitimate organizations do not want to deal in objects with unclear ownership titles. Since the mid-1990s, after several books, magazines, and newspapers began exposing the subject to the general public, many dealers, auction houses and museums have grown more careful about checking the provenance of objects that are available for purchase in case they are looted. Some museums in the United States and elsewhere have agreed to check the provenance of works in their collections with the implied promise that suspect works would be returned to rightful owners if the evidence so dictates. But the process is time-consuming and slow, and very few disputed works have been found in public collections.\n\nIn the 1990s and 2000s, information has become more accessible due to political and economic changes as well as advances in technology. Privacy laws in some countries have expired so records that were once difficult to obtain are now open to the public. Information from former Soviet countries that was previously unobtainable is now available, and many organisations have posted information online, making it widely accessible.\n\nIn addition to the role of courts in determining restitution or compensation, some states have created official bodies for the consideration and resolution of claims. In the United Kingdom, the Spoliation Advisory Panel advises the Department for Culture, Media and Sport on such claims. The International Foundation for Art Research (IFAR), a not-for-profit educational and research organization, has helped provide information leading to restitution.\n\nIn 2013 the Canadian government created the Holocaust-era Provenance Research and Best-Practice Guidelines Project, through which they are investigating the holdings of six art galleries in Canada.\n\nOn 14 January 1992, historian Marc Jansen reported in an article in \"NRC Handelsblad\" that archival collections stolen from the Netherlands including the records of the International Archives for the Women's Movement (), which had been looted in 1940, had been found in Russia. The confiscated records were initially sent to Berlin and later was moved to Sudetenland for security reasons. At the end of the war, the Red Army took the documents from German-occupied Czechoslovakia and in 1945-46, stored them in the KGB's (), meaning special archive, which was housed in Moscow. Though agreements were drafted almost immediately after the discovery, bureaucratic delays kept the archives from being returned for eleven years. In 2003, the partial recovery of the papers of some of the most noted feminists in the pre-war period, including Aletta Jacobs and Rosa Manus, some 4,650 books and periodicals, records of the International Council of Women and International Woman Suffrage Alliance, among many photographs were returned. Approximately half of the original collection is still unrecovered.\n\nIn early 2012, over one thousand pieces of artwork were discovered at the home of Cornelius Gurlitt, the son of Hildebrand Gurlitt, of which about 200-300 pieces are suspected of being looted art, some of which may have been exhibited in the degenerate art exhibition held by the Nazis before World War II in several large German cities. The collection contains works by Marc Chagall, Otto Dix, and Henri Matisse, Renoir and Max Liebermann amongst many others.\n\nIn January 2014, researcher Dominik Radlmaier of the city of Nuremberg announced that eight objects had been identified as lost art with a further eleven being under strong suspicion. The city's research project was started in 2004 and Radlmaier has been investigating full-time since then.\n\nIn Walbrzych, Poland two amateur explorers — Piotr Koper and Andreas Richter — claim to have found a rumored armored train that is believed to be filled with gold, gems and weapons. The train was rumored to be sealed in a tunnel in the closing days of World War II before the collapse of The Third Reich. Only 10% of the tunnel has been explored because much of the tunnel has collapsed. Finding the train will be an expensive and complicated operation involving a lot of funding, digging, and drilling. However, to support their claims the explorers said experts have examined the site with ground-penetrating, thermal and magnetic sensors that picked up signs of a railway tunnel with metal tracks. The legitimacy of these claims has yet to be determined, yet the explorers are requesting 10% of the value of whatever is within the train if their findings are correct. Poland's deputy culture minister, Piotr Zuchowski, said he was \"99 percent convinced\" that the train had finally been found but scientists claim that the explorers' findings are false.\n\n\nNotes\nFurther reading\n\n"}
{"id": "5285895", "url": "https://en.wikipedia.org/wiki?curid=5285895", "title": "Neutral stimulus", "text": "Neutral stimulus\n\nA neutral stimulus is a stimulus which initially produces no specific response other than focusing attention. In classical conditioning, when used together with an unconditioned stimulus, the neutral stimulus becomes a conditioned stimulus. With repeated presentations of both the neutral stimulus and the unconditioned stimulus, the neutral stimulus will elicit a response as well, known as a conditioned response. Once the neutral stimulus elicits a conditioned response, the neutral stimulus becomes known as a conditioned stimulus. The conditioned response is the same as the unconditioned response, but occurs in the presence of the conditioned stimulus rather than the unconditioned stimulus.\n\nIvan Pavlov conducted multiple experiments investigating digestion in dogs in which neutral, unconditioned, and conditioned stimuli were used. In these experiments, the neutral stimulus was the sound of a bell ringing. This sound was presented to the dogs along with food, which acted as an unconditioned stimulus. The presentation of a neutral stimulus does not result in any particular response, but the presentation of an unconditioned stimulus results in an unconditioned response, which was the dogs salivating in Pavlov's experiments. After conditioning, the bell ringing became a conditioned stimulus. Pavlov later used the sound of a metronome as a neutral stimulus in studies on cerebral cortex activity.\n\n"}
{"id": "1617709", "url": "https://en.wikipedia.org/wiki?curid=1617709", "title": "Nirvikalpa", "text": "Nirvikalpa\n\nNirvikalpa (Sanskrit : निर्विकल्प) is a Sanskrit adjective with the general sense of \"not wavering,\" \"admitting no doubt,\" \"free from change or differences.\" In the Yoga Sutras of Patanjali it refers to meditation without an object.\n\nNirvikalpa (Sanskrit : निर्विकल्प) is a Sanskrit adjective with the general sense of \"not admitting an alternative\", \"not wavering,\" \"admitting no doubt,\" \"free from change or differences.\" It is formed by applying the contra-existential prepositional prefix ni (\"away, without, not\") to the term vikalpa (\"alternative, variant thought or conception\").\n\nIn the Yoga Sutras of Patanjali, \"nirvikalpa samadhi\" is a synonym for \"Asamprajnata Samadhi\", the highest stage of samadhi. Samadhi is of two kinds, with and without support of an object of meditation:\nSwami Sivananda \"Nirbija Samadhi\", \"without seeds,\" as follows:\n\"Nirvikalpaka yoga\" is a technical term in the philosophical system of Shaivism, in which there is a complete identification of the \"I\" and Shiva, in which the very concepts of name and form disappear and Shiva alone is experienced as the real Self. In that system, this experience occurs when there is complete cessation of all thought-constructs.\n\nWhile Patanjali was influenced by Buddhism, and incorporated Buddhist thought and terminology, the term \"nirvikalpa samadhi\" is unusual in a Buddhist context, though some authors have equated \"nirvikalpa samadhi\" with the formless jhanas and/or \"nirodha samapatti\". Yet, according to Jianxin Li, it is \"asamprajnata samadhi\", c.q. \"savikalpa samadhi\" and \"sabija samadhi\", Patanjali's first stage of meditation with a (subtle) object, that may be compared to the \"arupa jhanas\" of Buddhism, and to \"Nirodha-Samapatti\". Crangle also notes that \"sabija-asamprajnata samadhi\" resembles the four formless \"jhanas\". According to Crangle, the fourth \"arupa jhana\" is the stage of transition to Patanjali's \"consciousness without seed,\" c.q. \"nirvikalpa samadhi\". Crangle further notes that the first \"jhana\" also resembles \"sabija-asamprajnata samadhi\". According to Gombrich and Wynne the first and second \"jhana\" represent concentration, whereas the third and fourth \"jhana\" combine concentration with mindfulness.\n\nIn the Buddhist canonical texts, the word \"jhāna\" is never explicitly used to denote the four formless jhānas; they are instead referred to as \"āyatana\". However, they are sometimes mentioned in sequence after the first four jhānas (other texts. e.g. MN 121 treat them as a distinct set of attainments) and thus came to be treated by later exegetes as jhānas. The immaterial attainments have more to do with expanding, while the Jhanas (1–4) focus on concentration.\n\nThe relation between \"dhyāna\" and insight is a core problem in the study of early Buddhism. According to tradition, the Buddha had mastered several forms of formless meditation states, without attaining liberation, or the cessation of suffering and rebirth. This was attained when he recalled his past lives, gained insight into the cycle of rebirth, and gained direct insight into the four noble truths. Yet, according to Schmithausen, the four noble truths as \"liberating insight\" may be a later addition to texts such as Majjhima Nikaya 36, and liberating insight and samadhi are alternately accnetuated as the highest means to salvation throughout the Buddhist traditions.\n\nThe technical Yogacara term ' is translated by Edward Conze as \"undifferentiated cognition\". Conze notes that, in Yogacara, only the actual experience of ' can prove the reports given of it in scriptures. He describes the term as used in the Yogacara context as follows:\nA different sense in Buddhist usage occurs in the Sanskrit expression ' (Pali: ') that means \"makes free from uncertainty (or false discrimination)\" <nowiki>=</nowiki> \"distinguishes, considers carefully\".\n\n\n\n\n\n\n"}
{"id": "8955445", "url": "https://en.wikipedia.org/wiki?curid=8955445", "title": "O. J. Simpson", "text": "O. J. Simpson\n\nOrenthal James Simpson (born July 9, 1947), nicknamed The Juice, is an American former running back, broadcaster, actor, advertising spokesman, and convicted robber.\n\nSimpson attended the University of Southern California (USC), where he played football for the USC Trojans and won the Heisman Trophy in 1968. He played professionally as a running back in the NFL for 11 seasons, primarily with the Buffalo Bills from 1969 to 1977. He also played for the San Francisco 49ers from 1978 to 1979. In 1973, he became the first NFL player to rush for more than 2,000 yards in a season. He holds the record for the single season yards-per-game average, which stands at 143.1. He is the only player to ever rush for over 2,000 yards in the 14-game regular season NFL format.\n\nSimpson was inducted into the College Football Hall of Fame in 1983 and the Pro Football Hall of Fame in 1985. After retiring from football, he began new careers in acting and football broadcasting.\n\nIn 1994, Simpson was arrested and charged with the murders of his ex-wife, Nicole Brown Simpson, and her friend, Ron Goldman. He was acquitted by a jury after a lengthy and internationally publicized trial. The families of the victims subsequently filed a civil suit against him, and in 1997 a civil court awarded a $33.5 million judgment against him for the victims' wrongful deaths.\n\nIn 2007, Simpson was arrested in Las Vegas, Nevada, and charged with the felonies of armed robbery and kidnapping. In 2008, he was convicted and sentenced to 33 years imprisonment, with a minimum of nine years without parole. He served his sentence at the Lovelock Correctional Center near Lovelock, Nevada. On July 20, 2017, Simpson was granted parole. He was eligible for release from prison on October 1, 2017, and was released shortly after midnight on that date.\nBorn and raised in San Francisco, California, Simpson is a son of Eunice (née Durden), a hospital administrator, and Jimmy Lee Simpson, a chef and bank custodian. His father was a well-known drag queen in the San Francisco Bay Area. Later in life, Jimmy Simpson announced that he was gay. He died of AIDS.\n\nSimpson's maternal grandparents were from Louisiana, and his aunt gave him the name Orenthal, which she said was the name of a French actor she liked. Simpson has one brother, Melvin Leon \"Truman\" Simpson, one living sister, Shirley Simpson-Baker, and one deceased sister, Carmelita Simpson-Durio. As a child, Simpson developed rickets and wore braces on his legs until the age of five, giving him his bowlegged stance. His parents separated in 1952, and Simpson was raised by his mother.\n\nSimpson grew up in San Francisco and lived with his family in the housing projects of the Potrero Hill neighborhood. In his early teenage years, he joined a street gang called the Persian Warriors and was briefly incarcerated at the San Francisco Youth Guidance Center. Future wife Marquerite, his childhood sweetheart, described Simpson as \"really an awful person then\"; after his third arrest, a meeting with Willie Mays during which the baseball star encouraged Simpson to avoid trouble helped persuade him to reform. At Galileo High School (currently Galileo Academy of Science and Technology) in San Francisco, Simpson played for the school football team, the Galileo Lions.\n\nAlthough Simpson was an All-City football player at Galileo, his mediocre high-school grades prevented him from attracting the interest of many college recruiters. After a childhood friend's injury in the Vietnam War influenced Simpson to stay out of the military, he enrolled at City College of San Francisco in 1965. He played football both ways as a running back and defensive back and was named to the Junior College All-American team as a running back. City College won the Prune Bowl against Long Beach State, and many colleges sought Simpson as a transfer student for football.\n\nSimpson chose to attend the University of Southern California (USC), which he had admired as a young football fan, over the University of Utah and played running back for head coach John McKay in 1967 and 1968. Simpson led the nation in rushing both years under McKay: in 1967 with 1,543 yards and 13 touchdowns, and in 1968 with 1,880 yards on 383 carries.\n\nAs a junior in 1967, Simpson was a close runner-up in the Heisman Trophy balloting to quarterback Gary Beban of UCLA. In that year's Victory Bell rivalry game between the teams, USC was down by six points in the fourth quarter with under eleven minutes remaining. On their own 36, USC backup quarterback Toby Page called an audible on third and seven. Simpson's 64-yard touchdown run tied the score, and the extra point provided a 21–20 lead, which was the final score. This was the biggest play in what is regarded as one of the greatest football games of the 20th century.\n\nAnother dramatic touchdown in the same game is the subject of the Arnold Friberg oil painting, \"O.J. Simpson Breaks for Daylight.\" Simpson also won the Walter Camp Award in 1967 and was a two-time consensus All-American.\n\nSimpson was an aspiring track athlete; in 1967 he lost a 100 m race at Stanford against the then-British record holder Menzies Campbell. Prior to playing football at Southern Cal, he ran in the USC sprint relay quartet that broke the world record in the 4 x 110-yard relay at the NCAA track championships in Provo, Utah on June 17, 1967.\nAs a senior in 1968, Simpson rushed for 1,709 yards and 22 touchdowns in the regular season, earning the Heisman Trophy, the Maxwell Award, and Walter Camp Award. He still holds the record for the Heisman's largest margin of victory, defeating runner-up Leroy Keyes by 1,750 points. In the Rose Bowl on New Year's Day, #2 USC faced top-ranked Ohio State; Simpson ran for 171 yards, including an 80-yard touchdown run in a 27–16 loss.\n\nThe first selection 1969 AFL-NFL Common Draft was held by the AFL's Buffalo Bills, after finishing 1–12–1 in 1968. They took Simpson, but he demanded what was then the largest contract in professional sports history: $650,000 over five years. This led to a standoff with Bills' owner Ralph Wilson, as Simpson threatened to become an actor and skip professional football. Eventually, Wilson agreed to pay Simpson.\n\nSimpson entered professional football with high expectations, but struggled in his first three years, averaging only 622 yards per season. Bills coach John Rauch, not wanting to build an offense around one running back, assigned Simpson to do blocking and receiving duties at the expense of running the ball. In 1971, Rauch resigned as head coach and the Bills brought in Harvey Johnson. Despite Johnson devising a new offense for Simpson, Simpson was still ineffective that year. After the 1971 season, the Bills fired Johnson and brought in Lou Saban as head coach. Unlike Rauch, Saban made Simpson the centerpiece of the Bills offense.\n\nIn 1972, Simpson rushed for over 1,000 yards for the first time in his career, gaining a league-leading total of 1,251 yards. In 1973, Simpson became the first player to break the highly coveted 2,000 yard rushing mark, with 2,003 total rushing yards and 12 touchdowns. Simpson broke the mark during the last game of the season against the New York Jets with a 7-yard rush. That same game also saw Simpson break Jim Brown's single-season rushing record of 1,863 yards. For his performance, Simpson won that year's NFL MVP Award and Bert Bell Award. While other players have broken the 2,000-yard mark since Simpson, his record was established in a time when the NFL only had 14 games per season, as opposed to the 16-game seasons that began in 1978.\n\nSimpson gained more than 1,000 rushing yards for each of his next three seasons. He didn't lead the league in rushing in 1974, but did cross the 1,000-yard barrier despite a knee injury. In game 11 of 1974, he passed Ken Willard as the rushing leader among active players, a position he maintained until his retirement over five seasons later. Simpson also made his first and only playoff appearance during the 1974 season. In a against the Pittsburgh Steelers, Simpson rushed for 49 yards on 15 attempts and caught a touchdown pass, but the Bills lost the game 32–14.\n\nSimpson won the rushing title again in 1975, rushing for 1,817 yards and 16 touchdowns. Simpson also had a career-high 426 receiving yards and 7 receiving touchdowns that season. Simpson once again led the league in rushing in 1976, rushing for 1,503 yards and 8 touchdowns. Simpson had the best game of his career during that season's Thanksgiving game against the Detroit Lions on November 25. In that game, Simpson rushed for a then-record 273 yards on 29 attempts and scored two touchdowns. Despite Simpson's performance, the Bills would lose the game 27–14.\n\nSimpson played in only seven games in 1977, as his season was cut short by injury.\n\nBefore the 1978 season, the Bills traded Simpson to his hometown San Francisco 49ers for a series of draft picks. Simpson played in San Francisco for two seasons, rushing for 1,053 yards and four touchdowns. His final NFL game was on December 16, 1979, a 31–21 loss to the Atlanta Falcons at Atlanta–Fulton County Stadium. His final play was a 10-yard run on 3rd and 10 for a first down.\n\nSimpson gained 11,236 rushing yards, placing him 2nd on the NFL's all-time rushing list when he retired; he now stands at 21st. He was named NFL Player of the Year in 1973, and played in six Pro Bowls. He was the only player in NFL history to rush for over 2,000 yards in a 14-game season and he's the only player to rush for over 200 yards in six different games in his career. From 1972 to 1976, Simpson averaged 1,540 rushing yards per (14 game) season, 5.1 yards per carry, and he won the NFL rushing title four times. Simpson was inducted into the Pro Football Hall of Fame in 1985, his first year of eligibility.\n\nSimpson played in only one playoff game during his 11-season Hall of Fame career: a 1974 Divisional Playoff between the Buffalo Bills and the Pittsburgh Steelers. Simpson was held to 49 rushing yards, 3 receptions for 37 yards, and one touchdown, and the Bills lost 14-32 to the team which went on to win Super Bowl IX.\n\nSimpson acquired the nickname \"Juice\" as a play on \"O.J.\", a common abbreviation for \"orange juice\". \"Juice\" is also a colloquial synonym for electricity or electrical power, and hence a metaphor for any powerful entity; the Bills' offensive line at Simpson's peak was nicknamed \"The Electric Company\".\n\n\nEven before his retirement from the NFL, Simpson embarked on a film career with parts in films such as the television mini-series \"Roots\" (1977), and the dramatic motion pictures \"The Klansman\" (1974), \"The Towering Inferno\" (1974), \"The Cassandra Crossing\" (1976) and \"Capricorn One\" (1978). In 1979, he started his own film production company, Orenthal Productions, which dealt mostly in made-for-TV fare such as the family-oriented \"Goldie and the Boxer\" films with Melissa Michaelsen (1979 and 1981), and \"Cocaine and Blue Eyes\" (1983), the pilot for a proposed detective series on NBC. He also starred in the comedic \"Back to the Beach\" (1987) and \"The Naked Gun\" trilogy (1988, 1991, 1994). \n\nBesides his acting career, Simpson worked as a commentator for \"Monday Night Football\" and \"The NFL on NBC.\" He also appeared in the audience of \"Saturday Night Live\" during its second season and hosted an episode during its third season.\n\nSimpson starred in the un-televised two-hour-long film pilot for \"Frogmen,\" an \"A-Team\"-like adventure series that Warner Bros. Television completed in 1994, a few months before the murders. NBC had not yet decided whether to order the series when Simpson's arrest cancelled the project. While searching his home, the police obtained a videotaped copy of the pilot as well as the script and dailies. Although the prosecution investigated reports that Simpson, who played the leader of a group of former United States Navy SEALs, received \"a fair amount of\" military training—including use of a knife—for \"Frogmen,\" and there is a scene in which he holds a knife to the throat of a woman, this material was not introduced as evidence during the trial.\n\nNBC executive Warren Littlefield said in July 1994 that the network would probably never air the pilot if Simpson were convicted; if he were acquitted, however, one television journalist speculated that \"\"Frogmen\" would probably be on the air before the NBC peacock could unfurl its plume\". Most pilots that are two hours long are aired as TV movies whether or not they are ordered as series. Because—as the \"Los Angeles Times\" later reported—\"the appetite for all things O.J. appeared insatiable\" during the trial, Warner Bros. and NBC estimated that a gigantic, Super Bowl-like television audience would have watched the \"Frogmen\" film. Co-star Evan Handler said that the studio's decision not to air it or release it on home video, and forego an estimated $14 million in profits, was \"just about the only proof you have that there is some dignity in the advertising and television business\".\n\nIn 2006, Simpson starred in his own improv, hidden-camera prank TV show, \"Juiced.\" Typical of the genre, Simpson would play a prank on everyday people while secretly filming them and at the end of each prank, he would shout, \"You've been Juiced!\" Less typical, each episode opened with topless strippers dancing around Simpson, who is dressed as a pimp. He sings his own rap song, which includes the lyrics \"Don't you know there's no stopping the Juice / When I'm on the floor I'm like a lion on the loose / Better shoot me with a tranquilizer dart / Don't be stupid, I'm not a Simpson named Bart.\" In one episode, Simpson is at a used car lot in Las Vegas where he attempts to sell his white Bronco (made famous during the chase in Los Angeles prior to his arrest). A bullet hole in the front of the SUV is circled with his autograph, and he pitches it to a prospective buyer by saying that if they \"ever get into some trouble and have to get away, it has escapability.\" In another sketch called \"B-I-N-G-O.J.\", Simpson pretends to be having an affair with another man's girlfriend. Later he transforms into an old white man whose dying wish is to call a game of bingo. \"Juiced\" aired as a one-time special on pay-per-view television and was later released on DVD.\n\nIn 1975, \"People\" magazine described Simpson as \"the first black athlete to become a bona fide lovable media superstar.\" Chuck Barnes helped him form business relationships with Chevrolet and ABC early in his career. By 1971, \"New York\" wrote that Simpson was already wealthy enough to, \"retire this week if [he] wanted to.\" His amiable persona and natural charisma landed Simpson numerous endorsement deals. From 1975, he appeared in advertisements with the Hertz rental car company, in whose commercials he was depicted running through airports, serving as an embodiment of speed; Simpson estimated that the very successful campaign raised the recognition rate among people he met from 30% to 90%. He was also a longtime spokesman for Pioneer Chicken and owned two franchises, one of which was destroyed during the 1992 Los Angeles riots; as well as HoneyBaked Ham, the pX Corporation, and Calistoga Water Company's line of Napa Naturals soft drinks. He also appeared in comic book ads for Dingo cowboy boots.\n\nAt age 19 on June 24, 1967, Simpson married Marguerite L. Whitley. Together, they had three children: Arnelle L. Simpson (b. 1968), Jason L. Simpson (b. 1970), and Aaren Lashone Simpson (1977–1979). In August 1979, Aaren drowned in the family's swimming pool.\n\nSimpson met Nicole Brown in 1977, while she was working as a waitress at the nightclub \"The Daisy\". Although still married to his first wife, Simpson began dating Brown. Simpson and Marguerite\ndivorced in March 1979.\n\nBrown and Simpson were married on February 2, 1985, five years after his retirement from professional football. The couple had two children, Sydney Brooke Simpson (b. 1985) and Justin Ryan Simpson (b. 1988). The marriage lasted seven years, during which Simpson pleaded no contest to spousal abuse in 1989. Brown filed for divorce on February 25, 1992, citing irreconcilable differences. In 1993, after the divorce, Brown and Simpson made an attempt at reconciliation, but according to Sheila Weller \"they were a dramatic, fractious, mutually obsessed couple before they married, after they married, after they divorced in 1992, and after they reconciled.\"\n\nOn June 12, 1994, Nicole Brown Simpson and Ron Goldman were found stabbed to death outside Nicole's condominium in the Brentwood area of Los Angeles. Simpson was a person of interest in their murders. Simpson did not turn himself in, and on June 17 he became the object of a low-speed pursuit in a white 1993 Ford Bronco SUV owned and driven by Al Cowlings. TV stations interrupted coverage of the 1994 NBA Finals to broadcast the incident live. With an estimated audience of 95 million people, the event was described as \"the most famous ride on American shores since Paul Revere's\". The pursuit, arrest, and trial were among the most widely publicized events in American history. The trial, often characterized as the Trial of the Century because of its international publicity similar to that of Sacco and Vanzetti and the Lindbergh kidnapping, culminated after eleven months on October 3, 1995, when the jury rendered a verdict of \"not guilty\" for the two murders. An estimated 100 million people nationwide tuned in to watch or listen to the verdict announcement. Following Simpson's acquittal, no additional arrests or convictions related to the murders were made.\n\nImmediate reaction to the verdict was notable for its division along racial lines: a poll of Los Angeles County residents showed that most African Americans there felt that justice had been served by the \"not guilty\" verdict, while the majority of whites and Latinos expressed an opinion that it had not. O. J. Simpson's integrated defense counsel included Johnnie Cochran, Robert Kardashian, Robert Shapiro, and F. Lee Bailey. Marcia Clark was the lead prosecutor for the State of California.\n\nAccording to a 2016 poll, 83% of white Americans and 57% of black Americans believe Simpson was guilty of the murders.\n\nFollowing Simpson's acquittal of criminal charges, Ron Goldman's family filed a civil lawsuit against Simpson. Daniel Petrocelli represented plaintiff Fred Goldman (Ronald Goldman's father), while Robert Baker represented Simpson. Superior Court Judge Hiroshi Fujisaki presided, and he barred television and still cameras, radio equipment, and courtroom sketch artists from the courtroom. On October 23, 1996, opening statements were made, and on January 16, 1997, both sides rested their cases.\n\nOn February 5, 1997, a civil jury in Santa Monica, California unanimously found Simpson liable for the wrongful death of and battery against Goldman, and battery against Brown. Simpson was ordered to pay $33,500,000 in damages. In February 1999, an auction of Simpson's Heisman Trophy and other belongings netted almost $500,000, which went to the Goldman family.\n\nThe Goldman family also tried to collect Simpson's NFL $28,000 yearly pension but failed to collect any money.\n\nIn 1997, Simpson was evicted from the estate in which he had lived for 20 years, at 360 North Rockingham Avenue, after defaulting on the mortgage. In July 1998, the house was demolished by its next owner, Kenneth Abdalla, an investment banker and president of the Jerry's Famous Deli chain. The property's address has since been renumbered to 380 North Rockingham Avenue.\n\nA 2000 \"Rolling Stone\" article reported that Simpson still made a significant income by signing autographs. He subsequently moved from California to Florida, settling in Miami. In Florida, among a few states, a person's residence cannot be seized to collect a debt under most circumstances.\n\nOn September 5, 2006, Goldman's father took Simpson back to court to obtain control over Simpson's \"right to publicity\", for purposes of satisfying the judgment in the civil court case. On January 4, 2007, a federal judge issued a restraining order prohibiting Simpson from spending any advance he may have received on a canceled book deal and TV interview about the 1994 murders. The matter was dismissed before trial for lack of jurisdiction. On January 19, 2007, a California state judge issued an additional restraining order, ordering Simpson to restrict his spending to \"ordinary and necessary living expenses\".\n\nOn March 13, 2007, a judge prevented Simpson from receiving any further compensation from the defunct book deal and TV interview, and the judge ordered the bundled book rights to be auctioned. In August 2007, a Florida bankruptcy court awarded the rights to the book to the Goldman family, to partially satisfy an unpaid civil judgment. Originally titled \"If I Did It\", the book was renamed \"If I Did It: Confessions of the Killer\", with the word \"If\" reduced in size to make the title appear to read \"I Did It: Confessions of the Killer\". Additional material was added by members of the Goldman family, investigative journalist Dominick Dunne, and author Pablo Fenjves.\n\nThe State of California claims Simpson owes $1.44 million in back taxes. A tax lien was filed in his case on September 1, 1999.\n\nIn the late 1990s, Simpson attempted to register \"O. J. Simpson\", \"O. J.\", and \"The Juice\" as trademarks for \"a broad range of goods, including figurines, trading cards, sportswear, medallions, coins, and prepaid telephone cards\". A \"concerned citizen\", William B. Ritchie, sued to oppose the granting of federal registration on the grounds that doing so would be immoral and scandalous. Simpson gave up the effort in 2000.\n\nIn February 2001, Simpson was arrested in Miami-Dade County, Florida, for simple battery and burglary of an occupied conveyance, for yanking the glasses off another motorist during a traffic dispute three months earlier. If convicted, Simpson could have faced up to 16 years in prison, but he was tried and quickly acquitted of both charges in October 2001.\n\nOn December 4, 2001, Simpson's Miami home was searched by the FBI on suspicion of ecstasy possession and money laundering. The FBI had received a tip that Simpson was involved in a major drug trafficking ring after 10 other suspects were arrested in the case. Simpson's home was thoroughly searched for two hours, but no illegal drugs were discovered, and no arrest or formal charges were filed following the search. However, investigators uncovered equipment capable of stealing satellite television programming, which eventually led to Simpson's being sued in federal court.\n\nOn July 4, 2002, Simpson was arrested in Miami-Dade County, Florida, for water speeding through a manatee protection zone and failing to comply with proper boating regulations. The misdemeanor boating regulation charge was dropped, and Simpson was fined for the speeding infraction.\n\nIn March 2004, satellite television network DirecTV, Inc. accused Simpson in a Miami federal court of using illegal electronic devices to pirate its broadcast signals. The company later won a $25,000 judgment, and Simpson was ordered to pay an additional $33,678 in attorney's fees and costs.\n\nOn the night of September 13, 2007, a group of men led by Simpson entered a room at the Palace Station hotel-casino and took sports memorabilia at gunpoint, which resulted in Simpson's being questioned by police. Simpson admitted to taking the items, which he said had been stolen from him, but denied breaking into the hotel room; he also denied that he or anyone else carried a gun. He was released after questioning.\n\nTwo days later, Simpson was arrested and initially held without bail. Along with three other men, Simpson was charged with multiple felony counts, including criminal conspiracy, kidnapping, assault, robbery, and using a deadly weapon. Bail was set at $125,000, with stipulations that Simpson have no contact with the co-defendants and that he surrender his passport. Simpson did not enter a plea.\n\nBy the end of October 2007, all three of Simpson's co-defendants had plea-bargained with the prosecution in the Clark County, Nevada, court case. Walter Alexander and Charles H. Cashmore accepted plea agreements in exchange for reduced charges and their testimony against Simpson and three other co-defendants, including testimony that guns were used in the robbery. Co-defendant Michael McClinton told a Las Vegas judge that he too would plead guilty to reduced charges and testify against Simpson that guns were used in the robbery. After the hearings, the judge ordered that Simpson be tried for the robbery.\n\nOn November 8, 2007, Simpson had a preliminary hearing to decide whether he would be tried for the charges. He was held over for trial on all 12 counts. Simpson pleaded not guilty on November 29, and the trial was reset from April to September 8, 2008. Court officers and attorneys announced, on May 22, 2008, that long questionnaires with at least 115 queries would be given to a jury pool of 400 or more.\n\nIn January 2008, Simpson was taken into custody in Florida and flown to Las Vegas, where he was incarcerated at the county jail for violating the terms of his bail by attempting to contact Clarence \"C. J.\" Stewart, a co-defendant in the trial. District Attorney David Roger of Clark County provided District Court Judge Jackie Glass with evidence that Simpson had violated his bail terms. A hearing took place on January 16, 2008. Glass raised Simpson's bail to US$250,000 and ordered that he remain in county jail until 15 percent was paid in cash. Simpson posted bond that evening and returned to Miami the next day.\n\nSimpson and his co-defendant were found guilty of all charges on October 3, 2008. On October 10, 2008, Simpson's counsel moved for a new trial (\"trial de novo\") on grounds of judicial errors and insufficient evidence. Simpson's attorney announced he would appeal to the Nevada Supreme Court if Judge Glass denied the motion. The attorney for Simpson's co-defendant, C. J. Stewart, petitioned for a new trial, alleging Stewart should have been tried separately and cited possible misconduct by the jury foreman.\n\nSimpson faced a possible life sentence with parole on the kidnapping charge, and mandatory prison time for armed robbery. On December 5, 2008, Simpson was sentenced to a total of thirty-three years in prison, with the possibility of parole after nine years, in 2017. On September 4, 2009, the Nevada Supreme Court denied a request for bail during Simpson's appeal. In October 2010, the Nevada Supreme Court affirmed his convictions. He served his sentence at the Lovelock Correctional Center where his inmate ID number was #1027820.\n\nA Nevada judge agreed on October 19, 2012, to \"reopen the armed robbery and kidnapping case against O. J. Simpson to determine if the former football star was so badly represented by his lawyers that he should be freed from prison and get another trial\". A hearing was held beginning May 13, 2013, to determine if Simpson was entitled to a new trial. On November 27, 2013, Judge Linda Bell denied Simpson's bid for a new trial on the robbery conviction. In her ruling, Bell wrote that all of Simpson's contentions lacked merit.\n\nOn July 31, 2013, the Nevada Parole Board granted Simpson parole on some convictions, but his imprisonment continued based on the weapons and assault convictions. The board considered Simpson's prior record of criminal convictions and good behavior in prison in coming to the decision. At his parole hearing on July 20, 2017, the board decided to grant Simpson parole. He was released on October 1, 2017, having served nine years.\n\nPablo Fenjves ghostwrote the 2007 book \"If I Did It\" based on interviews with Simpson. The book was published by Beaufort Books, a New York City publishing house owned by parent company Kampmann & Company/Midpoint Trade Books. All rights and proceeds from the book were awarded to the family of murder victim Ron Goldman.\n\n\n\n\n\n"}
{"id": "49090", "url": "https://en.wikipedia.org/wiki?curid=49090", "title": "Ohm's law", "text": "Ohm's law\n\nOhm's law states that the current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality, the resistance, one arrives at the usual mathematical equation that describes this relationship:\n\nwhere is the current through the conductor in units of amperes, \"V\" is the voltage measured \"across\" the conductor in units of volts, and \"R\" is the resistance of the conductor in units of ohms. More specifically, Ohm's law states that the \"R\" in this relation is constant, independent of the current. Ohm's law is an empirical relation which accurately describes the conductivity of the vast majority of electrically conductive materials over many orders of magnitude of current. However some materials do not obey Ohm's law, these are called non-ohmic.\n\nThe law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. Ohm explained his experimental results by a slightly more complex equation than the modern form above (see History).\n\nIn physics, the term \"Ohm's law\" is also used to refer to various generalizations of the law; for example the vector form of the law used in electromagnetics and material science:\n\nwhere J is the current density at a given location in a resistive material, E is the electric field at that location, and \"σ\" (sigma) is a material-dependent parameter called the conductivity. This reformulation of Ohm's law is due to Gustav Kirchhoff.\n\nIn January 1781, before Georg Ohm's work, Henry Cavendish experimented with Leyden jars and glass tubes of varying diameter and length filled with salt solution. He measured the current by noting how strong a shock he felt as he completed the circuit with his body. Cavendish wrote that the \"velocity\" (current) varied directly as the \"degree of electrification\" (voltage). He did not communicate his results to other scientists at the time, and his results were unknown until Maxwell published them in 1879.\n\nFrancis Ronalds delineated “intensity” (voltage) and “quantity” (current) for the dry pile – a high voltage source – in 1814 using a gold-leaf electrometer. He found for a dry pile that the relationship between the two parameters was not proportional under certain meteorological conditions.\nOhm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book \"Die galvanische Kette, mathematisch bearbeitet\" (\"The galvanic circuit investigated mathematically\"). He drew considerable inspiration from Fourier's work on heat conduction in the theoretical explanation of his work. For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant voltage. He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature. He then added test wires of varying length, diameter, and material to complete the circuit. He found that his data could be modeled through the equation\nwhere \"x\" was the reading from the galvanometer, \"l\" was the length of the test conductor, \"a\" depended on the thermocouple junction temperature, and \"b\" was a constant of the entire setup. From this, Ohm determined his law of proportionality and published his results.\n\nIn modern notation we would write,\n\nwhere formula_5 is the open-circuit emf of the thermocouple, formula_6 is the internal resistance of the thermocouple and formula_7 is the resistance of the test wire. In terms of the length of the wire this becomes,\n\nwhere formula_9 is the resistance of the test wire per unit length. Thus, Ohm's coefficients are,\n\nOhm's law was probably the most important of the early quantitative descriptions of the physics of electricity. We consider it almost obvious today. When Ohm first published his work, this was not the case; critics reacted to his treatment of the subject with hostility. They called his work a \"web of naked fancies\" and the German Minister of Education proclaimed that \"a professor who preached such heresies was unworthy to teach science.\" The prevailing scientific philosophy in Germany at the time asserted that experiments need not be performed to develop an understanding of nature because nature is so well ordered, and that scientific truths may be deduced through reasoning alone. Also, Ohm's brother Martin, a mathematician, was battling the German educational system. These factors hindered the acceptance of Ohm's work, and his work did not become widely accepted until the 1840s. However, Ohm received recognition for his contributions to science well before he died.\n\nIn the 1850s, Ohm's law was known as such and was widely considered proved, and alternatives, such as \"Barlow's law\", were discredited, in terms of real applications to telegraph system design, as discussed by Samuel F. B. Morse in 1855.\n\nThe electron was discovered in 1897 by J. J. Thomson, and it was quickly realized that it is the particle (charge carrier) that carries electric currents in electric circuits. In 1900 the first (classical) model of electrical conduction, the Drude model, was proposed by Paul Drude, which finally gave a scientific explanation for Ohm's law. In this model, a solid conductor consists of a stationary lattice of atoms (ions), with conduction electrons moving randomly in it. A voltage across a conductor causes an electric field, which accelerates the electrons in the direction of the electric field, causing a drift of electrons which is the electric current. However the electrons collide with and scatter off of the atoms, which randomizes their motion, thus converting the kinetic energy added to the electron by the field to heat (thermal energy). Using statistical distributions, it can be shown that the average drift velocity of the electrons, and thus the current, is proportional to the electric field, and thus the voltage, over a wide range of voltages.\n\nThe development of quantum mechanics in the 1920s modified this picture somewhat, but in modern theories the average drift velocity of electrons can still be shown to be proportional to the electric field, thus deriving Ohm's law. In 1927 Arnold Sommerfeld applied the quantum Fermi-Dirac distribution of electron energies to the Drude model, resulting in the free electron model. A year later, Felix Bloch showed that electrons move in waves (Bloch waves) through a solid crystal lattice, so scattering off the lattice atoms as postulated in the Drude model is not a major process; the electrons scatter off impurity atoms and defects in the material. The final successor, the modern quantum band theory of solids, showed that the electrons in a solid cannot take on any energy as assumed in the Drude model but are restricted to energy bands, with gaps between them of energies that electrons are forbidden to have. The size of the band gap is a characteristic of a particular substance which has a great deal to do with its electrical resistivity, explaining why some substances are electrical conductors, some semiconductors, and some insulators.\n\nWhile the old term for electrical conductance, the mho (the inverse of the resistance unit ohm), is still used, a new name, the siemens, was adopted in 1971, honoring Ernst Werner von Siemens. The siemens is preferred in formal papers.\n\nIn the 1920s, it was discovered that the current through a practical resistor actually has statistical fluctuations, which depend on temperature, even when voltage and resistance are exactly constant; this fluctuation, now known as Johnson–Nyquist noise, is due to the discrete nature of charge. This thermal effect implies that measurements of current and voltage that are taken over sufficiently short periods of time will yield ratios of V/I that fluctuate from the value of R implied by the time average or ensemble average of the measured current; Ohm's law remains correct for the average current, in the case of ordinary resistive materials.\n\nOhm's work long preceded Maxwell's equations and any understanding of frequency-dependent effects in AC circuits. Modern developments in electromagnetic theory and circuit theory do not contradict Ohm's law when they are evaluated within the appropriate limits.\n\nOhm's law is an empirical law, a generalization from many experiments that have shown that current is approximately proportional to electric field for most materials. It is less fundamental than Maxwell's equations and is not always obeyed. Any given material will break down under a strong-enough electric field, and some materials of interest in electrical engineering are \"non-ohmic\" under weak fields.\n\nOhm's law has been observed on a wide range of length scales. In the early 20th century, it was thought that Ohm's law would fail at the atomic scale, but experiments have not borne out this expectation. As of 2012, researchers have demonstrated that Ohm's law works for silicon wires as small as four atoms wide and one atom high.\n\nThe dependence of the current density on the applied electric field is essentially quantum mechanical in nature; (see Classical and quantum conductivity.) A qualitative description leading to Ohm's law can be based upon classical mechanics using the Drude model developed by Paul Drude in 1900.\n\nThe Drude model treats electrons (or other charge carriers) like pinballs bouncing among the ions that make up the structure of the material. Electrons will be accelerated in the opposite direction to the electric field by the average electric field at their location. With each collision, though, the electron is deflected in a random direction with a velocity that is much larger than the velocity gained by the electric field. The net result is that electrons take a zigzag path due to the collisions, but generally drift in a direction opposing the electric field.\n\nThe drift velocity then determines the electric current density and its relationship to E and is independent of the collisions. Drude calculated the average drift velocity from p = −\"eEτ\" where p is the average momentum, −\"e\" is the charge of the electron and τ is the average time between the collisions. Since both the momentum and the current density are proportional to the drift velocity, the current density becomes proportional to the applied electric field; this leads to Ohm's law.\n\nA hydraulic analogy is sometimes used to describe Ohm's law. Water pressure, measured by pascals (or PSI), is the analog of voltage because establishing a water pressure difference between two points along a (horizontal) pipe causes water to flow. Water flow rate, as in liters per second, is the analog of current, as in coulombs per second. Finally, flow restrictors—such as apertures placed in pipes between points where the water pressure is measured—are the analog of resistors. We say that the rate of water flow through an aperture restrictor is proportional to the difference in water pressure across the restrictor. Similarly, the rate of flow of electrical charge, that is, the electric current, through an electrical resistor is proportional to the difference in voltage measured across the resistor.\n\nFlow and pressure variables can be calculated in fluid flow network with the use of the hydraulic ohm analogy. The method can be applied to both steady and transient flow situations. In the linear laminar flow region, Poiseuille's law describes the hydraulic resistance of a pipe, but in the turbulent flow region the pressure–flow relations become nonlinear.\n\nThe hydraulic analogy to Ohm's law has been used, for example, to approximate blood flow through the circulatory system.\n\nIn circuit analysis, three equivalent expressions of Ohm's law are used interchangeably:\n\nEach equation is quoted by some sources as the defining relationship of Ohm's law,\nor all three are quoted, or derived from a proportional form,\nor even just the two that do not correspond to Ohm's original statement may sometimes be given.\n\nThe interchangeability of the equation may be represented by a triangle, where V (voltage) is placed on the top section, the I (current) is placed to the left section, and the R (resistance) is placed to the right. The line that divides the left and right sections indicates multiplication, and the divider between the top and bottom sections indicates division (hence the division bar).\n\nResistors are circuit elements that impede the passage of electric charge in agreement with Ohm's law, and are designed to have a specific resistance value \"R\". In a schematic diagram the resistor is shown as a zig-zag symbol. An element (resistor or conductor) that behaves according to Ohm's law over some operating range is referred to as an \"ohmic device\" (or an \"ohmic resistor\") because Ohm's law and a single value for the resistance suffice to describe the behavior of the device over that range.\n\nOhm's law holds for circuits containing only resistive elements (no capacitances or inductances) for all forms of driving voltage or current, regardless of whether the driving voltage or current is constant (DC) or time-varying such as AC. At any instant of time Ohm's law is valid for such circuits.\n\nResistors which are in \"series\" or in \"parallel\" may be grouped together into a single \"equivalent resistance\" in order to apply Ohm's law in analyzing the circuit.\n\nWhen reactive elements such as capacitors, inductors, or transmission lines are involved in a circuit to which AC or time-varying voltage or current is applied, the relationship between voltage and current becomes the solution to a differential equation, so Ohm's law (as defined above) does not directly apply since that form contains only resistances having value R, not complex impedances which may contain capacitance (\"C\") or inductance (\"L\").\n\nEquations for time-invariant AC circuits take the same form as Ohm's law. However, the variables are generalized to complex numbers and the current and voltage waveforms are complex exponentials.\n\nIn this approach, a voltage or current waveform takes the form formula_12, where \"t\" is time, \"s\" is a complex parameter, and \"A\" is a complex scalar. In any linear time-invariant system, all of the currents and voltages can be expressed with the same \"s\" parameter as the input to the system, allowing the time-varying complex exponential term to be canceled out and the system described algebraically in terms of the complex scalars in the current and voltage waveforms.\n\nThe complex generalization of resistance is impedance, usually denoted \"Z\"; it can be shown that for an inductor,\n\nand for a capacitor,\n\nWe can now write,\n\nwhere V and I are the complex scalars in the voltage and current respectively and Z is the complex impedance.\n\nThis form of Ohm's law, with \"Z\" taking the place of \"R\", generalizes the simpler form. When \"Z\" is complex, only the real part is responsible for dissipating heat.\n\nIn the general AC circuit, \"Z\" varies strongly with the frequency parameter \"s\", and so also will the relationship between voltage and current.\n\nFor the common case of a steady sinusoid, the \"s\" parameter is taken to be formula_16, corresponding to a complex sinusoid formula_17. The real parts of such complex current and voltage waveforms describe the actual sinusoidal currents and voltages in a circuit, which can be in different phases due to the different complex scalars.\n\nOhm's law is one of the basic equations used in the analysis of electrical circuits. It applies to both metal conductors and circuit components (resistors) specifically made for this behaviour. Both are ubiquitous in electrical engineering. Materials and components that obey Ohm's law are described as \"ohmic\" which means they produce the same value for resistance (R = V/I) regardless of the value of V or I which is applied and whether the applied voltage or current is DC (direct current) of either positive or negative polarity or AC (alternating current).\n\nIn a true ohmic device, the same value of resistance will be calculated from R = V/I regardless of the value of the applied voltage V. That is, the ratio of V/I is constant, and when current is plotted as a function of voltage the curve is \"linear\" (a straight line). If voltage is forced to some value V, then that voltage V divided by measured current I will equal R. Or if the current is forced to some value I, then the measured voltage V divided by that current I is also R. Since the plot of I versus V is a straight line, then it is also true that for any set of two different voltages V and V applied across a given device of resistance R, producing currents I = V/R and I = V/R, that the ratio (V−V)/(I−I) is also a constant equal to R. The operator \"delta\" (Δ) is used to represent a difference in a quantity, so we can write ΔV = V−V and ΔI = I−I. Summarizing, for any truly ohmic device having resistance R, V/I = ΔV/ΔI = R for any applied voltage or current or for the difference between any set of applied voltages or currents.\n\nThere are, however, components of electrical circuits which do not obey Ohm's law; that is, their relationship between current and voltage (their I–V curve) is \"nonlinear\" (or non-ohmic). An example is the p-n junction diode (curve at right). As seen in the figure, the current does not increase linearly with applied voltage for a diode. One can determine a value of current (I) for a given value of applied voltage (V) from the curve, but not from Ohm's law, since the value of \"resistance\" is not constant as a function of applied voltage. Further, the current only increases significantly if the applied voltage is positive, not negative. The ratio \"V\"/\"I\" for some point along the nonlinear curve is sometimes called the \"static\", or \"chordal\", or DC, resistance, but as seen in the figure the value of total \"V\" over total \"I\" varies depending on the particular point along the nonlinear curve which is chosen. This means the \"DC resistance\" V/I at some point on the curve is not the same as what would be determined by applying an AC signal having peak amplitude ΔV volts or ΔI amps centered at that same point along the curve and measuring ΔV/ΔI. However, in some diode applications, the AC signal applied to the device is small and it is possible to analyze the circuit in terms of the \"dynamic\", \"small-signal\", or \"incremental\" resistance, defined as the one over the slope of the V–I curve at the average value (DC operating point) of the voltage (that is, one over the derivative of current with respect to voltage). For sufficiently small signals, the dynamic resistance allows the Ohm's law small signal resistance to be calculated as approximately one over the slope of a line drawn tangentially to the V-I curve at the DC operating point.\n\nOhm's law has sometimes been stated as, \"for a conductor in a given state, the electromotive force is proportional to the current produced.\" That is, that the resistance, the ratio of the applied electromotive force (or voltage) to the current, \"does not vary with the current strength .\" The qualifier \"in a given state\" is usually interpreted as meaning \"at a constant temperature,\" since the resistivity of materials is usually temperature dependent. Because the conduction of current is related to Joule heating of the conducting body, according to Joule's first law, the temperature of a conducting body may change when it carries a current. The dependence of resistance on temperature therefore makes resistance depend upon the current in a typical experimental setup, making the law in this form difficult to directly verify. Maxwell and others worked out several methods to test the law experimentally in 1876, controlling for heating effects.\n\nOhm's principle predicts the flow of electrical charge (i.e. current) in electrical conductors when subjected to the influence of voltage differences; Jean-Baptiste-Joseph Fourier's principle predicts the flow of heat in heat conductors when subjected to the influence of temperature differences.\n\nThe same equation describes both phenomena, the equation's variables taking on different meanings in the two cases. Specifically, solving a heat conduction (Fourier) problem with \"temperature\" (the driving \"force\") and \"flux of heat\" (the rate of flow of the driven \"quantity\", i.e. heat energy) variables also solves an analogous electrical conduction (Ohm) problem having \"electric potential\" (the driving \"force\") and \"electric current\" (the rate of flow of the driven \"quantity\", i.e. charge) variables.\n\nThe basis of Fourier's work was his clear conception and definition of thermal conductivity. He assumed that, all else being the same, the flux of heat is strictly proportional to the gradient of temperature. Although undoubtedly true for small temperature gradients, strictly proportional behavior will be lost when real materials (e.g. ones having a thermal conductivity that is a function of temperature) are subjected to large temperature gradients.\n\nA similar assumption is made in the statement of Ohm's law: other things being alike, the strength of the current at each point is proportional to the gradient of electric potential. The accuracy of the assumption that flow is proportional to the gradient is more readily tested, using modern measurement methods, for the electrical case than for the heat case.\n\nOhm's law, in the form above, is an extremely useful equation in the field of electrical/electronic engineering because it describes how voltage, current and resistance are interrelated on a \"macroscopic\" level, that is, commonly, as circuit elements in an electrical circuit. Physicists who study the electrical properties of matter at the microscopic level use a closely related and more general vector equation, sometimes also referred to as Ohm's law, having variables that are closely related to the V, I, and R scalar variables of Ohm's law, but which are each functions of position within the conductor. Physicists often use this continuum form of Ohm's Law:\n\nwhere \"E\" is the electric field vector with units of volts per meter (analogous to \"V\" of Ohm's law which has units of volts), \"J\" is the current density vector with units of amperes per unit area (analogous to \"I\" of Ohm's law which has units of amperes), and \"ρ\" (Greek \"rho\") is the resistivity with units of ohm·meters (analogous to \"R\" of Ohm's law which has units of ohms). The above equation is sometimes written as J = formula_19E where \"σ\" (Greek \"sigma\") is the conductivity which is the reciprocal of ρ.\nThe voltage between two points is defined as:\n\nwith formula_21 the element of path along the integration of electric field vector E. If the applied E field is uniform and oriented along the length of the conductor as shown in the figure, then defining the voltage V in the usual convention of being opposite in direction to the field (see figure), and with the understanding that the voltage V is measured differentially across the length of the conductor allowing us to drop the Δ symbol, the above vector equation reduces to the scalar equation:\n\nSince the E field is uniform in the direction of wire length, for a conductor having uniformly consistent resistivity ρ, the current density J will also be uniform in any cross-sectional area and oriented in the direction of wire length, so we may write:\n\nSubstituting the above 2 results (for \"E\" and \"J\" respectively) into the continuum form shown at the beginning of this section:\n\nThe electrical resistance of a uniform conductor is given in terms of resistivity by:\nwhere \"l\" is the length of the conductor in SI units of meters, \"a\" is the cross-sectional area (for a round wire \"a\" = \"πr\" if \"r\" is radius) in units of meters squared, and ρ is the resistivity in units of ohm·meters.\n\nAfter substitution of \"R\" from the above equation into the equation preceding it, the continuum form of Ohm's law for a uniform field (and uniform current density) oriented along the length of the conductor reduces to the more familiar form:\n\nA perfect crystal lattice, with low enough thermal motion and no deviations from periodic structure, would have no resistivity, but a real metal has crystallographic defects, impurities, multiple isotopes, and thermal motion of the atoms. Electrons scatter from all of these, resulting in resistance to their flow.\n\nThe more complex generalized forms of Ohm's law are important to condensed matter physics, which studies the properties of matter and, in particular, its electronic structure. In broad terms, they fall under the topic of constitutive equations and the theory of transport coefficients.\n\nIf an external B-field is present and the conductor is not at rest but moving at velocity v, then an extra term must be added to account for the current induced by the Lorentz force on the charge carriers.\n\nIn the rest frame of the moving conductor this term drops out because v= 0. There is no contradiction because the electric field in the rest frame differs from the E-field in the lab frame: E′ = E + v×B.\nElectric and magnetic fields are relative, see Lorentz transformation.\n\nIf the current J is alternating because the applied voltage or E-field varies in time, then reactance must be added to resistance to account for self-inductance, see electrical impedance. The reactance may be strong if the frequency is high or the conductor is coiled.\n\nIn a conductive fluid, such as a plasma, there is a similar effect. Consider a fluid moving with the velocity formula_28 in a magnetic field formula_29. The relative motion induces an electric field formula_30 which exerts electric force on the charged particles giving rise to an electric current formula_31. The equation of motion for the electron gas, with a number density formula_32, is written as\n\nwhere formula_34, formula_35 and formula_36 are the charge, mass and velocity of the electrons, respectively. Also, formula_37 is the frequency of collisions of the electrons with ions which have a velocity field formula_38. Since, the electron has a very small mass compared with that of ions, we can ignore the left hand side of the above equation to write\n\nwhere we have used the definition of the current density, and also put formula_40 which is the electrical conductivity. This equation can also be equivalently written as\n\nwhere formula_42 is the electrical resistivity. It is also common to write formula_43 instead of formula_44 which can be confusing since it is the same notation used for the magnetic diffusivity defined as formula_45.\n\n\n"}
{"id": "54364", "url": "https://en.wikipedia.org/wiki?curid=54364", "title": "Pasigraphy", "text": "Pasigraphy\n\nA pasigraphy (from Greek πᾶσι \"pasi\" \"to all\" and γράφω \"grapho\" \"to write\") is a writing system where each written symbol represents a concept (rather than a word or sound or series of sounds in a spoken language).\n\nThe aim (as with ordinary numerals 1, 2, 3, etc.) is to be intelligible to persons of all languages. The term was first applied to a system proposed in 1796, though a number of pasigraphies had been devised prior to that; Leopold Einstein reviews 60 attempts at creating an international auxiliary language, the majority of the 17th–18th century projects being pasigraphies of one kind or another, and several pasigraphies and auxiliary languages, including some sample texts, are also reviewed in Arika Okrent's book on constructed languages. Leibniz wrote about the alphabet of human thought and Alexander von Humboldt corresponded with Peter Stephen Du Ponceau who proposed a universal phonetic alphabet.\n\nExamples of pasigraphies include Blissymbols and Real Character.\n\n"}
{"id": "1928729", "url": "https://en.wikipedia.org/wiki?curid=1928729", "title": "Physical plane", "text": "Physical plane\n\nThe physical plane (also known as a hyperplane), physical world, or physical universe, in emanationist metaphysics such as are found in Neoplatonism, Hermeticism, Hinduism and Theosophy, refers to the visible reality of space and time, energy and matter: the physical universe in Occultism and esoteric cosmology is the lowest or densest of a series of planes of existence (hyperplanes that are said to be nested).\n\n"}
{"id": "14941587", "url": "https://en.wikipedia.org/wiki?curid=14941587", "title": "Professional practice of behavior analysis", "text": "Professional practice of behavior analysis\n\nThe professional practice of behavior analysis is one domain of behavior analysis: the others being radical behaviorism, experimental analysis of behavior and applied behavior analysis. The professional practice of behavior analysis is the delivery of interventions to consumers that are guided by the principles of behaviorism and the research of both the experimental analysis of behavior and applied behavior analysis. Professional practice seeks maximum precision to change behavior most effectively in specific instances. Behavior analysts are mental health professionals and, in some states, may hold a license, certificate or registration as a behavior analyst. In other states, there are no laws governing their practice and, as such, the practice may be prohibited as falling under the practice definition of other mental health professionals. This is rapidly changing as Behavior Analysts are becoming more and more common.\n\nThe professional practice of behavior analysis is a hybrid discipline with specific influences coming from counseling, psychology, education, special education, communication disorders, physical therapy and criminal justice. As a discipline it has its own conferences, organizations, certification processes and awards.\n\nThe Behavior Analysis Certification Board (BACB) defines behavior analysis as:\nAs the above suggests, behavior analysis is based on the principles of operant and respondent conditioning. Applied behavior analysis (ABA) include the use of behavior management, behavioral engineering and behavior therapy. Behavior analysis is an active, environmental-based approach.\n\nCurrently in the U.S. some behavior analysts at the masters level are licensed; others work with an international certification where licenses are unavailable, although this may not be allowed in some states or jurisdictions. At the doctoral level many are licensed as psychologists with Diplomate status in behavioral psychology or licensed as licensed behavior analysts. Diplomate status alone, however, does not allow one to practice in every state and each state's regulatory statute must be reviewed for the appropriateness and legality of practice.\n\nThe Behavior Analyst Certification Board (BACB) offers a technical certificate in behavior analysis. The American Psychological Association offers a diplomate (post PhD and licensed certification) in behavioral psychology.\n\nBACB is a private non-profit organization without governmental powers to regulate behavior analytic practice. However it does wield the power to suspend or revoke certification from those certified if they violate the strict ethical guidelines of practice. As many states are without a licensure act, this has been sufficient to deter violators as it removes their ability to vendor with the state, schools, and insurance companies under that certification. While the BACB certification means that candidates have satisfied entry-level requirements in behavior analytic training, certificants are able to practice independently within the scope of their practice and training. Thus, a BCBA (such as those who go into marketing, engineering, or other approved fields in which BCBAs work) who has never trained to work nor worked with children diagnosed with autism should not attempt to do so independently. Most health insurance companies also recognize the BCBA credential as one conferring the capability and the right to practice independently in many states (including California with the recent passage of SB 946 into law). Some states still require certificants to be licensed by their respective jurisdictions for independent practice when treating behavioral health or medical problems, and a number of states including Arizona and Nevada have created a specific BCBA licensing program (for a full list please refer to the BACB website). Licensed certificants must operate within the scope of their license and within their areas of expertise. Where the government regulates behavior analytic services, unlicensed certificants may be supervised by a licensed professional and operate within the scope of their supervisor's license when treating disorders if that jurisdiction allows such supervision. Unlicensed certificants who provide behavior analytic training for educational or optimal performance purposes do not require licensed supervision, unless the law or precedent prohibits such practice. Where the government does not regulate the treatment of medical or psychological disorders certificants should practice in accord with the laws of their state, province, or country. All certificants must practice within their personal areas of expertise.\n\nThe model licensing act for behavior analysts has been revised several times to reflect best practices and policy. Previous versions included provisions that would have made it in practice more difficult then to obtain the necessary experiential hours for license and independent practice as a clinical psychologist.\n\nOnce the person is licensed public protection is still monitored by the licensing board as well as the BACB, both of which make sure that the person receives sufficient ongoing education, and the BACB and licensing board investigate ethical complaints. A licensed behavior analyst would have equal training, knowledge, skills and abilities in their discipline as would a mental health counselor or marriage and family therapist in their discipline. In February 2008, Indiana, Arizona, Massachusetts, Vermont, Oklahoma and other states now have legislation pending to create licensure for behavior analysts. Pennsylvania was the first state in 2008 to license behavior specialists to cover behavior analysts. Arizona, less than three weeks later, became the first state to license behavior analysts. Other states such as Nevada and Wisconsin have also passed behavior analytic licensure.\n\nIn California, after the defeat of a bill to create a license for BCBAs in 2011, the state government instead passed SB 946 which mandates that all non-governmental insurance agencies reimburse for BCBA for behavior therapy in treating autism, starting in 2012. Unlike many weaker bills mandating that autism be covered by insurance, SB 946 does not currently impose an arbitrary cap on services by age or funding amount – in this it is similar to other treatments such as those for heart attacks or other chronic conditions.\n\nBehavior analytic services can be and often are delivered through various treatment modalities. These include:\n\nThe two \"primary methods\" for delivering behavior analytic services are consultation and/or direct therapy; the former involves three parties: consultant, consultee and a client whose behavior is changed (who may or may not be present for all meetings).\n\nConsultation can involve working with the consultee (i.e., a parent or teacher) to build a plan around the behavior of a client (i.e., a child or student), or training the consultees themselves to modify the behavior of the client. Within the domain of parent–child consultation, standard intervention includes teaching parents skills such as basic reinforcement, time-out and how to manipulate different factors to modify behavior.\n\nDirect therapy involves the relationship of behavior analyst and client, usually one-on-one, in which the analyst is responsible for directly modifying the behavior of their client. Direct therapy is also used in schools but can also be found in group homes, in a behavior modification facility and in behavior therapy (where the focus may be on tasks such as quitting smoking, modifying behaviors for sex offenders or other types of offenders, modifying behaviors related to mood disorders) or to encourage job seeking behavior in psychiatric patients.\n\nTwo older and less used models still exist for the delivery of behavior analytic services. These models worked mostly with normal or typically developing populations. These two models are the \"Behavioral Coaching\" and the \"Behavioral Counseling\" model. Both were very popular in the 1960s–1980s but have recently seen a decline in popularity, in spite of their success, as proponents argued the merits of holding strictly to learning theory. The Association for Behavior Analysis International still retains a special interest group in behavioral counseling and coaching.\n\nBehavioral counseling was very popular throughout the 1970s and at least into the early 1980s. Behavioral counseling is an active action–oriented approach that works with the typically developing population but also assists people with specific/discrete problems such as career decision making, drinking, smoking or rehabilitation after injury.\n\nThe behavioral coaching model is sometimes referred to as life coaching. However, like counselors and psychologists, life coaches can have varied orientations/change theories (see behavioral change theories). Behavioral life coaches operate mainly from a behavior analytic orientation. Unlike therapy this model is applied to people who desire to achieve a specific goal such as increasing their assertiveness with others. This model is educational and is usually presented as an alternative to therapy. Coaches use behavioral techniques such as objective setting, goal setting, self-control training and behavioral activation to help clients achieve specific life goals. Behavioral coaching was sometimes used to teach job skills to people having mental retardation or head injury. In this area the model made extensive use of task analysis, direct instruction, role play, reinforcement and error correction. Often this approach employs techniques of direct instruction.\n\nBehavioral counseling was largely seen as a growth model that tried to increase the individuals sense of \"freedom\" by helping the client reduce punishment or coercion in their lives, build skills, and increase access to reinforcement. B.F. Skinner created a video discussing the processes involved and the importance of reinforcement to increase the sense of \"freedom\". Behavioral counseling attempts to use in-session reinforcement to improve decision-making, functional assessment of the clients problem, and behavioral interventions to reduce problem behaviors.\n\nSome behavioral counselors approach therapy from a social learning perspective but many held a position based on the use of behavioral psychology with a focus on the use of operant, respondent conditioning procedures. Some who did adopt a position on modeling held closer to the behavioral view of modeling as generalized imitation developed through learning processes.\n\nThe behavioral counseling approach became very popular in weight reduction and is on the American Psychological Association's list of evidence-based practices for weight loss. Behavioral counseling for weight loss by Richard B. Stuart led to the commercial program called \"Weight Watchers\". Recently, efforts have been made to resurrect interest in behavioral counseling as a method to effectively deliver services to normal problemed populations.\n\nAmong the available approaches to treating autism, early intensive behavioral interventions (EIBIs) have demonstrated efficacy in promoting social and language development and in reducing behaviors that interfere with learning and cognitive functioning. In addition, such therapies have led to increased intellectual skills and increased adaptive functioning. Even with past successes, behavior therapists continue to develop models of social skills.\n\nThese are generally treatments based on applied behavior analysis (ABA) and involve intensive training of the therapists, extensive time spent in ABA therapy (20–40 hours per week) and weekly supervision by experienced clinical supervisors—known as board certified behavior analysts. ABA therapy often employs principles of overlearning to help acquire mastery and fluency of skills.\n\nThe ABA approach teaches many skills such as appropriate play (a precursor to social interaction and engagement with the world and others), social, motor and verbal behaviors as well as reasoning skills and the ability to self-regulate appropriately. ABA therapy is used to teach behaviors to individuals with autism who may not otherwise observe these behaviors spontaneously through imitation.\n\nImitation can also be directly trained. ABA therapies teach these skills through use of behavioral observation and reinforcement or prompting to teach each step of a behavior.\n\nExtensive research exists to show that behavior analysis is an effective treatment for autism with literally hundreds of studies showing its effectiveness with persons of all ages in enhancing functioning, building skills and independence as well as improving life quality. What remains controversial are claims of behavior analysis \"curing autism\". This controversy exists because behavior analysis is used to alter rates of behavior, and not the condition of \"autism.\" Nonetheless, behavior analysis is used to treat the behaviors of many in the autistic population. While several small studies exist showing that behavior analysis holds promise in this area, the number of well-controlled studies do not rise to the level required by the American Psychological Association to hold the treatment as empirically supported in this area.\n\nAn increasing amount of research in the field of applied behavior analysis is concerned with autism; and it is a common misconception that behavior analysts work almost exclusively with individuals with autism and that ABA is synonymous with discrete trials teaching. ABA principles can also be used with a range of typical or atypical individuals whose issues vary from developmental delays, significant behavioral problems or undesirable habits.\n\nCurriculum development in behavior analytic programs for children with autism is important. Curriculum should carefully task analyze the skill needed to be learned and then ensure that proper tool skills have been taught before the skill itself is attempted to be taught. Applied behavior analysis is often confused as a \"table-only\" therapy. Properly performed, applied behavior analysis should be done in both table and natural environments depending on the student's progress and needs. Once a student has mastered a skill at the table the team should move the student into a natural environment for further training and generalization of the skill.\n\nFrequently standardized assessments such as the Assessment of Basic Language and Learning Skills (ABLLS) is used to create a baseline of the learner's functional skill set. The ABLLS breaks down the learner's strengths and weaknesses to best tailor the applied behavior analysis curriculum to them. By focusing on the exact skills that need help the teacher does not teach a skill the student knows. This can also prevent student frustration at attempting a skill for which they are not ready.\n\nMany families have fought school districts for such programs. Donald Baer, a behavior analyst who often testified as an expert witness, provided several letters to lawyers before he died. Ohio State has archived those letters.\n\nDiscrete trials were originally used by people studying classical conditioning to demonstrate stimulus–stimulus pairing. Discrete trials are often contrasted with free operant procedures, like ones used by B.F. Skinner in learning experiments with rats and pigeons, to show how learning was influenced by rates of reinforcement. The discrete trials method was adapted as a therapy for developmentally delayed children and individuals with autism. For example, Ole Ivar Lovaas used discrete trials to teach autistic children skills including making eye contact, following simple instructions, advanced language and social skills. These discrete trials involved breaking a behavior into its most basic functional unit and presenting the units in a series.\n\nA discrete trial usually consists of the following: the antecedent, the behavior of the student and a consequence. If the student's behavior matches what is desired the consequence is something positive: food, candy, a game, praise, etc. If the behavior was not correct the teacher offers the correct answer then repeats the trial possibly with more prompting, if needed.\n\nThere is usually an inter-trial interval that allows for a few seconds to separate each trial to allow the student to process the information, teach the student to wait and make the onset of the next trial more discrete. Discrete trials can be used to develop most skills which includes cognitive, verbal communication, play, social and self-help skills. There is a carefully laid out procedure for error correction and a problem solving model to use if the program gets stuck. Discrete trial is sometimes referred to as the Lovaas technique.\n\nDiscrete trials have been helpful in the treatment of pediatric feeding problems as well as in the prevention of feeding problems.\n\nIn language training, many free operant procedures emerged in the late 1960s and early 1970s. These procedures did not try to train discrimination first, and then passively wait for generalization, but instead worked from the start on actively promoting generalization. Initially the model was referred to as \"incidental teaching\" but later was called \"milieu language teaching\" and finally \"natural language teaching\". Peterson (2007) completed a comprehensive review of 57 studies on these training procedures. This review found that 84% of the studies of the natural language procedures looked at maintenance and 94% looked at generalization and were able to provide direct support of its occurrence as part of the training.\n\nDougher's edited volume titled \"Clinical Behavior Analysis\" on Context Press highlights the application of behavior analysis to adult outpatients. He identifies four comprehensive behavior analytic programs: Stephen Hayes \"et al.\" acceptance and commitment therapy (ACT), Jacobson et al. behavioral activation (BA), Kohlenberg & Tsai's functional analytic psychotherapy, Linehan Dialectical Behavior Therapy, and the community reinforcement approach for treating addictions. In addition, the book highlights several recent areas of functional analysis research for common clinical problems. Many of these areas are specified in the section on behavior therapy.\n\nThe study of behavioral factors related to addicitions has a long history. Thus it is no surprise many behavioral treatments would be found to be efficacious. One efficacious approach is the community reinforcement approach. The community reinforcement approach has considerable research supporting it as efficacious. Started in the 1970s by Nathan H. Azrin and his graduate student Hunt, the community reinforcement approach is a comprehensive operant program built on a functional assessment of a client's drinking behavior and the use of positive reinforcement and contingency management for nondrinking. When combined with disulfiram (an aversive procedure) community reinforcement showed remarkable effects. One component of the program that appears to be particularly strong is the non-drinking club. Applications of community reinforcement to public policy has become the recent focus of this approach.\n\nAn offshoot of the community reinforcement approach is the community reinforcement approach and family training. This program is designed to help family members of substance abusers feel empowered to engage in treatment. The rates of success have varied somewhat by study but seem to cluster around 70%. The program uses a variety of interventions based on functional assessment including a module to prevent domestic violence. Partners are trained to use positive reinforcement, various communication skills and natural consequences.\n\nWith children, applied behavior analysis provides the core of the positive behavior support movement and creates the basis of Teaching-Family Model homes. Teaching-Family homes have been found to reduce recidivism for delinquent youths both while they are in the homes and after they leave. Operant procedures form the basis of behavioral parent training developed from social learning theorists. The etiological models for antisocial behavior show considerable correlation with negative reinforcement and response matching. Behavioral parent training or Parent Management Training has been very successful in the treatment of conduct disorders in children and adolescents with recent research focusing on making it more culturally sensitive. In addition, behavioral parent training has been shown to reduce corporal or abusive child discipline tactics. Behavior analysts typically adhere to a behavioral model of child development in their practice (see child development).\n\nRecent studies showing that behavior analysis can reduce recidivism have led to a resurgence in behavior therapy facilities. Of particular interest has been the growing research on the Teaching-Family Model which was developed by Montrose Wolf and clearly reduces recidivism rates. In addition, behaviorally-based early intervention programs have shown effectiveness.\n\nMethods of counter-conditioning and respondent extinction, called exposure therapy, are often employed by many behavior therapists in the treatment of phobias, anxiety disorders such as post-traumatic stress disorder (PTSD), and addictions (cue exposure). Prolonged exposure therapy has been particularly helpful with PTSD. Several procedures to block respondent conditioning such as blocking and overshadowing are sometimes used in behavioral medicine to prevent conditioned taste aversion for patients with chemotherapy treatments. Exposure with Response Prevention (ERP) is a respondent extinction procedure often used to treat obsessive–compulsive behavior. Escape response blocking is critical for this procedure. For PTSDs exposure therapy is one of the few evidence-based techniques. Recent research suggests exposure therapy is an excellent means of alleviating both the anxiety and cognitive symptoms specific to PTSD with no additive effect for additional cognitive components. Several authors have argued that exposure by itself is necessary and sufficient to produce behavior change in reducing fear in social phobics and helping them engage more effectively with others. \"The Washington Post\" ran a story that only exposure therapy is proven for PTSD and that cognitive therapy or even drug therapy are not shown at this time to be effective.\n\nKamiya (1968) demonstrated that the alpha rhythm in humans could be operantly conditioned. He published an influential article in \"Psychology Today\" that summarized research showing subjects learn to discriminate when alpha was present or absent, and that they could use feedback to shift the dominant alpha frequency about 1 Hz. Almost half of his subjects reported experiencing a pleasant \"alpha state\" characterized as an \"alert calmness\". These reports may have contributed to the perception of alpha biofeedback as a shortcut to a meditative state. He also studied the electroencephalography (EEG) correlates of meditative states. Operant conditioning of EEG has had considerable support in many areas including attention deficit hyperactivity disorder (ADHD) and even seizure disorders. Early studies of the procedure included the treatment of seizure disorders. Luber and colleagues (1981) conducted a double blind crossover study showing that seizure activity decreased by 50% in the contingent conditioning of inhibiting brain waves as opposed to the non-contingent use. Sterman (2000) reviewed 18 studies of a total of 174 clients and found 82% of the participants had significant seizure reduction (30% less weekly seizures).\n\nBehavior analysis with organizations is sometimes combined with systems theory in an approach called organizational behavior management. This approach has shown success particularly in the area of behavior-based safety. Behavior safety research has lately become focused on factors that lead programs to being retained in institutions long after the designer leaves.\n\nDirect instruction and Direct Instruction: the former representing the process and the latter a specific curriculum that highlights that process remain both current and controversial in behavior analysis. The essential features are a carefully structured fast paced program based on teacher-directed small group instruction. One controversy that remains is that teacher creativity is admonished in the program. Even with such issues to be worked out positive gains in reading for the approach have been reported in the literature since 1968. An example of the positive gains reported by Meyer (1984) found that 34% of children in the DISTAR group were accepted to college as compared to only 17% of the control school. Current research is focused on peer delivery of the program.\n\nSchool-wide positive behavior support is based on the use of behavior analytic procedures delivered in an organizational behavior management approach. School-wide behavioral support has been increasingly accepted by administrators, law–makers and teachers as a way to improve safety in classrooms.\n\nCurriculum-based measurement and curriculum matching is another active area of application. Curriculum-based measurement uses rate and reading performance as the primary variable in determining reading levels. The goal is to better match children to the appropriate curriculum level to remove frustration as well as to track reading performance over time to see if it is improving with intervention. This model also serves as the basis for response to intervention models.\n\nFunctional behavioral assessment was mandated in the United States for children who meet criteria under the \"individuals with disabilities education act\". This approach has precluded many procedures for modifying and maintaining children in not just the school system, but in many cases in the regular education setting. Even children with severe behavior problems appear to be helped.\n\nTeaching children to recruit attention has become a very important area in education. In many cases one function of children's disruptive behavior is to get attention.\n\nOne area of interest in hospitals is the blocking effect—especially for conditioned taste aversion. This area of interest is considered important in the prevention of weigh loss during chemotherapy for cancer patients. Another area of growing interest in the hospital setting is the use of operant-based biofeedback with those suffering from cerebral palsy or minor spinal injuries.\n\nBrucker's group at the University of Miami has had some success with specific operant conditioning-based biofeedback procedures to enhance functioning. While such methods are not a cure, and gains tend to be in the moderate range, they do show ability to help remaining central nervous system cells to regain some control over lost areas of functioning.\n\nBehavioral interventions have been very helpful in reducing problem behaviors in residential treatment centers. The type of residential versus mental retardation does not appear to be a factor. Behavioral interventions have been found to be successful even when medication interventions fail.\n\nProbably one of the most interesting applications of behavior analysis in the 1960s was its contribution to the space program. Research in this area is used to train astronauts including the chimpanzees sent into space. Continued work in this area focuses on ensuring that astronauts who live in confined areas and space do not develop behavioral health problems. Most of this work was led by pioneer behaviorist Joseph V. Brady.\n\nOpen communication and a supportive relationship between educational systems and families allow the student to receive a beneficial education. This pertains to typical learners as well as to individuals who need additional services. It was not until the 1960s that researchers began exploring behavior analysis as a method to educate those children who fall somewhere along the autism spectrum. Behavior analysts agree that consistency in and out of the school classroom is key in order for children with autism to maintain proper standing in school and continue to develop to their greatest potential.\n\nApplied behavior analysts sometimes work with a team to address a person's educational or behavioral needs. Other professionals such as speech therapists, physicians and the primary caregivers are treated as key to the implementation of successful therapy in the applied behavior analysis (ABA) model. The ABA method relies on behavior principles to develop treatments appropriate for the individual. Regular meetings with professionals to discuss programming are one way to establish a successful working relationship between a family and their school. It is beneficial when a caregiver can conduct generalization procedures outside of school. In the ABA framework, developing and maintaining a structured working relationship between parents or guardians and professionals is essential to ensure consistent treatment.\n\nWhen working directly with clients, behavior analysts engage in a process of collaborative goal setting. Goal setting ensures that the client is already under stimulus control of the goal and is thus more likely to engage in behavior to achieve it. Behavior analytic programs are ultimately skill building, they enhance functioning, lead to higher quality of life, and build self-control. One of the most distinguishing features of behavior analysis has been its core belief that all individuals have a right to the most effective treatment for their condition. and a right to the most effective educational strategy available.\n\nApplied behavior analysis is the applied side of the experimental analysis of behavior. It is based on the principles of operant and respondent conditioning and represents a major approach to behavior therapies. Its origin can be traced back to Teodoro Ayllon and Jack Michael's 1959 article \"The psychiatric nurse as a behavioral engineer\" as well as to initial efforts to implement teaching machines.\n\nThe research basis of ABA can be found in the theoretical work of behaviorism and radical behaviorism originating with the work of B.F. Skinner. In 1968 Baer, Wolf and Risley wrote an article that was the source of contemporary applied behavior analysis providing the criteria to judge the adequacy of research and practice in applied behavior analysis. It became the core and centerpiece behavioral engineering.\n\nWork in respondent conditioning (what some would term classical conditioning) began with the work of Joseph Wolpe in the 1960s. It was improved by the work of Edna B Foa who did extensive research on exposure and response prevention for obsessive–compulsive disorder (OCD). In addition, she worked on exposure therapy for post-traumatic stress disorder.\n\nOver the years most behavior analysts have existed and conducted research in many areas and University departments: behavior analysis, psychology, special education, regular education, speech–language pathology, communication disorders, school psychology, criminal justice and family life. They have belonged to many organizations including the American Psychological Association (APA) and have most often found a core intellectual home in the Association for Behavior Analysis International.\n\nWith a core focus on enhanced functioning and skill development behavior analytic interventions under the heading behavior therapy have come to form the core of evidence-based practices in speech–language pathology, organizational behavior management, education, mental health and addiction treatments. In the area of mental health and addictions a recent article looked at APA's list for well-established and promising practices and found a considerable number of them based on the principles of operant conditioning and respondent conditioning. A 1985 meta-analysis of social skills training methods found operant conditioning procedures had the largest effect size, the greatest generalization and the shortest training time; modeling, coaching, and social cognitive techniques, respectively, had smaller and smaller effect sizes.\n\nBehavior analysis remains one of the most active research areas in all of psychology, counseling, special education, developmental disability, mental health and other studies of human behavior. Current research in behavior analysis focuses on expanding the tradition by looking at setting events, behavioral activation, the Matching law, relational frame theory, stimulus equivalences and covert conditioning as exemplified in Skinner's model of rule-governed behavior \"Verbal Behavior\". Behavior analysis has moved past being just basic interventions for problems and into more comprehensive analyses of child development behavior.\n\nExperimental psychopathology is a behavior therapy area in which animal models are developed to simulate human pathology. For example, Wolpe studied cats to build his theory of human anxiety. This work continues today in the study of both pathology and treatment.\n\nInitially, applied behavior analysis used punishment such as shouting and slaps to reduce unwanted behaviors. Ethical opposition to such aversive practices caused them to fall out of favor and has stimulated development of less aversive methods. In general, aversion therapy and punishment are now less frequently used as ABA treatments due to legal restrictions. However, procedures such as odor aversion, covert sensitization and other covert conditioning procedures, based on punishment or aversion strategies, are still used effectively in the treatment of pedophiles. In addition, with some populations such as conduct disorder in children there is considerable evidence that has developed to show that all positive programs can produce change but that children will not enter into the normal range without punishment procedures. These programs have shifted to using child time-out and response–cost procedures to ensure that clients rights to effective interventions are met.\n\nIn 1973 the APA removed homosexuality from its Diagnostic and Statistical Manual yet it kept \"ego dystonic\" homosexuality as a condition until the DSM III-R (1987). In 1974 Ole Ivar Lovaas, pioneer of the use of discrete trial teaching (DTT) to treat autism, was the second author on a journal article describing the use of ABA to reduce \"feminine\" behaviors and increase \"masculine\" behaviors of a male child in an effort to prevent adult transsexualism. Treatments designed to uphold traditional sex-role behaviors were opposed by some behavior analysts who argued that the intervention was not justified. In the late 1960s Wolpe refused to treat homosexual behavior arguing that it was easier and more productive to treat the religious guilt than the homosexuality. He instead provided assertiveness training to a homosexual client. Most behavior analysts and behavior therapists have not worked in sexual re–orientation therapy since Gerald Davison argued that the issue was not one of effectiveness but of ethics. When he wrote the paper presenting this position, Davison was president of the Association for the Advancement of Behavior Therapy, now the Association for Behavioral and Cognitive Therapies, and thus his views carried much weight. Davison argued that homosexuality is not pathological and is only a problem if it is regarded as one by society and the therapist.\n\nThe use of punishment and aversion therapy procedures are a constant ethical challenge for behavior analysts. One of the original reasons for the development of the Behavior Analyst Certification Board were cases of abuse from behaviorists. Both continue to draw proponents and opposition, however, in some of the more controversial cases some middle ground has been found through legislation (see Judge Rotenberg Educational Center).\n\nA study in 1991 showed that behavior modification was effective in sex offender treatment and covert sensitization, and it has been shown to have some effects on reducing recidivism. However Gene Able, who has done extensive research in this area, suggests that it is not as effective outside of the package which contains odor aversion, satiation therapy (mastubatory reconditioning), and various social skills training programs including empathy training. Current behavior analysis programs offer this type of comprehensive treatment approach. In addition they use a combination of functional assessment, behavior chain analysis and risk assessment to create relapse prevention strategies and to help the offender to develop better self-control.\n\nWith sex offenders who have retardation, comprehensive behavioral programming has been effective at least in the short run. This treatment included formal academic and vocational training, sex education, a unit token economy, and individual behavior therapy including sexual reconditioning. In addition it included supported competitive employment, fading of program structure, and increased community participation.\n\nThere are multiple journals which produce articles on the clinical applications of applied behavior analysis. The most popular, and widely used, of these journals is the \"Journal of Applied Behavior Analysis\". There are many other journals dedicated to this field. Some of these include \"The Behavior Analyst Today\", the \"International Journal of Behavioral Consultation and Therapy\" and three new journals scheduled for release in 2008: \"Behavior Analysis in Sports, Health, Fitness and Behavioral Medicine\", \"the Journal of Behavior Analysis in Crime and Victim: Treatment and Prevention\" as well as the Association for Behavior Analysis International's \"Behavior Analysis in Practice\".\n\nThe Association for Behavior Analysis International has a special interest group for practitioner issues, behavioral counseling, and clinical behavior analysis. The Association for Behavior Analysis International has larger special interest groups for autism and behavioral medicine. The Association for Behavior Analysis International serves as the core intellectual home for behavior analysts. The Association for Behavior Analysis International sponsors multiple conferences/year, including the annual conference, annual autism conference, biannual international conference, and other conferences on specific issues such as behavioral theory and sustainability.\n\nThe Association for Behavioral and Cognitive Therapies (ABCT) also has an interest group in behavior analysis, which focuses on clinical behavior analysis. In addition, the Association for Behavioral and Cognitive Therapies has a special interest group on addictions.\n\nDoctoral level behavior analysts who are psychologists belong to the American Psychological Association's division 25: Behavior analysis. APA offers a diplomate in behavioral psychology.\n"}
{"id": "41138014", "url": "https://en.wikipedia.org/wiki?curid=41138014", "title": "Radiation law for human mobility", "text": "Radiation law for human mobility\n\nThe radiation law is way of modeling human mobility (geographic mobility, human migration) and it gives better empirical predictions than the gravity model of migration which is widely used in this subject.\n\nWaves of migration due to displacement by war, or exploitation in the hope of geographical discoveries could be observed in the past, however with new technological advancements in transportation keep making it easier and cheaper to get to one place from another. With intercontinental flights we even can travel to another continent, on a business trip for instance, and come back within a few hours. Not only time but road networks and flight networks are being used more and more intensively also, and there is an increasing need to describe the patterns of human peoples' mobility and their effect on network usage, whether the network is a transportation, communication or some other type of network.\n\nRadiation models appeared first in physics to study the process of energetic particles or waves travel through vacuum. The model in the social science describes the flows of people between different locations. Daily commuting is the major part of the flows, so modeling job seeking has to be an important part of the model and so it is in the radiation model. People look for jobs in every county starting with their own home county. The number of open jobs formula_1 depends on the size of the resident population formula_2. The potential employment opportunity (e.g. conditions, income, working hour, etc.) is formula_3 with the distribution of formula_4. Then, for each county formula_5 job opportunities are assigned, which are random draws from the formula_4 distribution. \nIndividuals then chooses the job which is closest to their home county and provides the highest formula_3. Thus, they take into account the proximity to their home county and the benefits it can provide. This optimization gives the migration flows (called commuting fluxes) between counties across the country. This is analogous to the model in physics that describes the radiation and absorption process, that's why it's called the radiation model. An important feature of the model is that the average flux between two counties does not depend on the benefit distribution, the number of job opportunities and the total number of commuters. The fundamental equation of the radiation model gives the average flux between two counties,\n\nwhere formula_9 is the total number of commuters from county formula_10, formula_11 and formula_12 are the population in county formula_10 and formula_14 respectively, and formula_15 is the total population in the circle centered at formula_10 and touching formula_14 excluding the source and the destination population. The model is not static as the Gravity model, and has clear implications which can be empirically verified.\n\nThe population density around Utah is much lower than around Alabama and so are the job opportunities, given that the population of the two states is the same. Thus, the fundamental equation implies that people from Utah have to travel further to find suitable jobs on average than people from Alabama, and indeed, this is what the data shows. The Gravity model gives bad predictions both on short and long distance commuting, while the prediction of the Radiation model is close to the census data. Further empirical testing shows that the Radiation model underestimates the flow in case of big cities, but generalizing the fundamental equation the model can give at least as good predictions as the Gravity model.\n\nIn 1971 famed economist William Alonso produced a working paper that describes a mathematical model of human mobility. In that manuscript Alonso remarks: \"It is almost as if an urban area were a radioactive body, emitting particles at a steady rate[.]\" In addition to many of the same mathematical terms used by Simini et al., Alonso's radiation model includes measures of climate (degree days) and wealth (per capita income) for both the emitting and receiving locales, but only includes the distance between these urban areas as opposed to a radial measure of intervening population density.\n\nThe most influential model to describe trade patterns, and in a similar way, describe human mobility is the gravity model of trade. The model predicts, that the migration flow is proportional to the population of the cities/countries, and it is reciprocal in a quadratic order in the distance between them. Although, it is an intuitive description of the flows, and it is used to describe gravitational forces in physics, in terms of migration it does not perform well empirically. Moreover, the model just simply assumes the given functional form without any theoretical background.\n"}
{"id": "129203", "url": "https://en.wikipedia.org/wiki?curid=129203", "title": "Recall election", "text": "Recall election\n\nA recall election (also called a recall referendum or representative recall) is a procedure by which, in certain polities, voters can remove an elected official from office through a direct vote before that official's term has ended. Recalls, which are initiated when sufficient voters sign a petition, have a history dating back to ancient Athenian democracy and feature in several contemporary constitutions. In indirect or representative democracy, people's representatives are elected and these representatives rule for a specific period of time. However, where the facility to recall exists, should any representative come to be perceived as not properly discharging their responsibilities, then they can be called back with the written request of specific number or proportion of voters.\n\nThe recall referendum arrived in Latin America shortly after its introduction at the US subnational level, in 1923 and 1933, to Cordoba and Entre Ríos provinces, respectively, both in Argentina. There, recall exists at the provincial level in Chaco (introduced in 1957), Chubut (1994), Córdoba (1923, 1987), Corrientes (1960), La Rioja (1986), Rio Negro (1988), Santiago del Estero and Tierra del Fuego (1991); other provinces include it for their municipalities, namely, Entre Ríos (1933), Neuquén (1957), Misiones (1958), San Juan (1986), San Luis (1987). It is also included in Buenos Aires City (1996).\n\nIn 1995, the Legislative Assembly of British Columbia enacted representative recall. In the province of British Columbia, voters in a provincial riding can petition to have their representative in parliament removed from office, even if that MLA is also the premier. (Holding a seat in the legislature is not constitutionally necessary to be premier, however.) If enough registered voters sign the petition, the speaker of the legislature announces in parliament that the member has been recalled and the lieutenant governor drops the writ for a by-election as soon as possible, giving voters the opportunity to replace the politician in question. By January 2003, 22 recall efforts had been launched. No-one has been recalled so far, but one representative, Paul Reitsma, resigned in 1998 when it looked as if the petition to recall him would have enough signatures to spur a recall election. Reitsma resigned during the secondary verification stage and the recall count ended.\n\nIn Nova Scotia, the Atlantica Party campaigned for a recall in the 2017 provincial election.\n\nIn Colombia, the recall referendum was included by the constitution in 1991. The constitutional replacement was launched as an answer to the movement known as \"la séptima papeleta\" (the seventh ballot), which requested a constitutional reform to end violence, narcoterrorism, corruption and increasing citizenship apathy. The definition of recall referendum in relation to programmatic vote was approved. It obliges candidates running for office to register a government plan which is later on considered to activate the recall. Since the time the mechanism was regulated by Law 134 in 1994, until 2015, 161 attempts led 41 referendums and none of them succeeded since the threshold of participation was not reached. In 2015, a new law (303/2015) reduced the number of signatures required to activate a recall referendum (from 40 per cent to 30 per cent of the total of votes obtained by the elected authority) and the threshold (dropping from the 50 per cent to the 40 per cent of valid votes on the day of the elections of the challenged authority). The change in the regulation, also quickening the registration of promoters, led to a considerable increase in the number of attempts.\n\nArticle 14 of the Constitution of Latvia enables the recall of the entire Saiema, though not of specific representatives:\n\nArticle 10 of the constitution of the Philippines allows for the recall of local officials. The Local Government Code, as amended, enabled the provisions of the constitution to be applied. Elected officials from provincial governors to the barangay councilors are allowed to be recalled. At least 25% of the electorate in a specific place must have their signatures verified in a petition in order for the recall to take place.\n\nThe president, vice president, members of Congress, and the elected officials of the Autonomous Region in Muslim Mindanao cannot be removed via recall.\n\nThe last recall election above the barangay level was the 2015 Puerto Princesa mayoral recall election.\n\nRecall regulations were introduced in Peru by the Democratic Constituent Congress (\"Congreso Constituyente Democrático\") which drafted a new constitution after Alberto Fujimori's \"autogolpe\" in 1992. Between 1997 and 2013, more than 5000 recall referendums were activated against democratically elected authorities from 747 Peruvian municipalities (45.5% of all municipalities). This makes Peru the world's most intensive user of this mechanism.\n\nWhile recalls are not provided for at the federal level in Switzerland, six cantons allow them:\n\nThe possibility of recall referendums (together with the popular election of executives, the initiative and the legislative referendum) was introduced into several cantonal constitutions after the 1860s in the course of a broad movement for democratic reform. The instrument has never been of any practical importance—the few attempts at recall so far have failed, usually because the required number of signatures was not collected—and it was abolished in the course of constitutional revisions in Aargau (1980), Baselland (1984) and Lucerne (2007). The only successful recall so far happened in the Canton of Aargau in the year 1862. However, the possibility of recalling municipal executives was newly introduced in Ticino in 2011, with 59% of voters in favor, as a reaction to the perceived problem of squabbling and dysfunctional municipal governments.\n\nIn Taiwan, according to the Additional Articles of the Constitution of the Republic of China, the recall of the president or the vice president shall be initiated upon the proposal of one-fourth of all members of the Legislative Yuan, and also passed by two-thirds of all the members. The final recall must be passed by more than one-half of the valid ballots in a vote in which more than one-half of the electorate in the free area of the Republic of China takes part.\n\nA year after the 2015 Ukrainian local elections, voters can achieve a recall election of an elected deputy or mayor if as many signatures as voters are collected.\n\nRecall first appeared in Colonial America in the laws of the General Court of the Massachusetts Bay Colony in 1631. This version of the recall involved one elected body removing another official. During the American Revolution, the Articles of Confederation stipulated that state legislatures might recall delegates from the Continental Congress. According to New York Delegate John Lansing, the power was never exercised by any state. The Virginia Plan, issued at the outset of the Philadelphia Convention of 1787, proposed to pair recall with rotation in office and to apply these dual principles to the lower house of the national legislature. The recall was rejected by the Constitutional Convention. However, the anti-Federalists used the lack of recall provision as a weapon in the ratification debates.\n\nSeveral states proposed adopting a recall for US senators in the years immediately following the adoption of the Constitution. However, it did not pass.\n\nOnly two governors have ever been successfully recalled. In 1921, Governor Lynn Frazier of North Dakota was recalled during a dispute about state-owned industries. In 2003, Governor Gray Davis of California was recalled over the state budget. Additionally, in 1988, a recall was approved against Governor Evan Mecham of Arizona, but he was impeached and convicted before it got on the ballot.\n\nIn Alaska, Georgia, Kansas, Minnesota, Montana, Rhode Island, and Washington, specific grounds are required for a recall. Some form of malfeasance or misconduct while in office must be identified by the petitioners. The target may choose to dispute the validity of the grounds in court, and a court then judges whether the allegations in the petition rise to a level where a recall is necessary. In the November 2010 general election, Illinois passed a referendum to amend the state constitution to allow a recall of the state's governor, in light of former Governor Rod Blagojevich's corruption scandal. In the other eleven states that permit statewide recall, no grounds are required and recall petitions may be circulated for any reason. However, the target is permitted to submit responses to the stated reasons for recall.\n\nThe minimum number of signatures to qualify a recall, and the time limit to do so, vary among the states. In addition, the handling of recalls, once they qualify, differs. In some states a recall triggers a simultaneous special election, where the vote on the recall, as well as the vote on the replacement if the recall succeeds, are on the same ballot. In the 2003 California recall election, over 100 candidates appeared on the replacement portion of the ballot. In other states, a separate special election is held after the target is recalled, or a replacement is appointed by the Governor or some other state authority.\n\nIn 2011, there were at least 150 recall elections in the United States. Of these, 75 officials were recalled, and nine officials resigned under threat of recall. Recalls were held in 17 states in 73 different jurisdictions. Michigan had the most recalls (at least 30). The year set a record for number of state legislator recall elections (11 elections) beating the previous one-year high (three elections). Three jurisdictions adopted the recall in 2011.\n\nOf recall elections, 52 were for city council, 30 were for mayor, 17 were for school board, 11 were for state legislators, and one was for a prosecuting attorney (York County, Nebraska). The largest municipality to hold a recall was Miami-Dade County, Florida, for mayor.\n\nThe busiest day was November 8 (Election Day) with 26 recalls. In 34 jurisdictions, recalls were held over multiple days.\n\n\n\nNote: Wisconsin's Jim Holperin has the distinction of being the only U.S. politician to have been subjected to recall from service in two different legislative bodies: the Wisconsin State Assembly in 1990 and the Wisconsin State Senate in 2011. Both attempts were unsuccessful.\n\n\nArticle 72 of the Constitution of Venezuela enables the recall of any elected representative, including the President. This provision was used in the Venezuelan recall referendum, 2004, which attempted to remove President Hugo Chavez:\n\nGeneral\n\n"}
{"id": "211960", "url": "https://en.wikipedia.org/wiki?curid=211960", "title": "Reinforcement", "text": "Reinforcement\n\nIn behavioral psychology, reinforcement is a consequence applied that will strengthen an organism's future behavior whenever that behavior is preceded by a specific antecedent stimulus. This strengthening effect may be measured as a higher frequency of behavior (e.g., pulling a lever more frequently), longer duration (e.g., pulling a lever for longer periods of time), greater magnitude (e.g., pulling a lever with greater force), or shorter latency (e.g., pulling a lever more quickly following the antecedent stimulus).\n\nRewarding stimuli, which are associated with \"wanting\" and \"liking\" (desire and pleasure, respectively) and appetitive behavior, function as positive reinforcers; the converse statement is also true: positive reinforcers provide a desirable stimulus. Reinforcement does not require an individual to consciously perceive an effect elicited by the stimulus. Thus, reinforcement occurs only if there is an observable strengthening in behavior. However, there is also negative reinforcement, which is characterized by taking away an undesirable stimulus. An ibuprofen is a negative reinforcer because it takes away pain.\n\nIn most cases, the term \"reinforcement\" refers to an enhancement of behavior, but this term is also sometimes used to denote an enhancement of memory; for example, \"post-training reinforcement\" refers to the provision of a stimulus (such as food) after a learning session in an attempt to increase the retained breadth, detail, and duration of the individual memories or overall memory just formed. The memory-enhancing stimulus can also be one whose effects are directly rather than only indirectly emotional, as with the phenomenon of \"flashbulb memory,\" in which an emotionally highly intense stimulus can incentivize memory of a set of a situation's circumstances well beyond the subset of those circumstances that caused the emotionally significant stimulus, as when people of appropriate age are able to remember where they were and what they were doing when they learned of the assassination of John F. Kennedy or of the September 11, 2001, terrorist attacks.\n\nReinforcement is an important part of operant or instrumental conditioning.\n\nIn the behavioral sciences, the terms \"positive\" and \"negative\" refer when used in their strict technical sense to the nature of the action performed by the conditioner rather than to the responding operant's evaluation of that action and its consequence(s). \"Positive\" actions are those that add a factor, be it pleasant or unpleasant, to the environment, whereas \"negative\" actions are those that remove or withhold from the environment a factor of either type. In turn, the strict sense of \"reinforcement\" refers only to reward-based conditioning; the introduction of unpleasant factors and the removal or withholding of pleasant factors are instead referred to as \"punishment,\" which when used in its strict sense thus stands in contradistinction to \"reinforcement.\" Thus, \"positive reinforcement\" refers to the addition of a pleasant factor, \"positive punishment\" refers to the addition of an unpleasant factor, \"negative reinforcement\" refers to the removal or withholding of an unpleasant factor, and \"negative punishment\" refers to the removal or withholding of a pleasant factor.\n\nThis usage is at odds with some non-technical usages of the four term combinations, especially in the case of the term \"negative reinforcement,\" which is often used to denote what technical parlance would describe as \"positive punishment\" in that the non-technical usage interprets \"reinforcement\" as subsuming both reward and punishment and \"negative\" as referring to the responding operant's evaluation of the factor being introduced. By contrast, technical parlance would use the term \"negative reinforcement\" to describe encouragement of a given behavior by creating a scenario in which an unpleasant factor is or will be present but engaging in the behavior results in either escaping from that factor or preventing its occurrence, as in Martin Seligman's experiments involving dogs' learning processes regarding the avoidance of electric shock.\n\nB.F. Skinner was a well-known and influential researcher who articulated many of the theoretical constructs of reinforcement and behaviorism. Skinner defined reinforcers according to the change in response strength (response rate) rather than to more subjective criteria, such as what is pleasurable or valuable to someone. Accordingly, activities, foods or items considered pleasant or enjoyable may not necessarily be reinforcing (because they produce no increase in the response preceding them). Stimuli, settings, and activities only fit the definition of reinforcers if the behavior that immediately precedes the potential reinforcer increases in similar situations in the future; for example, a child who receives a cookie when he or she asks for one. If the frequency of \"cookie-requesting behavior\" increases, the cookie can be seen as reinforcing \"cookie-requesting behavior\". If however, \"cookie-requesting behavior\" does not increase the cookie cannot be considered reinforcing.\n\nThe sole criterion that determines if a stimulus is reinforcing is the change in probability of a behavior after administration of that potential reinforcer. Other theories may focus on additional factors such as whether the person expected a behavior to produce a given outcome, but in the behavioral theory, reinforcement is defined by an increased probability of a response.\n\nThe study of reinforcement has produced an enormous body of reproducible experimental results. Reinforcement is the central concept and procedure in special education, applied behavior analysis, and the experimental analysis of behavior and is a core concept in some medical and psychopharmacology models, particularly addiction, dependence, and compulsion.\n\nLaboratory research on reinforcement is usually dated from the work of Edward Thorndike, known for his experiments with cats escaping from puzzle boxes. A number of others continued this research, notably B.F. Skinner, who published his seminal work on the topic in The Behavior of Organisms, in 1938, and elaborated this research in many subsequent publications. Notably Skinner argued that positive reinforcement is superior to punishment in shaping behavior. Though punishment may seem just the opposite of reinforcement, Skinner claimed that they differ immensely, saying that positive reinforcement results in lasting behavioral modification (long-term) whereas punishment changes behavior only temporarily (short-term) and has many detrimental side-effects.\nA great many researchers subsequently expanded our understanding of reinforcement and challenged some of Skinner's conclusions. For example, Azrin and Holz defined punishment as a “consequence of behavior that reduces the future probability of that behavior,” and some studies have shown that positive reinforcement and punishment are equally effective in modifying behavior. Research on the effects of positive reinforcement, negative reinforcement and punishment continue today as those concepts are fundamental to learning theory and apply to many practical applications of that theory.\n\nThe term \"operant conditioning\" was introduced by B. F. Skinner to indicate that in his experimental paradigm the organism is free to operate on the environment. In this paradigm the experimenter cannot trigger the desirable response; the experimenter waits for the response to occur (to be emitted by the organism) and then a potential reinforcer is delivered. In the classical conditioning paradigm the experimenter triggers (elicits) the desirable response by presenting a reflex eliciting stimulus, the \"Unconditional Stimulus\" (UCS), which he pairs (precedes) with a neutral stimulus, the \"Conditional Stimulus\" (CS).\n\n\"Reinforcement\" is a basic term in operant conditioning. For the punishment aspect of operant conditioning – see punishment (psychology).\n\nPositive reinforcement occurs when a desirable event or stimulus is presented as a consequence of a behavior and the behavior increases.\n\nNegative reinforcement occurs when the rate of a behavior increases because an aversive event or stimulus is removed or prevented from happening.\n\nReinforcers serve to increase behaviors whereas punishers serve to decrease behaviors; thus, positive reinforcers are stimuli that the subject will work to attain, and negative reinforcers are stimuli that the subject will work to be rid of or to end. The table below illustrates the adding and subtracting of stimuli (pleasant or aversive) in relation to reinforcement vs. punishment.\n\n\nA primary reinforcer, sometimes called an \"unconditioned reinforcer\", is a stimulus that does not require pairing with a different stimulus in order to function as a reinforcer and most likely has obtained this function through the evolution and its role in species' survival. Examples of primary reinforcers include food, water, and sex. Some primary reinforcers, such as certain drugs, may mimic the effects of other primary reinforcers. While these primary reinforcers are fairly stable through life and across individuals, the reinforcing value of different primary reinforcers varies due to multiple factors (e.g., genetics, experience). Thus, one person may prefer one type of food while another avoids it. Or one person may eat lots of food while another eats very little. So even though food is a primary reinforcer for both individuals, the value of food as a reinforcer differs between them.\n\nA secondary reinforcer, sometimes called a conditioned reinforcer, is a stimulus or situation that has acquired its function as a reinforcer after pairing with a stimulus that functions as a reinforcer. This stimulus may be a primary reinforcer or another conditioned reinforcer (such as money). An example of a secondary reinforcer would be the sound from a clicker, as used in clicker training. The sound of the clicker has been associated with praise or treats, and subsequently, the sound of the clicker may function as a reinforcer. Another common example is the sound of people clapping – there is nothing inherently positive about hearing that sound, but we have learned that it is associated with praise and rewards.\n\nWhen trying to distinguish primary and secondary reinforcers in human examples, use the \"caveman test.\" If the stimulus is something that a caveman would naturally find desirable (e.g., candy) then it is a primary reinforcer. If, on the other hand, the caveman would not react to it (e.g., a dollar bill), it is a secondary reinforcer. As with primary reinforcers, an organism can experience satiation and deprivation with secondary reinforcers.\n\n\nIn his 1967 paper, \"Arbitrary and Natural Reinforcement\", Charles Ferster proposed classifying reinforcement into events that increase frequency of an operant as a natural consequence of the behavior itself, and events that are presumed to affect frequency by their requirement of human mediation, such as in a token economy where subjects are \"rewarded\" for certain behavior with an arbitrary token of a negotiable value. In 1970, Baer and Wolf created a name for the use of natural reinforcers called \"behavior traps\". A behavior trap requires only a simple response to enter the trap, yet once entered, the trap cannot be resisted in creating general behavior change. It is the use of a behavioral trap that increases a person's repertoire, by exposing them to the naturally occurring reinforcement of that behavior. Behavior traps have four characteristics:\n\nAs can be seen from the above, artificial reinforcement is in fact created to build or develop skills, and to generalize, it is important that either a behavior trap is introduced to \"capture\" the skill and utilize naturally occurring reinforcement to maintain or increase it. This behavior trap may simply be a social situation that will generally result from a specific behavior once it has met a certain criterion (e.g., if you use edible reinforcers to train a person to say hello and smile at people when they meet them, after that skill has been built up, the natural reinforcer of other people smiling, and having more friendly interactions will naturally reinforce the skill and the edibles can be faded).\n\nMuch behavior is not reinforced every time it is emitted, and the pattern of intermittent reinforcement strongly affects how fast an operant response is learned, what its rate is at any given time, and how long it continues when reinforcement ceases. The simplest rules controlling reinforcement are continuous reinforcement, where every response is reinforced, and extinction, where no response is reinforced. Between these extremes, more complex \"schedules of reinforcement\" specify the rules that determine how and when a response will be followed by a reinforcer.\n\nSpecific schedules of reinforcement reliably induce specific patterns of response, irrespective of the species being investigated (including humans in some conditions). However, the quantitative properties of behavior under a given schedule depend on the parameters of the schedule, and sometimes on other, non-schedule factors. The orderliness and predictability of behavior under schedules of reinforcement was evidence for B.F. Skinner's claim that by using operant conditioning he could obtain \"control over behavior\", in a way that rendered the theoretical disputes of contemporary comparative psychology obsolete. The reliability of schedule control supported the idea that a radical behaviorist experimental analysis of behavior could be the foundation for a psychology that did not refer to mental or cognitive processes. The reliability of schedules also led to the development of applied behavior analysis as a means of controlling or altering behavior.\n\nMany of the simpler possibilities, and some of the more complex ones, were investigated at great length by Skinner using pigeons, but new schedules continue to be defined and investigated.\n\n\nSimple schedules have a single rule to determine when a single type of reinforcer is delivered for a specific response.\n\nSimple schedules are utilized in many differential reinforcement procedures:\n\n\nCompound schedules combine two or more different simple schedules in some way using the same reinforcer for the same behavior. There are many possibilities; among those most often used are:\n\nThe psychology term \"superimposed schedules of reinforcement\" refers to a structure of rewards where two or more simple schedules of reinforcement operate simultaneously. Reinforcers can be positive, negative, or both. An example is a person who comes home after a long day at work. The behavior of opening the front door is rewarded by a big kiss on the lips by the person's spouse and a rip in the pants from the family dog jumping enthusiastically. Another example of superimposed schedules of reinforcement is a pigeon in an experimental cage pecking at a button. The pecks deliver a hopper of grain every 20th peck, and access to water after every 200 pecks.\n\nSuperimposed schedules of reinforcement are a type of compound schedule that evolved from the initial work on simple schedules of reinforcement by B.F. Skinner and his colleagues (Skinner and Ferster, 1957). They demonstrated that reinforcers could be delivered on schedules, and further that organisms behaved differently under different schedules. Rather than a reinforcer, such as food or water, being delivered every time as a consequence of some behavior, a reinforcer could be delivered after more than one instance of the behavior. For example, a pigeon may be required to peck a button switch ten times before food appears. This is a \"ratio schedule\". Also, a reinforcer could be delivered after an interval of time passed following a target behavior. An example is a rat that is given a food pellet immediately following the first response that occurs after two minutes has elapsed since the last lever press. This is called an \"interval schedule\".\n\nIn addition, ratio schedules can deliver reinforcement following fixed or variable number of behaviors by the individual organism. Likewise, interval schedules can deliver reinforcement following fixed or variable intervals of time following a single response by the organism. Individual behaviors tend to generate response rates that differ based upon how the reinforcement schedule is created. Much subsequent research in many labs examined the effects on behaviors of scheduling reinforcers.\n\nIf an organism is offered the opportunity to choose between or among two or more simple schedules of reinforcement at the same time, the reinforcement structure is called a \"concurrent schedule of reinforcement\". Brechner (1974, 1977) introduced the concept of superimposed schedules of reinforcement in an attempt to create a laboratory analogy of social traps, such as when humans overharvest their fisheries or tear down their rainforests. Brechner created a situation where simple reinforcement schedules were superimposed upon each other. In other words, a single response or group of responses by an organism led to multiple consequences. Concurrent schedules of reinforcement can be thought of as \"or\" schedules, and superimposed schedules of reinforcement can be thought of as \"and\" schedules. Brechner and Linder (1981) and Brechner (1987) expanded the concept to describe how superimposed schedules and the social trap analogy could be used to analyze the way energy flows through systems.\n\nSuperimposed schedules of reinforcement have many real-world applications in addition to generating social traps. Many different human individual and social situations can be created by superimposing simple reinforcement schedules. For example, a human being could have simultaneous tobacco and alcohol addictions. Even more complex situations can be created or simulated by superimposing two or more concurrent schedules. For example, a high school senior could have a choice between going to Stanford University or UCLA, and at the same time have the choice of going into the Army or the Air Force, and simultaneously the choice of taking a job with an internet company or a job with a software company. That is a reinforcement structure of three superimposed concurrent schedules of reinforcement.\n\nSuperimposed schedules of reinforcement can create the three classic conflict situations (approach–approach conflict, approach–avoidance conflict, and avoidance–avoidance conflict) described by Kurt Lewin (1935) and can operationalize other Lewinian situations analyzed by his force field analysis. Other examples of the use of superimposed schedules of reinforcement as an analytical tool are its application to the contingencies of rent control (Brechner, 2003) and problem of toxic waste dumping in the Los Angeles County storm drain system (Brechner, 2010).\n\nIn operant conditioning, concurrent schedules of reinforcement are schedules of reinforcement that are simultaneously available to an animal subject or human participant, so that the subject or participant can respond on either schedule. For example, in a two-alternative forced choice task, a pigeon in a Skinner box is faced with two pecking keys; pecking responses can be made on either, and food reinforcement might follow a peck on either. The schedules of reinforcement arranged for pecks on the two keys can be different. They may be independent, or they may be linked so that behavior on one key affects the likelihood of reinforcement on the other.\n\nIt is not necessary for responses on the two schedules to be physically distinct. In an alternate way of arranging concurrent schedules, introduced by Findley in 1958, both schedules are arranged on a single key or other response device, and the subject can respond on a second key to change between the schedules. In such a \"Findley concurrent\" procedure, a stimulus (e.g., the color of the main key) signals which schedule is in effect.\n\nConcurrent schedules often induce rapid alternation between the keys. To prevent this, a \"changeover delay\" is commonly introduced: each schedule is inactivated for a brief period after the subject switches to it.\n\nWhen both the concurrent schedules are variable intervals, a quantitative relationship known as the matching law is found between relative response rates in the two schedules and the relative reinforcement rates they deliver; this was first observed by R.J. Herrnstein in 1961. Matching law is a rule for instrumental behavior which states that the relative rate of responding on a particular response alternative equals the relative rate of reinforcement for that response (rate of behavior = rate of reinforcement). Animals and humans have a tendency to prefer choice in schedules.\n\nShaping is reinforcement of successive approximations to a desired instrumental response. In training a rat to press a lever, for example, simply turning toward the lever is reinforced at first. Then, only turning and stepping toward it is reinforced. The outcomes of one set of behaviours starts the shaping process for the next set of behaviours, and the outcomes of that set prepares the shaping process for the next set, and so on. As training progresses, the response reinforced becomes progressively more like the desired behavior; each subsequent behaviour becomes a closer approximation of the final behaviour.\n\nChaining involves linking discrete behaviors together in a series, such that each result of each behavior is both the reinforcement (or consequence) for the previous behavior, and the stimuli (or antecedent) for the next behavior. There are many ways to teach chaining, such as forward chaining (starting from the first behavior in the chain), backwards chaining (starting from the last behavior) and total task chaining (in which the entire behavior is taught from beginning to end, rather than as a series of steps). An example is opening a locked door. First the key is inserted, then turned, then the door opened.\n\nForward chaining would teach the subject first to insert the key. Once that task is mastered, they are told to insert the key, and taught to turn it. Once that task is mastered, they are told to perform the first two, then taught to open the door. Backwards chaining would involve the teacher first inserting and turning the key, and the subject then being taught to open the door. Once that is learned, the teacher inserts the key, and the subject is taught to turn it, then opens the door as the next step. Finally, the subject is taught to insert the key, and they turn and open the door. Once the first step is mastered, the entire task has been taught. Total task chaining would involve teaching the entire task as a single series, prompting through all steps. Prompts are faded (reduced) at each step as they are mastered.\n\n\nPersuasion is a form of human interaction. It takes place when one individual expects some particular response from one or more other individuals and deliberately sets out to secure the response through the use of communication. The communicator must realize that different groups have different values.\n\nIn instrumental learning situations, which involve operant behavior, the persuasive communicator will present his message and then wait for the receiver to make a correct response. As soon as the receiver makes the response, the communicator will attempt to fix the response by some appropriate reward or reinforcement.\n\nIn conditional learning situations, where there is respondent behavior, the communicator presents his message so as to elicit the response he wants from the receiver, and the stimulus that originally served to elicit the response then becomes the reinforcing or rewarding element in conditioning.\n\nA lot of work has been done in building a mathematical model of reinforcement. This model is known as MPR, short for mathematical principles of reinforcement. Peter Killeen has made key discoveries in the field with his research on pigeons.\n\nThe standard definition of behavioral reinforcement has been criticized as circular, since it appears to argue that response strength is increased by reinforcement, and defines reinforcement as something that increases response strength (i.e., response strength is increased by things that increase response strength). However, the correct usage of reinforcement is that something is a reinforcer \"because\" of its effect on behavior, and not the other way around. It becomes circular if one says that a particular stimulus strengthens behavior because it is a reinforcer, and does not explain why a stimulus is producing that effect on the behavior. Other definitions have been proposed, such as F.D. Sheffield's \"consummatory behavior contingent on a response\", but these are not broadly used in psychology.\n\nIncreasingly understanding of the role reinforcers play is moving away from a \"strengthening\" effect to a \"signalling\" effect. That is, the view that reinforcers increase responding because they signal the behaviours that a likely to result in reinforcement. While in most practical applications, the effect of any given reinforcer will be the same regardless of whether the reinforcer is signalling or strengthening, this approach helps to explain a number of behavioural phenomenon including patterns of responding on intermittent reinforcement schedules (fixed interval scallops) and the differential outcomes effect. \n\nIn the 1920s Russian physiologist Ivan Pavlov may have been the first to use the word \"reinforcement\" with respect to behavior, but (according to Dinsmoor) he used its approximate Russian cognate sparingly, and even then it referred to strengthening an already-learned but weakening response. He did not use it, as it is today, for selecting and strengthening new behaviors. Pavlov's introduction of the word \"extinction\" (in Russian) approximates today's psychological use.\n\nIn popular use, \"positive reinforcement\" is often used as a synonym for \"reward\", with people (not behavior) thus being \"reinforced\", but this is contrary to the term's consistent technical usage, as it is a dimension of behavior, and not the person, which is strengthened. \"Negative reinforcement\" is often used by laypeople and even social scientists outside psychology as a synonym for \"punishment\". This is contrary to modern technical use, but it was B.F. Skinner who first used it this way in his 1938 book. By 1953, however, he followed others in thus employing the word \"punishment\", and he re-cast \"negative reinforcement\" for the removal of aversive stimuli.\n\nThere are some within the field of behavior analysis who have suggested that the terms \"positive\" and \"negative\" constitute an unnecessary distinction in discussing reinforcement as it is often unclear whether stimuli are being removed or presented. For example, Iwata poses the question: \"...is a change in temperature more accurately characterized by the presentation of cold (heat) or the removal of heat (cold)?\" Thus, reinforcement could be conceptualized as a pre-change condition replaced by a post-change condition that reinforces the behavior that followed the change in stimulus conditions.\n\nReinforcement and punishment are ubiquitous in human social interactions, and a great many applications of operant principles have been suggested and implemented. Following are a few examples.\n\nPositive and negative reinforcement play central roles in the development and maintenance of addiction and drug dependence. An addictive drug is intrinsically rewarding; that is, it functions as a primary positive reinforcer of drug use. The brain's reward system assigns it incentive salience (i.e., it is \"wanted\" or \"desired\"), so as an addiction develops, deprivation of the drug leads to craving. In addition, stimuli associated with drug use – e.g., the sight of a syringe, and the location of use – become associated with the intense reinforcement induced by the drug. These previously neutral stimuli acquire several properties: their appearance can induce craving, and they can become conditioned positive reinforcers of continued use. Thus, if an addicted individual encounters one of these drug cues, a craving for the associated drug may reappear. For example, anti-drug agencies previously used posters with images of drug paraphernalia as an attempt to show the dangers of drug use. However, such posters are no longer used because of the effects of incentive salience in causing relapse upon sight of the stimuli illustrated in the posters.\n\nIn drug dependent individuals, negative reinforcement occurs when a drug is self-administered in order to alleviate or \"escape\" the symptoms of physical dependence (e.g., tremors and sweating) and/or psychological dependence (e.g., anhedonia, restlessness, irritability, and anxiety) that arise during the state of drug withdrawal.\n\nAnimal trainers and pet owners were applying the principles and practices of operant conditioning long before these ideas were named and studied, and animal training still provides one of the clearest and most convincing examples of operant control. Of the concepts and procedures described in this article, a few of the most salient are: availability of immediate reinforcement (e.g. the ever-present bag of dog yummies); contingency, assuring that reinforcement follows the desired behavior and not something else; the use of secondary reinforcement, as in sounding a clicker immediately after a desired response; shaping, as in gradually getting a dog to jump higher and higher; intermittent reinforcement, reducing the frequency of those yummies to induce persistent behavior without satiation; chaining, where a complex behavior is gradually put together.\n\nProviding positive reinforcement for appropriate child behaviors is a major focus of parent management training. Typically, parents learn to reward appropriate behavior through social rewards (such as praise, smiles, and hugs) as well as concrete rewards (such as stickers or points towards a larger reward as part of an incentive system created collaboratively with the child). In addition, parents learn to select simple behaviors as an initial focus and reward each of the small steps that their child achieves towards reaching a larger goal (this concept is called \"successive approximations\"). They may also use indirect rewards such through progress charts. Providing positive reinforcement in the classroom can be beneficial to student success. When applying positive reinforcement to students, it's crucial to make it individualized to that student's needs. This way, the student understands why they are receiving the praise, they can accept it, and eventually learn to continue the action that was earned by positive reinforcement. For example, using rewards or extra recess time might apply to some students more, whereas others might accept the enforcement by receiving stickers or check marks indicating praise.\n\nBoth psychologists and economists have become interested in applying operant concepts and findings to the behavior of humans in the marketplace. An example\nis the analysis of consumer demand, as indexed by the amount of a commodity that is purchased. In economics, the degree to which price influences consumption is called \"the price elasticity of demand.\" Certain commodities are more elastic than others; for example, a change in price of certain foods may have a large effect on the amount bought, while gasoline and other essentials may be less affected by price changes. In terms of operant analysis, such effects may be interpreted in terms of motivations of consumers and the relative value of the commodities as reinforcers.\n\nAs stated earlier in this article, a variable ratio schedule yields reinforcement after the emission of an unpredictable number of responses. This schedule typically generates rapid, persistent responding. Slot machines pay off on a variable ratio schedule, and they produce just this sort of persistent lever-pulling behavior in gamblers. Because the machines are programmed to pay out less money than they take in, the persistent slot-machine user invariably loses in the long run. Slots machines, and thus variable ratio reinforcement, have often been blamed as a factor underlying gambling addiction.\n\nNudge theory (or nudge) is a concept in behavioural science, political theory and economics which argues that positive reinforcement and indirect suggestions to try to achieve non-forced compliance can influence the motives, incentives and decision making of groups and individuals, at least as effectively – if not more effectively – than direct instruction, legislation, or enforcement.\n\nThe concept of praise as a means of behavioral reinforcement in humans is rooted in B.F. Skinner's model of operant conditioning. Through this lens, praise has been viewed as a means of positive reinforcement, wherein an observed behavior is made more likely to occur by contingently praising said behavior. Hundreds of studies have demonstrated the effectiveness of praise in promoting positive behaviors, notably in the study of teacher and parent use of praise on child in promoting improved behavior and academic performance, but also in the study of work performance. Praise has also been demonstrated to reinforce positive behaviors in non-praised adjacent individuals (such as a classmate of the praise recipient) through vicarious reinforcement. Praise may be more or less effective in changing behavior depending on its form, content and delivery. In order for praise to effect positive behavior change, it must be contingent on the positive behavior (i.e., only administered after the targeted behavior is enacted), must specify the particulars of the behavior that is to be reinforced, and must be delivered sincerely and credibly.\n\nAcknowledging the effect of praise as a positive reinforcement strategy, numerous behavioral and cognitive behavioral interventions have incorporated the use of praise in their protocols. The strategic use of praise is recognized as an evidence-based practice in both classroom management and parenting training interventions, though praise is often subsumed in intervention research into a larger category of positive reinforcement, which includes strategies such as strategic attention and behavioral rewards.\n\nBraiker identified the following ways that manipulators control their victims:\n\nTraumatic bonding occurs as the result of ongoing cycles of abuse in which the intermittent reinforcement of reward and punishment creates powerful emotional bonds that are resistant to change.\n\nMost video games are designed around some type of compulsion loop, adding a type of positive reinforcement through a variable rate schedule to keep the player playing the game, though this can also lead to video game addiction.\nAs part of a trend in the monetization of video games in the 2010s, some games offered \"loot boxes\" as rewards or purchasable by real-world funds that offered a random selection of in-game items, distributed by rarity. The practice has been tied to the same methods that slot machines and other gambling devices dole out rewards, as it follows a variable rate schedule. While the general perception that loot boxes are a form of gambling, the practice is only classified as such in a few countries as gambling and otherwise legal. However, methods to use those items as virtual currency for online gambling or trading for real-world money has created a skin gambling market that is under legal evaluation.\n\nAshforth discussed potentially destructive sides of leadership and identified what he referred to as petty tyrants: leaders who exercise a tyrannical style of management, resulting in a climate of fear in the workplace. Partial or intermittent negative reinforcement can create an effective climate of fear and doubt. When employees get the sense that bullies are tolerated, a climate of fear may be the result.\n\nIndividual differences in sensitivity to reward, punishment, and motivation have been studied under the premises of reinforcement sensitivity theory and have also been applied to workplace performance.\n\n"}
{"id": "40119773", "url": "https://en.wikipedia.org/wiki?curid=40119773", "title": "Saṃvega", "text": "Saṃvega\n\nSaṃvega is a Buddhist term which indicates a sense of shock, anxiety and spiritual urgency to reach liberation and escape the suffering of samsara. According to Thanissaro Bhikku, \"saṃvega\" is the \"first emotion you're supposed to bring to the training\" and can be defined as: \nThe oppressive sense of shock, dismay, and alienation that come with realizing the futility and meaninglessness of life as it's normally lived; a chastening sense of our own complacency and foolishness in having let ourselves live so blindly; and an anxious sense of urgency in trying to find a way out of the meaningless cycle.\n\n\"Saṃvega\" is also associated with the development of energy (\"viriya\") and right effort, according to Buddhagosa's Atthasālinī: Energy has exerting as its characteristic, strengthening the co-existent states as function, and opposition to giving way as manifestation. It has been said: \"He in whom \"saṃvega\" is present exerts himself properly,\" hence energy has \"saṃvega\", or the basic condition of making energy as proximate cause. Right exertion should be regarded as the root of all attainments. - DhsA. 121\n\nThere are eight bases of \"saṃvega\" (\"saṃvega vatthu\"). They are \"birth, old age, sickness, death, suffering in the\nwoeful worlds, the round of suffering as rooted in the past, the round of suffering as rooted in the future, and the round of suffering in the search for food in the present.\" \"Saṃvega\" can therefore be developed by practicing meditation on death (maranasati) and the charnel ground meditations as outlined in the Satipatthana sutta. In the Upajjhatthana Sutta the Buddha taught that everyone (monks and householders) should practice the five daily recollections as a way to arouse energy and \"saṃvega\".\n\nFor \"saṃvega\" to be an effective drive to practice, it must be accompanied by another emotion called \"pasada\", a \"clarity and serene confidence.\" \"Pasada\" is what keeps \"saṃvega\" from turning into nihilistic despair by providing a sense of confidence that there is a way out, namely nibbana.\n\n"}
{"id": "422247", "url": "https://en.wikipedia.org/wiki?curid=422247", "title": "Self-awareness", "text": "Self-awareness\n\nSelf-awareness is the capacity for introspection and the ability to recognize oneself as an individual separate from the environment and other individuals. It is not to be confused with consciousness in the sense of qualia. While consciousness is being aware of one's environment and body and lifestyle, self-awareness is the recognition of that awareness. Self-awareness is how an individual consciously knows and understands his/her own character, feelings, motives, and desires. There are two broad categories of self-awareness: internal self-awareness and external self-awareness.\n\nThere are questions regarding what part of the brain allows us to be self-aware and how we are biologically programmed to be self-aware. V.S. Ramachandran has speculated that mirror neurons may provide the neurological basis of human self-awareness. In an essay written for the Edge Foundation in 2009, Ramachandran gave the following explanation of his theory: \"... I also speculated that these neurons can not only help simulate other people's behavior but can be turned 'inward'—as it were—to create second-order representations or meta-representations of your \"own\" earlier brain processes. This could be the neural basis of introspection, and of the reciprocity of self awareness and other awareness. There is obviously a chicken-or-egg question here as to which evolved first, but... The main point is that the two co-evolved, mutually enriching each other to create the mature representation of self that characterizes modern humans.\"\n\nStudies have been done mainly on primates to test if self-awareness is present. Apes, monkeys, elephants, and dolphins have been studied most frequently. The most relevant studies to this day that represent self-awareness in animals have been done on chimpanzees, dolphins, and magpies. Self-awareness in animals is tested through mirror self recognition. Animals that show mirror self recognition go through four stages 1) social response, 2) physical mirror inspection, 3) repetitive mirror testing behavior, and 4) the mark test; which involves the animals spontaneously touching a mark on their body which would have been difficult to see without the mirror.\n\nDavid DeGrazia states that there are three types of self-awareness in animals; the first being, bodily self-awareness. This sense of awareness allows animals to understand that they are different from the rest of the environment; it is also the reason why animals do not eat themselves. Bodily-awareness also includes proprioception and sensation. The second type of self-awareness in animals is, social self-awareness. This type of awareness is seen in highly social animals and is the awareness that they have a role within themselves in order to survive. This type of awareness allows animals to interact with each other. The final type of self-awareness is introspective awareness. This awareness is responsible for animals to understand feelings, desires, and beliefs.\n\nThe Red Spot Technique created and experimented by Gordon Gallup studies self-awareness in animals (primates). In this technique, a red odorless spot is placed on an anesthetized primate's forehead. The spot is placed on the forehead so that it can only be seen through a mirror. Once the individual awakens, independent movements toward the spot after seeing their reflection in a mirror are observed. During the Red Spot Technique, after looking in the mirror, chimpanzees used their fingers to touch the red dot that was on their forehead and, after touching the red dot they would even smell their fingertips. \"Animals that can recognize themselves in mirrors can conceive of themselves,\" says Gallup. Another prime example are elephants. Three elephants were exposed to large mirrors where experimenters studied the reaction when the elephants saw their reflection. These elephants were given the \"litmus mark test\" in order to see whether they were aware of what they were looking at. This visible mark was applied on the elephants and the researchers reported a large progress with self-awareness. The elephants shared this success rate with other animals such as monkeys and dolphins.\n\nChimpanzees and other apes – species which have been studied extensively – compare the most to humans with the most convincing findings and straightforward evidence in the relativity of self-awareness in animals so far. Dolphins were put to a similar test and achieved the same results. Diana Reiss, a psycho-biologist at the New York Aquarium discovered that bottlenose dolphins can recognize themselves in mirrors.\n\nResearchers also used the mark test or mirror test to study the magpie's self-awareness. As a majority of birds are blind below the beak, Prior and colleagues marked the birds’ neck with three different colors: red, yellow and a black imitation, as magpies are originally black. When placed in front of a mirror, the birds with the red and yellow spots began scratching at their necks, signaling the understanding of something different being on their bodies. During one trial with a mirror and a mark, three out of the five magpies showed a minimum of one example of self-directed behavior. The magpies explored the mirror by moving toward it and looking behind it. One of the magpies, Harvey, during several trials would pick up objects, pose, do some wing-flapping, all in front of the mirror with the objects in his beak. This represents a sense of self-awareness; knowing what is going on within himself and in the present. The authors suggest that self-recognition in birds and mammals may be a case of convergent evolution, where similar evolutionary pressures result in similar behaviors or traits, although they arrive at them via different routes.<ref name=\"PLoS Biology (DOI:10.1371/journal.pbio.0060202)\"></ref>\n\nA few slight occurrences of behavior towards the magpie's own body happened in the trial with the black mark and the mirror. It is assumed in this study that the black mark may have been slightly visible on the black feathers. Prior and Colleagues, stated \"This is an indirect support for the interpretation that the behavior towards the mark region was elicited by seeing the own body in the mirror in conjunction with an unusual spot on the body.\"\n\nThe behaviors of the magpies clearly contrasted with no mirror present. In the no-mirror trials, a non-reflective gray plate of the same size and in the same position as the mirror was swapped in. There were not any mark directed self-behaviors when the mark was present, in color, or in black. Prior and Colleagues' data quantitatively matches the findings in chimpanzees. In summary of the mark test, the results show that magpies understand that a mirror image represents their own body; magpies show to have self-awareness.\n\nAmong ants, 23 of 24 adult ants, from three species, scratched at small blue dots painted on their fronts when they were able to see the dot in a mirror. None of the ants scratched their fronts when they had no mirror to see the dot. None tried to scratch the blue dot on the mirror. When they had a mirror and a brown dot similar to their own color, only one of thirty ants scratched the brown dot; researchers said she was darker than average so the dot was visible. They also reacted to the mirror itself. Even without dots, 30 out of 30 ants touched the mirror with legs, antennae and mouths, while 0 of 30 ants touched a clear glass divider, with ants on the other side.\n\nA more recent study introduces a different approach to examining self-awareness in animals, suggesting a new ethological approach which may shed light on different ways of testing for cognition, and helps advance the debate among ethologists (and philosophers) on consciousness. The research by Gatti Cazzolla published in 2016, with the title adapted from the novel by Lewis Carroll “\"Self-consciousness: beyond the looking-glass and what dogs found there\"“, could change the way some experiments on animal behaviour are validated.\n\nThe study shows that the “sniff test of self-recognition (STSR)”, as defined in the study, even when applied to multiple individuals living in groups with different ages and sexes, provides meaningful evidence of self-awareness in dogs. As such, these results may show that this capacity is not a feature specific only to great apes, humans and a few other animals, but instead that observing self-recognition in animals depends on the method researchers use to verify it.\n\nAttempts to verify this idea have been made before, but most of them were only observational, lacked empirical evidences or had been carried out only with a single individual and not repeated systematically with other dogs of different sex and age (for example the ethologist Marc Bekoff in 2001 used a \"yellow snow test\" to measure how long his dog was sniffing his scent of urine and those of the other dogs in the area). Therefore, the final test of self-recognition in a species phylogenetically distant from apes (thus with different sensory modalities and communication behaviour) as the dog, was not obtained.\n\nThe innovative approach to test the self-awareness with a smell test \"highlights the need to shift the paradigm of the anthropocentric idea of consciousness to a species-specific perspective\"—said Roberto Cazzolla Gatti: \"We would never expect that a mole or a bat can recognize theirselves in a mirror, but now we have strong empirical evidences to suggest that if species other than primates are tested on chemical or auditory perception base we could get really unexpected results”.\n\nAn organism can be effectively altruistic without being self-aware, aware of any distinction between egoism and altruism, or aware of qualia in others. This by simple reactions to specific situations which happens to benefit other individuals in the organism's natural environment. If self-awareness led to a necessity of an emotional empathy mechanism for altruism and egoism being default in its absence, that would have precluded evolution from a state without self-awareness to a self-aware state in all social animals. The ability of the theory of evolution to explain self-awareness can be rescued by abandoning the hypothesis of self-awareness being a basis for cruelty.\n\nSelf-awareness has been called \"arguably the most fundamental issue in psychology, from both a developmental and an evolutionary perspective.\"\n\nSelf-awareness theory, developed by Duval and Wicklund in their 1972 landmark book \"A theory of objective self awareness\", states that when we focus our attention on ourselves, we evaluate and compare our current behavior to our internal standards and values. This elicits a state of objective self-awareness. We become self-conscious as objective evaluators of ourselves. However self-awareness is not to be confused with self-consciousness. Various emotional states are intensified by self-awareness. However, some people may seek to increase their self-awareness through these outlets. People are more likely to align their behavior with their standards when made self-aware. People will be negatively affected if they don't live up to their personal standards. Various environmental cues and situations induce awareness of the self, such as mirrors, an audience, or being videotaped or recorded. These cues also increase accuracy of personal memory. In one of Demetriou's neo-Piagetian theories of cognitive development, self-awareness develops systematically from birth through the life span and it is a major factor for the development of general inferential processes. Moreover, a series of recent studies showed that self-awareness about cognitive processes participates in general intelligence on a par with processing efficiency functions, such as working memory, processing speed, and reasoning. Albert Bandura's theory of self-efficacy builds on our varying degrees of self-awareness. It is \"the belief in one's capabilities to organize and execute the courses of action required to manage prospective situations.\" A person's belief in their ability to succeed sets the stage to how they think, behave and feel. Someone with a strong self-efficacy, for example, views challenges as mere tasks that must be overcome, and are not easily discouraged by setbacks. They are aware of their flaws and abilities and choose to utilize these qualities to the best of their ability. Someone with a weak sense of self-efficacy evades challenges and quickly feels discouraged by setbacks. They may not be aware of these negative reactions, and therefore do not always change their attitude. This concept is central to Bandura's social cognitive theory, \"which emphasizes the role of observational learning, social experience, and reciprocal determinism in the development of personality.\"\n\nIndividuals become conscious of themselves through the development of self-awareness. This particular type of self-development pertains to becoming conscious of one's own body and mental state of mind including thoughts, actions, ideas, feelings and interactions with others. \"Self-awareness does not occur suddenly through one particular behavior: it develops gradually through a succession of different behaviors all of which relate to the self.\" The monitoring of one's mental states is called metacognition and it is considered to be an indicator that there is some concept of the self. It is developed through an early sense of non-self components using sensory and memory sources. In developing self–awareness through self-exploration and social experiences one can broaden his social world and become more familiar with the self.\n\nAccording to Emory University's Philippe Rochat, there are five levels of self-awareness which unfold in early development and six potential prospects ranging from \"Level 0\" (having no self-awareness) advancing complexity to \"Level 5\" (explicit self-awareness).\n\n\nIt is to be kept in mind that as an infant comes into this world, they have no concept of what is around them, nor for the significance of others around them. It is throughout the first year that they gradually begin to acknowledge that their body is actually separate from that of their mother, and that they are an \"active, causal agent in space\". By the end of the first year, they additionally realize that their movement, as well, is separate from movement of the mother. That is a huge advance, yet they are still quite limited and cannot yet know what they look like, \"in the sense that the infant cannot recognize its own face\". By the time an average toddler reaches 18–24 months, they will discover themselves and recognize their own reflection in the mirror, however in many African populations this is not the case until the age of six. They begin to acknowledge the fact that the image in front of them, who happens to be them, moves; indicating that they appreciate and can consider the relationship between cause and effect that is happening. By the age of 24 months the toddler will observe and relate their own actions to those actions of other people and the surrounding environment. Once an infant has gotten a lot of experience, and time, in front of a mirror, it is only then that they are able to recognize themselves in the reflection, and understand that it is them. For example, in a study, an experimenter took a red marker and put a fairly large red dot (so it is visible by the infant) on the infant's nose, and placed him/her in front of a mirror. Prior to 15 months of age, the infant will not react to this, but after 15 months of age, they will either touch their nose, wondering what it is they have on their face, or point to it. This indicates the appearance that he/she recognizes that the image they see in the reflection of the mirror is themselves. There is somewhat of the same thing called the mirror-self recognition task, and it has been used as a research tool for numerous years, and has given, and lead to, key foundations of the infant's sense/awareness of self. For example, \"for Piaget, the objectification of the bodily self occurs as the infant becomes able to represent the body's spatial and causal relationship with the external world (Piaget, 1954). Facial recognition places a big pivotal point in their development of self-awareness. By 18 months, the infant can communicate his/her name to others, and upon being shown a picture they are in, they can identify themselves. By two years old, they also usually acquire gender category and age categories, saying things such as \"I am a girl, not a boy\" and \"I am a baby or child, not a grownup\". Evidently, it is not at the level of an adult or an adolescent, but as an infant moves to middle childhood and onwards to adolescence, they develop a higher level of self-awareness and self-description.\n\nAs infants develop their senses, using multiple senses of in order to recognize what is around them, infants can become affected by something known as \"facial multi stimulation\". In one experiment by Filippetti, Farroni, and Johnson, an infant of around five months in age is given what is known as an “enfacement illusion”. “Infants watched a side-by-side video display of a peer’s face being systematically stroked on the cheek with a paintbrush. During the video presentation, the infant’s own cheek was stroked in synchrony with one video and in asynchrony with the other”. Infants were proven to recognize and project an image of a peer with that of their own, showing beginning signs of facial recognition cues onto one’s self, with the assistance of an illusion.\n\nAround school age a child's awareness of personal memory transitions into a sense of one's own self. At this stage, a child begins to develop interests along with likes and dislikes. This transition enables the awareness of an individual's past, present, and future to grow as conscious experiences are remembered more often. As a preschooler, they begin to give much more specific details about things, instead of generalizing. For example, the infant will talk about the Los Angeles Lakers basketball team, and the New York Rangers hockey team, instead of the infant just stating that he likes sports. Furthermore, they will start to express certain preferences (e.g., Tod likes mac and cheese) and will start to identify certain possessions of theirs (e.g., Lara has a bird as a pet at home). At this age, the infant is in the stage Piaget names the \"pre operational\" stage of development. The infant is very inaccurate at judging themselves because they do not have much to go about. For example, an infant at this stage will not associate that they are strong with their ability to cross the jungle gym at their school, nor will they associate the fact that they can solve a math problem with their ability to count.\n\nOne becomes conscious of their emotions during adolescence. Most children are aware of emotions such as shame, guilt, pride and embarrassment by the age of two, but do not fully understand how those emotions affect their life. By age 13, children become more in touch with these emotions and begin to apply them to their own lives. A study entitled \"The Construction of the Self\" found that many adolescents display happiness and self-confidence around friends, but hopelessness and anger around parents due to the fear of being a disappointment. Teenagers were also shown to feel intelligent and creative around teachers, and shy, uncomfortable and nervous around people they were not familiar with.\n\nIn adolescent development, the definition self-awareness also has a more complex emotional context due to the maturity of adolescents compared to those in the early childhood phase, and these elements can include but are not limited to self-image, self-concept, and self–consciousness along many other traits that can relate to Rochat's final level of self awareness, however it is still a distinct concept within its own previous definition. Social interactions mainly separate the element of self-awareness in adolescent rather than in childhood, as well as further developed emotional recognition skills in adolescents. Sandu, Pânișoară, and Pânișoară demonstrate these in their work with teenagers and demonstrates that there is a mature sense of self-awareness with students who were aged 17, which in term provides a clear structure with how elements like self-concept, self-image, and self-consciousness relate to self-awareness.\n\nAs children reach their adolescent stages of life, the acute sense of emotion has widened into a meta cognitive state in which mental health issues can become more prevalent due to their heightened emotional and social development. There are elements of contextual behavioral science such as Self-as-Content, Self-as-Process and Self-as-Context, involved with adolescent self-awareness that can associate with mental health. Moran, Almada, and McHugh presented the idea that these domains of self are associated with adolescent mental health in various capacities. Anger management is also a domain of mental health that is associated with the concept of self-awareness in teens. Self-awareness training has been linked to lowering anger management issues and reducing aggressive tendencies in adolescents “Persons having sufficient self-awareness promote relaxation and awareness about themselves and when going angry, at the first step they become aware of anger in their inside and accept it, then try to handle it”.\n\nAn early philosophical discussion of self-awareness is that of John Locke. Locke was apparently influenced by René Descartes' statement normally translated 'I think, therefore I am' (\"Cogito ergo sum\"). In chapter XXVII \"On Identity and Diversity\" of Locke's \"An Essay Concerning Human Understanding\" (1689) he conceptualized consciousness as the repeated self-identification of oneself through which moral responsibility could be attributed to the subject—and therefore punishment and guiltiness justified, as critics such as Nietzsche would point out, affirming \"...the psychology of conscience is not 'the voice of God in man'; it is the instinct of cruelty ... expressed, for the first time, as one of the oldest and most indispensable elements in the foundation of culture.\" John Locke does not use the terms \"self-awareness\" or \"self-consciousness\" though.\n\nAccording to Locke, personal identity (the self) \"depends on consciousness, not on substance\". We are the same person to the extent that we are conscious of our past and future thoughts and actions in the same way as we are conscious of our present thoughts and actions. If consciousness is this \"thought\" which doubles all thoughts, then personal identity is only founded on the repeated act of consciousness: \"This may show us wherein personal identity consists: not in the identity of substance, but ... in the identity of consciousness.\" For example, one may claim to be a reincarnation of Plato, therefore having the same soul. However, one would be the same person as Plato only if one had the same consciousness of Plato's thoughts and actions that he himself did. Therefore, self-identity is not based on the soul. One soul may have various personalities.\n\nLocke argues that self-identity is not founded either on the body or the substance, as the substance may change while the person remains the same. \"Animal identity is preserved in identity of life, and not of substance\", as the body of the animal grows and changes during its life. describes a case of a prince and a cobbler in which the soul of the prince is transferred to the body of the cobbler and vice versa. The prince still views himself as a prince, though he no longer looks like one. This border-case leads to the problematic thought that since personal identity is based on consciousness, and that only oneself can be aware of his consciousness, exterior human judges may never know if they really are judging—and punishing—the same person, or simply the same body. Locke argues that one may be judged for the actions of one's body rather than one's soul, and only God knows how to correctly judge a man's actions. Men also are only responsible for the acts of which they are conscious. This forms the basis of the insanity defense which argues that one cannot be held accountable for acts in which they were unconsciously irrational, or mentally ill— In reference to man's personality, Locke claims that \"whatever past actions it cannot reconcile or appropriate to that present self by consciousness, it can be no more concerned in it than if they had never been done: and to receive pleasure or pain, i.e. reward or punishment, on the account of any such action, is all one as to be made happy or miserable in its first being, without any demerit at all.\"\n\nThe medical term for not being aware of one's deficits is anosognosia, or more commonly known as a lack of insight. Having a lack of awareness raises the risks of treatment and service nonadherence. Individuals who deny having an illness may be against seeking professional help because they are convinced that nothing is wrong with them. Disorders of self-awareness frequently follow frontal lobe damage. There are two common methods used to measure how severe an individual's lack of self-awareness is. The Patient Competency Rating Scale (PCRS) evaluates self-awareness in patients who have endured a traumatic brain injury. PCRS is a 30-item self-report instrument which asks the subject to use a 5-point Likert scale to rate his or her degree of difficulty in a variety of tasks and functions. Independently, relatives or significant others who know the patient well are also asked to rate the patient on each of the same behavioral items. The difference between the relatives’ and patient's perceptions is considered an indirect measure of impaired self-awareness. The limitations of this experiment rest on the answers of the relatives. Results of their answers can lead to a bias. This limitation prompted a second method of testing a patient's self-awareness. Simply asking a patient why they are in the hospital or what is wrong with their body can give compelling answers as to what they see and are analyzing.<ref name=\"http://www.thebarrow.org/stellent/fragments/v2GetSiteLogo/images/BarrowV2.gif\"></ref>\n\nAnosognosia was a term coined by Joseph Babinski to describe the clinical condition in which an individual suffered from left hemiplegia following a right cerebral hemisphere stroke yet denied that there were any problems with their left arm or leg. This condition is known as anosognosia for hemiplegia (AHP). This condition has evolved throughout the years and is now used to describe people who lack subjective experience in both neurological and neuropsychological cases. A wide variety of disorders are associated with anosognosia. For example, patients who are blind from cortical lesions might in fact be unaware that they are blind and may state that they do not suffer from any visual disturbances. Individuals with aphasia and other cognitive disorders may also suffer from anosognosia as they are unaware of their deficiencies and when they make certain speech errors, they may not correct themselves due to their unawareness. Individuals who suffer from Alzheimer's disease lack awareness; this deficiency becomes more intense throughout their disease. A key issue with this disorder is that people who do have anosognosia and suffer from certain illnesses may not be aware of them, which ultimately leads them to put themselves in dangerous positions and/or environments. To this day there are still no available treatments for AHP, but it has been documented that temporary remission has been used following vestibular stimulation.\n\nDissociative identity disorder or multiple personality disorder (MPD) is a disorder involving a disturbance of identity in which two or more separate and distinct personality states (or identities) control an individual's behavior at different times. One identity may be different from another, and when an individual with DID is under the influence of one of their identities, they may forget their experiences when they switch to the other identity. \"When under the control of one identity, a person is usually unable to remember some of the events that occurred while other personalities were in control.\" They may experience time loss, amnesia, and adopt different mannerisms, attitudes, speech and ideas under different personalities. They are often unaware of the different lives they lead or their condition in general, feeling as though they are looking at their life through the lens of someone else, and even being unable to recognize themselves in a mirror. \nTwo cases of DID have brought awareness to the disorder, the first case being that of Eve. This patient harbored three different personalities: Eve White the good wife and mother, Eve Black the party girl, and Jane the intellectual. Under stress, her episodes would worsen. She even tried to strangle her own daughter and had no recollection of the act afterward. Eve went through years of therapy before she was able to learn how to control her alters and be mindful of her disorder and episodes. Her condition, being so rare at the time, inspired the book and film adaptation The Three Faces of Eve, as well as a memoir by Eve herself entitled I'm Eve. Doctors speculated that growing up during the Depression and witnessing horrific things being done to other people could have triggered emotional distress, periodic amnesia, and eventually DID. In the second case, Shirley Mason, or Sybil, was described as having over 16 separate personalities with different characteristics and talents. Her accounts of horrific and sadistic abuse by her mother during childhood prompted doctors to believe that this trauma caused her personalities to split, furthering the unproven idea that this disorder was rooted in child abuse, while also making the disorder famous. In 1998 however, Sybil's case was exposed as a sham. Her therapist would encourage Sybil to act as her other alter ego although she felt perfectly like herself. Her condition was exaggerated in order to seal book deals and television adaptations. Awareness of this disorder began to crumble shortly after this finding. To this day, no proven cause of DID has been found, but treatments such as psychotherapy, medications, hypnotherapy, and adjunctive therapies have proven to be very effective.\n\nAutism spectrum disorder (ASD) is a range of neurodevelopmental disabilities that can adversely impact social communication and create behavioral challenges (Understanding Autism, 2003). \"Autism spectrum disorder (ASD) and autism are both general terms for a group of complex disorders of brain development. These disorders are characterized, in varying degrees, by difficulties in social interaction, verbal and nonverbal communication and repetitive behaviors.\" ASDs can also cause imaginative abnormalities and can range from mild to severe, especially in sensory-motor, perceptual and affective dimensions. Children with ASD may struggle with self-awareness and self acceptance. Their different thinking patterns and brain processing functions in the area of social thinking and actions may compromise their ability to understand themselves and social connections to others. About 75% diagnosed autistics are mentally handicapped in some general way and the other 25% diagnosed with Asperger's Syndrome show average to good cognitive functioning. When we compare our own behavior to the morals and values that we were taught, we can focus more attention on ourselves which increases self-awareness. To understand the many effects of autism spectrum disorders on those afflicted have led many scientists to theorize what level of self-awareness occurs and in what degree. Research found that ASD can be associated with intellectual disability and difficulties in motor coordination and attention. It can also result in physical health issues as well, such as sleep and gastrointestinal disturbances. As a result of all those problems, individuals are literally unaware of themselves. It is well known that children suffering from varying degrees of autism struggle in social situations. Scientists at the University of Cambridge have produced evidence that self-awareness is a main problem for people with ASD. Researchers used functional magnetic resonance scans (FMRI) to measure brain activity in volunteers being asked to make judgments about their own thoughts, opinions, preferences, as well as about someone else's. One area of the brain closely examined was the ventromedial pre-frontal cortex (vMPFC) which is known to be active when people think about themselves.\nA study out of Stanford University has tried to map out brain circuits with understanding self-awareness in Autism Spectrum Disorders. This study suggests that self-awareness is primarily lacking in social situations but when in private they are more self-aware and present. It is in the company of others while engaging in interpersonal interaction that the self-awareness mechanism seems to fail. Higher functioning individuals on the ASD scale have reported that they are more self-aware when alone unless they are in sensory overload or immediately following social exposure. Self-awareness dissipates when an autistic is faced with a demanding social situation. This theory suggests that this happens due to the behavioral inhibitory system which is responsible for self-preservation. This is the system that prevents human from self-harm like jumping out of a speeding bus or putting our hand on a hot stove. Once a dangerous situation is perceived then the behavioral inhibitory system kicks in and restrains our activities. \"For individuals with ASD, this inhibitory mechanism is so powerful, it operates on the least possible trigger and shows an over sensitivity to impending danger and possible threats. Some of these dangers may be perceived as being in the presence of strangers, or a loud noise from a radio. In these situations self-awareness can be compromised due to the desire of self preservation, which trumps social composure and proper interaction.\nThe Hobson hypothesis reports that autism begins in infancy due to the lack of cognitive and linguistic engagement which in turn results in impaired reflective self-awareness. In this study ten children with Asperger's Syndrome were examined using the Self-understanding Interview. This interview was created by Damon and Hart and focuses on seven core areas or schemas that measure the capacity to think in increasingly difficult levels. This interview will estimate the level of self understanding present. \"The study showed that the Asperger group demonstrated impairment in the 'self-as-object' and 'self-as-subject' domains of the Self-understanding Interview, which supported Hobson's concept of an impaired capacity for self-awareness and self-reflection in people with ASD.\". Self-understanding is a self description in an individual's past, present and future. Without self-understanding it is reported that self-awareness is lacking in people with ASD.\nJoint attention (JA) was developed as a teaching strategy to help increase positive self-awareness in those with autism spectrum disorder. JA strategies were first used to directly teach about reflected mirror images and how they relate to their reflected image. Mirror Self Awareness Development (MSAD) activities were used as a four-step framework to measure increases in self-awareness in those with ASD. Self-awareness and knowledge is not something that can simply be taught through direct instruction. Instead, students acquire this knowledge by interacting with their environment. Mirror understanding and its relation to the development of self leads to measurable increases in self-awareness in those with ASD. It also proves to be a highly engaging and highly preferred tool in understanding the developmental stages of self- awareness.\nThere have been many different theories and studies done on what degree of self-awareness is displayed among people with autism spectrum disorder. Scientists have done research about the various parts of the brain associated with understanding self and self-awareness. Studies have shown evidence of areas of the brain that are impacted by ASD. Other theories suggest that helping an individual learn more about themselves through Joint Activities, such as the Mirror Self Awareness Development may help teach positive self-awareness and growth. In helping to build self-awareness it is also possible to build self-esteem and self acceptance. This in turn can help to allow the individual with ASD to relate better to their environment and have better social interactions with others.\n\nSchizophrenia is a chronic psychiatric illness characterized by excessive dopamine activity in the mesolimbic tract and insufficient dopamine activity in the mesocortical tract leading to symptoms of psychosis along with poor cognition in socialization. Under the Diagnostic and Statistical Manual of Mental Disorders, people with schizophrenia have a combination of positive, negative and psychomotor symptoms. These cognitive disturbances involve rare beliefs and/or thoughts of a distorted reality that creates an abnormal pattern of functioning for the patient. The cause of schizophrenia has a substantial genetic component involving many genes. While the heritability of schizophrenia has been found to be around 80%, only about 60% of sufferers report a positive family history of the disorder, and ultimately the cause is thought to be a combination of genetic and environmental factors. It is believed that the experience of stressful life events is an environmental factor that can trigger the onset of schizophrenia in individuals who already are at risk from genetics and age. The level of self-awareness among patients with schizophrenia is a heavily studied topic.\n\nSchizophrenia as a disease state is characterized by severe cognitive dysfunction and it is uncertain to what extent patients are aware of this deficiency. Medalia and Lim (2004), investigated patients’ awareness of their cognitive deficit in the areas of attention, nonverbal memory, and verbal memory. Results from this study (N=185) revealed large discrepancy in patients’ assessment of their cognitive functioning relative to the assessment of their clinicians. Though it is impossible to access ones’ consciousness and truly understand what a schizophrenic believes, regardless in this study, patients were not aware of their cognitive dysfunctional reasoning. In the DSM-5, to receive a diagnosis of schizophrenia, they must have two or more of the following symptoms in the duration of one month: delusions*, hallucinations*, disorganized speech*, grossly disorganized/catatonic behavior and negative symptoms (*these three symptoms above all other symptoms must be present to correctly diagnose a patient.) Sometimes these symptoms are very prominent and are treated with a combination of antipsychotics (i.e. haloperidol, loxapine), atypical antipsychotics (such as clozapine and risperdone) and psychosocial therapies that include family interventions and socials skills. When a patient is undergoing treatment and recovering from the disorder, the memory of their behavior is present in a diminutive amount; thus, self-awareness of diagnoses of schizophrenia after treatment is rare, as well as subsequent to onset and prevalence in the patient.\n\nThe above findings are further supported by a study conducted by Amador and colleagues. The study suggests a correlation exists between patient insight, compliance and disease progression. Investigators assess insight of illness was assessed via Scale to Assess Unawareness of Mental Disorder and was used along with rating of psychopathology, course of illness, and compliance with treatments in a sample of 43 patients. Patients with poor insight are less likely to be compliant with treatment and are more likely to have a poorer prognosis. Patients with hallucinations sometimes experience positive symptoms, which can include delusions of reference, thought insertion/withdrawal, thought broadcast, delusions of persecution, grandiosity and many more. These psychoses skew the patient's perspectives of reality in ways in which they truly believe are really happening. For instance, a patient that is experiencing delusions of reference may believe while watching the weather forecast that when the weatherman says it will rain, he is really sending a message to the patient in which rain symbolizes a specific warning completely irrelevant to what the weather is. Another example would be thought broadcast, which is when a patient believes that everyone can hear their thoughts. These positive symptoms sometimes are so severe to where the schizophrenic believes that something is crawling on them or smelling something that is not there in reality. These strong hallucinations are intense and difficult to convince the patient that they do not exist outside of their cognitive beliefs, making it extremely difficult for a patient to understand and become self-aware that what they are experiencing is in fact not there.\n\nFurthermore, a study by Bedford and Davis (2013) was conducted to look at the association of denial vs. acceptance of multiple facets of schizophrenia (self reflection, self perception and insight) and its effect on self-reflection (N=26). Study results suggest patients with increased disease denial have lower recollection for self evaluated mental illnesses. To a great extent, disease denial creates a hardship for patients to undergo recovery because their feelings and sensations are intensely outstanding. But just as this and the above studies imply, a large proportion of schizophrenics do not have self-awareness of their illness for many factors and severity of reasoning of their diagnoses.\n\nBipolar disorder is an illness that causes shifts in mood, energy, and ability to function. Self-awareness is crucial in those suffering from this disease, as they must be able to distinguish between feeling a certain way because of the disorder or because of separate issues. \"Personality, behavior, and dysfunction affect your bipolar disorder, so you must 'know' yourself in order to make the distinction.\" This disorder is a difficult one to diagnose, as self-awareness changes with mood. \"For instance, what might appear to you as confidence and clever ideas for a new business venture might be a pattern of grandiose thinking and manic behavior\". Issues occur between understanding irrationality in a mood swing and being completely wrapped in a manic episode, rationalizing that the exhibited behaviors are normal.\n\nIt is important to be able to distinguish what are symptoms of bipolar disorder and what is not. A study done by Mathew et al. was done with the aim of \"examining the perceptions of illness in self and among other patients with bipolar disorder in remission\".\n\nThe study took place at the Department of Psychiatry, Christian Medical College, Vellore, India, which is a centre that specializes in the \"management of patients with mental and behavioural disorders\". Eighty two patients (thirty two female and fifty male) agreed to partake in the study. These patients met the \"International Classification of Diseases – 10 diagnostic criteria for a diagnosis of bipolar disorder I or II and were in remission\" and were put through a variety of baseline assessments before beginning the study. These baseline assessments included using a vignette, which was then used as an assessment tool during their follow-up. Patients were then randomly divided into two groups, one who would be following a \"structured educational intervention programme\" (experimental group), while the other would be following \"usual care\" (control group).\n\nThe study was based on an interview in which patients were asked an array of open-eded questions regarding topics such as \"perceived causes, consequences, severity and its effects on body, emotion, social network and home life, and on work, severity, possible course of action, help-seeking behaviour and the role of the doctor/healer\". The McNemar test was then used to compare the patients perspective of the illness versus their explanation of the illness. The results of the study show that the beliefs that patients associated with their illness corresponds with the possible causes of the disorder, whereas \"studies done among patients during periods of active psychosis have recorded disagreement between their assessments of their own illness\". This ties in to how difficult self-awareness is within people who suffer from bipolar disorder.\n\nAlthough this study was done on a population that were in remission from the disease, the distinction between patients during \"active psychosis\" versus those in remission shows the evolution of their self-awareness throughout their journey to recovery.\n\nBodily (self-)awareness is related to proprioception and visualization\n\nBodily self-awareness in human development refers to one’s awareness of their body as a physical object, with physical properties, that can interact with other objects.Tests have shown that at the age of only a few months old, toddlers are already aware of the relationship between the proprioceptive and visual information they receive. This is called first-person self-awareness.\n\nAt around 18 months old and later, children begin to develop reflective self-awareness, which is the next stage of bodily awareness and involves children recognizing themselves in reflections, mirrors, and pictures. Children who have not obtained this stage of bodily self-awareness yet will tend to view reflections of themselves as other children and respond accordingly, as if they were looking at someone else face to face. In contrast, those who have reached this level of awareness will recognize that they see themselves, for instance seeing dirt on their face in the reflection and then touching their own face to wipe it off.\n\nSlightly after toddlers become reflectively self-aware, they begin to develop the ability to recognize their bodies as physical objects in time and space that interact and impact other objects. For instance, a toddler placed on a blanket, when asked to hand someone the blanket, will recognize that they need to get off it to be able to lift it. This is the final stage of body self-awareness and is called objective self-awareness.\n\nIn health and medicine, body-awareness is a construct that refers to a person’s overall ability to direct their focus on various internal sensations accurately. Both proprioception and interoception allow individuals to be consciously aware of various sensations. Proprioception allows individuals and patients to focus on sensations in their muscles and joints, posture, and balance, while interoception is used to determine sensations of the internal organs, such as fluctuating heartbeat, respiration, lung pain, or satiety. Over-acute body-awareness, under-acute body-awareness, and distorted body-awareness are symptoms present in a variety of health disorders and conditions, such as obesity, anorexia nervosa, and chronic joint pain. For example, a distorted perception of satiety present in a patient suffering from anorexia nervosa.\n\nSelf-discrimination in plants is found within their roots, tendrils and flowers that avoid themselves but not others in their environment.\n\nSelf-awareness in plants is a fringe topic in the field of self-awareness, and is researched predominantly by botanists. The claim that plants are capable of perceiving self lies in the evidence found that plants will not reproduce with themselves due to a gene selecting mechanism. In addition, vining plants have been shown to avoid coiling around themselves, due to chemical receptors in the plants' tendrils. Unique to plants, awareness of self means that the plant can recognise self, whereas all other known conceptions of self-awareness is the ability to recognise what is not self.\n\nResearch by June B. Nasrallah discovered that the plant's pollination mechanism also serves as a mechanism against self-reproduction, which lays out the foundation of scientific evidence that plants could be considered as self-aware organisms. The SI (Self-incompatibility) mechanism in plants is unique in the sense that awareness of self derives from the capacity to recognise self, rather than non-self. The SI mechanism function depends primarily on the interaction between genes \"S\"-locus receptor protein kinase (SRK) and \"S\"-locus cysteine-rich protein gene (\"SCR).\" In cases of self-pollination, SRK and SCR bind to activate SKR, Inhibiting pollen from fertilizing. In cases of cross-pollination, SRK and SCR do not bind and therefor SRK is not activated, causing the pollen to fertilise. In simple terms, the receptors either accept or reject the genes present in the pollen, and when the genes are from the same plant, the SI mechanism described above creates a reaction to prevent the pollen from fertilising.\n\nThe research by Yuya Fukano and Akira Yamawo provides a link between self-discrimination in vining plants and amongst other classifications where the mechanism discovery has already been established. It also contributes to the general foundation of evidence of self-discrimination mechanisms in plants. The article makes the claim that the biological self-discrimination mechanism that is present in both flowering plants and ascidians, are also present in vining plants. They tested this hypothesis by doing touch tests with self neighbouring and non-self neighbouring pairs of plants. the test was performed by placing the sets of plants close enough for their tendrils to interact with one-another. Evidence of self-discrimination in above-ground plants is demonstrated in the results of the touch testing, which showed that in cases of connected self plants, severed self plants and non-self plants, the rate of tendril activity and likeliness to coil was higher among separated plants then those attached via rhizomes.\n\nTheater also concerns itself with other awareness besides self-awareness. There is a possible correlation between the experience of the theater audience and individual self-awareness. As actors and audiences must not \"break\" the fourth wall in order to maintain context, so individuals must not be aware of the artificial, or the constructed perception of his or her reality. This suggests that both self-awareness and the social constructs applied to others are artificial continuums just as theater is. Theatrical efforts such as \"Six Characters in Search of an Author\", or \"The Wonderful Wizard of Oz\", construct yet another layer of the fourth wall, but they do not destroy the primary illusion. Refer to Erving Goffman's \"Frame Analysis: An Essay on the Organization of Experience\".\n\nIn science fiction, self-awareness describes an essential human property that often (depending on the circumstances of the story) bestows personhood onto a non-human. If a computer, alien or other object is described as \"self-aware\", the reader may assume that it will be treated as a completely human character, with similar rights, capabilities and desires to a normal human being. The words \"sentience\", \"sapience\" and \"consciousness\" are used in similar ways in science fiction.\n\n"}
{"id": "2396733", "url": "https://en.wikipedia.org/wiki?curid=2396733", "title": "Self-criticism", "text": "Self-criticism\n\nSelf-criticism involves how an individual evaluates oneself. Self-criticism in psychology is typically studied and discussed as a negative personality trait in which a person has a disrupted self-identity. The opposite of self-criticism would be someone who has a coherent, comprehensive, and generally positive self-identity. Self-criticism is often associated with major depressive disorder. Some theorists define self-criticism as a mark of a certain type of depression (introjective depression), and in general people with depression tend to be more self critical than those without depression. People with depression are typically higher on self-criticism than people without depression, and even after depressive episodes they will continue to display self-critical personalities. Much of the scientific focus on self-criticism is because of its association with depression.\n\nSidney Blatt has proposed a theory of personality which focuses on self-criticism and dependency. Blatt's theory is significant because he evaluates dimensions of personality as they relate to psychopathology and therapy. According to Blatt, personality characteristics affect our experience of depression, and are rooted in the development of our interpersonal interactions and self-identity. He theorizes that personality can be understood in terms of two distinct dimensions - interpersonal relatedness and self-definition. These two dimensions not only represent personality characteristics, but are products of a lifelong developmental process. Disruption in self-definition or identity leads to self-criticism, and disruption in relatedness leads to dependency. Zuroff (2016) found that self-criticism showed stability across time both as a personality trait and as an internal state. Such a finding is important as it supports the fact that self-criticism can be measured in the same manner as other personality traits.\n\nSimilar to Blatt's two personality dimensions, Aaron Beck (1983) defines social dependency and autonomy as dimensions of personality that are relevant for depression. Autonomy refers to how much the person relies on \"preserving and increasing his independence, mobility, and personal rights\". Furthermore, self-criticism involves holding oneself responsible for any past or present failures. Someone who is a self-critic will attribute negative events as a result of deficiencies in their own character or performance. The personality characteristics that Beck describes as self-critical are usually negative for the person experiencing them. His description of their experience with self-criticism as a personality characteristic is therefore important because it will be similar to their experience of depression.\n\nSelf-criticism as a personality trait has been linked to several negative effects. In a study examining behavior differences between personality types, Mongrain (1998) found that self-critics experienced greater negative affect, perceived support worse than others, and made fewer requests for support. Those who were high in self-criticism did not differ in the amount of support they received, only in how they accepted or requested it. Participants categorized as being higher in self-criticism had fewer interpersonal goals as well as more self-presentation goals. Among romantic partners, self-criticism predicts a decrease in agreeable comments and an increase in blaming.\n\nGiven that self-criticism is typically seen as a negative personality characteristic, it is important to note how some people develop such a trait. As described by the personality theories above, self-criticism often represents a disruption in some characteristic. This disruption could be rooted back in the person's childhood experience. Children of parents who use restrictive and rejecting practices have been shown to have higher levels of self-criticism at age 12. In this same study, women displayed stable levels of self-criticism from age 12 into young adulthood, while men did not. These results show that parenting style can influence the development of self-critical personality, and these effects may potentially last into young adulthood. Another study found that women who were higher in self-criticism reported both that their father was more dominant and their parents maintained strict control and were inconsistent in their expressions of affection. Not surprisingly, these women also reported that their parents tended to seek out achievement and success from their children, as opposed to remaining passive. These studies show that certain experiences in childhood are associated with self-criticism, and the self-critical personality type then extends into later phases of development.\n\nChild maltreatment, which is associated with the development of depression, may also be a risk factor for future self-criticism. Mothers who reported having experience maltreatment as children also perceived themselves as less efficacious mothers. A factor analysis showed that the perception of being less efficacious was mediated by self-criticism, over and above the effects of depressive status. This research shows that self-criticism in particular plays an important role in the relationship between childhood maltreatment and maternal efficacy. In a study assessing child maltreatment and self-injury Glassman and et al (2007) found that self-criticism specifically was a mediator for the relationship between maltreatment and self-injury. This is particularly important because it shows that self-criticism may play a role in leading to self-injury. Understanding the origins of self-criticism in maltreatment could help prevent such behaviors. Given this research, it seems that self-criticism plays a role in the lasting effects of childhood maltreatment. Assessing self-criticism in preventing maltreatment as well as treating those who have been maltreated could therefore support further research in the area.\n\nSelf-criticism is an important aspect of personality and development, but is also significant in terms of what this trait means for psychopathology. Most theorists described above account for self-criticism as a maladaptive characteristic, so unsurprisingly many researchers have found self-criticism to be connected to depression.\n\nSelf-criticism is associated with several other negative variables. In one sample, differences in self-criticism as a personality trait were associated with differences in perceived support, negative affect, self-image goals, and overt self-criticism. These are all characteristics that pertain to the experience of depression, revealing that self-criticism affects depression. The persistence of self-criticism as a personality trait can leave some people vulnerable to developing depression. As stated above, Blatt theorized that people who were more self-critical and focused on achievement concerns were more likely to develop a specific type of depression, which he called introjective depression. Both Blatt and Beck have developed measures to assess self-criticism and the experience of depression. \nIn addition to the fact that many personality theorists classified self-criticism as marking a certain \"type\" of depression, it has been shown to be a risk factor for the development of depression.\n\nThere has been a great deal of research assessing whether certain personality characteristics can lead to depression, among them self-criticism. In one study self-criticism was a significant predictor of depression in medical students, who go through extreme stress during and after medical school. Controlling for initial symptoms, self-criticism was a stronger predictor than even previous depression status both 2 years and 10 years after the initial assessment. In a sample with a history of depression, Mongrain and Leather (2006) found that measures of self-criticism were associated with the number of past episodes of depression. The personality was indicative of depression history, but self-criticism in an interaction with immature dependence was able to predict future episodes of depression as well.\n\nIn a sample of people who either currently have depression or are in remission from a depressive episode, individuals reported both higher levels of self-criticism and lower levels of self-compassion. This same study found that self-critical individuals were also at an increased risk of experiencing depression chronically over the course of their lives. Self-criticism was also able to explain the variance in depression status for currently depressed, remitted depressed, and never depressed patients, over and above other variables. Carver and Ganellen (1983) assessed self-criticism by breaking it down into three distinct categories: Over-generalization of negative events, high standards, and self-criticism. These three categories all deal with self-critical cognitions, and are measured by the Attitude Toward Self Scale, which Carver and Ganellen created.\n\nIn addition to acting as a risk factor for depression, self-criticism also affects the efficacy of depression treatment. Self-criticism as a trait characteristic therefore persists throughout a person's entire life. This means a person can display persistent, long term levels of self-criticism as a personality trait, but levels of self-criticism can vary from moment to moment depending on the person's current mental state. Therefore, in terms of treatment for depression, it could be difficult for clinicians to accurately assess decreases in self-criticism. In a particular session, state levels of self-criticism may increase or decrease, but in the long term it is not as easy to see if trait levels of self-criticism have been reduced, and a reduction in trait self-criticism is more important in terms of effectively treating depression. In other words, it is likely easier to reduce state levels of self-criticism, so researchers who develop treatments for depression should have the goal of treating long-term, trait self-criticism.\n\nIt is possible that change in depression symptoms may not necessarily co-occur with change in personality factors, and given that self-criticism as a personality factor has been shown to lead to depression, this could be problematic. One study found that positive change in depression occurred before any change in self-critical perfectionism. The authors of this study suggested that this has implications for deciding how long to provide treatment. If treatment ends as depression fades away, the underlying personality characteristics that affect depression may not have changed. In such a case extending treatment beyond the point when positive change is seen in depression symptoms may give the best results. This same study also found that levels of perfectionism (which is related to self-critical personality) predicted the rate of change in depression status.\n\nSelf-criticism is known as autonomy in Beck's personality model, and there has been research looking at his conception of sociotropy and autonomy. Sociotropy characterizes people who are socially dependent, and their main source of distress is interpersonal relationships. Autonomy, however refers to self-critical individuals who are more concerned with independence and achievement. In a study examining treatment differences between these groups, Zettle, Haflich & Reynolds (1992) found that autonomous, self-critical individuals had better results in individual therapy than in group therapy. This research shows that personality characteristics can influence what kind of treatment is best for an individual, and that clinicians should be aware of these differences. Therefore self-criticism is both a warning sign for the development of depression and affects how it is treated. It is an important facet of depression research, as it is important for how we might prevent and treat this debilitating disorder.\n\nfMRI finds that engaging in self-criticism activates areas in the lateral prefrontal cortex and dorsal anterior cingulate cortex which are brain areas responsible for processing error detection and correction. In contrast, engaging in self-reassurance activates the left temporal pole and insula areas previously found to be activated in compassion and empathy. Those that as a psychological trait engage in self-criticism tend to show an activated dorsolateral prefrontal activity, while ventrolateral prefrontal cortex activity was found in those with the trait of self-reassurance.\n\nSome Religion cultures consider self-criticism to be a positive and indispensable practice for lifetime satisfaction. See Confession (Judaism), Confession (Lutheran Church), and Sacrament of Penance.\n\nIn some communist states, party members who had fallen out of favor with the nomenklatura were sometimes forced to undergo \"self-criticism\" sessions, producing either written or verbal statements detailing their ideological errors and affirming their renewed belief in the Party line. Self-criticism, however, did not guarantee political rehabilitation, and often offenders were still expelled from the Party, or in some cases even executed.\n\nIn the Soviet Union, \"criticism and self-criticism\" were known as \"kritika i samokritika\" ().\n\nIn the People's Republic of China, self-criticism—called \"ziwo pipan\" () or \"jiǎntǎo\" ()—is an important part of Maoist practice. Mandatory self-criticism as a part of political rehabilitation or prior to execution—common under Mao, ended by Deng Xiaoping, and partially revived by Xi Jinping—is known as a struggle session, in reference to class struggle.\n\nUnder the Khmer Rouge, self-criticism sessions were known as \"rien sot\", meaning \"religious education\". In his memoir \"The Gate\", François Bizot recalls observing the Khmer Rouge engaging in frequent self-criticism to reinforce group cohesion during his imprisonment in rural Cambodia in 1971:\n\nFrench Marxist philosopher Louis Althusser wrote \"Essays in Self-Criticism\" focused on the issue of ideologically correcting ideas expressed in his prior works, most prominently For Marx and Reading Capitol. \n\n"}
{"id": "16344393", "url": "https://en.wikipedia.org/wiki?curid=16344393", "title": "Sheff v. O'Neill", "text": "Sheff v. O'Neill\n\nSheff v. O'Neill refers to a 1989 lawsuit and the subsequent 1996 Connecticut Supreme Court case (\"Sheff v. O'Neill\", 238 Conn. 1, 678 A.2d 1267) that resulted in a landmark decision regarding civil rights and the right to education.\n\nOn April 27, 1989, eighteen school aged children from the metropolitan Hartford, Connecticut area, acting through their parents, commenced a civil action in the Hartford Superior Court. The lead plaintiff was fourth-grader Milo Sheff. The suit named the State of Connecticut, constitutionally elected officials, including Gov. William A. O'Neill, and others from various state commissions and agencies as defendants. The plaintiffs alleged significant constitutional violations under applicable sections of the State constitution which they believe constituted a denial of their fundamental rights to an education and rights to equal protection under the law. The reason for the case was that the resources the state spent on schools in areas with majority black/Latino populations were lower than those spent on schools in areas mainly inhabited by white people.\n\nIn 1995, Judge Harry Hammer ruled in favor of the State in the case. His decision rejected claims that officials are obligated to correct educational inequities, no matter how they came to be. Further, he ruled that without proof that government action helped foster racial isolation, courts cannot require steps that would change the composition of the city and suburban school enrollments. \n\nThis decision was appealed to the Connecticut Supreme Court. On July 9, 1996, the court overturned Hammer's ruling, in a split 4-3 decision authored by Chief Justice Ellen Ash Peters (\"Sheff v. O'Neill\", 678 A.2d 1267 (1996), 678 A.2d 1267). Peters was joined in the majority opinion by Justices Robert Berdon, Flemming L. Norcott, Jr., and Joette Katz. Justice David Borden authored the dissent, with Justices Robert Callahan and Richard Palmer concurring with the dissent. The court ruled that the state had an affirmative obligation to provide Connecticut's school children with a substantially equal educational opportunity and that this constitutionally guaranteed right encompasses the access to a public education which is not substantially and materially impaired by racial and ethnic isolation. The Court further concluded that school districting, based upon town and city boundary lines, is unconstitutional, and cited a statute that bounds school districts by town lines as a key factor in the high concentrations of racial and ethnic minorities in Hartford. \n\nAs a result of the Connecticut Supreme Court decision, in 1997 the Connecticut State Legislature passed legislation titled \"An Act Enhancing Educational Choices and Opportunities\", which encourages voluntary actions toward racial integration. The act also included a number of other measures related to magnet and regional charter schools and included a requirement for the Connecticut State Department of Education to come up with a five-year plan to assess and eliminate inequalities between school districts. \n\nIn 1998, the Sheff plaintiffs filed a motion for a court order to require the state to adhere to the Supreme Court ruling.. On March 3, 1999 Superior Court Judge Julia L. Aurigemma ruled that the state of Connecticut had complied with the decision of the Connecticut Supreme Court. \n\nIn 2002, Judge Aurigemma held a hearing on the progress of the case and negotiations began on a settlement which was approved in 2003 . It included a goal of having 30 percent of Hartford minority students in reduced-isolation school settings by 2007. \nIn 2007, the 2003 settlement expired short of its goal. An independent Trinity College report found that only 9 percent of Hartford's minority students attended less racially isolated schools. The plaintiffs brought the issue back to court in 2007 and the two sides began talks on a second settlement.\n\nIn June 2008, a second settlement was negotiated , calling for building more magnet schools in the Hartford suburbs and expanding the number of openings for Hartford children in suburban public schools. The new settlement also included state-run technical and agricultural high schools. \n\nIn Dec 2008, the state and the plaintiffs issued a 50-page document that outlined exactly how the new goals would be met. The plan called for a mix of existing programs, creating new magnet and charter schools, increasing support for the programs and collecting data on progress. \n\n"}
{"id": "1126522", "url": "https://en.wikipedia.org/wiki?curid=1126522", "title": "Speculative reason", "text": "Speculative reason\n\nSpeculative reason, sometimes called theoretical reason or pure reason, is theoretical (or logical, deductive) thought, as opposed to practical (active, willing) thought. The distinction between the two goes at least as far back as the ancient Greek philosophers, such as Plato and Aristotle, who distinguished between theory (\"theoria,\" or a wide, bird's eye view of a topic, or clear vision of its structure) and practice (\"praxis\"), as well as \"techne\".\n\nSpeculative reason is contemplative, detached, and certain, whereas practical reason is engaged, involved, active, and dependent upon the specifics of the situation. Speculative reason provides the universal, necessary principles of logic, such as the principle of non-contradiction, which must apply everywhere, regardless of the specifics of the situation. \n\nOn the other hand, practical reason is the power of the mind engaged in deciding what to do. It is also referred to as moral reason, because it involves action, decision, and particulars. Though many other thinkers have erected systems based on the distinction, two important later thinkers who have done so are Aquinas (who follows Aristotle in many respects) and Immanuel Kant.\n\n"}
{"id": "1035767", "url": "https://en.wikipedia.org/wiki?curid=1035767", "title": "Stockholm Convention on Persistent Organic Pollutants", "text": "Stockholm Convention on Persistent Organic Pollutants\n\nStockholm Convention on Persistent Organic Pollutants is an international environmental treaty, signed in 2001 and effective from May 2004, that aims to eliminate or restrict the production and use of persistent organic pollutants (POPs).\n\nIn 1995, the Governing Council of the United Nations Environment Programme (UNEP) called for global action to be taken on POPs, which it defined as “chemical substances that persist in the environment, bio-accumulate through the food web, and pose a risk of causing adverse effects to human health and the environment”.\n\nFollowing this, the Intergovernmental Forum on Chemical Safety (IFCS) and the International Programme on Chemical Safety (IPCS) prepared an assessment of the 12 worst offenders, known as the \"dirty dozen\".\n\nThe INC met five times between June 1998 and December 2000 to elaborate the convention, and delegates adopted the Stockholm Convention on POPs at the Conference of the Plenipotentiaries convened from 22–23 May 2001 in Stockholm, Sweden.\n\nThe negotiations for the Convention were completed on 23 May 2001 in Stockholm. The convention entered into force on 17 May 2004 with ratification by an initial 128 parties and 151 signatories. Co-signatories agree to outlaw nine of the dirty dozen chemicals, limit the use of DDT to malaria control, and curtail inadvertent production of dioxins and furans.\n\nParties to the convention have agreed to a process by which persistent toxic compounds can be reviewed and added to the convention, if they meet certain criteria for persistence and transboundary threat. The first set of new chemicals to be added to the Convention were agreed at a conference in Geneva on 8 May 2009.\n\nAs of June 2018, there are 182 parties to the Convention, (181 states and the European Union). Notable non-ratifying states include the United States, Israel, Malaysia, and Italy.\n\nThe Stockholm Convention was adopted to EU legislation in REGULATION (EC) No 850/2004.\n\nKey elements of the Convention include the requirement that developed countries provide new and additional financial resources and measures to eliminate production and use of intentionally produced POPs, eliminate unintentionally produced POPs where feasible, and manage and dispose of POPs wastes in an environmentally sound manner. Precaution is exercised throughout the Stockholm Convention, with specific references in the preamble, the objective, and the provision on identifying new POPs.\n\nWhen adopting the Convention, provision was made for a procedure to identify additional POPs and the criteria to be considered in doing so. At the first meeting of the Conference of the Parties (COP1), held in Punta del Este, Uruguay from 2–6 May 2005, the POPRC was established to consider additional candidates nominated for listing under the Convention.\n\nThe Committee is composed of 31 experts nominated by parties from the five United Nations regional groups and reviews nominated chemicals in three stages. The Committee first determines whether the substance fulfills POP screening criteria detailed in Annex D of the Convention, relating to its persistence, bioaccumulation, potential for long-range environmental transport (LRET), and toxicity. If a substance is deemed to fulfill these requirements, the Committee then drafts a risk profile according to Annex E to evaluate whether the substance is likely, as a result of its LRET, to lead to significant adverse human health and/or environmental effects and therefore warrants global action. Finally, if the POPRC finds that global action is warranted, it develops a risk management evaluation, according to Annex F, reflecting socioeconomic considerations associated with possible control measures. Based on this, the POPRC decides to recommend that the COP list the substance under one or more of the annexes to the Convention. The POPRC has met annually in Geneva, Switzerland since its establishment. \nThe seventh meeting of the Persistent Organic Pollutants Review Committee (POPRC-7) of the Stockholm Convention on Persistent Organic Pollutants (POPs) took place from 10–14 October 2011 in Geneva, Switzerland. POPRC-8 was held from 15–19 October 2012 in Geneva, POPRC-9 to POPRC-13 were held in Rome.\n\nThere were initially twelve distinct chemicals listed in three categories. Two chemicals, hexachlorobenzene and polychlorinated biphenyls, were listed in both categories A and C.\n\n\n\n\nPOPRC-7 considered three proposals for listing in Annexes A, B and/or C of the Convention: chlorinated naphthalenes (CNs), hexachlorobutadiene (HCBD) and pentachlorophenol (PCP), its salts and esters. The proposal is the first stage of the POPRC's work in assessing a substance, and requires the POPRC to assess whether the proposed chemical satisfies the criteria in Annex D of the Convention. The criteria for forwarding a proposed chemical to the risk profile preparation stage are persistence, bioaccumulation, potential for long-range environmental transport (LRET), and adverse effects.\n\nPOPRC-8 proposed hexabromocyclododecane for listing in Annex A, with specific exemptions for production and use in expanded polystyrene and extruded polystyrene in buildings. This proposal was agreed at the sixth Conference of Parties on 28 April-10 May 2013.\n\nPOPRC-9 proposed di-,tri-,tetra-,penta-,hexa-, hepta- and octa-chlorinated napthalenes, and hexachlorobutadiene for listing in Annexes A and C. It also set up further work on pentachlorophenol, its salts and esters, and decabromodiphenyl ether, perfluorooctanesulfonic acid, its salts and perfluorooctane sulfonyl chloride.\n\nAlthough some critics have alleged that the treaty is responsible for the continuing death toll from malaria, in reality the treaty specifically permits the public health use of DDT for the control of mosquitoes (the malaria vector). There are also ways to prevent high amounts of DDT consumed by using other malaria vectors such as window screens. As long as there are specific measures taken, such as use of DDT indoors, then the limited amount of DDT can be used in a regulated fashion. From a developing country perspective, a lack of data and information about the sources, releases, and environmental levels of POPs hampers negotiations on specific compounds, and indicates a strong need for research.\n\nAnother controversy would be certain POPs (which are continually active, specifically in the Arctic Biota) that were mentioned in the Stockholm Convention, but were not part of the Dirty Dozen such as Perfluorooctone sulfonates (PFOs). PFOs have many general uses such as stain repellents but have many properties which can make it a dangerous due to the fact that PFOs can be highly resistant to environmental breakdown. PFOs can be toxic in terms of increased offspring death, decrease in body weight, and the disruption of neurological systems. What makes this compound controversial is the economic and political impact it can have among various countries and businesses.\n\n\n\n\n"}
{"id": "3687249", "url": "https://en.wikipedia.org/wiki?curid=3687249", "title": "Substantial truth", "text": "Substantial truth\n\nSubstantial truth is a legal doctrine affecting libel and slander laws in common law jurisdictions such as the United States or the United Kingdom.\n\nUnder the United States law, a statement cannot be held to be actionable as slanderous or libelous if the statement is true; the substantial truth doctrine extends this protection by holding that a statement with \"slight inaccuracies of expression\" do not make the alleged libel false.\n\nThis doctrine is applied in matters in which truth is used as an absolute defense to a defamation claim brought against a public figure, but only false statements made with \"actual malice\" are subject to sanctions. A defendant using truth as a defense in a defamation case is not required to justify every word of the alleged defamatory statements. It is sufficient to prove that \"the substance, the gist, the sting, of the matter is true.\"\n\n"}
{"id": "24983621", "url": "https://en.wikipedia.org/wiki?curid=24983621", "title": "The Unconquerable World", "text": "The Unconquerable World\n\nThe Unconquerable World: Power, Nonviolence, and The Will of the People is a book on the power of nonviolence by Jonathan Schell published in 2003.\n\nSchell starts by discussing the cultural embeddedness of men, patriotism and death in battle (going back to the Athenian - Pericles). From this classic root political morality has held onto the need for 'standing up for principles with force', which in practice quickly descends to \"plunder, exploitation and massacre\".\n\nIn the 5th century, St. Augustine conjoined this with Christian love... by theorising 'separate realms' for political and religious morality. Politics has thus long been wedded to violence... it is hard to conceptualise a political ideology that does not have a resort to violence clause. As Schell says there has been, \"an age old reliance of politics on violent means\". (p. 4)\n\nSchell then puts forward his main thesis that \"violence has now become dysfunctional as a political instrument\" (p. 7) and that \"forms of non-violent action can serve effectively in the place of violence at every level of political affairs\". (p. 8)\n\nThe key political progress has been the idea of democracy - even the worst democracy - carries within it the principle of equality which is a deeply seated contradiction to an also deeply embedded practice of inequality - see Tocqueville. Ironically modern national democracy allowed for a new kind of army, in which it was possible to mobilise masses of men prepared to die - apparently in defence of their own national interest and the principle of democracy. The disaster of the modern war system was fed by an unholy confluence of democracy, science, industrial revolution, and imperialism which developed through the 19th century.\n\nPower came to be widely defined by the ability to wage war. He describes imperialism as \"a monotonous record of one sided slaughter\" (p. 75) He argues that there was an end of limited war between 'great powers' after 1870 (p. 44). After that we had 1914 and the period of total or world war until 1946. Then the period of Cold War in which the public appearances of a strategic balance of power became more important than the reality of a balance of actual power. The A-bomb made a balance of actual power obsolete (p. 62).\n\nThe first modern people's war was the Spanish resistance to the French invasion 1807-14 (p. 68). A people's war that showed how a superior force can be worn down by lesser military strength contrary to the classic rules of conventional warfare. The main need was to endure, and an armed population. George Washington also understood this need to endure. \"Washington was always aware that his most important task was to insure the survival of his own forces - not strictly for military purposes but to personify the unconquerable will of the American people\" (p. 157).\n\nThe big downside of people's war is that the whole people then become subject to retaliation (p. 81). In the Japanese retaliation against the Chinese communists, the population dropped from 45 million to 25 million. With this level of violence, political ideals are what buoys the resistance up. It also allowed humanitarian treatment of prisoners by the communist forces to be maintained. \n\nIn a people's war it is important that war is kept subordinate to politics. This is the first stage of extracting politics from the war machine. Politics here means the creation of a civil administration from the ground up. For Mao the most important goal and foundation of this politics was the redistribution of land from rich to poor.\n\nA militarised politics can easily segue into a totalitarian politics as it did in China via conventional warfare against the American-backed Kuomintang. \n\nPeople's war became the principal instrument of self-determination and social change in the third world from around the 1950s. If a population is united, an imperial war against it is difficult to win. Charles de Gaulle understood this in relation to Algeria in 1958. Even when he had achieved a military victory it did not lead to a political one.\n\nThe nuclear standoff produced a stalemate in the war system. The last resort was unusable. '\"In both (nuclear deterrence and people war) violence became not so much an instrument for producing physical effects as a kind of bloody system of communication, through which the antagonists produced messages to one another about will.\" (p. 97) Nuclear deterrence became a conflict waged by appearances to produce intangible effects on leaderships and populations. People's war also came to be decided by intangible effects on hearts and minds of both sides. In both situations the capacity of violence for achieving political ends is thrown into doubt.\n\nA similar mindset of belief in violence pervaded western theories of revolution. Right, left and centre theorists and leaders agreed that that revolution had to be violent. Power was only understood as an effect of violent coercive rule. (except for a few voices, e.g., Tolstoy)\n\nTwo expressly political theorists thought consistently about nonviolence: Mahatma Gandhi and Hannah Arendt.\n\nMohandas K. Gandhi believed that courage was a more important attribute of those seeking liberation than a desire for nonviolence or Satyagraha. At one point he even acted as conscription agent for the British military in South Africa. (date?) Gandhi appealed to the 'spiritual' strength of Indian peoples to oppose the Western Imperial (war) system.\n\nThe assumption here is that tyrants and ruling classes alike only have the power that we invest in them. Give them nothing and they are naked and have no power. \"The central role of consent in all government meant that non-cooperation - the withdrawal of consent - was something more than a morally satisfying activity; it was a powerful weapon in the real world.\" (p. 129) \"Non-cooperation is not a passive state, it is an intensely active state - more active than physical resistance or violence.\"' (p. 130) Schell quoting from \"Gandhi's Essential Writings\" (p. 99)\n\nThe Gandhi programme of Satyagraha (\"the quiet and irresistible pursuit of truth\") was accompanied by 'the constructive programme'. This campaigned and organised to achieve concrete goals such as justice for workers, peace between factions, village hygiene and diet, the condition of women, etc. The idea is to do anything that obstructed the solidarity of the nation and that facilitates the production of a democratic political culture. \"Constructive effort is political power\" \"Gandhi's Essential Writings\" (p. 259)\n\nSchell looks for the nonviolent actions that are part of what are often represented as broadly violent revolutions. (p. 143) He defines violence: \"Violence is the method by which the ruthless few can subdue the passive many. Non-violence is a means by which the active many can overcome the ruthless few.\" (p. 144). He discusses The Glorious Revolution of 1689 in England and the American, French and Russian Revolutions. He makes the interesting observation that revolutions are typified as violent and the subsequent establishment of a new regime is assumed to be peaceful, whereas the reverse has \"more often been the case\". (p. 144) in the French revolution, the American, even the Russian...(p. 175) (p. 178). More people died making Potemkin the film than in the actual storming of the Winter Palace! The Russian Revolution was more of a victory of a 'mass minority' ... than a real people's revolution as happened in France or America (p. 183).\n\nThe English 'Glorious Revolution' of 1689 \"London was in fact the first of many modern capitals whose rebellious spirit was to infect and destroy the allegiance of an army of an ancien regime\". This was William of Orange versus King James, with defections at the non-battle at Salisbury.\n\nThere has been a near universal failure of theorists to predict the non-violent fall of powers. He quotes Thomas Paine: \"Tis not in numbers but in unity that our great strength lies\". In the American Revolution Committees of Correspondence were formed for \"mutually fostering and co-ordinating activity\". These were the basic political units.\n\nEdmund Burke realised that it is only by cultivating the love and admiration of the people that a ruler can raise taxes or an army or get voted in. This love \"infuses into (army and navy) that liberal obedience without which your army would be base rabble and you navy nothing but rotten timber\" (p. 159).\n\nSchell's sources here are Adam Michnik, Václav Havel and György Konrád as sources to explain the 'unsuspected' weakness of the Soviet empire. In their beginnings they \"did not aim at state power\" (p. 191) but aimed at \"achieving immediate changes in daily life\" within what was called 'civil society' a term that goes back to Paine. (p. 194)\n\nThe setting up of institutions independent from the state or the ruling classes... is the key activity in preparing for a peoples revolution. In Eastern Europe this took the form of civic and cultural activity (p. 195), e.g., Workers' Defence Committee provided concrete assistance to those in trouble with the authorities, and their families. Other terms for similar activity:\n\n\nVáclav Havel suggested there should be no discourse with the centre of power but that activists should \"fight only for those concrete causes, and fight for them unswervingly to the end\" (p. 196). Compare this with Gandhi's earlier call for courage . Chosen well these independent institutions will rattle the establishment. The idea of cultivating 'a predisposition to truth' (Havel/ Gandhi) is seen as key (p. 197). \n\nThe final collapse of the USSR is not like the rest of Eastern Europe as it was a mainly top down operation rather than one of peoples power. There was still a remarkable and unexpected lack of violence.\n\nThe main thesis is reiterated: \"The professionals of power, in or out of government, were consistently caught off-guard by the failures of superior force and the successes of nonviolence\" (p. 216). Contemporary political theory could 'neither foresee nor explain' the successes of non-violent people power.\n\nHannah Arendt's redefinition of the word 'power' has a strong echo from Paine. She says: \"Power corresponds to the human ability not just to act but to act in concert.\" (Schell p. 218). She goes so far as to deny ascription of 'power' to individual action.\n\nMax Weber is probably more realistic in asserting that power is the assertion of will in a social relationship. (p. 220) But this does mean that in limiting its meaning Hannah Arendt can discuss political power in more detail. The assertion of the will of the tyrant is ultimately an illusion, the tyrant is dependent on his supporters.\nViolence can never create real political power in Arendt's sense: \"To substitute violence for power can bring victory, but the price is very high; for it is not only paid by the vanquished, it is also paid by the victor in terms of his own power.\" (Schell p. 222). According to John Stuart Mill public opinion guides will and \"a great part of all power consists in... willing allegiance\" (Schell p. 229).\n\nSchell sees the 'taming of violence' being written into liberalisms 'genetic code'. His example is the Civil Rights Movement in the USA. Although he sees nonviolent action being only 'curative' or reforming of a liberal democratic system.\n\n"}
{"id": "46333271", "url": "https://en.wikipedia.org/wiki?curid=46333271", "title": "Vertebrate land invasion", "text": "Vertebrate land invasion\n\nThe aquatic to terrestrial transition of vertebrate organisms occurred in the late Devonian era and was an important step in the evolutionary history of modern land vertebrates. The transition allowed animals to escape competitive pressure from the water and explore niche opportunities on land. Fossils from this period have allowed scientists to identify some of the species that existed during this transition, such as Tiktaalik and Acanthostega. Many of these species were also the first to develop adaptations suited to terrestrial over aquatic life, such as neck mobility and hindlimb locomotion.\n\nThe late Devonian vertebrate transition was preceded by the plant and invertebrate terrestrial invasion. These invasions allowed for the appropriate niche development that would ultimately facilitate the vertebrate invasion. While the late Devonian event was the first land invasion by vertebrate organisms, aquatic species have continued to develop adaptations suited to terrestrial life (and vice versa) from the late Devonian to the Holocene.\n\nThe vertebrate species that were important to the initial water to land transition can be qualified as being in one of five groups: Sarcopterygian fishes, prototetrapods, aquatic tetrapods, true tetrapods, and terrestrial tetrapods. Many morphological changes occurred throughout this transition. Mechanical support structures changed from fins to limbs, the method of locomotion changed from swimming to walking, respiratory structures changed from gills to lungs, feeding mechanisms changed from suction feeding to biting, and mode of reproduction changed from larval development to metamorphosis.\n\nLungfish appeared approximately 400 million years ago. It is a species that endured rapid evolution during the Devonian era, which became known as the dipnoan renaissance. The Acanthostega species, known as the fish with legs, is considered a tetrapod by structural findings but is postulated to have perhaps never left the aquatic environment. Its legs are not well-suited to support its weight. The bones of its forearm, the radius and ulna, are very thin at the wrist and also unable to support it on land. It also lacks a sacrum and strong ligaments at the hip, which would be integral to supporting the animal against gravity. In this sense, the species is considered a tetrapod but not one that has adapted well enough to walk on land. Furthermore, its gill bars have a supportive brace characterized for use as an underwater ear because it can pick up noise vibrations through the water. Tetrapods that adapted to terrestrial living adapted these gill bones to pick up sounds through air, and they later became the middle ear bones seen in mammalian tetrapods. Ichthyostega, on the other hand, is considered to be a fully terrestrial tetrapod that perhaps depended on water for its aquatic young. Comparisons between the skeletal features of Acanthostega and Ichthyostega reveal that they had different habits. Acanthostega is likely exclusive to an aquatic environment, while Ichthyostega is progressed in the aquatic to terrestrial transition by living dominantly on the shores.\n\nAn evolutionary timeline of the late Devonian vertebrate terrestrial invasion demonstrates the changes that took place. A group of fish from the Givetian stage began developing limbs, and eventually evolved into aquatic tetrapods in the Famennian stage. Pederpes, Westlothiana, Protogyrinus, and Crassigyrinus descended from these species into the carboniferous period and were the first land vertebrates.\n\nA particularly important transitional species is one known as Tiktaalik. It has a fin, but the fin has bones within it that are similar to mammalian tetrapods. It has an upper arm bone, a lower arm bone, forearm bones, a wrist, and fingerlike projections. Essentially, it is a fin that can support the animal. Similarly, it also has a neck that allows independent head movement from the body. Its ribs are also able to support the body in gravity. Its skeletal features exhibit its ability as a fish that can live in shallow water and also venture onto land.\n\nIt took many millions of years for vertebrates to transition out of water onto land. During this time, both the competitive pressures that would push species out of the water and the niche occupation incentives that would pull species onto land were slowly building. The culmination of these driving factors are what ultimately facilitated the vertebrate transition.\n\nScientists believe that a long period of time where biotic and abiotic factors in the aquatic environment were unfavourable to certain aquatic organisms is what pushed their transition to shallower waters. Some of these push factors are environmental hypoxia, unfavourable aquatic temperatures, and increased salinity. Other constantly present factors such as predation, competition, waterborne diseases and parasites also contributed to the transition.\n\nA theory put forth by Joseph Barrell possibly helps explain what may have initiated these push factors to become relevant in the late Devonian. The extensive oxidized sediments that were present in Europe and North America during the late Devonian are evidence of severe droughts during this time. These droughts would cause small ponds and lakes to dry out, forcing certain aquatic organisms to move on land to find other bodies of water. Natural selection on these organisms eventually led to the evolution of the first terrestrial vertebrates.\n\nThe pull factors were secondary to the push factors, and only became significant once the pressures to leave the aquatic environment became significant. These were largely the niches and opportunities that were available for exploitation in the terrestrial environment, and include higher environmental oxygen partial pressures, favourable temperatures, and the lack of competitors and predators on land. The plants and invertebrates that had preceded the vertebrate invasion also provided opportunities in the form of abundant prey and lack of predators.\n\nThere were many challenges that the first land vertebrates faced. These challenges allowed for rapid natural selection and niche domination, resulting in an adaptive radiation that produced many different vertebrate land species in a relatively short period of time.\n\nDepending on the water depth at which a species lives, the visual perception of many aquatic species is better suited to darker environments than those on land. Similarly, hearing in aquatic organisms is better optimized for sounds underwater, where the speed and amplitude of sound is greater than in air.\n\nHomeostasis was almost definitely a challenge for land invading vertebrates. Gas exchange and water balance are highly different in water and in air. Homeostasis mechanisms suitable for a terrestrial environment may have been necessary to develop before these organisms invaded land.\n\nThe primary anatomical barrier is the development of lungs for proper gas exchange, however other anatomical barriers also exist. The stressors of the musculoskeletal system are different in air than they are in water, and the muscles and bones must be strong enough to withstand the increased effects of gravity on land.\n\nMany behaviours, such as reproduction, are specifically optimized to a wet environment. Navigation and locomotion are also highly different in aquatic environments compared to terrestrial environments.\n\nThe ancestral species of tetrapods that lived entirely in water had tall and narrow skulls with eyes facing sideways and forwards to maximize visibility for predators and prey in the aquatic environment. As the ancestors of early tetrapods started inhabiting shallower waters, these species had flatter skulls with eyes at the tops of their heads, which made it possible to spot food above them. Once the tetrapods transitioned onto land, the lineages evolved to have tall and narrow skulls with eyes facing sideways and forwards again. This allowed them to navigate through the terrestrial environment and look for predators and prey.\n\nFish do not have necks, so the head is directly connected to the shoulders. In contrast, land animals use necks to move their heads so they can look down to see the food on the ground. The greater the mobility of the neck, the more visibility the land animal has. As lineages moved from completely aquatic environments to shallower waters and land, they gradually evolved vertebral columns that increased neck mobility. The first neck vertebra that evolved permitted the animals to have flexion and extension of the head so that they can see up and down. The second neck vertebra evolved to allow rotation of the neck for moving the head left and right. As tetrapod species continued to evolve on land, adaptations included seven or more vertebrae, allowing increasing neck mobility.\n\nThe sacrum connects the pelvis and hindlimbs and is useful for motion on land. The aquatic ancestors of tetrapods did not have a sacrum, so it was speculated to have evolved for locomotive function exclusive to terrestrial environments. However, the Acanthostega species is one of the earliest lineages to have a sacrum, even though it is a fully aquatic species. Once species moved onto land, the trait was adapted for terrestrial locomotion support, which is evidenced by additional vertebrae fusing similarly to permit additional support. This is an example of exaptation, where a trait performs a function that did not arise through natural selection for its current use.\n\nAs the lineages evolved to adapt to terrestrial environments, many lost traits that were better suited for the aquatic environment. Many lost their gills, which were only useful for obtaining oxygen in water. Their tail fins became smaller. They lost the lateral line system, a network of canals along the skull and jaw that are sensitive to vibration, which does not work outside of an aquatic environment.\n\nFor successful land invasion, the species had several pre-adaptations like air-breathing and limb-based locomotion. Aspects such as reproduction and swallowing, however, have bound these species to the aquatic environment. These pre-adaptations have allowed vertebrates to venture onto land hundreds of times, but were not able to accomplish the same degree of prolific radiation into diverse terrestrial species. To understand the potential of future invasions, studies must evaluate the models of evolutionary steps taken in past invasions. The commonalities to current and future invasions may then be elucidated to predict the effects of environmental changes.\n"}
{"id": "33959", "url": "https://en.wikipedia.org/wiki?curid=33959", "title": "Witchcraft", "text": "Witchcraft\n\nWitchcraft or witchery broadly means the practice of and belief in magical skills and abilities exercised by solitary practitioners and groups. \"Witchcraft\" is a broad term that varies culturally and societally, and thus can be difficult to define with precision, and cross-cultural assumptions about the meaning or significance of the term should be applied with caution. Witchcraft often occupies a religious divinatory or medicinal role, and is often present within societies and groups whose cultural framework includes a magical world view.\n\nThe concept of witchcraft and the belief in its existence have persisted throughout recorded history. They have been present or central at various times and in many diverse forms among cultures and religions worldwide, including both \"primitive\" and \"highly advanced\" cultures, and continue to have an important role in many cultures today. Scientifically, the existence of magical powers and witchcraft are generally believed to lack credence and to be unsupported by high-quality experimental testing, although individual witchcraft practices and effects may be open to scientific explanation or explained via mentalism and psychology.\n\nHistorically, the predominant concept of witchcraft in the Western world derives from Old Testament laws against witchcraft, and entered the mainstream when belief in witchcraft gained Church approval in the Early Modern Period. It posits a theosophical conflict between good and evil, where witchcraft was generally evil and often associated with the Devil and Devil worship. This culminated in deaths, torture and scapegoating (casting blame for human misfortune), and many years of large scale witch-trials and witch hunts, especially in Protestant Europe, before largely ceasing during the European Age of Enlightenment. Christian views in the modern day are diverse and cover the gamut of views from intense belief and opposition (especially from Christian fundamentalists) to non-belief, and in some churches even approval. From the mid-20th century, witchcraft – sometimes called contemporary witchcraft to clearly distinguish it from older beliefs – became the name of a branch of modern paganism. It is most notably practiced in the Wiccan and modern witchcraft traditions, and no longer practices in secrecy.\n\nThe Western mainstream Christian view is far from the only societal perspective about witchcraft. Many cultures worldwide continue to have widespread practices and cultural beliefs that are loosely translated into English as \"witchcraft\", although the English translation masks a very great diversity in their forms, magical beliefs, practices, and place in their societies. During the Age of Colonialism, many cultures across the globe were exposed to the modern Western world via colonialism, usually accompanied and often preceded by intensive Christian missionary activity \"(see \"Christianization\")\". Beliefs related to witchcraft and magic in these cultures were at times influenced by the prevailing Western concepts. Witch hunts, scapegoating, and killing or shunning of suspected witches still occurs in the modern era, with killings both of victims for their supposedly magical body parts, and of suspected witchcraft practitioners.\n\nSuspicion of modern medicine due to beliefs about illness being due to witchcraft also continues in many countries to this day, with tragic healthcare consequences. HIV/AIDS and Ebola virus disease are two examples of often-lethal infectious disease epidemics whose medical care and containment has been severely hampered by regional beliefs in witchcraft. Other severe medical conditions whose treatment is hampered in this way include tuberculosis, leprosy, epilepsy and the common severe bacterial Buruli ulcer. Public healthcare often requires considerable education work related to epidemology and modern health knowledge in many parts of the world where belief in witchcraft prevails, to encourage effective preventive health measures and treatments, to reduce victim blaming, shunning and stigmatization, and to prevent the killing of people and endangering of animal species for body parts believed to convey magical abilities.\n\nThe word \"witch\" is of uncertain origin. There are numerous etymologies that it could be derived from. One popular belief is that it is \"related to the English words wit, wise, wisdom [Germanic root *weit-, *wait-, *wit-; Indo-European root *weid-, *woid-, *wid-],\" so \"craft of the wise.\" Another is from the Old English wiccecræft, a compound of \"wicce\" (\"witch\") and \"cræft\" (\"craft\").\n\nIn anthropological terminology, witches differ from sorcerers in that they don't use physical tools or actions to curse; their maleficium is perceived as extending from some intangible inner quality, and one may be unaware of being a witch, or may have been convinced of his/her nature by the suggestion of others. This definition was pioneered in a study of central African magical beliefs by E. E. Evans-Pritchard, who cautioned that it might not correspond with normal English usage.\n\nHistorians of European witchcraft have found the anthropological definition difficult to apply to European witchcraft, where witches could equally use (or be accused of using) physical techniques, as well as some who really had attempted to cause harm by thought alone. European witchcraft is seen by historians and anthropologists as an ideology for explaining misfortune; however, this ideology has manifested in diverse ways, as described below.\n\nHistorically the witchcraft label has been applied to practices people believe influence the mind, body, or property of others against their will—or practices that the person doing the labeling believes undermine social or religious order. Some modern commentators believe the malefic nature of witchcraft is a Christian projection. The concept of a magic-worker influencing another person's body or property against their will was clearly present in many cultures, as traditions in both folk magic and religious magic have the purpose of countering malicious magic or identifying malicious magic users. Many examples appear in early texts, such as those from ancient Egypt and Babylonia. Malicious magic users can become a credible cause for disease, sickness in animals, bad luck, sudden death, impotence and other such misfortunes. Witchcraft of a more benign and socially acceptable sort may then be employed to turn the malevolence aside, or identify the supposed evil-doer so that punishment may be carried out. The folk magic used to identify or protect against malicious magic users is often indistinguishable from that used by the witches themselves.\n\nThere has also existed in popular belief the concept of white witches and white witchcraft, which is strictly benevolent. Many neopagan witches strongly identify with this concept, and profess ethical codes that prevent them from performing magic on a person without their request.\n\nWhere belief in malicious magic practices exists, such practitioners are typically forbidden by law as well as hated and feared by the general populace, while beneficial magic is tolerated or even accepted wholesale by the people – even if the orthodox establishment opposes it.\n\nProbably the most widely known characteristic of a witch was the ability to cast a spell, \"spell\" being the word used to signify the means employed to carry out a magical action. A spell could consist of a set of words, a formula or verse, or a ritual action, or any combination of these. Spells traditionally were cast by many methods, such as by the inscription of runes or sigils on an object to give it magical powers; by the immolation or binding of a wax or clay image (poppet) of a person to affect him or her magically; by the recitation of incantations; by the performance of physical rituals; by the employment of magical herbs as amulets or potions; by gazing at mirrors, swords or other specula (scrying) for purposes of divination; and by many other means.\n\nIn Christianity and Islam, sorcery came to be associated with heresy and apostasy and to be viewed as evil. Among the Catholics, Protestants, and secular leadership of the European Late Medieval/Early Modern period, fears about witchcraft rose to fever pitch and sometimes led to large-scale witch-hunts. The key century was the fifteenth, which saw a dramatic rise in awareness and terror of witchcraft, culminating in the publication of the \"Malleus Maleficarum\" but prepared by such fanatical popular preachers as Bernardino of Siena. Throughout this time, it was increasingly believed that Christianity was engaged in an apocalyptic battle against the Devil and his secret army of witches, who had entered into a diabolical pact. In total, tens or hundreds of thousands of people were executed, and others were imprisoned, tortured, banished, and had lands and possessions confiscated. The majority of those accused were women, though in some regions the majority were men. In early modern Scots, the word Warlock came to be used as the male equivalent of witch (which can be male or female, but is used predominantly for females). From this use, the word passed into Romantic literature and ultimately 20th-century popular culture. Accusations of witchcraft were often combined with other charges of heresy against such groups as the Cathars and Waldensians.\n\nThe \"Malleus Maleficarum,\" (Latin for \"Hammer of The Witches\") was a witch-hunting manual written in 1486 by two German monks, Heinrich Kramer and Jacob Sprenger. It was used by both Catholics and Protestants for several hundred years, outlining how to identify a witch, what makes a woman more likely than a man to be a witch, how to put a witch on trial, and how to punish a witch. The book defines a witch as evil and typically female. The book became the handbook for secular courts throughout Renaissance Europe, but was not used by the Inquisition, which even cautioned against relying on the work, and was later officially condemned by the Catholic Church in 1490.\n\nIn the modern Western world, witchcraft accusations have often accompanied the satanic ritual abuse moral panic. Such accusations are a counterpart to blood libel of various kinds, which may be found throughout history across the globe.\n\nThroughout the early modern period, the English term \"witch\" was not exclusively negative in meaning, and could also indicate cunning folk. As Alan McFarlane noted, \"There were a number of interchangeable terms for these practitioners, 'white', 'good', or 'unbinding' witches, blessers, wizards, sorcerers, however 'cunning-man' and 'wise-man' were the most frequent.\" The contemporary Reginald Scot explained, \"At this day it is indifferent to say in the English tongue, 'she is a witch' or 'she is a wise woman'\". Folk magicians throughout Europe were often viewed ambivalently by communities, and were considered as capable of harming as of healing, which could lead to their being accused as \"witches\" in the negative sense. Many English \"witches\" convicted of consorting with demons seem to have been cunning folk whose fairy familiars had been demonised; many French \"devins-guerisseurs\" (\"diviner-healers\") were accused of witchcraft, and over one half the accused witches in Hungary seem to have been healers.\n\nSome of the healers and diviners historically accused of witchcraft have considered themselves mediators between the mundane and spiritual worlds, roughly equivalent to shamans. Such people described their contacts with fairies, spirits often involving out-of-body experiences and travelling through the realms of an \"other-world\".\nBeliefs of this nature are implied in the folklore of much of Europe, and were explicitly described by accused witches in central and southern Europe. Repeated themes include participation in processions of the dead or large feasts, often presided over by a horned male deity or a female divinity who teaches magic and gives prophecies; and participation in battles against evil spirits, \"vampires\", or \"witches\" to win fertility and prosperity for the community.\n\nÉva Pócs states that reasons for accusations of witchcraft fall into four general categories:\n\nShe identifies three varieties of witch in popular belief:\n\"Neighbourhood witches\" are the product of neighbourhood tensions, and are found only in self-sufficient serf village communities where the inhabitants largely rely on each other. Such accusations follow the breaking of some social norm, such as the failure to return a borrowed item, and any person part of the normal social exchange could potentially fall under suspicion. Claims of \"sorcerer\" witches and \"supernatural\" witches could arise out of social tensions, but not exclusively; the supernatural witch in particular often had nothing to do with communal conflict, but expressed tensions between the human and supernatural worlds; and in Eastern and Southeastern Europe such supernatural witches became an ideology explaining calamities that befell entire communities.\n\nBelief in witchcraft continues to be present today in some societies and accusations of witchcraft are the trigger of serious forms of violence, including murder. Such incidents are common in places such as Burkina Faso, Ghana, India, Kenya, Malawi, Nepal and Tanzania. Accusations of witchcraft are sometimes linked to personal disputes, jealousy, and conflicts between neighbors or family over land or inheritance. Witchcraft-related violence is often discussed as a serious issue in the broader context of violence against women.\n\nIn Tanzania, about 500 older women are murdered each year following accusations against them of witchcraft or of being a witch. Apart from extrajudicial violence, there is also state-sanctioned violence in some jurisdictions. For instance, in Saudi Arabia practicing witchcraft and sorcery is a crime punishable by death and the country has executed people for this crime in 2011, 2012 and 2014.\n\nChildren in some regions of the world, such as parts of Africa, are also vulnerable to violence related to witchcraft accusations. Such incidents have also occurred in immigrant communities in the UK, including the much publicized case of the murder of Victoria Climbié.\n\nModern practices identified by their practitioners as \"witchcraft\" have grown dramatically since the early 20th century. Generally portrayed as revivals of pre-Christian European ritual and spirituality, they are understood to involve varying degrees of magic, shamanism, folk medicine, spiritual healing, calling on elementals and spirits, veneration of ancient deities and archetypes, and attunement with the forces of nature.\n\nThe first Neopagan groups to publicly appear, during the 1950s and 60s, were Gerald Gardner's Bricket Wood coven and Roy Bowers' Clan of Tubal Cain. They operated as initiatory secret societies. Other individual practitioners and writers such as Paul Huson also claimed inheritance to surviving traditions of witchcraft.\n\nDuring the 20th century, interest in witchcraft in English-speaking and European countries began to increase, inspired particularly by Margaret Murray's theory of a pan-European witch-cult originally published in 1921, since discredited by further careful historical research. Interest was intensified, however, by Gerald Gardner's claim in 1954 in \"Witchcraft Today\" that a form of witchcraft still existed in England. The truth of Gardner's claim is now disputed too, with different historians offering evidence for or against the religion's existence prior to Gardner.\n\nThe Wicca that Gardner initially taught was a witchcraft religion having a lot in common with Margaret Murray's hypothetically posited cult of the 1920s. Indeed, Murray wrote an introduction to Gardner's \"Witchcraft Today\", in effect putting her stamp of approval on it. Wicca is now practised as a religion of an initiatory secret society nature with positive ethical principles, organised into autonomous covens and led by a High Priesthood. There is also a large \"Eclectic Wiccan\" movement of individuals and groups who share key Wiccan beliefs but have no initiatory connection or affiliation with traditional Wicca. Wiccan writings and ritual show borrowings from a number of sources including 19th and 20th-century ceremonial magic, the medieval grimoire known as the Key of Solomon, Aleister Crowley's Ordo Templi Orientis and pre-Christian religions. Both men and women are equally termed \"witches.\" They practice a form of duotheistic universalism.\n\nSince Gardner's death in 1964, the Wicca that he claimed he was initiated into has attracted many initiates, becoming the largest of the various witchcraft traditions in the Western world, and has influenced other Neopagan and occult movements.\n\nWiccan literature has been described as aiding the empowerment of young women through its lively portrayal of female protagonists. Part of the recent growth in Neo-Pagan religions has been attributed to the strong media presence of fictional works such as the Buffy the Vampire Slayer and Harry Potter series with their depictions of witchcraft. Widespread accessibility to related material through internet media such as chat rooms and forums is also thought to be driving this development. Wiccan beliefs are currently often found to be compatible with liberal ideals such as the Green movement, and particularly with feminism by providing young women with means for empowerment and for control of their own lives. This is the case particularly in North America due to the strong presence of feminist ideals. The 2002 study Enchanted Feminism: The Reclaiming Witches of San Francisco suggests that Wiccan religion represents the second wave of feminism that has also been redefined as a religious movement.\n\nStregheria is an Italian witchcraft religion popularised in the 1980s by Raven Grimassi, who claims that it evolved within the ancient Etruscan religion of Italian peasants who worked under the Catholic upper classes.\n\nModern Stregheria closely resembles Charles Leland's controversial late-19th-century account of a surviving Italian religion of witchcraft, worshipping the Goddess Diana, her brother Dianus/Lucifer, and their daughter Aradia. Leland's witches do not see Lucifer as the evil Satan that Christians see, but a benevolent god of the Sun and Moon).\n\nThe ritual format of contemporary Stregheria is roughly similar to that of other Neopagan witchcraft religions such as Wicca. The pentagram is the most common symbol of religious identity. Most followers celebrate a series of eight festivals equivalent to the Wiccan Wheel of the Year, though others follow the ancient Roman festivals. An emphasis is placed on ancestor worship.\n\nTraditional witchcraft is a term used to refer to a variety of contemporary forms of witchcraft. Pagan studies scholar Ethan Doyle White described it as \"a broad movement of aligned magico-religious groups who reject any relation to Gardnerianism and the wider Wiccan movement, claiming older, more \"traditional\" roots. Although typically united by a shared aesthetic rooted in European folklore, the Traditional Craft contains within its ranks a rich and varied array of occult groups, from those who follow a contemporary Pagan path that is suspiciously similar to Wicca to those who adhere to Luciferianism\". According to British Traditional Witch Michael Howard, the term refers to \"any non-Gardnerian, non-Alexandrian, non-Wiccan or pre-modern form of the Craft, especially if it has been inspired by historical forms of witchcraft and folk magic\". Another definition was offered by Daniel A. Schulke, the current Magister of the Cultus Sabbati, when he proclaimed that \"traditional witchcraft\" \"refers to a coterie of initiatory lineages of ritual magic, spellcraft and devotional mysticism\". Some forms of traditional witchcraft are the Feri Tradition, Cochrane's Craft and the Sabbatic craft.\n\nSatanism is a broad term referring to diverse beliefs that share a symbolic association with, or admiration for, Satan, who is seen as a liberating figure. While it is heir to the same historical period and pre-Enlightenment beliefs that gave rise to modern witchcraft, it is generally seen as completely separate from modern witchcraft and Wicca, and has little or no connection to them.\n\nModern witchcraft considers Satanism to be the \"dark side of Christianity\" rather than a branch of Wicca: – the character of Satan referenced in Satanism exists only in the theology of the three Abrahamic religions, and Satanism arose as, and occupies the role of, a rebellious counterpart to Christianity, in which all is permitted and the self is central. (Christianity can be characterized as having the diametrically opposite views to these.) Such beliefs become more visibly expressed in Europe after the Enlightenment, when works such as Milton's \"Paradise Lost\" were described anew by romantics who suggested that they presented the biblical Satan as an allegory representing crisis of faith, individualism, free will, wisdom and enlightenment; a few works from that time also begin to directly present Satan in a less negative light, such as \"Letters from the Earth\". The two major trends are theistic Satanism and atheistic Satanism; the former venerates Satan as a supernatural patriarchal deity, while the latter views Satan as merely a symbolic embodiment of certain human traits.\n\nOrganized groups began to emerge in the mid 20th century, including the Ophite Cultus Satanas (1948) and The Church of Satan (1966). After seeing Margaret Murray's book \"The God of the Witches\" the leader of Ophite Cultus Satanas, Herbert Arthur Sloane, said he realized that the horned god was Satan (\"Sathanas\"). Sloane also corresponded with his contemporary Gerald Gardner, founder of the Wicca religion, and implied that his views of Satan and the horned god were not necessarily in conflict with Gardner's approach. However, he did believe that, while \"gnosis\" referred to knowledge, and \"Wicca\" referred to wisdom, modern witches had fallen away from the true knowledge, and instead had begun worshipping a fertility god, a reflection of the creator god. He wrote that \"the largest existing body of witches who are true Satanists would be the Yezedees\". Sloane highly recommended the book \"The Gnostic Religion\", and sections of it were sometimes read at ceremonies. It was estimated that there were up to 100,000 Satanists worldwide by 2006, twice the number estimated in 1990. Satanistic beliefs have been largely permitted as a valid expression of religious belief in the West. For example, they were allowed in the British Royal Navy in 2004, and an appeal was considered in 2005 for religious status as a right of prisoners by the Supreme Court of the United States. Contemporary Satanism is mainly an American phenomenon, although it began to reach Eastern Europe in the 1990s around the time of the fall of the Soviet Union.\n\nLuciferianism, on the other hand, is a belief system and does not revere the devil figure or most characteristics typically affixed to Satan. Rather, Lucifer in this context is seen as one of many morning stars, a symbol of enlightenment, independence and human progression. Madeline Montalban was an English witch who adhered to a specific form of luciferianism which revolved around the veneration of Lucifer, or Lumiel, whom she considered to be a benevolent angelic being who had aided humanity's development. Within her Order, she emphasised that her followers discover their own personal relationship with the angelic beings, including Lumiel. Although initially seeming favourable to Gerald Gardner, by the mid-1960s she had become hostile towards him and his Gardnerian tradition, considering him to be \"a 'dirty old man' and sexual pervert.\" She also expressed hostility to another prominent Pagan Witch of the period, Charles Cardell, although in the 1960s became friends with the two Witches at the forefront of the Alexandrian Wiccan tradition, Alex Sanders and his wife, Maxine Sanders, who adopted some of her Luciferian angelic practices. In contemporary times luciferian witches exist within traditional witchcraft.\n\nThe belief in sorcery and its practice seem to have been widespread in the Ancient Near East and Nile Valley. It played a conspicuous role in the cultures of ancient Egypt and in Babylonia. The latter tradition included an Akkadian anti-witchcraft ritual, the Maqlû. A section from the Code of Hammurabi (about 2000 B.C.) prescribes:\n\nAccording to the New Advent Catholic Encyclopedia: \n\nThe King James Version uses the words \"witch\", \"witchcraft\", and \"witchcrafts\" to translate the Masoretic \"kāsháf\" () and (\"qésem\"); these same English terms are used to translate \"pharmakeia\" in the Greek New Testament. Verses such as and (\"Thou shalt not suffer a witch to live\") thus provided scriptural justification for Christian witch hunters in the early modern period (see Christian views on magic).\n\nThe precise meaning of the Hebrew , usually translated as \"witch\" or \"sorceress\", is uncertain. In the Septuagint, it was translated as \"pharmakeía\" or \"pharmakous\". In the 16th century, Reginald Scot, a prominent critic of the witch trials, translated , φαρμακεία, and the Vulgate's Latin equivalent \"veneficos\" as all meaning \"poisoner\", and on this basis, claimed that \"witch\" was an incorrect translation and poisoners were intended. His theory still holds some currency, but is not widely accepted, and in is listed alongside other magic practitioners who could interpret dreams: magicians, astrologers, and Chaldeans. Suggested derivations of include \"mutterer\" (from a single root) or \"herb user\" (as a compound word formed from the roots \"kash\", meaning \"herb\", and \"hapaleh\", meaning \"using\"). The Greek φαρμακεία literally means \"herbalist\" or one who uses or administers drugs, but it was used virtually synonymously with \"mageia\" and \"goeteia\" as a term for a sorcerer.\n\nThe Bible provides some evidence that these commandments against sorcery were enforced under the Hebrew kings:\nNote that the Hebrew word \"ob\", translated as \"familiar spirit\" in the above quotation, has a different meaning than the usual English sense of the phrase; namely, it refers to a spirit that the woman is familiar with, rather than to a spirit that physically manifests itself in the shape of an animal.\n\nThe New Testament condemns the practice as an abomination, just as the Old Testament had (Galatians 5:20, compared with Revelation 21:8; 22:15; and Acts 8:9; 13:6). The word in most New Testament translations is \"sorcerer\"/\"sorcery\" rather than \"witch\"/\"witchcraft\".\n\nJewish law views the practice of witchcraft as being laden with idolatry and/or necromancy; both being serious theological and practical offenses in Judaism. Although Maimonides vigorously denied the efficacy of all methods of witchcraft, and claimed that the Biblical prohibitions regarding it were precisely to wean the Israelites from practices related to idolatry. It is acknowledged that while magic exists, it is forbidden to practice it on the basis that it usually involves the worship of other gods. Rabbis of the Talmud also condemned magic when it produced something other than illusion, giving the example of two men who use magic to pick cucumbers (Sanhedrin 67a). The one who creates the illusion of picking cucumbers should not be condemned, only the one who actually picks the cucumbers through magic.\n\nHowever, some of the rabbis practiced \"magic\" themselves or taught the subject. For instance, Rabbah created a person and sent him to Rav Zeira, and Hanina and Hoshaiah studied every Friday together and created a small calf to eat on Shabbat (Sanhedrin 67b). In these cases, the \"magic\" was seen more as divine miracles (i.e., coming from God rather than \"unclean\" forces) than as witchcraft.\n\nJudaism does make it clear that Jews shall not try to learn about the ways of witches (Book of Deuteronomy 18: 9–10) and that witches are to be put to death (Exodus 22:17).\n\nJudaism's most famous reference to a medium is undoubtedly the Witch of Endor whom Saul consults, as recounted in 1 Samuel 28.\n\nDivination, and magic in Islam, encompass a wide range of practices, including black magic, warding off the evil eye, the production of amulets and other magical equipment, evocation, casting lots, and astrology. Muslims do commonly believe in magic (\"sihr\") and explicitly forbid its practice. \"Sihr\" translates from Arabic as sorcery or black magic. The best known reference to magic in Islam is surah al-Falaq of the Qur'an, which is known as a prayer to God to ward off black magic:\nAlso according to the Qur'an:\nIslam distinguishes between God-given gifts, which can heal sickness, and possession, and sorcery. Good supernatural powers are therefore a \"special gift from God\", whereas sorcery or black magic is achieved through help of jinn and demons. In the Qurʾānic narrative, the Prophet Sulayman had the power to speak with animals and command jinn, and he thanks God for this نعمة (i.e. gift, privilege, favour, bounty), which is only given to him with God’s permission. The Prophet Muhammad was accused of being a magician by his opponents.\n\nIt is a common belief that jinn can possess a human, thus requiring exorcism (\"ruqya\") derived from the Prophet's \"sunnah\" to cast off the jinn or devils from the body of the possessed. The practice of seeking help from the jinn is prohibited and can lead to possession. The \"ruqya\" contains verses of the Qur'an as well as prayers specifically targeted against demons. The knowledge of which verses of the Qur'an to use in what way is what is considered \"magic knowledge.\"\n\nA \"hadith\" recorded in states: \"Seventy thousand people of my followers will enter Paradise without accounts, and they are those who do not practice Ar-Ruqya and do not see an evil omen in things, and put their trust in their Lord.\" Ibn Qayyim al-Jawziyya, a scholar, commented on this \"hadith\", stating: That is because these people will enter Paradise without being called to account because of the perfection of their Tawheed, therefore he described them as people who did not ask others to perform ruqyah for them. Hence he said \"and they put their trust in their Lord.\" Because of their complete trust in their Lord, their contentment with Him, their faith in Him, their being pleased with Him and their seeking their needs from Him, they do not ask people for anything, be it ruqyah or anything else, and they are not influenced by omens and superstitions that could prevent them from doing what they want to do, because superstition detracts from and weakens Tawheed\".\n\nIbn al-Nadim hold, exorcists gain their power by their obedience to God, while sorcerers please the demons by acts of disobedience and sacrifices and they in return do him a favor. Being pious and strictly following the teachings of the Qur'an can increase the probability to perform magic or miracles, that is distinguished from witchcraft, the latter practised in aid with demons.\n\nA \"hadith\" recorded in narrates that one who has eaten seven Ajwa dates in the morning will not be adversely affected by magic in the course of that day.\n\nStudents of the history of religion have linked several magical practises in Islam with pre-Islamic Turkish and East African customs. Most notable of these customs is the Zār.\n\nIn Southern African traditions, there are three classifications of somebody who uses magic. The \"tagati\" is usually improperly translated into English as \"witch\", and is a spiteful person who operates in secret to harm others. The \"sangoma\" is a diviner, somewhere on a par with a fortune teller, and is employed in detecting illness, predicting a person's future (or advising them on which path to take), or identifying the guilty party in a crime. She also practices some degree of medicine. The \"inyanga\" is often translated as \"witch doctor\" (though many Southern Africans resent this implication, as it perpetuates the mistaken belief that a \"witch doctor\" is in some sense a \"practitioner\" of malicious magic). The \"inyanga\"s job is to heal illness and injury and provide customers with magical items for everyday use. Of these three categories the \"tagati\" is almost exclusively female, the \"sangoma\" is usually female, and the \"inyanga\" is almost exclusively male.\n\nMuch of what witchcraft represents in Africa has been susceptible to misunderstandings and confusion, thanks in no small part to a tendency among western scholars since the time of the now largely discredited Margaret Murray to approach the subject through a comparative lens vis-a-vis European witchcraft. Okeja argues that witchcraft in Africa today plays a very different social role than in Europe of the past—or present—and should be understood through an African rather than post-colonial Western lens.\n\nComplimentary remarks about witchcraft by a native Congolese initiate: \"From witchcraft ... may be developed the remedy (\"kimbuki\") that will do most to raise up our country.\" \"Witchcraft ... deserves respect ... it can embellish or redeem (\"ketula evo vuukisa\").\" \"The ancestors were equipped with the protective witchcraft of the clan (\"kindoki kiandundila kanda\"). ... They could also gather the power of animals into their hands ... whenever they needed. ... If we could make use of these kinds of witchcraft, our country would rapidly progress in knowledge of every kind.\" \"You witches (\"zindoki\") too, bring your science into the light to be written down so that ... the benefits in it ... endow our race.\"\n\nIn eastern Cameroon, the term used for witchcraft among the Maka is \"djambe\" and refers to a force inside a person; its powers may make the proprietor more vulnerable. It encompasses the occult, the transformative, killing and healing.\n\nIn some Central African areas, malicious magic users are believed by locals to be the source of terminal illness such as AIDS and cancer. In such cases, various methods are used to rid the person from the bewitching spirit, occasionally physical and psychological abuse. Children may be accused of being witches, for example a young niece may be blamed for the illness of a relative. Most of these cases of abuse go unreported since the members of the society that witness such abuse are too afraid of being accused of being accomplices. It is also believed that witchcraft can be transmitted to children by feeding. Parents discourage their children from interacting with people believed to be witches.\n\nEvery year, hundreds of people in the Central African Republic are convicted of witchcraft.\n\nChristian militias in the Central African Republic have also kidnapped, burnt and buried alive women accused of being 'witches' in public ceremonies.\n\n, between 25,000 and 50,000 children in Kinshasa, Democratic Republic of the Congo, had been accused of witchcraft and thrown out of their homes. These children have been subjected to often-violent abuse during exorcisms, sometimes supervised by self-styled religious pastors. Other pastors and Christian activists strongly oppose such accusations and try to rescue children from their unscrupulous colleagues. The usual term for these children is \"enfants sorciers\" (child witches) or \"enfants dits sorciers\" (children accused of witchcraft). In 2002, USAID funded the production of two short films on the subject, made in Kinshasa by journalists Angela Nicoara and Mike Ormsby.\n\nIn April 2008, in Kinshasa, the police arrested 14 suspected victims (of penis snatching) and sorcerers accused of using black magic or witchcraft to steal (make disappear) or shrink men's penises to extort cash for cure, amid a wave of panic.\n\nAccording to one study, the belief in magical warfare technologies (such as \"bulletproofing\") in the Eastern Democratic Republic of the Congo serves a group-level function, as it increases group efficiency in warfare, even if it is suboptimal at the individual level. The authors of the study argue that this is one reason why the belief in witchcraft persists.\n\nIn Ghana, women are often accused of witchcraft and attacked by neighbours. Because of this, there exist six witch camps in the country where women suspected of being witches can flee for safety. The witch camps, which exist solely in Ghana, are thought to house a total of around 1000 women. Some of the camps are thought to have been set up over 100 years ago. The Ghanaian government has announced that it intends to close the camps.\n\nArrests were made in an effort to avoid bloodshed seen in Ghana a decade ago, when 12 alleged penis snatchers were beaten to death by mobs. While it is easy for modern people to dismiss such reports, Uchenna Okeja argues that a belief system in which such magical practices are deemed possible offer many benefits to Africans who hold them. For example, the belief that a sorcerer has \"stolen\" a man's penis functions as an anxiety-reduction mechanism for men suffering from impotence while simultaneously providing an explanation that is consistent with African cultural beliefs rather than appealing to Western scientific notions that are tainted by the history of colonialism (at least for many Africans).\n\nIt was reported on May 21, 2008 that in Kenya, a mob had burnt to death at least 11 people accused of witchcraft.\n\nIn Malawi it is also common practice to accuse children of witchcraft and many children have been abandoned, abused and even killed as a result. As in other African countries both African traditional healers and their Christian counterparts are trying to make a living out of exorcising children and are actively involved in pointing out children as witches. Various secular and Christian organizations are combining their efforts to address this problem.\n\nAccording to William Kamkwamba, witches and wizards are afraid of money, which they consider a rival evil. Any contact with cash will snap their spell and leave the wizard naked and confused. So placing cash, such as kwacha around a room or bed mat will protect the resident from their malevolent spells.\n\nIn Nigeria, several Pentecostal pastors have mixed their evangelical brand of Christianity with African beliefs in witchcraft to benefit from the lucrative witch finding and exorcism business—which in the past was the exclusive domain of the so-called witch doctor or traditional healers. These pastors have been involved in the torturing and even killing of children accused of witchcraft. Over the past decade, around 15,000 children have been accused, and around 1,000 murdered. Churches are very numerous in Nigeria, and competition for congregations is hard. Some pastors attempt to establish a reputation for spiritual power by \"detecting\" child witches, usually following a death or loss of a job within a family, or an accusation of financial fraud against the pastor. In the course of \"exorcisms\", accused children may be starved, beaten, mutilated, set on fire, forced to consume acid or cement, or buried alive. While some church leaders and Christian activists have spoken out strongly against these abuses, many Nigerian churches are involved in the abuse, although church administrations deny knowledge of it.\n\nAmong the Mende (of Sierra Leone), trial and conviction for witchcraft has a beneficial effect for those convicted. \"The witchfinder had warned the whole village to ensure the relative prosperity of the accused and sentenced ... old people. ... Six months later all of the people ... accused, were secure, well-fed and arguably happier than at any [previous] time; they had hardly to beckon and people would come with food or whatever was needful. ... Instead of such old and widowed people being left helpless or (as in Western society) institutionalized in old people's homes, these were reintegrated into society and left secure in their old age ... . ... Old people are 'suitable' candidates for this kind of accusation in the sense that they are isolated and vulnerable, and they are 'suitable' candidates for 'social security' for precisely the same reasons.\"\n\nIn Kuranko language, the term for witchcraft is \"suwa'ye\" referring to \"extraordinary powers\".\n\nIn Tanzania in 2008, President Kikwete publicly condemned witchdoctors for killing albinos for their body parts, which are thought to bring good luck. 25 albinos have been murdered since March 2007. In Tanzania, albinos are often murdered for their body parts on the advice of witch doctors in order to produce powerful amulets that are believed to protect against witchcraft and make the owner prosper in life.\n\nBrua is an Afro-Caribbean religion and healing tradition that originates in Aruba, Bonaire, and Curaçao, in the Dutch Caribbean. A healer in this culture is called a \"kurioso\" or \"kuradó\", a man or woman who performs \"trabou chikí\" (little works) and \"trabou grandi\" (large treatments) to promote or restore health, bring fortune or misfortune, deal with unrequited love, and more serious concerns, in which sorcery is involved. Sorcery usually involves reference to the \"almasola\" or \"homber chiki\", a devil-like entity. \"Transcultural Psychiatry\" published a paper called \"Traditional healing practices originating in Aruba, Bonaire, and Curaçao: A review of the literature on psychiatry and Brua\" by Jan Dirk Blom, Igmar T. Poulina, Trevor L. van Gellecum and Hans W. Hoek of the Parnassia Psychiatric Institute.\n\nIn 1645, Springfield, Massachusetts, experienced America's first accusations of witchcraft when husband and wife Hugh and Mary Parsons accused each other of witchcraft. At America's first witch trial, Hugh was found innocent, while Mary was acquitted of witchcraft but sentenced to be hanged for the death of her child. She died in prison. From 1645–1663, about eighty people throughout England's Massachusetts Bay Colony were accused of practicing witchcraft. Thirteen women and two men were executed in a witch-hunt that lasted throughout New England from 1645–1663.\n\nThe Salem witch trials followed in 1692–93. These witch trials were the most famous in British North America and took place in the coastal settlements near Salem, Massachusetts. Prior to the witch trials, nearly 300 men and women had been suspected of partaking in witchcraft, and 19 of these people were hanged, and one was “pressed to death”. The Salem witch trials were a series of hearings before local magistrates followed by county court trials to prosecute people accused of witchcraft in Essex, Suffolk and Middlesex Counties of colonial Massachusetts, between February 1692 and May 1693. Over 150 people were arrested and imprisoned, with even more accused who were not formally pursued by the authorities. The two courts convicted 29 people of the capital felony of witchcraft. Nineteen of the accused, 14 women and 5 men, were hanged. One man who refused to enter a plea was crushed to death under heavy stones in an attempt to force him to do so. At least five more of the accused died in prison.\n\nDespite being generally known as the \"Salem\" witch trials, the preliminary hearings in 1692 were conducted in a variety of towns across the province: Salem Village (now Danvers), Salem Town, Ipswich, and Andover. The best known trials were conducted by the Court of Oyer and Terminer in 1692 in Salem Town. All 26 who went to trial before this court were convicted. The four sessions of the Superior Court of Judicature in 1693, held in Salem Town, but also in Ipswich, Boston, and Charlestown, produced only 3 convictions in the 31 witchcraft trials it conducted. Likewise, alleged witchcraft was not isolated to New England. In 1706 Grace Sherwood the \"Witch of Pungo\" was imprisoned for the crime in Princess Anne County, Virginia.\n\nIn Maryland, there is a legend of Moll Dyer, who escaped a fire set by fellow colonists only to die of exposure in December 1697. The historical record of Dyer is scant as all official records were burned in a courthouse fire, though the county courthouse has on display the rock where her frozen body was found. A letter from a colonist of the period describes her in most unfavourable terms. A local road is named after Dyer, where her homestead was said to have been. Many local families have their own version of the Moll Dyer affair, and her name is spoken with care in the rural southern counties.\n\nAccusations of witchcraft and wizardry led to the prosecution of a man in Tennessee as recently as 1833. \"The Crucible\" by Arthur Miller is a dramatized and partially fictionalized story of the Salem witch trials that took place in the Massachusetts Bay Colony during 1692–93.\n\nIn Diné culture, witches are seen as the polar opposite of ceremonial people. While spiritual leaders perform \"sings\" for healing, protection and other beneficial purposes, all practices referred to as \"witchcraft\" are intended to hurt and curse. Witches are associated with harm to the community and transgression of societal standards, especially those relating to family and the dead.\n\nThe \"yee naaldlooshii\" is the type of witch known in English as a \"skin-walker\". They are believed to take the forms of animals in order to travel in secret and do harm to the innocent. In the Navajo language, ' translates to \"with it, he goes on all fours\". While perhaps the most common variety seen in horror fiction by non-Navajo people, the ' is one of several varieties of Navajo witch, specifically a type of \"\".\n\nCorpse powder or corpse poison (, literally \"witchery\" or \"harming\") is a substance made from powdered corpses. The powder is used by witches to curse their victims. The effect of the \"\" is a curse and disease, usually indicated by an immediate action to administration of the poison, like fainting, swelling of the tongue, or lockjaw. Sometimes, however, the victims simply wastes away, as from a normal disease.\n\nTraditional Navajos usually hesitate to discuss things like witches and witchcraft with non-Navajos.\n\nWitchcraft was also an important part of the social and cultural history of late-Colonial Mexico, during the Mexican Inquisition. Spanish Inquisitors viewed witchcraft as a problem that could be cured simply through confession. Yet, as anthropologist Ruth Behar writes, witchcraft, not only in Mexico but in Latin America in general, was a \"conjecture of sexuality, witchcraft, and religion, in which Spanish, indigenous, and African cultures converged.\" Furthermore, witchcraft in Mexico generally required an interethnic and interclass network of witches. Yet, according to anthropology professor Laura Lewis, witchcraft in colonial Mexico ultimately represented an \"affirmation of hegemony\" for women, Indians, and especially Indian women over their white male counterparts as a result of the casta system.\n\nIn modern history, notoriety has been awarded to a place called Catemaco, in the state of Veracruz, which has a history of witchcraft, and where the practice of witchcraft by contemporary \"brujos\" and \"brujas\" thrives.\n\nIn Mexico City, people who practice brujería, Santería, voodoo, ocultism and magic may find items, herbs and supplies at the \"mercado de Sonora\".\n\nIn Chile there is a tradition of the Kalku in Mapuche religion; and the Warlocks of Chiloé in the folklore and Chilote mythology.\n\nThe presence of the witch is a constant in the ethnographic history of colonial Brazil, especially during the several denunciations and confessions given to the Congregation for the Doctrine of the Faith of Bahia (1591–1593), Pernambuco and Paraíba (1593–1595).\n\nBelief in the supernatural is strong in all parts of India, and lynchings for witchcraft are reported in the press from time to time. Around 750 people were killed as witches in Assam and West Bengal between 2003 and 2008. Officials in the state of Chhattisgarh reported in 2008 that at least 100 women are maltreated annually as suspected witches. A local activist stated that only a fraction of cases of abuse are reported. In Indian mythology, a common perception of a witch is a being with her feet pointed backwards.\n\nApart from other types of Violence against women in Nepal, the malpractice of abusing women in the name of witchcraft is also really prominent. According to the statistics in 2013, there was a total of 69 reported cases of abuse to women due to accusation of performing witchcraft. The perpetrators of this malpractice are usually neighbors, so-called witch doctors and family members. The main causes of these malpractices are lack of education, lack of awareness and superstition. According to the statistics by INSEC, the age group of women who fall victims to the witchcraft violence in Nepal is 20–40.\n\nIn Japanese folklore, the most common types of witch can be separated into two categories: those who employ snakes as familiars, and those who employ foxes.\n\nThe fox witch is, by far, the most commonly seen witch figure in Japan. Differing regional beliefs set those who use foxes into two separate types: the \"kitsune-mochi\", and the \"tsukimono-suji\". The first of these, the \"kitsune-mochi\", is a solitary figure who gains his fox familiar by bribing it with its favourite foods. The \"kitsune-mochi\" then strikes up a deal with the fox, typically promising food and daily care in return for the fox's magical services. The fox of Japanese folklore is a powerful trickster in and of itself, imbued with powers of shape changing, possession, and illusion. These creatures can be either nefarious; disguising themselves as women in order to trap men, or they can be benign forces as in the story of \"The Grateful foxes\". However, once a fox enters the employ of a human it almost exclusively becomes a force of evil to be feared. A fox under the employ of a human can provide many services. The fox can turn invisible and find secrets its master desires. It can apply its many powers of illusion to trick and deceive its master's enemies. The most feared power of the \"kitsune-mochi\" is the ability to command his fox to possess other humans. This process of possession is called Kitsunetsuki.\n\nBy far, the most commonly reported cases of fox witchcraft in modern Japan are enacted by \"tsukimono-suji\" families, or \"hereditary witches\". The \"Tsukimono-suji\" is traditionally a family who is reported to have foxes under their employ. These foxes serve the family and are passed down through the generations, typically through the female line. \"Tsukimono-suji\" foxes are able to supply much in the way of the same mystical aid that the foxes under the employ of a \"kitsune-mochi\" can provide its more solitary master with. In addition to these powers, if the foxes are kept happy and well taken care of, they bring great fortune and prosperity to the \"Tsukimono-suji\" house. However, the aid in which these foxes give is often overshadowed by the social and mystical implications of being a member of such a family. In many villages, the status of local families as \"tsukimono-suji\" is often common, everyday knowledge. Such families are respected and feared, but are also openly shunned. Due to its hereditary nature, the status of being \"Tsukimono-suji\" is considered contagious. Because of this, it is often impossible for members of such a family to sell land or other properties, due to fear that the possession of such items will cause foxes to inundate one's own home. In addition to this, because the foxes are believed to be passed down through the female line, it is often nearly impossible for women of such families to find a husband whose family will agree to have him married to a \"tsukimono-suji\" family. In such a union the woman's status as a \"Tsukimono-suji\" would transfer to any man who married her.\n\nWitchcraft in the Philippines is often classified as malevolent, with practitioners of black magic called \"Mangkukulam\" in Tagalog and \"Mambabarang\" in Cebuano; there are also practitioners of benevolent, white magic, in addition to some who practise both. \"Mambabarang\" in particular are noted for their ability to command insects and other invertebrates to accomplish a task, such as delivering a curse to a target.\n\nMagic and witchcraft in the Philippines varies considerably across the different ethnic groups, and is commonly a modern manifestation of pre-Colonial spirituality interwoven with Catholic religious elements such as the invocation of saints and the use of pseudo-Latin prayers (\"oración\") in spells, and \"anting-anting\" (amulets).\n\nPractitioners of traditional herbal-based medicine and divination called \"albularyo\" are not considered witches. They are perceived to be either quack doctors or a quasi-magical option when western medicine fails to identify or cure an ailment that is thus suspected to be of supernatural, often malevolent, origin. Feng shui, an influence of Filipino Chinese culture, is also not classified as witchcraft as it is considered a separate realm of belief altogether.\n\nSaudi Arabia continues to use the death penalty for sorcery and witchcraft. In 2006 Fawza Falih Muhammad Ali was condemned to death for practicing witchcraft. There is no legal definition of sorcery in Saudi, but in 2007 an Egyptian pharmacist working there was accused, convicted, and executed. Saudi authorities also pronounced the death penalty on a Lebanese television presenter, Ali Hussain Sibat, while he was performing the \"hajj\" (Islamic pilgrimage) in the country.\n\nIn 2009 the Saudi authorities set up the Anti-Witchcraft Unit of their Committee for the Promotion of Virtue and the Prevention of Vice police.\n\nIn April 2009, a Saudi woman Amina Bint Abdulhalim Nassar was arrested and later sentenced to death for practicing witchcraft and sorcery. In December 2011, she was beheaded. A Saudi man has been beheaded on charges of sorcery and witchcraft in June 2012. A beheading for sorcery occurred in 2014.\n\nIn June 2015, Yahoo reported: \"The Islamic State group has beheaded two women in Syria on accusations of \"sorcery,\" the first such executions of female civilians in Syria, the Syrian Observatory for Human Rights said Tuesday.\"\nISIS decapitated a man in Iraq over sorcery.\n\nAn expedition sent to what is now the Xinjiang region of western China by the PBS documentary series \"Nova\" found a fully clothed female Tocharian mummy wearing a black conical hat of the type now associated with witches in Europe in the storage area of a small local museum, indicative of an Indo-European priestess.\n\nWitchcraft in Europe between 500–1750 was believed to be a combination of sorcery and heresy. While sorcery attempts to produce negative supernatural effects through formulas and rituals, heresy is the Christian contribution to witchcraft in which an individual makes a pact with the Devil. In addition, heresy denies witches the recognition of important Christian values such as baptism, salvation, Christ and sacraments. The beginning of the witch accusations in Europe took place in the 14th and 15th centuries; however as the social disruptions of the 16th century took place, witchcraft trials intensified. \nIn Early Modern European tradition, witches were stereotypically, though not exclusively, women. European pagan belief in witchcraft was associated with the goddess Diana and dismissed as \"diabolical fantasies\" by medieval Christian authors. Witch-hunts first appeared in large numbers in southern France and Switzerland during the 14th and 15th centuries. The peak years of witch-hunts in southwest Germany were from 1561 to 1670.\n\nIt was commonly believed that individuals with power and prestige were involved in acts of witchcraft and even cannibalism. Because Europe had a lot of power over individuals living in West Africa, Europeans in positions of power were often accused of taking part in these practices. Though it is not likely that these individuals were actually involved in these practices, they were most likely associated due to Europe’s involvement in things like the slave trade, which negatively affected the lives of many individuals in the Atlantic World throughout the fifteenth through seventeenth centuries.\n\nThe familiar witch of folklore and popular superstition is a combination of numerous influences. The characterization of the witch as an evil magic user developed over time.\n\nEarly converts to Christianity looked to Christian clergy to work magic more effectively than the old methods under Roman paganism, and Christianity provided a methodology involving saints and relics, similar to the gods and amulets of the Pagan world. As Christianity became the dominant religion in Europe, its concern with magic lessened.\n\nThe Protestant Christian explanation for witchcraft, such as those typified in the confessions of the Pendle witches, commonly involves a diabolical pact or at least an appeal to the intervention of the spirits of evil. The witches or wizards engaged in such practices were alleged to reject Jesus and the sacraments; observe \"the witches' sabbath\" (performing infernal rites that often parodied the Mass or other sacraments of the Church); pay Divine honour to the Prince of Darkness; and, in return, receive from him preternatural powers. It was a folkloric belief that a Devil's Mark, like the brand on cattle, was placed upon a witch's skin by the devil to signify that this pact had been made. Witches were most often characterized as women. Witches disrupted the societal institutions, and more specifically, marriage. It was believed that a witch often joined a pact with the devil to gain powers to deal with infertility, immense fear for her children's well-being, or revenge against a lover. They were also depicted as lustful and perverted, and it was thought that they copulated with the devil at the Sabbath.\n\nThe Church and European society were not always so zealous in hunting witches or blaming them for misfortunes. Saint Boniface declared in the 8th century that belief in the existence of witches was un-Christian. The emperor Charlemagne decreed that the burning of supposed witches was a pagan custom that would be punished by the death penalty. In 820 the Bishop of Lyon and others repudiated the belief that witches could make bad weather, fly in the night, and change their shape. This denial was accepted into Canon law . Other rulers such as King Coloman of Hungary declared that witch-hunts should cease because witches (more specifically, strigas) do not exist.\n\nThe Church did not invent the idea of witchcraft as a potentially harmful force whose practitioners should be put to death. This idea is commonplace in pre-Christian religions. According to the scholar Max Dashu, the concept of medieval witchcraft contained many of its elements even before the emergence of Christianity. These can be found in Bacchanalias, especially in the time when they were led by priestess Paculla Annia (188BC–186BC).\n\nPowers typically attributed to European witches include turning food poisonous or inedible, flying on broomsticks or pitchforks, casting spells, cursing people, making livestock ill and crops fail, and creating fear and local chaos.\n\nHowever, even at a later date, not all witches were assumed to be harmful practicers of the craft. In England, the provision of this curative magic was the job of a witch doctor, also known as a cunning man, white witch, or wise man. The term \"witch doctor\" was in use in England before it came to be associated with Africa. Toad doctors were also credited with the ability to undo evil witchcraft. (Other folk magicians had their own purviews. Girdle-measurers specialised in diagnosing ailments caused by fairies, while magical cures for more mundane ailments, such as burns or toothache, could be had from charmers.)\n\nHistorians Keith Thomas and his student Alan Macfarlane study witchcraft by combining historical research with concepts drawn from anthropology. They argued that English witchcraft, like African witchcraft, was endemic rather than epidemic. Older women were the favorite targets because they were marginal, dependent members of the community and therefore more likely to arouse feelings of both hostility and guilt, and less likely to have defenders of importance inside the community. Witchcraft accusations were the village's reaction to the breakdown of its internal community, coupled with the emergence of a newer set of values that was generating psychic stress.\nIn Wales, fear of witchcraft mounted around the year 1500. There was a growing alarm of women's magic as a weapon aimed against the state and church. The Church made greater efforts to enforce the canon law of marriage, especially in Wales where tradition allowed a wider range of sexual partnerships. There was a political dimension as well, as accusations of witchcraft were levied against the enemies of Henry VII, who was exerting more and more control over Wales.\n\nThe records of the Courts of Great Sessions for Wales, 1536–1736 show that Welsh custom was more important than English law. Custom provided a framework of responding to witches and witchcraft in such a way that interpersonal and communal harmony was maintained, Showing to regard to the importance of honour, social place and cultural status. Even when found guilty, execution did not occur.\n\nBecoming king in 1603, James I Brought to England and Scotland continental explanations of witchcraft. His goal was to divert suspicion away from male homosociality among the elite, and focus fear on female communities and large gatherings of women. He thought they threatened his political power so he laid the foundation for witchcraft and occultism policies, especially in Scotland. The point was that a widespread belief in the conspiracy of witches and a witches' Sabbath with the devil deprived women of political influence. Occult power was supposedly a womanly trait because women were weaker and more susceptible to the devil.\n\nIn 1944 Helen Duncan was the last person in Britain to be imprisoned for fraudulently claiming to be a witch.\n\nIn the United Kingdom children believed to be witches or seen as possessed by evil spirits can be subject to severe beatings, traumatic exorcism, and/or other abuse. There have even been child murders associated with witchcraft beliefs. The problem is particularly serious among immigrant or former immigrant communities of African origin but other communities, such as those of Asian origin are also involved. Step children and children seen as different for a wide range of reasons are particularly at risk of witchcraft accusations. Children may be beaten or have chilli rubbed into their eyes during exorcisms. This type of abuse is frequently hidden and can include torture. A 2006 recommendation to record abuse cases linked to witchcraft centrally has not yet been implemented. Lack of awareness among social workers, teachers and other professionals dealing with at risk children hinders efforts to combat the problem.\n\nThere is a 'money making scam' involved. Pastors accuse a child of being a witch and later the family pays for exorcism. If a child at school says that his/her pastor called the child a witch that should become a child safeguarding issue.\n\nAs in most European countries, women in Italy were more likely suspected of witchcraft than men. Women were considered dangerous due to their supposed sexual instability, such as when being aroused, and also due to the powers of their menstrual blood.\n\nIn the 16th century, Italy had a high portion of witchcraft trials involving love magic. The country had a large number of unmarried people due to men marrying later in their lives during this time. This left many women on a desperate quest for marriage leaving them vulnerable to the accusation of witchcraft whether they took part in it or not. Trial records from the Inquisition and secular courts discovered a link between prostitutes and supernatural practices. Professional prostitutes were considered experts in love and therefore knew how to make love potions and cast love related spells. Up until 1630, the majority of women accused of witchcraft were prostitutes. A courtesan was questioned about her use of magic due to her relationship with men of power in Italy and her wealth. The majority of women accused were also considered \"outsiders\" because they were poor, had different religious practices, spoke a different language, or simply from a different city/town/region. Cassandra from Ferrara, Italy, was still considered a foreigner because not native to Rome where she was residing. She was also not seen as a model citizen because her husband was in Venice.\n\nFrom the 16th-18th centuries, the Catholic Church enforced moral discipline throughout Italy. With the help of local tribunals, such as in Venice, the two institutions investigated a woman's religious behaviors when she was accused of witchcraft.\n\nFranciscan friars from New Spain introduced Diabolism, belief in the devil, to the indigenous people after their arrival in 1524.\nBartolomé de las Casas believed that human sacrifice was not diabolic, in fact far off from it, and was a natural result of religious expression.\nMexican Indians gladly took in the belief of Diabolism and still managed to keep their belief in creator-destroyer deities.\n\nIn pre-Christian times, witchcraft was a common practice in the Cook Islands. The native name for a sorcerer was \"tangata purepure\" (a man who prays). The prayers offered by the \"ta'unga\" (priests) to the gods worshiped on national or tribal \"marae\" (temples) were termed \"karakia\"; those on minor occasions to the lesser gods were named \"pure\". All these prayers were metrical, and were handed down from generation to generation with the utmost care. There were prayers for every such phase in life; for success in battle; for a change in wind (to overwhelm an adversary at sea, or that an intended voyage be propitious); that his crops may grow; to curse a thief; or wish ill-luck and death to his foes. Few men of middle age were without a number of these prayers or charms. The succession of a sorcerer was from father to son, or from uncle to nephew. So too of sorceresses: it would be from mother to daughter, or from aunt to niece. Sorcerers and sorceresses were often slain by relatives of their supposed victims.\n\nA singular enchantment was employed to kill off a husband of a pretty woman desired by someone else. The expanded flower of a Gardenia was stuck upright—a very difficult performance—in a cup (i.e., half a large coconut shell) of water. A prayer was then offered for the husbands speedy death, the sorcerer earnestly watching the flower. Should it fall the incantation was successful. But if the flower still remained upright, he will live. The sorcerer would in that case try his skill another day, with perhaps better success.\n\nAccording to Beatrice Grimshaw, a journalist who visited the Cook Islands in 1907, the uncrowned Queen Makea was believed to have possessed the mystic power called \"mana\", giving the possessor the power to slay at will. It also included other gifts, such as second sight to a certain extent, the power to bring good or evil luck, and the ability already mentioned to deal death at will.\n\nA local newspaper informed that more than 50 people were killed in two Highlands provinces of Papua New Guinea in 2008 for allegedly practicing witchcraft. An estimated 50–150 alleged witches are killed each year in Papua New Guinea.\n\nAmong the Russian words for \"witch\", ведьма (ved'ma) literally means \"one who knows\", from Old Slavic вѣдъ \"to know\"). Another frequent term is колдунья (koldun'ya), \"sorcerer\" being колдун (koldun).\n\nPagan practices formed a part of Russian and Eastern Slavic culture; the Russian people were deeply superstitious. The witchcraft practiced consisted mostly of earth magic and herbology; it was not so significant which herbs were used in practices, but how these herbs were gathered. Ritual centered on harvest of the crops and the location of the sun was very important. One source, pagan author Judika Illes, tells that herbs picked on Midsummer's Eve were believed to be most powerful, especially if gathered on Bald Mountain near Kiev during the witches' annual revels celebration. Botanicals should be gathered, \"During the seventeenth minute of the fourteenth hour, under a dark moon, in the thirteenth field, wearing a red dress, pick the twelfth flower on the right.\"\n\nSpells also served for midwifery, shape-shifting, keeping lovers faithful, and bridal customs. Spells dealing with midwifery and childbirth focused on the spiritual wellbeing of the baby. Shape-shifting spells involved invocation of the wolf as a spirit animal. To keep men faithful, lovers would cut a ribbon the length of his erect penis and soak it in his seminal emissions after sex while he was sleeping, then tie seven knots in it; keeping this talisman of knot magic ensured loyalty. Part of an ancient pagan marriage tradition involved the bride taking a ritual bath at a bathhouse before the ceremony. Her sweat would be wiped from her body using raw fish, and the fish would be cooked and fed to the groom.\n\nDemonism, or black magic, was not prevalent. Persecution for witchcraft, mostly involved the practice of simple earth magic, founded on herbology, by solitary practitioners with a Christian influence. In one case investigators found a locked box containing something bundled in a kerchief and three paper packets, wrapped and tied, containing crushed grasses. Most rituals of witchcraft were very simple—one spell of divination consists of sitting alone outside meditating, asking the earth to show one's fate.\n\nWhile these customs were unique to Russian culture, they were not exclusive to this region. Russian pagan practices were often akin to paganism in other parts of the world. The Chinese concept of \"chi\", a form of energy that often manipulated in witchcraft, is known as bioplasma in Russian practices. The western concept of an \"evil eye\" or a \"hex\" was translated to Russia as a \"spoiler\". A spoiler was rooted in envy, jealousy and malice. Spoilers could be made by gathering bone from a cemetery, a knot of the target's hair, burned wooden splinters and several herb Paris berries (which are very poisonous). Placing these items in sachet in the victim's pillow completes a spoiler. The Sumerians, Babylonians, Assyrians, and the ancient Egyptians recognized the evil eye from as early as 3,000 BCE; in Russian practices it is seen as a sixteenth-century concept.\n\nThe dominant societal concern those practicing witchcraft was not whether paganism was effective, but whether it could cause harm. Peasants in Russian and Ukrainian societies often shunned witchcraft, unless they needed help against supernatural forces. Impotence, stomach pains, barrenness, hernias, abscesses, epileptic seizures, and convulsions were all attributed to evil (or witchcraft). This is reflected in linguistics; there are numerous words for a variety of practitioners of paganism-based healers. Russian peasants referred to a witch as a \"chernoknizhnik\" (a person who plied his trade with the aid of a black book), \"sheptun\"/\"sheptun'ia\" (a \"whisperer\" male or female), \"lekar\"/\"lekarka\" or \"znakhar\"/\"znakharka\" (a male or female healer), or \"zagovornik\" (an incanter).\n\nIronically enough, there was universal reliance on folk healers – but clients often turned them in if something went wrong. According to Russian historian Valerie A. Kivelson, witchcraft accusations were normally thrown at lower-class peasants, townspeople and Cossacks. People turned to witchcraft as a means to support themselves. The ratio of male to female accusations was 75% to 25%. Males were targeted more, because witchcraft was associated with societal deviation. Because single people with no settled home could not be taxed, males typically had more power than women in their dissent.\n\nThe history of Witchcraft had evolved around society. More of a psychological concept to the creation and usage of Witchcraft can create the assumption as to why women are more likely to follow the practices behind Witchcraft. Identifying with the soul of an individual’s self is often deemed as \"feminine\" in society. There is analyzed social and economic evidence to associate between witchcraft and women.\n\nWitchcraft trials occurred frequently in seventeenth-century Russia, although the \"great witch-hunt\" is believed to be a predominately Western European phenomenon. However, as the witchcraft-trial craze swept across Catholic and Protestant countries during this time, Orthodox Christian Europe indeed partook in this so-called \"witch hysteria.\" This involved the persecution of both males and females who were believed to be practicing paganism, herbology, the black art, or a form of sorcery within and/or outside their community. Very early on witchcraft legally fell under the jurisdiction of the ecclesiastical body, the church, in Kievan Rus' and Muscovite Russia. Sources of ecclesiastical witchcraft jurisdiction date back as early as the second half of the eleventh century, one being Vladimir the Great's first edition of his State Statute or \"Ustav\", another being multiple references in the \"Primary Chronicle\" beginning in 1024.\n\nThe sentence for an individual found guilty of witchcraft or sorcery during this time, and in previous centuries, typically included either burning at the stake or being tested with the \"ordeal of cold water\" or \"judicium aquae frigidae\". The cold-water test was primarily a Western European phenomenon, but was used as a method of truth in Russia prior to, and post, seventeenth-century witchcraft trials in Muscovy. Accused persons who submerged were considered innocent, and ecclesiastical authorities would proclaim them \"brought back,\" but those who floated were considered guilty of practicing witchcraft, and burned at the stake or executed in an unholy fashion. The thirteenth-century bishop of Vladimir, Serapion Vladimirskii, preached sermons throughout the Muscovite countryside, and in one particular sermon revealed that burning was the usual punishment for witchcraft, but more often the cold water test was used as a precursor to execution.\n\nAlthough these two methods of torture were used in the west and the east, Russia implemented a system of fines payable for the crime of witchcraft during the seventeenth century. Thus, even though torture methods in Muscovy were on a similar level of harshness as Western European methods used, a more civil method was present. In the introduction of a collection of trial records pieced together by Russian scholar Nikolai Novombergsk, he argues that Muscovite authorities used the same degree of cruelty and harshness as Western European Catholic and Protestant countries in persecuting witches. By the mid-sixteenth century the manifestations of paganism, including witchcraft, and the black arts—astrology, fortune telling, and divination—became a serious concern to the Muscovite church and state.\n\nTsar Ivan IV (reigned 1547–1584) took this matter to the ecclesiastical court and was immediately advised that individuals practicing these forms of witchcraft should be excommunicated and given the death penalty. Ivan IV, as a true believer in witchcraft, was deeply convinced that sorcery accounted for the death of his wife, Anastasiia in 1560, which completely devastated and depressed him, leaving him heartbroken. Stemming from this belief, Ivan IV became majorly concerned with the threat of witchcraft harming his family, and feared he was in danger. So, during the Oprichnina (1565–1572), Ivan IV succeeded in accusing and charging a good number of boyars with witchcraft whom he did not wish to remain as nobles. Rulers after Ivan IV, specifically during the Time of Troubles (1598–1613), increased the fear of witchcraft among themselves and entire royal families, which then led to further preoccupation with the fear of prominent Muscovite witchcraft circles.\n\nAfter the Time of Troubles, seventeenth-century Muscovite rulers held frequent investigations of witchcraft within their households, laying the ground, along with previous tsarist reforms, for widespread witchcraft trials throughout the Muscovite state. Between 1622 and 1700 ninety-one people were brought to trial in Muscovite courts for witchcraft. Although Russia did partake in the witch craze that swept across Western Europe, the Muscovite state did not persecute nearly as many people for witchcraft, let alone execute a number of individuals anywhere close to the number executed in the west during the witch hysteria.\n\nWitches have a long history of being depicted in art, although most of their earliest artistic depictions seem to originate in Early Modern Europe, particularly the Medieval and Renaissance periods. Many scholars attribute their manifestation in art as inspired by texts such as \"Canon Episcopi\", a demonology-centered work of literature, and \"Malleus Maleficarum\", a \"witch-craze\" manual published in 1487, by Heinrich Kramer and Jacob Sprenger.\n\n\"Canon Episcopi\", a ninth-century text that explored the subject of demonology, initially introduced concepts that would continuously be associated with witches, such as their ability to fly or their believed fornication and sexual relations with the devil. The text refers to two women, Diana the Huntress and Herodias, who both express the duality of female sorcerers. Diana was described as having a heavenly body and as the \"protectress of childbirth and fertility\" while Herodias symbolized \"unbridled sensuality\". They thus represent the mental powers and cunning sexuality that witches used as weapons to trick men into performing sinful acts which would result in their eternal punishment. These characteristics were distinguished as Medusa-like or Lamia-like traits when seen in any artwork (Medusa's mental trickery was associated with Diana the Huntress's psychic powers and Lamia was a rumored female figure in the Medieval ages sometimes used in place of Herodias).\nOne of the first individuals to regularly depict witches after the witch-craze of the medieval period was Albrecht Dürer, a German Renaissance artist. His famous 1497 engraving \"The Four Witches\", portrays four physically attractive and seductive nude witches. Their supernatural identities are emphasized by the skulls and bones lying at their feet as well as the devil discreetly peering at them from their left. The women's sensuous presentation speaks to the overtly sexual nature they were attached to in early modern Europe. Moreover, this attractiveness was perceived as a danger to ordinary men who they could seduce and tempt into their sinful world. Some scholars interpret this piece as utilizing the logic of the \"Canon Episcopi\", in which women used their mental powers and bodily seduction to enslave and lead men onto a path of eternal damnation, differing from the unattractive depiction of witches that would follow in later Renaissance years.\nDürer also employed other ideas from the Middle Ages that were commonly associated with witches. Specifically, his art often referred to former 12th- to 13th-century Medieval iconography addressing the nature of female sorcerers. In the Medieval period, there was a widespread fear of witches, accordingly producing an association of dark, intimidating characteristics with witches, such as cannibalism (witches described as \"[sucking] the blood of newborn infants\") or described as having the ability to fly, usually on the back of black goats. As the Renaissance period began, these concepts of witchcraft were suppressed, leading to a drastic change in the sorceress' appearances, from sexually explicit beings to the 'ordinary' typical housewives of this time period. This depiction, known as the 'Waldensian' witch became a cultural phenomenon of early Renaissance art. The term originates from the 12th-century monk Peter Waldo, who established his own religious sect which explicitly opposed the luxury and commodity-influenced lifestyle of the Christian church clergy, and whose sect was excommunicated before being persecuted as \"practitioners of witchcraft and magic\".\n\nSubsequent artwork exhibiting witches tended to consistently rely on cultural stereotypes about these women. These stereotypes were usually rooted in early Renaissance religious discourse, specifically the Christian belief that an \"earthly alliance\" had taken place between Satan's female minions who \"conspired to destroy Christendom\".\n\nAnother significant artist whose art consistently depicted witches was Dürer's apprentice, Hans Baldung Grien, a 15th-century German artist. His chiaroscuro woodcut, \"Witches\", created in 1510, visually encompassed all the characteristics that were regularly assigned to witches during the Renaissance. Social beliefs labeled witches as supernatural beings capable of doing great harm, possessing the ability to fly, and as cannibalistic. The urn in \"Witches\" seems to contain pieces of the human body, which the witches are seen consuming as a source of energy. Meanwhile, their nudity while feasting is recognized as an allusion to their sexual appetite, and some scholars read the witch riding on the back of a goat-demon as representative of their \"flight-inducing [powers]\". This connection between women's sexual nature and sins was thematic in the pieces of many Renaissance artists, especially Christian artists, due to cultural beliefs which characterized women as overtly sexual beings who were less capable (in comparison to men) of resisting sinful temptation.\n\n\n\n"}
