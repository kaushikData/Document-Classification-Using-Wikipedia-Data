{"id": "34960688", "url": "https://en.wikipedia.org/wiki?curid=34960688", "title": "Advesha", "text": "Advesha\n\nAdvesha (Sanskrit; Pali: \"adosa\"; Tibetan Wylie: \"zhes sdang med pa\") is a Buddhist term translated as \"non-aggression\" or \"non-hatred\". It is defined as the absence of an aggressive attitude towards someone or something that causes pain. It is one of the mental factors within the Abhidharma teachings. \n\nThe Abhidharma-samuccaya states: \n\n\n\n"}
{"id": "699441", "url": "https://en.wikipedia.org/wiki?curid=699441", "title": "Avidya (Hinduism)", "text": "Avidya (Hinduism)\n\nAvidyā is a Sanskrit word whose literal meaning is ignorance, misconceptions, misunderstandings, incorrect knowledge, and it is the opposite of \"Vidya\". It is used extensively in Hindu texts, including the Upanishads, and in other Indian religions such as Buddhism and Jainism, particularly in the context of metaphysical reality.\n\nAvidyā, in all Dharmic systems, represents fundamental ignorance and misperception of the phenomenal world. However, the Indian religions disagree on the details, for example with Hinduism considering a denial and misconceptions of Atman (soul, self) as a form of Avidya, and Buddhism considering the denial and misconceptions of An-atman (non-soul, non-self) as a form of Avidya.\n\n\"Avidyā\" (अविद्या) is a Vedic Sanskrit word, and is a compound of \"a\" and \"vidya\", meaning \"not vidya\". The word \"vidya\" is derived from the Sanskrit root \"Vid\", which means \"to know, to perceive, to see, to understand\". Therefore, \"avidya\" means to \"not know, not perceive, not understand\". The \"Vid*\"-related terms appears extensively in the Rigveda and other Vedas. Avidya is usually rendered as \"ignorance\" in English translations of ancient Indian texts, sometimes as \"spiritual ignorance\".\n\nThe word avidyā is derived from the Proto-Indo-European root *\"weid\"-, meaning \"to see\" or \"to know\". It is a cognate of Latin \"vidēre\" (which would turn to \"video\") and English \"wit\".\n\nWhile Avidya found in Indian philosophies is translated as \"ignorance\", states Alex Wayman, this is a mistranslation because Avidya means more than ignorance. He suggests the term \"unwisdom\" to be a better rendition. The term includes not only ignorance out of darkness, but also obscuration, misconceptions, mistaking illusion to be reality or impermanent to be permanent or suffering to be bliss or non-self to be self (delusions). Incorrect knowledge is another form of Avidya, states Wayman.\n\nAvidya represents fundamental ignorance, state Jones and Ryan, a misperception of the phenomenal world. In Hinduism, Avidya includes confusing the mundane reality to be the only reality, and it as a permanent though it is ever changing. Its doctrines assert that there is a spiritual reality consisting of Atman-Brahman, one that is the true, eternal, imperishable reality beyond time.\n\nAvidya in the earliest Vedic texts is ignorance, and in later Vedic texts evolves to include anything that is a \"positive hindrance\" to spiritual or nonspiritual knowledge. In the Upanishads, the concept includes \"lack of knowledge, inadequate knowledge and false knowledge\".\n\nThe effect of avidya is to suppress the real nature of things and present something else in its place. In effect it is not different from Maya (pronounced Māyā) or illusion. Avidya relates to the individual Self (\"Ātman\"), while Maya is an adjunct of the cosmic Self (\"Brahman\"). In both cases it connotes the principle of differentiation of an experienced reality into the subject ('I') and an object, as is implicit in human thinking. Avidya stands for that delusion which breaks up the original unity (refer: nonduality) of what is real and presents it as subject and object and as doer and result of the deed. What keeps humanity captive in Samsara is this avidya. This ignorance,\"the ignorance veiling our true self and the truth of the world\", is not lack of erudition; it is ignorance about the nature of 'Being' (\"Sat\"). It is a limitation that is natural to human sensory or intellectual apparatus. This is responsible for all the misery of humanity. Advaita Vedanta holds that the eradication of it should be humanity's only goal and that will automatically mean realisation of the Self (\"Ātman\").\n\nAdi Shankara says in his Introduction to his commentary on the Brahma Sutras, \"Owing to an absence of discrimination, there continues a natural human behaviour in the form of 'I am this' or 'This is mine'; this is avidya. It is a superimposition of the attributes of one thing on another. The ascertainment of the nature of the real entity by separating the superimposed thing from it is vidya (knowledge, illumination)\". In Shankara's philosophy avidya cannot be categorized either as 'absolutely existent' or as 'absolutely non-existent'.\n\n"}
{"id": "4817120", "url": "https://en.wikipedia.org/wiki?curid=4817120", "title": "Belle Steel", "text": "Belle Steel\n\nBelle Steel was an 18th-century Presbyterian from Poleglass, County Antrim, Ireland.\n\nIn Cathal O'Byrne's book \"As I Roved Out\" there are a couple of chapters on Steel. She lived in a cottage in the townland of Poleglass, on the outskirts of Belfast and Lisburn, along what is now the Stewartstown Road. Her place in history comes from her sympathy for the Roman Catholic faithful who, under Penal Law, were denied their church buildings for worship. Steel was the trusted custodian of the sacred vessels used in the Liturgy, as well as a small horn used to summon the faithful to Mass. Her name is commemorated in the name of one of the roads through Poleglass. She kept watch and provided warnings to the people attending mass when the soldiers were approaching. She is depicted in many of Belfast's nationalist murals.\n\n"}
{"id": "12311883", "url": "https://en.wikipedia.org/wiki?curid=12311883", "title": "Berkley Center for Religion, Peace, and World Affairs", "text": "Berkley Center for Religion, Peace, and World Affairs\n\nThe Berkley Center for Religion, Peace, and World Affairs is an academic research center at Georgetown University in Washington, DC dedicated to the interdisciplinary study of religion, ethics, and politics. The center was founded in 2006 under a gift from William R. Berkley, a member of Georgetown's Board of Directors. The center's founding director was Thomas Banchoff.\n"}
{"id": "872938", "url": "https://en.wikipedia.org/wiki?curid=872938", "title": "Biocomplexity", "text": "Biocomplexity\n\nBiocomplexity is the study of complex structures and behaviors that arise from nonlinear interactions of active biological agents, which may range in scale from molecules to cells to organisms. Almost every biological system exhibits complexity - emergent properties where the ensemble possesses capabilities that its individual agents lack. Classical examples of biocomplexity include the behavior of molecular motors during DNA transcription, genetic and metabolic networks within cells, the interacting filaments of the cytoskeleton, which allow the cell to move, the differentiation, organization and movement of cells during embryonic development, the function of the networks of neurons which compose the brain and the schooling of fish or birds. \n\nPrimarily as a result of funding policy changes at the American National Science Foundation around 2000, some researchers have begun to use the term biocomplexity in a narrower sense to denote the complex behavioral, biological, social, chemical, and physical interactions of living organisms with their environment. This relatively new subfield of biocomplexity encompasses other domains such as biodiversity and ecology.\n\nBiocomplexity research aims to provide quantitative models of complex biological phenomena both to understand them in their own right and to interpret and guide quantiative biomedical experimentation.\n\nKluwer planned to publish a journal called \"Biocomplexity\". A disappointingly low number of submitted manuscripts resulted in the publisher cancelling the journal's launch issue.\n\n\n\n"}
{"id": "10989687", "url": "https://en.wikipedia.org/wiki?curid=10989687", "title": "Black elite", "text": "Black elite\n\nThe black elite in the South of the United States started forming before the American Civil War among free blacks who managed to acquire property. Of the free people of color in North Carolina in the censuses from 1790 to 1810, 80% can be traced to African Americans free in Virginia during the colonial period. Free blacks migrated from Virginia to other states as did their neighbors. Extensive research into colonial court records, wills and deeds has demonstrated that most of those free families came from relationships or marriages between white women, servant or free, and black men, servant, free or slave. Such relationships were part of the more fluid relationships among the working class before the boundaries of slavery hardened.\n\nDuring the 19th century, there were additional relationships between whites and blacks, not always of a consensual nature. What is notorious is that white slaveholders could take advantage of slave women because of their power. There were also slaveholders who had caring relationships, common-law marriages, and real marriages with black slave women, and who sometimes freed them and their children. Some slaveholders did provide for their mixed-race children by ensuring they got education although, in other cases, they were simply apprenticed to a trade or craft. Whatever property the father passed on to the child was important in helping that person get a start in life. \n\nThe mulattos patterned their lives after \"polite\" white society. \n\nIn the South, because of their head start in acquiring property, the black elite began to exercise leadership roles within the church, black schools, and community, but as in any society, there were natural leaders who rose up from many classes. \n\nCatering services and other skilled employment were important because they had the white contacts needed to remain within the “status quo”. The black elite also enjoyed the benefits of living within the white neighborhoods which further isolated them from the darker-skinned African Americans which caused them to blame them for the downward shifts in life-style choices. They felt that by “emulating” the white man could social standing and class be achieved. \nThe Civil Rights Movement and affirmative action brought about many changes for the black elite. As the old elite died away, it made room for the new black elite to emerge. Within its realm are the educated, the entrepreneurs, actors, singers, and those who comprise the top nine percent of the elite status.\n\n"}
{"id": "39105", "url": "https://en.wikipedia.org/wiki?curid=39105", "title": "Boehm system", "text": "Boehm system\n\nThe Boehm system is a system of keywork for the flute, created by inventor and flautist Theobald Boehm between 1831 and 1847. \n\nPrior to the development of the Boehm system, flutes were most commonly made of wood, with an inverse conical bore, eight keys, and tone holes (the openings where the fingers are placed to produce specific notes) that were small in size, and thus easily covered by the fingertips. Boehm's work was inspired by an 1831 concert in London, given by soloist Charles Nicholson who, with his father in the 1820s, had introduced a flute constructed with larger tone holes than were used in previous designs. This large-holed instrument could produce greater volume of sound than other flutes, and Boehm set out to produce his own large-holed design.\n\nIn addition to large holes, Boehm provided his flute with \"full venting\", meaning that all keys were normally open (previously, several keys were normally closed, and opened only when the key was operated). Boehm also wanted to locate tone holes at acoustically optimal points on the body of the instrument, rather than locations conveniently covered by the player's fingers. To achieve these goals, Boehm adapted a system of axle-mounted keys with a series of \"open rings\" (called \"brille\", German for \"eyeglasses\", as they resembled the type of eyeglass frames common during the 19th century) that were fitted around other tone holes, such that the closure of one tone hole by a finger would also close a key placed over a second hole.\n\nIn 1832 Boehm introduced a new conical-bore flute, which achieved a fair degree of success. Boehm, however, continued to look for ways to improve the instrument. Finding that an increased volume of air produced a stronger and clearer tone, he replaced the conical bore with a cylindrical bore, finding that a parabolic contraction of the bore near the embouchure hole improved the instrument's low register. He also found that optimal tone was produced when the tone holes were too large to be covered by the fingertips, and he developed a system of finger plates to cover the holes. These new flutes were at first made of silver, although Boehm later produced wooden versions. \n\nThe cylindrical Boehm flute was introduced in 1847, with the instrument gradually being adopted almost universally by professional and amateur players in Europe and around the world during the second half of the 19th century. The instrument was adopted for the performance of orchestral and chamber music, opera and theater, wind ensembles (e.g., military and civic bands), and most other music which might be loosely described as relating to \"Western classical music\" (including, for example, jazz). Many further refinements have been made, and countless design variations are common among flutes today (the \"offset G\" key, addition of the low B foot, etc.) The concepts of the Boehm system have been applied across the range of flutes available, including piccolos, alto flutes, bass flutes, and so on, as well as other wind instruments. The material of the instrument may vary (many piccolos are made of wood, some very large flutes are wooden or even made of PVC).\n\nThe flute is perhaps the oldest musical instrument, other than the human voice itself. There are very many flutes, both traversely blown and end-blown \"fipple\" flutes, currently produced which are not built on the Boehm model.\n\nThe fingering system for the saxophone closely resembles the Boehm system. A key system inspired by Boehm's for the clarinet family is also known as the \"Boehm system\", although it was developed by Hyacinthe Klosé and not Boehm himself. The Boehm system was also adapted for a small number of flageolets. Boehm did work on a system for the bassoon, and Boehm-inspired oboes have been made, but non-Boehm systems remain predominant for these instruments. The Albert system is another key system for the clarinet.\n\n\n"}
{"id": "35856983", "url": "https://en.wikipedia.org/wiki?curid=35856983", "title": "Brenda Cherry", "text": "Brenda Cherry\n\nBrenda Cherry (born March 19, 1958), is an American Civil Rights Activist from Paris, Texas. \n\nCherry is the President and co-founder of Concerned Citizens for Racial Equality, a non-profit civil rights organization located in Paris, Texas. Founded in 2003, CCFRE co-sponsored events with the U.S. Department of Justice, Lone Star Legal Aid, and the American Civil Liberties Union.\n\nCherry grew up in Blossom Texas. Her late father, Zeb Reynolds, was a farmer, and her mother, Irene Whitney, was a domestic worker. She began school at T.G. Givens, which was then, a segregated school in Paris, Texas. She had to be bused 10 miles to the school even though there was a local school district in the town where she lived. After the schools were forced to integrate, she attended the formerly \"whites only\" school in Blossom. Cherry graduated from Prairiland High School in Pattonville, Texas. She went on to attend Paris Junior College and East Texas State University. Cherry worked as a Licensed Vocational Nurse for 9 years before becoming a civil rights activist. Cherry has two daughters, Shauncia Cherry and Tiffany Cherry, and one son, Rico Lewis.\n\nOn October 23, 2003, an 11-year-old black student was reportedly attacked by a principal at Crockett Middle School in Paris, Texas. The child was removed from the school and sent to a detention center. Cherry, along with a small group of people, including the child’s mother and grandfather, staged a protest in front of the school. During the protest, the child was released from the detention center. This event marked the beginning of several future protest demonstrations in the town.\n\nShaquanda Cotton was an African-American student that attended Paris High School in Paris, Texas. On March 16, 2006, Cotton, aged 14, was arrested for allegedly shoving a school hall monitor. \nHaving no prior criminal history, Cotton was tried and sentenced to up to 7 years in the Texas Youth Commission. During the same time frame, the judge in the case, Chuck Superville, sentenced a Caucasian 14-year-old girl who had a prior criminal history to probation after committing the confirmed crime of arson. Writer Darwin Campbell with \"African-American News and Issues\" broke the story, which garnered attention to Cotton’s sentencing. After serving a year and one month in prison, the \"Chicago Tribune\" story written by Howard Witt led the case to national exposure. A large protest led by comedian Rickey Smiley was held at the Lamar County Courthouse in Paris, Texas. Approximately two weeks after the \"Tribune\" article and protest, ShaQuanda was released. Cherry was interviewed by the BBC regarding Cotton, and the case was included in a BBC documentary by United Kingdom reporter Julian O'Halloran.\n\nBrandon McClelland was a 24-year-old African American male who lived in Paris, Texas. On September 16, 2008, his body was found partially dismembered and mutilated on a farm road approximately 17 miles from his home. The local newspaper, \"The Paris News\", reported the death as a hit-and-run by an unknown truck. Due to the activism of Cherry and the Concerned Citizens for Racial Equality, the case gained national attention.\n\nAfter dispute about whether McClelland was dragged under the truck or behind it, and while awaiting autopsy reports, \"USA Today\" interviewed Cherry. \"Authorities have not seriously considered the possibility that this was a hate crime. There's a problem in Paris, Texas. I don't see a difference in getting dragged behind a truck and getting dragged under a truck.\"\n\nFurther investigation revealed that McClelland was last seen alive with two Caucasian men, Shannon Finley and Charles Ryan Crostley, both of whom had extensive criminal history. \"Anderson Cooper 360°\" reported that the two men were charged with killing McClelland and both subsequently plead \"not guilty.\" Special prosecutor Toby Shook cited lack of evidence to the case, and the murder charges were dropped in June 2009. The two men were released. CBS News quoted Cherry, \"His body was dragged and nobody gets charged? Even if a trucker came forward, that's all it takes? Even the trucker's not charged? If you hit someone, you don't get charged? Nobody gets charged with this? It's not surprising, but it's sad. It appears that a black man's life means nothing here in Paris.\" Multiple protests have been led by Cherry, along with Jim Blackwell and the New Black Panther Party. The case remains an open murder investigation.\n\nBrandarian Thomas was a 14-year-old African American student at Travis Junior High School in Paris, Texas. In June 2008, Police handcuffed and arrested Thomas at his home for allegedly \"touching a female student's butt and crotch\" in class. Thomas denied the accusations. Thomas' mother contacted Cherry, who convinced her to take the case to trial rather than accept a plea deal. \"Texas Observer\" reporter Forrest Wilder documented the case. Cherry views the Cotton and Thomas prosecutions as extreme examples of a larger pattern of racial discrimination in Paris public schools. Cherry stated, \"The black kids are punished more often, more harshly. I think they go to extremes; they punish them for little things. A lot of times, they see a black kid as more menacing.\" Though prosecutors offered a plea deal to Thomas (two years of probation in exchange for a guilty plea), Cherry helped convince Thomas' mother to take the matter to trial in front of the same judge who heard the Cotton case. \"It's ridiculous,\" Cherry stated. \"He was 14. You're going to ruin his life because you can't make him plead guilty?\"\n\nCornelius Gill was an 18-year-old African American high school student in Paris, Texas. On November 11, 2009, Gill was picking up pecans when he was approached by a Paris police officer who questioned him about a stolen car that had been found a few blocks away. Gill refused to be questioned and said a curse word. The officer arrested Gill and placed him in handcuffs behind his back. When another police car pulled up, Gill was lifted and body slammed face down onto the vehicle. Cherry witnessed a portion of the incident and she, along with Gill's mother, filed a complaint with the Paris Police Department. Cherry obtained the video of the incident. Gill was sentenced to 100 days in jail by Judge Chuck Superville. Although the video footage shows no evidence of such, the officer claimed Gill assaulted him while placing him in the patrol car. Gill was charged with Assault of a Public Servant and Resisting Arrest. The complaint resulted in a suspension of the officer, but the suspension was lifted when the officer appealed. The case resulted in an outcry from civilians regarding police brutality and unnecessary force.\n\nAccording to the \"Huffington Post\", Cherry stated that the ruling sends a message to local police that they can do whatever they want. \"What's going to stop the officer from doing that again and again? It makes me fearful,\" she said. The dash cam video was released after Concerned Citizens for Racial Equality filed an open records request. \"We're already in a bad situation here in Paris, and this just makes it worse,\" Cherry stated.\n\nBobby Yates was a 51-year-old African American man in Paris, Texas. Yates was paralyzed from the waist down due to a gunshot wound during a hunting accident approximately 20 years earlier. His lower body was removed from his hips down due to an infection. On a late night in 2008, Yates made a 911 call to the local police stating that he had been beaten by a 16-year-old white female and two adult males who were at his home that refused to leave. When the police came, the female claimed that Yates had sexually assaulted her. The two males were with her the entire time. The police sided with the female, and Yates was later arrested and charged with sexual assault. Cherry along with her organization held protests and filed complaints on Yates' behalf. After 3 years of legal action, the sexual assault charge was dismissed.\nAaron Hart was an 18-year-old mentally retarded youth with an IQ of 47 when he was arrested and charged with sexual assault. Hart had been playing with a 5-year-old child and they exposed and touched each other's private parts. At trial, Hart's court-appointed attorney called no witnesses on Hart's behalf and failed to obtain an independent doctor for a competency hearing. Judge Eric Clifford sentenced Hart to 100 years in prison on June 10, 2009, claiming that he had no other choice. Within the same time frame, Clifford had sentenced a defendant charged with the same offense, and without a disability, to four years in prison. Hart was allegedly sexually assaulted while awaiting trial in Lamar County Jail. Cherry and her organization advocated on Hart's behalf. Numerous protest marches were held, led by CCFRE and Jim Blackwell of the Tarrant County Local Organizing Committee. The case gained national attention. \"This is just horrible,\" stated Cherry in a \"Los Angeles Times\" article after Judge Clifford made his ruling. \"How could the judge not take his mental ability into consideration?\"\n\nHart was given a new trial on appeal. Hart ultimately accepted a plea deal and was sentenced to 10 years in prison where he remains.\n\nTurner Industries is a privately owned industrial plant with facilities in several cities and states including Paris, Texas. In February 2009, Concerned Citizens for Racial Equality received complaints and photos regarding a noose, racist graffiti, Confederate flags, and salary and advancement discrimination against black workers in the Turner Industries Paris plant. Activism led to a federal investigation by the Equal Employment Opportunity Commission. The findings led to a federal lawsuit which was settled out of court, as well as changes in Turner Industries employment practices. Cherry stated, \"One of my biggest concerns regarding the racist graffiti, noose, and other things found at the plant is the mentality of those who put it there. Those same people serve on juries, and some go on to have supervisory positions or other positions of authority.\" The Turner Industries investigation was one of the largest civil rights investigations ever undertaken in the State of Texas.\n\nOn April 20, 2011, a 14-year-old African American autistic student was taken by an 18-year-old Caucasian student into the greenhouse at Paris High School. The 18-year-old threw a rope over a beam and placed the other end around the waist of the 14-year-old. He kicked his feet and beat his hand with a stick until bloody when the 14-year-old attempted to get free. The girlfriend of the 18-year-old student stood laughing and recorded the incident with her cell phone. When the group returned to class, a male teacher took the recording and erased it. After the school refused to discipline the 18-year-old and his girlfriend, or the teacher who erased the evidence, Cherry and CCFRE advocated on behalf of the mother, Tina Washington, and her son. The county attorney refused to charge the 18-year-old student even though it was a felony offense. Several protests were led by Cherry and Jim Blackwell regarding the incident. The case is currently under investigation by the U.S. Department of Education, Office for Civil Rights.\n\nConcerned Citizens for Racial Equality co-sponsored a community reconciliation meeting with the U.S. Department of Justice which was featured in the \"Chicago Tribune\". \"I'm here to talk about racism. I don't see any sense in playing games, pretending it doesn't exist,\" stated Cherry. \"When you go in the schools and see mostly black kids sitting in detention—it's racism. In court, we get high bonds, we get longer sentences. If that's not racism, what is it?\"\n\n\"This town is being forced to look at things they never wanted to look at before,\" stated Cherry in an excerpt from \"Newsweek\". Cherry, along with Lone Star Legal Aid, a local nonprofit law firm, compiled statistics showing that the Paris Independent School District punished black kids eight times more often than white ones, even though blacks make up a minority of the population. The U.S. Department of Education ruled that there wasn't enough evidence to attribute the discrepancy to racism. \"The New York Times\" quoted Cherry, \"I think we are probably stuck in 1930 right about now. If you complain about anything, you are going to be punished.\"\n\nCherry has been an advocate for civil and human rights, and continues her work to address race and humanitarian issues.\n"}
{"id": "4024", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "Butterfly effect\n\nIn chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\n\nThe term, coined by Edward Lorenz, is derived from the metaphorical example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as the flapping of the wings of a distant butterfly several weeks earlier. Lorenz discovered the effect when he observed that runs of his weather model with initial condition data that was rounded in a seemingly inconsequential manner would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\n\nThough Lorenz gave a name to the phenomenon, the idea that small causes may have large effects in general and in weather specifically was earlier recognized by French mathematician and engineer Henri Poincaré and American mathematician and philosopher Norbert Wiener. Edward Lorenz's work placed the concept of \"instability\" of the earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\n\nThe butterfly effect can also be demonstrated by very simple systems.\n\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says that \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\n\nChaos theory and the sensitive dependence on initial conditions were described in the literature in a particular case of the three-body problem by Henri Poincaré in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\n\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\n\nThe idea that one butterfly could eventually have a far-reaching ripple effect on subsequent historic events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury about time travel.\n\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\n\nLorenz wrote:\nIn 1963 Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated: Following suggestions from colleagues, in later speeches and papers Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly’s wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\n\nThe phrase refers to the idea that a butterfly's wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions; one set of conditions leads to a tornado while the other set of conditions doesn't. The flapping wing represents a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different—but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\n\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\n\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations.\n\nRecurrence, the approximate return of a system towards its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\n\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical.\n\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any δ > 0, there are y in \"M\", with distance \"d\"(. , .) such that formula_3 and such that\n\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent.\n\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\n\nwhich, unlike most chaotic maps, has a closed-form solution:\n\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself – it is non-periodic. This solution equation clearly demonstrates the two key features of chaos – stretching and folding: the factor 2 shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range [0, 1].\n\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat overstated.\"\n\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and Delos and co-workers.\n\nOther authors suggest that the butterfly effect can be observed in quantum systems. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\n\nThe journalist Peter Dizikes, writing in \"The Boston Globe\" in 2008, notes that popular culture likes the idea of the butterfly effect, but gets it wrong. Whereas Lorenz suggested correctly with his butterfly metaphor that predictability \"is inherently limited\", popular culture supposes that each event can be explained by finding the small reasons that caused it. Dizikes explains: \"It speaks to our larger expectation that the world should be comprehensible – that everything happens for a reason, and that we can pinpoint all those reasons, however small they may be. But nature itself defies this expectation.\"\n\n\n\n"}
{"id": "32817779", "url": "https://en.wikipedia.org/wiki?curid=32817779", "title": "Cold peace", "text": "Cold peace\n\nA cold peace is a state of relative peace between two countries which is marked by the enforcement of a peace treaty ending the state of war while the government or populace of at least one of the parties to the treaty continues to domestically treat the treaty with vocal disgust. \n\nIt is contrasted against a cold war, in which at least two states which are not openly pursuing a state of war against each other, openly or covertly support conflicts between each other's client states or allies. Cold peace, while marked by similar levels of mistrust and antagonistic domestic policy between the two governments and populations, do not result in proxy wars, formal incursions, or similar conflicts.\n\nThe Camp David Accords, the Egypt–Israel Peace Treaty and the aftermath of relations between Israel and Egypt are considered a modern example of a cold peace. After having engaged each other in five prior wars, the populations had become weary of the loss of life, and the negotiation of the Accords and Treaty were considered a high point of the Middle East Peace Process. However, Egyptian popular support for the treaty plummeted following the 1981 assassination of Anwar Sadat and the 1982 Israeli invasion of Lebanon, and perception of the treaty has not recovered in the Egyptian populace ever since. \n\nThis drop in support for the treaty was not entirely reflected in Egyptian government policy, as from 1981 until his 2011 ouster, Sadat's successor Hosni Mubarak continued to retain the treaty's terms, while also playing public sentiment against Israelis and Jews through state media. Following Mubarak's ouster and the installation of a military junta in power until inauguration of the next civilian government, protesters voiced strong opposition against the 1979 treaty with Israel, and the Israeli response to Palestinian attacks on Israeli civilians and military personnel resulted in the withdrawal of the Egyptian ambassador over the deaths of 5 Egyptian security personnel in the Sinai, ostensibly by either Palestinian militants or Israeli military personnel engaged in a retaliatory air raid on Gaza. \n\nThe lack of Egyptian support for the 1979 treaty is due in part to panethnic and religious fundamentalist sympathies in Egypt for Palestinian and other Arab Muslim militancies against Israel, a Jewish-majority state currently in conflict over the territory of Israel and the Palestinian territories, as well as Egyptian nationalist sentiment against Israel dating back to before Israel's independence in 1948. Furthermore, while most of the letter of both the Accords and the treaty has been maintained, the spirit of normalization which had been intended are perceived as not having been fulfilled.\n\nA ceasefire signed between India and Pakistan over Kashmir has kept open hostilities from developing between the two countries, but numerous incidents involving Pakistani nationals, such as the 2008 Mumbai attacks, likewise Indian involvement in Baluchistan have often strained both diplomatic relations and popular support for peace between the two nuclear powers.\n"}
{"id": "42942751", "url": "https://en.wikipedia.org/wiki?curid=42942751", "title": "Creative Writers and Day-Dreaming", "text": "Creative Writers and Day-Dreaming\n\nCreative Writers and Day-Dreaming (), was an informal talk given in 1907 by Sigmund Freud, and subsequently published in 1908, on the relationship between unconscious phantasy and creative art.\n\nFreud's argument – that artists, reviving memories of childhood daydreams and play activities, succeeded in making them acceptable through their aesthetic technique – was to be widely influential for interwar modernism.\n\nFreud began his talk by raising the question of where writers drew their material from, suggesting that children at play, and adults day-dreaming, both provided cognate activities to those of the literary artist. Heroic and erotic daydreams or preconscious phantasies in both men and women were seen by Freud as providing substitute satisfactions for everyday deprivations; and the same phantasies were in turn turned into shareable (public) artistic constructs by the creative writer, where they could serve as cultural surrogates for the universal instinctual renunciations inherent in civilization.\n\nFreud saw the aesthetic principle as the ability to turn the private phantasy into a public artefact, using artistic pleasure to release a deeper pleasure founded on the release of forbidden (unconscious) material. The process allowed the writer him/herself to emerge from their introversion and return to the public world. If the phantasies came too close to the unconscious repressed, however, the process would fail, leading either to creative inhibition or to a rejection of the artwork itself.\n\nFreud himself epitomised his essay's argument a decade later in his \"Introductory Lectures\", stating of the true artist that: \"he understands how to work over his daydreams in such a way as to make them lose what is too personal in them and repels strangers, and to make it possible for others to share in the enjoyment of them. He understands, too, how to tone them down so that they do not easily betray their origin from proscribed sources...he has thus achieved \"through\" his phantasy what originally he had achieved only \"in\" his phantasy – honour, power and the love of women\".\n\n\n\n"}
{"id": "1029074", "url": "https://en.wikipedia.org/wiki?curid=1029074", "title": "Desire realm", "text": "Desire realm\n\nThe desire realm (Sanskrit: \"kāmadhātu\") is one of the trailokya or three realms (Sanskrit: \"dhātu\", Tibetan: \"khams\") in Buddhist cosmology into which a being wandering in \"\" may be reborn. The other two are the form realm, (Sanskrit \"rūpadhātu\") and the formless realm (S. \"ārūpadhātu\").\n\nWithin the desire realm are either five or six domains (Sanskrit: \"gati\", also sometimes translated as \"realm\"). In Tibetan Buddhism, there are six domains () and in Theravada Buddhism there are only five, because the domain of the Asuras is not regarded as separate from that of the Nāgas. \nThe five realms are also found in Taoism and Jainism.\n\nThe \"Śūraṅgama Sūtra\" in Mahayana Buddhism regarded the 10 kinds of Xian as separate immortal realms between the Deva and human realms.\n\nThe six domains of the desire realm are also known as the \"six paths of suffering\", the \"six planes\", and the \"six lower realms\". In schools of thought that use the ten realms system, these six domains are often contrasted negatively with the \"four higher realms\" of Śrāvaka, Pratyekabuddha, Bodhisattva and full Buddha, which are considered to be the spiritual goals of the different Buddhist traditions.\n\nA being's Karma (previous actions and thoughts) determines which of the six domains it will be reborn into. A sentient being may also ascend to one of the higher realms beyond the six domains of the desire realm by practicing various types of meditation, specifically the Eight Dhyānas.\n\nThe 8th century Buddhist monument Borobudur in Central Java incorporated the trailokya into the architectural design with the plan of mandala that took the form of a stepped stone pyramid crowned with stupas.\n\nThe six domains of the desire realm are as follows:\n\n\nThe \"Deva domain\" (also known as the \"God domain\" or \"Blissful State\") is the domain of bliss. The disadvantage of this domain is that things are so very comfortable there, that these beings completely neglect to work towards enlightenment. Instead they gradually use up the good karma they had previously accumulated, and so they subsequently fall to a lower rebirth.\n\nThe Deva domain is sometimes also referred to as the \"gods' domain\", because its inhabitants are so powerful within their own domain, that compared to humans, they resemble the gods of Greek or Roman mythology. However, while the Devas may be referred to as gods, they are not immortal, omniscient, nor omnipotent, and they do not act as creators or judges at death, so they are notably very distinct from the monotheistic Western concept of God, although they are very similar to the gods of most European polytheism.\n\nThe \"Asura domain\" (also known as \"the Jealous God domain\") is the domain of the Asuras (demigods). They are here because of actions in past lives based on egotistic jealousy, envy, insincerity, struggle, combat or rationalization of the world. They may be here because in human form they had good intentions but committed bad actions such as harming others. The Asuras of some other domains, however, are fully malevolent (such as the corruptor Mara) and can be more closely related to the translation of demon that is sometimes ascribed to them. These evil Asuras can be alternatively referred to as Rakshasas.\n\nThe Asuras are said to experience a much more pleasurable life than humans, but they are plagued by envy for the Devas, whom they can see just as animals can see humans.\n\nThe \" domain\" (also known as the \"Human domain\") is based on passion, desire, doubt, and pride.\n\nBuddhists see this domain as the realm of human existence. Although it may not be the most pleasurable domain to live in, a human rebirth is in fact considered to be by far the most advantageous of all possible rebirths in samsara, because a human rebirth is the only samsaric domain from which one can directly attain Bodhi (enlightenment), either in the present rebirth (for Buddhas and Arhats) or in a future rebirth in a Deva domain (for Anagamis). This is because of the unique possibilities that a human rebirth offers: beings in higher domains just choose to enjoy the pleasures of their realms and neglect working towards enlightenment, while beings in lower domains are too busy trying to avoid the suffering and pain of their worlds to give a second thought to liberation. Humans have just the right balance: enough suffering to motivate them to achieve liberation, but not too much that every moment of their lives is consumed by it.\n\nA human rebirth is considered to have tremendous potential when used correctly, however in most cases humans waste their lifetimes in materialistic pursuits rather than working towards enlightment, and so end up reinforcing their unhelpful emotions, thoughts, and actions, rather than letting go of them. Because of this, it is almost always the case that one descends to a lower domain of rebirth after a human life, rather than immediately going on to another human birth, or going up to a higher domain.\n\nIn the lower domains, such as the animal domain, it is a very slow and difficult process to accumulate enough merit to achieve a human rebirth once again, so it may be countless lifetimes before one has another chance. This means that we should not choose to delay attaining enlightment for a future life, since that may be thousands or even millions of years in the future, but should instead embrace the teachings of the Buddha in our present life, so that we can attain enlightenment here and now.\n\nThe \"Tiryagyoni domain\" (alternately spelled \"Tiryag-yoni\" or \"tiracchānayoni\") (also known as the \"Animal domain\") is based on strong mental states of stupidity and prejudice cultivated in a previous life or lives.\n\nBuddhists believe that this domain is the domain of existence of the nonhuman animals on the Earth. Although humans and animals live in separate domains of existence, they can still see each other because their domains are so close to each other in the vertical cosmology, just like how the Devas and Asuras can see each other despite being in separate domains.\n\nThe \"Preta domain\" (also known as the \"Hungry Ghost domain\") is a rebirth based on strong possessiveness and desire which were cultivated in a previous life or lives.\n\nThe sentient beings in this domain are known as \"hungry ghosts\". They are constantly extremely hungry and thirsty, but they cannot satisfy these needs. In Tibetan versions of the Bhavacakra these beings are drawn with narrow necks and large bellies. This represents the fact that their desires torment them, but they are completely unable to satisfy themselves.\n\nThe \"Naraka domain\" (also known as the \"Niraya domain\" or the \"Hell domain\") is a rebirth based on strong states of hatred cultivated in a previous life or lives.\n\nThe Buddhist view of Naraka differs significantly from the concept of Hell in most monotheistic religions in that those being punished in Naraka are not trapped permanently, but will eventually be released, in contrast to most monotheistic religions which say that Hell is permanent. Exceptions include Judaism and Catholicism, which have a temporary hell domain called purgatory, in which souls are punished and purified of their sins by fire and then continue on their journey to paradise. In Buddhism souls remain in Naraka until their negative Karma is used up, at which point they are reborn into another domain.\n\nThe Bhavachakra or \"Wheel of Life\" is a popular teaching tool often used in the Indo-Tibetan tradition. It is a kind of diagram which portrays these realms and the mechanism that causes these samsaric rebirths. In this depiction, the realm of the Devas is shown at the top, followed clockwise by the realms of the Asuras, the Animals, Naraka, the Pretas, and the Humans. Close examination will show that the Buddha is shown as being present in every one of these realms.\n\nIn Mahayana and Vajrayana Buddhism, there are some sayings reflecting a tradition that the manner of a sentient being's death indicates the world in which it will be reborn. A common one is in the \"Verses on the Structure of the Eight Consciousnesses\" (八識規矩補註), reads: \"to birth in saints the last body temperature in top of head, to deva in eyes, to human in heart, to hungry ghosts in belly, to animals in knee cap, to the hells-realm in sole of feet.\" The Tibetan Buddhist text Bardo Thodol describes further on the experiences to these realms.\n\nTaisen Deshimaru in his book \"Vrai Zen, Introduction au Shōbōgenzō\" (True Zen, Introduction to Shōbōgenzō) explains: \n\nTenzin Wangyal Rinpoche and Chögyal Namkai Norbu Rinpoche have published literature teaching a \"Practice of the Six Lokas\" designed to \"purify the karmic traces that lead to rebirth in the different realms,\" wherein the six lokas are also cognate with the principal six chakra system of Vajrayana.\n\n\n"}
{"id": "25727150", "url": "https://en.wikipedia.org/wiki?curid=25727150", "title": "Dravya", "text": "Dravya\n\nDravya () is a term used to refer to a substance. According to the Jain philosophy, the universe is made up of six eternal substances: sentient beings or souls (\"jīva\"), non-sentient substance or matter (\"pudgala\"), principle of motion (\"dharma\"), the principle of rest (\"adharma\"), space (\"ākāśa\") and time (\"kāla\"). The latter five are united as the \"ajiva\" (the non-living). As per the Sanskrit etymology, \"dravya\" means substances or entity, but it may also mean real or fundamental categories.\n\nJain philosophers distinguish a substance from a body, or thing, by declaring the former as a simple element or reality while the latter as a compound of one or more substances or atoms. They claim that there can be a partial or total destruction of a body or thing, but no substance can ever be destroyed. \n\nAccording to Jain philosophy, this universe consists of infinite \"jivas\" or souls that are uncreated and always existing. There are two main categories of souls: un-liberated mundane embodied souls that are still subject to transmigration and rebirths in this \" samsara \" due to karmic bondage and the liberated souls that are free from birth and death. All souls are intrinsically pure but are found in bondage with karma since beginning-less time. A soul has to make efforts to eradicate the karmas attain its true and pure form.\n\n10th-century Jain monk Nemichandra describes the soul in \"Dravyasamgraha\":\n\nThe qualities of the soul are \"chetana\" (consciousness) and \"upyoga\" (knowledge and perception). Though the soul experiences both birth and death, it is neither really destroyed nor created. Decay and origin refer respectively to the disappearing of one state and appearing of another state and these are merely the modes of the soul. Thus Jiva with its attributes and modes, roaming in \"samsara\" (universe), may lose its particular form and assume a new one. Again this form may be lost and the original acquired.\n\nMatter is classified as solid, liquid, gaseous, energy, fine Karmic materials and extra-fine matter i.e. ultimate particles. \"Paramāṇu\" or ultimate particle (atoms or sub-atomic particles) is the basic building block of all matter. It possesses at all times four qualities, namely, a color (\"varna\"), a taste (\"rasa\"), a smell (\"gandha\"), and a certain kind of palpability (\"sparsha\", touch). One of the qualities of the \"paramāṇu\" and \"pudgala\" is that of permanence and indestructibility. It combines and changes its modes but its basic qualities remain the same. It cannot be created nor destroyed and the total amount of matter in the universe remains the same.\n\n\"Dharma\" means the principles of Motion that pervade the entire universe. Dharma and Adharma are by themselves not motion or rest but mediate motion and rest in other bodies. Without \"Dharma\" motion is not possible. The medium of motion helps matter and the sentient that are prone to motion to move, like water (helps) fish. However, it does not set in motion those that do not move.\n\nWithout \"adharma\", rest and stability is not possible in the universe. The principle of rest helps matter and the sentient that are liable to stay without moving, like the shade helps travellers. It does not stabilize those that move. According to Champat Rai Jain:\n\nSpace is a substance that accommodates the living souls, the matter, the principle of motion, the principle of rest and time. It is all-pervading, infinite and made of infinite space-points.\n\n\"Kāla\" is a real entity according to Jainism and is said to be the cause of continuity and succession. Champat Rai Jain in his book \"\"The Key of Knowledge\" wrote: Jaina philosophers call the substance of Time as \"Niścay\" Time to distinguish it from \"vyavhāra\" (practical) Time which is a measure of duration- hours, days and the like.\n\nOut of the six \"dravyas\", five except time have been described as \"astikayas\", that is, extensions or conglomerates. Since like conglomerates, they have numerous space points, they are described as \"astikaya\". There are innumerable space points in the sentient substance and in the media of motion and rest, and infinite ones in space; in matter they are threefold (i.e. numerable, innumerable and infinite). Time has only one; therefore it is not a conglomerate. Hence the corresponding conglomerates or extensions are called—\"jivastikaya\" (soul extension or conglomerate), \"pudgalastikaya\" (matter conglomerate), \"dharmastikaya\" (motion conglomerate), \"adharmastikaya\" (rest conglomerate) and \"akastikaya\" (space conglomerates). Together they are called \"pancastikaya\" or the five \"astikayas\".\n\nThese substances have some common attributes or gunas such as:\n\nThere are some specific attributes that distinguish the dravyas from each other:\n\n\n"}
{"id": "773618", "url": "https://en.wikipedia.org/wiki?curid=773618", "title": "Evil demon", "text": "Evil demon\n\nThe evil demon, also known as malicious demon and evil genius, is a concept in Cartesian philosophy. In the first of his 1641 \"Meditations on First Philosophy\", Descartes imagines that an evil demon, of \"utmost power and cunning has employed all his energies in order to deceive me.\" This evil demon is imagined to present a complete illusion of an external world, so that Descartes can say, \"I shall think that the sky, the air, the earth, colours, shapes, sounds and all external things are merely the delusions of dreams which he has devised to ensnare my judgement. I shall consider myself as not having hands or eyes, or flesh, or blood or senses, but as falsely believing that I have all these things.\"\n\nSome Cartesian scholars opine that the demon is also omnipotent, and thus capable of altering mathematics and the fundamentals of logic, though omnipotence of the evil demon would be contrary to Descartes' hypothesis, as he rebuked accusations of the evil demon having omnipotence.\n\nIt is one of several methods of systematic doubt that Descartes employs in the \"Meditations.\"\n\nPrior to the Meditations proper, Descartes gives a synopsis of each Meditation and says of Meditation One that \"reasons are provided which give us possible grounds for doubt about all things, especially material things\" and that whilst the usefulness of such extensive doubt may not be immediately apparent, \"its greatest benefit lies in \nThe eventual result of this doubt is to \n\nDescartes offers some standard reasons for doubting the reliability of the senses culminating in the dream argument and then extends this with the deceiving God argument. Descartes refers to \"the long-standing opinion that there is an omnipotent God who made me the kind of creature that I am\" and suggests that this God may have \"brought it about that there is no earth, no sky, no extended thing, no shape, no size, no place, while at the same time ensuring that all these things appear to me to exist just as they do now?\" Furthermore, this God may have \"brought it about that I too go wrong every time I add two and three or count the sides of a square, or in some even simpler matter, if that is imaginable\".\n\nAfter the deceiving God argument Descartes concludes that he is \"compelled to admit that there is not one of my former beliefs about which a doubt may not properly be raised\". \n\nIt is only after arriving at this conclusion that Descartes introduces the evil demon.\n\nAlthough Descartes has provided arguments for doubting all his former beliefs he notes that \"my habitual opinions keep coming back\". It is to deal with this problem that Descartes decides he must do more than just acknowledge that the beliefs are open to doubt and must deceive himself, \"by pretending for a time that these former opinions are utterly false and imaginary\" and that he shall do this \"until the weight of preconceived opinion is counter-balanced and the distorting influence of habit no longer prevents my judgement from perceiving things correctly\".\n\nIt is to achieve this state of denial that Descartes says he will suppose that \"some malicious demon of the utmost power and cunning has employed all his energies in order to deceive me\".\n\nThe evil demon is also mentioned at the beginning of Meditation Two. Descartes says that if there is \"a deceiver of supreme power and cunning who is deliberately and constantly deceiving me\" then he himself must undoubtedly exist for the deceiver can \"never bring it about that I am nothing so long as I think that I am something\". A little later he says, \"But what shall I now say that I am, when I am supposing that there is some supremely powerful and, if it is permissible to say so, malicious deceiver, who is deliberately trying to trick me in every way he can?\"\n\nSome writers, e.g. Williams and Musgrave, make no distinction between the deceiving God and evil demon arguments and regard anything said about the deceiving God as being equivalent to saying something about the evil demon.\n\nOther writers acknowledge that Descartes makes mention of both but then claim they are 'epistemologically equivalent'. Kenny says, \"the two hypotheses do not differ in any respect of epistemological importance... The content of the two hypotheses is the same...\" Newman says, \"Descartes' official position is that the Evil Genius Doubt is merely one among multiple hypotheses that can motivate the more general hyperbolic doubt... Even so, I regularly speak in terms of the evil genius... as a kind of mnemonic for the more general doubt about our cognitive nature.\"\n\nIf they are epistemologically equivalent then the question arises as to why Descartes temporarily shifted from the deceiving God to the evil demon. It is tempting to think it is because there is a relevant theological difference. In Meditation Three Descartes is going to establish not only that there is a God but that God is not a deceiver. When Descartes first introduces the evil demon he says, \"I will suppose therefore that not God, who is supremely good and the source of truth, but rather some malicious demon.\" Kenny says, \"The hypothesis of the evil genius is substituted for that of the deceitful God simply because it is less offensive and less patently incoherent.\" However, at least in Meditation One, Descartes doesn't have a problem in postulating a deceiving God and he rejects the objection that such deception is inconsistent with God's supreme goodness. He says, \"if it were inconsistent with his goodness to have created me such that I am deceived all the time, it would seem equally foreign to his goodness to allow me to be deceived even occasionally; yet this last assertion cannot be made.\" This is consistent with what he writes in the \"Principles\" where he says, \"we have been told that God who created us can do all that he desires, and we do not yet know whether he may not have willed to create us in such a way that we shall always be deceived even in the things that we think ourselves to know best.\"\n\nOther writers insist that it is important to maintain the distinction between the deceiving God and the evil demon. Gouhier (quoted by Kenny) argues that the deceiving God is an intellectual scruple that will disappear when metaphysics demonstrates its falsity whilst the evil demon is a methodological procedure designed to make a certain experiment and it ceases with that experiment. He says, \"Neither the purpose nor the content of the two hypotheses allow us to regard the one as a variant of the other.\"\n\nVendler argues that literary form of the Meditations is heavily influenced by St. Ignatius of Loyola's Spiritual Exercises to which Descartes will have been exposed during his training at the Jesuit college of La Fleche. As such, \"The demon in the First Meditation is not evoked to serve as an epistomological menace, but as a psychological device: following Loyola's advice \"age contra!\" (go against!), it provides a counterweight to our inordinate inclination to trust the senses.\" He adds, \"the 'demon-argument' is not an argument at all. Descartes does not need another argument at this stage: the dream argument has already shown the unreliability of the senses and the deceiver-God argument the uncertainty of mathematics. For one thing, the demon does not even touch mathematics or geometry. Why should he? He is evoked by Descartes to cure his inordinate attachment to the senses; he does not complain (and would not) of a similar attachment to mathematics or geometry.\" Hatfield takes a similar line saying, \"Descartes adopts a common practice from the spiritual exercises upon which his metaphysical meditations are modelled, devising a program for training the will to keep the old beliefs at bay\" adding, \"It seems likely that he chose to call his hypothetical deceiver a \"malicious demon\" in order to avoid having the meditator concentrate extensively on the thought that God could be a deceiver, a proposition he considered false and one he intended to refute later.\"\n\nAmong the accusations of blasphemy made against Descartes by Protestants was that he was positing an omnipotent malevolent God. Voetius accused Descartes of blasphemy in 1643. Jacques Triglandius and Jacobus Revius, theologians at Leiden University, made similar accusations in 1647, accusing Descartes of \"hold[ing] God to be a deceiver\", a position that they stated to be \"contrary to the glory of God\". Descartes was threatened with having his views condemned by a synod, but this was prevented by the intercession of the Prince of Orange (at the request of the French Ambassador Servien).\n\nThe accusations referenced a passage in the \"First Meditation\" where Descartes stated that he supposed not an optimal God but rather an evil demon \"summe potens & callidus\" (translated as \"most highly powerful and cunning\"). The accusers identified Descartes' concept of a \"deus deceptor\" with his concept of an evil demon, stating that only an omnipotent God is \"summe potens\" and that describing the evil demon as such thus demonstrated the identity. Descartes' response to the accusations was that in that passage he had been expressly distinguishing between \"the supremely good God, the source of truth, on the one hand, and the malicious demon on the other\". He did not directly rebut the charge of implying that the evil demon was omnipotent, but asserted that simply describing something with \"some attribute that in reality belongs only to God\" does not mean that that something is being held to actually \"be\" a supreme God.\n\nAccording to Janowski, \"The alleged distinction between the respective powers of God and the evil genius that escaped the attention of the two theologians also escaped the attention of a host of distinguished Cartesian scholars (Alquié, Beck, Brehier, Chevalier, Frankfurt, Gilson, Kenny, Laporte, Kemp-Smith, Wilson), who, only seldom interested in interpreting Descartes' philosophy through the prism of doctrinal orthodoxy, also insist on the omnipotence of the evil genius.\" He further claims that the reason for this is that there is progression through the \"First Meditation\", leading to the introduction of the concept of the evil genius \"which crowns the process begun at the outset of the Meditations.\"\n\nHowever, it is not quite so straightforward. For example, Wilson notes that \"Gouhier has shown, the hypothesis of the malign spirit takes over from that of the Deceiving God from the end of the First Meditation to the beginning of the Third—where the latter figure is resubstituted without comment or explanation. As Gouhier has also noted, the summary of 'doubts' in the concluding passage ... does not include mention of mathematical propositions—which are not again brought into discussion until the Third Meditation.\" She adds in the accompanying footnote that, even if one has to concede that the text doesn't reveal any sharp distinction between the power hypothetically ascribed to the 'malignant spirit' and that genuinely attributable to God, \"Gouhier's observation is essentially accurate, and useful in understanding the rhetoric and organization of the first three Meditations. It may also have some deeper significance, because of the association ... of the possibility of deception in mathematics with the doctrine of the creation of the eternal truths.\"\n\nSimilarly, Kenny who does say the evil genius is substituted for that of the deceitful God \"simply because it is less offensive and less patently incoherent. The content of the two hypotheses is the same, namely that an omnipotent deceiver is trying to deceive.\" goes on to note that, \"If the two hypostheses differ at all, it is the first that is more skeptical than the second. God...may have made him go wrong in mathematics...the evil genius merely reinforces the doubt that the external world may be a dream.\" When Kenny says the evil genius is simply a substitute for the deceitful God he is not trying to establish that, therefore, the evil genius was omnipotent, instead he is challenging the view that evil genius somehow progressed on from God and is rejecting the view that \"the evil genius is to serve a more radically skeptical purpose than the hypothesis of the deceitful God.\"\n\nAccording to Janowski, the fact that the demon is not said to challenge mathematics, implies either that the evil demon is not omnipotent or that Descartes retracted Universal Doubt. Janowski notes that in the \"Principles of Philosophy\" (I, 15) Descartes states that Universal Doubt applies even to \"the demonstration of mathematics\", and so concludes that either Descartes' \"Meditation\" is flawed, lacking a reason for doubting mathematics, or that the charges of blasphemy were well placed, and Descartes \"was\" supposing an omnipotent evil demon.\n\nHowever, this is only a problem if one assumes that Descartes was withdrawing the notion of a deceitful God and replacing it with the evil demon. More recent commentators take the argument to have reached its conclusion with the deceitful God. When Descartes says, \"I will suppose therefore that not God, who is supremely good and the source of truth, but rather some malicious demon ...\" he is not rejecting the notion of a deceitful God on the grounds that God is not a deceiver for this is something he is not entitled to rely on because, as he says at the beginning of Meditation three, he doesn't \"yet even know for sure whether there is a God at all\". Instead, he is introducing an aid to the meditator who finds that, despite the arguments presented, \"habitual opinions keep coming back\". Kenny says, \"The purpose of taking seriously the hypothesis of the evil genius is to counterbalance natural credulity and keep in mind the doubts raised by the supposition of the deceitful God.\" When the role of the demon is understood this way the issue of the demon's omnipotence becomes unimportant.\n\nIn 1968 James Cornman and Keith Lehrer suggested something they called the braino machine that \"operates by influencing the brain of a subject who wears a special cap, called a \"braino cap.\" When the braino cap is placed on a subject's head, the operator of the braino can affect his brain so as to produce any hallucination in the subject that the operator wishes. The braino is a hallucination-producing machine. The hallucinations produced by it may be as complete, systematic, and coherent as the operator of the braino desires to make them.\" The braino argument was intended to show that, even if it is sometimes possible to tell when we are hallucinating, it is not possible to know that we are not hallucinating. If the braino is operated by an evil being (whom Cornman and Lehrer call Dr. O) then it would be possible for Dr. O to create in me experiences that are identical to the ones I am having now. If that were the case then the experiences so created would not constitute knowledge for the source of those experiences would be the machine and not the world. However, since they are indistinguishable from my current experiences it follows that my current experiences are also insufficient to generate knowledge.\n\nIn 1973 in the introduction to his book \"Thought\", Gilbert Harman said, \"it might be suggested that you have not the slightest reason to believe that you are in the surroundings you suppose you are in ... various hypotheses could explain how things look and feel. You might be sound asleep and dreaming or a playful brain surgeon might be giving you these experiences by stimulating your cortex in a special way. You might really be stretched out on a table in his laboratory with wires running into your head from a large computer. Perhaps you have always been on that table. Perhaps you are quite a different person from what you seem...\"\n\nSuch scenarios had been used many times in science fiction but in philosophy it is now routine to refer to being like a 'brain in a vat' after Hilary Putnam produced an argument which, ironically, purported to show that \"the supposition that we are actually brains in a vat, although it violates no physical law, and is perfectly consistent with everything we have experienced, cannot possibly be true. \"It cannot possibly be true\", because it is, in a certain way, self-refuting.\"\n\nPutnam's argument notwithstanding, the brain in a vat scenario is usually presented as a sceptical argument and in many ways equivalent to Descartes' deceiving God and evil demon.\n\nOne crucial difference that prevents such scenarios being a direct substitute for the deceiving God and evil demon is that they generally presuppose that we have heads or bodies whereas it is important for Descartes to argue that he can doubt the existence of his body and that he can only be sure he is a 'thinking thing'. Harman's version of the story does, however, add the final thought that having a brain \"might be just part of the myth you are being given\".\n\n"}
{"id": "16305358", "url": "https://en.wikipedia.org/wiki?curid=16305358", "title": "Femme nue couchée", "text": "Femme nue couchée\n\nFemme nue couchée () is an 1862 painting by French Realist painter Gustave Courbet (1819–1877). It depicts a young dark-haired woman reclining on a couch, wearing only a pair of shoes and stockings. Behind her, partly drawn red curtains reveal an overcast sky seen through a closed window. The work is likely influenced by Goya's \"La maja desnuda\".\n\nThe painting was initially owned by Alexandre Berthier and later by Marcell Nemes. In 1913, it was bought by the Hungarian collector Ferenc Hatvany. At one time, he painted a copy of the painting and, as a practical joke, sent it to be exhibited as the original at a Belgrade exhibition. Together with the rest of Hatvany's collection, the painting was looted from a Budapest bank vault during the 1945 Soviet conquest of the city in World War II. After it was briefly seen attached to the tarpaulin of a Soviet military vehicle on Buda Castle Hill, the painting appeared to have vanished without a trace.\n\nIt surfaced again in 2000 and 2003, when it was offered for sale first to the Museum of Fine Arts and then to the Commission for Art Recovery (CAR) by a Slovak man claiming to be an antiques dealer, but who appeared to his interlocutors to be involved with the Slovak organised crime scene. The dealer produced an affidavit, judged reliable by the CAR, stating that the painting was given by Soviet soldiers to a doctor from a village near Bratislava in return for his medical treatment of a wounded soldier. An inspection of the picture's craquelure determined that the painting was indeed the original and not Hatvany's copy.\n\nAfter five years of negotiations, Interpol involvement and diplomatic wrangling between the US and Slovakian government, the CAR managed to acquire the painting for Hatvany's heirs in exchange for a USD 300,000 reward. It was shown to the public for the first time since the 1930s in a 2007 Courbet exposition at the Grand Palais in Paris. The painting was sold at auction on 9 November 2015 for 15.3 million U.S. dollars, four times the previous auction record for a Courbet painting.\n\n"}
{"id": "49242908", "url": "https://en.wikipedia.org/wiki?curid=49242908", "title": "Gardner Bishop", "text": "Gardner Bishop\n\nGardner L. Bishop was a barber and civil rights activist in Washington, D.C. His work for equal schools for black and white children in the 1940s and 1950s included organizing the student strike at Browne Junior High School and contributing to the historic \"Bolling\" \"v. Sharpe\" case that made school segregation unconstitutional in the District. Bishop, originally from North Carolina, was known for his outspokenness and his drive to end elitism in the black community. \n\nGardner LaClede Bishop was a barber and activist remembered for his contributions to the school integration movement in the District of Columbia in the 1940s and 1950s. He was born on January 20, 1909 in Rocky Mount, North Carolina. He learned how to express his opinions early on, winning prizes as a high school debater in his home state. Although he attended a year of college at Shaw University in Raleigh, North Carolina, he did not graduate. In 1930 he moved to Washington, D.C. He lived east of the Anacostia River and, like his father, began working as a barber. After challenging the comments of racist customers on multiple occasions, he was fired. Not cowed, he opened B&D Barber Shop at 1515 U Street NW in 1940, which he owned and operated through his retirement in 1985. His black clientele could get both “a haircut and an earful” from the “barber of U Street.” He was a member of the Presbyterian Church and was married for 58 years to Thelma Crutchfield Bishop, who died in 1989. They had three children: Judine Bishop Johnson, Anita Harley, and Gardner L. Bishop Jr. He died of kidney failure on November 25, 1992 at age 82.\n\nBishop focused not only on the inequalities between black schools and white schools, but also on the differences between upper and lower class black schools and attitudes. He was appalled by the snobbishness of the elite black Washingtonians. Part of his motivation for organizing a strike at Browne Junior High School came from his rebuffed requests to transfer his daughter to Banneker Junior High. Middle class black officials informed him that because he was only a barber, his daughter could not go to Banneker, a school meant for the middle class. This rebuke infuriated Bishop, and fueled his mistrust of some of the activist organizations he saw as furthering the agendas of the upper classes of society, including the NAACP. Bishop verbalized his sense of frustration and oppression, saying, “We were on the bottom shelf. I’m black and I’m poor, so I’m segregated twice.”\n\nWhen Bishop came to the nation’s capital, the public school system was segregated, and the facilities for black children and white children were far from equal. As the population of the city expanded in the post World War II years, the schools, particularly the black schools, became overcrowded. Between 1930 and 1950 the African American population alone doubled to 280,000 people, about 35% of D.C.’s total population. The shifting demographics of the city, with whites fleeing to the suburbs and the proportion of the black population steadily increasing, led to an unequal distribution of students amongst the available facilities. Taxes were allocated to fund black schools and white schools based on population statistics collected in the decennial census, so by the end of the 1940s the black schools were receiving far less than their fair share of tax dollars based on the numbers of black children enrolled because of the outdated census data.\n\nGardner Bishop was unwilling to accept a substandard education for his children. His oldest daughter, Judine, attended Browne in 1947. He was outraged by the use of schools that whites had abandoned, not to mention the long walks that students had to take in the middle of their school day to move to a different building. Bishop did not see this shifting schedule as an acceptable solution to the overcrowding. He and other disgruntled parents began holding meetings at Jones Memorial Church to discuss how to present their grievances to the school board. They eventually became known as the Consolidated Parents Group. Bishop took charge as the most visible and vocal member of the group, but came to rely on a set of leaders within the group that grew to have hundreds of members. With Bishop as president, there were two vice presidents, Marie W. Smith and Burma Whitted, a secretary, Unity T. Macklin, and a treasurer, James Haley Sr. At one of their first meetings they voted to stage a student “sit-out” to boycott the situation at Browne Junior High School.\n\nBishop’s organizational efforts did not stop at Browne. In 1949, he took a group of black students to the newly opened Sousa Junior High School, a school for whites with many amenities. The students were given a tour, but they were not allowed to enroll. This incident resulted in a lawsuit, with Spotswood Bolling, one of the students on the tour, as a plaintiff. The case was later argued in front of the Supreme Court with four other cases as part of the historic Brown v. Board of Education decision. Bolling v. Sharpe was unique because it introduced the argument that segregation itself was unconstitutional. Additionally, the NAACP was not involved in the Bolling case because of Bishop’s continued suspicion of middle class black organizations.\n\nBishop’s activism made an impact because his efforts were broad yet focused. He targeted specific injustices in the school system on a variety of fronts. The strike made a social statement and allowed people to see the problem up close. The court case attacked segregation from a legal standpoint, aimed at changing the laws themselves. In addition, Bishop wrote Letters to the Editor of the Washington Post, on topics including school buildings and high school capacity, putting his arguments into print. He was determined to ensure that whites did not become complacent because of the concessions allowed for middle class blacks in the school system. He worked for the lowest members of society, those whose voices and rights were consistently ignored by the established powers. Although Gardner Bishop was not the most prominent civil rights activist of his time, he serves as a reminder that no matter what background people come from, if they speak their minds and fight for what they believe in they can leave a lasting impact.\n\n"}
{"id": "319060", "url": "https://en.wikipedia.org/wiki?curid=319060", "title": "Grand Comoro day gecko", "text": "Grand Comoro day gecko\n\nGrand Comoro day gecko (Phelsuma v-nigra comoraegrandensis ) is a small diurnal subspecies of geckos. It lives in the Comoros and typically inhabits trees and bushes. The Grand Comoro day gecko feeds on insects and nectar.\n\nThis lizard belongs to the smallest day geckos. It can reach a maximum length of approximately 10 cm. The body colour is bright green, which may have a blue hue. There is a red v-shaped stripe on the snout and two red bars between the eyes. On the back there often are a large number of small red-brick coloured dots which may form a faint mid dorsal stripe. The flanks are grey. There is a v-shaped marking on the throat. The ventral side is yellowish white. This lizard also does not have eyelids like all day geckos.\n\nThis species only inhabits the island Grand Comoro in the Comoros.\n\n\"Phelsuma v-nigra comoraegrandensis\" inhabits moist forests, palm trees and human dwellings.\n\nThese day geckos feed on various insects and other invertebrates. They also like to lick soft, sweet fruit, pollen, and nectar.\n\nAt a temperature of 28 °C, the young will hatch after approximately 45 days. The juveniles measure 35 mm.\n\nThese animals should be housed in pairs and need a medium-sized, well planted terrarium. The daytime temperature should be between 28 and 30 °C and 24 and 26 °C at night. The humidity should be around 75–90%. In captivity, these animals can be fed with crickets, wax moth larvae, fruit flies, mealworms, and houseflies.\n\n"}
{"id": "46478160", "url": "https://en.wikipedia.org/wiki?curid=46478160", "title": "Gross National Well-being", "text": "Gross National Well-being\n\nGross National Wellness or Well-being (GNW) is a socioeconomic development and measurement framework. The GNW / GNH Index consists of 7 dimensions: economic, environmental, physical, mental, work, social, and political. Most wellness areas include both subjective results (via survey) and objective data.\n\nThe GNW Index is also known as the first GNH Index or Gross National Happiness Index, not to be confused with Bhutan's GNH Index. Both econometric frameworks are different in authorship, creation dates, and geographic scope. The GNW / GNH index is a global development measurement framework published in 2005 by the International Institute of Management in the United States. \nThe Gross National Happiness phrase was coined in 1972 by Sicco Mansholt, one of the founders of the European Union and the fourth President of the European Commission, and was later popularized by Bhutan in the late 1990s. However, no GNH Index existed until 2005.\n\nThe GNH philosophy suggested that the ideal purpose of governments is to promote happiness. The philosophy remained difficult to implement due to the subjective nature of happiness and the lack of exact quantitative definition of GNH and the lack of a practical model to measure the impact of economic policies on the subjective well-being of the citizens.\n\nThe GNW Index paper proposed the first GNH Index as a solution to help with the implementation of the GHN philosophy and was designed to transform the first generation abstract subjective political mission statement into a second generation implementation holistic (objective and subjective) concept and by treating happiness as a socioeconomic development metric that word provide an alternative to the traditional GDP indicator, the new metric would integrates subjective and objective socioeconomic development policy framework and measurement indicators.\n\nThe GNW Index is a secular econometric model that tracks 7 subjective and objective development areas with no religious measurement components. On the other hand, Bhutan's GNH Index is a local development framework and measurement index, published by the Centre for Bhutan Studies in 2012 based on 2011 Index function designed by Alkire-Foster at Oxford University. The Bhutan's GNH Index is customized to the country's Buddhist cultural and spiritual values, it tracks 9 subjective happiness areas including spiritual measurement such as prayers recitation and other Karma indicators. The concepts and issues at the heart of Bhutanese approach are similar to the secular GNH Index.\n\nIn 2006, a policy white paper providing recommendations for implementing the GNW Index metric was published by the International Institute of Management. The paper is widely referenced by academic and policy maker citing the GNW / GNH index as a potential model for local socioeconomic development and measurement.\n\nThe subjective survey part of the GNW measurement system is structured into seven areas or dimensions.\nEach area or dimension satisfaction rating is scaled from 0–10: 0 being very dissatisfied, 5 being neutral, and 10 is very satisfied.\n\n\nThe survey also asks four qualitative questions to identify key causes of happiness and unhappiness:\n\n\nOne major criticism is that the use of subjective surveys can lead to unreliable conclusions.\n"}
{"id": "2763451", "url": "https://en.wikipedia.org/wiki?curid=2763451", "title": "Hypostasis (philosophy and religion)", "text": "Hypostasis (philosophy and religion)\n\nHypostasis (Greek: ὑπόστασις) is the underlying state or underlying substance and is the fundamental reality that supports all else. In Neoplatonism the hypostasis of the soul, the intellect (\"nous\") and \"the one\" was addressed by Plotinus.\n\nIn Christian theology, a \"hypostasis\" is one of the three \"hypostases\" (Father, Son, Holy Spirit) of the Trinity.\n\nPseudo-Aristotle used \"hypostasis\" in the sense of material substance.\n\nNeoplatonists argue that beneath the surface phenomena that present themselves to our senses are three higher spiritual principles, or hypostases, each one more sublime than the preceding. For Plotinus, these are: the Soul, the Intellect, and the One .\n\nIn early Christian writings, hypostasis is used to denote \"being\" or \"substantive reality\" and is not always distinguished in meaning from \"ousia\" ('essence' or 'substance'). It was used in this way by Tatian and Origen, and also in the anathemas appended to the Nicene Creed of 325.\n\nIt was mainly under the influence of the Cappadocian Fathers that the terminology was clarified and standardized so that the formula \"three hypostases in one ousia\" came to be accepted as an epitome of the orthodox doctrine of the Trinity. Specifically, Basil of Caesarea argues that the two terms are not synonymous and that they, therefore, are not to be used indiscriminately in referring to the godhead. He writes:\nThis consensus, however, was not achieved without some confusion at first in the minds of Western theologians since in the West the vocabulary was different. Many Latin-speaking theologians understood \"hypo-stasis\" as \"sub-stantia\" (substance); thus when speaking of three \"hypostases\" in the godhead, they might suspect three \"substances\" or tritheism. However, from the middle of the fifth century onwards, marked by Council of Chalcedon, the word came to be contrasted with \"ousia\" and used to mean \"individual reality,\" especially in the trinitarian and Christological contexts. The Christian concept of the Trinity is often described as being one god existing in three distinct \"hypostases/personae/persons\".\n\n"}
{"id": "23292380", "url": "https://en.wikipedia.org/wiki?curid=23292380", "title": "Information hiding", "text": "Information hiding\n\nIn computer science, information hiding is the principle of segregation of the \"design decisions\" in a computer program that are most likely to change, thus protecting other parts of the program from extensive modification if the design decision is changed. The protection involves providing a stable interface which protects the remainder of the program from the implementation (the details that are most likely to change).\n\nWritten another way, information hiding is the ability to prevent certain aspects of a class or software component from being accessible to its clients, using either programming language features (like private variables) or an explicit exporting policy.\n\nThe term \"encapsulation\" is often used interchangeably with information hiding. Not all agree on the distinctions between the two though; one may think of information hiding as being the principle and encapsulation being the technique. A software module hides information by encapsulating the information into a module or other construct which presents an interface.\n\nA common use of information hiding is to hide the physical storage layout for data so that if it is changed, the change is restricted to a small subset of the total program. For example, if a three-dimensional point (\"x\",\"y\",\"z\") is represented in a program with three floating point scalar variables and later, the representation is changed to a single array variable of size three, a module designed with information hiding in mind would protect the remainder of the program from such a change.\n\nIn object-oriented programming, information hiding (by way of nesting of types) reduces software development risk by shifting the code's dependency on an uncertain implementation (design decision) onto a well-defined interface. Clients of the interface perform operations purely through it so if the implementation changes, the clients do not have to change.\n\nIn his book on object-oriented design, Grady Booch defined encapsulation as \"the process of compartmentalizing the elements of an abstraction that constitute its structure and behavior; encapsulation serves to separate the contractual interface of an abstraction and its implementation.\"\n\nThe purpose is to achieve potential for change: the internal mechanisms of the component can be improved without impact on other components, or the component can be replaced with a different one that supports the same public interface. Encapsulation also protects the integrity of the component, by preventing users from setting the internal data of the component into an invalid or inconsistent state. Another benefit of encapsulation is that it reduces system complexity and thus increases robustness, by limiting the interdependencies between software components.\n\nIn this sense, the idea of encapsulation is more general than how it is applied in OOP: for example, a relational database is encapsulated in the sense that its only public interface is a Query language (SQL for example), which hides all the internal machinery and data structures of the database management system. As such, encapsulation is a core principle of good software architecture, at every level of granularity.\n\nEncapsulating software behind an interface allows the construction of objects that mimic the behavior and interactions of objects in the real world. For example, a simple digital alarm clock is a real-world object that a lay person can use and understand. They can understand what the alarm clock does, and how to use it through the provided interface (buttons and screen), without having to understand every part inside of the clock. Similarly, if the clock were replaced by a different model, the lay person could continue to use it in the same way, provided that the interface works the same.\n\nIn the more concrete setting of an object-oriented programming language, the notion is used to mean either an information hiding mechanism, a bundling mechanism, or the combination of the two. (See Encapsulation (object-oriented programming) for details.)\n\nThe concept of information hiding was first described by David Parnas in . Before then, modularity was discussed by Richard Gauthier and Stephen Pont in their 1970 book \"Designing Systems Programs\" although modular programming itself had been used at many commercial sites for many years previously – especially in I/O sub-systems and software libraries – without acquiring the 'information hiding' tag – but for similar reasons, as well as the more obvious code reuse reason.\n\nInformation hiding serves as an effective criterion for dividing any piece of equipment, software or hardware, into modules of functionality. For instance, a car is a complex piece of equipment. In order to make the design, manufacturing, and maintenance of a car reasonable, the complex piece of equipment is divided into modules with particular interfaces hiding design decisions. By designing a car in this fashion, a car manufacturer can also offer various options while still having a vehicle which is economical to manufacture.\n\nFor instance, a car manufacturer may have a luxury version of the car as well as a standard version. The luxury version comes with a more powerful engine than the standard version. The engineers designing the two different car engines, one for the luxury version and one for the standard version, provide the same interface for both engines. Both engines fit into the engine bay of the car which is the same between both versions. Both engines fit the same transmission, the same engine mounts, and the same controls. The differences in the engines are that the more powerful luxury version has a larger displacement with a fuel injection system that is programmed to provide the fuel air mixture that the larger displacement engine requires.\n\nIn addition to the more powerful engine, the luxury version may also offer other options such as a better radio with CD player, more comfortable seats, a better suspension system with wider tires, and different paint colors. With all of these changes, most of the car is the same between the standard version and the luxury version. The radio with CD player is a module which replaces the standard radio, also a module, in the luxury model. The more comfortable seats are installed into the same seat mounts as the standard types of seats. Whether the seats are leather or plastic, or offer lumbar support or not, doesn't matter.\n\nThe engineers design the car by dividing the task up into pieces of work which are assigned to teams. Each team then designs their component to a particular standard or interface which allows the team flexibility in the design of the component while at the same time ensuring that all of the components will fit together.\n\nMotor vehicle manufacturers frequently use the same core structure for several different models, in part as a cost-control measure. Such a \"platform\" also provides an example of information hiding, since the floorplan can be built without knowing whether it is to be used in a sedan or a hatchback.\n\nAs can be seen by this example, information hiding provides flexibility. This flexibility allows a programmer to modify the functionality of a computer program during normal evolution as the computer program is changed to better fit the needs of users. When a computer program is well designed decomposing the source code solution into modules using the principle of information hiding, evolutionary changes are much easier because the changes typically are local rather than global changes.\n\nCars provide another example of this in how they interface with drivers. They present a standard interface (pedals, wheel, shifter, signals, gauges, etc.) on which people are trained and licensed. Thus, people only have to learn to drive a car; they don't need to learn a completely different way of driving every time they drive a new model. (Granted, there are manual and automatic transmissions and other such differences, but on the whole, cars maintain a unified interface.)\n\n\n"}
{"id": "758680", "url": "https://en.wikipedia.org/wiki?curid=758680", "title": "Inner Party", "text": "Inner Party\n\nIn the world of George Orwell's \"Nineteen Eighty-Four\", Oceania is split into three \"classes\": the Inner Party, the Outer Party and the proles. The Inner Party generally regulates Ingsoc and the Thought Police, and keeps all Outer Party members under close supervision through technologies like telescreen, while the proles live in extremely depressed, but unmonitored conditions.\n\nThe Inner Party represents the oligarchical political class in Oceania. It is generally represented by Big Brother. Inner Party members enjoy a quality of life that is much better than that of the Outer Party members and the proles. The telescreens in their homes can ostensibly be turned off indefinitely (although O'Brien tells Winston and Julia it is unwise to turn it off more than 30 minutes at a time); however, this revelation may be a lie to Winston and Julia, as conversations they had after O'Brien turned off his telescreen were later played back to them anyway. \n\nInner Party members also have access to spacious living quarters, personal servants, private motor vehicles (motor vehicles are highly restricted and not allowed to Outer Party or Proles), and high quality food, drink and consumer goods in contrast to the low quality gin, synthetic coffee and improperly manufactured cigarettes consumed by the Outer Party and the Proles. Inner Party members have access to wine, as well as real coffee, tea, sugar, milk and well-made cigarettes. Inner Party neighbourhoods are kept clean and presentable, compared to Outer Party and Prole neighbourhoods.\n\nProspective members of the Inner Party are selected at a young age according to a series of tests; racial origin and family heritage are of no importance in this process as long as their loyalty is proven. Goldstein's book states that a child born to Inner Party parents is not automatically born into the Inner Party and that all racial groups in Oceania, including \"Jews, negroes and South Americans of pure Indian blood\" are represented in the ranks. Visually, Inner Party members are always identifiable in public by their black jumpsuits. No Outer Party member or Prole may venture into Inner Party neighbourhoods without permission from an Inner Party member. \n\nIn the novel, O'Brien is the only character Winston meets who is a member of the Inner Party.\n\nGoldstein's book explains the rationale behind the class divisions in Oceania, but the book is found to be made by an Inner Party committee that O'Brien was a part of.\n\n"}
{"id": "11367563", "url": "https://en.wikipedia.org/wiki?curid=11367563", "title": "Institute for Complex Adaptive Matter", "text": "Institute for Complex Adaptive Matter\n\nThe Institute for Complex Adaptive Matter (ICAM) is an international multicampus collective of scientists studying emergent phenomena in biology, chemistry and physics and in wider context. ICAM was founded in 1999 at the University of California by Nobel laureate Robert B. Laughlin and physicist David Pines and since 2004 has received funding from the National Science Foundation.\n\n"}
{"id": "291912", "url": "https://en.wikipedia.org/wiki?curid=291912", "title": "Introduction to gauge theory", "text": "Introduction to gauge theory\n\nA gauge theory is a type of theory in physics. The word gauge means a measurement, a thickness, an in-between distance, (as in railroad tracks) or a resulting number of units per certain parameter (a number of loops in an inch of fabric or a number of lead balls in a pound of ammunition). Modern theories describe physical forces in terms of fields, e.g., the electromagnetic field, the gravitational field, and fields that describe forces between the elementary particles. A general feature of these field theories is that the fundamental fields cannot be directly measured; however, some associated quantities can be measured, such as charges, energies, and velocities. For example, say you cannot measure the diameter of a lead ball, but you can determine how many lead balls, which are equal in every way, are required to make a pound. Using the number of balls, the elemental mass of lead, and the formula for calculating the volume of a sphere from its diameter, one could indirectly determine the diameter of a single lead ball. In field theories, different configurations of the unobservable fields can result in identical observable quantities. A transformation from one such field configuration to another is called a gauge transformation; the lack of change in the measurable quantities, despite the field being transformed, is a property called gauge invariance. For example, if you could measure the color of lead balls and discover that when you change the color, you still fit the same number of balls in a pound, the property of \"color\" would show gauge invariance. Since any kind of invariance under a field transformation is considered a symmetry, gauge invariance is sometimes called gauge symmetry. Generally, any theory that has the property of gauge invariance is considered a gauge theory.\n\nFor example, in electromagnetism the electric and magnetic fields, E and B are observable, while the potentials \"V\" (\"voltage\") and A (the vector potential) are not. Under a gauge transformation in which a constant is added to \"V\", no observable change occurs in E or B.\n\nWith the advent of quantum mechanics in the 1920s, and with successive advances in quantum field theory, the importance of gauge transformations has steadily grown. Gauge theories constrain the laws of physics, because all the changes induced by a gauge transformation have to cancel each other out when written in terms of observable quantities. Over the course of the 20th century, physicists gradually realized that all forces (fundamental interactions) arise from the constraints imposed by \"local\" gauge symmetries, in which case the transformations vary from point to point in space and time. Perturbative quantum field theory (usually employed for scattering theory) describes forces in terms of force-mediating particles called gauge bosons. The nature of these particles is determined by the nature of the gauge transformations. The culmination of these efforts is the Standard Model, a quantum field theory that accurately predicts all of the fundamental interactions except gravity.\n\nThe earliest field theory having a gauge symmetry was Maxwell's formulation, in 1864–65, of electrodynamics (\"A Dynamical Theory of the Electromagnetic Field\"). The importance of this symmetry remained unnoticed in the earliest formulations. Similarly unnoticed, Hilbert had derived Einstein's equations of general relativity by postulating a symmetry under any change of coordinates. Later Hermann Weyl, inspired by success in Einstein's general relativity, conjectured (incorrectly, as it turned out) in year 1919 that invariance under the change of scale or \"gauge\" (a term inspired by the various track gauges of railroads) might also be a local symmetry of electromagnetism. Although Weyl's choice of the gauge was incorrect, the name \"gauge\" stuck to the approach. After the development of quantum mechanics, Weyl, Fock and London modified their gauge choice by replacing the scale factor with a change of wave phase, and applying it successfully to electromagnetism. Gauge symmetry was generalized mathematically in 1954 by Chen Ning Yang and Robert Mills in an attempt to describe the strong nuclear forces. This idea, dubbed Yang–Mills theory, later found application in the quantum field theory of the weak force, and its unification with electromagnetism in the electroweak theory.\n\nThe importance of gauge theories for physics stems from their tremendous success in providing a unified framework to describe the quantum-mechanical behavior of electromagnetism, the weak force and the strong force. This gauge theory, known as the Standard Model, accurately describes experimental predictions regarding three of the four fundamental forces of nature.\n\nHistorically, the first example of gauge symmetry to be discovered was classical electromagnetism. A static electric field can be described in terms of an electric potential (voltage) that is defined at every point in space, and in practical work it is conventional to take the Earth as a physical reference that defines the zero level of the potential, or ground. But only \"differences\" in potential are physically measurable, which is the reason that a voltmeter must have two probes, and can only report the voltage difference between them. Thus one could choose to define all voltage differences relative to some other standard, rather than the Earth, resulting in the addition of a constant offset. If the potential formula_1 is a solution to Maxwell's equations then, after this gauge transformation, the new potential formula_2 is also a solution to Maxwell's equations and no experiment can distinguish between these two solutions. In other words, the laws of physics governing electricity and magnetism (that is, Maxwell equations) are invariant under gauge transformation. Maxwell's equations have a gauge symmetry.\n\nGeneralizing from static electricity to electromagnetism, we have a second potential, the magnetic vector potential A, which can also undergo gauge transformations. These transformations may be local. That is, rather than adding a constant onto \"V\", one can add a function that takes on different values at different points in space and time. If A is also changed in certain corresponding ways, then the same E and B fields result. The detailed mathematical relationship between the fields E and B and the potentials \"V\" and A is given in the article Gauge fixing, along with the precise statement of the nature of the gauge transformation. The relevant point here is that the fields remain the same under the gauge transformation, and therefore Maxwell's equations are still satisfied.\n\nGauge symmetry is closely related to charge conservation. Suppose that there existed some process by which one could briefly violate conservation of charge by creating a charge \"q\" at a certain point in space, 1, moving it to some other point 2, and then destroying it. We might imagine that this process was consistent with conservation of energy. We could posit a rule stating that creating the charge required an input of energy \"E\"=\"qV\" and destroying it released \"E\"=\"qV\", which would seem natural since \"qV\" measures the extra energy stored in the electric field because of the existence of a charge at a certain point. Outside of the interval during which the particle exists, conservation of energy would be satisfied, because the net energy released by creation and destruction of the particle, \"qV\"-\"qV\", would be equal to the work done in moving the particle from 1 to 2, \"qV\"-\"qV\". But although this scenario salvages conservation of energy, it violates gauge symmetry. Gauge symmetry requires that the laws of physics be invariant under the transformation formula_2, which implies that no experiment should be able to measure the absolute potential, without reference to some external standard such as an electrical ground. But the proposed rules \"E\"=\"qV\" and \"E\"=\"qV\" for the energies of creation and destruction \"would\" allow an experimenter to determine the absolute potential, simply by comparing the energy input required to create the charge \"q\" at a particular point in space in the case where the potential is formula_1 and formula_5 respectively. The conclusion is that if gauge symmetry holds, and energy is conserved, then charge must be conserved.\n\nAs discussed above, the gauge transformations for classical (i.e., non-quantum mechanical) general relativity are arbitrary coordinate transformations. Technically, the transformations must be invertible, and both the transformation and its inverse must be smooth, in the sense of being differentiable an arbitrary number of times.\n\nSome global symmetries under changes of coordinate predate both general relativity and the concept of a gauge. For example, Galileo and Newton introduced the notion of translation invariance, an advancement from the Aristotelian concept that different places in space, such as the earth versus the heavens, obeyed different physical rules.\n\nSuppose, for example, that one observer examines the properties of a hydrogen atom on Earth, the other—on the Moon (or any other place in the universe), the observer will find that their hydrogen atoms exhibit completely identical properties. Again, if one observer had examined a hydrogen atom today and the other—100 years ago (or any other time in the past or in the future), the two experiments would again produce completely identical results. The invariance of the properties of a hydrogen atom with respect to the time and place where these properties were investigated is called translation invariance.\n\nRecalling our two observers from different ages: the time in their experiments is shifted by 100 years. If the time when the older observer did the experiment was \"t\", the time of the modern experiment is \"t\"+100 years. Both observers discover the same laws of physics. Because light from hydrogen atoms in distant galaxies may reach the earth after having traveled across space for billions of years, in effect one can do such observations covering periods of time almost all the way back to the Big Bang, and they show that the laws of physics have always been the same.\n\nIn other words, if in the theory we change the time \"t\" to \"t\"+100 years (or indeed any other time shift) the theoretical predictions do not change.\n\nIn Einstein's general relativity, coordinates like \"x\", \"y\", \"z\", and \"t\" are not only \"relative\" in the global sense of translations like formula_6, rotations, etc., but become completely arbitrary, so that, for example, one can define an entirely new time-like coordinate according to some arbitrary rule such as formula_7, where formula_8 has units of time, and yet Einstein's equations will have the same form.\n\nInvariance of the form of an equation under an arbitrary coordinate transformation is customarily referred to as general covariance, and equations with this property are referred to as written in the covariant form. General covariance is a special case of gauge invariance.\n\nMaxwell's equations can also be expressed in a generally covariant form, which is as invariant under general coordinate transformation as Einstein's field equation.\n\nUntil the advent of quantum mechanics, the only well known example of gauge symmetry was in electromagnetism, and the general significance of the concept was not fully understood. For example, it was not clear whether it was the fields E and B or the potentials V and A that were the fundamental quantities; if the former, then the gauge transformations could be considered as nothing more than a mathematical trick.\n\nIn quantum mechanics, a particle such as an electron is also described as a wave. For example, if the double-slit experiment is performed with electrons, then a wave-like interference pattern is observed. The electron has the highest probability of being detected at locations where the parts of the wave passing through the two slits are in phase with one another, resulting in constructive interference. The frequency of the electron \"wave\" is related to the kinetic energy of an individual electron \"particle\" via the quantum-mechanical relation \"E\" = \"hf\". If there are no electric or magnetic fields present in this experiment, then the electron's energy is constant, and, for example, there will be a high probability of detecting the electron along the central axis of the experiment, where by symmetry the two parts of the wave are in phase.\n\nBut now suppose that the electrons in the experiment are subject to electric or magnetic fields. For example, if an electric field was imposed on one side of the axis but not on the other, the results of the experiment would be affected. The part of the electron wave passing through that side oscillates at a different rate, since its energy has had −\"eV\" added to it, where −\"e\" is the charge of the electron and \"V\" the electrical potential. The results of the experiment will be different, because phase relationships between the two parts of the electron wave have changed, and therefore the locations of constructive and destructive interference will be shifted to one side or the other. It is the electric potential that occurs here, not the electric field, and this is a manifestation of the fact that it is the potentials and not the fields that are of fundamental significance in quantum mechanics.\n\nIt is even possible to have cases in which an experiment's results differ when the potentials are changed, even if no charged particle is ever exposed to a different field. One such example is the Aharonov–Bohm effect, shown in the figure. In this example, turning on the solenoid only causes a magnetic field B to exist within the solenoid. But the solenoid has been positioned so that the electron cannot possibly pass through its interior. If one believed that the fields were the fundamental quantities, then one would expect that the results of the experiment would be unchanged. In reality, the results are different, because turning on the solenoid changed the vector potential A in the region that the electrons do pass through. Now that it has been established that it is the potentials V and A that are fundamental, and not the fields E and B, we can see that the gauge transformations, which change V and A, have real physical significance, rather than being merely mathematical artifacts.\n\nNote that in these experiments, the only quantity that affects the result is the \"difference\" in phase between the two parts of the electron wave. Suppose we imagine the two parts of the electron wave as tiny clocks, each with a single hand that sweeps around in a circle, keeping track of its own phase. Although this cartoon ignores some technical details, it retains the physical phenomena that are important here. If both clocks are sped up by the same amount, the phase relationship between them is unchanged, and the results of experiments are the same. Not only that, but it is not even necessary to change the speed of each clock by a \"fixed\" amount. We could change the angle of the hand on each clock by a \"varying\" amount θ, where θ could depend on both the position in space and on time. This would have no effect on the result of the experiment, since the final observation of the location of the electron occurs at a single place and time, so that the phase shift in each electron's \"clock\" would be the same, and the two effects would cancel out. This is another example of a gauge transformation: it is local, and it does not change the results of experiments.\n\nIn summary, gauge symmetry attains its full importance in the context of quantum mechanics. In the application of quantum mechanics to electromagnetism, i.e., quantum electrodynamics, gauge symmetry applies to both electromagnetic waves and electron waves. These two gauge symmetries are in fact intimately related. If a gauge transformation θ is applied to the electron waves, for example, then one must also apply a corresponding transformation to the potentials that describe the electromagnetic waves. Gauge symmetry is required in order to make quantum electrodynamics a renormalizable theory, i.e., one in which the calculated predictions of all physically measurable quantities are finite.\n\nThe description of the electrons in the subsection above as little clocks is in effect a statement of the mathematical rules according to which the phases of electrons are to be added and subtracted: they are to be treated as ordinary numbers, except that in the case where the result of the calculation falls outside the range of 0≤θ<360°, we force it to \"wrap around\" into the allowed range, which covers a circle. Another way of putting this is that a phase angle of, say, 5° is considered to be completely equivalent to an angle of 365°. Experiments have verified this testable statement about the interference patterns formed by electron waves. Except for the \"wrap-around\" property, the algebraic properties of this mathematical structure are exactly the same as those of the ordinary real numbers.\n\nIn mathematical terminology, electron phases form an Abelian group under addition, called the circle group or \"U\"(1). \"Abelian\" means that addition commutes, so that θ + φ = φ + θ. Group means that addition associates and has an identity element, namely \"0\". Also, for every phase there exists an inverse such that the sum of a phase and its inverse is 0. Other examples of abelian groups are the integers under addition, 0, and negation, and the nonzero fractions under product, 1, and reciprocal.\nAs a way of visualizing the choice of a gauge, consider whether it is possible to tell if a cylinder has been twisted. If the cylinder has no bumps, marks, or scratches on it, we cannot tell. We could, however, draw an arbitrary curve along the cylinder, defined by some function θ(\"x\"), where \"x\" measures distance along the axis of the cylinder. Once this arbitrary choice (the choice of gauge) has been made, it becomes possible to detect it if someone later twists the cylinder.\n\nIn 1954, Chen Ning Yang and Robert Mills proposed to generalize these ideas to noncommutative groups. A noncommutative gauge group can describe a field that, unlike the electromagnetic field, interacts with itself. For example, general relativity states that gravitational fields have energy, and special relativity concludes that energy is equivalent to mass. Hence a gravitational field induces a further gravitational field. The nuclear forces also have this self-interacting property.\n\nSurprisingly, gauge symmetry can give a deeper explanation for the existence of interactions, such as the electric and nuclear interactions. This arises from a type of gauge symmetry relating to the fact that all particles of a given type are experimentally indistinguishable from one another. Imagine that Alice and Betty are identical twins, labeled at birth by bracelets reading A and B. Because the girls are identical, nobody would be able to tell if they had been switched at birth; the labels A and B are arbitrary, and can be interchanged. Such a permanent interchanging of their identities is like a global gauge symmetry. There is also a corresponding local gauge symmetry, which describes the fact that from one moment to the next, Alice and Betty could swap roles while nobody was looking, and nobody would be able to tell. If we observe that Mom's favorite vase is broken, we can only infer that the blame belongs to one twin or the other, but we cannot tell whether the blame is 100% Alice's and 0% Betty's, or vice versa. If Alice and Betty are in fact quantum-mechanical particles rather than people, then they also have wave properties, including the property of superposition, which allows waves to be added, subtracted, and mixed arbitrarily. It follows that we are not even restricted to complete swaps of identity. For example, if we observe that a certain amount of energy exists in a certain location in space, there is no experiment that can tell us whether that energy is 100% A's and 0% B's, 0% A's and 100% B's, or 20% A's and 80% B's, or some other mixture. The fact that the symmetry is local means that we cannot even count on these proportions to remain fixed as the particles propagate through space. The details of how this is represented mathematically depend on technical issues relating to the spins of the particles, but for our present purposes we consider a spinless particle, for which it turns out that the mixing can be specified by some arbitrary choice of gauge θ(\"x\"), where an angle θ = 0° represents 100% A and 0% B, θ = 90° means 0% A and 100% B, and intermediate angles represent mixtures.\n\nAccording to the principles of quantum mechanics, particles do not actually have trajectories through space. Motion can only be described in terms of waves, and the momentum \"p\" of an individual particle is related to its wavelength λ by \"p\" = \"h\"/\"λ\". In terms of empirical measurements, the wavelength can only be determined by observing a change in the wave between one point in space and another nearby point (mathematically, by differentiation). A wave with a shorter wavelength oscillates more rapidly, and therefore changes more rapidly between nearby points. Now suppose that we arbitrarily fix a gauge at one point in space, by saying that the energy at that location is 20% A's and 80% B's. We then measure the two waves at some other, nearby point, in order to determine their wavelengths. But there are two entirely different reasons that the waves could have changed. They could have changed because they were oscillating with a certain wavelength, or they could have changed because the gauge function changed from a 20-80 mixture to, say, 21-79. If we ignore the second possibility, the resulting theory doesn't work; strange discrepancies in momentum will show up, violating the principle of conservation of momentum. Something in the theory must be changed.\n\nAgain there are technical issues relating to spin, but in several important cases, including electrically charged particles and particles interacting via nuclear forces, the solution to the problem is to impute physical reality to the gauge function θ(\"x\"). We say that if the function θ oscillates, it represents a new type of quantum-mechanical wave, and this new wave has its own momentum \"p\" = \"h\"/\"λ\", which turns out to patch up the discrepancies that otherwise would have broken conservation of momentum. In the context of electromagnetism, the particles A and B would be charged particles such as electrons, and the quantum mechanical wave represented by θ would be the electromagnetic field. (Here we ignore the technical issues raised by the fact that electrons actually have spin 1/2, not spin zero. This oversimplification is the reason that the gauge field θ comes out to be a scalar, whereas the electromagnetic field is actually represented by a vector consisting of \"V\" and A.) The result is that we have an explanation for the presence of electromagnetic interactions: if we try to construct a gauge-symmetric theory of identical, non-interacting particles, the result is not self-consistent, and can only be repaired by adding electric and magnetic fields that cause the particles to interact.\n\nAlthough the function θ(\"x\") describes a wave, the laws of quantum mechanics require that it also have particle properties. In the case of electromagnetism, the particle corresponding to electromagnetic waves is the photon. In general, such particles are called gauge bosons, where the term \"boson\" refers to a particle with integer spin. In the simplest versions of the theory, gauge bosons are massless, but it is also possible to construct versions in which they have mass, as is the case for the gauge bosons that transmit the nuclear decay forces.\n\nThese books are intended for general readers and employ the barest minimum of mathematics.\n"}
{"id": "15014170", "url": "https://en.wikipedia.org/wiki?curid=15014170", "title": "Judgment (mathematical logic)", "text": "Judgment (mathematical logic)\n\nIn mathematical logic, a judgment (or judgement) or assertion is a statement or enunciation in the metalanguage. For example, typical judgments in first-order logic would be \"that a string is a well-formed formula\", or \"that a proposition is true\". Similarly, a judgment may assert the occurrence of a free variable in an expression of the object language, or the provability of a proposition. In general, a judgment may be any inductively definable assertion in the metatheory.\n\nJudgments are used in formalizing deduction systems: a logical axiom expresses a judgment, premises of a rule of inference are formed as a sequence of judgments, and their conclusion is a judgment as well (thus, hypotheses and conclusions of proofs are judgments). A characteristic feature of the variants of Hilbert-style deduction systems is that the \"context\" is not changed in any of their rules of inference, while both natural deduction and sequent calculus contain some context-changing rules. Thus, if we are interested only in the derivability of tautologies, not hypothetical judgments, then we can formalize the Hilbert-style deduction system in such a way that its rules of inference contain only judgments of a rather simple form. The same cannot be done with the other two deductions systems: as context is changed in some of their rules of inferences, they cannot be formalized so that hypothetical judgments could be avoided—not even if we want to use them just for proving derivability of tautologies.\n\nThis basic diversity among the various calculi allows such difference, that the same basic thought (e.g. deduction theorem) must be proven as a metatheorem in Hilbert-style deduction system, while it can be declared explicitly as a rule of inference in natural deduction.\n\nIn type theory, some analogous notions are used as in mathematical logic (giving rise to connections between the two fields, e.g. Curry-Howard correspondence). The abstraction in the notion of \"judgment\" in mathematical logic can be exploited also in foundation of type theory as well.\n\nIn logic, logical assertion is a statement that asserts that a certain premise is true, and is useful for statements in proof. It is equivalent to a sequent with an empty antecedent.\n\nFor example, if \"p\" = \"\"x\" is even\", the implication\nis thus true. We can also write this using the logical assertion symbol, as\n\nIn computer programming and programming language semantics, these are used in the form of assertions; one example is a loop invariant.\n\n\n\n"}
{"id": "41953539", "url": "https://en.wikipedia.org/wiki?curid=41953539", "title": "Knowledge of human nature", "text": "Knowledge of human nature\n\nKnowledge of human nature is the ability to correctly assess the behavior or character of people based on a first impression, and to gauge how they think and predict how they will act.\n\nLife experience, intuition, intelligence, and wisdom are the decisive factors which contribute to this ability. Knowledge of human nature is not innate, but is acquired through frequent contact with people and experience with many different people.\n\nKnowledge of human nature can be used to judge people correctly, to motivate them, to give other people good advice, to deepen relationships, etc. However, it can also be used to exploit people to one's own advantage, for example, if one wants to persuade people, to seduce them, or to sell them something.\n\nThere are numerous models for the theoretical acquisition of knowledge of human nature, such as the Myers-Briggs Type Indicator or the Enneagram.\n\n\n"}
{"id": "158925", "url": "https://en.wikipedia.org/wiki?curid=158925", "title": "Maxims of equity", "text": "Maxims of equity\n\nMaxims of equity are legal maxims that serve as a set of general principles or rules which are said to govern the way in which equity operates. They tend to illustrate the qualities of equity, in contrast to the common law, as a more flexible, responsive approach to the needs of the individual, inclined to take into account the parties’ conduct and worthiness. They were developed by the English Court of Chancery and other courts that administer equity jurisdiction, including the law of trusts. Although the most fundamental and time honored of the maxims, listed on this page, are often referred to on their own as the 'maxims of equity' or 'the equitable maxims', it cannot be said that there is a definitive list of them. Like other kinds of legal maxims or principles, they were originally, and sometimes still are, expressed in Latin.\n\nMaxims of equity are not a rigid set of rules, but are, rather, general principles which can be deviated from in specific cases. Snell's \"Equity\", an English treatise, takes the view that the \"Maxims do not cover the whole ground, and moreover they overlap, one maxim contains by implication what belongs to another. Indeed it would not be difficult to reduce all under two: 'Equity will not suffer a wrong to be without a remedy' and 'Equity acts on the person'\".\n\nSometimes phrased as \"equity regards as done what should have been done\", this maxim means that when individuals are required, by their agreements or by law, to perform some act of legal significance, equity will regard that act as having been done as it ought to have been done, even before it has actually happened. This makes possible the legal phenomenon of equitable conversion.\n\nThe consequences of this maxim, and of equitable conversion, are significant in their bearing on the risk of loss in transactions. When parties enter a contract for a sale of real property, the buyer is deemed to have obtained an equitable right that becomes a legal right only after the deal is completed.\n\nDue to his equitable interest in the outcome of the transaction, the buyer who suffers a breach may be entitled to the equitable remedy of specific performance (although not always, see below). If he is successful in seeking a remedy at law, he is entitled to the value of the property at the time of breach regardless of whether it has appreciated or depreciated.\n\nThe fact that the buyer may be forced to suffer a depreciation in the value of the property means that he bears the risk of loss if, for example, the improvements on the property he bought burn down while he is still in escrow.\n\nProblems may sometimes arise because, through some lapse or omission, insurance coverage is not in force at the time a claim is made. If the policyholder has clearly been at fault in this connection, because, for example, he has not paid premiums when he should have, then it will normally be quite reasonable for an insurer to decline to meet the claim. However, it gets more difficult if the policyholder is no more at fault than the insurer. The fair solution in the circumstances may be arrived at by applying the principle that equity regards that as done that ought to be done. In other words, what would the position have been if what should have been done had been done?\n\nThus, we know in one case, premiums on a life insurance policy were overdue. The insurer's letter to the policyholder warning him of this fact was never received by the policyholder, who died shortly after the policy consequently lapsed. It was clear that if the notice had been received by the policyholder, he or his wife would have taken steps to ensure the policy continued in force, because the policyholder was terminally ill at the time and the coverage provided by the policy was something his wife was plainly going to require in the foreseeable future. Since the policyholder would have been fully entitled to pay the outstanding premium at that stage, regardless of his physical condition, the insurer (with some persuasion from the Bureau) agreed that the matter should be dealt with as if the policyholder had done so. In other words, his widow was entitled to the sum assured less the outstanding premium. In other similar cases, however, it has not been possible to follow the same principle because there has not been sufficiently clear evidence that the policy would have been renewed.\n\nAnother illustration of the application of this equitable principle was in connection with motor vehicle insurance. A policyholder was provided with coverage on the basis that she was entitled to a \"no claims\" discount from her previous insurer. Confirmation to this effect from the previous insurer was required. When that was not forthcoming, her coverage was cancelled by the brokers who had issued the initial coverage note. This was done without reference to the insurer concerned whose normal practice in such circumstances would have been to maintain coverage and to require payment of the full premium until proof of the no claims discount was forthcoming. Such proof was eventually obtained by the policyholder, but only after she had been involved in an accident after the cancellation by the brokers of the policy. Here again, the fair outcome was to look at what would have happened if the insurer's normal practice had been followed. In such circumstances, the policyholder would plainly have still had a policy at the time of the accident. The insurer itself had not acted incorrectly at any stage. However, in the circumstances, it was equitable for it to meet the claim.\n\nWhen seeking an equitable relief, the one that has been wronged has the stronger hand. The stronger hand is the one that has the capacity to ask for a legal remedy (judicial relief). In equity, this form of remedy is usually one of specific performance or an injunction (injunctive relief). These are superior remedies to those administered at common law such as damages. The Latin legal maxim is \"ubi jus ibi remedium\" (\"where there is a wrong, there must be a remedy\"), sometimes cited as ubi jus ibi remediam.\n\nThe maxim is necessarily subordinate to positive principles and cannot be applied either to subvert established rules of law or to give the courts a jurisdiction hitherto unknown, and it is only in a general not in a literal sense that the maxim has force.\n\nCase law dealing with the principle of this maxim at law include \"Ashby v White\" and \"Bivens v. Six Unknown Named Agents\". The application of this principle at law was key in the decision of \"Marbury v. Madison\", wherein it was necessary to establish that Marbury had a right to his commission in the first place in order for Chief Justice Marshall to make his more wide-ranging decision.\n\nWhere two persons have an equal right, the property will be divided equally. \n\nThis maxim flows from the fundamental notion of equality or impartiality due to the conception of Equity and is the source of many equitable doctrines. The maxim is of very wide application. The rule of ordinary law may give one party an advantage over the other. But, the court of equity where it can, put the litigating parties on a footing of equality. Equity proceeds in the principle that a right or liability should as far as possible, be equalized among all interested.\nIn the Simple Word's then Two parties have equal Rights in any of property so the same was distributed equal as per their concerned Law.\n\nTo receive equitable relief, the petitioning party must be willing to complete all of its own obligations as well. The applicant to a court of equity is just as much subject to the power of that court as the defendant. This maxim may also overlap with the clean hands maxim (see below).\n\n\"Vigilantibus non dormientibus aequitas subvenit\".\n\nA person who has been wronged must act relatively swiftly to preserve their rights. Otherwise, they are guilty of laches, an untoward delay in litigation with the presumed intent of denying claims. This differs from a statute of limitations, in that a delay is particularized to individual situations, rather than a general prescribed legal amount of time. In addition, even where a limitation period has not yet run, laches may still occur. The equitable rule of laches and acquiescence was first introduced in \"Chief Young Dede v. African Association Ltd\"\n\nAlternatives:\n\nGenerally speaking, near performance of a general obligation will be treated as sufficient unless the law requires perfect performance, such as in the exercise of an option. Text writers give an example of a debtor leaving a legacy to his creditor equal to or greater than his obligation. Equity regards such a gift as performance of the obligation so the creditor cannot claim both the legacy and payment of the debt.\nWhere a claimant is under an obligation to do one thing but does another, his action may be treated as close enough approximation of the required act. A claimant who has undertaken an obligation, will, through his later conduct be interpreted as fulfilment of that obligation.\n\nIn England, there was a distinction drawn between the jurisdiction of the law courts and that of the chancery court. Courts of law had jurisdiction over property as well as persons and their coercive power arose out of their ability to adjust ownership rights. Courts of equity had power over persons. Their coercive power arose from the ability, on authority of the crown, to hold a violator in contempt, and take away his or her freedom (or money) until he or she purged himself or herself of his or her contumacious behavior. This distinction helped preserve a separation of powers between the two courts.\n\nNevertheless, courts of equity also developed a doctrine that an applicant must assert a \"property interest\". This was a limitation on their own power to issue relief. This does not mean that the courts of equity had taken jurisdiction over property. Rather, it means that they came to require that the applicant assert a right of some significant substance as opposed to a claim for relief based on an injury to mere emotional or dignitary interests.\n\nToday, a mortgagor refers to his interest in the property as his \"equity\". The origin of the concept, however, was actually a mirror-image of the current practice.\n\nAt common law, a mortgage was a conveyance of the property, with a condition subsequent, that if the grantor paid the secured indebtedness to the grantee on or before a date certain (the \"law\" day) then the conveyance would be void, otherwise to remain in full force and effect. As was inevitable, debtors would be unable to pay on the law day, and if they tendered the debt after the time had passed, the creditor owed no duty to give the land back. So then the debtor would run to the court of equity, plead that there was an unconscionable forfeiture about to occur, and beg the court to grant an equitable decree requiring the lender to surrender the property upon payment of the secured debt with interest to date. And the equity courts granted these petitions quite regularly and often without regard for the amount of time that had lapsed since the law day had passed. The lender could interpose a defense of laches, saying that so much time had gone by (and so much improvement and betterment had taken place) that it would be inequitable to require undoing the finality of the mortgage conveyance. Other defenses, including equitable estoppel, were used to bar redemption as well.\n\nThis unsettling system had a negative impact on the willingness of lenders to accept real estate as collateral security for loans. Since a lender could not re-sell the property until it had been in uncontested possession for years, or unless it could show changed circumstances, the value of real estate collateral was significantly impaired. Impaired, that is, until lawyers concocted the bill of foreclosure, whereby a mortgagee could request a decree that unless the mortgagor paid the debt by a date certain (and after the law date set in the mortgage), the mortgagor would thereafter be barred and foreclosed of all right, title and equity of redemption in and to the mortgaged premises.\n\nTo complete the circle, one needs to understand that when a mortgagor fails to pay an installment when due, and the mortgagee accelerates the mortgage, requiring immediate repayment of the entire mortgage indebtedness, the mortgagor does not have a right to pay the past-due installment(s) and have the mortgage reinstated. In \"Graf v. Hope Building Corp.\", the New York Court of Appeals observed that in such a case, there was no forfeiture, only the operation of a clause fair on its face, to which the mortgagor had freely assented. In the latter 20th Century, New York's lower courts eroded the \"Graf\" doctrine to such a degree that it appears that it is no longer the law, and that a court of conscience has the power to mandate that a default be excused if it is equitable to do so. Of course, now that the pendulum is swinging in the opposite direction, we can expect courts to explain where the limits on the newly expanded equity of redemption lie...and it is probably not a coincidence that the cases that have eroded \"Graf v. Hope Building Corp.\" have been accompanied by the rise of arbitration as a means for enforcing mortgages.\n\nAlso: Equity will not compel a court to do a vain and useless thing. It would be an idle gesture for the court to grant reformation of a contract and then to deny to the prevailing party an opportunity to perform it as modified.\n\nIt is often stated that one who comes into equity must come with clean hands (or alternatively, equity will not permit a party to profit by his own wrong). In other words, if you ask for help about the actions of someone else but have acted wrongly, then you do not have clean hands and you may not receive the help you seek. For example, if you desire your tenant to vacate, you must have not violated the tenant's rights.\n\nHowever, the requirement of clean hands does not mean that a \"bad person\" cannot obtain the aid of equity. \"Equity does not demand that its suitors shall have led blameless lives.\" The defense of unclean hands only applies if there is a nexus between the applicant's wrongful act and the rights he wishes to enforce.\n\nIn \"D & C Builders Ltd v Rees\", a small building firm did some work on the house of a couple named Rees. The bill came to £732, of which the Rees had already paid £250. When the builders asked for the balance of £482, the Rees announced that the work was defective, and they were only prepared to pay £300. As the builders were in serious financial difficulties (as the Rees knew), they reluctantly accepted the £300 \"in completion of the account\". The decision to accept the money would not normally be binding in contract law, and afterwards the builders sued the Rees for the outstanding amount. The Rees claimed that the court should apply the doctrine of promissory estoppel, which can make promises binding even when unsupported by consideration. However, Lord Denning refused to apply the doctrine, on the grounds that the Rees had taken unfair advantage of the builders' financial difficulties, and therefore had not come \"with clean hands\".\n\nWhen a court of equity is presented with a good claim to equitable relief, and it is clear that the plaintiff \"also\" sustained monetary damages, the court of equity has jurisdiction to render legal relief, e.g., monetary damages. Hence equity does not stop at granting equitable relief, but goes on to render a full and complete collection of remedies.\n\nThus, \"where a court of equity has all the parties before it, it will adjudicate upon all of the rights of the parties connected with the subject matter of the action, so as to avoid a multiplicity of suits.\" This is the basis for the procedures of interpleader, class action, and the more rarely used Bill of Peace.\n\nThis maxim, also expressed as \"Aequitas sequitur legem\", means more fully that \"equity will not allow a remedy that is contrary to law.\"\n\nThe Court of Chancery never claimed to override the courts of common law. Story states \"where a rule, either of the common or the statute law is direct, and governs the case with all its circumstances, or the particular point, a court of equity is as much bound by it as a court of law, and can as little justify a departure from it.\" According to Edmund Henry Turner Snell, “It is only when there is some important circumstance disregarded by the common law rules that equity interferes.” Cardozo wrote in his dissent in \"Graf v. Hope Building Corporation\", 254 N.Y 1 at 9 (1930), \"Equity works as a supplement for law and does not supersede the prevailing law.\"\n\nMaitland says, “We ought not to think of common law and equity as of two rival systems.\" \"Equity . of law was to be obeyed, but when all this had been done yet something might be needful, something that equity would require.\" The goal of law and equity was the same but due to historical reasons they chose a different path. Equity respected every word of law and every right at law but where the law was defective, in those cases, equity provides equitable right and remedies.\n\nIn the Province of Ontario, Canada, instead of equity following the law, the law follows equity – as per the Ontario Courts of Justice Act:\n\n(2) \"Where a rule of equity conflicts with a rule of the common law, the rule of equity prevails.\" R.S.O. 1990, c. C.43, s. 96 (2); 1993, c. 27, Sched.\n\nA volunteer is defined in equity as one who has not offered consideration for a benefit they have received or expect to receive. For example, if a person A expects from past conversations and friendship to receive property under any will of person B, but person B dies before writing this into their will, person A, having not made any contribution to person B, will not be able to seek equity's aid. \n\nThis maxim is very important in restitution. Restitution developed as a series of writs called special assumpsit, which were later additions in the courts of law, and were more flexible tools of recovery, based on equity. Restitution could provide means of recovery when people bestowed benefits on one another (such as giving money or providing services) according to contracts that would have been legally unenforceable.\n\nHowever, pursuant to the equitable maxim, restitution does not allow a volunteer or \"officious intermeddler\" to recover. \n\nThose successfully pleading benefit from an estoppel (promise relied on to their detriment) will not be considered volunteers for the purpose of this maxim.\n\nEquity will provide no specific remedies where the parties' causes are to be seen to be equal, or where neither has been wronged.\n\nThe significance of this maxim is that applicants to the chancellors often did so because of the formal pleading of the law courts, and the lack of flexibility they offered to litigants. Law courts and legislature, as lawmakers, through the limits of the substantive law they had created, thus inculcated a certain status quo that affected private conduct, and private ordering of disputes. Equity could alter that status quo, ignoring the clearly imposed limits of legal relief, or legal defences. But courts applying equity are reluctant to do so. This maxim reflects this. If the law firmly denied a cause of action or suggested equities between the parties were as a matter of policy equal, equity would provide no relief; if the law did provide relief, then the applicant would be obligated to bring a legal, rather than equitable action. This maxim overlaps with the previously mentioned \"equity follows the law.\"\n\nThis maxim operates where there are two or more competing equitable interests; when two equities are equal the original interest (i.e., the first in time) will succeed.\n\nIf a donor has made an imperfect gift, i.e. lacking the formalities required at common law, equity will not assist the intended donee. This maxim is a subset of equity will not assist a volunteer .\n\nNote the exception in \"Strong v Bird\" (1874) LR 18 Eq 315. If the donor appoints the intended donee as executor of his/her will, and the donor subsequently dies, equity will perfect the imperfect gift.\n\nEquity prevents a party from relying upon an absence of a statutory formality if to do so would be unconscionable and unfair. This can occur in secret trusts, constructive trusts etc.\n\nIf there is no trustee, whoever has legal title to the trust property will be considered the trustee. Otherwise, a court may appoint a trustee. In Ireland, the trustee may be any administrator of a charity to which the trust is related.\n\nDue to limits in old Common Law, no remedy was had for beneficiaries if, for example, a trustee ran off with the trust property. To remedy this and protect intended recipients of trust property, Equity regarded the beneficiary as the true (eventual) owners of the trust property.\n\n\n"}
{"id": "59104236", "url": "https://en.wikipedia.org/wiki?curid=59104236", "title": "Mit Borrás", "text": "Mit Borrás\n\nMit Borrás (Madrid *1982) is a Spanish artist. \n\nHe received his BFA in Madrid, Spain and specialization in Art and Cultural Studies in Berlin, (Germany). He developed his research on Media Art at the Twente University in Enschede, (the Netherlands) (2006) where he participated intensely in the Dutch contemporary art scene. He relocated back to Madrid where he oversees the production of video art festivals while also carrying out projects as an independent curator. In 2010 he moved to Berlin to participate in the Glogauair Residency and co-directed Fünf Galerie together with Rachel Lamot. He has worked as a coordinator of several cultural projects regarding media art and was working as a curator for the Instituto Cervantes in Berlin. He also currently combines his artistic practice with his work as an art critic in Neo2 Magazine.\n\nHis work is exhibited internationally in selected exhibitions such as in the Hara Museum, Tokyo (2010), Kreuzberg Pavillon, Germany (2013) and Norway (2016), Fonoteca Nacional, Mexico DF (2010), Palacio Fernandini, Lima (2016), Aleph Projects, Tel Aviv (2017), and the Museum of Fine Arts in Chile (2017). He has also been represented by Inés Barrenechea Gallery (2010-14). He continues to lecture within England, Colombia, Peru, the Netherlands, Spain and Germany. His work has been selected and awarded by Transmediale, (2011), United Creators Awards (2009), Loop Barcelona (2010, 2015-17), Art Lima (2016) and The Media Art Biennale in Chile (2017). He currently lives and works in Madrid and is a member of the international agency of artists and theorists Front Views in Berlin. Mit Borrás is represented by Exgirlfriend Gallery \n\nMit Borrás addresses technology and progress as new cult paradigms, often through the visual language of health and sport. He toys with the idea of ergonomics and softness in the development of new industrial forms, present in the works carried out since 2014 such as: Control Pool, Orthopedics, Youth Cloud, Techno Light, Devices, Artificial Rocks, Drone State of Mind or Ergonomic Technology. These works praise the idea of technology and well-being as a structure model of our time. Borrás emphasizes the objects that surround us as the objects that eventually define us as individuals and as a society; drones, gadgets, prosthetics, immobilizers, textile trends and their microperforated elastic fabrics, synthetic materials, medical and biomechanical engineering, and the design of products for leisure and relax. \n\nThe artist uses intersecting lenses of the contemporary body and technology to interpret the homo consumericus behavior and diagnose the crusade a Western individual of the XXI Century. Inworks such as Ergonomic Shape (2016) in which a block of granite is supported on a medicine ball, or the Stones series (2015) in which the artist disables tactile screens by tying them to large pieces of quartz, it is common to find disconnect between technology and nature. This confrontation between digital system and cosmos is presented by the artist as a reinterpreation of Sagan’s idea that everything that is, what has been or what will be. For the artist, this implies a confrontation between a gullible society in a consumerist system and the innate course of nature. Works such as the palm trees of Palm Deformation (2016) or Club (2016) in which a gallery equipped with a strobe light and techno music is flooded with smoke, make allusion to all those current elements that are part of our life of evasion, fun and lightness, a lightness understood in the manner of Gilles Lipovetsky: like a ghost that frivolizes and makes provisional everything that was previously designed to be eternal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24666570", "url": "https://en.wikipedia.org/wiki?curid=24666570", "title": "Morphogenetic robotics", "text": "Morphogenetic robotics\n\nMorphogenetic robotics generally refers to the methodologies that address challenges in robotics inspired by biological morphogenesis. \n\nMorphogenetic robotics is related to, but differs from, epigenetic robotics. The main difference between morphogenetic robotics and epigenetic robotics is that the former focuses on self-organization, self-reconfiguration, self-assembly and self-adaptive control of robots using genetic and cellular mechanisms inspired from biological early morphogenesis (\"activity-independent development\"), during which the body and controller of the organisms are developed simultaneously, whereas the latter emphasizes the development of robots' cognitive capabilities, such as language, emotion and social skills, through experience during the lifetime (\"activity-dependent development\"). Morphogenetic robotics is closely connected to developmental biology and systems biology, whilst epigenetic robotics is related to developmental cognitive neuroscience emerged from cognitive science, developmental psychology and neuroscience.\n\nMorphogenetic robotics includes, but is not limited to the following main topics:\n\n\n"}
{"id": "1834450", "url": "https://en.wikipedia.org/wiki?curid=1834450", "title": "Nassi–Shneiderman diagram", "text": "Nassi–Shneiderman diagram\n\nA Nassi–Shneiderman diagram (NSD) in computer programming is a graphical design representation for structured programming. This type of diagram was developed in 1972 by Isaac Nassi and Ben Shneiderman who were both graduate students at SUNY-Stony Brook. These diagrams are also called structograms, as they show a program's structures.\n\nFollowing a top-down design, the problem at hand is reduced into smaller and smaller subproblems, until only simple statements and control flow constructs remain. Nassi–Shneiderman diagrams reflect this top-down decomposition in a straightforward way, using nested boxes to represent subproblems. Consistent with the philosophy of structured programming, Nassi–Shneiderman diagrams have no representation for a GOTO statement.\n\nNassi–Shneiderman diagrams are only rarely used for formal programming. Their abstraction level is close to structured program code and modifications require the whole diagram to be redrawn. Nonetheless, they can be useful for sketching processes and high-level designs.\n\nNassi–Shneiderman diagrams are almost isomorphic with flowcharts. Everything that can be represented with a Nassi–Shneiderman diagram can also be represented with a flowchart. For flowcharts of programs, almost everything that can be represent with a flowchart can also be represented with a Nassi–Shneiderman diagram. The exceptions are constructs like goto and the C programming language \"break\" and \"continue\" statements for loops.\n\nIn Germany, Nassi–Shneiderman diagrams were standardised in 1985 as DIN 66261. They are still used in German introductions to programming, for example Böttcher and Kneißl's introduction to C, Baeumle-Courth and Schmidt's introduction to C and Kirch's introduction to C#.\n\nNassi–Shneiderman diagrams can also be used in technical writing.\n\nProcess blocks: the process block represents the simplest of steps and requires no analysis. When a process block is encountered the action inside the block is performed and we move onto the next block.\n\nBranching blocks: there are two types of branching blocks. First is the simple True/False or Yes/No branching block which offers the program two paths to take depending on whether or not a condition has been fulfilled. These blocks can be used as a looping procedure stopping the program from continuing until a condition has been fulfilled.\n\nThe second type of branching block is a multiple branching block. This block is used when a select case is needed in a program. The block usually contains a question or select case. The block provides the program with an array of choices and is often used in conjunction with sub process blocks to save space.\n\nTesting loops: this block allows the program to loop one or a set of processes until a particular condition is fulfilled. The process blocks covered by each loop are subset with a side-bar extending out from the condition.\n\nThere are two main types of testing loops, test first and test last blocks. The only difference between the two is the order in which the steps involved are completed.\nIn the test first situation, when the program encounters the block it tests to see if the condition is fulfilled, then, if it is not completes the process blocks and then loops back. The test is performed again and, if the condition is still unfulfilled, it processes again. If at any stage the condition is fulfilled the program skips the process blocks and continues onto the next block.\n\nThe test last block is simply reversed, the process blocks are completed before the test is performed. The test last loop allows for the process blocks to be performed at least once before the first test.\n\nConcurrent execution can be drawn like this:\n\n\n\n"}
{"id": "49517892", "url": "https://en.wikipedia.org/wiki?curid=49517892", "title": "Nebraska Innocence Project", "text": "Nebraska Innocence Project\n\nThe Nebraska Innocence Project is a member organization Nebraska-based chapter of a U.S non-profit organization called the Innocence Project, located in Omaha, Nebraska. Its mission is to use DNA testing to investigate, litigate and exonerate those in prison who were wrongfully convicted. The chapter was founded in 2005 by a group of volunteers who were inspired by the work of Barry Scheck and Peter Neufeld, founders of the Innocence Project in 1992.\n\nA group of volunteers including an Iowa attorney formed the Iowa/Nebraska Innocence Project in 2005 after meeting the founders of the Innocence Project, Barry Scheck and Peter Neufeld, and several exonerated people at a national conference. Their goal was to form a local version of the project. By 2007 the Iowa/Nebraska Innocent Project became the Nebraska Innocence Project. The organization is part of the Innocence Network, which is a group of 40 innocence projects across the nation, all working to provide accessible information, consultation on cases, and media resources pro-bono.\n\nThe Nebraska Innocence Project works primarily with cases involving DNA testing. According to the New York Innocence Project, more than 75% of people who have been exonerated in the United States through DNA testing, have served in prison on the basis of faulty eyewitness accounts. Meanwhile, a recent change to Nebraska state law will allow the project to look at cases not involving DNA such as arson cases, Shaken Baby Syndrome, bite marks, and show print-analysis. The Nebraska Innocence Project relies on the work of volunteers and essentially works without a budget. A partnership between the Nebraska chapter and the Midwest Innocence Project, which launched in October 2015, allows Nebraska to take advantage of the Midwest Innocence Project's paid administration, which they intend to do by hiring a paid legal director. To exonerate wrongfully convicted prisoners, the Nebraska Innocence Project sends requests to the Midwest Innocence Project, based in Kansas City, Missouri who prepares the cases and sends them back to Nebraska for investigation and possible litigation. Legal director, Tricia Bushnell assists the Nebraska chapter by organizing cases and finding experts who could help the case.\n\nCommonly referred to the Beatrice 6, a 1985 case in Gage County involved the rape and murder of 68-year-old woman, Helen Wilson, in her apartment. Six people, three men and three women were found guilty of committing the crime. After surfacing evidence that their conviction was based on coerced confessions and the malpractice of forensics, DNA testing of the original evidence proved their innocence. The DNA evidence was the first to be used as means to exonerate someone in the state of Nebraska. The six were awarded between $35,000-$500,00 by the state of Nebraska for their wrongful conviction under the Wrongful Conviction Act.\n\nIn 1975 Juneal Pratt was convicted of rape, sexual assault and robbery of two sisters from Iowa in an Omaha motel. At 19 he was arrested and was picked from a lineup by the two sisters months later despite witnesses who claimed he was at home at the time of the crime. He was sentenced to 32 to 90 years in prison. Pratt has since maintained his innocence and requested DNA testing after a 2001 Nebraska law was passed that allows convicted felons to request DNA testing of preserved evidence if such technology was unavailable during their trial.\n\n\n"}
{"id": "21436", "url": "https://en.wikipedia.org/wiki?curid=21436", "title": "Negligence", "text": "Negligence\n\nNegligence (Lat. \"negligentia\") is a failure to exercise appropriate and or ethical ruled care expected to be exercised amongst specified circumstances. The area of tort law known as \"negligence\" involves harm caused by failing to act as a form of \"carelessness\" possibly with extenuating circumstances. The core concept of negligence is that people should exercise reasonable care in their actions, by taking account of the potential harm that they might foreseeably cause to other people or property.\n\nSomeone who suffers loss caused by another's negligence may be able to sue for damages to compensate for their harm. Such loss may include physical injury, harm to property, psychiatric illness, or economic loss. The law on negligence may be assessed in general terms according to a five-part model which includes the assessment of duty, breach, actual cause, proximate cause, and damages.\n\nSome things must be established by anyone who wants to sue in negligence. These are what are called the \"elements\" of negligence.\n\nMost jurisdictions say that there are four elements to a negligence action:\n\nSome jurisdictions narrow the definition down to three elements: duty, breach and proximately caused harm. Some jurisdictions recognize five elements, duty, breach, actual cause, proximate cause, and damages. However, at their heart, the various definitions of what constitutes negligent conduct are very similar.\n\nThe legal liability of a defendant to a plaintiff is based on the defendant's failure to fulfil a responsibility, recognised by law, of which the plaintiff is the intended beneficiary. The first step in determining the existence of a legally recognised responsibility is the concept of an obligation or duty. In the tort of negligence the term used is duty of care \n\nThe case of \"Donoghue v Stevenson\" [1932] established the modern law of negligence, laying the foundations of the duty of care and the fault principle which, (through the Privy Council), have been adopted throughout the Commonwealth. May Donoghue and her friend were in a café in Paisley. The friend bought Mrs Donoghue a ginger beer float. She drank some of the beer and later poured the remainder over her ice-cream and was horrified to see the decomposed remains of a snail exit the bottle. Donoghue suffered nervous shock and gastro-enteritis, but did not sue the cafe owner, instead suing the manufacturer, Stevenson. (As Mrs Donoghue had not herself bought the ginger beer, the doctrine of privity precluded a contractual action against Stevenson).\n\nThe Scottish judge, Lord MacMillan, considered the case to fall within a new category of delict (the Scots law nearest equivalent of tort). The case proceeded to the House of Lords, where Lord Atkin interpreted the biblical ordinance to 'love thy neighbour' as a legal requirement to 'not harm thy neighbour.' He then went on to define neighbour as \"persons who are so closely and directly affected by my act that I ought reasonably to have them in contemplation as being so affected when I am directing my mind to the acts or omissions that are called in question.\"\n\nIn England the more recent case of \"Caparo Industries Plc v Dickman\" [1990] introduced a 'threefold test' for a duty of care. Harm must be (1) reasonably foreseeable (2) there must be a relationship of proximity between the plaintiff and defendant and (3) it must be 'fair, just and reasonable' to impose liability. However, these act as guidelines for the courts in establishing a duty of care; much of the principle is still at the discretion of judges.\n\nIn Australia, \"Donoghue v Stevenson\" was used as a persuasive precedent in the case of \"Grant v Australian Knitting Mills\" (AKR) (1936). This was a landmark case in the development of negligence law in Australia.\n\nWhether a duty of care is owed for psychiatric, as opposed to physical, harm was discussed in the Australian case of \"Tame v State of New South Wales; Annetts v Australian Stations Pty Ltd\" (2002). Determining a duty for mental harm has now been subsumed into the \"Civil Liability Act 2002\" in New South Wales. The application of Part 3 of the \"Civil Liability Act 2002\" (NSW) was demonstrated in \"Wicks v SRA (NSW); Sheehan v SRA (NSW)\".\n\nOnce it is established that the defendant owed a duty to the plaintiff/claimant, the matter of whether or not that duty was breached must be settled. The test is both subjective and objective. The defendant who knowingly (subjective,Which is totally based on observation and personal prejudice or view) exposes the plaintiff/claimant to a substantial risk of loss, breaches that duty. The defendant who fails to realize the substantial risk of loss to the plaintiff/claimant, which any reasonable person [objective,Which is totally based on ground facts and reality without any personal prejudice or point of view.] in the same situation would clearly have realized, also breaches that duty. However, whether the test is objective or subjective may depend upon the particular case involved.\n\nThere is a reduced threshold for the standard of care owed by children. In the Australian case of McHale v Watson, McHale, a 9-year-old girl was blinded in one eye after being hit by the ricochet of a sharp metal rod thrown by a 12-year-old boy, Watson. The defendant child was held not to have the level of care to the standard of an adult, but of a 12-year-old child with similar experience and intelligence. Kitto J explained that a child's lack of foresight is a characteristic they share with others at that stage of development.\n\nCertain jurisdictions, also provide for breaches where professionals, such as doctors, fail to warn of risks associated with medical treatments or procedures. Doctors owe both objective and subjective duties to warn; and breach of either is sufficient to satisfy this element in a court of law. For example, the Civil Liability Act in Queensland outlines a statutory test incorporating both objective and subjective elements. For example, an obstetrician who fails to warn a mother of complications arising from childbirth may be held to have breached their professional duty of care.\n\nIn \"Donoghue v Stevenson\", Lord Atkin declared that \"the categories of negligence are never closed\"; and in \"Dorset Yacht v Home Office\" it was held that the government had no immunity from suit when they negligently failed to prevent the escape of juvenile offenders who subsequently vandalise a boatyard. In other words, all members of society have a duty to exercise reasonable care toward others and their property. In \"Bolton v. Stone\" (1951), the House of Lords held that a defendant was not negligent if the damage to the plaintiff were not a reasonably foreseeable consequence of his conduct. In the case, a Miss Stone was struck on the head by a cricket ball while standing outside a cricket ground. Finding that no batsman would normally be able hit a cricket ball far enough to reach a person standing as far away as was Miss Stone, the court held her claim would fail because the danger was not reasonably or sufficiently foreseeable. As stated in the opinion, 'reasonable risk' cannot be judged with the benefit of hindsight. In \"Roe v Minister of Health\", Lord Denning said the past should not be viewed through rose coloured spectacles, finding no negligence on the part of medical professionals accused of using contaminated medical jars, since contemporary standards would have indicated only a low possibility of medical jar contamination.\n\n\"For the rule in the U.S., see\": Calculus of negligence\n\nFurther establishment of conditions of intention or malice where applicable may apply in cases of gross negligence.\n\nIn order for liability to result from a negligent act or omission, it is necessary to prove not only that the injury was caused by that negligence, but also that there is a legally sufficient connection between the act and the negligence.\n\nFor a defendant to be held liable, it must be shown that the particular acts or omissions were the cause of the loss or damage sustained. Although the notion sounds simple, the causation between one's breach of duty and the harm that results to another can at times be very complicated. The basic test is to ask whether the injury would have occurred 'but for', or without, the accused party's breach of the duty owed to the injured party. In Australia, the High Court has held that the 'but for' test is not the exclusive test of causation because it cannot address a situation where there is more than one cause of damage. When 'but for' test is not satisfied and the case is an exceptional one, a commonsense test ('Whether and Why' test) will be applied\nEven more precisely, if a breaching party materially increases the risk of harm to another, then the breaching party can be sued to the value of harm that he caused.\n\nAsbestos litigations which have been ongoing for decades revolve around the issue of causation. Interwoven with the simple idea of a party causing harm to another are issues on insurance bills and compensations, which sometimes drove compensating companies out of business.\n\nSometimes factual causation is distinguished from 'legal causation' to avert the danger of defendants being exposed to, in the words of Cardozo, J., \"liability in an indeterminate amount for an indeterminate time to an indeterminate class.\" It is said a new question arises of how remote a consequence a person's harm is from another's negligence. We say that one's negligence is 'too remote' (in England) or not a 'proximate cause' (in the U.S.) of another's harm if one would 'never' reasonably foresee it happening. Note that a 'proximate cause' in U.S. terminology (to do with the chain of events between the action and the injury) should not be confused with the 'proximity test' under the English duty of care (to do with closeness of relationship). The idea of legal causation is that if no one can foresee something bad happening, and therefore take care to avoid it, how could anyone be responsible? For instance, in \"Palsgraf v. Long Island Rail Road Co.\" the judge decided that the defendant, a railway, was not liable for an injury suffered by a distant bystander. The plaintiff, Palsgraf, was hit by coin-operated scale which toppled because of fireworks explosion that fell on her as she waited on a train platform. The scales fell because of a far-away commotion but it was not clear that what type of commotion caused the scale to fall,either it was the explosion's effect or the confused movement of the terrified people. A train conductor had run to help a man into a departing train. The man was carrying a package as he jogged to jump in the train door. The package had fireworks in it. The conductor mishandled the passenger or his package, causing the package to fall. The fireworks slipped and exploded on the ground causing shockwaves to travel through the platform.Which became the cause of commotion on platform & As a consequence, the scales fell. Because Palsgraf was hurt by the falling scales, she sued the train company who employed the conductor for negligence.\n\nThe defendant train company argued it should not be liable as a matter of law, because despite the fact that they employed the employee, who was negligent, his negligence was too remote from the plaintiff's injury. On appeal, the majority of the court agreed, with four judges adopting the reasons, written by Judge Cardozo, that the defendant owed no duty of care to the plaintiff, because a duty was owed only to foreseeable plaintiffs. Three judges dissented, arguing, as written by Judge Andrews, that the defendant owed a duty to the plaintiff, regardless of foreseeability, because all men owe one another a duty not to act negligently.\n\nSuch disparity of views on the element of remoteness continues to trouble the judiciary. Courts that follow Cardozo's view have greater control in negligence cases. If the court can find that, as a matter of law, the defendant owed no duty of care to the plaintiff, the plaintiff will lose his case for negligence before having a chance to present to the jury. Cardozo's view is the majority view. However, some courts follow the position put forth by Judge Andrews. In jurisdictions following the minority rule, defendants must phrase their remoteness arguments in terms of proximate cause if they wish the court to take the case away from the jury.\n\nRemoteness takes another form, seen in \"The Wagon Mound (No. 2)\". The Wagon Mound was a ship in Sydney harbour. The ship leaked oil creating a slick in part of the harbour. The wharf owner asked the ship owner about the danger and was told he could continue his work because the slick would not burn. The wharf owner allowed work to continue on the wharf, which sent sparks onto a rag in the water which ignited and created a fire which burnt down the wharf. The Privy Council determined that the wharf owner 'intervened' in the causal chain, creating a responsibility for the fire which canceled out the liability of the ship owner.\n\nIn Australia the concept of remoteness, or proximity, was tested with the case of \"Jaensch v Coffey\". The wife of a policeman, Mrs Coffey suffered a nervous shock injury from the aftermath of a motor vehicle collision although she was not actually at the scene at the time of the collision. The court upheld that, in addition to it being reasonably foreseeable that his wife might suffer such an injury, it required that there be sufficient proximity between the plaintiff and the defendant who caused the collision. Here there was sufficient causal proximity. See also \"Kavanagh v Akhtar\", \"Imbree v McNeilly\", and \"Tame v NSW\".\n\nEven though there is breach of duty, and the cause of some injury to the defendant, a plaintiff may not recover unless he can prove that the defendant's breach caused a pecuniary injury. This should not be mistaken with the requirements that a plaintiff prove harm to recover. As a general rule, a plaintiff can only rely on a legal remedy to the point that he proves that he suffered a loss; it was reasonably foreseeable. It means something more than pecuniary loss is a necessary element of the plaintiff's case in negligence. When damages are not a necessary element, a plaintiff can win his case without showing that he suffered any loss; he would be entitled to nominal damages and any other damages according to proof. (See \"Constantine v Imperial Hotels Ltd\" [1944] KB]).\n\nNegligence is different in that the plaintiff must prove his loss, and a particular kind of loss, to recover. In some cases, a defendant may not dispute the loss, but the requirement is significant in cases where a defendant cannot deny his negligence, but the plaintiff suffered no pecuniary loss as a result even though he had suffered emotional injury or damage but he cannot be compensated for these kind of losses.The plaintiff can be compensated for emotional or non-pecuniary losses on the condition that If the plaintiff can prove pecuniary loss, then he can also obtain damages for non-pecuniary injuries, such as emotional distress.\n\nThe requirement of pecuniary loss can be shown in a number of ways. A plaintiff who is physically injured by allegedly negligent conduct may show that he had to pay a medical bill. If his property is damaged, he could show the income lost because he could not use it, the cost to repair it, although he could only recover for one of these things.\n\nThe damage may be physical, purely economic, both physical and economic (loss of earnings following a personal injury,) or reputational (in a defamation case).\n\nIn English law, the right to claim for purely economic loss is limited to a number of 'special' and clearly defined circumstances, often related to the nature of the duty to the plaintiff as between clients and lawyers, financial advisers, and other professions where money is central to the consultative services.\n\nEmotional distress has been recognized as an actionable tort. Generally, emotional distress damages had to be parasitic. That is, the plaintiff could recover for emotional distress caused by injury, but only if it accompanied a physical or pecuniary injury.\n\nA claimant who has suffered only emotional distress and no pecuniary loss would not recover for negligence. However, courts have recently allowed recovery for a plaintiff to recover for purely emotional distress under certain circumstances. The state courts of California allowed recovery for emotional distress aloneeven in the absence of any physical injury, when the defendant physically injures a relative of the plaintiff, and the plaintiff witnesses it.\n\nThe eggshell skull rule is a legal doctrine upheld in some tort law systems, which holds that a tortfeasor is liable for the full extent of damage caused, even where the extent of the damage is due to the unforeseen frailty of the claimant. The eggshell skull rule was recently maintained in Australia in the case of \"Kavanagh v Akhtar\".\n\nDamages place a monetary value on the harm done, following the principle of \"restitutio in integrum\" (Latin for \"restoration to the original condition\"). Thus, for most purposes connected with the quantification of damages, the degree of culpability in the breach of the duty of care is irrelevant. Once the breach of the duty is established, the only requirement is to compensate the victim.\n\nOne of the main tests that is posed when deliberating whether a claimant is entitled to compensation for a tort, is the \"reasonable person\". The test is self-explanatory: would a reasonable person (as determined by a judge or jury), under the given circumstances, have done what the defendant did to cause the injury in question; or, in other words, would a reasonable person, acting reasonably, have engaged in similar conduct when compared to the one whose actions caused the injury in question? Simple as the \"reasonable person\" test sounds, it is very complicated. It is a risky test because it involves the opinion of either the judge or the jury that can be based on limited facts. However, as vague as the \"reasonable person\" test seems, it is extremely important in deciding whether or not a plaintiff is entitled to compensation for a negligence tort.\n\nDamages are compensatory in nature. Compensatory damages addresses a plaintiff/claimant's losses (in cases involving physical or mental injury the amount awarded also compensates for pain and suffering). The award should make the plaintiff whole, sufficient to put the plaintiff back in the position he or she was before Defendant's negligent act. Anything more would unlawfully permit a plaintiff to profit from the tort.\n\nThere are also two other general principles relating to damages. Firstly, the award of damages should take place in the form of a single lump sum payment. Therefore, a defendant should not be required to make periodic payments (however some statutes give exceptions for this). Secondly, the Court is not concerned with how the plaintiff uses the award of damages. For example, if a plaintiff is awarded $100,000 for physical harm, the plaintiff is not required to spend this money on medical bills to restore them to their original position - they can spend this money any way they want.\n\n\nThe United States generally recognizes four elements to a negligence action: duty, breach, proximate causation and injury. A plaintiff who makes a negligence claim must prove all four elements of negligence in order to win his or her case. Therefore, if it is highly unlikely that the plaintiff can prove one of the elements, the defendant may request judicial resolution early on, to prevent the case from going to a jury. This can be by way of a demurrer, motion to dismiss, or motion for summary judgment.\n\nThe elements allow a defendant to test a plaintiff's accusations before trial, as well as providing a guide to the finder of fact at trial (the judge in a bench trial, or jury in a jury trial) to decide whether the defendant is or is not liable. Whether the case is resolved with or without trial again depends heavily on the particular facts of the case, and the ability of the parties to frame the issues to the court. The duty and causation elements in particular give the court the greatest opportunity to take the case from the jury, because they directly involve questions of policy. The court can find that regardless of any disputed facts, the case may be resolved as a matter of law from undisputed facts because as a matter of law the defendant cannot be legally responsible for the plaintiff's injury under a theory of negligence.\n\nOn appeal, depending on the disposition of the case and the question on appeal, the court reviewing a trial court's determination that the defendant was negligent will analyze at least one of the elements of the cause of action to determine if it is properly supported by the facts and law. For example, in an appeal from a final judgment after a jury verdict, the appellate court will review the record to verify that the jury was properly instructed on each contested element, and that the record shows sufficient evidence for the jury's findings. On an appeal from a dismissal or judgment against the plaintiff without trial, the court will review \"de novo\" whether the court below properly found that the plaintiff could not prove any or all of his or her case.\n\n\n\n"}
{"id": "6379557", "url": "https://en.wikipedia.org/wiki?curid=6379557", "title": "Object diagram", "text": "Object diagram\n\nAn object diagram in the Unified Modeling Language (UML), is a diagram that shows a complete or partial view of the structure of a modeled system at a specific time.\n\nIn the Unified Modeling Language (UML), an object diagram focuses on some particular set of objects and attributes, and the links between these instances. A correlated set of object diagrams provides insight into how an arbitrary view of a system is expected to evolve over time. In early UML specifications the object diagram is described as:\n\nThe latest UML 2.5 specification does not explicitly define object diagrams, but provides a notation for instances of classifiers.\n\nObject diagrams and class diagrams are closely related and use almost identical notation. Both diagrams are meant to visualize static structure of a system. While class diagrams show classes, object diagrams display instances of classes (objects). Object diagrams are more concrete than class diagrams. They are often used to provide examples or act as test cases for class diagrams. Only aspects of current interest in a model are typically shown on an object diagram.\n\nEach object and link on an object diagram is represented by an \"InstanceSpecification\". This can show an object's classifier (e.g. an abstract or concrete class) and instance name, as well as attributes and other structural features using \"slots\". Each \"slot\" corresponds to a single attribute or feature, and may include a value for that entity.\n\nThe name on an instance specification optionally shows an instance name, a ':' separator, and optionally one or more classifier names separated by commas. The contents of slots, if any, are included below the names, in a separate attribute compartment.\nA link is shown as a solid line, and represents an instance of an association.\n\nConsider one possible way of modeling production of the Fibonacci sequence.\n\nIn the first UML object diagram on the right, the instance in the leftmost instance specification is named \"v1\", has \"IndependentVariable\" as its classifier, plays the \"NMinus2\" role within the \"FibonacciSystem\", and has a slot for the \"val\" attribute with a value of \"0\". The second object is named \"v2\", is of class \"IndependentVariable\", plays the \"NMinus1\" role, and has \"val = 1\". The \"DependentVariable\" object is named \"v3\", and plays the \"N\" role. The topmost instance, an anonymous instance specification, has \"FibonacciFunction\" as its classifier, and may have an instance name, a role, and slots, but these are not shown here. The diagram also includes three named links, shown as lines. Links are instances of an association.\n\nIn the second diagram, at a slightly later point in time, the \"IndependentVariable\" and \"DependentVariable\" objects are the same, but the slots for the \"val\" attribute have different values. The role names are not shown here.\n\nIn the last object diagram, a still later snapshot, the same three objects are involved. Their slots have different values. The instance and role names are not shown here.\n\nIf you are using a UML modeling tool, you will typically draw object diagrams using some other diagram type, such as on a class diagram. An object instance may be called an \"instance specification\" or just an \"instance\". A link between instances is generally referred to as a \"link\". Other UML entities, such as an aggregation or composition symbol (a diamond) may also appear on an object diagram.\n\n"}
{"id": "12325830", "url": "https://en.wikipedia.org/wiki?curid=12325830", "title": "Otis Taylor (American football)", "text": "Otis Taylor (American football)\n\nOtis Taylor (born August 11, 1942) is a former American college and professional American football player, for Prairie View A&M University and the American Football League's Kansas City Chiefs. Standing 6-foot-3 and weighing 215 pounds, Taylor possessed sure hands during his career and served as a devastating downfield blocker, springing Chiefs running backs for many long runs.\n\nTaylor was selected in the 1965 AFL draft (Chiefs) and the NFL draft, by the Philadelphia Eagles. After a famous \"baby-sitting\" incident, in which Taylor \"escaped\" from NFL scouts, he was signed for the Chiefs by their legendary scout Lloyd Wells.\n\nTaylor caught five touchdown passes during his rookie year, and followed that up in 1966 by leading the AFL with a 22.4 yd/catch average and finishing second in receiving yards (1,297). At season's end, he was voted First-team All-AFL and was selected for the 1966 AFL All-Star team. Taylor led the AFL in receiving touchdowns in 1967 with 11 and led the NFL in receiving yards in 1971 with 1,110. He made the AFC-NFC Pro Bowl twice and in 1971 was named Consensus All-Pro by the Associated Press (AP), the Newspaper Enterprise Association (NEA), the Pro Football Writers Association (PFWA) and Pro Football Weekly. The PFWA also named him First-team All-Pro for the 1972 season. Taylor ranks second on the Chiefs' all-time list in receptions (410), receiving yards (7,306), receiving touchdowns (57) and 100-yard games (20).\n\nTaylor combined with running back Robert Holmes for what was at the time the longest reception in Chiefs history in 1969 when he caught a pass from quarterback Mike Livingston for 79 yards, then lateraled to Holmes, who carried it another 14 yards for a touchdown. However, Taylor's most memorable highlight from that season came in the fourth and final AFL-NFL World Championship Game on January 11, 1970, when he caught a short pass, turned downfield and stiff-armed his way to a 46-yard touchdown in the Chiefs 23-7 upset victory over the NFL's champion, the Minnesota Vikings, who, before Super Bowl IV, had been dubbed by some as \"the greatest team in pro football history\".\n\n\"Otis made my job easy,\" former Chiefs quarterback and Hall of Famer Len Dawson said. \"If you got the pass to Otis, you knew he'd catch it.\"\n\nThe Professional Football Researchers Association named Taylor to the PRFA Hall of Very Good Class of 2006 \n\nOn November 1, 1970, the Chiefs led the Oakland Raiders 17–14 late in the fourth quarter, and a long run for a first-down run by Dawson apparently sealed victory for the Chiefs in the final minute when Dawson, as he lay on the ground, was speared by Raiders' cheap-shot artist defensive end Ben Davidson, who dove into Dawson with his helmet, provoking Taylor to attack Davidson.\n\nAfter a bench-clearing brawl, offsetting penalties were called, nullifying the first down under the rules in effect at that time. The Chiefs were obliged to punt, and the Raiders tied the game on a George Blanda field goal with eight seconds to play. Davidson's hit against Dawson not only cost the Chiefs a win, but helped Oakland win the AFC West with a season record of 8–4–2, while defending world champion Kansas City finished 7–5–2 and out of the playoffs. The very next season, the rule for offsetting personal foul penalties was changed to separate penalties during the play, and penalties after the play. The rule change was largely due to this play.\n\nAfter his time as a player had come to a close, Taylor became a scout for the Kansas City Chiefs. During the 1987 NFL Player's strike, Taylor was arriving at Arrowhead Stadium and was assaulted by Jack Del Rio, who was a new player to the organization in 1987 and was striking with his teammates. Del Rio mistook Taylor for a replacement player and was told Taylor was actually a Chief's legend and retired player by fans who had come upon the assault. He later pressed charges against Del Rio and the two settled out of court.\n\n"}
{"id": "4797500", "url": "https://en.wikipedia.org/wiki?curid=4797500", "title": "Pazend", "text": "Pazend\n\nPazend or Pazand Middle Persian: 𐭯𐭠𐭰𐭭𐭣 Avestan: \"𐬞𐬀𐬌𐬙𐬌 𐬰𐬀𐬌𐬥𐬙𐬌‎\" is one of the writing systems used for the Middle Persian language. It was based on the Avestan alphabet, a phonetic alphabet originally used to write Avestan, the language of the Avesta, the primary sacred texts of Zoroastrianism.\n\nPazend's principal use was for writing the commentaries (\"Zend\") on and/or translations of the Avesta. The word \"Pazend\" ultimately derives from the Avestan words \"paiti zainti\", which can be translated as either \"for commentary purposes\" or \"according to understanding\" (phonetically).\n\nPazend had the following characteristics, both of which are to be contrasted with Pahlavi, which is one of the other systems used to write Middle Persian:\n\nIn combination with its religious purpose, these features constituted a \"sanctification\" of written Middle Persian. The use of the Avestan alphabet to write Middle Persian required the addition of one symbol to the Avestan alphabet: This character, to support Middle Persian's , phoneme had not previously been needed.\n\nFollowing the fall of the Sassanids, after which Zoroastrianism came to be gradually supplanted by Islam, Pazend lost its purpose and soon ceased to be used for original composition. In the late 11th or early 12th century, Indian Zoroastrians (the Parsis) began translating Avestan or Middle Persian texts into Sanskrit and Gujarati. Some Middle Persian texts were also \"transcribed\" into the Avestan alphabet. The latter process, being a form of interpretation, was known as 'pa-zand'. \"Pazand texts, transcribed phonetically, represent a late and often corrupt Middle Persian pronunciation, and so present their own problems.\" \"The corruptions during this process are sometimes considerable.\" Among the transcribed texts are the prefaces (\"dibacheh\") to prayers in Avestan. These prefatory prayers are invariably written in Pazend because of the need for \"accurate\" pronunciation. This practice has led to the misconception that \"Pazend\" is the name of a language.\n\nFollowing Abraham Hyacinthe Anquetil-Duperron's translation of some of the texts of the Avesta in the late 18th century, the term \"Zend-Avesta\" was mistakenly used to refer to the sacred texts themselves (as opposed to commentaries on them). This usage subsequently led to the equally mistaken use of \"Pazend\" for the Avestan script as such and \"Zend\" for the Avestan language.\n\n"}
{"id": "40437234", "url": "https://en.wikipedia.org/wiki?curid=40437234", "title": "Proactive law", "text": "Proactive law\n\nProactive law seeks a new approach to legal issues in businesses and societies. Instead of perceiving law as a constraint that companies and people in general need to comply with, proactive law considers law as an instrument that can create success and foster sustainable relationships, which in the end carries the potential to increase value for companies, individuals, and societies in general.\n\nThe word proactive is the opposite of reactive, meaning that the approach to law is based on an ex ante view rather than an ex post view. According to the dictionary of Merriam-Webster, the word proactive refers to \"acting in anticipation of future problems, needs, or changes\".\nThus, the proactive law approach challenges the traditional backwards and failure oriented approach to law by acting in anticipation of legal disputes, taking control of potential problems, providing solutions, and self-initiation, instead of reacting to failures and shortcomings.\n\nThe hotbed of the proactive law movement is the Nordic countries and Finland in particular. The movement took off in the late 1990s and is almost similar to the American movement – Law as a Competitive Source. Both of these parallel evolutions are founded on the work of Louis M. Brown, developed in the 1950s known as the preventive approach to law.\n\nThe proactive law movement has become more visible in recent years, but the idea of an ex ante view is not new. It is generally known that the earlier a dispute or a potential dispute is addressed, the better the chances of a fair, just and prompt solution. Louis M. Brown was the first to introduce the ex ante view in his ground-laying book “Preventive Law”. Although he identified and organized the preventive law into a distinctive way of thinking, he was not the inventor of this approach. It has been, and still is, well known to many legal professionals and every business manager that:\n\n“It usually costs less to avoid getting into trouble than to pay for getting out of trouble.”\n\nTo understand the general principles of proactive law requires understanding the core principles of preventive law, as these principles create the foundation for proactive law and proactive contracting. Edward Dauer identifies four core principles of preventive law:\n\n\nThe proactive law movement encompasses the basic principles of preventive law stated above, namely preventing what is not desirable, and keeping problems and risks from materializing. \nThus, as proactive law consists of preventive law, the characteristics above constitute the foundation of proactive law.\n\nTo this preventive dimension of law, proactive law adds a second aspect, which is often neglected in traditional law – known as the \"promotive dimension\".\n\nThe nature of the promotive dimension is positive and constructive and promotes what is desirable while encouraging good behavior. This is where we find the distinction to preventive law. \nIn a legal context, proactive law emphasizes the importance of collaboration between legal professionals and other disciplines to achieve the desired goals in circumstances where legal expertise collaborates with other disciplines. Proactive law therefore emphasizes the need for dialogue between different understandings. In a medical context, the preventive law prevents ill health, while proactive law promotes well-being.\n\nThe proactive law approach is based on legal certainty, literacy, and cross-professional collaboration to “localize the mines and preventing them from exploding.”\n\nIn addition to navigating past the mines, the legal professionals should create economic value, and thus must be outcome-orientated to exploit the promotive dimension. Lawyers and in-house counsel thus must act to achieve results by watching for changes or opportunities and setting improved goals. To do so, legal professionals must highlight opportunities to build a solid business foundation, roadmaps for performance, trust, and better sustainable relationships.\n\nBesides achieving these business goals, it is important to focus on legal risk management to prevent disputes. Many legal disputes arise due to misunderstandings and disappointed expectations. However, careful attention to legal clarity along with early warning mechanisms, and enhanced collaboration between business partners, through establishment of common goals, avoids the business from getting to the stage of dispute.\n\nIt is essential in proactive law that legal professionals, managers, and other involved stakeholders collaborate on a cross-professional basis to avoid disputes. In addition to avoiding disputes, it is also important to promote creative thinking. To develop new ideas, and concepts that correspond to the needs, problems or challenges, it is necessary to look towards the future rather than the past, maybe by using already known approaches, but also non-existing approaches. This invites businesses, authorities and researchers to develop solutions through creative thinking.\n\nThus, proactive law is about problem-solving, detecting real-life causes for potential misunderstandings and failures, but most of all it is about fostering and promoting fruitful and sustainable relationships that enables the stakeholders to reach their goals, creating value for business, individuals, and society as a whole.\n\nAn indication that Proactive Law is gaining prominence is the fact that the European Commission has published an opinion on Proactive Law.\n\nThe EESC urges a paradigm shift, as the time has come to give up the centuries-old reactive approach to law and to adopt a pro active approach. It is time to look at law in a different way: to look forward rather than back, to focus on how the law is used and oper ates in everyday life and how it is received in the community it seeks to regulate. While responding to and resolving problems remain important, preventing causes of problems is vital, along with serving the needs and facilitating the productive interaction of citizens and businesses.\n\nBy its very nature, the Community legal system is precisely the type of area in which the proactive approach should be adopted when planning, drawing up and implementing laws; against this backdrop, the EESC would argue that rules and regu lations are not the only way nor always the best way to achieve the desired objectives; at times, the regulator may best support valuable goals by refraining from regulating and, where appropri ate, encouraging self-regulation and co-regulation. This being the case, the fundamental principles of subsidiarity, proportionality, precaution and sustainability take on new importance and a new dimension.\n\nThe EESC believes that the single market can benefit greatly when EU law and its makers — legislators and administrators in the broadest sense — shift their focus from inward, from inside the legal system, rules and institutions, to outward, to the users of the law: to society, citizens and businesses that the legal system is intended to serve.\n\nWhile the transposition and implementation of laws are important steps towards better regulation at EU level, regulatory success should be measured by how the goals are achieved at the level of the users of the law, EU citizens and businesses. The laws should be communicated in ways that are meaningful to their intended audience, first and foremost to those whose behaviour is affected and not just to the relevant institutions and administrators.\n\nThe application of the Proactive Law approach should be considered systematically in all lawmaking and implementation within the EU. The EESC strongly believes that by making this approach not only part of the Better Regulation agenda, and but also a priority for legislators and administrators at the EU, national and regional levels, it would be possible to build a strong legal foundation for individuals and businesses to prosper.\n\nWhile Proactive Law in general has gained increased attention at the highest legislatory authorities, businesses seem to be the first movers in this developing research field.\n\nWhen Proactive Law is adapted to businesses the approach is called Proactive Contracting and Proactive Contract Management. The idea is that Proactive Contracting may constitute a sustainable competitive advantage, when implemented thoroughly.\n\nIn recent years law has gained increased prominence as a source of competitive advantage to businesses. This prominence originated with the work of scholars and professionals in the United States and Europe. In the US, research of Robert Bird has contributed to the movement of Law as a Competitive Advantage. \nRobert Bird uses a strategy framework to analyze the attributes of law necessary to obtain a competitive advantage. In Europe, the proactive law movement constitutes the same. \nAs contracts and contract law are major components of successful R&D, IT, operations management, outsourcing etc., it is important that the contracts and contracting processes involved are successful. \nIn the words of Tim Cummins, CEO of the IACCM, “contracts lie at the heart of most business relationships, certainly within Western cultures and economies, and increasingly among all companies or entities that seek to operate in an international market”.\n\nAs international trading grows in value and quantity, it is important for businesses to excel in contracting. Businesses that can manage contracting successfully have a competitive advantage ceteris paribus.\n\nBy incorporating both a preventive and a promotive focus, the proactive law movement encourages both legal goals, which are often preventive—and business goals, which tend to be promotive.\n\nBy combining these two approaches to reaching goals, businesses take advantage of both the preventive focus and the promotive focus.\nTory Higgins has developed a psychological theory called the Regulatory Focus Theory that focuses on motivations and how people tend to achieve their goals.\n\nThe two different orientations towards goal achievement, which have been recognized by Tory Higgins, are:\n\n(1) a preventive focus that emphasizes safety, responsibility and security, and\n\n(2) a promotive focus that emphasizes hopes, accomplishments, and advancements.\n\nThus, the way of businesses to pursuing a goal depends on whether their approach is preventive or promotive. According to Mayer and Weber, people with a preventive focus view goals as minimal targets that produce a low-intensity response when the goal is achieved. On the other hand, people with a promotive approach see goals as maximal targets. Thus, “achievement of the goal results in feelings of high-intensity happiness instead of low-intensity calm”.\n\nIf businesses succeed in incorporating both the preventive and promotive approach in reaching their goals, they may able to utilize the advantages of both legal and business goals.\n"}
{"id": "14926951", "url": "https://en.wikipedia.org/wiki?curid=14926951", "title": "Samaritan's dilemma", "text": "Samaritan's dilemma\n\nThe Samaritan's dilemma is a dilemma in the act of charity. It hinges on the idea that when presented with charity, in some location such as a soup kitchen, a person will act in one of two ways: using the charity to improve their situation, or coming to rely on charity as a means of survival. The term \"Samaritan's dilemma\" was coined by economist James M. Buchanan.\n\nThe argument against charity frequently cites the Samaritan's dilemma as reason to forgo charitable contributions. It is also a common argument against communism and socialism, claiming that state aid is equivalent to charity, and that the beneficiaries of such aid will become slothful or otherwise negligent members of society. The more aid that is received the less likely the recipient will seek out a permanent solution for their condition. \n\nThe dilemma's name is a reference to the biblical Parable of the Good Samaritan.\n\nA study published in 2016 looked at 5089 major natural disasters in 81 developing countries over a 33 year period. Results showed that among the countries that received natural disaster relief had less incentives to provide their own natural disaster protections. This effect is exacerbated the poorer and less-developed the country is that receives the aid. \n\nMuch foreign aid comes in the form of monetary compensation for damages or direct relief such as food or water. This type of direct aid does not provide any incentive for the recipient to become independent. By introducing incentives that developing countries can meet by building up natural disaster protection the effects of the Samaritan's dilemma can be effectively mitigated.\n\n\n"}
{"id": "2242938", "url": "https://en.wikipedia.org/wiki?curid=2242938", "title": "Self in Jungian psychology", "text": "Self in Jungian psychology\n\nThe Self in Jungian psychology is one of the Jungian archetypes, signifying the unification of consciousness and unconsciousness in a person, and representing the psyche as a whole. The Self, according to Carl Jung, is realized as the product of individuation, which in his view is the process of integrating one's personality. For Jung, the Self is symbolized by the circle (especially when divided in four quadrants), the square, or the mandala.\n\nWhat distinguishes Jungian psychology is the idea that there are two centers of the personality. The ego is the center of consciousness, whereas the Self is the center of the total personality, which includes consciousness, the unconscious, and the ego. The Self is both the whole and the center. While the ego is a self-contained little center of the circle contained within the whole, the Self can be understood as the greater circle.\n\nJung considered that from birth every individual has an original sense of wholeness - of the Self - but that with development a separate ego-consciousness crystallizes out of the original feeling of unity. This process of ego-differentiation provides the task of the first half of one's life-course, though Jungians also saw psychic health as depending on a periodic return to the sense of Self, something facilitated by the use of myths, initiation ceremonies, and rites of passage.\n\nOnce ego-differentiation had been successfully achieved and the individual is securely anchored in the external world, Jung considered that a new task then arose for the second half of life - a return to, and conscious rediscovery of, the Self: individuation. Marie-Louise von Franz states that \"The actual processes of individuation - the conscious coming-to-term with one's own inner center (psychic nucleus) or Self - generally begins with a wounding of the personality\". The ego reaches an impasse of one sort or another; and has to turn for help to what she termed \"a sort of hidden regulating or directing tendency...[an] organizing center\" in the personality: \"Jung called this center the 'Self' and described it as the totality of the whole psyche, in order to distinguish it from the 'ego', which constitutes only a small part of the psyche\".\n\nUnder the Self's guidance, a succession of archetypal images emerges, gradually bringing their fragmentary aspects of the Self increasingly closer to its totality. The first to appear, and the closest to the ego, would be the shadow or personal unconscious - something which is at the same time the first representative of the total personality, and which may indeed be at times conflated with the Self. Next to appear would be the Anima and Animus, the soul-image, which again, by a kind of psychological short-cut, may be taken as identical to the whole Self. Ideally however, the animus or anima comes to play a mediatory role between the ego and the Self. The third main archetype to emerge is the Mana figure of the wise old man/woman - a representative of the collective unconscious still closer to the Self.\n\nThereafter comes the archetype of the Self itself - the last point on the route to self-realization of individuation. In Jung's words, \"the Self...embraces ego-consciousness, shadow, anima, and collective unconscious in indeterminable extension. As a totality, the self is a \"coincidentia oppositorum\"; it is therefore bright and dark and yet neither\". Alternatively, he stated that \"the Self is the total, timeless man...who stands for the mutual integration of conscious and unconscious\". Jung recognized many dream images as representing the self, including a stone, the world tree, an elephant, and the Christ.\n\nVon Franz considered that \"the dark side of the Self is the most dangerous thing of all, precisely because the Self is the greatest power in the psyche. It can cause people to 'spin' megalomanic or other delusionary fantasies that catch them up\", so that the victim \"thinks with mounting excitement that he has grasped the great cosmic riddles; he therefore loses all touch with human reality. \n\nIn everyday life, the Self may be projected onto such powerful figures as the state, God, the universe or fate. When such projections are withdrawn, there can be a destructive inflation of the personality - one potential counterbalance to this being however the social or collective aspects of the Self.\n\nFritz Perls may have had the Jungians in mind when he objected that 'many psychologists like to write the self with a capital S, as if the self would be something precious, something extraordinarily valuable. They go at the discovery of the self like a treasure-digging. The self means nothing but this thing as it is defined by \"otherness\"'.\n\nYoung-Eisendrath and Hall write that 'in Jung's work, self can refer to the notion of inherent subjective individuality, the idea of an abstract center or central ordering principle, and the account of a process developing over time'.\n\n"}
{"id": "34500802", "url": "https://en.wikipedia.org/wiki?curid=34500802", "title": "Sensus divinitatis", "text": "Sensus divinitatis\n\nSensus divinitatis (\"sense of divinity\"), also referred to as \"sensus deitatis\" (\"sense of deity\") or \"semen religionis\" (\"seed of religion\"), is a term first used by French Protestant reformer John Calvin to describe a hypothetical human sense. Instead of knowledge of the environment (as with, for example, smell or sight), the \"sensus divinitatis\" is alleged to give humans a knowledge of God.\n\nIn Calvin's view, there is no reasonable non-belief. Neo-Calvinists who adhere to the presuppositionalist school of Christian apologetics sometimes appeal to a \"sensus divinitatis\" to argue that there are no genuine atheists:\n\nAnalytic philosopher Alvin Plantinga of the University of Notre Dame posits a modified form of the \"sensus divinitatis\" whereby all have the sense, only it does not work properly in some humans, due to sin's noetic effects. (see Reformed epistemology)\n\nJonathan Edwards, the 18th-century American Calvinist preacher and theologian, claimed that while every human being has been granted the capacity to know God, successful use of these capacities requires an attitude of \"true benevolence\".\n\nPhilosopher Steven Maitzen claimed in 2006 that the demographics of religious belief make the existence of the \"sensus divinitatis\" unlikely, as this sense appears so unevenly distributed.\n\nHowever, Maitzen may have confused Aquinas's \"sensus dei\" with \"sensus divinitatis\"—\"sensus divinitatis\" (a religious sense) only necessitates a core religious/faith component to one's beliefs, whereas the \"sensus dei\" aims at a natural knowledge of God—compare \"In the Twilight of Western Thought\" by Herman Dooyeweerd (1894–1977).\n"}
{"id": "57354478", "url": "https://en.wikipedia.org/wiki?curid=57354478", "title": "Sexual assault of LGBT persons", "text": "Sexual assault of LGBT persons\n\nSexual assault of LGBT persons is a form of violence that occurs within the LGBT community. While sexual assault and other forms of interpersonal violence can occur in all forms of relationships, it is found that sexual minorities experience it at rates that are equal to or higher than their heterosexual counterparts. There is a lack of research on this specific problem for the LGBT population as a whole, but there does exist a substantial amount of research on college LGBT students who have experienced sexual assault. \n\nThere are varying definitions as to what sexual assault is defined as. According to the United States Department of Justice:\"The term 'sexual assault' means any nonconsensual sexual act proscribed by Federal, tribal, or State law, including when the victim lacks capacity to consent.\"Definitions and laws of sexual assault vary from state to state. The website FindLaw allows users to click on their corresponding to state to read about how their state defines what sexual assault is, as well as what laws and limitations exist. Sexual assault of LGBT individuals refers to the act of sexual violence against persons who identify as lesbian, gay, bisexual, or transgender amongst other sexualities and sexual minorities. \n\nThe Rape, Abuse and Incest National Network, also known as RAINN, puts out general statistics of sexual assault, including:\n\n\nAccording to a 2010 findings on The National Intimate Partner and Sexual Violence Survey, put out by the Centers for Disease Control and Prevention, it concluded that:\n\n\nIn a paper done by Emily Rothman, Deinera Exner, and Allyson Baughman, it was found that, in regard to lifetime sexual assault victimization, the estimated prevalence for lesbian and bisexual women was approximately 12.6 - 85%. For gay and bisexual men, it was 11.8-54.0%.\n\nThe underlying issues of sexual assault against LGBT persons includes homophobia and transphobia among other forms of prejudice against sexual minorities. These forms of prejudice end up creating unsafe spaces for LGBT persons where they do not feel comfortable expressing themselves or do not feel safe living as their authentic self. \n\nAccording to the Pride Resource Center at Colorado State University, homophobia is defined as:\"Homophobia is a pervasive, irrational fear of homosexuality. Homophobia includes the fear heterosexuals have of any homosexual feelings within themselves, any overt mannerisms or actions that would suggest homosexuality, and the resulting desire to suppress or stamp out homosexuality. And it also includes the self-hatred and self-denial of homosexuals who know what they are but have been taught all their lives by a heterosexual society that people like themselves are sick, sinful and criminal.”According to Planned Parenthood, transphobia is defined as:\"Transphobia is the fear, hatred, disbelief, or mistrust of people who are transgender, thought to be transgender, or whose gender expression doesn’t conform to traditional gender roles. Transphobia can prevent transgender and gender nonconforming people from living full lives free from harm.\"These phobias, as well as myths centered around LGBT individuals, influence a culture that holds back persons from identifying how they choose and places them into a binary that doesn't encompass all possible ways of identifying. One of the main myths that can harm LGBT individuals is the idea that they have not met the right guy/girl yet. This perpetuates heterosexism and denies the possibility of one having attraction to the same gender. \n\nOne of the biggest misconceptions that comes out of transphobia centers around the use of bathrooms. Made popular by North Carolina's \"Bathroom Bill\", it brought up the concern that by allowing individuals to use the bathroom that corresponds to the gender they identify as, predators could easily enter bathrooms and take advantage of individuals claiming that they identify as the gender the bathroom is. Legislation, such as House Bill 2, only perpetuate the discrimination of transgender individuals, rather than give them support to live their lives as the gender by which they identify. Laws have always existed that prohibit individuals from entering bathrooms with the purpose of harming others.\n"}
{"id": "16853466", "url": "https://en.wikipedia.org/wiki?curid=16853466", "title": "Social panic", "text": "Social panic\n\nA social panic is a state where a social or community group reacts negatively and in an extreme or irrational manner to unexpected or unforeseen changes in their expected social status quo. According to \"Folk Devils and Moral Panics\" by Stanley Cohen, the definition can be broken down to many different sections. The sections, which were identified by Erich Goode and Nachman Ben-Yehuda in 1994, include concern, hostility, consensus, disproportionality, and volatility. Concern, which is not to be mistaken with fear, is about the possible or potential threat. Hostility occurs when an outrage occurs toward the people who were a part of the problem and agencies who are accountable. These people are seen as the enemy since their behavior is viewed as a danger to society. Consensus includes a distributed agreement that an actual threat is going to take place. This is where the media and other sources come in to aid in spreading of the panic. Disproportionality compares how people react and the actual seriousness of the condition. Volatility is when there is no longer any more panic.\n\n\nThe media plays a crucial part in delivering social reaction. According to Stanley Cohen, there are three processes that the media expresses: Exaggeration and distortion, prediction, and symbolization.\n\nIn this process, the media can \"over-report\" with their choice of words. For example, the word \"disturbance\" can be used to mean having a noise complaint due to loud music next door and a group of people acting violently by throwing rocks and setting vehicles on fire. The wording of the stories can make a minor problem seem more serious than it really is. This can make people overreact in response to relatively minor problems and may lead them to believe that disturbances, acts of terrorism, riots, and instances have the same meaning.\n\nFurthermore, the headlines used by the media might cause society to act irrationally to a story about minor issues. They can be misleading and can report information that has nothing to do with the actual story. Negative words such as \"violence\" can be used when there was no violence involved. The media can also point to specific characteristics that are the reason for the crime that was committed. For example, a story can discuss a murder, but the headline might focus on the hoodie the culprit was wearing. Emphasizing the hoodie will draw attention to what the person was wearing instead of the murder that took place. This causes people to become paranoid and overreact when they see someone wearing clothing that looks suspicious.\n\nThis is where the media speculates that an incident might occur again. The media can report that an event will occur in the future, which is not always the case. People involved describe what should be done the next time it happens and what precautions should be taken. Predicting the future can cause people to constantly think about what could go wrong and lead to catastrophe. This can cause major stress and cause people to have social panics more often. However, there are certain situations where making predictions is necessary for security, such as hurricanes, earthquakes, and other natural disasters.\n\nThis involves stereotypes, words and images that possess symbolic powers that can cause different emotions. Symbolization can be described in three processes. It includes words such as \"deviant\" and, as Cohen would say, \"it becomes symbolic of a certain status.\" By this he means that the word represents something meaningful. Then the object, which can include clothing, represents the word. Therefore, the object can also symbolize the status. Neutral words can symbolize an event or emotion. For example, people can have specific feelings connected to the word \"Hiroshima\" that remind them of the bombing that occurred there. Furthermore, the use of labels given to a person or word puts them in a certain group in society. Those individuals that are in that group are viewed and interpreted based on their label.\n\nSymbolization, exaggeration and distortion are similar in the sense that they both use misleading headlines that creates negative symbols. For example, images can be posed to seem more dramatic or intense than they really are. Through both of these procedures, it is easy to cause people to come to conclusions that the news and photographs always display reality.\n\nAfter the events of 9/11, people were left in fear of crisis occurring again. According to Robert Wuthnow in his book \"Be Very Afraid\", people have responded aggressively, spending large amounts of money in fighting terrorism. The United States has spent that billions and trillions on defense and homeland security respectively. However, the problem lies in how we react.\n\nSince people have become more defensive, the focus needs to be on the correct way to act instead of an improper response. As mentioned earlier, predicting about the endless possibilities about what can happen can be just dangerous as the threat itself. People don't believe they can defend themselves from future terrorist's attacks. Individuals were constantly reminded of the concern and fear they should be experiencing by the tremendous amount of media coverage and books being published after the September 11 attacks. The event caused \"personal engagement\" throughout the country. In Boston, people questioned others about ties they had with Osama bin Laden. These attacks were unlike any other attacks since people experienced them firsthand whether on the news or in person. The natural response of Americans during this time was to take action—facing the fear of terrorism, whether by taking revenge or through urging caution.\n\nAngela McRobbie and Sarah Thornton claim that Stanley Cohen's work on moral panic is outdated and argue that more modern information is required. McRobbie suggests that idea of moral panic has become so common that the media knowingly and mindfully uses it. Thornton argues that the media originally caused moral panics inadvertently; however, the media now manipulates it on its own.\n\nYvonne Jewkes acknowledged problems and formed concepts about moral panic. To begin with, she described the term as vague and the failure to clarify the position of the public \"as media audiences or a body of opinion\". She goes on to explain that the social panics are not gladly received by the government. There is also no proof that society has an extensive social anxiety surrounding them. Jewkew, as also mentioned by McRobbie, believes that moral panic is widely used by the media. She concluded that in order for this to become a \"sound conceptual basis\" it needs to be revised and carefully improved.\n\n"}
{"id": "26298897", "url": "https://en.wikipedia.org/wiki?curid=26298897", "title": "Soviet Jewry Movement", "text": "Soviet Jewry Movement\n\nThe Soviet Jewry movement was an international human rights campaign that advocated for the right of Jews in the Soviet Union to emigrate.\n\nThe earliest organized effort was the Cleveland Council on Soviet Anti-Semitism, a grassroots organization that brought attention to the plight of Soviet Jews from 1963 until 1983. It began as a study group led by three of the founding members of Beth Israel - The West Temple in 1963: Louis Rosenblum, Herbert Caron, and Abe Silverstein. Though the council included prominent rabbis, pastors, priests, and city officials, many initial council members were fellow congregants. As the first such group in the world, this organization spawned other local councils and a national organization. Between 1964-69, the Cleveland council developed educational tools, such as organizational handbooks for other communities, the newsletter Spotlight, and media presentations. They also devised protest strategies that became integral to the movement to free Soviet Jewry. One of the council's most successful activities was the People-to-People program of the late 1960s, which represented 50,000 members.\n\nAlthough not officially sponsored by Beth Israel – The West Temple, the temple provided office space to the council from 1964–78, and the council periodically reported to the congregation's Social Action Committee. Although the Cleveland council was still active in 1985, by the late 1970s the Jewish Community Federation had taken over the major local organizing effort for Soviet Jewry. By 1993, the Cleveland Council on Soviet Anti-Semitism no longer needed to exist, as it had accomplished its mission, and the Soviet Union had also ceased to exist.\n\nLater, Student Struggle for Soviet Jewry, founded by Jacob Birnbaum at Yeshiva University in 1964. In 1969, the Jewish Defense League began a series of protests and vigils while employing militant activism in order to publicize the persecution of Soviet Jewry. The Union of Councils for Soviet Jews was formed in 1970 as an umbrella organization of all groups working to win the right to emigrate for oppressed Jewish citizens of the Soviet Union.\n\nThe movement was represented in Israel by Nativ, a clandestine agency that sought to publicize the cause of Soviet Jewry and encourage their emigration to Israel.\n\nThroughout the timeline of the movement to free Jews from the USSR -- 1964 - 1991 -- tensions existed between the Jewish Establishment groups, represented by the umbrella organization the American Jewish Conference on Soviet Jewry and its successor the National Conference on Soviet Jewry. Differences revolved around policy and action. Behind the scenes, the clandestine Israeli Soviet Jewry office, Nativ (known as the Lishka), supported the ACSJ and NCSJ, which it had helped create. Such conflicts between Establishment and nascent, independent groups -- such as between the NAACP and SNCC in the civil rights movement -- are not new.\n\n\n"}
{"id": "26685", "url": "https://en.wikipedia.org/wiki?curid=26685", "title": "Statistics", "text": "Statistics\n\nStatistics is a branch of mathematics dealing with data collection, organization, analysis, interpretation and presentation. In applying statistics to, for example, a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.\nSee glossary of probability and statistics.\n\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a \"distribution\" (sample or population): \"central tendency\" (or \"location\") seeks to characterize the distribution's central or typical value, while \"dispersion\" (or \"variability\") characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\n\nA standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\"). Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. \n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\nStatistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. In more recent years statistics has relied more on statistical software to produce tests such as descriptive analysis.\n\nSome definitions are:\n\nStatistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.\n\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.\n\nIn applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".\n\nIdeally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. \"Descriptive statistics\" can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).\n\nWhen a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, \"inferential statistics\" is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.\n\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models. The idea of making inferences based on sampled data began around the mid-1600s in connection with estimating populations and developing precursors of life insurance.\n\nTo use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method.\nThe difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\n\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective.\nAn experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated.\nWhile the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\nThe basic steps of a statistical experiment are:\n\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.\n\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).\n\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).\n\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\n\nA statistic is a random variable that is a function of the random sample, but \"not a function of unknown parameters\". The probability distribution of the statistic, though, may have unknown parameters.\n\nConsider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution \"does not depend on the unknown parameter\" is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.\n\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H, asserts that the defendant is innocent, whereas the alternative hypothesis, H, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H (status quo) stands in opposition to H and is maintained unless H is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily \"accept\" H but \"fails to reject\" H. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\nWhat statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.\n\nWorking from a null hypothesis, two basic forms of error are recognized:\n\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n\nA statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\nMost studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does \"not\" imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\nThe standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n\nAlthough in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.\n\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nSome well-known statistical tests and procedures are:\n\nMisuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book \"How to Lie with Statistics\" outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).\n\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"\n\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)\n\nSome scholars pinpoint the origin of statistics to 1663, with the publication of \"Natural and Political Observations upon the Bills of Mortality\" by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its \"stat-\" etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nIts mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The method of least squares was first described by Adrien-Marie Legendre in 1805.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight, eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded \"Biometrika\" as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.\n\nRonald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".\n\nThe second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper \"The Correlation between Relatives on the Supposition of Mendelian Inheritance\", which was the first to use the statistical term, variance, his classic 1925 work \"Statistical Methods for Research Workers\" and his 1935 \"The Design of Experiments\", where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. In his 1930 book \"The Genetical Theory of Natural Selection\" he applied statistics to various biological concepts such as Fisher's principle). Nevertheless, A.W.F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\". (about the sex ratio), the Fisherian runaway, a concept in sexual selection about a positive feedback runaway affect found in evolution.\n\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.\n\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.\n\n\"Applied statistics\" comprises descriptive statistics and the application of inferential statistics. \"Theoretical statistics\" concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing \"mathematical statistics\". Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n\nThere are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.\n\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This tradition has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.\n\n\n"}
{"id": "2988239", "url": "https://en.wikipedia.org/wiki?curid=2988239", "title": "Student fee", "text": "Student fee\n\nA student fee is a fee charged to students at a school, college, university or other place of learning that is in addition to any matriculation and/or tuition fees. It may be charged to support student organizations and student activities (for which it can be called an activity fee) or for intercollegiate programs such as intramural sports or visiting academics; or, at a public university or college, as a means to remedy shortfalls in state funding (in which case it can often be called a technology fee). Further fees may then be charged for features and facilities such as insurance, health and parking provision.\n\nIn the United States, the constitutionality of mandatory student activity fees has been adjudicated several times by the Supreme Court. Most recently, the Court has ruled that public universities may subsidize by means of a mandatory student activity fees so long as such fees (see \"Board of Regents of the University of Wisconsin System v. Southworth\" ).\n\n"}
{"id": "2013774", "url": "https://en.wikipedia.org/wiki?curid=2013774", "title": "Swart gevaar", "text": "Swart gevaar\n\nSwart gevaar (Afrikaans for \"black danger\") was a term used during apartheid in South Africa to refer to the perceived security threat of the majority black African population to the white South African government.\n\n"}
{"id": "33184758", "url": "https://en.wikipedia.org/wiki?curid=33184758", "title": "Tadesse Meskela", "text": "Tadesse Meskela\n\nTadesse Meskela is the General Manager of the Oromia Coffee Farmers Cooperative Union of Ethiopia and was featured in the documentary \"Black Gold\". He is a proponent of fair-trade, and speaks publicly in support of it around the world. \n\nHe grew up in the countryside outside Addis Ababa, Bishoftu. Tadesse Meskela studied his way out of poverty and eventually made it to university. By the early 1990s he was working for the state Agricultural Bureau. He attended a two-month co-operative training placement in Japan and was inspired to develop a cooperative union system to help Ethiopian coffee farmers attain a fairer recompense for their produce. The Oromia Coffee Farmers Co-operative Union was established in 1999. In the year 2004, the union has facilitated the constructed of four new schools, seventeen additional classrooms, four health centres, two clean water supply stations, and in terms of dividends, $2 million has been given back to the farmers.\n\n\n"}
{"id": "853622", "url": "https://en.wikipedia.org/wiki?curid=853622", "title": "The Open Theater", "text": "The Open Theater\n\nThe Open Theater was an experimental theatre group active from 1963 to 1973.\n\nIt was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. Megan Terry (often left out of the list of the founders of the Open Theatre due to her being a woman and pioneering in feminist drama, but nonetheless a co-founder of the group), Sam Shepard, Peter Feldman, and Joseph Chaikin collectively founded the Open Theater; Chaikin joined after leaving the Living Theater following the arrest of Julian Beck and Judith Malina for tax evasion. He felt that the Living Theater had become less interested in artistic exploration and experimentation, and more interested in political activism. He felt that actors needed specific training to do the sorts of pieces that the Living Theater did. The group's intent was to continue Chilton's exploration of a \"post-method\", post-absurd acting technique, by way of a collaborative and wide-ranging process that included exploration of political, artistic, and social issues, which were felt to be critical to the success of avant-garde theatre. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as \"sound and movement\" and \"transformations\", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's \"poor theater\" in Poland. According to playwright Megan Terry the notion of a minimalist aesthetic was fueled by the company's quest to achieve narrative insight and political accountability through the body of the actor:\n\nThe Open Theater formed as an offshoot of the Living Theater. The Living Theater initially divided because actor Joseph Chaikin felt that the troupe had become less interested in artistic exploration and experimentation, and more interested in political activism. He felt that actors needed specific training to do the sorts of pieces that the Living Theater did.\n\nChaikin’s theatrical exploration came from within; he was interested in the personal experience of the actor, and of the actor’s relationship to the rest of the community. He developed improvisational exercises designed to help the actor become freer. The technique was initially inspired by method acting, in which the actor draws on his or her own experiences and emotions, but the goal of Chaikin’s work was to free the actor from the natural restraints of method acting. He called his technique the “sound and movement” technique. In his book entitled \"The Presence of the Actor\", Chaikin wrote “Traditional acting in America has become a blend of that same kind of synthetic ‘feeling’ and sentimentality which characterizes the Fourth of July parade, Muzak, church services, and political campaigns.” This further explains the purpose for the Open Theater and the reasoning behind opening such a space.\n\nChaikin used ensemble casts as a way to explore the relationship between the actor and the outside world. He relied on the performers to interact not as characters in fictional settings, but as real people in real situations. While the Living Theater’s ensemble was very external and audience oriented, Chaikin’s was internal and oriented within the troupe itself. In 1963, Chaikin said “Working together, we teach ourselves.” The ensemble worked in the \"poor theater\" style. There was no need for sets, costumes, props, or any of the other theatrical elements. There were no moving lights, only enough stationary light to be able tosee. There was no music, instead, the actors would use their voices to create the sound effects. There were hardly any aspects of \"rich\" theater involved.\n\nOn his reasons for using ensembles instead of a traditional cast and show, Chaikin said:\n\n“I felt a terrific longing for a kind of ensemble. I wanted to play with actors, actors who felt a sensitivity for one another… In order to come to a vocabulary, we had to teach each other: we had no ambitions other than to meet and play around… Off-off Broadway’s impulse was a terrific dissatisfaction with what is possible on Broadway… Off-off broadway is really an attack on the fourth wall. I want to destroy the fourth-wall business. I have difficulty believing most of what happens on Broadway. Mary Martin’s like a character in a television commercial: nobody’s like that.\"\n\nThe ensemble met on Spring Street in New York, in a warehouse.\n\nChaikin disbanded the Open Theater in 1973, amid fears that it was becoming too institutionalized. He later went to work for the Public Theater. Chaikin continued to create workshops, but he was increasingly working on mainstream projects.\n\nDuring the first two years of its existence, there were no productions performed by the Open Theater. Instead, there were occasional open rehearsals or workshops.\n\nThe first major production was \"The Serpent,\" and it toured both nationally and internationally.\"The Serpent\" typifies what is extraordinary about the Open Theater. The production is a perfect example of experimental theatre at the time and how willing the company was to employ unconventional theatre techniques that were based on improvisation in order to created a very pointed piece. According to the playwright Jean Claude Van Itallie, a member of the company, the piece was conceived with the idea that the actors would act as priests and that there would be a sense of one-ness amongst the congregation that they are questioning the same things in which the priest is questioning. Questions such as where does evil come from? What provokes people to commit murder? Is maintaining innocence a possibility within the contaminated world that which we walk upon? Though most of the piece is done in choreographed movement, mime, human sound-effects, hand-held instruments, there is text as well coming directly from the Bible along with a number of speeches written exclusively for the show.\n\nThe plot of \"The Serpent\" takes an extremely non-direct route in order to answer these questions. The play opens with a group of actors onstage where one recites a graphic description of an autopsy while the other members of the group are creating different sound effects and movements that match the text. Then the remaining members of the ensemble join the stage to partake in a stylized reenactment of the John F Kennedy assassination. One actor counts slowly to twelve while the other actors partake in the reenactment in slow motion, moving to a new position and new sound effect per count. This scene is run forward, backwards, and out of order to the point that it becomes so ritualized that there is now an element of comedy in play within this depiction of someone’s untimely death. There is a sense of realization of how the horror of the murder rushes over the ensemble as they break free from their rigid formation and become instead a group of individuals fleeing from the evil that has just occurred. The ensemble then moves to the retelling of the banishment of Adam and Eve from the Garden of Eden. Four actors take the shape of the serpent that convinces Eve to eat the forbidden fruit and are present as Eve then moves to convince Adam to partake of it. After being banished by God from the Garden, the actors then partake in a ritual enactment of the first true discovery of sexual love which takes place alongside a recitation of how the descendants of Adam begat the rest of Mankind. This passionate celebration of sexuality is very much embraced onstage. The ensemble eventually collapses slowly to the floor when, at first, a hum falls over the group which soon turns into song. They eventually rise to their feet and travel down the aisles embracing audience members and smiling until the song is complete.\n\n\"The Serpent\" aims to remind the audience of the idea that we are all caught in a neverending battle between the fact that we are neither as innocent nor as guilty as we may think we might be, having fallen victim to the at times foul planet that we reside on. The goal of the play was not to find answers the specific questions which were asked, but rather to visualize these inquiries through improvisation and to help the actors, as well as the audience members, to find their own personal truths.\n\nSome of the company's best known works include Terry's \"Viet Rock\" (1966) with musical compositions by Marianne de Pury, Jean-Claude van Itallie's \"America Hurrah\" (1966) and \"The Serpent\" (1969).\n\nOther works included \"Endgame, American Hurrah, The Mutation Show, Nightwalk, and Terminal.\"\n\nMembers of the theatre simulated an orgy in Death Valley in a scene in the 1970 movie \"Zabriskie Point\". The U.S. Justice Department later investigated the film questioning whether the orgy violated the Mann Act which criminalized the interstate transport of females for \"immoral purposes\". However, the movie producers pointed out that no actual sex had taken place and that the actors had not crossed a state line since the town of Zabriskie Point is in California.\n\nAfter the company's dissolution, its members formed The Talking Band, and Medicine Show Theatre Ensemble and Spiderwoman Theater. Chaikin went on to have a celebrated career as a theatre director until his death in 2003.\n"}
{"id": "44339398", "url": "https://en.wikipedia.org/wiki?curid=44339398", "title": "Theodor Wolff", "text": "Theodor Wolff\n\nTheodor Wolff (2 August 1868 – 23 September 1943) was a German writer who was influential as a journalist, critic and newspaper editor. He was born and died in Berlin. Between 1906 and 1933 he was the chief editor of the politically liberal newspaper \"Berliner Tageblatt\".\n\nHis talent as a writer won praise from an unlikely quarter: In 1939 Joseph Goebbels recommended his Propaganda Ministry staff to study Wolff's contributions in back numbers of the newspaper that he had edited. According to Goebbels, despite his being Jewish, the quality of Wolff's writing was matched by only very few in Germany.\n\nTheodor Wolff was born in Berlin, second of the four recorded children of a fabric wholesaler from Silesia called Adam Wolff by his marriage to Recha, née Davidsohn. Recha was a doctor's daughter from Danzig. Wolff grew up in a prosperous Jewish family. He rapidly achieved good results at the prestigious in Berlin.\n\nHe married in 1902, in Paris, the actress Marie Louise Charlotte Anna Hickethier (known as Änne), coming from a Protestant Prussian family. The couple had three children: Richard Wolff (born in Paris, 14 June 1906), Rudolf Wolff (born in Berlin, 9 July 1907) and Lilly Wolff (born in Berlin, 7 August 1909). The children were baptised as Protestants.\n\nIn 1887 Wolff's cousin Rudolf Mosse recruited him to his successful publishing conglomerate. Mosse was 25 years older than Wolff, to whom he provided a thorough commercial and journalistic training across all the departments of his publishing business, the \"Mosse-Verlag\". During these years Wolff also found time to write some early novels, inspired by Theodor Fontane whom he greatly admired, and several plays which were staged in Berlin, though in his memoirs he would later describe these as \"not particularly distinguished\". In 1889 he was one of the ten co-founders of the Berlin theatre company, (\"Free stage\").\n\nWollf's written contributions to the \"Tageblatt\" focused initially on cultural matters and literature, but he soon switched his focus towards political journalism. An early journalistic success at the newspaper was a daily bulletin on the health of the emperor (who was dying of throat cancer). Emperor Frederick died in June 1888 and Wolff embarked on an itinerant career, writing pieces and sending them in to Berlin by telegraph from, successively, Denmark, Sweden, Norway and Italy. In 1894 the paper sent him to Paris, where he would live for the next twelve years.\n\nAs the Paris correspondent of the \"Berliner Tageblatt\" Wolff, identified in print only as \"our Paris correspondent\", produced numerous contributions covering public life in France. During 1896 he became known for his coverage of the Dreyfus affair, one of the most high-profile news stories of the decade in western Europe. His early ambitions to become a novelist were now being overtaken by his success as a journalist.\n\nIn autumn 1906, Rudolf Mosse offered Theodor Wolff the top job at the \"Berliner Tageblatt\". Between 1906 and 1933 Wolff served the \"Tageblatt\" as editor-in-chief, developing it into one of Germany's most influential newspapers. Circulation under his stewardship rose from 100,000 to more than 300,000. His powerful prose was notably on display in the Monday editions of the paper for which he wrote the lead article, frequently exhorting fellow citizens to political participation. In respect of foreign policy, he quickly positioned the \"Tageblatt\" in opposition to \"great power politics\", imperial and military assertiveness and the risk of international isolation to which these were leading Germany. On domestic issues the paper's attitude under Wolff favoured civil rights and a liberal-democratic approach, advocating a \"parliamentarisation\" (\"Parlamentarisierung\") of the constitution and vigorously opposing the \"Dreiklassenwahlrecht\" which had been introduced in 1849 and which was, by the beginning of the twentieth century, widely perceived as a badly flawed application of the democratic ideal.\n\nAt this time Wolff promoted numerous writers including Victor Auburtin whose individualistic approach he valued and who played an important part in defining the newspaper's liberal profile. Meanwhile, Germany's traditionalist Chancellor, Bernhard von Bülow, refused any interviews or statements to the \"Tageblatt\" while his successor, Theobald von Bethmann Hollweg, instructed all government departments to steer well clear of the newspaper's reports and opinions.\n\nIn July 1916, the \"Berliner Tageblatt\" was temporarily banned. Wolff reacted by refusing to publish anything for several months, which was picked up and used abroad in the savage propaganda battle that was a major element in the increasingly desperate conduct of the First World War. Wolff refused to compromise his editorial line, and the \"Berliner Tageblatt\" continued to promote the politically toxic view that the only route to a lasting peace was for Germany to come to an understanding with France.\n\nIn November 1918 Theodor Wolff was one of the founders of the German Democratic Party (\"Deutsche Demokratische Partei\", DDP), committed to individual freedom and social responsibility. He had himself played a central role in the party's defining manifesto, but he took no leadership position within the DDP preferring, not for the last time, the role of a powerful newspaper editor. It was in this role that he called on the government to reject the Treaty of Versailles. In 1920 Hermann Müller, who in the early summer was briefly the German Chancellor, invited Wolff to take on the position of German Ambassador to Paris but, again, Wolff opted to remain a Berlin-based journalist.\n\nOn 4 December 1926 Wolff resigned his DDP membership. The resignation came in response to the acceptance by a large number of the party's of tightened censorship laws against so-called dirty and trashy literature. (Kurt Tucholsky, one of the \"Tageblatt\"<nowiki>'</nowiki>s most high-profile contributors, had also, on the world stage, sharply condemned DDP support for these measures.) Wolff nevertheless remained powerfully influential, a leading advocate for democracy and moderation, welcomed as a dinner guest by various government ministers. Wolff was also continuing to attract important writers as contributors for the \"Tageblatt\". In 1926 he persuaded the pugnaciously liberal journalist-lawyer Rudolf Olden to move his base from Vienna to Berlin.\n\nGermany's military defeat and the catastrophic economic aftermath did much to discredit democratic politics during the 1920s, which saw a corresponding growth in support for right wing politics, which in their turn favoured somewhat 'tribal' definitions of the political sphere. Theodor Wolff and his \"Berliner Tageblatt\" (the \"Jewish news-sheet\", \"das Judenblatt\") were increasingly targeted by and during the 1920s nationalists were increasingly setting the country's political agenda. His name started to appear on the death lists of various radical-right and populist groups, causing Wolff to become anxious that he might share the fate of Walther Rathenau, the generally popular Jewish Foreign Minister and fellow DDP member who had been shot dead by a gang of three extremists in June 1922. Anxiety that he might be murdered by racist extremists remained with him for the rest of his life. His counterpart at the right wing Hugenberg media group, editor in chief , stirred up popular hatred of Wolff, whom he identified as a representing the liberal metropolitan press.\n\nThe Reichstag election of September 1930 transformed the country's political landscape, with the National Socialist party increasing its share of the popular vote from 2.6% to 18.3%, which under the country's multi-party system left it as the second largest party in the Reichstag. Right wing extremism was suddenly mainstream and at the \"Berliner Tageblatt\" the publisher's proprietor, Hans Lachmann-Mosse, who had taken over from his father-in-law, mandated a turn towards the right for the newspaper. The business was also in financial trouble thanks to poorly timed investment decisions and intensified competition from the by now openly anti-Semitic Hugenberg Group.\n\nThe night of 27 to 28 February 1933 was the night of the Reichstag fire. It was also the night on which Theodor Wolff, whose hostility to a Nazi future was undiminished and who had been warned by colleagues that his name was on the SA death list, fled Berlin. His initial destination, traveling via Munich, was the Tirol. From there he moved on to Switzerland. However, the Swiss refused to issue him with a residence permit and by the end of 1933 Theodor Wolff and his wife had ended up in Nice, which since 1860 had been part of France.\n\nTheodor Wolff's last lead article in the \"Berliner Tageblatt\" appeared on 5 March 1933, the day of the last multi-party German election until 1949. In March 1933 the \"Tageblatt\"<nowiki>'</nowiki>s proprietor (who himself, being Jewish, was effectively deprived of control over his business later in the month), removed Wolff from his editorship responding to political pressure following the flight from Berlin. In May 1933 Wolff's books were among those listed by the government for the public book burnings. Wolff celebrated his 65th birthday in 1933 and played very little part in the political struggles of the many German Jewish exiles who were gravitating to Nice at this time. His opinions nevertheless remained clear enough, and on 26 October 1937 he was deprived of his German citizenship.\n\nIn exile Wolff reverted to writing books, while still contributing occasional pieces of journalism to (non-German) newspapers. Two historical-political works met with little success. He dedicated his last novel, \"Die Schwimmerin\" (\"The [female] Swimmer\") to his secretary from the Berlin days, Ilse Stöbe (1911–1942). His project to have a film produced on the basis of this novel, with Greta Garbo in the starring role, could not be realised.\n\nWolff continued to distance himself from Zionist separatism, believing throughout his life in a \"German-Jewish symbiosis\" (\"deutsch-jüdische Symbiose\").\n\nAfter the Fall of France on 22 June 1940 Theodor Wolff applied, unsuccessfully, for permission to emigrate to United States: he remained in Nice. The coastal strip along the south-eastern part of France including, from late 1942, Nice was in the process of being annexed by Italy, and on 23 May 1943 Theodor Wolff was arrested by the Italian civil authorities. He was handed over to the Gestapo and interned in a Marseilles jail before being transferred to the Drancy detention centre near Paris. Drancy was used as a collection point for Jewish internees scheduled for deportation to internment camps and death camps in Germany, and Wolff now found himself transferred to Sachsenhausen concentration camp to the east of Berlin. Now aged 75, and ill with Phlegmon, his fellow internees pleaded successfully for him to be sent to the in Berlin. Here he was admitted on 20 September 1943: he died three days later.\n\nTheodor Wolff's body is buried in the \"row of honour\" at Berlin's Weißensee Cemetery.\n\nIn 1961 the Theodor Wolff Prize for newspaper journalism was founded, and it has since 1962 been awarded annually. Since 1973 the prize has been awarded by the .\n\n"}
{"id": "56474254", "url": "https://en.wikipedia.org/wiki?curid=56474254", "title": "Through the Dark (2016 film)", "text": "Through the Dark (2016 film)\n\nThrough the Dark is a 4 minute 3D-animated interactive film, which is viewed in a web browser and rendered in real-time. The film is a collaboration between Google Play Music and Australian hip hop group Hilltop Hoods, featuring the band's song of the same title (\"Through the Dark\"). The song was written by band member MC Pressure (Daniel Smith) after his son was diagnosed with leukemia at eight-years-old. Based on this experience, which is told through the song's lyrics, the film tells an emotional story of a boy and his father. \"Through the Dark\" has been awarded over 50 accolades at international award shows.\n\n\"Through the Dark\" is experienced within a web browser and uses standard web technologies such as WebGL and Web Audio API. When viewed on a mobile device, the film's virtual camera is mapped to the device's accelerometer. The camera angle within the film is changed when the device is rotated or tilted, allowing the viewer to move between two worlds of light and dark. When viewed on a computer, the movement of the camera is controlled by scrolling.\n\n"}
{"id": "1150749", "url": "https://en.wikipedia.org/wiki?curid=1150749", "title": "Universalizability", "text": "Universalizability\n\nThe concept of universalizability was set out by the 18th-century German philosopher Immanuel Kant as part of his work \"Groundwork of the Metaphysic of Morals\". It is part of the first formulation of his categorical imperative, which states that the only morally acceptable maxims of our actions are those that could rationally be willed to be universal law. \n\nThe precise meaning of universalizability is contentious, but the most common interpretation is that the categorical imperative asks whether the maxim of your action could become one that everyone could act upon in similar circumstances. An action is morally acceptable if it can be universalized (i.e., everyone could do it). \n\nFor instance, one can determine whether a maxim of lying to secure a loan is moral by attempting to universalize it and applying reason to the results. If everyone lied to secure loans, the very practices of promising and lending would fall apart, and the maxim would then become impossible.\n\nKant calls such acts examples of a contradiction in conception, which is much like a performative contradiction, because they undermine the very basis for their existence.\n\nKant's notion of universalizability has a clear antecedent in Rousseau's idea of a general will. Both notions provide for a radical separation of will and nature, leading to the idea that true freedom lies substantially in self-legislation. \n"}
{"id": "49901", "url": "https://en.wikipedia.org/wiki?curid=49901", "title": "Virtue", "text": "Virtue\n\nVirtue (, \"arete\") is moral excellence. A virtue is a trait or quality that is deemed to be morally good and thus is valued as a foundation of principle and good moral being. Personal virtues are characteristics valued as promoting collective and individual greatness. In other words, it is a behavior that shows high moral standards. Doing what is right and avoiding what is wrong. The opposite of virtue is vice. \n\nThe four classic cardinal virtues are temperance, prudence, courage, and justice. Christianity derives the three theological virtues of faith, hope and love (charity) from 1 Corinthians. Together these make up the seven virtues. Buddhism's four brahmavihara (\"Divine States\") can be regarded as virtues in the European sense. The Japanese Bushidō code is characterized by up to ten virtues, including rectitude, courage, and benevolence.\n\nThe ancient Romans used the Latin word \"virtus\" (derived from \"vir\", their word for \"man\") to refer to all of the \"excellent qualities of men, including physical strength, valorous conduct, and moral rectitude.\" The French words \"vertu\" and \"virtu\" came from this Latin root. In the 13th century, the word \"virtue\" was \"borrowed into English\".\n\nDuring Egyptian civilization, Maat or Ma'at (thought to have been pronounced *[muʔ.ʕat]), also spelled māt or mayet, was the ancient Egyptian concept of truth, balance, order, law, morality, and justice. Maat was also personified as a goddess regulating the stars, seasons, and the actions of both mortals and the deities. The deities set the order of the universe from chaos at the moment of creation. Her (ideological) counterpart was Isfet, who symbolized chaos, lies, and injustice.\n\nThe four classic cardinal virtues are:\n\nThis enumeration is traced to Greek philosophy and was listed by Plato in addition to piety: (hosiotēs), with the exception that wisdom replaced prudence as virtue. Some scholars consider either of the above four virtue combinations as mutually reducible and therefore not cardinal.\n\nIt is unclear whether multiple virtues were of later construct, and whether Plato subscribed to a unified view of virtues. In \"Protagoras\" and \"Meno\", for example, he states that the separate virtues cannot exist independently and offers as evidence the contradictions of acting with wisdom, yet in an unjust way; or acting with bravery (fortitude), yet without wisdom.\n\nIn his work \"Nicomachean Ethics\", Aristotle defined a virtue as a point between a deficiency and an excess of a trait. The point of greatest virtue lies not in the exact middle, but at a golden mean sometimes closer to one extreme than the other. However, the virtuous action is not simply the \"mean\" (mathematically speaking) between two opposite extremes. As Aristotle says in the Nicomachean Ethics: \"at the right times, about the right things, towards the right people, for the right end, and in the right way, is the intermediate and best condition, and this is proper to virtue.\" This is not simply splitting the difference between two extremes. For example, generosity is a virtue between the two extremes of miserliness and being profligate. Further examples include: courage between cowardice and foolhardiness, and confidence between self-deprecation and vanity. In Aristotle's sense, virtue is excellence at being human.\n\nSeneca, the Roman Stoic, said that perfect prudence is indistinguishable from perfect virtue. Thus, in considering all consequences, a prudent person would act in the same way as a virtuous person. The same rationale was expressed by Plato in Meno, when he wrote that people only act in ways that they perceive will bring them maximum good. It is the lack of wisdom that results in the making of a bad choice instead of a prudent one. In this way, wisdom is the central part of virtue. Plato realized that because virtue was synonymous with wisdom it could be taught, a possibility he had earlier discounted. He then added \"correct belief\" as an alternative to knowledge, proposing that knowledge is merely correct belief that has been thought through and \"tethered\".\n\nThe term \"virtue\" itself is derived from the Latin \"virtus\" (the personification of which was the deity Virtus), and had connotations of \"manliness\", \"honour\", worthiness of deferential respect, and civic duty as both citizen and soldier. This virtue was but one of many virtues which Romans of good character were expected to exemplify and pass on through the generations, as part of the Mos Maiorum; ancestral traditions which defined \"Roman-ness\". Romans distinguished between the spheres of private and public life, and thus, virtues were also divided between those considered to be in the realm of private family life (as lived and taught by the paterfamilias), and those expected of an upstanding Roman citizen.\n\nMost Roman concepts of virtue were also personified as a numinous deity. The primary Roman virtues, both public and private, were:\n\nIn 410 CE, Aurelius Prudentius Clemens listed seven \"heavenly virtues\" in his book Psychomachia (Battle of Souls) which is an allegorical story of conflict between vices and virtues. The virtues depicted were:\n\nIn the 8th Century, upon the occasion of his coronation as Holy Roman Emperor Charlemagne published a list of knightly virtues:\n\nLove God\nLove your neighbour\nGive alms to the poor\nEntertain strangers\nVisit the sick\nBe merciful to prisoners\nDo ill to no man, nor consent unto such\nForgive as ye hope to be forgiven\nRedeem the captive\nHelp the oppressed\nDefend the cause of the widow and orphan\nRender righteous judgement\nDo not consent to any wrong\nPersevere not in wrath\nShun excess in eating and drinking\nBe humble and kind\nServe your liege lord faithfully\nDo not steal\nDo not perjure yourself, nor let others do so\nEnvy, hatred and violence separate men from the Kingdom of God\nDefend the Church and promote her cause.\n\nLoving God and obeying his laws, in particular the Ten Commandments, are central to Jewish conceptions of virtue. Wisdom is personified in the first eight chapters of the Book of Proverbs and is not only the source of virtue but is depicted as the first and best creation of God (Proverbs 8:12-31). Wisdom is also celebrated in the Book of Wisdom.\n\nA classic articulation of the Golden Rule came from the first century Rabbi Hillel the Elder. Renowned in the Jewish tradition as a sage and a scholar, he is associated with the development of the Mishnah and the Talmud and, as such, one of the most important figures in Jewish history. Asked for a summary of the Jewish religion in the most concise terms, Hillel replied (reputedly while standing on one leg): \"That which is hateful to you, do not do to your fellow. That is the whole Torah. The rest is commentary; go and learn.\"\n\nIn Christianity, the three theological virtues are faith, hope and love, a list which comes from 1 Corinthians 13:13 ( \"pistis\" (faith), \"elpis\" (hope), \"agape\" (love), ). The same chapter describes love as the greatest of the three, and further defines love as \"patient, kind, not envious, boastful, arrogant, or rude.\" (The Christian virtue of love is sometimes called charity and at other times a Greek word agape is used to contrast the love of God and the love of humankind from other types of love such as friendship or physical affection.)\n\nChristian scholars frequently add the four Greek cardinal virtues (prudence, justice, temperance, and courage) to the theological virtues to give the seven virtues; for example, these seven are the ones described in the \"Catechism of the Catholic Church\", sections 1803–1829.\n\nThe Bible mentions additional virtues, such as in the \"Fruit of the Holy Spirit,\" found in Galatians 5:22-23: \"By contrast, the fruit of the Spirit it is benevolent-love: joy, peace, longsuffering, kindness, benevolence, faithfulness, gentleness, and self-control. There is absolutely no law against such a thing.\"\n\nThe medieval and renaissance periods saw a number of models of sin listing the seven deadly sins and the virtues opposed to each.\n\nIn Islam, the Qur'an is believed to be the literal word of God, and the definitive description of virtue. Muhammad is considered an ideal example of virtue in human form. The hadiths, his reported sayings, are central to the Islamic understanding of virtue.\n\nVirtue is defined in hadith. It is reported by An-Nawwas bin Sam'an:\nWabisah bin Ma’bad reported:\nFor Muslims fulfilling the human rights are valued as an important building block of Islam, According to Muslim beliefs Allah will forgive individual sins but the bad treatment of humans and injustice with others will only be pardoned by the humans and not by Allah.\nIn Jainism, attainment of enlightenment is possible only if the seeker possesses certain virtues. All Jains are supposed to take up the five vows of ahimsa (non violence), satya (truthfulness), asteya (non stealing), aparigraha (non attachment and brahmacharya (celibacy) before becoming a monk. These vows are laid down by the Tirthankaras. Other virtues which are supposed to be followed by both monks as well as laypersons include forgiveness, humility, self-restraint and straightforwardness. These vows assists the seeker to escape from the karmic bondages thereby escaping the cycle of birth and death to attain liberation.\n\nVirtue is a much debated and an evolving concept in ancient scriptures of Hinduism. The essence, need and value of virtue is explained in Hindu philosophy as something that cannot be imposed, but something that is realized and voluntarily lived up to by each individual. For example, Apastamba explained it thus: \"virtue and vice do not go about saying - here we are!; neither the Gods, Gandharvas, nor ancestors can convince us - this is right, this is wrong; virtue is an elusive concept, it demands careful and sustained reflection by every man and woman before it can become part of one's life.\n\nVirtues lead to \"punya\" (Sanskrit: पुण्य, holy living) in Hindu literature; while vices lead to \"pap\" (Sanskrit: पाप, sin). Sometimes, the word \"punya\" is used interchangeably with virtue.\n\nThe virtues that constitute a dharmic life - that is a moral, ethical, virtuous life - evolve in vedas and upanishads. Over time, new virtues were conceptualized and added by ancient Hindu scholars, some replaced, others merged. For example, Manusamhita initially listed ten virtues necessary for a human being to live a \"dharmic\" life: \"Dhriti\" (courage), \"Kshama\" (forgiveness), \"Dama\" (temperance), \"Asteya\" (Non-covetousness/Non-stealing), \"Saucha\" (inner purity), \"Indriyani-graha\" (control of senses), \"dhi\" (reflective prudence), \"vidya\" (wisdom), \"satyam\" (truthfulness), \"akrodha\" (freedom from anger). In later verses, this list was reduced to five virtues by the same scholar, by merging and creating a more broader concept. The shorter list of virtues became: \"Ahimsa\" (Non-violence), \"Dama\" (self restraint), \"Asteya\" (Non-covetousness/Non-stealing), \"Saucha\" (inner purity), \"Satyam\" (truthfulness).\n\nThe Bhagavad Gita - considered one of the epitomes of historic Hindu discussion of virtues and an allegorical debate on what is right and what is wrong - argues some virtues are not necessarily always absolute, but sometimes relational; for example, it explains a virtue such as Ahimsa must be re-examined when one is faced with war or violence from the aggressiveness, immaturity or ignorance of others.\n\nBuddhist practice as outlined in the Noble Eightfold Path can be regarded as a progressive list of virtues.\n\nBuddhism's four \"brahmavihara\" (\"Divine States\") can be more properly regarded as virtues in the European sense. They are:\n\nThere are also the Paramitas (\"perfections\"), which are the culmination of having acquired certain virtues. In Theravada Buddhism's canonical Buddhavamsa there are Ten Perfections (\"dasa pāramiyo\"). In Mahayana Buddhism, the Lotus Sutra (\"Saddharmapundarika\"), there are Six Perfections; while in the Ten Stages (\"Dasabhumika\") Sutra, four more \"Paramitas\" are listed.\n\nIn the Bahá'í Faith, virtues are direct spiritual qualities that the human soul possesses, inherited from the world of God. The development and manifestation of these virtues is the theme of the \"Hidden Words\" of Bahá'u'lláh and are discussed in great detail as the underpinnings of a divinely-inspired society by `Abdu'l-Bahá in such texts as \"The Secret of Divine Civilization\".\n\n\"Virtue\", translated from Chinese \"de\" (), is also an important concept in Chinese philosophy, particularly Daoism. \"De\" () originally meant normative \"virtue\" in the sense of \"personal character; inner strength; integrity\", but semantically changed to moral \"virtue; kindness; morality\". Note the semantic parallel for English \"\", with an archaic meaning of \"inner potency; divine power\" (as in \"by virtue of\") and a modern one of \"moral excellence; goodness\".\n\nIn early periods of Confucianism, moral manifestations of \"virtue\" include \"ren\" (\"humanity\"), \"xiao\" (\"filial piety\"), and \"li\" (\"proper behavior, performance of rituals\"). The notion of ren - according to Simon Leys - means \"humanity\" and \"goodness\". Ren originally had the archaic meaning in the Confucian Book of Poems of \"virility\", but progressively took on shades of ethical meaning. Some scholars consider the virtues identified in early Confucianism as non-theistic philosophy.\n\nThe Daoist concept of \"De\", compared to Confucianism, is more subtle, pertaining to the \"virtue\" or ability that an individual realizes by following the Dao (\"the Way\"). One important normative value in much of Chinese thinking is that one's social status should result from the amount of virtue that one demonstrates, rather than from one's birth. In the \"Analects\", Confucius explains \"de\" as follows: \"He who exercises government by means of his virtue may be compared to the north polar star, which keeps its place and all the stars turn towards it.\" In later periods, particularly from the Tang dynasty period, Confucianism as practiced, absorbed and melded its own concepts of virtues with those from Daoism and Buddhism.\n\nIn Hagakure, Yamamoto Tsunetomo encapsulates his views on 'virtue' in the four vows he makes daily:\n\nYamamoto goes on to say:\nIf one dedicates these four vows to the gods and Buddhas every morning, he will have the strength of two men and never slip backward. One must edge forward like the inchworm, bit by bit. The gods and Buddhas, too, first started with a vow.\n\nThe Bushidō code is typified by seven virtues:\n\nOthers that are sometimes added to these:\n\nWhile religious scriptures generally consider \"dharma\" or \"aṟam\" (the Tamil term for virtue) as a divine virtue, Valluvar describes it as a way of life rather than any spiritual observance, a way of harmonious living that leads to universal happiness. For this reason, Valluvar keeps \"aṟam\" as the cornerstone throughout the writing of the Kural literature. Valluvar considered justice as a facet or product of \"aram.\" While ancient Greek philosophers such as Plato, Aristotle, and their descendants opined that justice cannot be defined and that it was a divine mystery, Valluvar positively suggested that a divine origin is not required to define the concept of justice. In the words of V. R. Nedunchezhiyan, justice according to Valluvar \"dwells in the minds of those who have knowledge of the standard of right and wrong; so too deceit dwells in the minds which breed fraud.\"\n\nFor the Rationalist philosopher René Descartes, virtue consists in the correct reasoning that should guide our actions. Men should seek the sovereign good that Descartes, following Zeno, identifies with virtue, as this produces a solid blessedness or pleasure. For Epicurus the sovereign good was pleasure, and Descartes says that in fact this is not in contradiction with Zeno's teaching, because virtue produces a spiritual pleasure, that is better than bodily pleasure. Regarding Aristotle's opinion that happiness depends on the goods of fortune, Descartes does not deny that these goods contribute to happiness, but remarks that they are in great proportion outside one's own control, whereas one's mind is under one's complete control.\n\nImmanuel Kant, in his \"Observations on the Feeling of the Beautiful and Sublime\", expresses true virtue as different from what commonly is known about this moral trait. In Kant's view, to be goodhearted, benevolent and sympathetic is not regarded as true virtue. The only aspect that makes a human truly virtuous is to behave in accordance with moral principles. Kant presents an example for more clarification; suppose that you come across a needy person in the street; if your sympathy leads you to help that person, your response does not illustrate your virtue. In this example, since you do not afford helping all needy ones, you have behaved unjustly, and it is out of the domain of principles and true virtue. Kant applies the approach of four temperaments to distinguish truly virtuous people. According to Kant, among all people with diverse temperaments, a person with melancholy frame of mind is the most virtuous whose thoughts, words and deeds are one of principles.\n\nFriedrich Nietzsche's view of virtue is based on the idea of an order of rank among people. For Nietzsche, the virtues of the strong are seen as vices by the weak and slavish, thus Nietzsche's virtue ethics is based on his distinction between master morality and slave morality. Nietzsche promotes the virtues of those he calls \"higher men\", people like Goethe and Beethoven. The virtues he praises in them are their creative powers (“the men of great creativity” - “the really great men according to my understanding” (WP 957)).\nAccording to Nietzsche these higher types are solitary, pursue a \"unifying project\", revere themselves and are healthy and life-affirming. Because mixing with the herd makes one base, the higher type “strives instinctively for a citadel and a secrecy where he is saved from the crowd, the many, the great majority…” (BGE 26). The 'Higher type' also \"instinctively seeks heavy responsibilities\" (WP 944) in the form of an \"organizing idea\" for their life, which drives them to artistic and creative work and gives them psychological health and strength. The fact that the higher types are \"healthy\" for Nietzsche does not refer to physical health as much as a psychological resilience and fortitude. Finally, a Higher type affirms life because he is willing to accept the eternal return of his life and affirm this forever and unconditionally.\n\nIn the last section of \"Beyond Good and Evil\", Nietzsche outlines his thoughts on the noble virtues and places solitude as one of the highest virtues:\n\nAnd to keep control over your four virtues: courage, insight, sympathy, solitude. Because solitude is a virtue for us, since it is a sublime inclination and impulse to cleanliness which shows that contact between people (“society”) inevitably makes things unclean. Somewhere, sometime, every\ncommunity makes people – “base.” (BGE §284)\n\nNietzsche also sees truthfulness as a virtue:\n\nGenuine honesty, assuming that this is our virtue and we cannot get rid of it, we free spirits – well then, we will want to work on it with all the love and malice at our disposal and not get tired of ‘perfecting’ ourselves in our virtue, the only one we have left: may its glory come to rest like a gilded, blue evening glow of mockery over this aging culture and its dull and dismal seriousness! (Beyond Good and Evil, §227)\n\nThese are the virtues that Benjamin Franklin used to develop what he called 'moral perfection'. He had a checklist in a notebook to measure each day how he lived up to his virtues.\n\nThey became known through Benjamin Franklin's autobiography.\n\n\nMarc Jackson in his book \"Emotion and Psyche\" puts forward a new development of the virtues. He identifies the virtues as what he calls the good emotions \"The first group consisting of love, kindness, joy, faith, awe and pity is good\" These virtues differ from older accounts of the virtues because they are not character traits expressed by action, but emotions that are to be felt and developed by feeling not acting.\nAyn Rand held that her morality, the , contained a single axiom: existence exists, and a single choice: to live. All values and virtues proceed from these. To live, man must hold three fundamental values that one develops and achieves in life: Reason, Purpose, and Self-Esteem. A value is \"that which one acts to gain and/or keep ... and the virtue[s] [are] the act[ions] by which one gains and/or keeps it.\" The primary virtue in Objectivist ethics is rationality, which as Rand meant it is \"the recognition and acceptance of reason as one's only source of knowledge, one's only judge of values and one's only guide to action.\" These values are achieved by passionate and consistent action and the virtues are the policies for achieving those fundamental values. Ayn Rand describes seven virtues: rationality, productiveness, pride, independence, integrity, honesty and justice. The first three represent the three primary virtues that correspond to the three fundamental values, whereas the final four are derived from the virtue of rationality. She claims that virtue is not an end in itself, that virtue is not its own reward nor sacrificial fodder for the reward of evil, that life is the reward of virtue and happiness is the goal and the reward of life. Man has a single basic choice: to think or not, and that is the gauge of his virtue. Moral perfection is an unbreached rationality, not the degree of your intelligence but the full and relentless use of your mind, not the extent of your knowledge but the acceptance of reason as an absolute.\n\nChristopher Peterson and Martin Seligman, two leading researchers in positive psychology, recognizing the deficiency inherent in psychology's tendency to focus on dysfunction rather than on what makes a healthy and stable personality, set out to develop a list of \"Character Strengths and Virtues\". After three years of study, 24 traits (classified into six broad areas of virtue) were identified, having \"a surprising amount of similarity across cultures and strongly indicat[ing] a historical and cross-cultural convergence.\" These six categories of virtue are courage, justice, humanity, temperance, transcendence, and wisdom. Some psychologists suggest that these virtues are adequately grouped into fewer categories; for example, the same 24 traits have been grouped into simply: Cognitive Strengths, Temperance Strengths, and Social Strengths.\n\nThe opposite of a virtue is a vice. Vice is a habitual, repeated practice of wrongdoing. One way of organizing the vices is as the corruption of the virtues.\n\nAs Aristotle noted, however, the virtues can have several opposites. Virtues can be considered the mean between two extremes, as the Latin maxim dictates \"in medio stat virtus\" - in the centre lies virtue. For instance, both cowardice and rashness are opposites of courage; contrary to prudence are both over-caution and insufficient caution; the opposites of pride (a virtue) are undue humility and excessive vanity. A more \"modern\" virtue, tolerance, can be considered the mean between the two extremes of narrow-mindedness on the one hand and over-acceptance on the other. Vices can therefore be identified as the opposites of virtues - but with the caveat that each virtue could have many different opposites, all distinct from each other.\n\n\n"}
