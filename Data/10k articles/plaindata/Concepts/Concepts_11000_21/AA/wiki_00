{"id": "93070", "url": "https://en.wikipedia.org/wiki?curid=93070", "title": "A New Kind of Science", "text": "A New Kind of Science\n\nA New Kind of Science is a best-selling, controversial book by Stephen Wolfram, published by his own company in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems \"simple programs\" and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.\n\nThe thesis of \"A New Kind of Science\" (\"NKS\") is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world. Since its nascent beginnings in the 1930s, computation has been primarily approached from two traditions: engineering, which seeks to build practical systems using computations; and mathematics, which seeks to prove theorems about computation. However, as recently as the 1970s, computing has been described as being at the crossroads of mathematical, engineering, and empirical traditions.\n\nWolfram introduces a third tradition that seeks to empirically investigate computation for its own sake: He argues that an entirely new method is needed to do so because traditional mathematics fails to meaningfully describe complex systems, and that there is an upper limit to complexity in all systems.\n\nThe basic subject of Wolfram's \"new kind of science\" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; primitive recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:\n\nGenerally, simple programs tend to have a very simple abstract framework. Simple cellular automata, Turing machines, and combinators are examples of such frameworks, while more complex cellular automata do not necessarily qualify as simple programs. It is also possible to invent new frameworks, particularly to capture the operation of natural systems. The remarkable feature of simple programs is that a significant percentage of them are capable of producing great complexity. Simply enumerating all possible variations of almost any class of programs quickly leads one to examples that do unexpected and interesting things. This leads to the question: if the program is so simple, where does the complexity come from? In a sense, there is not enough room in the program's definition to directly encode all the things the program can do. Therefore, simple programs can be seen as a minimal example of emergence. A logical deduction from this phenomenon is that if the details of the program's rules have little direct relationship to its behavior, then it is very difficult to directly engineer a simple program to perform a specific behavior. An alternative approach is to try to engineer a simple overall computational framework, and then do a brute-force search through all of the possible components for the best match.\n\nSimple programs are capable of a remarkable range of behavior. Some have been proven to be universal computers. Others exhibit properties familiar from traditional science, such as thermodynamic behavior, continuum behavior, conserved quantities, percolation, sensitive dependence on initial conditions, and others. They have been used as models of traffic, material fracture, crystal growth, biological growth, and various sociological, geological, and ecological phenomena. Another feature of simple programs is that, according to the book, making them more complicated seems to have little effect on their overall complexity. \"A New Kind of Science\" argues that this is evidence that simple programs are enough to capture the essence of almost any complex system.\n\nIn order to study simple rules and their often complex behaviour, Wolfram argues that it is necessary to systematically explore all of these computational systems and document what they do. He further argues that this study should become a new branch of science, like physics or chemistry. The basic goal of this field is to understand and characterize the computational universe using experimental methods.\n\nThe proposed new branch of scientific exploration admits many different forms of scientific production. For instance, qualitative classifications are often the results of initial forays into the computational jungle. On the other hand, explicit proofs that certain systems compute this or that function are also admissible. There are also some forms of production that are in some ways unique to this field of study. For example, the discovery of computational mechanisms that emerge in different systems but in bizarrely different forms.\n\nAnother kind of production involves the creation of programs for the analysis of computational systems. In the \"NKS\" framework, these themselves should be simple programs, and subject to the same goals and methodology. An extension of this idea is that the human mind is itself a computational system, and hence providing it with raw data in as effective a way as possible is crucial to research. Wolfram believes that programs and their analysis should be visualized as directly as possible, and exhaustively examined by the thousands or more. Since this new field concerns abstract rules, it can in principle address issues relevant to other fields of science. However, in general Wolfram's idea is that novel ideas and mechanisms can be discovered in the computational universe, where they can be represented in their simplest forms, and then other fields can choose among these discoveries for those they find relevant.\n\nWolfram has since expressed \"A central lesson of \"A New Kind of Science\" is that there’s a lot of incredible richness out there in the computational universe. And one reason that’s important is that it means that there’s a lot of incredible stuff out there for us to 'mine' and harness for our purposes.\"\n\nWhile Wolfram advocates simple programs as a scientific discipline, he also argues that its methodology will revolutionize other fields of science. The basis of his argument is that the study of simple programs is the minimal possible form of science, grounded equally in both abstraction and empirical experimentation. Every aspect of the methodology advocated in \"NKS\" is optimized to make experimentation as direct, easy, and meaningful as possible while maximizing the chances that the experiment will do something unexpected. Just as this methodology allows computational mechanisms to be studied in their simplest forms, Wolfram argues that the process of doing so engages with the mathematical basis of the physical world, and therefore has much to offer the sciences.\n\nWolfram argues that the computational realities of the universe make science hard for fundamental reasons. But he also argues that by understanding the importance of these realities, we can learn to use them in our favor. For instance, instead of reverse engineering our theories from observation, we can enumerate systems and then try to match them to the behaviors we observe. A major theme of \"NKS\" is investigating the structure of the possibility space. Wolfram argues that science is far too ad hoc, in part because the models used are too complicated and unnecessarily organized around the limited primitives of traditional mathematics. Wolfram advocates using models whose variations are enumerable and whose consequences are straightforward to compute and analyze.\n\nWolfram argues that one of his achievements is in providing a coherent system of ideas that justifies computation as an organizing principle of science. For instance, he argues that the concept of \"computational irreducibility\" (that some complex computations are not amenable to short-cuts and cannot be \"reduced\"), is ultimately the reason why computational models of nature must be considered in addition to traditional mathematical models. Likewise, his idea of intrinsic randomness generation—that natural systems can generate their own randomness, rather than using chaos theory or stochastic perturbations—implies that computational models do not need to include explicit randomness.\n\nBased on his experimental results, Wolfram developed the principle of computational equivalence (PCE): the principle states that systems found in the natural world can perform computations up to a maximal (\"universal\") level of computational power. Most systems can attain this level. Systems, in principle, compute the same things as a computer. Computation is therefore simply a question of translating input and outputs from one system to another. Consequently, most systems are computationally equivalent. Proposed examples of such systems are the workings of the human brain and the evolution of weather systems.\n\nThe principle can be restated as follows: almost all processes that are not obviously simple are of equivalent sophistication. From this principle, Wolfram draws an array of concrete deductions which he argues reinforce his theory. Possibly the most important among these is an explanation as to why we experience randomness and complexity: often, the systems we analyze are just as sophisticated as we are. Thus, complexity is not a special quality of systems, like for instance the concept of \"heat,\" but simply a label for all systems whose computations are sophisticated. Wolfram argues that understanding this makes possible the \"normal science\" of the \"NKS\" paradigm.\n\nAt the deepest level, Wolfram argues that—like many of the most important scientific ideas—the principle of computational equivalence allows science to be more general by pointing out new ways in which humans are not \"special\"; that is, it has been claimed that the complexity of human intelligence makes us special, but the Principle asserts otherwise. In a sense, many of Wolfram's ideas are based on understanding the scientific process—including the human mind—as operating within the same universe it studies, rather than being outside it.\n\nThere are a number of specific results and ideas in the \"NKS\" book, and they can be organized into several themes. One common theme of examples and applications is demonstrating how little complexity it takes to achieve interesting behavior, and how the proper methodology can discover this behavior.\n\nFirst, there are several cases where the \"NKS\" book introduces what was, during the book's composition, the simplest known system in some class that has a particular characteristic. Some examples include the first primitive recursive function that results in complexity, the smallest universal Turing Machine, and the shortest axiom for propositional calculus. In a similar vein, Wolfram also demonstrates many simple programs that exhibit phenomena like phase transitions, conserved quantities, continuum behavior, and thermodynamics that are familiar from traditional science. Simple computational models of natural systems like shell growth, fluid turbulence, and phyllotaxis are a final category of applications that fall in this theme.\n\nAnother common theme is taking facts about the computational universe as a whole and using them to reason about fields in a holistic way. For instance, Wolfram discusses how facts about the computational universe inform evolutionary theory, SETI, free will, computational complexity theory, and philosophical fields like ontology, epistemology, and even postmodernism.\n\nWolfram suggests that the theory of computational irreducibility may provide a resolution to the existence of free will in a nominally deterministic universe. He posits that the computational process in the brain of the being with free will is actually complex enough so that it cannot be captured in a simpler computation, due to the principle of computational irreducibility. Thus, while the process is indeed deterministic, there is no better way to determine the being's will than, in essence, to run the experiment and let the being exercise it.\n\nThe book also contains a vast number of individual results—both experimental and analytic—about what a particular automaton computes, or what its characteristics are, using some methods of analysis.\n\nThe book contains a new technical result in describing the Turing completeness of the Rule 110 cellular automaton. Very small Turing machines can simulate Rule 110, which Wolfram demonstrates using a 2-state 5-symbol universal Turing machine. Wolfram conjectures that a particular 2-state 3-symbol Turing machine is universal. In 2007, as part of commemorating the book's fifth anniversary, Wolfram's company offered a $25,000 prize for proof that this Turing machine is universal. Alex Smith, a computer science student from Birmingham, UK, won the prize later that year by proving Wolfram's conjecture.\n\nEvery year, Wolfram and his group of instructors organize a summer school. From 2003 to 2006, these classes were held at Brown University. In 2007, the summer school began being hosted by the University of Vermont at Burlington, with the exception of 2009 which was held at the Istituto di Scienza e Tecnologie dell'Informazione of the CNR in Pisa, Italy. In 2012, the program was held at Curry College in Milton, Massachusetts. Since 2013, the Wolfram Summer School has been held annually at Bentley University in Waltham, Massachusetts. After 14 consecutive summer schools, more than 550 people have participated, some of whom continued developing their 3-week research projects as their Master's or Ph.D theses. Some of the research done in the summer school has resulted in publications.\n\nPeriodicals gave \"A New Kind of Science\" coverage, including articles in \"The New York Times\", \"Newsweek\", \"Wired\", and \"The Economist\". Some scientists criticized the book as abrasive and arrogant, and perceived a fatal flaw—that simple systems such as cellular automata are not complex enough to describe the degree of complexity present in evolved systems, and observed that Wolfram ignored the research categorizing the complexity of systems. Although critics accept Wolfram's result showing universal computation, they view it as minor and dispute Wolfram's claim of a paradigm shift. Others found that the work contained valuable insights and refreshing ideas. Wolfram addressed his critics in a series of blog posts.\n\nIn an article published on April 3, 2018, \"A New Kind of Science\" was listed among the 190 books recommended by Bill Gates.\n\nA tenet of \"NKS\" is that the simpler the system, the more likely a version of it will recur in a wide variety of more complicated contexts. Therefore, \"NKS\" argues that systematically exploring the space of simple programs will lead to a base of reusable knowledge. However, many scientists believe that of all possible parameters, only some actually occur in the universe. For instance, of all possible permutations of the symbols making up an equation, most will be essentially meaningless. \"NKS\" has also been criticized for asserting that the behavior of simple systems is somehow representative of all systems.\n\nA common criticism of \"NKS\" is that it does not follow established scientific methodology. For instance, \"NKS\" does not establish rigorous mathematical definitions, nor does it attempt to prove theorems; and most formulas and equations are written in Mathematica rather than standard notation. Along these lines, \"NKS\" has also been criticized for being heavily visual, with much information conveyed by pictures that do not have formal meaning. It has also been criticized for not using modern research in the field of complexity, particularly the works that have studied complexity from a rigorous mathematical perspective. And it has been criticized for misrepresenting chaos theory: \"Throughout the book, he equates chaos theory with the phenomenon of sensitive dependence on initial conditions (SDIC).\"\n\n\"NKS\" has been criticized for not providing specific results that would be immediately applicable to ongoing scientific research. There has also been criticism, implicit and explicit, that the study of simple programs has little connection to the physical universe, and hence is of limited value. Steven Weinberg has pointed out that no real world system has been explained using Wolfram's methods in a satisfactory fashion.\n\nThe Principle of computational equivalence has been criticized for being vague, unmathematical, and for not making directly verifiable predictions. It has also been criticized for being contrary to the spirit of research in mathematical logic and computational complexity theory, which seek to make fine-grained distinctions between levels of computational sophistication, and for wrongly conflating different kinds of universality property. Moreover, critics such as Ray Kurzweil have argued that it ignores the distinction between hardware and software; while two computers may be equivalent in power, it does not follow that any two programs they might run are also equivalent. Others suggest it is little more than a rechristening of the Church–Turing thesis.\n\nWolfram's speculations of a direction towards a fundamental theory of physics have been criticized as vague and obsolete. Scott Aaronson, Professor of Computer Science at University of Texas Austin, also claims that Wolfram's methods cannot be compatible with both special relativity and Bell's theorem violations, and hence cannot explain the observed results of Bell test experiments. However, Aaronson's arguments are either right and apply to the entire scientific field of Quantum gravity that seeks to find theories unifying relativity and quantum mechanics or they are fundamentally flawed (e.g. under a non-local hidden variable theory of superdeterminism acknowledged by Bell himself), and even explored by e.g. physics Nobel laureate Gerard 't Hooft, see also replies to criticism of digital physics.\n\nEdward Fredkin and Konrad Zuse pioneered the idea of a computable universe, the former by writing a line in his book on how the world might be like a cellular automaton, and later further developed by Fredkin using a toy model called Salt. It has been claimed that \"NKS\" tries to take these ideas as its own but Wolfram's model of the universe is a rewriting network and not a cellular automaton as Wolfram himself has suggested a cellular automaton cannot account for relativistic features such as no absolute time frame. Jürgen Schmidhuber has also charged that his work on Turing machine-computable physics was stolen without attribution, namely his idea on enumerating possible Turing-computable universes.\n\nIn a 2002 review of \"NKS\", the Nobel laureate and elementary particle physicist Steven Weinberg wrote, \"Wolfram himself is a lapsed elementary particle physicist, and I suppose he can't resist trying to apply his experience with digital computer programs to the laws of nature. This has led him to the view (also considered in a 1981 paper by Richard Feynman) that nature is discrete rather than continuous. He suggests that space consists of a set of isolated points, like cells in a cellular automaton, and that even time flows in discrete steps. Following an idea of Edward Fredkin, he concludes that the universe itself would then be an automaton, like a giant computer. It's possible, but I can't see any motivation for these speculations, except that this is the sort of system that Wolfram and others have become used to in their work on computers. So might a carpenter, looking at the moon, suppose that it is made of wood.\"\n\nNobel laureate Gerard 't Hooft has more recently also suggested a cellular automaton-based unifying theory of quantum gravity as an interpretation of superstring theory where the evolution equations are classical, \"[b]oth the bosonic string theory and superstring theory can be reformulated in terms of a special basis of states, defined on a space-time lattice with lattice length formula_1\"\n\nWolfram's claim that natural selection is not the fundamental cause of complexity in biology has led non-scientist journalist Chris Lavers to state that Wolfram does not understand the theory of evolution.\n\n\"NKS\" has been heavily criticized as not being original or important enough to justify its title and claims.\n\nThe authoritative manner in which \"NKS\" presents a vast number of examples and arguments has been criticized as leading the reader to believe that each of these ideas was original to Wolfram; in particular, one of the most substantial new technical results presented in the book, that the rule 110 cellular automaton is Turing complete, was not proven by Wolfram, but by his research assistant, Matthew Cook. However, the notes section at the end of his book acknowledges many of the discoveries made by these other scientists citing their names together with historical facts, although not in the form of a traditional bibliography section. Additionally, the idea that very simple rules often generate great complexity is already an established idea in science, particularly in chaos theory and complex systems.\n\n\n"}
{"id": "3152236", "url": "https://en.wikipedia.org/wiki?curid=3152236", "title": "Anthropodermic bibliopegy", "text": "Anthropodermic bibliopegy\n\nAnthropodermic bibliopegy is the practice of binding books in human skin. As of April 2016, The Anthropodermic Book Project \"has identified 47 alleged anthropodermic books in the world's libraries and museums. Of those, 30 books have been tested or are in the process of being tested. Seventeen of the books have been confirmed as having human skin bindings and nine were proven to be not of human origin but of sheep, pig, cow, or other animals.\" (The confirmed figures as of August 2017 have increased to 18 bindings identified as human and 14 disproved.)\n\n\"Bibliopegy\" ( ) is a rare synonym for bookbinding. It combines the Ancient Greek βιβλίον (\"biblion\" = book) and πηγία (\"pegia\", from \"pegnynai\" = to fasten). The earliest reference in the Oxford English Dictionary dates from 1876; Merriam-Webster gives the date of first use as \"circa\" 1859 and the OED records an instance of \"bibliopegist\" for a bookbinder from 1824.\n\nThe word \"anthropodermic\" ( ), combining the Ancient Greek ἄνθρωπος (\"anthropos\" = man or human) and δέρμα (\"derma\" = skin), does not appear in the Oxford English Dictionary and appears never to be used in contexts other than bookbinding. The phrase 'anthropodermic bibliopegy' has been used at least since Lawrence S. Thompson's article on the subject, published in 1946. The practice of binding a book in the skin of its author - as with \"The Highwayman\", discussed below - has been called 'autoanthropodermic bibliopegy' (from αὐτός \"autos\", self).\n\nOne of the earliest examples of a binding claimed to be in human skin is the account published in 1606 of the execution of Henry Garnet for his involvement in the Gunpowder Plot, \"A True and Perfect Relation of the Whole Proceedings Against ... Garnet a Jesuit\". A copy auctioned in 2007 was claimed by the auctioneer to be bound in Garnet's own skin.\n\nAn early reference to a book bound in human skin is found in the travels of Zacharias Conrad von Uffenbach. Writing about his visit to Bremen in 1710: \nDuring the French Revolution, there were rumours that a tannery for human skin had been established at Meudon outside Paris. The Carnavalet Museum owns a volume containing the French Constitution of 1793 and \"Declaration of the Rights of Man\" described as 'passing for being made in human skin imitating calf'.\n\nThe majority of well-attested anthropodermic bindings date from the 19th century.\n\nSurviving examples of human skin bindings have often been commissioned, performed, or collected by medical doctors, who have access to cadavers, sometimes those of executed criminals, such as the case of John Horwood in 1821 and the Red Barn Murder in 1828. Another tradition, with less supporting evidence, is that books of erotica or the occult have been bound in human skin.\n\nThe Royal College of Surgeons of Edinburgh preserves a notebook bound in the skin of the murderer William Burke after his execution and subsequent public dissection by Professor Alexander Monro in 1829.\n\nWhat Lawrence Thompson called \"the most famous of all anthropodermic bindings\" is exhibited at the Boston Athenaeum, titled \"The Highwayman: Narrative of the Life of James Allen alias George Walton\". It is by James Allen, who made his deathbed confession in 1837 and asked for a copy bound in his own skin to be presented to a man he once tried to rob and admired for his bravery.\n\nThe Newberry Library in Chicago owns an Arabic manuscript written in 1848, with a handwritten note that it is bound in human skin, though \"it is the opinion of the conservation staff that the binding material is not human skin, but rather highly burnished goat\". This book is mentioned in the novel \"The Time Traveler's Wife\", much of which is set in the Newberry.\n\nThe French astronomer Camille Flammarion's book \"Les terres du ciel\" (The Worlds of the Sky) (1877) was bound with the skin donated from a female admirer.\n\nA portion of the binding in the copy of Dale Carnegie's \"Lincoln the Unknown\" that is part of the collection of Temple University's Charles L. Blockson Collection was \"taken from the skin of a Negro at a Baltimore Hospital and tanned by the Jewell Belting Company\".\n\nThe National Library of Australia holds a book of 18th century poetry with the inscription \"Bound in human skin\" on the first page.\n\nAn exhibition of fine bindings at the Grolier Club in 1903 included, in a section of 'Bindings in Curious Materials', three editions of Holbein's '\"Dance of Death\"' in 19th century human skin bindings; two of these now belong to the John Hay Library at Brown University. Other examples of the Dance of Death include an 1856 edition offered at auction by Leonard Smithers in 1895 and an 1842 edition from the personal library of Florin Abelès was offered at auction by Piasa of Paris in 2006. Bookbinder Edward Hertzberg describes the Monastery Hill Bindery having been approached by \"[a]n Army Surgeon ... with a copy of Holbein's \"Dance of Death\" with the request that we bind it in a piece of human skin, which he brought along.\"\n\nThe identification of human skin bindings has been attempted by examining the pattern of hair follicles, to distinguish human skin from that of other animals typically used for bookbinding, such as calf, sheep, goat, and pig. This is a necessarily subjective test, made harder by the distortions in the process of treating leather for binding. Testing a DNA sample is possible in principle, but DNA can be destroyed when skin is tanned, it degrades over time, and it can be contaminated by human readers.\n\nInstead, peptide mass fingerprinting (PMF) and matrix-assisted laser desorption/ionization (MALDI) have recently been used to identify the material of bookbindings. A tiny sample is extracted from the book's covering and the collagen analysed by mass spectrometry to identify the variety of proteins which are characteristic of different species. PMF can identify skin as belonging to a primate; since monkeys were almost never used as a source of skin for bindings, this implies human skin.\n\nThe Historical Medical Library of the College of Physicians of Philadelphia owns five anthropodermic books, confirmed by peptide mass fingerprinting in 2015, of which three were bound from the skin of one woman. This makes it the largest collection of such books in one institution. The books can be seen in the associated Mütter Museum.\n\nThe John Hay Library at Brown University owns four anthropodermic books, also confirmed by PMF: Vesalius's De Humani Corporis Fabrica, two nineteenth-century editions of Holbein's \"Dance of Death\", and \"Mademoiselle Giraud, My Wife\" (1891).\n\nThree books in the libraries of Harvard University have been reputed to be bound in human skin, but peptide mass fingerprinting has confirmed only one, \"Des destinées de l'ame\" by Arsène Houssaye, held in the Houghton Library. (The other two books at Harvard were determined to be bound in sheepskin, the first being Ovid's \"Metamorphoses\" held in the Countway Library, the second being a treatise on Spanish law, \"Practicarum quaestionum circa leges regias Hispaniae\", held in the library of Harvard Law School.)\n\nThe Harvard skin book belonged to Dr Ludovic Bouland of Strasbourg, who owned a second, \"De integritatis & corruptionis virginum notis\", now in the Wellcome Library in London. The Wellcome also owns a notebook labelled as bound in the skin of 'the Negro whose Execution caused the War of Independence', presumably Crispus Attucks, but the library doubts that it is actually human skin.\n\nPeptide mass fingerprinting was also used to determine the binding material for a miniature devotional book in the University of California's Bancroft Library, \"L'office de l'Eglise en françois\". It is now known not to be bound in human skin but horse hide, or a mixture of horse and goatskin.\n\n\nThe binding of books in human skin is also a common element within horror films and works of fiction.\n\nFiction\n\nTelevision and cinema\n\nVideo games\n\n"}
{"id": "4247621", "url": "https://en.wikipedia.org/wiki?curid=4247621", "title": "Arborescent", "text": "Arborescent\n\nArborescent () is a term used by the French thinkers Deleuze and Guattari to characterize thinking marked by insistence on totalizing principles, binarism, and dualism. The term, first used in \"A Thousand Plateaus\" (1980) where it was opposed to the rhizome, comes from the way genealogy trees are drawn: unidirectional progress, with no possible retroactivity and continuous binary cuts (thus enforcing a dualist metaphysical conception, criticized by Deleuze). Rhizomes, on the contrary, mark a horizontal and non-hierarchical conception, where anything may be linked to anything else, with no respect whatsoever for specific species: rhizomes are heterogeneous links between things that have nothing to do between themselves (for example, Deleuze and Guattari linked together desire and machines to create the - most surprising - concept of desiring machines). Horizontal gene transfer is also an example of rhizomes, opposed to the arborescent evolutionism theory. Deleuze also criticizes the Chomsky hierarchy of formal languages, which he considers a perfect example of arborescent dualistic theory.\n"}
{"id": "58535876", "url": "https://en.wikipedia.org/wiki?curid=58535876", "title": "Ateyyat El Abnoudy", "text": "Ateyyat El Abnoudy\n\nAteyyat El Abnoudy (born October 26, 1939 - October 5, 2018) also known as Ateyyat Awad Mahmoud Khal ,was an Egyptian journalist, lawyer, actress, producer, and movie director. She was born in a small village along the Nile Delta in Egypt. El-Abnoudy is considered to be one of the pioneer Arab female movie directors as her films have inspired the works of many Arab women in the industry. She has been nicknamed the \"poor people's filmmaker\" due to the subject matter that inspired her to make films: civil rights issues and the unfortunate living circumstances of impoverished Arabs. \n\nAteyyat El Abnoudy has been the recipient of many awards for her films. Her film, \"Horse of Mud (1971)\" won her three international prizes. In 1972, Abnoudy also won the French Critics Prize at Grenoble Film Festival. El-Abnoudy was one of the earliest Arab women filmmakers to have been working in Egypt, and led the way for many other film-making Arab women to pursue a career in the industry. \n\nAteyyat El Abnoudy began her early life being raised in a small village by her two parents in a working class family. El-Abnoudy attended the University of Cairo where she began working at a theatre and as an actress to put herself through school and to attain her law degree. This is where she met her first husband, a journalist and poet named Abdel-Rahman El Abnoudy. Because of Abdel's career, Atteyyat was in the arts circle which meant networking with many writers, poets, and the artistic types of Egypt. She gives thanks to one of the people who helped her in her journey Nasser, his coming into power allowed for the filmmaker to attend post-secondary and take further steps to beginning her career.\n\nWhile deciding on where she fits in the film world, Ateyyat worked in many different roles in the theater, such as stage manager and assistant. When she found her passion in film-making she decided to continue her education. In 1972 she attended Cairo’s Higher Film Institute to finish her film studies. In the two years she spent there, she created her first two films called \"Horse of Mud\", which she received awards for. At the time that film was created to filmmaker was still new to the industry.\n\nWhile Abnoudy was nearing her graduation date she was also in the process of creating her first documentary \"Horse of Mud\" . Abnoudy stated that she wanted to have what she referred to as a style of poetic realism. The ways in which she captured the images of her actresses was a way she states would show that the best. According to Van de Peer, the \"Horse of Mud\" wasn’t only Abnoudy’s first documentary, but it was also Egypt’s first documentary by a woman.\n\nAbnoudy worked on creating original work. Work that circulates are the lives that people in third world countries are living in as Van de peer states in his piece. She likes to focus on the things in our everyday lives that influence us. The impacts that society has on our lives. In terms of a poetic approach, a few of her movies are short films. Like the saying: a little says a lot, that is the way in which Abnoudy’s films speak to her viewers. She wants her audience to understand the message she is using her films to relay. Every time her films are watched she wants the viewer to learn her message and relate to the film in a new way each time\n\nAt the time that Abnoudy was in the process of sculpting her craft, according to Van de Peer in his articles, Egypt was undermining the freedom of expression, especially of women. Van de Peer states that the feminist documentary writer expresses her poetic realism, along with that Abnoudy uses her films \"Permissible Dreams\" (1982) and \"Responsible Women\" (1994) to make her viewers feel empathy and appropriation.\n\nThe interest that she had in law, her work as an actress, and the circle of people she surrounded herself with inspired her to become a filmmaker.\n\nAteyyat El Abnoudy started her acting career as a means to financially support herself in school while she studied journalism. When El-Abnoudy's career as a journalist began, she took a particular interest in the poor people of Egypt, specifically Cairo. This later inspired her to take up production and become a filmmaker who shined a light on some of the unfortunate life circumstances of those in Egypt. El-Abnoudy quickly became known by two titles: the \"poor people's filmmaker\" and the \"mother of documentaries\" as she inspired many Arab women filmmakers to follow in her footsteps. \n\nEl-Abnoudy started making films since 1971, most of which are known for dealing with political, social, and economic issues in Egypt. During the time she began making films Egypt had just overcome a long and complex colonial history with Britain. The film industry in Egypt was known for being very cultural and state endorsed, specifically to promote Arab filmmakers and censor what was being published in the country. Ateyyat El Abnoudy saw this as a challenge and, as one of the first female filmmakers, she directed her first documentary called \"Horse of Mud\" in \"1971.\" El-Abnoudy further contested the censorship of Egypt filmmakers when she became the first female to establish her own production company, Abnoud Film, supporting small filmmakers similar to her. \n\nHer first film, \"Horse of Mud (1971),\" is considered to capture the dignity of Cairo's poorest peoples. This film follows the process of brick-making, a repetitive, painful, yet necessary task. \"Horse of Mud,\" illuminates the struggles of surviving in Cairo as an impoverished person by intertwining brick-makers personal stories to the work they do to survive. \n\nAbnoudy was a female documentary writer who based her films on a connection, she wanted her audience to bond with the actors according to Van de Peer the Films \"Horse of Mud\" and \"Sad Song of Touha\" and \"The Sandwich\" were the winners of prizes from the foreign film festivals.\n\n\nThe Physicalities of Documentaries by African Women written by Stefanie Van De Peer\n\nPermissible Documentaries: Representation in Ateyyat El Abnoudy's documentaries written by Stefanie Van De Peer\n\nPopular Egyptian Cinema: Gender, Class, and Nation written by Viola Shafik\n\n on IMDb\n\n1993 Ateyyat El-Abnoudy interview with Kevin Thomas\n\nAteyyat El Abnoudy Interview with Rebecca Hillauer\n"}
{"id": "4795825", "url": "https://en.wikipedia.org/wiki?curid=4795825", "title": "Basic points unifying Theravāda and Mahāyāna", "text": "Basic points unifying Theravāda and Mahāyāna\n\nThe Basic Points Unifying the Theravāda and the Mahāyāna is an important Buddhist ecumenical statement created in 1967 during the First Congress of the World Buddhist Sangha Council (WBSC), where its founder Secretary-General, the late Venerable Pandita Pimbure Sorata Thera, requested the Ven. Walpola Rahula to present a concise formula for the unification of all the different Buddhist traditions. This text was then unanimously approved by the Council.\n\nVen. Walpola Sri Rahula in 1981 offered an alternative to the Nine-point formula above restating it as follows:\n\n\n\n"}
{"id": "7506663", "url": "https://en.wikipedia.org/wiki?curid=7506663", "title": "Behavioral contrast", "text": "Behavioral contrast\n\nBehavioral contrast refers to a change in the strength of one response that occurs when the rate of reward of a second response, or of the first response under different conditions, is changed. For example, suppose that a pigeon in an operant chamber pecks a key for food reward. Sometimes the key is red, sometimes green, but food comes with equal frequency in either case. Then suddenly pecking the key when it is green brings food less frequently. Positive contrast is seen when the rate of response to the red key goes up, even though the frequency of reward in red remains unchanged. Likewise, increasing the reward to green tends to reduce the response rate to red (negative contrast). This sort of contrast effect may occur following changes in the amount, frequency, or nature of the reward, and it has been shown to occur with various experimental designs and response measures (e.g. response rate, running speed).\n\nIn 1942, Crespi measured the speed of rats running to various amounts of reward at the end of an alley. He found that the greater the magnitude of reward, the faster the rat would run to get the reward. In the middle of his experiment Crespi shifted some of his animals from a large reward to a small reward. These animals now ran even more slowly than control animals that had been trained on small reward throughout the experiment. This overshoot is an example of successive negative contrast. Likewise, other animals shifted from small to large reward ran faster than those trained on the larger reward throughout (successive positive contrast). Crespi originally called these effects \"depression\" and \"elation\" respectively, but, in 1949, Zeaman suggested changing the names to \"negative contrast\" and \"positive contrast\". The combined concept of behavioral contrast is sometimes also referred to as the Crespi Effect. \n\nIn 1981, Bower discovered that positive contrast may be reduced because the response measure hits a ceiling. Thus, if contrast is the subject of an experiment, reward sizes may need to be adjusted to keep the response below such a ceiling. In 1996, Flaherty suggested that negative contrast was related to frustration; that is, the sudden shift to a low reward causes frustration for the person or the animal, and this frustration interferes with the behavior the subject is performing.\n"}
{"id": "49965092", "url": "https://en.wikipedia.org/wiki?curid=49965092", "title": "Camera eats first", "text": "Camera eats first\n\n‘Camera Eats First’ is the behavior and global phenomenon of people taking photos of their meals with digital or smartphone cameras before they eat, mostly followed by uploading the photos to the social media. The term refers to how people feed their cameras first by taking photos of their food before feeding themselves. It derives from professional food photography while the behavior of the ‘Camera Eats First’ is generally for personal use such as keeping photographic food diaries instead of commercial purposes. It can also be referred as online food photography, food porn and photogenic food.\n\nOne reason for the rise of ‘Camera Eats First’ is the rise of digital convenience. The global participation in social media is on continual surge with the advancement in technology and commonness of digital devices that serve as mediums for social media. The veteran food photographer of The New York Times, The Wall Street Journal and Saveur magazine Dave Hagerman mentioned that Instagram, contains the most examples of ‘Camera Eats First’ photos. This platform provides a way for people to share and indulge in their common obsession of food globally, thus encouraging taking pictures of food for personal use. More importantly, keeping a photogenic food diary is being treated as a form of self representation, showing who they are from what they eat in accordance to the quote of “Tell me what you eat, and I will tell you what you are,” by the French philosopher and gourmand Jean Anthelme Brillat-Savarin, 1825.\n\nThe 'Camera Eats First' phenomenon is becoming more common all over the world with the emergence of smartphone and social media. After taking photos of their food, people will usually share the photos on social media such as Instagram, Facebook, Twitter and Pinterest. According to Webstagram, there are more than 180 million photos with the hashtag #food currently on Instagram. Other hashtags such as #foodporn and #foodie are often added to these photos. It is estimated that 90 new photos hash-tagged #foodporn are uploaded to Instagram every minute. The phenomenon is especially more prevalent among the younger generation. According to a survey done by News Limited, “54 per cent of 18-24 year olds have taken a photo of their food while eating out, while 39 per cent have posted it somewhere online. This compares with only 5 per cent of over-50s who say they share food snaps on forums such as Facebook and Twitter.”\n\nThrough sharing food photos on social media, users can form connections with other people and strengthen interpersonal bonds. Food is always a community event and a bonding experience which can bridge the gap between people and share joy. Thus, sharing food photos also acts as a \"social glue\" to gather people together to participate in each other's eating experiences. Through the sharing, a private dining experience is turned into a communal bonding activity. People can communicate and share their emotions with others, for example, happiness in a party. In addition, people can satisfy the psychological needs of “belongingness and love” and “esteem” according to the Maslow's hierarchy of needs as they can share their experiences and show off to the world what they are eating.\n\nSharing food photos can facilitate people’s obsession with food, but it can also help promote restaurants. Sharing of food photo can be a visual pull that will subconsciously alert people to check out new restaurants. Carmel Winery, a restaurant in Israel, gained $400,000 from free promotion by food photos on social media and successfully boosted its sales by 13% owing to the help of “Camera Eats First”. On the other hand, some chefs in Hong Kong would take \"food porn\" photos and post them on social media because they thought that the \"Camera Eats First\" phenomenon served as a channel for them to collect feedback from customers which is a vital information for them to make improvements to their dishes.\n\nAccording to a study mentioned on Bit of News, taking photos of pleasurable food, such as cakes, before eating it can increase the savoring — \"the increased anticipation built up from taking photos of the food made it taste better.\" If people don't take a photo, they may even feel something is missing from the savoring experience.\n\nThe 'Camera Eats First' behavior was banned in some French restaurants, as diners may infringe upon chefs' intellectual property. Some French chefs criticized that this behavior would ruin the surprise of future diners, as they might have seen the photos of dishes shared on the internet before coming to the restaurant. Diners posting the photos on social media may give rise to intellectual property disputes about other chefs copying the ideas and presentation of the plates, which are the creative elements contributed by the original chefs.\n\nThe \"Camera Eats First\" behavior may be considered as an impolite table manner. When diners photograph the food, they may place too much emphasis on the appearance of the food and try to capture the food at its best angle by rearranging tables. It may disrupt other people dining and spoil the enjoyment of their meal. They may also leave their partners in a state of hunger and impatience. \n\nOn the other hand, while people are busy photographing their food and sharing it online, they will have less time to communicate with their friends and family. One of the functions of eating—enabling families and friends to gather together and enhance intimacy—is lost. The 'Camera Eats First\" behavior may result in worsening relationships and weakening connection with others in contrast with the positive effect mentioned.\n"}
{"id": "422572", "url": "https://en.wikipedia.org/wiki?curid=422572", "title": "Chemical castration", "text": "Chemical castration\n\nChemical castration is castration via anaphrodisiac drugs, whether to reduce libido and sexual activity, to treat cancer, or otherwise. Unlike surgical castration, where the gonads are removed through an incision in the body, chemical castration does not remove organs, nor is it a form of sterilization. Chemical castration is generally considered reversible when treatment is discontinued, although permanent effects in body chemistry can sometimes be seen, as in the case of bone density loss increasing with length of use of DMPA.\n\nIn May 2016, \"The New York Times\" reported that a number of countries use chemical castration on rapists and pedophiles, often in return for reduced sentences.\n\nWhen used on men, these drugs can reduce sex drive, compulsive sexual fantasies, and capacity for sexual arousal. Life-threatening side effects are rare, but some users show increases in body fat and reduced bone density, which increase long-term risk of cardiovascular disease and osteoporosis. They may also experience gynecomastia (development of larger-than-normal mammary glands in males) as seen in Alan Turing's case.\n\nWhen used on women, the effects are similar, though there is little research about chemically lowering women's sex drive or female-specific anaphrodisiacs, since most research focuses on the opposite, but anti-androgenic hormone regimens would lower testosterone in women which can impact sex drive or sexual response. These drugs also deflate the breast glands and expand the size of the nipple. Also seen is a sudden shrink in bone mass and discoloration of the lips, reduced body hair, and muscle mass.\n\nThe first use of chemical castration occurred in 1944, when diethylstilbestrol was used with the purpose of lowering men's testosterone.\nThe antipsychotic agent benperidol is sometimes used to diminish sexual urges in people who display inappropriate sexual behavior, and can likewise be given by depot injection. But benperidol does not affect testosterone and is therefore not a castration agent.\nChemical castration is often seen as an easier alternative to life imprisonment or the death penalty because it allows the release of sex offenders while reducing or eliminating the chance that they reoffend.\n\nIn 1981, in an experiment by P. Gagne, 48 males with long-standing histories of sexually deviant behaviour were given medroxyprogesterone acetate for as long as 12 months. Forty of those subjects were recorded as to have diminished desires for deviant sexual behaviour, less frequent sexual fantasies, and greater control over sexual urges. The research recorded a continuation of this more positive behaviour after the administration of the drug had ended with no evidence of adverse side effects and recommended medroxyprogesterone acetate along with counselling as a successful method of treatment for serial sex offenders.\n\nCiting the observation that spaying causes female animals to stop mating in the same species as castration cause male animals to stop mating, and that in the animal species where females continue their mating behaviour after being spayed the males also continue to mate after being castrated, there are scientists who argue that it makes no biological sense to assume that any treatment that emulates castration would remove sex drive in men but not in women. These scientists argue that these observations, along with the fact that humans are animals and subject to evolution, show that it is flawed to think that male sexuality would be treatable by medication if female sexuality is not. \n\nSome criminologists argue that the appearance of a lower recidivism rate in male sex offenders who take chemical castration treatment than in those who do not can be explained by factors other than biological effects of the medication. One hypothesis is that men who accept the negative effects of hormonal treatment in exchange for shorter prison sentence are distinct in that they value freedom from incarceration higher than men who rather stay in prison for a longer time than face the side effects of chemical castration. These criminologists explain apparently lower recidivism as an artifact of men who accept chemical castration being more engaged in hiding the evidence for reoffending, and that paroling such offenders constitute a risk of releasing criminals who commit as many new crimes as others but are better at hiding it. These criminologists also argue that police investigators treating castrated men as less likely to reoffend than non-castrated men may cause an investigation bias and self-fulfilling prophecy, and that men who sell some of their prescribed medicines on the black market for drugs get a hidden income that improve their ability to afford measures to hide recidivism that is not available to men without such prescriptions.\n\nSome neurologists acknowledge that testosterone plays a role in sexual arousal but that reducing sex drive will likely not reduce inappropriate sex behaviour. These researchers argue that since a weaker internal signal in the brain means a higher requirement for external stimulation to create a feedback loop that tires the brain circuits out as in orgasm and lead to satisfaction, a reduction of the internal stimulation from hormones would make the required external stimulation stronger and also more specific, as weaker signals involve narrower ranges of other brain functions in their loops. These scientists therefore argue that the biological (as opposed to sociological) effect of reduced testosterone is to make it more difficult and not easier to use masturbation without pornography or other socially acceptable substitutes to manage remaining sex drive in a former offender, and that many community persons (both male and female) find that a lower initial arousal makes it more difficult to orgasm by masturbation without pornography or with non-preferred stimulation.\n\nIn March 2010, Guillermo Fontana of CNN reported that officials in Mendoza, a province in Argentina, approved the use of voluntary chemical castration for rapists, in return for reduced sentences.\n\nIn 2010, a repeat child sex offender who had been subject to chemical castration was accused of inappropriately touching and kissing a young girl. He was found not guilty by a jury, which was not informed of the context of his previous offenses.\n\nThe drug cyproterone acetate has been commonly used for chemical castration throughout Europe. It resembles the drug MPA used in America.\n\nIn the United Kingdom, computer scientist Alan Turing, famous for his contributions to mathematics and computer science, was a homosexual who was forced to undergo chemical castration in order to avoid imprisonment in 1952. At the time, homosexual acts between males were still illegal and homosexual orientation was widely considered to be a mental illness that could be treated with chemical castration. Turing experienced side effects such as breast enlargement and bloating of the physique. He died two years later, with the inquest returning a verdict of suicide, although recent research has cast doubt on this result. In 2009, the then British Prime Minister Gordon Brown issued a public apology for the \"appalling\" treatment of Turing after an online petition gained 30,000 signatures and international recognition. He was given a posthumous Royal Pardon in December 2013.\n\nIn the 1960s, German physicians used antiandrogens as a treatment for sexual paraphilia.\n\nIn 2008, an experimental intervention program was launched in three Portuguese prisons: Carregueira (Belas, Sintra), Paços de Ferreira and Funchal. The program developers note the voluntary nature of the program a crucial factor in its success. They initially planned to cover ten inmates per prison, contemplating a possible enlargement to other prisons in the future. The program also included a rehabilitation component.\n\nOn September 25, 2009, Poland legislated forcible chemical castration of child molesters. This law came into effect on June 9, 2010; therefore in Poland \"anyone guilty of raping a child under the age of 15 can now be forced to submit to chemical and psychological therapy to reduce sex drive at the end of a prison term\".\n\nOn April 30, 2010, a man in the United Kingdom found guilty of attempting to murder a 60-year-old woman in order to abduct and rape her two granddaughters agreed to undergo chemical castration as part of the terms of his sentence.\n\nOn March 6, 2012, Moldova legislated forcible chemical castration of child molesters; the law came into effect on July 1, 2012.\n\nOn June 5, 2012, Estonia passed a law to allow forced chemical castration of sex offenders.\n\nIn October and November 2013, the Macedonian authorities were working on developing a legal framework and standard procedure for implementation of chemical castration that would be used for convicted child molesters. The castration is intended to be voluntarily, where as for the child molesters that repeat the criminal act it should be mandatory.\n\nAfter the outrage following the gang rape of a woman in Delhi, the Government has submitted a draft proposing chemical castration along with an imprisonment of up to 30 years for rape convicts as part of the anti-rape law in India. The ministry is preparing a detailed bill and the recommended changes are under review.\nGovernment is also planning to re-define the Juvenile Act and lower their age. One of the accused in the rape case is a juvenile and aged a few months less than 18 years. A view has been expressed by a section that only those below 15 years should be described as juvenile.\n\nIn 2016, the Indonesian President Joko Widodo introduced a presidential regulation to allow chemical castration to be handed down as a punishment to child sex offenders and pedophiles. The regulation alters the contents of the 2002 Law on Child Protection.\n\nIn May 2009, two brothers from Haifa—convicted child molesters—agreed to undergo chemical castration to avoid committing further crimes.\n\nIn New Zealand, the antilibidinal drug cyproterone acetate is sold under the name Androcur. In November 2000 convicted paedophile Robert Jason Dittmer attacked a victim while on the drug. In 2009 a study into the effectiveness of the drug by Dr David Wales for the Corrections Department found that no research had been conducted in New Zealand into the effectiveness and such trials were \"ethically and practically very difficult to carry out.\"\n\nIn October 2011, the Russian parliament approved a law that allows a court-requested forensic psychiatrist to prescribe the chemical castration of convicted sex offenders who have harmed children under the age of 14.\n\nIn July 2011, South Korea enacted a law allowing judges the power to sentence sex offenders who have attacked children under the age of 16 to chemical castration. The law also allows for chemical castration to be ordered by a Ministry of Justice committee. On May 23, 2012, a serial sex offender legally called Park in the court case was ordered by the committee to undergo this treatment after his most recent attempted offense. On January 3, 2013, a South Korean court sentenced a 31-year-old man to 15 years in jail and chemical castration, the country's first-ever chemical castration sentence. In 2017, feminist-backed Moon Jaein administration expanded the sentencing to include all forms of rapes and sexual assault cases against women, including attempted rape.\n\nIn South Korea, taking pictures of women without their consent, even in public, is considered to be criminal sexual assault, punishable by a fine of under 10 million won and up to 5 years' imprisonment. In July 2017 an amendment to the law was voted on in favour of allowing for chemical castration of people taking such photographs. The amendment did not pass.\n\nIn 1966, John Money became the first American to employ chemical castration by prescribing medroxyprogesterone acetate (MPA, the base ingredient now used in DMPA) as a treatment for a patient dealing with pedophilic urges. The drug has thereafter become a mainstay of chemical castration in America. Despite its long history and established use, the drug has never been approved by the FDA for use as a treatment for sexual offenders.\n\nCalifornia was the first U.S. state to specify the use of chemical castration as a punishment for child molestation, following the passage of a modification to Section 645 of the California penal code in 1996. This law stipulates that anyone convicted of child molestation with a minor under 13 years of age may be treated with DMPA if they are on parole after their second offense and that offenders may not reject the treatment.\n\nThe passage of this law led to similar laws in other states such as Florida's Statute Section 794.0235 which was passed into law in 1997. As in California, treatment is mandatory after a second offense.\n\nAt least seven other states, including Georgia, Iowa, Louisiana, Montana, Oregon, Texas and Wisconsin, have experimented with chemical castration. In Iowa, as in California and Florida, offenders may be sentenced to chemical castration in all cases involving serious sex offenses. On June 25, 2008, following the Supreme Court ruling in \"Kennedy v. Louisiana\" that the execution of child rapists where the victim was not killed was ruled unconstitutional, Louisiana Governor Bobby Jindal signed Senate Bill 144, allowing Louisiana judges to sentence convicted rapists to chemical castration.\n\nThe American Civil Liberties Union of Florida opposes the administration of any drug that is dangerous or has significant irreversible effect as an alternative to incarceration; however, they do not oppose the use of antiandrogen drugs for sex offenders under carefully controlled circumstances as an alternative to incarceration. Law professor John Stinneford has argued that chemical castration is a cruel and unusual punishment because it exerts control over the mind of sex offenders to render them incapable of sexual desire and subjects them to the physical changes caused by the female hormones used.\n\nSome people have argued that, based on the 14th Amendment, the procedure fails to guarantee equal protection: although the laws mandating the treatment do so without respect to gender, the actual effect of the procedure disproportionately falls upon men. In the case of voluntary statutes, the ability to give informed consent is also an issue; in 1984, the U.S. state of Michigan's court of appeals held that mandating chemical castration as a condition of probation was unlawful on the grounds that the drug medroxyprogesterone acetate had not yet gained acceptance as being safe and reliable and also due to the difficulty of obtaining informed consent under these circumstances.\n\nA major medical use of chemical castration is in the treatment of hormone-dependent cancers, such as some prostate cancer, where it has largely replaced the practice of surgical castration.\n\nChemical castration involves the administration of antiandrogen drugs, such as cyproterone acetate, flutamide, or gonadotropin-releasing hormone agonists.\n\n\n"}
{"id": "10625535", "url": "https://en.wikipedia.org/wiki?curid=10625535", "title": "Cross-boundary subsidy", "text": "Cross-boundary subsidy\n\nCross-boundary subsidies are caused by organisms or materials that cross or traverse habitat patch boundaries, subsidizing the resident populations. The transferred organisms and materials may provide additional predators, prey, or nutrients to resident species, which can affect community and food web structure. Cross-boundary subsidies of materials and organisms occur in landscapes composed of different habitat patch types, and so depend on characteristics of those patches and on the boundaries in between them. Human alteration of the landscape, primarily through fragmentation, has the potential to alter important cross-boundary subsidies to increasingly isolated habitat patches. Understanding how processes that occur outside of habitat patches can affect populations within them may be important to habitat management.\n\nThe concept of cross-boundary subsidies developed out of a merging of ideas from the studies of landscape ecology and food web ecology. The ideas from landscape ecology allow the study of population, community, and food web dynamics to incorporate spatial relationships between landscape elements into an understanding of such dynamics (Polis et al. 1997).\n\nJanzen (1986) first defined cross-boundary subsidies as a process whereby organisms that disperse from one patch into another impact resident organisms by providing increased food resources or opportunities for reproduction, thus serving as a subsidy to the residents. By this definition, only the cross-boundary movement of organisms is considered, but broader definitions of cross-boundary subsidies can also include materials such as nutrients and detritus (i.e. Marburg et al. 2006, Facelli and Pickett 1991).\n\nCross-boundary subsidies are a subset of the more general process of spatial subsidies (see Polis et al. 1997). Cross-boundary subsidies acknowledge the presence and role of the boundary between different habitat patches in mediating flows of organisms and materials. In contrast, spatial subsidies require only that external inputs of materials and organisms originate from outside the patch of interest.\n\nFew attempts have been made to combine landscape and food web ecology in such a way that explicitly recognizes the importance of cross-boundary subsidies and spatial features of the landscape on food web dynamics. Often, spatial subsidies are treated as subsidies that simply arrive from outside the patch of interest, not addressing the landscape patterns and processes that may affect the movement of these inputs, such as boundary characteristics and patch connectivity. Polis et al. (1997) published a thorough review of spatially subsidized food web dynamics, focusing on the effect of subsidies on population, community, consumer-resource, and food web dynamics. One of the main conclusions was that subsidies of consumer species (organisms that eat other organisms to obtain energy) resulted in declines of food resources in the recipient patch. Callaway and Hastings (2002) built off Polis et al.’s conclusion with a model to show that subsidized consumers may not always drive down the resource in the recipient patch if consumers move between patches frequently. This might occur because consumers often move for reasons other than food resource acquisition.\n\nCadenasso et al. (2003) developed a framework for studying ecological boundaries, which has implications for understanding the dynamics of specific cross-boundary subsidies. The boundary is defined as the zone of the steepest gradient of change in some characteristic from one patch to another, such as rapidly decreasing light levels as habitat transitions from a field to a forest. In this framework, flows across variable landscapes are characterized by the type of flow (materials, energy, organisms, etc.), patch contrast (architecture, composition, process), and boundary structure (architecture, composition, symbolic and perceptual features). Considering a cross-boundary subsidy in terms of this framework shows how the boundary itself can mediate the subsidy. For example, Cadenasso and Pickett (2001) found that the decreased lateral vegetation at the boundary between a forest and field increased the amount of seeds transferred into the forest interior.\n\nAnother conceptual model that specifically considers cross-boundary subsidies is a model developed by Rand et al. (2006) of spillover from agriculture to wildland patches by predatory insects. The edge is permeable to insects that are habitat generalists and therefore capable of easily crossing the boundary between agriculture and wildland patches, whereas it is considered impermeable to insects that specialize on a particular patch type and cannot cross the boundary. In this model, edge permeability (habitat specialists vs. generalists), patch productivity, and complementary resource use (use of resources obtained in both agriculture and wildland patches) determine the expected impact of cross-boundary subsidies by predatory insects (Fig. 1).\n\nA spatial subsidy, in the context of landscape ecology, is a doner-mediated resource (nutrient, detritus, prey) which is passed from one habitat to a recipient (consumer) in a second habitat. As a result, the productivity of the recipient is increased (Polis et al., 1997). For example, a bear eats a salmon and acquires the resources that have passed through the marine environment across the habitat boundary and into a terrestrial environment.\n\nThe idea of a subsidy of materials or organisms across a patch boundary affecting resident populations has clear parallels with source-sink dynamics (Fagan et al. 1999). In this theory, local populations are connected by dispersal, and the extinction of local populations can be prevented through immigration from neighboring patches (Pulliam 1988). In source-sink dynamics, it is assumed that individuals from more productive patches will move to less productive patches with unsustainable populations (Pulliam 1988). Many examples of cross-boundary subsidies can be thought of as exhibiting source-sink dynamics. Rand et al. (2006) found that insects in a high productivity agricultural patch were able to sustain local populations in a lower productivity wildland patch through continued dispersal from the agricultural patch. The effect of these subsidies to local patches can also impact populations of other species in the recipient food web, because the subsidized population may compete with or prey upon other species more effectively than they would be able to without such an influx (Fagan et al. 1999).\n\nCross-boundary subsidies have important impacts on species interactions and food web dynamics. Subsidies of materials and organisms can affect all trophic, or feeding, levels of food webs either directly or indirectly. Inputs of nutrient and detritus from another patch generally increase the population growth of the resident producers (plants) and detritivores (Polis et al. 1997). Increased growth at the producer level can result in a bottom-up trophic effect, in which increases in populations at lower trophic levels support a higher population of consumers than would otherwise be possible in a closed system (Polis et al. 1997). allochthonous detrital inputs can also have strong impacts on food web dynamics over a variety of temporal scales, ranging from seconds to millennia, as in the case of fossil fuel formation from build-up of detritus over millennia (Moore et al. 2004).\n\nMany food webs rely on cross-boundary subsidies of detritus for sources of energy and nutrients (Huxel and McCann 1998). For example, a series of lakes in Wisconsin were examined for the presence of Coarse woody debris (CWD) and the characteristics of the surrounding landscape that might control its input to lakes. Coarse woody debris in these lakes is important for providing habitat and food resources for a variety of organisms including small fish (Werner and Hall 1988), algae, and detritivores (Bowen et al. 1998). Marburg et al. (2006) compared variation within and among lakes in CWD. They found that subsidies of CWD to lakes were lower when the lakes had human development along the shore. Development along the lakeshore can be thought of as an alteration to the characteristics of the patch boundary between the lake and forest. In this case, development decreased both forest density that is the source of CWD and also the permeability of the boundary to flows of CWD (Marburg et al. 2006).\n\nIn addition to bottom-up effects, top-down effects may also occur due to cross-boundary subsidies. In top-down effects, subsidies of consumers at the top level of the food web control populations at lower levels more so than would be expected by only the action of resident consumers (Polis et al. 1997). Consumers that cross boundaries may have a greater effect on the recipient patch population if prey in the recipient patch have a lower population growth rate than prey in the source patch (Fagan et al. 1999, Rand et al. 2006). Thus, cross-boundary subsidies may alter predator-prey/competitive interactions that can result in a disproportionate impact on the communities of the recipient patch.\n\nIn subsidizing top trophic levels, effects may also be felt at all lower trophic levels in a phenomenon known as a trophic cascade. An example of a trophic cascade that also acted as a cross-boundary subsidy is illustrated in a study by Knight et al. (2005) in which changes in the trophic structure of one ecosystem resulted in an effect that cascaded to the adjacent ecosystem. In ponds containing fish, dragonfly larvae were kept to a minimum by fish predation. The resulting low density of adult dragonfly predators led to a high density of bee pollinators. With fish present in adjacent ponds, bees were able to pollinate more flowers in the adjacent upland ecosystem than they were when fish were absent. The dragonfly population could be thought of as subsidized by the absence of fish predation. That subsidy was then transferred across the pond-upland boundary by adult dragonfly movement to affect the interaction between bee pollinators and plants.\n\nNative species that forage on resources that don't originate in their same habitat. This may increase their local abundances thereby affecting other species in the ecosystem. For example, Luskin et al (2017) found that native animals living protected primary rainforest in Malaysia found food subsidies in neighboring oil palm plantations. This subsidy allowed native animal populations to increase, which then triggered powerful secondary ‘cascading’ effects on forest tree community. Specically, crop-raiding wild boar (Sus scofa) built thousands of nests from the forest understory vegatation and this caused a 62% decline in forest tree sapling density over a 24-year study period. Such cross-boundary subsidy cascades may be widespread in both terrestrial and marine ecosystems and present significant conservation challenges.\n\nNative species that forage in farmland may increase their local abundances thereby affecting adjacent ecosystems within their landscape. For example, Luskin et al (2017) used two decades of ecological data from a protected primary rainforest in Malaysia to illustrate how subsidies from neighboring oil palm plantations triggered powerful secondary ‘cascading’ effects on natural habitats located >1.3 km away. They found that (i) oil palm fruit drove 100-fold increases in crop-raiding native wild boar (Sus scrofa), (ii) wild boar used thousands of understory plants to construct birthing nests in the pristine forest interior, and (iii) nest building caused a 62% decline in forest tree sapling density over the 24-year study period. The long-term, landscape-scale indirect effects from agriculture suggest its full ecological footprint may be larger in extent than is currently recognized. Cross-boundary subsidy cascades may be widespread in both terrestrial and marine ecosystems and present significant conservation challenges.\n\nAs landscapes become increasingly fragmented due to human activity, the influence of patch boundaries on individual patches becomes relatively more important (Murcia 1995). fragmentation can both cut off necessary subsidies to patches and increase the magnitude of subsidies from adjacent patches. For example, in a study of fragmentation of wildlands in an agriculturally dominated landscape, subsidies of habitat specialist insects to wildland patches were prevented by surrounding small, wildland patches with inhospitable agricultural land. This isolation reduced the potential for gene flow and long-term persistence of the population. Subsidies of other insects that specialized on agricultural crops were increased to wildland populations, increasing their effect on the resident wildland species (Duelli 1990).\n\nChanging the internal structure and composition of a patch may substantially alter cross-boundary subsidies. Logging may temporarily increase subsidies of nutrients and detritus to adjacent streams (Likens et al. 1970). Invasive species introduced to agricultural patches may act as a subsidy to adjacent wildland invasive populations, preventing native species establishment, even within the protected area (Janzen 1983).\n\nHuman alterations of patch characteristics may also curtail cross-boundary subsidies such as overfishing in marine systems, which may drastically reduce potentially crucial marine subsidies of organisms to freshwater and riparian systems (Zhang et al. 2003). For example, Helfield and Naiman (2002) found that riparian trees in Alaska obtain 24-26% of their nitrogen from marine sources, transferred from migrating salmon. In this system, salmon that feed in the ocean, incorporating marine nitrogen into their biomass, later return to their natal small streams where they spawn and die. Salmon carcasses transferred across the stream-riparian zone boundary by terrestrial predators or flooding events subsidized growth of terrestrial plants. Thus, marine overfishing may affect the productivity of Alaskan forests that depend on subsidies of marine-derived nitrogen.\n\nAs discussed above, cross-boundary subsidies depend on the characteristics of the patch boundary. Human-induced changes in these characteristics can affect boundary permeability to certain organisms or materials. For example, a cross-boundary subsidy of leaf litter from forest to an adjacent open field may be attenuated at the boundary if a road is present, making the boundary less permeable to flows of leaf litter (Facelli and Pickett 1991).\n\nHabitat management might benefit from recognizing the effect that humans may have on both individual patches and on the dynamics between patches. In such cases, managers may need to focus on patterns and processes that occur outside of their patch of interest, as these factors may also be important to internal population dynamics. An understanding of boundary features that influence the various flows of interest is necessary in managing for those flows.\n\nThe implications of invasive species and the use of biological control agents may also be closely related to the idea of cross-boundary subsidies. Introducing species into one patch for biocontrol may have unforeseen consequences on dynamics within adjacent patches.\n\nOther fields, such as public policy, can also benefit from considering cross-boundary subsidies. For example, governments often provide financial subsidies to fisheries, which have a negative effect on those ecosystems by encouraging overfishing (Munro and Sumaila 2002). Understanding what processes affect how those financial resources flow across that particular government-industry boundary is important to the maintenance of marine food webs. Considering cross-boundary effects will be essential to a complete understanding of potential consequences of human action on the landscape.\n\n"}
{"id": "2925145", "url": "https://en.wikipedia.org/wiki?curid=2925145", "title": "Daimonic", "text": "Daimonic\n\nThe idea of the daimonic typically means quite a few things: from befitting a demon and fiendish, to be motivated by a spiritual force or genius and inspired. As a psychological term, it has come to represent an elemental force which contains an irrepressible drive towards individuation. As a literary term, it can also mean the dynamic unrest that exists in us all that forces us into the unknown, leading to self-destruction and/or self-discovery.\n\nThe term is derived from Greek \"δαίμων\" (daimon, gen. daimonos): \"lesser god, guiding spirit, tutelary deity\", by way of Latin—dæmon: \"spirit\". \"Daimon\" itself is thought to be derived from \"daiomai\", with the meaning of \"to divide\" or \"to lacerate\".\nMarie-Louise von Franz delineated the term \"daiomai\" (see ref.), and indicates that its usage is specifically when someone perceived an occurrence which they attributed to the influence of a divine presence, amongst the examples provided by Franz, are from attributing to a daimon the occurrence of a horse becoming or being startled.\n\nFor the Minoan (3000-1100 BC) and Mycenaean (1500-1100 BC), \"daimons\" were seen as attendants or servants to the deities, possessing spiritual power. Later, the term \"daimon\" was used by writers such as Homer (8th century BC), Hesiod, and Plato as a synonym for \"theos\", or god. Some scholars, like van der Leeuw, suggest a distinction between the terms: whereas \"theos\" was the personification of a god (e.g. Zeus), \"daimon\" referred to something indeterminate, invisible, incorporeal, and unknown.\n\nDuring the period in which Homer was alive, people believed ailments were both caused and cured by daimons.\n\nHeraclitus Of Ephesus, who was born about 540 B.C., wrote:\n\nwhich is translated as, \"the character\" (ēthos) \"of a human\" (anthropōi) \"is the\" daimōn, or sometimes \"the character of a person is Fate\", and the variation \"An individuals character is their fate\" (\"idem\" \"Man's character is his fate\").\n\nAeschylus mentions the term Daimon in his play Agemmemnon, written during 458 B.C.\n\nSocrates thought the daimones to be gods or the children of gods.\n\nThe pre-Socratic Greek philosopher Empedocles (5th century BC) later employed the term in describing the psyche or soul. Similarly, those such as Plutarch (1st century AD) suggested a view of the daimon as being an amorphous mental phenomenon, an occasion of mortals to come in contact with a great spiritual power. Plutarch wrote \"De genio Socratis\".\n\nThe earliest pre-Christian conception of daimons or \"daimones\" also considered them ambiguous—not exclusively evil. But while daimons may have initially been seen as potentially good and evil, constructive and destructive, left to each man to relate to—the term eventually came to embody a purely evil connotation, with Xenocrates perhaps being one of the first to popularize this colloquial use.\n\nIn psychology, the daimonic refers to a natural human impulse within everyone to affirm, assert, perpetuate, and increase the self to its complete totality. If each Self undergoes a process of individuation, an involuntary and natural development towards individual maturity and harmony with collective human nature, then its driver is the daimonic, the force which seeks to overcome the obstacles to development, whatever the cost—both guide and guardian. Rollo May writes that the daimonic is \"any natural function which has the power to take over the whole person... The daimonic can be either creative or destructive, but it is normally both... The daimonic is obviously not an entity but refers to a fundamental, archetypal function of human experience -- an existential reality\". The daimonic is seen as an essentially undifferentiated, impersonal, primal force of nature which arises from the ground of being rather than the self as such.\n\nThe demands of the daimonic force upon the individual can be unorthodox, frightening, and overwhelming. With its obligation to protect the complete maturation of the individual and the unification of opposing forces within the Self, the inner urge can come in the form of a sudden journey (either intentional or serendipitous), a psychological illness, or simply neurotic and off-center behavior. Jung writes, \"The daimon throws us down, makes us traitors to our ideals and cherished convictions — traitors to the selves we thought we were.\" Ultimately, it is the will of man to achieve his humanity, but since parts of his humanity may be deemed unacceptable and disowned, its demands are too often resisted. It is no wonder Yeats described it as that \"other Will\". Confrontation with the daimonic can be considered similar to \"shadow-work\".\n\nThe psychologist Rollo May conceives of the daimonic as a primal force of nature which contains both constructive and destructive potentialities, but ultimately seeks to promote totality of the self. May introduced the daimonic to psychology as a concept designed to rival the terms 'devil' and 'demonic'. He believed the term demonic to be unsatisfactory because of our tendency, rooted in Judeo-Christian mythology, to project power outside of the self and onto devils and demons. The daimonic is also similar to Jung's shadow, but is viewed as less differentiated. A pitfall of the Jungian doctrine of the shadow is the temptation to project evil onto this relatively autonomous 'splinter personality' and thus unnecessarily fragment the individual and obviate freedom and responsibility. Finally, by comparison to Freud's death instinct (Thanatos), the daimonic is seen as less one-sided.\n\nWhile similar to several other psychological terms, noteworthy differences exist. The daimonic is often improperly confused with the term demonic.\n\nThe journey from innocence to experience is not an idea that originated with this term; rather the Hero's Journey is a topic older than literature itself. But the daimonic subsequently became a focus of the English Romantic movement in the 18th and 19th centuries.\n\nIn the diagram, the common threads of the daimonic concept are identified. Typically, the daimonic tale centers around the Solitary, the central character of the story, who usually is introduced in innocence, wealth, and often arrogance. However, under the masks of control and order lies a corruption and unconscious desire towards disintegration. Some event, either external or internal, leads the character towards some type of isolation where he is forced to confront his daimons.\n\nThe fall or descent (from hubris) into the liminal world where light and dark meet is usually very dramatic and often torturing for the hero and the audience alike, and comes in myriad forms. In the depths, in hitting bottom, he ultimately discovers his own fate and tragedy (catharsis), and in a final climax is either broken or driven towards rebirth and self-knowledge. The glory of the daimonic is in humble resurrection, though it claims more than it sets free as many foolish men are drawn into its vacuum never to return. As Stefan Zweig writes, the hero is unique for \"he becomes the daimon's master instead of the daimon's thrall\". The daimonic has been, and continues to be, a great source of creativity, inspiration, and fascination in all forms of art.\n\n"}
{"id": "27605494", "url": "https://en.wikipedia.org/wiki?curid=27605494", "title": "Diesel therapy", "text": "Diesel therapy\n\nDiesel therapy is a form of punishment in which prisoners are shackled and then transported for days or weeks. It has been described as \"the cruelest aspect of being a federal inmate.\" It has been alleged that some inmates are deliberately sent to incorrect destinations as an exercise of diesel therapy. Voluntary surrender at the prison where the inmate will serve his time is recommended as a way of avoiding diesel therapy. The case of former U.S. Representative George V. Hansen involved accusations of diesel therapy, as did the case of Susan McDougal, one of the few people who served prison time as a result of the Whitewater controversy. Diesel therapy is sometimes used on disruptive inmates, including gang members. Other alleged recipients include Rudy Stanko, who was also the defendant in the speeding case that ended Montana's \"free speed\" period.\n\nThe term \"diesel therapy,\" or \"dumping,\" is also used to refer to a method by law-enforcement personnel of getting rid of troublesome individuals by placing them on a bus to another jurisdiction. This is also known as bus therapy and is akin to Greyhound therapy in health care.\n\n"}
{"id": "51331", "url": "https://en.wikipedia.org/wiki?curid=51331", "title": "Dimensionless quantity", "text": "Dimensionless quantity\n\nIn dimensional analysis, a dimensionless quantity is a quantity to which no physical dimension is assigned. It is also known as a bare number or pure number or a quantity of dimension one and the corresponding unit of measurement in the SI is one (or 1) unit and it is not explicitly shown. Dimensionless quantities are widely used in many fields, such as mathematics, physics, chemistry, engineering, and economics. Examples of quantities to which dimensions are regularly assigned are length, time, and speed, which are measured in dimensional units, such as metre, second and metre per second. This is considered to aid intuitive understanding. However, especially in mathematical physics, it is often more convenient to drop the assignment of explicit dimensions and express the quantities without dimensions, e.g., addressing the speed of light simply by the dimensionless number .\n\nQuantities having dimension 1, \"dimensionless quantities\", regularly occur in sciences, and are formally treated within the field of dimensional analysis. In the nineteenth century, French mathematician Joseph Fourier and Scottish physicist James Clerk Maxwell led significant developments in the modern concepts of dimension and unit. Later work by British physicists Osborne Reynolds and Lord Rayleigh contributed to the understanding of dimensionless numbers in physics. Building on Rayleigh's method of dimensional analysis, Edgar Buckingham proved the theorem (independent of French mathematician Joseph Bertrand's previous work) to formalize the nature of these quantities. \n\nNumerous dimensionless numbers, mostly ratios, were coined in the early 1900s, particularly in the areas of fluid mechanics and heat transfer. Measuring \"ratios\" in the (derived) unit \"dB\" (decibel) finds widespread use nowadays. \n\nIn the early 2000s, the International Committee for Weights and Measures discussed naming the unit of 1 as the \"uno\", but the idea of just introducing a new SI-name for 1 was dropped.\n\nAll pure numbers are dimensionless quantities, for example 1, , , , and . Units of number such as the dozen, gross, googol, and Avogadro's number may also be considered dimensionless.\n\nDimensionless quantities are often obtained as ratios of quantities that are not dimensionless, but whose dimensions cancel out in the mathematical operation. Examples include calculating slopes or unit conversion factors. A more complex example of such a ratio is engineering strain, a measure of physical deformation defined as a change in length divided by the initial length. Since both quantities have the dimension \"length\", their ratio is dimensionless. Another set of examples is mass fractions or mole fractions often written using parts-per notation such as ppm (= 10), ppb (= 10), and ppt (= 10), or more confusingly as ratios of two identical units (kg/kg or mol/mol). For example, alcohol by volume, which characterizes the concentration of ethanol in an alcoholic beverage, could be written as mL / 100 mL.\n\nOther common proportions are percentages % (= 0.01),  ‰ (= 0.001) and angle units such as radians, degrees (°= ) and grads(= ). In statistics the coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the dispersion in the data.\n\nThe Buckingham theorem indicates that validity of the laws of physics does not depend on a specific unit system. A statement of this theorem is that any physical law can be expressed as an identity involving only dimensionless combinations (ratios or products) of the variables linked by the law (e. g., pressure and volume are linked by Boyle's Law – they are inversely proportional). If the dimensionless combinations' values changed with the systems of units, then the equation would not be an identity, and Buckingham's theorem would not hold.\n\nAnother consequence of the theorem is that the functional dependence between a certain number (say, \"n\") of variables can be reduced by the number (say, \"k\") of independent dimensions occurring in those variables to give a set of \"p\" = \"n\" − \"k\" independent, dimensionless quantities. For the purposes of the experimenter, different systems that share the same description by dimensionless quantity are equivalent.\n\nTo demonstrate the application of the theorem, consider the power consumption of a stirrer with a given shape.\nThe power, \"P\", in dimensions [M · L/T], is a function of the density, \"ρ\" [M/L], and the viscosity of the fluid to be stirred, \"μ\" [M/(L · T)], as well as the size of the stirrer given by its diameter, \"D\" [L], and the angular speed of the stirrer, \"n\" [1/T]. Therefore, we have a total of \"n\" = 5 variables representing our example. Those \"n\" = 5 variables are built up from \"k\" = 3 fundamental dimensions, the length: L (SI units: m), time: T (s), and mass: M (kg).\n\nAccording to the -theorem, the \"n\" = 5 variables can be reduced by the \"k\" = 3 dimensions to form \"p\" = \"n\" − \"k\" = 5 − 3 = 2 independent dimensionless numbers. These quantities are formula_1, commonly named the Reynolds number which describes the fluid flow regime, and formula_2, the power number, which is the dimensionless description of the stirrer.\n\nCertain universal dimensioned physical constants, such as the speed of light in a vacuum, the universal gravitational constant, Planck's constant, Coulomb's constant, and Boltzmann's constant can be normalized to 1 if appropriate units for time, length, mass, charge, and temperature are chosen. The resulting system of units is known as the natural units, specifically regarding these five constants, Planck units. However, not all physical constants can be normalized in this fashion. For example, the values of the following constants are independent of the system of units, cannot be defined, and can only be determined experimentally:\n\nPhysics often uses dimensionless quantities to simplify the characterization of systems with multiple interacting physical phenomena. These may be found by applying the Buckingham theorem or otherwise may emerge from making partial differential equations unitless by the process of nondimensionalization. Engineering, economics, and other fields often extend these ideas in design and analysis of the relevant systems.\n\n\n\n\n\n"}
{"id": "32909899", "url": "https://en.wikipedia.org/wiki?curid=32909899", "title": "Eddy current separator", "text": "Eddy current separator\n\nAn eddy current separator uses a powerful magnetic field to separate non-ferrous metals from waste after all ferrous metals have been removed previously by some arrangement of magnets. The device makes use of eddy currents to effect the separation. Eddy current separators are not designed to sort ferrous metals which become hot inside the eddy current field. This can lead to damage of the eddy current separator unit belt.\n\nThe eddy current separator is applied to a conveyor belt carrying a thin layer of mixed waste. At the end of the conveyor belt is an eddy current rotor. Non-ferrous metals are thrown forward from the belt into a product bin, while non-metals simply fall off the belt due to gravity.\n\nEddy current separators may use a rotating drum with permanent magnets, or may use an electromagnet depending on the type of separator.\n\nA patent for a device using eddy currents to separate non-ferrous metals from non-metals was granted to William Benson and Thomas Falconer of Eriez Magnetics in 1969.\n"}
{"id": "3003070", "url": "https://en.wikipedia.org/wiki?curid=3003070", "title": "Empirical modelling", "text": "Empirical modelling\n\nEmpirical modelling refers to any kind of (computer) modelling based on empirical observations rather than on mathematically describable relationships of the system modelled.\n\nEmpirical Modelling as a variety of empirical modelling\n\nEmpirical modelling is a generic term for activities that create models by observation and experiment. Empirical Modelling (with the initial letters capitalised, and often abbreviated to EM) refers to a specific variety of empirical modelling in which models are constructed following particular principles. Though the extent to which these principles can be applied to model-building without computers is an interesting issue (to be revisited below), there are at least two good reasons to consider Empirical Modelling in the first instance as computer-based. Without doubt, new technologies that are based on computers have had a transformative impact where the full exploitation of Empirical Modelling principles is concerned. What is more, the conception of Empirical Modelling has been closely associated with thinking about the role of the computer in model-building.\n\nAn empirical model operates on a simple semantic principle: the maker observes a close correspondence between the behaviour of the model and that of its referent. The crafting of this correspondence can be 'empirical' in a wide variety of senses: it may entail a trial-and-error process, may be based on computational approximation to analytic formulae, it may be derived as a black-box relation that affords no insight into 'why it works'.\n\nEmpirical Modelling is rooted on the key principle of William James's 'radical empiricism', which postulates that all knowing is rooted in connections that are given-in-experience. Empirical Modelling aspires to craft the correspondence between the model and its referent in such a way that its derivation can be traced to connections given-in-experience. Making connections in experience is an essentially individual human activity that requires skill and is highly context-dependent. Examples of such connections include: identifying familiar objects in the stream of thought, associating natural languages words with objects to which they refer, and subliminally interpreting the rows and columns of a spreadsheet as exam results of particular students in particular subjects.\n\nEmpirical Modelling principles\n\nIn Empirical Modelling, the process of construction is an incremental one in which the intermediate products are artefacts that evoke aspects of the intended (and sometimes emerging) referent through live interaction and observation. The connections evoked in this way have distinctive qualities: they are of their essence personal and experiential in character and are provisional in so far as they may be undermined, refined and reinforced as the model builder's experience and understanding of the referent develops. Following a precedent established by David Gooding in his account of the role that artefacts played in Michael Faraday's experimental investigation of electromagnetism, the intermediate products of the Empirical Modelling process are described as 'construals'. Gooding's account is a powerful illustration of how \"making construals\" can support the sense-making activities that lead to conceptual insights (cf. the contribution that Faraday's work made to electromagnetic theory) and to practical products (cf. Faraday's invention of the electric motor).\n\nThe activities associated with making a construal in the Empirical Modelling framework are depicted in Figure 1.\n\nThe eye icon at the centre the figure represents the maker's observation of the current state of development of the construal and its referent. The two arrows emanating from the eye represent the connection given-in-experience between the construal and its referent that is established in the mind of the maker. This connection is crafted through experimental interaction with the construal under construction and its emerging referent. As in genuine experiment, the scope of the interactions that can be entertained by the maker is inconceivably broad. At the maker's discretion, the interactions that characterise the construal are those that respect the connection given in the maker's experience. As the Empirical Modelling process unfolds, the construal, the referent, the maker's understanding and the context for the maker's engagement co-evolve in such a way that:\nEmpirical Modelling concepts\n\nIn Empirical Modelling. making and maintaining the connection given-in-experience between the construal and referent is based on three primary concepts: \"observables\", \"dependencies\" and \"agency\". Within both the construal and its referent, the maker identifies observables as entities that can take on a range of different values, and whose current values determine its current state. All state-changing interactions with the construal and referent are conceived as changes to the values of observables. A change to the value of one observable may be directly attributable to a change in the value of another observable, in which case these values are linked by a dependency. Changes to observable values are attributed to agents, amongst which the most important is the maker of the construal. When changes to observable values are observed to occur simultaneously, this can be construed as concurrent action on the part of different agents, or as concomitant changes to observables derived from a single agent action via dependencies. To craft the connection given-in-experience between the construal and referent, the maker constructs the construal in such a way that its observables, dependencies and agency correspond closely to those that are observed in the referent. To this end, the maker must conceive appropriate ways in which observables and agent actions in the referent can be given suitable experiential counterparts in the construal.\n\nThe semantic framework shown in Figure 1 resembles that adopted in working with spreadsheets, where the state that is currently displayed in the grid is meaningful only when experienced in conjunction with an external referent. In this setting, the cells serve as observables, their definitions specify the dependencies, and agency is enacted by changing the values or the definitions of cells. In making a construal, the maker explores the roles of each relevant agent by projecting agency upon it as if it were a human agent and identifying observables and dependencies from that perspective. By automating agency, construals can then be used to specify behaviours in much the same way that behaviours can be expressed using macros in conjunction with spreadsheets. In this way, animated construals can emulate program-like behaviours in which the intermediate states are meaningful and live to auditing by the maker.\n\nEnvironments to support Empirical Modelling\n\nThe development of computer environments for making construals has been an ongoing subject of research over the last thirty years. The many variants of such environments that have been implemented are based on common principles. The network of dependencies that currently connect observables is recorded as a family of definitions. Semantically such definitions resemble the definitions of spreadsheet cells, whereby changes to the values of observables on the right hand side propagate so as to change the value of the observable on the LHS in a conceptually indivisble manner. The dependencies in these networks are acyclic but are also reconfigurable: redefining an observable may introduce a new definition that alters the dependency structure. Observables built into the environment include scalars, geometric and screen display elements: these can be elaborated using multi-level list structures. A dependency is typically represented by a definition which uses a relatively simple functional expression to relate the value of an observable to the values of other observables. Such functions have typically been expressed in fragments of simple procedural code, but the most recent variants of environments of making construals also enable dependency relations to be expressed by suitably contextualised families of definitions. The maker can interact with a construal through redefining existing observables or introducing new observables in an open-ended unconstrained manner. Such interaction has a crucial role in the experimental activity that informs the incremental development of the construal. Triggered actions can be introduced to automate state-change: these perform redefinitions in response to specified changes in the values of observables.\n\nEmpirical Modelling as a broader view of computing\n\nIn Figure 1, identifying 'the computer' as the medium in which the construal is created is potentially misleading. The term COMPUTER is not merely a reference to a powerful computational device. In making construals, the primary emphasis is on the rich potential scope for interaction and perceptualisation that the computer enables when used in conjunction with other technologies and devices. The primary motivation for developing Empirical Modelling is to give a satisfactory account of computing that integrates these two complementary roles of the computer. The principles by which James and Dewey sought to reconcile perspectives on agency informed by logic and experience play a crucial role in achieving this integration.\n\nThe dual role for the computer implicit in Figure 1 is widely relevant to contemporary computing applications. On this basis, Empirical Modelling can be viewed as providing a foundation for a broader view of computing. This perspective is reflected in numerous Empirical Modelling publications on topics such as educational technology, computer-aided design and software development. Making construals has also been proposed as a suitable technique to support constructionism, as conceived by Seymour Papert, and to meet the guarantees for 'construction' as identified by Bruno Latour.\n\nEmpirical Modelling as generic sense-making?\n\nThe Turing machine provides the theoretical foundation for the role of the computer as a computational device: it can be regarded as modelling 'a mind following rules'. The practical applications of Empirical Modelling to date suggest that making construals is well-suited to supporting the supplementary role the computer can play in orchestrating rich experience. In particular, in keeping with the pragmatic philosophical stance of James and Dewey, making construals can fulfill an explanatory role by offering contingent explanations for human experience in contexts where computational rules cannot be invoked. In this respect, making construals may be regarded as modelling 'a mind making sense of a situation'.\n\nIn the same way that the Turing machine is a conceptual tool for understanding the nature of algorithms whose value is independent of the existence of the computer, Empirical Modelling principles and concepts may have generic relevance as a framework for thinking about sense-making without specific reference to the use of a computer. The contribution that William James's analysis of human experience makes to the concept of Empirical Modelling may be seen as evidence for this. By this token, Empirical Modelling principles may be an appropriate way to analyse varieties of empirical modelling that are not computer-based. For instance, it is plausible that the analysis in terms of observables, dependencies and agency that applies to interaction with electronic spreadsheets would also be appropriate for the manual spreadsheets that predated them.\n\nBackground\n\nEmpirical Modelling has been pioneered since the early 1980s by Meurig Beynon and the Empirical Modelling Research Group in Computer Science at the University of Warwick.\n\nThe term 'Empirical Modelling' (EM) has been adopted for this work since about 1995 to reflect the experiential basis of the modelling process in observation and experiment. Special purpose software supporting the central concepts of observable, dependency and agency has been under continuous development (mainly led by research students) since the late 1980s.\n\nThe principles and tools of EM have been used and developed by many hundreds of students within coursework, project work, and research theses. The undergraduate and MSc module 'Introduction to Empirical Modelling' was taught for many years up to 2013-14 until the retirement of Meurig Beynon and Steve Russ (authors of this article). There is a large website [1] containing research and teaching material with an extensive collection of refereed publications and conference proceedings.\n\nThe term 'construal' has been used since the early 2000s for the artefacts, or models, made with EM tools. The term has been adapted from its use by David Gooding in the book 'Experiment and the Making of Meaning' (1990) to describe the emerging, provisional ideas that formed in Faraday's mind, and were recorded in his notebooks, as he investigated electromagnetism, and made the first electric motors, in the 1800s.\n\nThe main practical activity associated with EM - that of 'making construals' - was the subject of an Erasmus+ Project CONSTRUIT! (2014-2017)[2].\n\n[1] http://www.dcs.warwick.ac.uk/modelling/ Empirical Modelling Research Group\n\n[2] https://warwick.ac.uk/fac/sci/dcs/research/em/welcome/ CONSTRUIT! Project web pages\n"}
{"id": "24855262", "url": "https://en.wikipedia.org/wiki?curid=24855262", "title": "Encounter at the Elbe", "text": "Encounter at the Elbe\n\nEncounter at the Elbe (in ) is a Soviet movie released in 1949 from Mosfilm, describing the conflict, spying, and collaboration between the Soviet Army advancing from the east and the U.S. Army advancing from the west. The two allied forces met each other for the first time on the River Elbe near the close of the Second World War. This meeting occurred on April 25, 1945, which was usually remembered as “Elbe Day” in Western Bloc nations and as the \"Encounter at the Elbe” in Eastern Bloc nations.\n\nThe movie was directed by Grigori Aleksandrov and Aleksey Utkin, with music by Dmitri Shostakovich, which included “Yearning for the Homeland” (in , the words by Yevgeniy Dolmatovsky), that became popular at that time in the Eastern Bloc nations and among the leftists in the Western Bloc nations including Japan.\n\n\n"}
{"id": "11563109", "url": "https://en.wikipedia.org/wiki?curid=11563109", "title": "Euphoria", "text": "Euphoria\n\nEuphoria () is the experience (or affect) of pleasure or excitement and intense feelings of well-being and happiness. Certain natural rewards and social activities, such as aerobic exercise, laughter, listening to or making music, and dancing, can induce a state of euphoria. Euphoria is also a symptom of certain neurological or neuropsychiatric disorders, such as mania. Romantic love and components of the human sexual response cycle are also associated with the induction of euphoria. Certain drugs, many of which are addictive, can cause euphoria, which at least partially motivates their recreational use.\n\nHedonic hotspots – i.e., the pleasure centers of the brain – are functionally linked. Activation of one hotspot results in the recruitment of the others. Inhibition of one hotspot results in the blunting of the effects of activating another hotspot. Therefore, the simultaneous activation of every hedonic hotspot within the reward system is believed to be necessary for generating the sensation of an intense euphoria.\n\nThe word \"euphoria\" is derived from the Ancient Greek terms : εὖ \"eu\" meaning \"well\" and φέρω \"pherō\" meaning \"to bear\". It is semantically opposite to dysphoria.\n\nA 1706 English dictionary defines euphoria as \"the well bearing of the Operation of a Medicine, i.e., when the patient finds himself eas'd or reliev'd by it\".\n\nIn the 1860s, the English physician Thomas Laycock described euphoria as the feeling of bodily well-being and hopefulness; he noted its misplaced presentation in the final stage of some terminal illnesses and attributed such euphoria to neurological dysfunction. Sigmund Freud's 1884 monograph \"Über Coca\" described (his own) consumption of cocaine producing \"the normal euphoria of a healthy person\", while about 1890 the German neuropsychiatrist Carl Wernicke lectured about the \"abnormal euphoria\" in patients with mania.\n\nA 1903 article in \"The Boston Daily Globe\" refers to euphoria as \"pleasant excitement\" and \"the sense of ease and well-being\". In 1920 \"Popular Science\" magazine described euphoria as \"a high sounding name\" meaning \"feeling fit\": normally making life worth living, motivating drug use, and ill formed in certain mental illnesses. Robert S. Woodworth's 1921 textbook \"Psychology: A study of mental life\", describes euphoria as an organic state which is the opposite of fatigue, and \"means about the same as feeling good.\"\n\nIn 1940 \"The Journal of Psychology\" defined euphoria as a \"state of general well being ... and pleasantly toned feeling.\" A decade later, finding \"ordinary feelings of well being\" difficult to evaluate, American addiction researcher Harris Isbell redefined euphoria as behavioral changes and objective signs typical of morphine. However, in 1957 British pharmacologist D. A. Cahal did not regard opioid euphoria as medically undesirable but an effect which \"enhance[s] the value of a major analgesic.\" The 1977 edition of \"A Concise Encyclopaedia of Psychiatry\" called euphoria \"a mood of contentment and well-being,\" with pathologic associations when used in a psychiatric context. As a sign of cerebral disease, it was described as bland and out of context, representing an inability to experience negative emotion.\n\nIn the 21st century, euphoria is \"generally\" defined as a state of great happiness, well-being and excitement, which may be normal, or abnormal and inappropriate when associated with psychoactive drugs, manic states, or brain disease or injury.\n\nHedonic hotspots – i.e., the pleasure centers of the brain – are functionally linked. Activation of one hotspot results in the recruitment of the others. Inhibition of one hotspot results in the blunting of the effects of activating another hotspot. Therefore, the simultaneous activation of every hedonic hotspot within the reward system is believed to be necessary for generating the sensation of euphoria.\n\nMany different types of stimuli can induce euphoria, including psychoactive drugs, natural rewards, and social activities. Affective disorders such as unipolar mania or bipolar disorder can involve euphoria as a symptom.\n\nContinuous physical exercise, particularly aerobic exercise, can induce a state of euphoria; for example, distance running is often associated with a \"runner's high\", which is a pronounced state of exercise-induced euphoria. Exercise is known to affect dopamine signaling in the nucleus accumbens, producing euphoria as a result, through increased biosynthesis of three particular neurochemicals: anandamide (an endocannabinoid), β-endorphin (an endogenous opioid), and phenethylamine (a trace amine and amphetamine analog).\n\nEuphoria can occur as a result of dancing to music, music-making, and listening to emotionally arousing music. Neuroimaging studies have demonstrated that the reward system plays a central role in mediating music-induced pleasure. Pleasurable emotionally arousing music strongly increases dopamine neurotransmission in the dopaminergic pathways that project to the striatum (i.e., the mesolimbic pathway and nigrostriatal pathway). Approximately 5% of the population experiences a phenomenon termed \"musical anhedonia\", in which individuals do not experience pleasure from listening to emotionally arousing music despite having the ability to perceive the intended emotion that is conveyed in passages of music.\n\nThe various stages of copulation may also be described as inducing euphoria in some people. Various analysts have described either the entire act of copulation, the moments leading to orgasm, or the orgasm itself as the pinnacle of human pleasure or euphoria.\n\nA euphoriant is a type of psychoactive drug which tends to induce euphoria. Most euphoriants are addictive drugs due to their reinforcing properties and ability to activate the brain's reward system.\n\nDopaminergic stimulants like amphetamine, methamphetamine, cocaine, MDMA, and methylphenidate are euphoriants. Nicotine is a parasympathetic stimulant that acts as a mild euphoriant in some people.\n\nSome textbooks state that caffeine is a mild euphoriant, others state that it is not a euphoriant, and one states that it is and is not a euphoriant.\n\nChewing areca nut (seeds from the \"Areca catechu\" palm) with slaked lime (calcium hydroxide) – a common practice in South- and Southeast Asia – produces stimulant effects and euphoria. The major psychoactive ingredients – arecoline (a muscarinic receptor partial agonist) and arecaidine (a GABA reuptake inhibitor) – are responsible for the euphoric effect.\n\nCertain depressants can produce euphoria; some of the euphoriant drugs in this class include alcohol in moderate doses, , and ketamine.\n\nSome barbiturates and benzodiazepines may cause euphoria. Euphoriant effects are determined by the drug's speed of onset, increasing dose, and with intravenous administration. Barbiturates more likely to cause euphoria include amobarbital, secobarbital and pentobarbital. Benzodiazepines more likely to cause euphoria are flunitrazepam, alprazolam and clonazepam. Benzodiazepines also tend to enhance opioid-induced euphoria.\n\nPregabalin induces dose-dependent euphoria. Occurring in a small percentage of individuals at recommended doses, euphoria is increasingly frequent at supratherapeutic doses (or with intravenous- or nasal administration). At doses five times the maximum recommended, intense euphoria is reported. Another GABA analogue, gabapentin, may induce euphoria. Characterized as opioid-like but less intense, it may occur at supratherapeutic doses, or in combination with other drugs, such as opioids or alcohol. Ethosuximide and perampanel can also produce euphoria at therapeutic doses.\n\nµ-Opioid receptor agonists are a set of euphoriants that include drugs such as heroin, morphine, codeine, oxycodone, and fentanyl. By contrast, κ-opioid receptor agonists, like the endogenous neuropeptide \"dynorphin\", are known to cause dysphoria, a mood state opposite to euphoria that involves feelings of profound discontent.\n\nCannabinoid receptor 1 agonists are a group of euphoriants that includes certain plant-based cannabinoids (e.g., THC from the cannabis plant), endogenous cannabinoids (e.g., anandamide), and synthetic cannabinoids.\n\nCertain gases, like nitrous oxide (NO, aka \"laughing gas\"), can induce euphoria when inhaled.\n\nAcute exogenous glucocorticoid administration is known to produce euphoria, but this effect is not observed with long-term exposure.\n\nFasting has been associated with improved mood, well-being, and sometimes euphoria. Various mechanisms have been proposed and possible applications in treating depression considered.\n\nAsphyxiation initially produces an extreme feeling of euphoria leading some people to intentionally induce temporary asphyxiation. Erotic asphyxiation typically employs strangulation to produce euphoria which enhances masturbation and orgasm. The choking game, prevalent in adolescents, uses brief hypoxia in the brain to achieve euphoria. Strangulation, or hyperventilation followed by breath holding are commonly used to achieve the effects. Accidental deaths occur from both practices but are often mislabeled as suicide.\n\nEuphoria is also strongly associated with both hypomania and mania, mental states characterized by a pathological heightening of mood, which may be either euphoric or irritable, in addition to other symptoms, such as pressured speech, flight of ideas, and grandiosity.\n\nAlthough hypomania and mania are syndromes with multiple etiologies (that is, ones that may arise from any number of conditions), they are most commonly seen in bipolar disorder, a psychiatric illness characterized by alternating periods of mania and depression.\n\nEuphoria may occur during auras of seizures typically originating in the temporal lobe, but affecting the anterior insular cortex. This euphoria is symptomatic of a rare syndrome called ecstatic seizures, often also involving mystical experiences. Euphoria (or more commonly dysphoria) may also occur in periods between epileptic seizures. This condition, \"interictal dysphoric disorder\", is considered an atypical affective disorder. Persons who experience feelings of depression or anxiety between or before seizures occasionally experience euphoria afterwards.\n\nSome persons experience euphoria in the prodrome – hours to days before the onset – of a migraine headache. Similarly, a euphoric state occurs in some persons following the migraine episode.\n\nEuphoria sometimes occurs in persons with multiple sclerosis as the illness progresses. This euphoria is part of a syndrome originally called \"euphoria sclerotica,\" which typically includes disinhibition and other symptoms of cognitive and behavioral dysfunction.\n\n"}
{"id": "6892236", "url": "https://en.wikipedia.org/wiki?curid=6892236", "title": "Ezra Pound's Three Kinds of Poetry", "text": "Ezra Pound's Three Kinds of Poetry\n\nEzra Pound distinguished three \"kinds of poetry:\" melopoeia, phanopoeia, and logopoeia.\n\nMelopoeia or melopeia is when words are \"charged\" beyond their normal meaning with some musical property which further directs its meaning, inducing emotional correlations by sound and rhythm of the speech.\n\nMelopoeia can be \"appreciated by a foreigner with a sensitive ear\" but does not translate well, according to Pound.\n\nPhanopoeia or phanopeia is defined as \"a casting of images upon the visual imagination,\" throwing the object (fixed or moving) on to the visual imagination. In the first publication of these three types, Pound refers to phanopoeia as \"imagism.\"\n\nPhanopoeia can be translated without much difficulty, according to Pound.\n\nLogopoeia or logopeia is defined by Pound as poetry that uses words for more than just their direct meaning, stimulating the visual imagination with phanopoeia and inducing emotional correlations with melopoeia.\n\nPound was said to have coined the word from Greek roots in a 1918 review of the \"Others\" poetry anthology — he defined the term as \"the dance of the intellect among words.\" Elsewhere he changes intellect to intelligence. In the \"New York Herald Tribune\" of 20 January 1929, he gave a less opaque definition: poetry which \"employs words not only for their direct meaning, but [...] takes count in a special way of habits of usage, of the context we expect to find with the word\".\n\nLogopoeia is the most recent kind of poetry and does not translate well, according to Pound, though he also claimed it was abundant in the poetry of Sextus Propertius (c.50BC-15BC).\n\nThe actual word \"logopoeia\" was not coined by Pound. The word already existed in classical Greek, as one sees by looking at any version of Liddell and Scott.\n"}
{"id": "1502517", "url": "https://en.wikipedia.org/wiki?curid=1502517", "title": "Flow diagram", "text": "Flow diagram\n\nFlow diagram is a collective term for a diagram representing a flow or set of dynamic relationships in a system. The term flow diagram is also used as a synonym for flowchart, and sometimes as a counterpart of the flowchart. \n\nFlow diagrams are used to structure and order a complex system, or to reveal the underlying structure of the elements and their interaction. \n\nThe term flow diagram is used in theory and practice in different meanings. Most commonly the flow chart and flow diagram are used in an interchangeable way in the meaning of a representation of a process. For example the \"Information Graphics: A Comprehensive Illustrated Reference\" by Harris (1999) gives two separate definitions: \n\nIn the second definition the meaning is limited to the representation of the physical route or flow. An example of such a diagram is the illustration of the flows in a nuclear submarine propulsion system, which shows different streams back and forth in the system. The representation of such a system can be supplemented by one or more flowcharts, which show the sequence of one of the flows in one direction, or any of the control flows to manage the system. \n\nThe physical movement of objects from one location to another can also be visualized in a mix of maps and flowchart or sankey diagram, which are called flow maps. Flow maps can indicate on a map, what flows, moves or migrates, in which direction, and in which quantities etc.\n\nIn theory and practice specific types of diagrams are also called a type of flow diagrams, such as:\n\n"}
{"id": "41047612", "url": "https://en.wikipedia.org/wiki?curid=41047612", "title": "Free, prior and informed consent", "text": "Free, prior and informed consent\n\nThe aim of Free Prior Informed consent (FPIC), is to establish bottom up participation and consultation of an Indigenous Population prior to the beginning of a development on ancestral land or using resources within the Indigenous Population's territory. Indigenous people (IP) have a special connection to their land and resources, they inhabit 20% of the earth's surface, these areas are environmentally rich in both renewable and non-renewable resources. The collective ownership style of most Indigenous Peoples conflicts with the modern global market and its continuous need for resources and land. To protect Indigenous peoples rights, international human rights law has created process and standards to safeguard their way of life and to encourage participation in the decision making process. One of these methods is the process of FPIC.\nThere is criticism that many international conventions and treaties only require consultation and not consent, which is a much higher threshold. Without the requirement for consent Indigenous People are not able to veto government projects and developments in their area which directly affects their lives and cultures. FPIC allows Indigenous Peoples to have the right to self-determination and self-governance in national and local government decision making process over projects that concern their lives and resources.\n\nExamples include natural resource management, economic development, uses of traditional knowledge, genetic resources, health care, and education.\n\nThe United Nations Permanent Forum on Indigenous Issues (UNPFII) has defined the concept of Free Prior Informed Consent as the following:\n\nThe UNPFII requires that Indigenous People should be consulted in a way which is appropriate for their customs. This means that not every member will have to agree. This has been criticised by some [women's rights] groups. The Indigenous People will determine who is to be consulted and must effectively communicate this with the Government and Developers. It is the duty of the state to make sure that FPIC has been carried out, if it has not then it is their issue to redress, not the company or the people wishing to carry out the project. The International Labour Organization requires that consultation takes place in a climate of mutual trust and circumstances are considered appropriate if they create favourable conditions for reaching agreement and consent.\nIn a pilot study by the UN-Reducing Emissions from Deforestation and forest Degradation UN-REDD of FPIC application in Vietnam the following steps were required;\n\nThe principle of FPIC within international development is most clearly stated in the United Nations Declaration on the Rights of Indigenous Peoples (UNDRIP). Article 10 states:\n\n\"Indigenous peoples shall not be forcibly removed from their lands or territories. No relocation shall take place without the free, prior and informed consent of the indigenous peoples concerned and after agreement on just and fair compensation and, where possible, with the option of return. \" \n\nArticles 11, 19, 28, and 29 of the declaration also explicitly use the term. It is further established in international conventions, notably the ILO Convention 169 on Indigenous and Tribal Peoples. Countries including Peru, Australia, and the Philippines have included FPIC in their national law.\n\nThe role of indigenous peoples' FPIC in decisions about infrastructure or extractive industries developed on their ancestral domain is an issue within international law. Projects lacking FPIC are called development aggression by indigenous peoples. The issue of indigenous peoples' lack of access to accountability and grievance mechanisms to address human rights violations has been formally raised with the United Nations Human Rights Council. Asian Indigenous peoples have urged the UN to address this before the economic integration of ASEAN in 2015, given the human rights records of member states such as Myanmar and Laos, which are among the world's most repressive societies.\n\nThe International Labour Organization (ILO) [] has been working with indigenous people since the 1920s, it currently has 187 member states, including New Zealand. ILO Convention 169 (the Convention) [] on Indigenous and tribal peoples is an international treaty adopted by the ILO in 1989. The Convention aims to overcome discriminatory practices affecting Indigenous People and enable them in the decision making process. The fundamental foundations of the Convention are participation and consultation. The requirement for consultation falls upon the Government of the state and not on private persons or companies, this may be delegated but ultimately the responsibility rests on the government. The need for consultation of IPs is written throughout the Convention a number of times and is referred to in : Articles 6, 7, 16 and 22. Article 6(1) states that governments should: \n\" “Consult the peoples concerned, through appropriate procedures and in particular through their representative institutions, whenever consideration is being given to legislative or administrative measures which may affect them directly\" \"'\n\nArticle 6 (2) states that this consultation will be carried out in good faith and in a form that is appropriate to the circumstances. The aim of this consultation process is to achieve an agreement or consent to the purposed development. \nThe Convention does not allow Indigenous People to veto any developments; the condition is for mere consultation not consent. The supervisor bodies of the ILO have stated that the consultation process cannot only be information sharing that there must be a chance for the Indigenous People to influence the decision making process. If consent is not achieved the nation-state must still respect other areas of the convention that include the Indigenous peoples right to their lands. Such as Article 16 (2) requires that free informed consent must be given where is the relocation of people. The treaty is legally binding on all states that ratify it, if a state chooses to ratify they may need to adjust domestic legislation. In nations such as NZ domestic legislation such as the Resource Management Act 1991 refers to the need to consider in developments Maori relationship with land and water sites. This spiritual and practical connection that Maori have to the land has been considered in a number cases before the court including the supreme court case of \"Paki v Attorney General\".\n\nThe United Nations describes FPIC both directly and indirectly in numerous conventions and treaties. One of the most direct is located in the United Nations Declaration on the Rights of Indigenous Peoples (UNDRIP), Article 19 states: \n\n\" \"States shall consult and cooperate in good faith with the Indigenous Peoples concerned through their own representative institutions in order to obtain their free, prior and informed consent before adopting and implementing legislative or administrative measures that may affect them. ”\"\nArticle 32 of UNDRIP requires that consultation is carried out with Indigenous Peoples before states can undertake projects that will affect Indigenous People Rights to land, territory and resources The above Articles only requires Consultation, however Article 10 requires that there is informed consent before the relocation Indigenous Peoples from their land. This allows Indigenous People the right to decide where they live and gives them the power stop any development which they disagree with.\nThe Universal Declaration of Human Rights (UDHR) is the most universally accepted standard of Human Rights. It does not directly mention FPIC but it does express the importance of Self- determination of all Peoples in Article 1. Article 7 declares that all are equal before the law, this means that one person has no more right to another in a nation, this is further endorsed by Article 17 which states that every person has the right to own property and that he shall not be arbitrary deprived of his property. The right for Self-determination is further protected in the International Covenant on Civil and Political Rights (ICCPR) and the International Covenant on Economic, Social and Cultural Rights (ICESCR), in Article 1 of both Documents. This includes economic self-determination, which for many IP is the control of their natural resources. \nThe ICCPR in Article 27 states that minorities shall not be denied access to their culture, in the Human Rights Committee (HRC) in General Comment 23, have found this meaning to include the right of Indigenous People to their land and resources. The HRC has interpreted this to mean that states have a positive duty to engage with IP prior to any development or granting of resource concession in IP lands.\nThe Committee on Economic, Social and Cultural Rights (CESCR), the supervisory body of the ICESCR, has even stated in their general comment No.23 that if Indigenous People's land has been taken without informed prior consent then they have the right to restitution or the return of their land or resources. This comes from their interpretation of Article 15 of the ICESCR . Article 15 protects Indigenous People's right to participate in their cultural life. The Comment by the CESCR is important as it goes beyond mere consultation. The need for FPIC has also been called upon by the Committee on the Elimination of Racial Discrimination (CERD) which requires that no state shall make a decision concerning the rights of IPs without their consent. The Convention on the Elimination of All Forms of Racial Discrimination (CERD) encourages Indigenous Peoples participation in decision making. However these are not legally binding decisions, only recommendations \n\nThe World Bank was one of the first multilateral financial institutions to create guidelines to protect the rights of Indigenous People's in the 1980s, when it recognized that development negatively impacted the lives and cultures of Indigenous People. Its first policy was in 1987. It was designed by Bank staff without consultation of Indigenous People and was a statement on the need to protect Indigenous People's. Then in 1991 the Bank's Operational Directive 4.20 document acknowledged the need for participation of Indigenous People in the consultation process.\n\nThe subsequent World Bank Policy on indigenous peoples was released in 2005, OP 4.10 focused on the reduction of poverty. In doing so the bank identified the intrinsic link that Indigenous People have with the land and the need for a consultation process which fully respects the human rights, human dignity, economics and culture of the people involved. The Bank stated that it will not lend money to a state or company unless there has been Free Prior Informed Consultation with the local Indigenous Population and that there is broad community support for the development.\nCritics have questioned why the term \"consultation\" has been used as opposed to consent. Stating that it means IP are not able to decline a project if they do not agree with it. Furthermore, \"community\" is an ambiguous term.\nIn August 2016 the World Bank adopted its new Environmental and Social Standards, including Environmental and Social Standard 7 (ESS7) on Indigenous Peoples/Sub-Saharan African Historically Underserved Traditional Local Communities (IPs), which requires Free Prior Informed Consent if the project will:\n\nDuring the UNFCCC climate change negotiations on reducing emissions from deforestation and forest degradation (REDD+), it was noted that the United Nations General Assembly had adopted UNDRIP, meaning that the Declaration and its FPIC provision applied to the negotiations. This reference was made in the context of a so-called safeguard for REDD+, specifically the instruction to have \"respect for the knowledge and rights of indigenous peoples and members of local communities\" when undertaking REDD+ activities.\n\nFollowing this, FPIC has been widely applied for demonstration projects on REDD+, particularly after the United Nations REDD Programme published a report on its efforts to develop a methodology for FPIC for REDD+ in the case of its country program in Vietnam. Early in 2013, the global United Nations REDD Programme issued guidelines for the application of FPIC, including an analysis of jurisprudence on FPIC in various contexts, that are mandatory for all UN-REDD country programmes.\n\nBolivia ratified ILO Convention 169 and in 2007 it also formally incorporated UNDRIP into its municipal law. In 2009 the nation also included the duty to consult Indigenous People's in its constitution, this is however a much less radical version of the draft, which required that consent be given for the exploration of all resource activities. These legal requirements are very significant in a nation that has a wealth of natural resources and a large Indigenous Population. The risk of giving Indigenous People this type of power to veto government projects is an increase social conflict in certain regions. This was seen with the conflict surrounding the Isiboro Se´cure National Park and Indigenous Territory (TIPNIS). A road was planned through the Park, coca growers were in favour of the project as it would expand their business. The Indigenous Population however opposed the idea, saying that consent should be needed for mega development in Indigenous Peoples territories. The result was large protests in La Paz. The fear was of damage to the vital river system, illegal logging and altering habitats of endangered animals in the area. The state engaged in consultation with the Indigenous Peoples, but this amplified the problem, with activists critiquing the governments lack of legal framework to protect Indigenous People. The government claimed that the expectations of the Indigenous People were unrealistic.\n\n"}
{"id": "11507473", "url": "https://en.wikipedia.org/wiki?curid=11507473", "title": "Global Peace Index", "text": "Global Peace Index\n\nGlobal Peace Index (GPI) measures the relative position of nations' and regions' peacefulness. The GPI ranks 163 independent states and territories according to their levels of peacefulness.\n\nThe GPI is a report produced by the Institute for Economics and Peace (IEP) and developed in consultation with an international panel of peace experts from peace institutes and think tanks with data collected and collated by the Economist Intelligence Unit. The Index was first launched in May 2007, with subsequent reports being released annually. It is claimed to be the first study to rank countries around the world according to their peacefulness. In 2017 it ranked 163 countries, up from 121 in 2007. In the past decade, the GPI has presented trends of increased global violence and less peacefulness. The study is the brainchild of Australian technology entrepreneur Steve Killelea, founder of Integrated Research, and is endorsed by individuals such as former UN Secretary-General Kofi Annan, the Dalai Lama, archbishop Desmond Tutu, former President of Finland and 2008 Nobel Peace Prize laureate Martti Ahtisaari, Nobel laureate Muhammad Yunus, economist Jeffrey Sachs, former president of Ireland Mary Robinson, former Deputy Secretary-General of the United Nations Jan Eliasson and former United States president Jimmy Carter. The updated index is released each year at events in London, Washington, DC; and at the United Nations Secretariat in New York between many others.\n\nThe GPI gauges global peace using three broad themes: the level of societal safety and security, the extent of ongoing domestic and international conflict and the degree of militarization. Factors are both internal such as levels of violence and crime within the country and external such as military expenditure and wars. It has been criticised by Riane Eisler for not including indicators specifically relating to violence against women and children, however reliable international data on these subjects is either unavailable or very sparsely reported in many countries.\n\nThe 2017 GPI indicates Iceland, New Zealand, Portugal, Austria, and Denmark to be the most peaceful countries and Syria, Afghanistan, Iraq, South Sudan, and Yemen to be the least peaceful. Long-term findings of the 2017 GPI include a less peaceful world over the past decade, a 2.14 per cent deterioration in the global level of peace in the past decade, growing inequality in peace between the most and least peaceful countries, a long-term reduction in the GPI Militarization domain, and a widening impact of terrorism, with historically high numbers of people killed in terrorist incidents over the past 5 years.\n\nThe expert panel for the 2016 and 2017 GPI consisted of:\n\n\nIn assessing peacefulness, the GPI investigates the extent to which countries are involved in ongoing domestic and international conflicts. It also seeks to evaluate the level of harmony or discord within a nation; ten indicators broadly assess what might be described as a safety and security in society. The assertion is that low crime rates, minimal incidences of terrorist acts and violent demonstrations, harmonious relations with neighboring countries, a stable political scene and a small proportion of the population being internally displaced or refugees can be suggestive of peacefulness.\n\nIn 2017, 23 indicators were used to establish each country's peacefulness score. The indicators were originally selected with the assistance of an expert panel in 2007 and are reviewed by the expert panel on an annual basis. The scores for each indicator are normalized on a scale of 1-5, whereby qualitative indicators are banded into five groupings and quantitative ones are scored from 1-5, to the third decimal point. A table of the indicators is below. In the table, UCDP stands for the Uppsala Conflict Data Program maintained by the University of Uppsala in Sweden, EIU for The Economist Intelligence Unit, UNSCT for the United Nations Survey of Criminal Trends and Operations of Criminal Justice Systems, ICPS is the International Center for Prison Studies at King's College London, IISS for the International Institute for Strategic Studies publication \"The Military Balance\", and SIPRI for the Stockholm International Peace Research Institute Arms Transfers Database.\n\nIndicators not already ranked on a 1 to 5 scale were converted by using the following formula: x=(x-Min(x))/(Max(x)-Min(x)) where Max(x) and Min(x) are the highest and lowest values for that indicator of the countries ranked in the index. The 0 to 1 scores that resulted were then converted to the 1 to 5 scale. Individual indicators were then weighted according to the expert panel's judgment of their importance. The scores were then tabulated into two weighted sub-indices: internal peace, weighted at 60% of a country's final score, and external peace, weighted at 40% of a country's final score. ‘Negative Peace’ which is defined as the absence of violence, or fear of violence is used as the definition of peace to create the Global Peace Index. An additional aim of the GPI database is to facilitate deeper study of the concept of positive peace, or those attitudes, institutions, and structures that drive peacefulness in society. The GPI also examines relationships between peace and reliable international measures, including democracy and transparency, education and material well-being. As such, it seeks to understand the relative importance of a range of potential determinants, or \"drivers\", which may influence the nurturing of peaceful societies, both internally and externally.\n\nThe main findings of the 2017 Global Peace Index are:\n\nStatistical analysis is applied to GPI data to uncover specific conditions conducive of peace. Researchers have determined that Positive Peace, which includes the attitudes, institutions, and structures that pre-empt conflict and facilitate functional societies, is the main driver of peace. The eight pillars of positive peace are well-functioning government, sound business environment, acceptance of the rights of others, good relations with neighbors, free flow of information, high levels of human capital, low levels of corruption, and equitable distribution of resources. Well-functioning government, low levels of corruption, acceptance of the rights of others and good relations with neighbours are more important in countries suffering from high levels of violence. Free flow of information and sound business environment become more important when a country is approaching the global average level of peacefulness, also described as the Mid-Peace level. Low levels of corruption is the only Pillar that is strongly significant across all three levels of peacefulness. This suggests it is an important transformational factor at all stages of nations’ development.\n\nThe Index has received endorsements as a political project from a number of major international figures, including the former Secretary-General of the United Nations Kofi Annan, former President of Finland and 2008 Nobel Peace Prize laureate Martti Ahtisaari, the Dalai Lama, archbishop Desmond Tutu, Muhammad Yunus, and former United States President Jimmy Carter Steve Killelea A.M., the Australian philanthropist who conceived the idea of the Index, argues that the Index \"is a wake-up call for leaders around the globe.\n\nThe Index has been widely recognized. Professor Jeffrey Sachs, Director of the Earth Institute at Columbia University said: \"The GPI continues its pioneering work in drawing the world’s attention to the massive resources we are squandering in violence and conflict. The lives and money wasted in wars, incarcerations, weapons systems, weapons trade, and more, could be directed to ending poverty, promoting education, and protecting the environment. The GPI will not only draw attention to these crucial issues, but help us understand them and to invest productively in a more peaceful world.”\n\nMarla Mossman of the Peace Alliance Leadership Council said she believes that the measurements of the Global Peace Index can be useful for crafting government policy, helping governments to identify problems and develop practical and relevant policies. Furthermore, she said that she saw the Index indicators as, “the measurements of the health of the nation. So we really can take the temperature of the world: are we healthy? Do we have a fever?”\n\nFollowing the release of the 2015 GPI, Professor Sir Lawrence Freedman of King's College in London called the Index, “an extraordinarily useful body of information,” and its analysis “the best indicator of future conflict is past conflict. The challenge is how we break that cycle.”\n\n\"The Economist\", in publishing the first edition of the index in 2007, admitted that, \"the index will run into some flak.\" Specifically, according to \"The Economist\", the weighting of military expenditure \"may seem to give heart to freeloaders: countries that enjoy peace precisely because others (often the USA) care for their defense.\" The true utility of the index may lie not in its specific rankings of countries now, but in how those rankings change over time, thus tracking when and how countries become more or less peaceful.\n\nIn 2012, the Economist suggested that “quantifying peace is a bit like trying to describe how happiness smells.” The publication admitted that the GPI has produced some “surprising results” and argued that “part of the appeal of the index is that readers can examine each of the variables in turn and think about how much weight to add to each.”\n\nThe Australian National University says that the GPI report “presents the latest and most comprehensive global data on trends in peace, violence and war” and “provides the world’s best analysis of the statistical factors associated with long-term peace as well as economic analysis on the macroeconomic impacts of everyday violence and war on the global economy.”\n\nThe GPI has been criticised for not including indicators specifically relating to violence against women and children. Back in 2007, Riane Eisler, writing in the \"Christian Science Monitor\", argued that, \"to put it mildly, this blind spot makes the index very inaccurate. She mentions a number of specific cases, including Egypt, where she claims 90% of women are subject to genital mutilation, and China, where, she says, \"female infanticide is still a problem,\" according to a 2000 UNICEF study.\n\nSenior Analyst at the Institute for Near East and Gulf Military Analysis, Sabhat Khan, argued that the Index should “involve more context on security environments.” Referring to the UAE's GPI ranking in particular, Khan argued that “the measurement usually used by such rankings is crude data without contextualising them;” for example, the UAE must bolster its security apparatus to respond to turbulence in neighbouring countries such as Iraq, Syria, and Yemen, which all rank at the very bottom of the GPI.\n\nDuring a Peace Forum in August 2017, Honduran President Juan Orlando Hernandez said that “receiving such high praise from an institute that once named this country the most violent in the world is extremely significant…My administration will keep fighting to protect all Honduran citizens.” The President has recently launched an initiative to build a series of safe parks across Honduras and hopes to see further improvement reflected in future GPI rankings.\n\nMalaysia ranked 29th in the 2017 GPI. The country's Communications and Multimedia Minister, Datuk Seri Salleh Said Keruak said that this ranking along with Malaysia's high place in the 2017 World Happiness Report was proof that the “government’s efforts have made Malaysia a safe and prosperous country.” He also admitted, “there’s still much room for improvement to make Malaysia the best among the better countries and that’s what we’re doing now.”\n\nAfter the release of the 2016 GPI, the Botswanan Office of the President released a proud statement, “in this year’s Index, Botswana was ranked as 28 out of 163 countries, up 3 places from last year. This continues to place Botswana above over half of the European region countries surveyed as well as all five of the Permanent Members of the United Nations Security Council…in addition Botswana was one of only five countries, to achieve a perfect score in the domestic and international conflict domain.”\n\nNavid Hanif, Director of the United Nations Office for ECOSOC Support and Coordination said, “it’s intuitive that peace is useful and peacefullness is a reward in itself, but the IEP is trying to make the conclusion more evidence-based. Now that the index covers 99% of the population, it has come a long way. The report systemically measures peacefulness and identifies the determinants of peace.”\n\nReacting to the 2017 results of the GPI, which ranked the Philippines 138 out of 163 countries, mainly because of poor scores in societal safety and security due to President Duterte's war on drugs, Philippine Presidential spokesman Ernesto Abella countered, “We’re not entirely sure where the GPI, Global Peace Index analyst… who apparently is supposed to be a local, is really coming from. Maybe there’s a political slant somewhere…based on survey results, the net satisfaction of Filipino people is quite high.”\n\nSierra Leone ranked 39th in the 2017 Global Peace Index. Former Chief of Staff and Office of National Security (ONS) advisor, Dr. Jonathan PJ Sandy, “Welcomed the 2017 Global Peace Index report released recently which ranked Sierra Leone in first position, as the most peaceful country in West Africa and third in the African continent…He observed that going by the report itself, [future] elections might be successfully held.” Presidential Spokesman, Abdulai Bayraytay “said the favourable Global Peace rating of Sierra Leone would serve as an impetus for the country to do more.”\n\nThe Independent: \"Global Peace Index: US Facing New Era of Instability as Middle East Sinks Further into Turmoil\": “An annual global peace index has concluded that US political turmoil had pushed North America into deep instability in 2016 while the Middle East sank deeper into turmoil. Despite depicting tumult across continents, the 2017 Global Peace Index said the world had overall become more peaceful in the past year when measured against a range of indicators.”\n\nBBC: \"Global Peace Index 2017: World 0.28% more peaceful than last year:\" “Levels of peace around the world have improved slightly for the first time since the Syrian war began, but harmony has decreased in the US and terrorism records have increased, a Sydney-based think-tank has found.”\n\nForbes: “The Global Peace Index, which the Institute compiles annually, paints a sombre picture: The world has become even less peaceful in 2016, continuing a decade-long trend of increased violence and strife. Published every year since 2008, the Index ranks 163 independent states and territories by their level of peacefulness.”\n\nForbes: \"The World's Most and Least Peaceful Countries [Infographic]:\" “The 2017 Global Peace Index has found that the world has become a slightly safer place over the past year. However, the political fallout and deep rooted division brought on by the US presidential election campaign has led to a deterioration of peace levels in North America.”\n\nThe Guardian: \"Fraught White House Campaign Blamed as US Bucks Global Trend Towards Peace:\" “The divisive nature of Donald Trump’s rise to the White House has increased mistrust of the US government and means social problems are likely to become more entrenched, said the authors of the annual Global Peace Index, in which 163 countries and territories are analysed.”\n\nHuffingtonPost: \"Global Peace Index 2017: Donald Trump Fallout Causes North America To Plummet Down Ranking\": “While the world became a safer place to live overall, the 2017 Global Peace Index found disruption caused by the perception of corruption and attacks on media in the US led to its deterioration.”\n\nThe Washington Times: \"U.S. Ranked the 114th Most Peaceful Nation on Earth says Annual Global Ranking:\" “The index is produced by the Australia-based Institute for Economics and Peace, which figures that the impact of strife worldwide is $14.3 trillion. News is not all bad, though. In a nutshell, the index found that 93 nations became ‘more peaceful’ in the last year, 68 were ‘less peaceful.’”\n\nBusiness Insider: \"The 12 Safest Countries in the World: “\"The think tank Institute for Economics and Peace recently published the Global Peace Index 2017, which reveals the safest — as well as the most dangerous — countries in the world. The report ranked 163 countries based on how peaceful they are. The rankings were determined by 23 factors, which included homicide rate, political terror, and deaths from internal conflict.”\n\nSputnik International: \"Terrorism, Conflicts Cost Over $14 Trillion to Global Economy\": “According to the latest estimation by the Global Peace Index that annually analyses the costs of security of living in countries and regions, worldwide terrorism is at an all-time high.”\n\nIndian news websites, ZeeNews, HindustanTimes, and Jagran Josh: The three Indian news agencies described the GPI’s ranking system, global peace trends, highlights from that year’s GPI and India’s own placement in the GPI. The Hindustan Times quoted the GPI and emphasized that “violence impacted India’s economy by USD 679.80 billion in 2016, 9 % of India’s GDP, or USD 525 per person”.,\n\nPhilstar, Filipino newspaper: “Among all the 163 countries, the Philippines is ranked 138. For perspective, India is ranked just one notch above, at 137. Despite this low ranking, however, it has remained relatively stable in this low rank over time a long time. Though the raw score has worsened over the previous year, the country’s rank has not been far off from this rank in previous years…Though the point of view of the report deserves respect concerning societal safety, another side of the story needs more hearing internationally.”\n\nWorld Economic Forum: \"These are the Most Peaceful Countries in the World: “\"The Global Peace Index ranks 163 countries according to their domestic and international conflicts, safety and security and degree of militarization. It found 93 had improved, while 68 had deteriorated, and overall peace levels had inched up 0.28%.”\n\nThe International Journal of Press/Politics: \"“\"Social Media and the Arab Spring: Politics comes first”: This article utilized the findings of the 2010 GPI to construct a human rights index, which was used in their overall study on the use of social media in political uprisings, and in the Arab Spring context in particular.\n\nJournal of Economics and Sustainable Development: “Security Challenges in Nigeria and the Implications for Business Activities and Sustainable Development”: The study utilizes GPI scores from 2009-2012 to examine implications for Nigeria's business environment and overall progress in national security.\n\nContemporary Security Policy: “Failed states and international order: constructing a post-Westphalian world”: The Global Peace Index, along with four other global indices, is used in this study's ranking of ‘state failure’. “Although this index focuses primarily on trends of armed conflict and violence it is relevant to state weakness and failure as the indicators measured for the assessment of ‘peace’ in this context are also indicative of state capacity.”\n\nBiological Reviews: “Does Infectious Disease cause Global Variation in the Frequency of Intrastate Armed Conflict and Civil War?”: This study used the 2008 Global Peace Index to build what they call a ‘path analysis,’ in which they sought to uncover “whether infectious disease causes the emergence of a collectivist culture.”\n\nPolitical Research Quarterly: “Measuring the Rule of Law”: This article attempts to measure the rule of law, and in doing so “correlated the rule of law indices with a measure of violent crime (for 2007) included in the Global Peace Index.”\n\nApplied Energy: “The Analysis of Security Cost for Different Energy Sources”: This study utilized the Global Peace Index in calculating a disruption probability from geopolitical instability, with the overall aim of analysing security costs for different sources of energy.\n\nInternational Political Science Review: “Measuring Effective Democracy: A Defence”: In the construction of an effective democracy index (EDI), the authors built a table that includes 2008 GPI scores as a dependent variable in a regression analysis of economic development and various indices of democracy.\n\nInstitute for Security Studies: “African Futures 2050- The Next Forty Years”: The African human security research institution utilized the findings of the Global Peace Index of 2010 to emphasize trends in drug crime and violence on the African continent.\n\nNature Communications: “Global Priorities for an Effective Information Basis of Biodiversity Distributions”: In their article about insufficient digital accessible information about ecosystems and biodiversity, the authors utilized the GPI to model the “effects of secure conditions” based on the index as a measure of political stability, armed conflict, and public safety levels.\n\nNordic Journal of Religion and Society: “Why are Danes and Swedes so Irreligious”: This article uses the Global Peace Index, and its very high rankings of Denmark (3rd in 2008) and Sweden (13th in 2008) to support claims that the countries’ lack of religiosity can be linked to prosperous societal structures.\n\nFood Security: “Tracking phosphorus security: indicators of phosphorus vulnerability in the global food system”: Along with eleven other indicators, the GPI was used as a measure of political instability for the development of a utilized in the development of a phosphorus vulnerability analysis, aimed at formulating food production methods and government policy.\n\nWorld Politics: “The System Worked: Global Economic Governance During the Great Recession”: Drezner uses GPI measurements, particularly the fact that interstate violence and military expenditures have decreased in the years studied, to bolster an argument suggesting that the Great Recession has not led to an increase in global violence and conflict.\n\nJournal of Sustainable Development Studies: “Insecurity and Socio-economic Development in Nigeria”: This sustainable development study utilized the GPI, in conjunction with the Human Development Index and the Corruption Perception Index to track fluctuations in Nigeria's socio-economic climate and insecurity issues over the past decade.\n\nHarvard Educational Review: “Peace Education in a Violent Culture”: In criticizing the United States’ culture of violence, the author refers to the developed country's remarkably low ranking on the Global Peace Index as evidence of violence's impact on societal peacefulness.\n\nInternational Security: “The Heart of the Matter: The Security of Women and the Security of States”: In this piece, the authors use the Global Peace Index as one of three measures of state security; the GPI is specifically used as a “general measure of state peacefulness.” The report concludes that higher levels of women's physical security correlates positively with the GPI.\n\nThe Equal Rights Review: “The Mental Health Gap in South Africa: A Human Rights Issue”: South Africa's poor GPI ranking, among other measures is cited by the authors as part of their overall argument that the national government is not implementing promises made towards the achievement of equality, as signatories of the United Nations Convention on the Rights of Persons with Disabilities (CRPD).\n\nEnvironment, Development and Sustainability: “Creating a ‘Values’ Chain for Sustainable Development in Developing Nations: Where Maslow meets Porter”: This study uses the ‘safety and security’ measures of the GPI, including political instability, level of violent crime, and likelihood of violent demonstrations, for supporting an argument that renders societal safety and security necessary for sustainable development.\n\nNations considered more peaceful have lower index scores. In 2013 researchers at the Institute for Economics and Peace harmonized the Global Peace Index database to ensure that the scores were comparable over time. The GPI Expert Panel decided that the Index would include nations and territories, but not micro-states. Countries covered by the GPI must either have a population of more than 1 million or a land area greater than 20,000 square kilometers.\nNote: The GPI's methodology is updated regularly and is improved to reflect the most up-to-date datasets. Each year's GPI report includes a detailed description of the methodology used.\n\n\n"}
{"id": "35991194", "url": "https://en.wikipedia.org/wiki?curid=35991194", "title": "Innocent prisoner's dilemma", "text": "Innocent prisoner's dilemma\n\nThe innocent prisoner's dilemma, or parole deal, is a detrimental effect of a legal system in which admission of guilt can result in reduced sentences or early parole. When an innocent person is wrongly convicted of a crime, legal systems which need the individual to admit guilt, for example as a prerequisite step leading to parole, punish an innocent person for his integrity, and reward a person lacking in integrity. There have been many cases where innocent prisoners were given the choice between freedom, in exchange for claiming guilt, and remaining imprisoned and telling the truth. Individuals have died in prison rather than admit to crimes which they did not commit.\n\nIt has been demonstrated in Britain that prisoners who freely admit their guilt are more likely to re-offend than prisoners who maintain their innocence. Other research, however, has found no clear link between denial of guilt and recidivism.\n\nUnited States law professor Daniel Medwed says convicts who go before a parole board maintaining their innocence are caught in a Catch-22 which he calls \"the innocent prisoner’s dilemma\". A false admission of guilt and remorse by an innocent person at a parole hearing may prevent a later investigation proving their innocence.\n\nMichael Naughton, founder of the Innocence Network UK (INUK), says work carried out by the INUK includes research and public awareness on wrongful convictions, which can effect policy reforms. Most important is the development of a system to assess prisoners maintaining innocence, to distinguish potentially innocent prisoners from the prisoners who claim innocence for other reasons like \"ignorance, misunderstanding or disagreement with criminal law; to protect another person or group from criminal conviction; or on 'abuse of process' or technical grounds in the hope of achieving an appeal.\" The system, he says, is being adopted by the prison parole board and prison service, for prisoners serving \"indeterminate sentences (where the prisoner has no release date and does not get out until a parole board decides he or she is no longer a risk to the public). Previously, such prisoners were treated as 'deniers' with no account taken of the various reasons for maintaining innocence, nor the fact that some may actually be innocent.\" Those prisoners are unable to achieve parole unless they undertake offence-behaviour courses that require the admission of guilt as a prerequisite. This was represented in the \"Porridge\" episode \"Pardon Me\". However, in recent years, this has diminished in significance; at the time Simon Hall ended his denials to murder in 2012, the Ministry of Justice denied that this would have any impact on his tariff, and his last online posting had concerned being released from prison in spite of his denials.\n\nThe murder of Linda Cook was committed in Portsmouth on 9 December 1986. The subsequent trial led to a miscarriage of justice when Michael Shirley, an 18-year-old Royal Navy sailor, was wrongly convicted of the crime and sentenced to life imprisonment. After serving the minimum 15 years, Shirley would have been released from prison had he confessed the killing to the parole board, but he refused to do so and said: \"I would have died in prison rather than admit something I didn't do. I was prepared to stay in forever if necessary to prove my innocence.\" (Shirley's conviction was eventually quashed by the Court of Appeal in 2003, on the basis of exculpatory DNA evidence.)\n\nThe Stephen Downing case involved the conviction and imprisonment in 1974 of a 17-year-old council worker, Stephen Downing, for the murder of a 32-year-old legal secretary, Wendy Sewell. His conviction was overturned in 2002, after Downing had served 27 years in prison. The case is thought to be the longest miscarriage of justice in British legal history, and attracted worldwide media attention. The case was featured in the 2004 BBC drama \"In Denial of Murder\" Downing claimed that had he falsely confessed he would have been released over a decade earlier. Because he did not admit to the crime he was classified as \"IDOM\" (In Denial of Murder) and ineligible for parole under English Law.\n\nIn the United States the reality of a person being innocent, called \"actual innocence\", is not sufficient reason for the justice system to release a prisoner. Once a verdict has been made, it is rare for a court to reconsider evidence of innocence which could have been presented at the time of the original trial. Decisions by the State Board of Pardons and Paroles regarding its treatment of prisoners who may be actually innocent have been criticized by the international community.\n\nHerbert Murray, who was convicted of murder in 1979, said, \"When the judge asked me did I have anything to say, I couldn't say, because tears were coming down and I couldn’t communicate. I couldn't turn around and tell the family that they got the wrong man.\" The judge said he believed the defense’s alibi witnesses; however, the judge was required by law to respect the jury’s decision. After being locked up for 19 years, his parole officer said \"Nineteen years is a long time. [...] But you’re no closer to the rehabilitative process than when you first walked into prison. The first step in that process is the internalization of guilt. You need to do some serious introspection, Mr. Murray, and come to grips with your behavior.\" Murray agreed with the parole officer, but maintained his innocence: \"I agree! But again, I just didn't do it.\"\n\nIn a news interview, Murray says he went before a parole board four times, maintaining his innocence until the fifth: \"I said what the hell, let me tell these people what they want to hear.\" He admitted to the parole board that he committed the crime and was taking responsibility. \"I felt like I sold my soul to the devil. Because before, I had that strength, because I stood on the truth. [...] I became so desperate to get out, I had to say something. I had to say something because what I said before didn't work.\" His parole was denied. After 29 years in prison, Medwed's Second Look clinic, a group dedicated to the release of innocent prisoners, assisted lawyers in his eighth parole board hearing which was successful, releasing him onto indefinite parole. Overturning the original conviction would be hampered by his admissions of guilt at his parole hearings.\n\nTimothy Brian Cole (1960–99) was an African American military veteran and a student wrongly convicted of raping a fellow student in 1985. Cole was convicted by a jury of rape, primarily based on the testimony of the victim, Michele Mallin. He was sentenced to 25 years in prison. While incarcerated, Cole was offered parole if he would admit guilt, but he refused. \"His greatest wish was to be exonerated and completely vindicated\", his mother stated in a press interview. Cole died after serving 14 years in prison.\n\nAnother man, Jerry Wayne Johnson, confessed to the rape in 1995. Further, Mallin later admitted that she was mistaken as to the identity of her attacker. She stated that investigators botched the gathering of evidence and withheld information from her, causing her to believe that Cole was the perpetrator. Mallin told police that the rapist smoked during the rape. However, Cole never smoked because of his severe asthma. DNA evidence later showed him to be innocent. Cole died in prison on December 2, 1999; ten years later, a district court judge announced \"to a 100 percent moral, factual and legal certainty\" that Timothy Cole did not commit the rape. He was posthumously pardoned.\n\nThe dilemma can occur even before conviction. Kalief Browder was arrested in May 2010 for allegedly stealing a backpack. He spent the next three years on Rikers Island awaiting trial, much of it in solitary. During court appearances, prosecutors routinely asked for a short delay which would turn into a much lengthier wait. At times, Browder was offered plea bargains, and at one point, he was encouraged to plead guilty to misdemeanors, for which he would be sentenced to time already served and released. When he refused the plea deal, insisting on his innocence, the judge noted \"If you go to trial and lose, you could get up to fifteen [years].\" Eventually, in May 2013, the case was dismissed because prosecutors had lost contact with the only witness they had to the alleged crime.\n\nGabe Tan reported a British conference in 2011, \"the dilemma of maintaining innocence\", concluded \"Denial is not a valid measure of risk. In fact, research has shown that prisoners who openly admit to their crimes have the highest risk of re-offending.\"\n\nIn 2011, Michael Naughton suggested the focus on new evidence by the Criminal Cases Review Commission, rather than an examination of serious problems with evidence at original trials, meant in many cases “that the dangerous criminals who committed these crimes remain at liberty with the potential to commit further serious crimes.”\n\nRobert A. Forde cited two studies at the conference. One, a ten-year study of 180 sex offenders by Harkins, Beech and Goodwill found prisoners who claimed to be innocent were the least likely to be re-convicted, and that those who 'admitted everything', claiming to be guilty, were most likely to re-offend. He also told the conference research by Hanson \"et al.\" in 2002, the denial by the prisoner of their offences had no bearing on their likelihood of re-offending.\n\n"}
{"id": "219877", "url": "https://en.wikipedia.org/wiki?curid=219877", "title": "International human rights law", "text": "International human rights law\n\nInternational human rights law (IHRL) is the body of international law designed to promote human rights on social, regional, and domestic levels. As a form of international law, international human rights law are primarily made up of treaties, agreements between sovereign states intended to have binding legal effect between the parties that have agreed to them; and customary international law. Other international human rights instruments, while not legally binding, contribute to the implementation, understanding and development of international human rights law and have been recognized as a source of \"political\" obligation.\n\nThe relationship between international human rights law and international humanitarian law is disputed among international law scholars. This discussion forms part of a larger discussion on fragmentation of international law. While pluralist scholars conceive international human rights law as being distinct from international humanitarian law, proponents of the constitutionalist approach regard the latter as a subset of the former. In a nutshell, those who favors separate, self-contained regimes emphasize the differences in applicability; international humanitarian law applies only during armed conflict. \n\nOn the other hand, a more systemic perspective explains that international humanitarian law represents a function of international human rights law; it includes general norms that apply to everyone at all time as well as specialized norms which apply to certain situations such as armed conflict between both state and military occupation (i.e. IHL) or to certain groups of people including refugees (e.g. the 1951 Refugee Convention), children (the Convention on the Rights of the Child), and prisoners of war (the 1949 Third Geneva Convention).\n\nThe General Assembly of the United Nations adopted the Vienna Declaration and Programme of Action in 1993, in terms of which the United Nations High Commissioner for Human Rights was established.\n\nIn 2006, the United Nations Commission on Human Rights was replaced with the United Nations Human Rights Council for the enforcement of international human rights law.\n\nThe Universal Declaration of Human Rights (UDHR) is a UN General Assembly declaration that does not in form create binding international human rights law. Many legal scholars cite the UDHR as evidence of customary international law.\n\nMore broadly, the UDHR has become an authoritative human rights reference. It has provided the basis for subsequent international human rights instruments that form non-binding, but ultimately authoritative international human rights law.\n\nBesides the adoption in 1966 of the two wide-ranging Covenants that form part of the International Bill of Human Rights (namely the International Covenant on Civil and Political Rights and the International Covenant on Economic, Social and Cultural Rights), other treaties have been adopted at the international level. These are generally known as \"human rights instruments\". Some of the most significant include the following:\n\n\nRegional systems of international human rights law supplement and complement national and international human rights law by protecting and promoting human rights in specific areas of the world. There are three key regional human rights instruments which have established human rights law on a regional basis:\n\n\nThe Organisation of American States and the Council of Europe, like the UN, have adopted treaties (albeit with weaker implementation mechanisms) containing catalogues of economic, social and cultural rights, in addition to the aforementioned conventions dealing mostly with civil and political rights:\n\n\nThe African Union (AU) is a supranational union consisting of 53 African countries. Established in 2001, the AU's purpose is to help secure Africa's democracy, human rights, and a sustainable economy, in particular by bringing an end to intra-African conflict and creating an effective common market.\n\nThe African Charter on Human and Peoples' Rights is the region's principal human rights instrument. It emerged under the aegis of the Organisation of African Unity (OAU) (since replaced by the African Union). The intention to draw up the African Charter on Human and Peoples' Rights was announced in 1979. The Charter was unanimously approved at the OAU's 1981 Assembly.\n\nPursuant to Article 63 (whereby it was to \"come into force three months after the reception by the Secretary General of the instruments of ratification or adherence of a simple majority\" of the OAU's member states), the African Charter on Human and Peoples' Rights came into effect on 21 October 1986, in honour of which 21 October was declared African Human Rights Day.\n\nThe African Commission on Human and Peoples' Rights (ACHPR) is a quasi-judicial organ of the African Union, tasked with promoting and protecting human rights and collective (peoples') rights throughout the African continent, as well as with interpreting the African Charter on Human and Peoples' Rights, and considering individual complaints of violations of the Charter. The Commission has three broad areas of responsibility:\n\n\nIn pursuit of these goals, the Commission is mandated to \"collect documents, undertake studies and researches on African problems in the field of human and peoples' rights, organise seminars, symposia and conferences, disseminate information, encourage national and local institutions concerned with human and peoples' rights and, should the case arise, give its views or make recommendations to governments.\"\n\nWith the creation of the African Court on Human and Peoples' Rights (under a protocol to the Charter which was adopted in 1998 and entered into force in January 2004), the Commission will have the additional task of preparing cases for submission to the Court's jurisdiction. In a July 2004 decision, the AU Assembly resolved that the future Court on Human and Peoples' Rights would be integrated with the African Court of Justice.\n\nThe Court of Justice of the African Union is intended to be the \"principal judicial organ of the Union.\" Although it has not yet been established, it is intended to take over the duties of the African Commission on Human and Peoples' Rights, as well as to act as the supreme court of the African Union, interpreting all necessary laws and treaties. The Protocol establishing the African Court on Human and Peoples' Rights entered into force in January 2004, but its merging with the Court of Justice has delayed its establishment. The Protocol establishing the Court of Justice will come into force when ratified by fifteen countries.\n\nThere are many countries in Africa accused of human rights violations by the international community and NGOs.\n\nThe Organization of American States (OAS) is an international organization headquartered in Washington, DC. Its members are the thirty-five independent nation-states of the Americas.\n\nOver the course of the 1990s, with the end of the Cold War, the return to democracy in Latin America, and the thrust toward globalisation, the OAS made major efforts to reinvent itself to fit the new context. Its stated priorities now include the following:\n\n\nThe Inter-American Commission on Human Rights (IACHR) is an autonomous organ of the Organization of American States, also based in Washington, D.C. Along with the Inter-American Court of Human Rights, based in San José, Costa Rica, it is one of the bodies that comprise the inter-American system for the promotion and protection of human rights. The IACHR is a permanent body which meets in regular and special sessions several times a year to examine allegations of human rights violations in the hemisphere. Its human rights duties stem from three documents:\n\n\nThe Inter-American Court of Human Rights was established in 1979 with the purpose of enforcing and interpreting the provisions of the American Convention on Human Rights. Its two main functions are therefore adjudicatory and advisory:\n\n\nMany countries in the Americas, including Colombia, Cuba, Mexico and Venezuela, have been accused of human rights violations.\n\nThe Council of Europe, founded in 1949, is the oldest organisation working for European integration. It is an international organisation with legal personality recognised under public international law, and has observer status at the United Nations. The seat of the Council is in Strasbourg in France.\n\nThe Council of Europe is responsible for both the European Convention on Human Rights and the European Court of Human Rights. These institutions bind the Council's members to a code of human rights which, although strict, is more lenient than that of the UN Charter on human rights.\n\nThe Council also promotes the European Charter for Regional or Minority Languages and the European Social Charter. Membership is open to all European states which seek European integration, accept the principle of the rule of law, and are able and willing to guarantee democracy, fundamental human rights and freedoms.\n\nThe Council of Europe is separate from the European Union, but the latter is expected to accede to the European Convention on Human Rights. The Council includes all the member states of European Union. The EU also has a separate human rights document, the Charter of Fundamental Rights of the European Union.\n\nThe European Convention on Human Rights has since 1950 defined and guaranteed human rights and fundamental freedoms in Europe. All 47 member states of the Council of Europe have signed this Convention, and are therefore under the jurisdiction of the European Court of Human Rights in Strasbourg. In order to prevent torture and inhuman or degrading treatment, the Committee for the Prevention of Torture was established.\n\nThe Council of Europe also adopted the Convention on Action against Trafficking in Human Beings in May 2005, for protection against human trafficking and sexual exploitation, the Council of Europe Convention on the Protection of Children against Sexual Exploitation and Sexual Abuse in October 2007, and the Convention on preventing and combating violence against women and domestic violence in May 2011.\n\nThe European Court of Human Rights is the only international court with jurisdiction to deal with cases brought by individuals rather than states. In early 2010, the court had a backlog of over 120,000 cases and a multi-year waiting list. About one out of every twenty cases submitted to the court is considered admissible. In 2007, the court issued 1,503 verdicts. At the current rate of proceedings, it would take 46 years for the backlog to clear.\n\nThere is currently no international court to administer international human rights law, but quasi-judicial bodies exist under some UN treaties (like the Human Rights Committee under the ICCPR). The International Criminal Court (ICC) has jurisdiction over the crime of genocide, war crimes and crimes against humanity. The European Court of Human Rights and the Inter-American Court of Human Rights enforce regional human rights law.\n\nAlthough these same international bodies also hold jurisdiction over cases regarding international humanitarian law, it is crucial to recognise, as discussed above, that the two frameworks constitute different legal regimes.\n\nThe United Nations human rights bodies do have some quasi-legal enforcement mechanisms. These include the treaty bodies attached to the seven currently active treaties, and the United Nations Human Rights Council complaints procedures, with Universal Periodic Review and United Nations Special Rapporteur (known as the 1235 and 1503 mechanisms respectively).\n\nThe enforcement of international human rights law is the responsibility of the nation state; it is the primary responsibility of the State to make the human rights of its citizens a reality.\n\nIn practice, many human rights are difficult to enforce legally, due to the absence of consensus on the application of certain rights, the lack of relevant national legislation or of bodies empowered to take legal action to enforce them.\n\nIn over 110 countries, national human rights institutions (NHRIs) have been set up to protect, promote or monitor human rights with jurisdiction in a given country. Although not all NHRIs are compliant with the Paris Principles, the number and effect of these institutions is increasing.\n\nThe Paris Principles were defined at the first International Workshop on National Institutions for the Promotion and Protection of Human Rights in Paris from 7 to 9 October 1991, and adopted by UN Human Rights Commission Resolution 1992/54 of 1992 and General Assembly Resolution 48/134 of 1993. The Paris Principles list a number of responsibilities for national human rights institutions.\n\nUniversal jurisdiction is a controversial principle in international law, whereby states claim criminal jurisdiction over people whose alleged crimes were committed outside the boundaries of the prosecuting state, regardless of nationality, country of residence or any other relationship to the prosecuting country. The state backs its claim on the grounds that the crime committed is considered a crime against all, which any state is authorised to punish. The concept of universal jurisdiction is therefore closely linked to the idea that certain international norms are \"erga omnes\", or owed to the entire world community, as well as the concept of \"jus cogens\".\n\nIn 1993, Belgium passed a \"law of universal jurisdiction\" to give its courts jurisdiction over crimes against humanity in other countries. In 1998, Augusto Pinochet was arrested in London following an indictment by Spanish judge Baltasar Garzón under the universal-jurisdiction principle.\n\nThe principle is supported by Amnesty International and other human rights organisations, which believe that certain crimes pose a threat to the international community as a whole, and that the community has a moral duty to act.\n\nOthers, like Henry Kissinger, argue that \"widespread agreement that human rights violations and crimes against humanity must be prosecuted has hindered active consideration of the proper role of international courts. Universal jurisdiction risks creating universal tyranny—that of judges\".\n\n\n"}
{"id": "31515791", "url": "https://en.wikipedia.org/wiki?curid=31515791", "title": "Ispendje", "text": "Ispendje\n\nİspençe was a tax levied on non-Muslims in the Ottoman Empire.\n\nİspençe was a land-tax on non-Muslims in parts of the Ottoman Empire; its counterpart, for Muslim taxpayers, was the resm-i çift - which was set at slightly lower rate. The treasury was well aware of the difference in tax takes, and the incentive to convert; the legal reforms of Bayezid II halved some criminal penalties on nonmuslim taxpayers \"so that the taxpayers shall not vanish\"; this rule was reconfimed, a century later, in 1587. In other cases, local taxes were imposed on nonmuslims specifically to encourage conversion.\n\nİspençe had existed in the Balkans before the Ottoman conquest; the Ottoman Empire typically adapted local taxes and institutions in each conquered area, leading to a patchwork of different taxes and rates. The concept of İspençe, theoretically a payment in lieu of corvee labour, was derived from the Byzantine \"zeugaratikion\", a land tax based on the zeugarion - the area of farmland which could be ploughed by a pair of oxen. The zeugarion itself was taken up as the Ottoman \"çift\", a word meaning \"pair\".\n\nDespite taxes being set centrally, by the Porte, there was some local variance; around 1718. the kadı of Janjevo complained to Istanbul that the local lord set ispençe at 80 akçes per year rather than official rate of 32.\n\nAs with other Ottoman taxes, there were various exemptions and loopholes; royal hunters, who provided birds-of-prey to the court, had an exemption from ispençe (and other taxes); they could pass on their job, and the tax exemption, to their sons. There were also some exemptions for those unable to work their land through disability, although it was expected that the elderly would have children able to work and therefore to pay ispençe.\n\nThe tax was paid by adult male heads of households; in the Morea it increased to 25 akces (from 20) between 1480 and 1512, and stayed at a similar level after that, (for most), confirmed by the tahrir of 1583; but widows may have paid a reduced rate, and Jews may have paid 125 akces.\n"}
{"id": "689975", "url": "https://en.wikipedia.org/wiki?curid=689975", "title": "Laban movement analysis", "text": "Laban movement analysis\n\nLaban movement analysis (LMA), sometimes Laban/Bartenieff movement analysis, is a method and language for describing, visualizing, interpreting and documenting human movement. It is based on the original work of Rudolf Laban, which was developed and extended by Lisa Ullmann, Irmgard Bartenieff, Warren Lamb and others. LMA draws from multiple fields including anatomy, kinesiology and psychology. It is used by dancers, actors, musicians and athletes; by health professionals such as physical and occupational therapists and psychotherapists; and in anthropology, business consulting and leadership development.\n\nLabanotation (or Kinetography Laban), a notation system for recording and analyzing movement, is used in LMA, but Labanotation is a separate system.\n\nLaban movement analysis is generally divided into four categories:\n\nOther categories, that are occasionally mentioned in some literature, are relationship and phrasing. These are less well defined. Relationship is the interaction between people, body parts or a person and an object. Phrasing is defined as being the personal expression of a movement.\n\nThese categories are in turn occasionally divided into kinematic and non-kinematic categories to distinguish which categories relate to changes to body relations over time and space.\n\nThe body category describes structural and physical characteristics of the human body while moving. This category is responsible for describing which body parts are moving, which parts are connected, which parts are influenced by others, and general statements about body organization.\n\nSeveral subcategories of body are:\n\nEffort, or what Laban sometimes described as dynamics, is a system for understanding the more subtle characteristics about movement with respect to inner intention. The difference between punching someone in anger and reaching for a glass is slight in terms of body organization – both rely on extension of the arm. The attention to the strength of the movement, the control of the movement and the timing of the movement are very different.\n\nEffort has four subcategories (effort factors), each of which has two opposite polarities (Effort elements).\n\nLaban named the combination of the first three categories (Space, Weight, and Time) the Effort Actions, or Action Drive. The eight combinations are descriptively named Float, Punch (Thrust), Glide, Slash, Dab, Wring, Flick, and Press. The Action Efforts have been used extensively in some acting schools, including ALRA, Manchester School of Theatre, LIPA and London College of Music to train in the ability to change quickly between physical manifestations of emotion.\n\nFlow, on the other hand, is responsible for the continuousness or ongoingness of motions. Without any Flow Effort, movement must be contained in a single initiation and action, which is why there are specific names for the Flow-less Action configurations of Effort. In general it is very difficult to remove Flow from much movement, and so a full analysis of Effort will typically need to go beyond the Effort Actions.\n\nWhile the Body category primarily develops connections within the body and the body/space intent, the way the body changes shape during movement is further experienced and analyzed through the Shape category. It is important to remember that all categories are related, and Shape is often an integrating factor for combining the categories into meaningful movement.\n\nThere are several subcategories in Shape:\n\nOne of Laban's primary contributions to Laban Movement Analysis (LMA) are his theories of Space. This category involves motion in connection with the environment, and with spatial patterns, pathways, and lines of spatial tension. Laban described a complex system of geometry based on crystalline forms, Platonic solids, and the structure of the human body. He felt that there were ways of organizing and moving in space that were specifically harmonious, in the same sense as music can be harmonious. Some combinations and organizations were more theoretically and aesthetically pleasing. As with music, Space Harmony sometimes takes the form of set 'scales' of movement within geometric forms. These scales can be practiced in order to refine the range of movement and reveal individual movement preferences. The abstract and theoretical depth of this part of the system is often considered to be much greater than the rest of the system. In practical terms, there is much of the Space category that does not specifically contribute to the ideas of Space Harmony.\n\nThis category also describes and notates choices which refer specifically to space, paying attention to:\n\nThe Space category is currently under continuing development, more so since exploration of non-Euclidean geometry and physics has evolved.\n\nLMA is used in Human-Computer Interaction as a means of extracting useful features from a human's movement to be understood by a computer, as well as generating realistic movement animation for virtual agents and robots.\n\nLaban movement analysis practitioners and educators who studied at LIMS, an accredited institutional member of the National Association of Schools of Dance (NASD), are known as \"Certified Movement Analysts\" (CMAs).\n\nLaban/Bartenieff and Somatic Studies International™ (LSSI), is an approved training program of ISMETA, and offers Movement Analysis and Somatic Practice training, which qualifies “Certified Movement Analysts & Somatic Practitioners” (CMA-SPs).\n\nOther courses offer LMA studies, including Integrated Movement Studies, which qualifies \"Certified Laban/Bartenieff Movement Analysts\" (CLMAs).\n\n\n\n"}
{"id": "33350819", "url": "https://en.wikipedia.org/wiki?curid=33350819", "title": "List of ethnic cleansing campaigns", "text": "List of ethnic cleansing campaigns\n\nThis article lists incidents that have been termed ethnic cleansing by some academic or legal experts. Not all experts agree on every case, particularly since there are a variety of definitions for the term ethnic cleansing. Where claims of ethnic cleansing originate from non-experts (e.g., journalists or politicians) this is noted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3564042", "url": "https://en.wikipedia.org/wiki?curid=3564042", "title": "Madonna–whore complex", "text": "Madonna–whore complex\n\nIn psychoanalytic literature, a Madonna–whore complex is the inability to maintain sexual arousal within a committed, loving relationship. First identified by Sigmund Freud, under the rubric of \"psychic impotence\", this psychological complex is said to develop in men who see women as either saintly Madonnas or debased prostitutes. Men with this complex desire a sexual partner who has been degraded (the whore) while they cannot desire the respected partner (the Madonna). Freud wrote: \"Where such men love they have no desire and where they desire they cannot love.\" Clinical psychologist Uwe Hartmann, writing in 2009, stated that the complex \"is still highly prevalent in today's patients\".\n\nThe term is also used popularly, if sometimes with subtly different meanings.\n\nFreud argued that the Madonna–whore complex was caused by a split between the affectionate and the sexual currents in male desire. Oedipal and castration anxiety fears prohibit the affection felt for past incestuous objects from being attached to women who are sensually desired: \"The whole sphere of love in such persons remains divided in the two directions personified in art as sacred and profane (or animal) love\". In order to minimize anxiety, the man categorizes women into two groups: women he can admire and women he finds sexually attractive. Whereas the man loves women in the former category, he despises and devalues the latter group. Psychoanalyst Richard Tuch suggests that Freud offered at least one alternative explanation for the Madonna–whore complex: This earlier theory is based not on oedipal-based castration anxiety but on man's primary hatred of women, stimulated by the child's sense that he had been made to experience intolerable frustration and/or narcissistic injury at the hands of his mother. According to this theory, in adulthood the boy-turned-man seeks to avenge these mistreatments through sadistic attacks on women who are stand-ins for mother.\n\nIt is possible that such a split may be exacerbated when the sufferer is raised by a cold but overprotective mother – a lack of emotional nurturing paradoxically strengthening an incestuous tie. Such a man will often court someone with maternal qualities, hoping to fulfill a need for maternal intimacy unmet in childhood, only for a return of the repressed feelings surrounding the earlier relationship to prevent sexual satisfaction in the new.\n\nAnother theory claims that the Madonna–whore complex derives from the representations of women as either madonnas or whores in mythology and Judeo-Christian theology rather than developmental disabilities of individual men.\n\nNaomi Wolf considered that the sexual revolution had paradoxically intensified the importance of the virgin–whore split, leaving women to contend with the worst aspects of both images. Others consider that both men and women find integrating sensuality and an ideal femininity difficult to do within the same relationship.\n\n\n\n"}
{"id": "37576019", "url": "https://en.wikipedia.org/wiki?curid=37576019", "title": "Phaedra complex", "text": "Phaedra complex\n\nThe Phaedra complex is primarily the sexual desire of a stepmother for her stepson, though the term has been extended to cover difficult relationships between stepparents and stepchildren in general.\n\nThe complex takes its name from Greek mythology. \nPhaedra was the daughter of Minos and Pasiphaë, wife of Theseus, sister of Ariadne, and the mother of Demophon of Athens and Acamas. Though married to Theseus, Phaedra fell in love with Hippolytus, Theseus' son born by either Hippolyta, queen of the Amazons, or Antiope, her sister.\n\nWhen Hippolytus refused Phaedra's advances, she falsely accused him of propositioning her. Phaedra eventually killed herself in remorse after his subsequent death.\n\n\nFrench philosopher Georges Bataille used the same term in a very different sense to describe the morbid desire for a corpse.\n\n\n"}
{"id": "7295638", "url": "https://en.wikipedia.org/wiki?curid=7295638", "title": "Primary/secondary quality distinction", "text": "Primary/secondary quality distinction\n\nThe primary/secondary quality distinction is a conceptual distinction in epistemology and metaphysics, concerning the nature of reality. It is most explicitly articulated by John Locke in his \"Essay concerning Human Understanding\", but earlier thinkers such as Galileo and Descartes made similar distinctions.\n\nPrimary qualities are thought to be properties of objects that are independent of any observer, such as solidity, extension, motion, number and figure. These characteristics convey facts. They exist in the thing itself, can be determined with certainty, and do not rely on subjective judgments. For example, if an object is spherical, no one can reasonably argue that it is triangular.\n\nSecondary qualities are thought to be properties that produce sensations in observers, such as color, taste, smell, and sound. They can be described as the effect things have on certain people. Knowledge that comes from secondary qualities does not provide objective facts about things.\n\nPrimary qualities are measurable aspects of physical reality. Secondary qualities are subjective.\n\n\nLeibniz was an early critic of the distinction, writing in his 1686 \"Discourse on Metaphysics\" that \"[i]t is even possible to demonstrate that the ideas of size, figure and motion are not so distinctive as is imagined, and that they stand for something imaginary relative to our perceptions as do, although to a greater extent, the ideas of color, heat, and the other similar qualities in regard to which we may doubt whether they are actually to be found in the nature of the things outside of us.\"\n\nGeorge Berkeley wrote his famous critique of this distinction in his book Three Dialogues between Hylas and Philonous. Berkeley maintained that the ideas created by sensations are all that people can know for sure. As a result, what is perceived as real consists only of ideas in the mind. The crux of Berkeley's argument is that once an object is stripped of all its secondary qualities, it becomes very problematic to assign any acceptable meaning to the idea that \"there is\" some object. Not that one cannot picture to oneself (in one's mind) that some object could exist apart from any perceiver — one clearly can do this — but rather, that one cannot give any \"content\" to this idea. Suppose that someone says that a particular mind-independent object (meaning, an object free of all secondary qualities) exists at some particular spatio-temporal location (in Newtonian terms, in some particular place and at some particular time). Now, none of this particularly means anything if one cannot specify a place and time. In that case it's still a purely imaginary, empty idea. This is not generally thought to be a problem because realists imagine that they can, in fact, specify a place and time for a 'mind-independent' object. What is overlooked is that they can only specify a place and time in place and time \"as we experience them\". Berkeley did not doubt that one can do this, but that it is objective. One has simply related ideas to experiences (the idea of an \"object\" to our \"experiences of space and time\"). In this case there is no space and time, and therefore no objectivity. Space and time as we experience them are always piecemeal (even when the piece of space is big, as in some astronomical photos), it is only in imagination that they are total and all-encompassing, which is how we definitely imagine (!) 'real' space and time as being. This is why Berkeley argued that the materialist has merely an \"idea\" of an unperceived object: because people typically do take our imagining or picturing, as guaranteeing an objective reality to the 'existence' of 'something'. In no adequate way has it been specified nor given any acceptable meaning. As such Berkeley comes to his conclusion that having a compelling image in the mind, one which connects to no specifiable thing external to us, does not guarantee an objective existence.\n\nKant, in his \"Prolegomena to Any Future Metaphysics That Will Be Able to Present Itself as a Science\", claimed that primary, as well as secondary, qualities are subjective. They are both mere appearances that are located in the mind of a knowing observer. In § 13, Remark II, he wrote: \"Long before Locke's time, but assuredly since him, it has been generally assumed and granted without detriment to the actual existence of external things, that many of their predicates may be said to belong not to the things in themselves, but to their appearances, and to have no proper existence outside our representation. Heat, color, and taste, for instance, are of this kind. Now, if I go farther, and for weighty reasons rank as mere appearances the remaining qualities of bodies also, which are called primary, such as extension, place, and in general space, with all that which belongs to it (impenetrability or materiality, space, etc.)—no one in the least can adduce the reason of its being inadmissible.\"\n\n"}
{"id": "1166245", "url": "https://en.wikipedia.org/wiki?curid=1166245", "title": "Principle of distributivity", "text": "Principle of distributivity\n\nThe principle of distributivity states that the algebraic distributive law is valid for classical logic, where both logical conjunction and logical disjunction are distributive over each other so that for any propositions \"A\", \"B\" and \"C\" the equivalences\nand\nhold.\n\nThe principle of distributivity is valid in classical logic, but invalid in quantum logic.\n\nThe article \"Is Logic Empirical?\" discusses the case that quantum logic is the correct, empirical logic, on the grounds that the principle of distributivity is inconsistent with a reasonable interpretation of quantum phenomena.\n"}
{"id": "26853847", "url": "https://en.wikipedia.org/wiki?curid=26853847", "title": "Public Smog", "text": "Public Smog\n\nPublic Smog is an \"atmospheric park\" created by San Francisco-based artist Amy Balkin and her supporters through the use of financial, political, and legal methods. The goal of Public Smog is to \"highlight the complexities and contradictions of current environmental protocols.\".\n\nThe public smog atmospheric park consists of two areas which fluctuate in size and location. The upper park opened above the European Union in 2006 and the lower park is located over California’s South Coast Air Quality Management District’s Coastal Zone in 2004. Each was opened up through the purchasing of emissions offsets and then retiring the purchased air from use. Both parts of the park are currently closed.\n\nSome of the main methods used to create Public Smog are the purchase and withholding of emissions offsets and attempting to add the Earth's atmosphere to UNESCO's World Heritage List.\n\n"}
{"id": "18134289", "url": "https://en.wikipedia.org/wiki?curid=18134289", "title": "Qualitative comparative analysis", "text": "Qualitative comparative analysis\n\nIn statistics, qualitative comparative analysis (QCA) is a data analysis technique for determining which logical conclusions a data set supports. The analysis begins with listing and counting all the combinations of variables observed in the data set, followed by applying the rules of logical inference to determine which descriptive inferences or implications the data supports. The technique was originally developed by Charles Ragin in 1987.\n\nIn the case of categorical variables, QCA begins by listing and counting all types of cases which occur, where each type of case is defined by its unique combination of values of its independent and dependent variables. For instance, if there were four categorical variables of interest, {A,B,C,D}, and A and B were dichotomous (could take on two values), C could take on five values, and D could take on three, then there would be 60 possible types of observations determined by the possible combinations of variables, not all of which would necessarily occur in real life. By counting the number of observations that exist for each of the 60 unique combination of variables, QCA can determine which descriptive inferences or implications are empirically supported by a data set. Thus, the input to QCA is a data set of any size, from small-N to large-N, and the output of QCA is a set of descriptive inferences or implications the data supports.\n\nIn QCA's next step, inferential logic or Boolean algebra is used to simplify or reduce the number of inferences to the minimum set of inferences supported by the data. This reduced set of inferences is termed the \"prime implicates\" by QCA adherents. For instance, if the presence of conditions A and B is always associated with the presence of a particular value of D, regardless of the observed value of C, then the value that C takes is irrelevant. Thus, all five inferences involving A and B and any of the five values of C may be replaced by the single descriptive inference \"(A and B) implies the particular value of D\".\n\nTo establish that the prime implicants or descriptive inferences derived from the data by the QCA method are causal requires establishing the existence of causal mechanism using another method such as process tracing, formal logic, intervening variables, or established multidisciplinary knowledge. The method is used in social science and is based on the binary logic of Boolean algebra, and attempts to ensure that all possible combinations of variables that can be made across the cases under investigation are considered.\n\nThe technique of listing case types by potential variable combinations assists with case selection by making investigators aware of all possible case types that would need to be investigated, at a minimum, if they exist, in order to test a certain hypothesis or to derive new inferences from an existing data set. In situations where the available observations constitute the entire population of cases, this method alleviates the small N problem by allowing inferences to be drawn by evaluating and comparing the number of cases exhibiting each combination of variables. The small N problem arises when the number of units of analysis (e.g. countries) available is inherently limited. For example: a study where countries are the unit of analysis is limited in that are only a limited number of countries in the world (less than 200), less than necessary for some (probabilistic) statistical techniques. By maximizing the number of comparisons that can be made across the cases under investigation, causal inferences are according to Ragin possible. This technique allows the identification of multiple causal pathways and interaction effects that may not be detectable via statistical analysis that typically requires its data set to conform to one model. Thus, it is the first step to identifying subsets of a data set conforming to particular causal pathway based on the combinations of covariates prior to quantitative statistical analyses testing conformance to a model; and helps qualitative researchers to correctly limit the scope of claimed findings to the type of observations they analyze.\n\nAs this is a logical (deterministic) and not a statistical (probabilistic) technique, with \"crisp-set\" QCA (csQCA), the original application of QCA, variables can only have two values, which is problematic as the researcher has to determine the values of each variable. For example: GDP per capita has to be divided by the researcher in two categories (e.g. low = 0 and high = 1). But as this variable is essentially a continuous variable, the division will always be arbitrary. A second, related problem is that the technique does not allow an assessment of the effect of the relative strengths of the independent variables (as they can only have two values). Ragin, and other scholars such as Lasse Cronqvist, have tried to deal with these issues by developing new tools that extend QCA, such as multi-value QCA (mvQCA) and fuzzy set QCA (fsQCA). Note: Multi-value QCA is simply QCA applied to observations having categorical variables with more than two values. Crisp-Set QCA can be considered a special case of Multi-value QCA. \n\nStatistical methodologists have argued that QCA's strong assumptions render its findings both fragile and prone to type I error. Simon Hug argues that deterministic hypotheses and error-free measures are exceedingly rare in social science and uses Monte Carlo simulations to demonstrate the fragility of QCA results if either assumption is violated. Chris Krogslund, Donghyun Danny Choi, and Mathias Poertner further demonstrate that QCA results are highly sensitive to minor parametric and model-susceptibility changes and are vulnerable to type I error. Bear F. Braumoeller further explores the vulnerability of the QCA family of techniques to both type I error and multiple inference. Braumoeller also offers a formal test of the null hypothesis and demonstrates that even very convincing QCA findings may be the result of chance.\n\nQCA can be performed probabilistically or deterministically with observations of categorical variables. For instance, the existence of a descriptive inference or implication is supported deterministically by the absence of any counter-example cases to the inference; i.e. if a researcher claims condition X implies condition Y, then, deterministically, there must not exist any counterexample cases having condition X, but not condition Y. However, if the researcher wants to claim that condition X is a probabilistic 'predictor' of condition Y, in another similar set of cases, then the proportion of counterexample cases to an inference to the proportion of cases having that same combination of conditions can be set at a threshold value of for example 80% or higher. For each prime implicant that QCA outputs via its logical inference reduction process, the \"coverage\" — percentage out of all observations that exhibit that implication or inference — and the \"consistency\" — the percentage of observations conforming to that combination of variables having that particular value of the dependent variable or outcome — are calculated and reported, and can be used as indicators of the strength of such a explorative probabilistic inference. In real-life complex societal processes, QCA enables the identification of multiple sets of conditions that are consistently associated with a particular output value in order to explore for causal predictors.\n\nFuzzy set QCA aims to handle variables, such as GDP per capita, where the number of categories, decimal values of monetary units, becomes too large to use mvQCA, or in cases were uncertainty or ambiguity or measurement error in the classification of a case needs to be acknowledged.\n\nQCA has now become used in many more fields than political science which Ragin first developed the method for. Today the method has been used in:\n"}
{"id": "3513550", "url": "https://en.wikipedia.org/wiki?curid=3513550", "title": "Radical unintelligibility", "text": "Radical unintelligibility\n\nRadical Unintelligibility, a term coined by Bernard Lonergan, is the philosophical idea that we can act against our better judgment. We can refuse to choose what we know is worth choosing. It is the refusal to make a decision that one deems one ought to make. Mortal sin is radically unintelligible: when we commit a mortal sin, we fully consent to do something despite knowing that it is wrong to do it.\n\n"}
{"id": "7625694", "url": "https://en.wikipedia.org/wiki?curid=7625694", "title": "Scott Snibbe", "text": "Scott Snibbe\n\nScott Snibbe (born 1969 in New York City) is an interactive media artist, researcher, and entrepreneur. He is one of the first artists to work with projector-based interactivity, where a computer-controlled projection onto a wall or floor changes in response to people moving across its surface, with his well-known full-body interactive work \"Boundary Functions\" (1998), premiering at Ars Electronica 1998.\n\nIn this floor-projected interactive artwork, people walk across a four-meter by four-meter floor. As they move, \"Boundary Functions\" uses a camera, computer and projector to draw lines between all of the people on the floor, forming a Voronoi Diagram. This diagram has particularly strong significance when drawn around people's bodies, surrounding each person with lines that outline his or her personal space - the space closer to that person than to anyone else. Snibbe states that this work \"shows that personal space, though we call it our own, is only defined by others and changes without our control\".\n\nSnibbe has recently become more broadly known for creating some of the first interactive art apps for iOS devices (iPhone, iPad, and iPod Touch). His first three apps—Gravilux, Bubble Harp, and Antograph—released in May, 2010 as ports of screen-based artwork from the 1990s Dynamic Systems Series, all rose into the top ten in the iTunes Store's Entertainment section, and have been downloaded over 400,000 times. Snibbe collaborated with Björk to produce Biophilia, the first full-length app album, which was released for iPad and iPhone in 2011.\n\nSnibbe received undergraduate and master's degrees in computer science and fine art from Brown University, where he studied with Dr. Andries van Dam and Dr. John Hughes. Snibbe studied animation at the Rhode Island School of Design with Amy Kravitz. After making several hand-drawn animated shorts, he turned to interactive art as his primary artistic medium. His first public interactive work, Motion Phone won an award from Prix Ars Electronica in 1996 and established him as a contributor to the field.\n\nSnibbe's work has been shown at the Whitney Museum of American Art (New York), San Francisco Museum of Modern Art (California), The Kitchen (New York), Eyebeam (New York), the NTT InterCommunication Center (Tokyo, Japan) and the Institute of Contemporary Arts (London, UK). His work is also shown and collected by science museums, including the Exploratorium (San Francisco, CA), the New York Hall of Science (Queens, NY), the Museum of Science and Industry (Chicago, IL), the Cité des Sciences et de l'Industrie (Paris, France), the London Science Museum (UK), and the Phaeno Science Center (Germany). He was also featured on a December 18, 2011 episode of CNN's The Next List.\n\nHe has received grants from the Rockefeller Foundation the National Endowment for the Arts, National Video Resources and awards from the Prix Ars Electronica Festival, the (Stuttgart Trickfilm-Festival), the Black Mariah Film Festival, and the Student Academy Awards.\n\nSnibbe has taught media art, animation, and computer science at UC Berkeley, California Institute of the Arts, and the San Francisco Art Institute. He worked as a Computer Scientist at Adobe Systems from 1994–1996, on the special effects and animation software Adobe After Effects, named on six patents for work in animation, interface, and motion tracking. He was an employee at Interval Research from 1996-2000 where he worked on Computer Vision, Computer Graphics and Haptics research projects, also receiving several patents in those fields.\n\nSnibbe is the founder of Snibbe Interactive, which distributes and develops immersive interactive experiences for use in museums, entertainment and branding; Scott Snibbe Studio which produces original apps and apps made in collaboration with other musicians and filmmakers; and the nonprofit research organization Sona Research, which researches the socially beneficial applications of interactive technologies. In 2009, Snibbe presented Sona Research's first research paper \"Social Immersive Media\" at the CHI 2009 conference, coining the term Social Immersive Media to describe interface techniques to create effective immersive interactive experiences focused on social interaction, and winning the best paper of conference award. In November, 2013 Snibbe and Jaz Banga debated Laura Sydell and Christopher M. Kelty in an Oxford style debate entitled, Patent Pending: Does the U.S. Patent System stifle innovation?\n\nInteractive Art for the Screen\n\niPhone and iPad Apps\n\nInteractive Projections\n\nElectromechanical Sculpture\n\nInternet Art\n\nPublic Art Installations\n\nPerformance\n\nFilm\n\n\n\n"}
{"id": "24495502", "url": "https://en.wikipedia.org/wiki?curid=24495502", "title": "Sega v. Accolade", "text": "Sega v. Accolade\n\nSega Enterprises Ltd. v. Accolade, Inc., 977 F.2d 1510 (9th Cir. 1992), is a case in which the United States Court of Appeals for the Ninth Circuit applied American intellectual property law to the reverse engineering of computer software. Stemming from the publishing of several Sega Genesis games by video game publisher Accolade, which had disassembled Genesis software in order to publish games without being licensed by Sega, the case involved several overlapping issues, including the scope of copyright, permissible uses for trademarks, and the scope of the fair use doctrine for computer code.\n\nThe case was filed in the U.S. District Court for the Northern District of California, which ruled in favor of Sega and issued an injunction against Accolade preventing them from publishing any more games for the Genesis and requiring them to recall all the existing Genesis games they had for sale. Accolade appealed the decision to the Ninth Circuit on the grounds that their reverse engineering of the Genesis was protected under fair use. The Ninth Circuit reversed the district court's order and ruled that Accolade's use of reverse engineering to publish Genesis titles was protected under fair use, and that its alleged violation of Sega trademarks was the fault of Sega. The case is frequently cited in matters involving reverse engineering and fair use under copyright law.\n\nIn March 1984, Sega Enterprises Ltd. was purchased by its former CEO, David Rosen, along with a group of backers. Hayao Nakayama, one of these backers, was named the new CEO of Sega. Following the crash of the arcade industry, Nakayama decided to focus development efforts on the home console market. During this time, Sega became concerned about software and hardware piracy in Southeast Asia, and particularly in Taiwan. Taiwan was not a signatory of the Berne Convention on copyright, limiting Sega's legal options in that region. However, Taiwan did allow prosecution for trademark infringement. Though Sega had created security systems in their consoles to keep their software from being pirated and to keep unlicensed publishers out, much like its competitor Nintendo, counterfeiters had discovered ways to prevent the Sega trademark from appearing on their games, bypassing the trademark altogether.\n\nAfter the release of the Sega Genesis in 1989, video game publisher Accolade began exploring options to release some of their PC game titles onto the console. At the time, however, Sega had a licensing deal in place for third-party developers that increased the costs to the developer. According to Accolade co-founder Alan Miller, \"One pays them between $10 and $15 per cartridge on top of the real hardware manufacturing costs, so it about doubles the cost of goods to the independent publisher.\" In addition to this, Sega required that it would be the exclusive publisher of Accolade's games if Accolade were to be licensed, preventing Accolade from releasing its games to other systems. To get around licensing, Accolade chose to seek an alternative way to bring their games to the Genesis by purchasing a console in order to decompile the executable code of three Genesis games and use it to program their new cartridges in a way that would allow them to disable the security lockouts that prevented playing of unlicensed games. This was done successfully to bring \"\" to the Genesis in 1990. In doing so, Accolade had also copied Sega's copyrighted game code multiple times in order to reverse engineer the software of Sega's licensed Genesis games.\n\nAs a result of the piracy and unlicensed development issues, Sega incorporated a technical protection mechanism into a new edition of the Genesis released in 1990, referred to as the Genesis III. This new variation of the Genesis included code known as the Trademark Security System (TMSS), which, when a game cartridge was inserted into the console, would check for the presence of the string \"SEGA\" at a particular point in the memory contained in the cartridge. If and only if the string was present, the console would run the game, and would briefly display the message: \"PRODUCED BY OR UNDER LICENSE FROM SEGA ENTERPRISES LTD.\" This system had a twofold effect: it added extra protection against unlicensed developers and software piracy, and it forced the Sega trademark to display when the game was powered up, making a lawsuit for trademark infringement possible if unlicensed software were to be developed. Accolade learned of this development at the Winter Consumer Electronics Show in January 1991, at which Sega showed the new Genesis III and demonstrated it screening and rejecting an \"Ishido\" game cartridge. With more games planned for the following year, Accolade successfully identified the TMSS file. They later added this file to the games \"HardBall!\", \"Star Control\", \"Mike Ditka Power Football\", and \"Turrican\".\n\nOn October 31, 1991, Sega filed suit against Accolade in the United States District Court for the Northern District of California, on charges of trademark infringement and unfair competition in violation of the Lanham Act. Copyright infringement, a violation of the Copyright Act of 1976, was added a month later to the list of charges. In response, Accolade filed a counterclaim for falsifying the source of its games by displaying the Sega trademark when the game was powered up. The case was heard by Judge Barbara A. Caulfield.\n\nSega argued that Accolade had infringed upon its copyrights because Accolade's games contained Sega's material. Accolade insisted that their use of Sega's material constituted fair use. However, Judge Caulfield did not accept this explanation since Accolade was a game manufacturer, their works were for financial gain, and because their works competed directly with Sega's licensed games, likely resulting in a sales decrease for Sega's games. Accolade's case was further hurt by a presentation by a Sega engineer named Takeshi Nagashima, who showed two Sega game cartridges that were able to run on the Genesis III without the trademark-displaying TMSS, and offered them to Accolade's defense team but would not reveal how that was possible. Ultimately, this would result in Accolade's defeat on April 3, 1992, when Judge Caulfield ruled in favor of Sega and issued an injunction prohibiting future sales by Accolade of Genesis-compatible games incorporating the Sega message or using the results of the reverse engineering. Almost a week later, Accolade was also required by the court to recall all of their Genesis-compatible games.\n\nThe decision in the district court ruling had been very costly to Accolade. According to Accolade co-founder Alan Miller, \"Just to fight the injunction, we had to pay at least half a million dollars in legal fees.\" On April 14, 1992, Accolade filed for a stay on the injunction pending appeal in the district court, but when the court did not rule by April 21, Accolade appealed the verdict to the Ninth Circuit of the U.S. Court of Appeals. A stay was granted on the mandate to recall all of Accolade's Genesis games, but the injunction preventing further reverse engineering and development of Genesis software was maintained until August 28, when the Ninth Circuit ordered it dissolved pending the appeal review.\n\nIn support of the appeal, the Computer & Communications Industry Association submitted an \"amicus curiae\" brief claiming that the district court had made errors in concluding that Accolade had infringed upon Sega's copyright by reverse engineering its software, extending copyright protection to method of operation, and failing to consider whether Accolade's games were substantially similar to Sega's copyrighted material. Amicus briefs were also submitted by the American Committee for Interoperable Systems, the Computer and Business Equipment Manufacturers Association, and copyright law professor Dennis S. Karjala from Arizona State University.\n\nIn reviewing the case, the court considered several factors in its own analysis, examining trademark and copyright issues separately. As in the district court trial, Nagashima showed the court a game cartridge that ran on the Genesis that did not display the trademark logo. However, the court was not moved by this, deciding that Nagashima's cartridges showed what one could do with knowledge of the TMSS, which Accolade did not possess. According to the court, because knowledge of how to avoid displaying the trademark on the Genesis III was not information that was public to the industry, Sega's attempt to prove that the display of their trademark was not required for games to be played on the console was insufficient. Writing for the opinion of the court, Judge Stephen Reinhardt stated, \"Sega knowingly risked two significant consequences: the false labeling of some competitors' products and the discouraging of other competitors from manufacturing Genesis-compatible games. Under the Lanham Act, the former conduct, at least, is clearly unlawful.\" The court then went on to cite \"Anti-Monopoly v. General Mills Fun Group\", which states in reference to the Lanham Act, \"The trademark is misused if it serves to limit competition in the manufacture and sales of a product. That is the special province of the limited monopolies provided pursuant to the patent laws.\" The judges in the case had decided that Sega had violated this provision of the act by utilizing its trademark to limit competition for software for its console.\n\nTo determine the status of Accolade's claim of fair use of Sega's copyrighted game code, the court reviewed four criteria of fair use: the nature of the copyrighted work, the amount of the copyrighted work used, the purpose of use, and the effects of use on the market for the work. Of note to the judges in reviewing Sega's copyright claim was the difference in size between the TMSS file and the sizes of Accolade's games. As noted by Judge Reinhardt in writing the opinion of the court, the TMSS file \"contains approximately twenty to twenty-five bytes of data. Each of Accolade's games contains a total of 500,000 to 1,500,000 bytes. According to Accolade employees, the header file is the only portion of Sega's code that Accolade copied into its own game programs.\" This made the games overwhelmingly original content, and according to Judge Reinhardt, to the benefit of the public to be able to compete with Sega's licensed games, especially if the games were dissimilar as contended in the appeal. The court did not accept the argument that Accolade's games competed directly with Sega's, noting that there was no proof that any of Accolade's published games had diminished the market for any of Sega's games. Despite claims from Sega's attorneys that the company had invested much time and effort into developing the Genesis, and that Accolade was capitalizing on this time and energy, the court rejected these claims under the notion that the console was largely functional, and its functional principles were not protected under the Copyright Act of 1976. On the matter of reverse engineering as a process, the court concluded that \"where disassembly is the only way to gain access to the ideas and functional elements embodied in a copyrighted computer program and where there is a legitimate reason for seeking such access, disassembly is a fair use of the copyrighted work, as a matter of law.\"\n\nOn August 28, 1992, the Ninth Circuit overturned the district court's verdict and ruled that Accolade's decompilation of the Sega software constituted fair use. The court's written opinion followed on October 20 and noted that the use of the software was non-exploitative, despite being commercial, and that the trademark infringement, being required by the TMSS for a Genesis game to run on the system, was inadvertently triggered by a fair use act and the fault of Sega for causing false labeling. As a result of the verdict being overturned, the costs of the appeal were assessed to Sega. The injunction remained in force, however, because Sega petitioned the appeals court to rehear the case.\n\nOn January 8, 1993, with Sega's petition for a rehearing still pending, the court took the unusual step of amending its October 20, 1992 opinion and lifted the injunction preventing Accolade from developing or selling Genesis software. This was followed by a formal denial of Sega's petition for a rehearing on January 26. As Accolade's counterclaim for false labeling under the Lanham Act was declined by the Ninth Circuit, this essentially left \"each party as free to act as it was before the issuance of preliminary injunctive relief\" while the district court considered the counterclaim. Sega and Accolade ultimately settled on April 30, 1993. As a part of this settlement, Accolade became an official licensee of Sega, and later developed and released \"Barkley Shut Up and Jam!\" while under license. The terms of the licensing, including whether or not any special arrangements or discounts were made to Accolade, were not released to the public. The financial terms of the settlement were also not disclosed, although both companies agreed to pay their own legal costs.\n\nIn an official statement, Sega of America chairman David Rosen expressed satisfaction with the settlement. According to Rosen, \"This settlement is a satisfactory ending to what was a very complex set of issues. Not only are we pleased to settle this case amicably, we've also turned a corner in our association with Accolade and now look forward to a healthy and mutually beneficial relationship in the future.\" Accolade's Alan Miller expressed more excitement with the settlement and the opportunities it presented for the company, saying in his statement, \"We are very pleased with the settlement, and we're excited about the new markets it opens to Accolade. Accolade currently experiences strong demand for its Sega Genesis products in North America and Europe. We will now be able to publish our products on the Sega Genesis and Game Gear systems throughout the world.\" Despite the settlement, however, Accolade had lost somewhere between $15 million and $25 million during the injunction period, according to Miller.\n\n\"Sega v. Accolade\" has been an influential case in matters involving reverse engineering of software and copyright infringement, and has been cited in numerous cases since 1993. The case has redefined how reverse engineering with unlicensed products is seen in legal issues involving copyright. Legally, the decision concurred that the nature of Accolade's work in reverse engineering the Sega Genesis was to access ideas that were deemed unprotected by copyright law, and could only be accessed by decompiling. By the verdict, the console's functional principles were established not to be protected by copyright, and that when no other means were available, reverse engineering the copyrighted software to access information about the console's functional principles is protected by the fair use doctrine. One such example of the precedent set by this case is \"Sony Computer Entertainment, Inc. v. Connectix Corporation\", which cited \"Sega v. Accolade\" in deciding that reverse engineering the Sony PlayStation BIOS was protected by fair use and was non-exploitative.\n\nAmong the influences of the verdict include \"Sega v. Accolade\"'s effect on the criteria for fair use and the responsibilities of trademark holders in legal examinations. Although Accolade had copied entire Genesis games in order to identify the TMSS, the court gave little weight to the criterion on the amount of the copyrighted work being copied, in light of the fact that Accolade had done so in order to create their own compatible software. Likewise, the nature of the work was also given less weight, essentially establishing a two-factor approach to evaluating fair use in the purpose of use and impact on the market. It was also the first time that the Lanham Act was interpreted to mean that confusion resulting from the placement of one's trademark on another work by means of a security program is the fault of the original registrant of the trademark.\n\n\"Sega v. Accolade\" also served to help establish that the functional principles of computer software cannot be protected by copyright law. Rather, the only legal protection to such principles can be through holding a patent or by trade secret. This aspect of the verdict has received criticism as well, citing that though the functional principles are not protectable under copyright law, the TMSS code is protectable, and that by allowing reverse engineering as fair use despite this security, the Ninth Circuit Court of Appeals has encouraged the copying of legally protected programs for the exploration of unprotected functionality.\n"}
{"id": "7023870", "url": "https://en.wikipedia.org/wiki?curid=7023870", "title": "Sex strike", "text": "Sex strike\n\nA sex strike, sometimes called a sex boycott, is a strike, a method of non-violent resistance in which one or multiple persons (usually women) refrain from sex with their partners to achieve certain goals. It is a form of temporary sexual abstinence.\n\nSex strikes have been used to protest many issues, from war to gang violence.\n\nThe most famous example of a sex strike in the arts is the Greek playwright Aristophanes' work \"Lysistrata\", an anti-war comedy. The female characters in the play, led by the eponymous Lysistrata, withhold sex from their husbands as part of their strategy to secure peace and end the Peloponnesian War.\n\nAmong the Igbo people of Nigeria, in pre-colonial times, the community of women periodically formed themselves into a Council, a kind of women's trade union. This was headed by the Agba Ekwe, 'the favoured one of the goddess Idemili and her earthly manifestation'. She carried her staff of authority and had the final word in public gatherings and assemblies. Central among her tasks was to ensure men's good behaviour, punishing male attempts at harassment or abuse. What men most feared was the Council's power of strike action. According to Ifi Amadiume, an Igbo anthropologist: \"The strongest weapon the Council had and used against the men was the right to order mass strikes and demonstrations by all women. When ordered to strike, women refused to perform their expected duties and roles, including all domestic, sexual and maternal services. They would leave the town \"en masse\", carrying only suckling babies. If angry enough, they were known to attack any men they met.\"\n\nCiting similar examples of women's strike action in hunter-gatherer and other precolonial traditions around the world, some anthropologists argue that it was thanks to solidarity of this kind—especially collective resistance to the possibility of rape—that language, culture, and religion became established in our species in the first instance. This controversial hypothesis is known as the \"Female Cosmetic Coalitions\", \"Lysistrata\", or \"sex strike\" theory of human origins.\n\nIn October 1997, the chief of the Military of Colombia, General Manuel Bonnet publicly called for a sex strike among the wives and girlfriends of the Colombian left-wing guerrillas, drug traffickers, and paramilitaries as part of a strategy—along with diplomacy—to achieve a ceasefire. Also the mayor of Bogota, Antanas Mockus, declared the capital a women-only zone for one night, suggesting men to stay at home to reflect on violence. The guerrillas ridiculed the initiatives, pointing at the fact that there were more than 2,000 women in their army. In the end the ceasefire was achieved, but lasted only a short time.\n\nIn September 2006 dozens of wives and girlfriends of gang members from Pereira, Colombia, started a sex strike called \"La huelga de las piernas cruzadas\" (\"the strike of crossed legs\") to curb gang violence, in response to 480 deaths due to gang violence in the coffee region. According to spokeswoman Jennifer Bayer, the specific target of the strike was to force gang members to turn in their weapons in compliance with the law. According to them, many gang members were involved in violent crime for status and sexual attractiveness, and the strike sent the message that refusing to turn in the guns was not sexy. In 2010 the city's murder rate saw the steepest decline in Colombia, down by 26.5%.\n\nIn June 2011, women organized in the so-called Crossed Legs Movement in the secluded town of Barbacoas in southwestern Colombia, started a sex strike to pressure the government to repair the road connecting Barbacoas and its neighboring towns and cities. They declared that if the men of the town were not going to demand action, they would refuse to have sex with them. The men of Barbacoas showed no support at the beginning of the campaign, but they soon joined in the protest campaign. After 112 days strike in October 2011, the Colombian government promised action on road repairs, and construction ensued.\n\nIn April 2009 a group of Kenyan women organised a week-long sex strike aimed at politicians, encouraging the wives of the president and prime minister to join in too, and offering to pay prostitutes for lost earnings if they joined in.\n\nIn 2003 Leymah Gbowee and the Women of Liberia Mass Action for Peace organized nonviolence protests that included a sex strike. Their actions led to peace in Liberia after a 14‑year civil war and the election of Ellen Johnson Sirleaf, country's first female head of state. Leymah Gbowee was awarded the 2011 Nobel Peace Prize \"for her non-violent struggle for the safety of women and for women's rights to full participation in peace-building work.\"\n\nIn the build-up to New Year's Eve in 2008, hundreds of Neapolitan women pledged to make their husbands and lovers \"sleep on the sofa\" unless they took action to prevent fireworks from causing serious injuries.\n\nDuring the summer of 2011, women in rural Mindanao imposed a several-week-long sex strike in an attempt to end fighting between their two villages.\n\nIn October 2014, Pricilla Nanyang, a politician in South Sudan, coordinated a meeting of women peace activists in Juba \"to advance the cause of peace, healing and reconciliation.\" Attendees issued a statement which called on women of South Sudan \"to deny their husbands conjugal rights until they ensure that peace returns.\"\n\nIn 2012, inspired by the 2003 Liberian sex strike, the Togolese opposition coalition \"Let's Save Togo\" asked women to abstain from sex for a week as a protest against President Faure Gnassingbé, whose family has been in power for more than 45 years. The strike aimed to \"motivate men who are not involved in the political movement to pursue its goals\". Opposition leader Isabelle Ameganvi views it as a possible \"weapon of the battle\" to achieve political change.\n\n\n\n• The Lysistrata project\n"}
{"id": "1983620", "url": "https://en.wikipedia.org/wiki?curid=1983620", "title": "Shouting fire in a crowded theater", "text": "Shouting fire in a crowded theater\n\n\"Shouting \"fire\" in a crowded theater\" is a popular metaphor for speech or actions made for the principal purpose of creating unnecessary panic. The phrase is a paraphrasing of Oliver Wendell Holmes, Jr.'s opinion in the United States Supreme Court case \"Schenck v. United States\" in 1919, which held that the defendant's speech in opposition to the draft during World War I was not protected free speech under the First Amendment of the United States Constitution.\n\nThe paraphrasing differs from Holmes's original wording in that it typically does not include the word \"falsely\", while also adding the word \"crowded\" to describe the theatre. The original wording used in Holmes's opinion (\"falsely shouting fire in a theatre and causing a panic\") highlights that speech that is dangerous false is not protected, as opposed to speech that is dangerous but also true.\n\nHolmes, writing for a unanimous Court, ruled that it was a violation of the Espionage Act of 1917 (amended by the Sedition Act of 1918), to distribute flyers opposing the draft during World War I. Holmes argued this abridgment of free speech was permissible because it presented a \"clear and present danger\" to the government's recruitment efforts for the war. Holmes wrote:\n\nThe First Amendment holding in \"Schenck\" was later partially overturned by \"Brandenburg v. Ohio\" in 1969, which limited the scope of banned speech to that which would be directed to and likely to incite imminent lawless action (e.g. a riot). The test in \"Brandenburg\" is the current Supreme Court jurisprudence on the ability of government to proscribe speech after that fact. Despite \"Schenck\" being limited, the phrase \"shouting \"fire\" in a crowded theater\" has since come to be known as synonymous with an action that the speaker believes goes beyond the rights guaranteed by free speech, reckless or malicious speech, or an action whose outcomes are obvious.\n\nChristopher M. Finan, Executive Director of the National Coalition Against Censorship, writes that Justice Holmes began to doubt his decision due to criticism received from free-speech activists. He also met the legal scholar Zechariah Chafee and discussed his \"Harvard Law Review\" article \"Freedom of Speech in War Times\". According to Finan, Holmes's change of heart influenced his decision to join the minority and dissent in the \"Abrams v. United States\" case. Abrams was deported for issuing flyers saying the US should not intervene in the Russian Revolution. Holmes and Brandeis said that \"a silly leaflet by an unknown man\" should not be considered illegal. Chafee argued in \"Free Speech in the United States\" that a better analogy in \"Schenk\" might be a man who stands in a theatre and warns the audience that there are not enough fire exits.\n\nIn his introductory remarks to a 2006 debate in defense of free speech, writer Christopher Hitchens parodied the Holmes judgement by opening \" Fire, fire ... fire. Now you've heard it\", before condemning the famous analogy as \"the fatuous verdict of the greatly over-praised Justice Oliver Wendell Holmes.\" Hitchens argued that the socialists imprisoned by the court's decision \"were the ones shouting fire when there really a fire in a very crowded theatre indeed... [W]ho's going to decide?\"\n\nPeople have indeed falsely shouted \"Fire!\" in crowded public venues and caused panics on numerous occasions, such as at the Royal Surrey Gardens Music Hall of London in 1856, a theater in New York's Harlem neighborhood in 1884, and in the Italian Hall disaster of 1913, which left 73 dead. In the Shiloh Baptist Church disaster of 1902, over 100 people died when \"fight\" was misheard as \"fire\" in a crowded church causing a panic and stampede.\n\nIn contrast, in the Brooklyn Theatre fire of 1876, the actors initially falsely claimed that the fire was part of the performance, in an attempt to avoid a panic. However, this delayed the evacuation and made the resulting panic far more severe.\n\n"}
{"id": "488403", "url": "https://en.wikipedia.org/wiki?curid=488403", "title": "Table setting", "text": "Table setting\n\nTable setting (laying a table) or place setting refers to the way to set a table with tableware—such as eating utensils and for serving and eating. The arrangement for a single diner is called a place setting. The practice of dictating the precise arrangement of tableware has varied across cultures and historical periods.\nInformal settings generally have fewer utensils and dishes but use a layout based on more formal settings. Utensils are arranged in the order and according to the manner in which the diner will use them. In the West, forks, plate, butter knife, and napkin generally are placed to the left of the dinner plate, and knives, spoons, stemware and tumblers, cups, and saucers to the right. (By contrast, formal settings in Armenia place the fork to the right of the dinner plate and informal settings in Turkey place the fork to the right of the dinner plate if not accompanied by a knife) Sauceboats and serving dishes, when used, either are placed on the table or, more formally, may be kept on a side table.\n\nAt an informal setting, fewer utensils are used and serving dishes are placed on the table. Sometimes the cup and saucer are placed on the right side of the spoon, about 30cm or 12 inches from the edge of the table. Often, in less formal settings, the napkin should be in the wine glass. However, such objects as napkin rings are very rare in the United Kingdom, Spain, Mexico, or Italy.\n\nUtensils are placed inward about 20cm or 8 inches from the edge of the table, with all placed either upon the same invisible baseline or upon the same invisible median line. Utensils in the outermost position are to be used first (for example, a soup spoon or a salad fork, later the dinner fork and the dinner knife). The blades of the knives are turned toward the plate. Glasses are placed an inch (2.5 cm) or so above the knives, also in the order of use: white wine, red wine, dessert wine, and water tumbler.\n\nThe most formal dinner is served from the kitchen. When the meal is served, in addition to the central plate (a service plate or dinner plate at supper; at luncheon, a service plate or luncheon plate) at each place there are a bread roll (generally on a bread plate, sometimes in the napkin), napkin, and flatware (knives and spoons to the right of the central plate, and forks to the left). Coffee is served in Butler Service style in demitasses, and a spoon placed on the saucer to the right of each handle. Serving dishes and utensils are not placed on the table for a formal dinner. The only exception in the West to these general rules is the protocol followed at the Spanish royal court, which was also adopted by the Austrian court, in which all cutlery was placed to the right of the central plate for each diner.\n\nAt a less formal dinner, not served from the kitchen, the dessert fork and spoon can be set above the plate, fork pointing right, spoon pointing left.\n\n"}
{"id": "26258076", "url": "https://en.wikipedia.org/wiki?curid=26258076", "title": "Term (time)", "text": "Term (time)\n\nA term is a period of duration, time or occurrence, in relation to an event. To differentiate an interval or duration, common phrases are used to distinguish the observance of length are near-term or short-term, medium-term or mid-term and long-term.\n\nIt is also used as part of a calendar year, especially one of the three parts of an academic term and working year in the United Kingdom: Michaelmas term, Hilary term / Lent term or Trinity term / Easter term, the equivalent to the American semester. In America there is a midterm election held in the middle of the four-year presidential term, there are also academic midterm exams.\n\nIn economics, it is the period required for economic agents to reallocate resources, and generally reestablish equilibrium. The actual length of this period, usually numbered in years or decades, varies widely depending on circumstantial context. During the long term, all factors are variable.\n\nIn finance or financial operations of borrowing and investing, what is considered \"long-term\" is usually above 3 years, with \"medium-term\" usually between 1 and 3 years and \"short-term\" usually under 1 year. It is also used in some countries to indicate a fixed term investment such as a term deposit.\n\nIn law, the term of a contract is the duration for which it is to remain in effect (not to be confused with the meaning of \"term\" that denotes any provision of a contract). A fixed-term contract is one concluded for a pre-defined time.\n\n"}
{"id": "31916744", "url": "https://en.wikipedia.org/wiki?curid=31916744", "title": "The Forgotten Prisoners", "text": "The Forgotten Prisoners\n\n\"The Forgotten Prisoners\" is an article by Peter Benenson published in \"The Observer\" on 28 May 1961. Citing the Universal Declaration of Human Rights articles 18 and 19, it announced a campaign on \"Appeal for Amnesty, 1961\" and called for \"common action\". The article also launched the book \"Persecution 1961\" and its stories of doctor Agostinho Neto, philosopher Constantin Noica, lawyer Antonio Amat and Ashton Jones and Patrick Duncan.\n\nBenenson reputedly wrote his article after having learnt that two Portuguese students from Coimbra were imprisoned in Portugal for raising a toast to freedom. The article was reprinted in newspapers across the world and provoked a flood of responses from the readers, marshalling groups in several countries to examine human rights abuses.\n\nWhile, in 2015, the original story still remains to be verified, the appeal marks the beginning of Amnesty International, founded in London the same year following the publication after Benenson enlisted a Conservative, a Liberal and a Labour MP.\n"}
{"id": "44586402", "url": "https://en.wikipedia.org/wiki?curid=44586402", "title": "The Holy State and the Profane State", "text": "The Holy State and the Profane State\n\nThe Holy State and the Profane State (Prophane in the original, sometimes shortened to The Holy State) is a 1642 book by English churchman and historian Thomas Fuller. It describes the holy state as existing in the family and in public life, gives rules of conduct, model \"characters\" for the various professions and profane biographies. It was perhaps the most popular of Fuller's writings, having been reprinted four times after the first run sold out.\n\nThe book contains four volumes, three of which outline the characteristics of archetypes such as \"the good husband\", \"the good servant\", and \"the good widow\", and the fourth containing biographies intended to illustrate profane people.\n\n"}
{"id": "6040692", "url": "https://en.wikipedia.org/wiki?curid=6040692", "title": "The unanswered questions", "text": "The unanswered questions\n\nThe phrase unanswered questions or undeclared questions (Sanskrit avyākṛta, Pali: avyākata - \"unfathomable, unexpounded\"), in Buddhism, refers to a set of common philosophical questions that Buddha refused to answer, according to Buddhist texts. The Pali texts give only ten, the Sanskrit texts fourteen questions.\n\nAccording to their subject matter the questions can be grouped in four categories.\n\n\n1. Is the world eternal?\n\n2. ...or not?\n\n3. ...or both?\n\n4. ...or neither?\n\n\n5. Is the world finite?\n\n6. ...or not?\n\n7. ...or both?\n\n8. ...or neither?\n\n\n9. Is the self identical with the body?\n\n10. ...or is it different from the body?\n\n\n11. Does the Tathagata (Buddha) exist after death?\n\n12. ...or not?\n\n13. ...or both?\n\n14. ...or neither?\n\nMajjhima Nikaya 63 & 72 in the Pali canon contain a list of ten unanswered questions about certain views (ditthi):\n\n\nThe Sabbasava Sutta (Majjhima Nikaya 2) also mentions 16 questions which are seen as \"unwise reflection\" and lead to attachment to views relating to a self. \n\n\nThe Buddha states that it is unwise to be attached to both views of having and perceiving a self and views about not having a self. Any view which sees the self as \"permanent, stable, everlasting, unchanging, remaining the same for ever and ever\" is \"becoming enmeshed in views, a jungle of views, a wilderness of views; scuffling in views, the agitation (struggle) of views, the fetter of views.\"\n\n\n"}
{"id": "23680", "url": "https://en.wikipedia.org/wiki?curid=23680", "title": "There's Plenty of Room at the Bottom", "text": "There's Plenty of Room at the Bottom\n\n\"There's Plenty of Room at the Bottom: An Invitation to Enter a New Field of Physics\" was a lecture given by physicist Richard Feynman at the annual American Physical Society meeting at Caltech on December 29, 1959. Feynman considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than those used at the time. Versions of the talk were printed in a few popular magazines within a year and newspapers covered the winning of the presented challenges in 1960 and again in 1985. The talk went unnoticed and did not inspire the conceptual beginnings of the field, but nanotechnology research advocates began citing it in the 1990s to establish the scientific credibility of their work.\n\nFeynman considered a number of interesting ramifications of a general ability to manipulate matter on an atomic scale. He was particularly interested in the possibilities of denser computer circuitry, and microscopes that could see things much smaller than is possible with scanning electron microscopes. These ideas were later realized by the use of the scanning tunneling microscope, the atomic force microscope and other examples of scanning probe microscopy and storage systems such as Millipede, created by researchers at IBM.\n\nFeynman also suggested that it should be possible, in principle, to make nanoscale machines that \"arrange the atoms the way we want\", and do chemical synthesis by mechanical manipulation.\n\nHe also presented the possibility of \"swallowing the doctor\", an idea that he credited in the essay to his friend and graduate student Albert Hibbs. This concept involved building a tiny, swallowable surgical robot.\n\nAs a thought experiment he proposed developing a set of one-quarter-scale manipulator hands slaved to the operator's hands to build one-quarter scale machine tools analogous to those found in any machine shop. This set of small tools would then be used by the small hands to build and operate ten sets of one-sixteenth-scale hands and tools, and so forth, culminating in perhaps a billion tiny factories to achieve massively parallel operations. He uses the analogy of a pantograph as a way of scaling down items. This idea was anticipated in part, down to the microscale, by science fiction author Robert A. Heinlein in his 1942 story \"Waldo\".\nAs the sizes got smaller, one would have to redesign some tools, because the relative strength of various forces would change. Although gravity would become unimportant, surface tension would become more important, Van der Waals attraction would become important, etc. Feynman mentioned these scaling issues during his talk. Nobody has yet attempted to implement this thought experiment, although it has been noted that some types of biological enzymes and enzyme complexes (especially ribosomes) function chemically in a way close to Feynman's vision. Feynman also mentioned in his lecture that it might be better eventually to use glass or plastic because their greater uniformity would avoid problems in the very small scale (metals and crystals are separated into domains where the lattice structure prevails). This could be a good reason to make machines and also electronics out of glass and plastic. At the present time, there are electronic components made of both materials. In glass, there are optical fiber cables that amplify the light pulses at regular intervals, using glass doped with the rare-earth element erbium. The doped glass is spliced into the fiber and pumped by a laser operating at a different frequency. In plastic, field effect transistors are being made with polythiophene, a plastic invented by Alan J. Heeger et al. that becomes an electrical conductor when oxidized. At this time, a factor of just 20 in electron mobility separates plastic from silicon.\n\nAt the meeting Feynman concluded his talk with two challenges, and he offered a prize of $1000 for the first individuals to solve each one. The first challenge involved the construction of a tiny motor, which, to Feynman's surprise, was achieved by November 1960 by Caltech graduate, William McLellan, a meticulous craftsman, using conventional tools. The motor met the conditions, but did not advance the art. The second challenge involved the possibility of scaling down letters small enough so as to be able to fit the entire \"Encyclopædia Britannica\" on the head of a pin, by writing the information from a book page on a surface 1/25,000 smaller in linear scale. In 1985, Tom Newman, a Stanford graduate student, successfully reduced the first paragraph of \"A Tale of Two Cities\" by 1/25,000, and collected the second Feynman prize. Newman's thesis adviser, R. Fabian Pease, had read the paper back in 1966; but it was another grad student in the lab, Ken Polasko, who had recently read it who suggested attempting the challenge. Newman was looking for some arbitrary random pattern for demonstrating their technology. Newman said, \"Text was ideal because it has so many different shapes.\"\n\n\"The New Scientist\" reported \"the scientific audience was captivated.\" Feynman had \"spun the idea off the top of his mind\" without even \"notes from beforehand\". There were no copies of the speech for those asking for copies. A \"foresighted admirer\" brought a tape recorder and an edited transcript, without Feynman's jokes, was made for publication by Caltech. In February 1960, Caltech's \"Engineering and Science\" published the speech. In addition to excerpts in \"The New Scientist\", versions were printed in \"The Saturday Review\" and \"Popular Science\". Newspapers announced the winning of the first challenge. It was included as the final chapter in the 1961 book, \"Miniaturization\".\n\nK. Eric Drexler later took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves, via computer control instead of control by a human operator, in his 1986 book \"Engines of Creation: The Coming Era of Nanotechnology\".\n\nAfter Feynman's death, scholars studying the historical development of nanotechnology have concluded that his actual role in catalyzing nanotechnology research was limited based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, has reconstructed the history of the publication and republication of Feynman's talk, along with the record of citations to “Plenty of Room” in the scientific literature. In Toumey's 2008 article, \"Reading Feynman into Nanotechnology\", he found 11 versions of the publication of “Plenty of Room\", plus two instances of a closely related talk by Feynman, “Infinitesimal Machinery,” which Feynman called “Plenty of Room, Revisited.” Also in Toumey's references are videotapes of that second talk.\n\nToumey found that the published versions of Feynman's talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981. Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s. This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by Drexler in his 1986 book, \"Engines of Creation: The Coming Era of Nanotechnology\", which cited Feynman, and in a cover article headlined \"Nanotechnology\", published later that year in a mass-circulation science-oriented magazine, \"OMNI\". The journal \"Nanotechnology\" was launched in 1989; the famous Eigler-Schweizer experiment, precisely manipulating 35 xenon atoms, was published in \"Nature\" in April 1990; and \"Science\" had a special issue on nanotechnology in November 1991. These and other developments hint that the retroactive rediscovery of Feynman's “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman.\n\nToumey's analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.\n\nFeynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past. More concretely, his stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, illustrated by President Clinton's January 2000 speech calling for a Federal program:\n\nWhile the version of the Nanotechnology Research and Development Act that was passed by the House in May 2003 called for a study of the technical feasibility of molecular manufacturing, this study was removed to safeguard funding of less controversial research before the Act was passed by the Senate and finally signed into law by President Bush on December 3, 2003.\n\n\n\n"}
{"id": "1405654", "url": "https://en.wikipedia.org/wiki?curid=1405654", "title": "Thermolabile", "text": "Thermolabile\n\nThermolabile refers to a substance which is subject to destruction, decomposition, or change in response to heat. This term is often used to describe biochemical substances.\n\nFor example, many bacterial exotoxins are thermolabile and can be easily inactivated by the application of moderate heat. \nEnzymes are also thermolabile and lose their activity when the temperature rises.\nLoss of activity in such toxins and enzymes is likely due to change in the three-dimensional structure of the toxin protein during exposure to heat.\nIn pharmaceutical compounds, heat generated during grinding may lead to degradation of thermolabile compounds.\n\nThis is of particular use in testing gene function. This is done by intentionally creating mutants which are thermolabile. Growth below the permissive temperature allows normal protein function, while increasing the temperature above the permissive temperature ablates activity, likely by denaturing the protein.\n\nThermolabile enzymes are also studied for their applications in DNA replication techniques, such as PCR, where thermostable enzymes are necessary for proper DNA replication. Enzyme function at higher temperatures may be enhanced with trehalose, which opens up the possibility of using normally thermolabile enzymes in DNA replication. \n\n"}
{"id": "471600", "url": "https://en.wikipedia.org/wiki?curid=471600", "title": "Total institution", "text": "Total institution\n\nA total institution is a place of work and residence where a great number of similarly situated people, cut off from the wider community for a considerable time, together lead an enclosed, formerly administered round of life. The concept is mostly associated with the work of sociologist Erving Goffman.\n\nThe term is sometimes credited as having been coined and defined by Canadian sociologist Erving Goffman in his paper \"On the Characteristics of Total Institutions\", presented in April 1957 at the Walter Reed Institute's Symposium on Preventive and Social Psychiatry. An expanded version appeared in Donald Cressey's collection, \"The Prison\", and was reprinted in Goffman's 1961 collection, \"Asylums\". Fine and Manning, however, note that Goffman heard the term in lectures by Everett Hughes (likely during the late-1940s seminar, \"Work and Occupations\"). Regardless of whether Goffman coined the term, he can be credited with popularizing it.\n\nTotal institutions are divided by Goffman into five different types:\nDavid Rothman states that \"historians have confirmed the validity of Goffman's concept of 'total institutions' which minimizes the differences in formal mission to establish a unity of design and structure.\"\n\nIn \"Discipline and Punish\", Michel Foucault discussed total institutions in the language of \"complete and austere institutions\".\n\nAccording to S. Lammers and A. Verhey, some 80 percent of Americans will ultimately die not in their home, but in a total institution.\n\nSociologists have pointed out that tourist venues such as cruise ships are acquiring many of the characteristics of total institutions. Tourists may not be aware that they are being controlled, even constrained, but the environment has been designed to subtly manipulate the behavior of patrons. These examples differ from the traditional examples in that the influence is short term.\n\nJohn Paul Wright, professor of criminal justice at the University of Cincinnati, has characterized many modern American universities as total institutions: \n\n"}
{"id": "30067", "url": "https://en.wikipedia.org/wiki?curid=30067", "title": "Tradition", "text": "Tradition\n\nA tradition is a belief or behavior passed down within a group or society with symbolic meaning or special significance with origins in the past. Common examples include holidays or impractical but socially meaningful clothes (like lawyers' wigs or military officers' spurs), but the idea has also been applied to social norms such as greetings. Traditions can persist and evolve for thousands of years—the word \"tradition\" itself derives from the Latin \"tradere\" literally meaning to transmit, to hand over, to give for safekeeping. While it is commonly assumed that traditions have ancient history, many traditions have been invented on purpose, whether that be political or cultural, over short periods of time. Various academic disciplines also use the word in a variety of ways. \n\nThe phrase \"according to tradition\", or \"by tradition\", usually means that whatever information follows is known only by oral tradition, but is not supported (and perhaps may be refuted) by physical documentation, by a physical artifact, or other quality evidence. \"Tradition\" is used to indicate the quality of a piece of information being discussed. For example, \"According to tradition, Homer was born on Chios, but many other locales have historically claimed him as theirs.\" This tradition may never be proven or disproven. In another example, \"King Arthur, by tradition a true British king, has inspired many well loved stories.\" Whether they are documented fact or not does not decrease their value as cultural history and literature. \n\nTraditions are a subject of study in several academic fields, especially in social sciences such as anthropology, archaeology, and biology.\n\nThe concept of tradition, as the notion of holding on to a previous time, is also found in political and philosophical discourse. For example, it is the basis of the political concept of traditionalism, and also strands of many world religions including traditional Catholicism. In artistic contexts, tradition is used to decide the correct display of an art form. For example, in the performance of traditional genres (such as traditional dance), adherence to guidelines dictating how an art form should be composed are given greater importance than the performer's own preferences. A number of factors can exacerbate the loss of tradition, including industrialization, globalization, and the assimilation or marginalization of specific cultural groups. In response to this, tradition-preservation attempts have now been started in many countries around the world, focusing on aspects such as traditional languages. Tradition is usually contrasted with the goal of modernity and should be differentiated from customs, conventions, laws, norms, routines, rules and similar concepts.\n\nThe English word \"tradition\" comes from the Latin \"traditio\", the noun from the verb \"tradere\" (to transmit, to hand over, to give for safekeeping); it was originally used in Roman law to refer to the concept of legal transfers and inheritance. According to Anthony Giddens and others, the modern meaning of tradition evolved during the Enlightenment period, in opposition to modernity and progress.\n\nAs with many other generic terms, there are many definitions of tradition. The concept includes a number of interrelated ideas; the unifying one is that tradition refers to beliefs, objects or customs performed or believed in the past, originating in it, transmitted through time by being taught by one generation to the next, and are performed or believed in the present.\n\nTradition can also refer to beliefs or customs that are Prehistoric, with lost or arcane origins, existing from \"time immemorial\". Originally, traditions were passed orally, without the need for a writing system. Tools to aid this process include poetic devices such as rhyme and alliteration. The stories thus preserved are also referred to as tradition, or as part of an oral tradition. Even such traditions, however, are presumed to have originated (been \"invented\" by humans) at some point. Traditions are often presumed to be ancient, unalterable, and deeply important, though they may sometimes be much less \"natural\" than is presumed. It is presumed that at least two transmissions over three generations are required for a practice, belief or object to be seen as traditional. Some traditions were deliberately invented for one reason or another, often to highlight or enhance the importance of a certain institution. Traditions may also be adapted to suit the needs of the day, and the changes can become accepted as a part of the ancient tradition. Tradition changes slowly, with changes from one generation to the next being seen as significant. Thus, those carrying out the traditions will not be consciously aware of the change, and even if a tradition undergoes major changes over many generations, it will be seen as unchanged.\n\nThere are various origins and fields of tradition; they can refer to:\n\nMany objects, beliefs and customs can be traditional. Rituals of social interaction can be traditional, with phrases and gestures such as saying \"thank you\", sending birth announcements, greeting cards, etc. Tradition can also refer to larger concepts practiced by groups (family traditions at Christmas), organizations (company's picnic) or societies, such as the practice of national and public holidays. Some of the oldest traditions include monotheism (three millennia) and citizenship (two millennia). It can also include material objects, such as buildings, works of art or tools.\n\nTradition is often used as an adjective, in contexts such as traditional music, traditional medicine, traditional values and others. In such constructions tradition refers to specific values and materials particular to the discussed context, passed through generations.\n\nThe term \"invention of tradition\", introduced by E. J. Hobsbawm, refers to situations when a new practice or object is introduced in a manner that implies a connection with the past that is not necessarily present. A tradition may be deliberately created and promulgated for personal, commercial, political, or national self-interest, as was done in colonial Africa; or it may be adopted rapidly based on a single highly publicized event, rather than developing and spreading organically in a population, as in the case of the white wedding dress, which only became popular after Queen Victoria wore a white gown at her wedding to Albert of Saxe-Coburg.\n\nAn example of an invention of tradition is the rebuilding of the Palace of Westminster (location of the British Parliament) in the Gothic style. Similarly, most of the traditions associated with monarchy of the United Kingdom, seen as rooted deep in history, actually date to 19th century. Other examples include the invention of tradition in Africa and other colonial holdings by the occupying forces. Requiring legitimacy, the colonial power would often invent a \"tradition\" which they could use to legitimize their own position. For example, a certain succession to a chiefdom might be recognized by a colonial power as traditional in order to favour their own candidates for the job. Often these inventions were based in some form of tradition, but were exaggerated, distorted, or biased toward a particular interpretation.\n\nInvented traditions are a central component of modern national cultures, providing a commonality of experience and promoting the unified national identity espoused by nationalism. Common examples include public holidays (particularly those unique to a particular nation), the singing of national anthems, and traditional national cuisine (see national dish). Expatriate and immigrant communities may continue to practice the national traditions of their home nation.\n\nIn science, tradition is often used in the literature in order to define the relationship of an author's thoughts to that of his or her field. In 1948, philosopher of science Karl Popper suggested that there should be a \"rational theory of tradition\" applied to science which was fundamentally sociological. For Popper, each scientist who embarks on a certain research trend inherits the tradition of the scientists before them as he or she inherits their studies and any conclusions that superseded it. Unlike myth, which is a means of explaining the natural world through means other than logical criticism, scientific tradition was inherited from Socrates, who proposed critical discussion, according to Popper. For Thomas Kuhn, who presented his thoughts in a paper presented in 1977, a sense of such a critical inheritance of tradition is, historically, what sets apart the best scientists who change their fields is an embracement of tradition.\n\nTraditions are a subject of study in several academic fields in social sciences—chiefly anthropology, archaeology, and biology—with somewhat different meanings in different fields. It is also used in varying contexts in other fields, such as history, psychology and sociology. Social scientists and others have worked to refine the commonsense concept of tradition to make it into a useful concept for scholarly analysis. In the 1970s and 1980s, Edward Shils explored the concept in detail. Since then, a wide variety of social scientists have criticized traditional ideas about tradition; meanwhile, \"tradition\" has come into usage in biology as applied to nonhuman animals.\n\nTradition as a concept variously defined in different disciplines should not be confused with various traditions (perspectives, approaches) in those disciplines.\n\nTradition is one of the key concepts in anthropology; it can be said that anthropology is the study of \"tradition in traditional societies\". There is however no \"theory of tradition\", as for most anthropologists the need to discuss what tradition is seems unnecessary, as defining tradition is both unnecessary (everyone can be expected to know what it is) and unimportant (as small differences in definition would be just technical). There are however dissenting views; scholars such as Pascal Boyer argue that defining tradition and developing theories about it are important to the discipline.\n\nIn archaeology, the term \"tradition\" is a set of cultures or industries which appear to develop on from one another over a period of time. The term is especially common in the study of American archaeology.\n\nBiologists, when examining groups of non-humans, have observed repeated behaviors which are taught within communities from one generation to the next. Tradition is defined in biology as \"a behavioral practice that is relatively enduring (i.e., is performed repeatedly over a period of time), that is shared among two or more members of a group, that depends in part on socially aided learning for its generation in new practitioners\", and has been called a precursor to \"culture\" in the anthropological sense.\n\nBehavioral traditions have been observed in groups of fish, birds, and mammals. Groups of orangutans and chimpanzees, in particular, may display large numbers of behavioral traditions, and in chimpanzees, transfer of traditional behavior from one group to another (not just within a group) has been observed. Such behavioral traditions may have evolutionary significance, allowing adaptation at a faster rate than genetic change.\n\nIn the field of musicology and ethnomusicology tradition refers to the belief systems, repertoire, techniques, style and culture that is passed down through subsequent generations. Tradition in music suggests a historical context with which one can perceive distinguishable patterns. Along with a sense of history, traditions have a fluidity that cause them to evolve and adapt over time.\nWhile both musicology and ethnomusicology are defined by being 'the scholarly study of music' they differ in their methodology and subject of research. 'Tradition, or traditions, can be presented as a context in which to study the work of a specific composer or as a part of a wide-ranging historical perspective.'\n\nThe concept of tradition, in early sociological research (around the turn of the 19th and 20th century), referred to that of the traditional society, as contrasted by the more modern industrial society. This approach was most notably portrayed in Max Weber's concepts of traditional authority and modern rational-legal authority. In more modern works, One hundred years later, sociology sees tradition as a social construct used to contrast past with the present and as a form of rationality used to justify certain course of action.\n\nTraditional society is characterized by lack of distinction between family and business, division of labor influenced primarily by age, gender, and status, high position of custom in the system of values, self-sufficiency, preference to saving and accumulation of capital instead of productive investment, relative autarky. Early theories positing the simple, unilineal evolution of societies from traditional to industrial model are now seen as too simplistic.\n\nIn 1981 Edward Shils in his book \"Tradition\" put forward a definition of tradition that became universally accepted. According to Shils, tradition is anything which is transmitted or handed down from the past to the present.\n\nAnother important sociological aspect of tradition is the one that relates to rationality. It is also related to the works of Max Weber (see theories of rationality), and were popularized and redefined in 1992 by Raymond Boudon in his book \"Action\". In this context tradition refers to the mode of thinking and action justified as \"it has always been that way\". This line of reasoning forms the basis of the logical flaw of the appeal to tradition (or \"argumentum ad antiquitatem\"), which takes the form \"this is right because we've always done it this way.\" In most cases such an appeal can be refuted on the grounds that the \"tradition\" being advocated may no longer be desirable, or, indeed, may never have been despite its previous popularity.\n\nThe idea of tradition is important in philosophy. Twentieth century philosophy is often divided between an 'analytic' tradition, dominant in Anglophone and Scandinavian countries, and a 'continental' tradition, dominant in German and Romance speaking Europe. Increasingly central to continental philosophy is the project of deconstructing what its proponents, following Martin Heidegger, call 'the tradition', which began with Plato and Aristotle. In contrast, some continental philosophers - most notably, Hans-Georg Gadamer - have attempted to rehabilitate the tradition of Aristotelianism. This move has been replicated within analytic philosophy by Alasdair MacIntyre. However, MacIntyre has himself deconstructed the idea of 'the tradition', instead posing Aristotelianism as one philosophical tradition in rivalry with others.\n\nThe concepts of tradition and traditional values are frequently used in political and religious discourse to establish the legitimacy of a particular set of values. In the United States in the twentieth and twenty-first centuries, the concept of tradition has been used to argue for the centrality and legitimacy of conservative religious values. Similarly, strands of orthodox theological thought from a number of world religions openly identify themselves as wanting a return to tradition. For example, the term \"traditionalist Catholic\" refers to those, such as Archbishop Lefebvre, who want the worship and practices of the church to be as they were before the Second Vatican Council of 1962–65. Likewise, Sunni Muslims are referred to as \"Ahlus Sunnah wa Al-Jamā‘ah\" (), literally \"people of the tradition [of Muhammad] and the community\", emphasizing their attachment to religious and cultural tradition.\n\nMore generally, tradition has been used as a way of determining the political spectrum, with right-wing parties having a stronger affinity to the ways of the past than left-wing ones. Here, the concept of adherence tradition is embodied by the political philosophy of traditionalist conservatism (or simply \"traditionalism\"), which emphasizes the need for the principles of natural law and transcendent moral order, hierarchy and organic unity, agrarianism, classicism and high culture, and the intersecting spheres of loyalty. Traditionalists would therefore reject the notions of individualism, liberalism, modernity, and social progress, but promote cultural and educational renewal, and revive interest in the church, the family, the state and local community. This view has been criticised for including in its notion of tradition practices which are no longer considered to be desirable, for example, stereotypical views of the place of women in domestic affairs.\n\nIn other societies, especially ones experiencing rapid social change, the idea of what is \"traditional\" may be widely contested, with different groups striving to establish their own values as the legitimate traditional ones. Defining and enacting traditions in some cases can be a means of building unity between subgroups in a diverse society; in other cases, tradition is a means of othering and keeping groups distinct from one another.\n\nIn artistic contexts, in the performance of traditional genres (such as traditional dance), adherence to traditional guidelines is of greater importance than performer's preferences. It is often the unchanging form of certain arts that leads to their perception as traditional. For artistic endeavors, tradition has been used as a contrast to \"creativity\", with traditional and folk art associated with unoriginal imitation or repetition, in contrast to fine art, which is valued for being original and unique. More recent philosophy of art, however, considers interaction with tradition as integral to the development of new artistic expression.\n\nIn the social sciences, \"tradition\" is often contrasted with \"modernity\", particularly in terms of whole societies. This dichotomy is generally associated with a linear model of social change, in which societies progress from being traditional to being modern. Tradition-oriented societies have been characterized as valuing filial piety, harmony and group welfare, stability, and interdependence, while a society exhibiting modernity would value \"individualism (with free will and choice), mobility, and progress.\" Another author discussing tradition in relationship to modernity, Anthony Giddens, sees tradition as something bound to ritual, where ritual guarantees the continuation of tradition. Gusfield and others, though, criticize this dichotomy as oversimplified, arguing that tradition is dynamic, heterogeneous, and coexists successfully with modernity even within individuals.\n\nTradition should be differentiated from \"customs, conventions, laws, norms, routines, rules\" and similar concepts. Whereas tradition is supposed to be invariable, they are seen as more flexible and subject to innovation and change. Whereas justification for tradition is ideological, the justification for other similar concepts is more practical or technical. Over time, customs, routines, conventions, rules and such can evolve into traditions, but that usually requires that they stop having (primarily) a practical purpose. For example, wigs worn by lawyers were at first common and fashionable; spurs worn by military officials were at first practical but now are both impractical and traditional.\n\nIn many countries, concerted attempts are being made to preserve traditions that are at risk of being lost. A number of factors can exacerbate the loss of tradition, including industrialization, globalization, and the assimilation or marginalization of specific cultural groups. Customary celebrations and lifestyles are among the traditions that are sought to be preserved. Likewise, the concept of tradition has been used to defend the preservation and reintroduction of minority languages such as Cornish under the auspices of the European Charter for Regional or Minority Languages. Specifically, the charter holds that these languages \"contribute to the maintenance and development of Europe's cultural wealth and traditions\". The Charter goes on to call for \"the use or adoption... of traditional and correct forms of place-names in regional or minority languages\". Similarly, UNESCO includes both \"oral tradition\" and \"traditional manifestations\" in its definition of a country's cultural properties and heritage. It therefore works to preserve tradition in countries such as Brazil.\n\nIn Japan, certain artworks, structures, craft techniques and performing arts are considered by the Japanese government to be a precious legacy of the Japanese people, and are protected under the Japanese Law for the Protection of Cultural Properties. This law also identifies people skilled at traditional arts as \"National Living Treasures\", and encourages the preservation of their craft.\n\nFor native peoples like the Māori in New Zealand, there is conflict between the fluid identity assumed as part of modern society and the traditional identity with the obligations that accompany it; the loss of language heightens the feeling of isolation and damages the ability to perpetuate tradition.\n\nThe phrase \"traditional cultural expressions\" is used by the World Intellectual Property Organization to refer to \"any form of artistic and literary expression in which traditional culture and knowledge are embodied. They are transmitted from one generation to the next, and include handmade textiles, paintings, stories, legends, ceremonies, music, songs, rhythms and dance.\"\n\n\n"}
{"id": "23647264", "url": "https://en.wikipedia.org/wiki?curid=23647264", "title": "Umpire abuse", "text": "Umpire abuse\n\nUmpire abuse refers to the act of abuse towards an umpire, referee, or other official in sport. The abuse can be verbal abuse (such as namecalling), or physical abuse (such as punching). For example, Australian Football League spectators use the term \"white maggot\" (derived from their formerly white uniforms) towards umpires at games, when they do not agree with an umpire's decision.\n\nUmpire abuse has become quite common in sport, practiced by players, coaches and spectators, with one Australian Football league having half the tribunal cases heard about umpire abuse. There have also been some high-profile cases of abuse towards the umpires in sport, with one Australian football player suspended for life after striking an umpire.\n\nIn 1996, Major League Baseball (MLB) player Roberto Alomar spat in umpire John Hirschbeck's face during a dispute. Alomar received a five-game suspension for the incident, but the punishment was served during the following season, and not the 1996 playoffs. MLB umpires, upset over the lack of an immediate suspension, threatened to go on strike before a federal judge prevented them from doing so.\n\nDuring the 2008 Beijing Olympics, Angel Valodia Matos from Cuba pushed and then kicked a referee in the face during a Taekwondo match. He was disqualified for taking too much injury time in the bronze medal match by referee Chakir Chelbat, before kicking Chelbat in the face. The referee required stitches in his lip after the attack. The World Taekwondo Federation has banned Matos and his coach from taekwondo competitions for life.\n\nIn 2016, Mark Jamar, an Essendon AFL player has been fined $1500 for umpire abuse. The umpire, Mathew Nicholls, reported Jamar after he expressed his annoyance that he wasn't awarded a free kick in a marking contest.\n\nLeagues and the like are trying to stop abuse towards umpires.\n\nIn Australian Rules Football, attempting to strike or striking an umpire, abusing or threatening an umpire, or disputing an umpires decision is a reportable offense, per the Laws of Australian Football. It is also possible to send a player off for up to the remainder of the game for abusing an umpire, however this is only usually practiced at amateur and junior level. There have also been other programs trailed, such as making players suspended for umpire abuse attend umpire training sessions.\n\nIn cricket, the preamble to the Laws of Cricket state that it is not within the spirit of cricket to abuse an umpire, and to dispute their decisions.\n\nIn ice hockey, it is against the rules to dispute a referee's decision, although the team captains can skate out and discuss the calls with the referees and linesmen. After a warning (and a minor penalty), arguing with a referee, or starting a fight with a referee is grounds for a game misconduct, which results in ejection for the offending player or coach.\n\nIn baseball, it is against the rules for any coach, manager, or player to question the umpire's judgement on a call on the field, or on balls and strikes. If a coach, manager, or player begins to walk toward the umpire with the intent to argue a call or balls and strikes, he will be warned to return to his bench or position. If he continues to advance, he will be ejected.\n\nIn the criminal justice system, some jurisdictions mandate more severe penalties when a person commits a crime against a sports official immediately prior to, during, or immediately following any athletic contest in which the umpire, referee, or judge is participating in an official capacity. For instance, in the State of California, Section 243.8 of the Penal Code specifies that Battery against a sports official shall result in a fine that does not exceed more than $2000, or imprisonment with the sentence not exceeding one year. Battery against a sports official has more severe penalties than simple battery against a civilian, as in Section 243. California's maximum incarceration penalty for Battery on a Sports Official is twice as lengthy as the maximum sentence for Simple Battery.\n"}
{"id": "32245", "url": "https://en.wikipedia.org/wiki?curid=32245", "title": "Universal property", "text": "Universal property\n\nIn various branches of mathematics, a useful construction is often viewed as the “most efficient solution” to a certain problem. The definition of a universal property uses the language of category theory to make this notion precise and to study it abstractly.\n\nThis article gives a general treatment of universal properties. To understand the concept, it is useful to study several examples first, of which there are many: all free objects, direct product and direct sum, free group, free lattice, Grothendieck group, Dedekind–MacNeille completion, product topology, Stone–Čech compactification, tensor product, inverse limit and direct limit, kernel and cokernel, pullback, pushout and equalizer.\n\nBefore giving a formal definition of universal properties, we offer some motivation for studying such constructions.\n\n\nSuppose that \"U\": \"D\" → \"C\" is a functor from a category \"D\" to a category \"C\", and let \"X\" be an object of \"C\". Consider the following dual (opposite) notions:\nAn initial morphism from \"X\" to \"U\" is an initial object in the category formula_1 of morphisms from \"X\" to \"U\". In other words, it consists of a pair (\"A\", \"Φ\") where \"A\" is an object of \"D\" and \"Φ\": \"X\" → \"U\"(\"A\") is a morphism in \"C\", such that the following initial property is satisfied:\n\nA terminal morphism from \"U\" to \"X\" is a terminal object in the comma category formula_2 of morphisms from \"U\" to \"X\". In other words, it consists of a pair (\"A\", \"Φ\") where \"A\" is an object of \"D\" and \"Φ\": \"U\"(\"A\") → \"X\" is a morphism in \"C\", such that the following terminal property is satisfied:\nThe term universal morphism refers either to an initial morphism or a terminal morphism, and the term universal property refers either to an initial property or a terminal property. In each definition, the existence of the morphism \"g\" intuitively expresses the fact that (\"A\", \"Φ\") is \"general enough\", while the uniqueness of the morphism ensures that (\"A\", \"Φ\") is \"not too general\".\n\nSince the notions of \"initial\" and \"terminal\" are dual, it is often enough to discuss only one of them, and simply reverse arrows in \"C\" for the dual discussion. Alternatively, the word \"universal\" is often used in place of both words.\n\nNote: some authors may call only one of these constructions a \"universal morphism\" and the other one a \"co-universal morphism\". Which is which depends on the author, although in order to be consistent with the naming of limits and colimits the latter construction should be named universal and the former couniversal. This article uses the unambiguous terminology of initial and terminal objects.\n\nBelow are a few examples, to highlight the general idea. The reader can construct numerous other examples by consulting the articles mentioned in the introduction.\n\nLet \"C\" be the category of vector spaces K\"-Vect over a field \"K\" and let \"D\" be the category of algebras K\"-Alg over \"K\" (assumed to be unital and associative). Let\nbe the forgetful functor which assigns to each algebra its underlying vector space.\n\nGiven any vector space \"V\" over \"K\" we can construct the tensor algebra \"T\"(\"V\") of \"V\". The tensor algebra is characterized by the fact:\nThis statement is an initial property of the tensor algebra since it expresses the fact that the pair (\"T\"(\"V\"), \"i\"), where \"i\" : \"V\" → \"U\"(\"T\"(\"V\")) is the inclusion map, is an initial morphism from the vector space \"V\" to the functor \"U\".\n\nSince this construction works for any vector space \"V\", we conclude that \"T\" is a functor from K\"-Vect to K\"-Alg. This means that \"T\" is \"left adjoint\" to the forgetful functor \"U\" (see the section below on relation to adjoint functors).\n\nA categorical product can be characterized by a terminal property. For concreteness, one may consider the Cartesian product in Set, the direct product in Grp, or the product topology in Top, where products exist.\n\nLet \"X\" and \"Y\" be objects of a category \"D\". The product of \"X\" and \"Y\" is an object \"X\" × \"Y\" together with two morphisms\nsuch that for any other object \"Z\" of \"D\" and morphisms \"f\" : \"Z\" → \"X\" and \"g\" : \"Z\" → \"Y\" there exists a unique morphism \"h\" : \"Z\" → \"X\" × \"Y\" such that \"f\" = π∘\"h\" and \"g\" = π∘\"h\".\n\nTo understand this characterization as a terminal property we take the category \"C\" to be the product category \"D\" × \"D\" and define the diagonal functor\nby Δ(\"X\") = (\"X\", \"X\") and Δ(\"f\" : \"X\" → \"Y\") = (\"f\", \"f\"). Then (\"X\" × \"Y\", (π, π)) is a terminal morphism from Δ to the object (\"X\", \"Y\") of \"D\" × \"D\": If (\"f\", \"g\") is any morphism from (\"Z\", \"Z\") to (\"X\", \"Y\"), then it must equal a morphism Δ(\"h\" : \"Z\" → \"X\" × \"Y\") = (\"h\", \"h\") from Δ(\"Z\") = (\"Z\", \"Z\") to Δ(\"X\" × \"Y\") = (\"X\" × \"Y\",\"X\" × \"Y\"), followed by (π, π).\n\nCategorical products are a particular kind of limit in category theory. One can generalize the above example to arbitrary limits and colimits.\n\nLet \"J\" and \"C\" be categories with \"J\" a small index category and let \"C\" be the corresponding functor category. The \"diagonal functor\"\nis the functor that maps each object \"N\" in \"C\" to the constant functor Δ(\"N\"): \"J\" → \"C\" to \"N\" (i.e. Δ(\"N\")(\"X\") = \"N\" for each \"X\" in \"J\").\n\nGiven a functor \"F\" : \"J\" → \"C\" (thought of as an object in \"C\"), the \"limit\" of \"F\", if it exists, is nothing but a terminal morphism from Δ to \"F\". Dually, the \"colimit\" of \"F\" is an initial morphism from \"F\" to Δ.\n\nDefining a quantity does not guarantee its existence. Given a functor \"U\" and an object \"X\" as above, there may or may not exist an initial morphism from \"X\" to \"U\". If, however, an initial morphism (\"A\", φ) does exist then it is essentially unique. Specifically, it is unique up to a \"unique\" isomorphism: if (\"A\"′, φ′) is another such pair, then there exists a unique isomorphism \"k\": \"A\" → \"A\"′ such that φ′ = \"U\"(\"k\")φ. This is easily seen by substituting (\"A\"′, φ′) for (\"Y\", \"f\") in the definition of the initial property.\n\nIt is the pair (\"A\", φ) which is essentially unique in this fashion. The object \"A\" itself is only unique up to isomorphism. Indeed, if (\"A\", φ) is an initial morphism and \"k\": \"A\" → \"A\"′ is any isomorphism then the pair (\"A\"′, φ′), where φ′ = \"U\"(\"k\")φ, is also an initial morphism.\n\nThe definition of a universal morphism can be rephrased in a variety of ways. Let \"U\" be a functor from \"D\" to \"C\", and let \"X\" be an object of \"C\". Then the following statements are equivalent:\n\nThe dual statements are also equivalent:\n\nSuppose (\"A\", φ) is an initial morphism from \"X\" to \"U\" and (\"A\", φ) is an initial morphism from \"X\" to \"U\". By the initial property, given any morphism \"h\": \"X\" → \"X\" there exists a unique morphism \"g\": \"A\" → \"A\" such that the following diagram commutes:\nIf \"every\" object \"X\" of \"C\" admits an initial morphism to \"U\", then the assignment formula_3 and formula_4 defines a functor \"V\" from \"C\" to \"D\". The maps φ then define a natural transformation from 1 (the identity functor on \"C\") to \"UV\". The functors (\"V\", \"U\") are then a pair of adjoint functors, with \"V\" left-adjoint to \"U\" and \"U\" right-adjoint to \"V\".\n\nSimilar statements apply to the dual situation of terminal morphisms from \"U\". If such morphisms exist for every \"X\" in \"C\" one obtains a functor \"V\": \"C\" → \"D\" which is right-adjoint to \"U\" (so \"U\" is left-adjoint to \"V\").\n\nIndeed, all pairs of adjoint functors arise from universal constructions in this manner. Let \"F\" and \"G\" be a pair of adjoint functors with unit η and co-unit ε (see the article on adjoint functors for the definitions). Then we have a universal morphism for each object in \"C\" and \"D\":\n\nUniversal constructions are more general than adjoint functor pairs: a universal construction is like an optimization problem; it gives rise to an adjoint pair if and only if this problem has a solution for every object of \"C\" (equivalently, every object of \"D\").\n\nUniversal properties of various topological constructions were presented by Pierre Samuel in 1948. They were later used extensively by Bourbaki. The closely related concept of adjoint functors was introduced independently by Daniel Kan in 1958.\n\n\n\n"}
{"id": "17456938", "url": "https://en.wikipedia.org/wiki?curid=17456938", "title": "Valuation (logic)", "text": "Valuation (logic)\n\nIn logic and model theory, a valuation can be:\n\nIn mathematical logic (especially model theory), a valuation is an assignment of truth values to formal sentences that follows a truth schema. Valuations are also called truth assignments.\n\nIn propositional logic, there are no quantifiers, and formulas are built from propositional variables using logical connectives. In this context, a valuation begins with an assignment of a truth value to each propositional variable. This assignment can be uniquely extended to an assignment of truth values to all propositional formulas.\n\nIn first-order logic, a language consists of a collection of constant symbols, a collection of function symbols, and a collection of relation symbols. Formulas are built out of atomic formulas using logical connectives and quantifiers. A structure consists of a set (domain of discourse) that determines the range of the quantifiers, along with interpretations of the constant, function, and relation symbols in the language. Corresponding to each structure is a unique truth assignment for all sentences (formulas with no free variables) in the language.\n\nIf formula_1 is a valuation, that is, a mapping from the atoms to the set formula_2, then the double-bracket notation is commonly used to denote a valuation; that is, formula_3 for a proposition formula_4.\n\n\n"}
{"id": "48974025", "url": "https://en.wikipedia.org/wiki?curid=48974025", "title": "William Costin", "text": "William Costin\n\nWilliam \"Billy\" Costin (c. 1780 - May 31, 1842) was a free African-American activist and scholar who successfully challenged District of Columbia slave codes in the Circuit Court of the District of Columbia.\n\nLittle is known of Costin's upbringing. His enslaved mother was Ann Dandridge Costin, and his father was reputedly her master John Dandridge, making her the half-sister of Martha Dandrige (who as a young widow later married George Washington, the future president.)\n\nAnn Dandridge Costin was said to be of African and Cherokee descent. Native American slavery had ended and she should have been free under Virginia law via her maternal ancestry, but the slave colony put priority on African ancestry. While Ann and several of her children lived at the Mount Vernon plantation owned by George Washington on the Potomac River in Fairfax County, Virginia, there is no evidence her son William lived there. He may have lived nearby with other family.\n\nCostin's legal status as \"free\" or \"enslaved\" is debated by historians, as is the identity of his father. He may have been the son of Martha Washington's brother, William Dandridge, or of Martha's son, John Parke (\"Jacky\") Custis. He would thus have qualified as either Martha's nephew or grandson.\n\nAround 1800, Costin moved from Mount Vernon to Washington City, what later became known as Washington, D.C. About that time, he married Delphy Judge, whom Martha Washington had given to her granddaughter Elizabeth Parke Custis Law as a wedding present in 1796. She and her children were manumitted in 1807 by Thomas Law, Elizabeth's husband. (see below).\n\nIn 1812, Costin built a house on A Street South on Capitol Hill. There he and his wife Delphy raised a large family.\n\nFrom 1818, Costin worked as a porter of the Bank of Washington. He worked to save his money and buy properties in the developing capital.\n\nIn 1818, Costin helped start a school for African-American children, which his daughter, Louisa Parke Costin (c. 1804-October 31, 1831), eventually led. It was known as the first public school for black children in the city. In the August 1835 Snow Riot, when a white mob burned abolitionist institutions and those associated with free blacks, it spared the school.\n\nIn addition to the school, Costin created other organizations. In 1821, he helped found the Israel Colored Methodist Episcopal Church, led by an African-American minister. In June 1825, Costin co-founded an African-American masonic lodge known as Social Lodge #1 (originally #7) In December 1825, he helped found the Columbian Harmony Society, providing burial benefits and a cemetery for use by African Americans. Working with nearly the same group with whom he started other organizations, including fellow hack driver William Wormley (ca. 1800-1855) and educator George Bell (1761–1843), Costin served as the Society's vice president through 1826.\n\nIn 1821, Costin challenged the part of a Black Code restricting African Americans in the District. It was an effort by the administration of Mayor Samuel Nicholas Smallwood to dissuade free blacks from settling there. The law required that free persons of color had \n\nto appear before the mayor with documents signed by three 'respectable' white inhabitants of their neighborhood vouching for their good character and means of subsistence. If the evidence was satisfactory to the mayor, the individuals were to post a yearly $20 bond with a 'good and respectable' white person as assurance of their 'good, sober and orderly conduct,' and to ensure that they would not become public charges or beggars in the streets. to post an annual twenty dollar cash bond and present three references from white neighbors, purportedly to guarantee their peaceful behavior. \n\nCostin refused to comply, and was fined five dollars by a justice of the peace. He appealed his fine to the court.\n\nIn the case, Chief Justice William Cranch accepted that the City charter authorized it \"to prescribe the terms and conditions upon which free Negroes and mulattoes may reside in the city.\" (He was a nephew of second U.S. President John Adams). Costin asked the court to strike the law entirely, saying that Congress could not delegate powers to the city that were unconstitutional, and that \"the Constitution knows no distinction of color.\"\n\nCranch defended the peace-bond law by pointing to certain barriers in the state voting and jury laws of the time, writing: \n\nIt is said that the constitution gives equal rights to all the citizens of the United States, in the several states. But that clause of the constitution does not prohibit any state from denying to some of its citizens some of the political rights enjoyed by others. In all the states certain qualifications are necessary to the right of suffrage; the right to serve on juries, and the right to hold certain offices; and in most of the states the absence of the African color is among those qualifications.\n\nBut Cranch conceded that the law was unfair to free blacks who had long lived in the city and contributed to it, noting that they could not compel whites to give surety, and that the law threatened to force families apart. He ruled that those who had lived in the District prior to the law's enactment were exempted from having to abide by it. He said, \"It would seem to be unreasonable to suppose that Congress intended to give the [city] corporation the power to banish those free persons of color who had been guilty of no crime.\"\n\nIn 1800, Costin married Philadelphia \"Delphy\" Judge (c. 1779-December 13, 1831), the younger sister of Oney \"Ona\" Maria, known as Oney Judge (c.1773—February 25, 1848), both of whom were daughters of Betty Davis (c. 1738-1795), and were so-called \"dower\" slaves of Martha Washington at Mount Vernon.\n\nAccording to Virginia estate law, the dower slaves passed to the Custis children upon Martha's death.\n\nIn 1807 and 1820, Costin purchased the freedom of seven relatives. In 1807, Thomas Law (October 23, 1756 – July 31, 1834) freed six of Costin's sisters and half-sisters for \"ten cents.\"\n\nLaw was the husband of Elizabeth (\"Eliza\") Parke Custis Law (August 21, 1776 –December 31, 1831), who inherited these slaves at the death of her grandmother, Martha Washington.\n\nIn October 1820, the purchase of Costin's apparent cousin, Leanthe, who worked at the Mt. Vernon Mansion House, and was the daughter of Caroline, involved two steps. First, George Washington Parke Custis sold her to Costin for an undisclosed sum. Twelve days later, Costin freed her for \"five dollars.\"\n\nCostin remained in cordial contact with the Custis family throughout his life. In 1835, Eliza's brother, George Washington Parke Custis, supported Costin's side business driving a horse-and-buggy taxi.\n\nCostin's funeral on June 4, 1842 was attended by U.S. Attorney Francis Scott Key, who had composed the song that became adopted as the national anthem.\n\nThe funeral was notable for the long line of hansom cabs driven by Costin's friends. The funeral procession included both white and black mourners, and a horseback processional.\n\n"}
