{"id": "46406575", "url": "https://en.wikipedia.org/wiki?curid=46406575", "title": "Aimlessness (Buddhism)", "text": "Aimlessness (Buddhism)\n\nAimlessness or uncommittedness or wishlessness (Sanskrit \"apraṇihita\" अप्रणिहित) is a form of concentration in some schools of Buddhist meditation. The concept is particularly associated with the teachings of Thích Nhất Hạnh, who counts aimlessness as the third form of concentration or \"Third Door of Liberation\". The term \"apraṇihita\" literally means 'to place nothing in front' and is used to designate someone who has no aims for the future and no desire for the objects of perception.\n\nAimless wandering refers to both \"samsara\" (the cycle of birth, death and rebirth) and a mindfulness practice of exploration without destination that often takes the form of a walking meditation (though it does not require movement). In this practice, attention is paid to one's sensory perception of the experience rather than one's thoughts about the experience.\n"}
{"id": "336", "url": "https://en.wikipedia.org/wiki?curid=336", "title": "Altruism", "text": "Altruism\n\nAltruism is the principle and moral practice of concern for happiness of other human beings and/or animals, resulting in a quality of life both material and spiritual. It is a traditional virtue in many cultures and a core aspect of various religious traditions and secular worldviews, though the concept of \"others\" toward whom concern should be directed can vary among cultures and religions. In Hindu / Sanathana Dharma (whose philosophy is \"inclusive\", i.e includes, and welcomes everybody), where the dictum \"Vasudaiva Kutumbakam\" (the world is ONE FAMILY) is in force, always, \"others\" means \"EVERYONE\", unconditionally.\n\nIn an extreme case, altruism may become a synonym of selflessness which is the opposite of selfishness.\n\nIn a common way of living, it doesn't deny the singular nature of the subject, but realizes the traits of the individual personality in relation to the others, with a true, direct and personal interaction with each of them. It is focusing both on a single person and the whole community. In a (not only) Christian practice, it is the law of love direct to the ego and his neighbour.\n\nThe word \"altruism\" was coined by the French philosopher Auguste Comte in French, as \"altruisme\", for an antonym of egoism. He derived it from the Italian \"altrui\", which in turn was derived from Latin \"alteri\", meaning \"other people\" or \"somebody else\".\n\nAltruism in biological observations in field populations of the day organisms is an individual performing an action which is at a cost to themselves (e.g., pleasure and quality of life, time, probability of survival or reproduction), but benefits, either directly or indirectly, another third-party individual, without the expectation of reciprocity or compensation for that action. Steinberg suggests a definition for altruism in the clinical setting, that is \"intentional and voluntary actions that aim to enhance the welfare of another person in the absence of any quid pro quo external rewards\".\n\nAltruism can be distinguished from feelings of loyalty, in that whilst the latter is predicated upon social relationships, altruism does not consider relationships. Much debate exists as to whether \"true\" altruism is possible in human psychology. The theory of psychological egoism suggests that no act of sharing, helping or sacrificing can be described as truly altruistic, as the actor may receive an intrinsic reward in the form of personal gratification. The validity of this argument depends on whether intrinsic rewards qualify as \"benefits\".\n\nThe term \"altruism\" may also refer to an ethical doctrine that claims that individuals are morally obliged to benefit others. Used in this sense, it is usually contrasted with egoism, which claims individuals are morally obligated to serve themselves first.\n\nThe concept has a long history in philosophical and ethical thought. The term was originally coined in the 19th century by the founding sociologist and philosopher of science, Auguste Comte, and has become a major topic for psychologists (especially evolutionary psychology researchers), evolutionary biologists, and ethologists. Whilst ideas about altruism from one field can affect the other fields, the different methods and focuses of these fields always lead to different perspectives on altruism. In simple terms, altruism is caring about the welfare of other people and acting to help them.\n\nMarcel Mauss's book \"The Gift\" contains a passage called \"Note on alms\". This note describes the evolution of the notion of alms (and by extension of altruism) from the notion of sacrifice. In it, he writes:\n\nAlms are the fruits of a moral notion of the gift and of fortune on the one hand, and of a notion of sacrifice, on the other. Generosity is an obligation, because Nemesis avenges the poor and the gods for the superabundance of happiness and wealth of certain people who should rid themselves of it. This is the ancient morality of the gift, which has become a principle of justice. The gods and the spirits accept that the share of wealth and happiness that has been offered to them and had been hitherto destroyed in useless sacrifices should serve the poor and children.\n\n\nIn the science of ethology (the study of animal behaviour), and more generally in the study of social evolution, altruism refers to behaviour by an individual that increases the fitness of another individual while decreasing the fitness of the actor. In evolutionary psychology this may be applied to a wide range of human behaviors such as charity, emergency aid, help to coalition partners, tipping, courtship gifts, production of public goods, and environmentalism.\n\nTheories of apparently altruistic behavior were accelerated by the need to produce theories compatible with evolutionary origins. Two related strands of research on altruism have emerged from traditional evolutionary analyses and from evolutionary game theory a mathematical model and analysis of behavioural strategies.\n\nSome of the proposed mechanisms are:\n\nSuch explanations do not imply that humans are always consciously calculating how to increase their inclusive fitness when they are doing altruistic acts. Instead, evolution has shaped psychological mechanisms, such as emotions, that promote altruistic behaviors.\n\nEvery single instance of altruistic behavior need not always increase inclusive fitness; altruistic behaviors would have been selected for if such behaviors on average increased inclusive fitness in the ancestral environment. This need not imply that on average 50% or more of altruistic acts were beneficial for the altruist in the ancestral environment; if the benefits from helping the right person were very high it would be beneficial to err on the side of caution and usually be altruistic even if in most cases there were no benefits.\n\nThe benefits for the altruist may be increased and the costs reduced by being more altruistic towards certain groups. Research has found that people are more altruistic to kin than to no-kin, to friends than to strangers, to those attractive than to those unattractive, to non-competitors than to competitors, and to members ingroups than to members of outgroup.\n\nThe study of altruism was the initial impetus behind George R. Price's development of the Price equation, which is a mathematical equation used to study genetic evolution. An interesting example of altruism is found in the cellular slime moulds, such as \"Dictyostelium mucoroides.\" These protists live as individual amoebae until starved, at which point they aggregate and form a multicellular fruiting body in which some cells sacrifice themselves to promote the survival of other cells in the fruiting body.\n\nSelective investment theory proposes that close social bonds, and associated emotional, cognitive, and neurohormonal mechanisms, evolved in order to facilitate long-term, high-cost altruism between those closely depending on one another for survival and reproductive success.\n\nSuch cooperative behaviors have sometimes been seen as arguments for left-wing politics such by the Russian zoologist and anarchist Peter Kropotkin in his 1902 book \"\" and Peter Singer in his book \"A Darwinian Left.\"\n\nJorge Moll and Jordan Grafman, neuroscientists at the National Institutes of Health and LABS-D'Or Hospital Network (J.M.) provided the first evidence for the neural bases of altruistic giving in normal healthy volunteers, using functional magnetic resonance imaging. In their research, published in the Proceedings of the National Academy of Sciences USA in October 2006, they showed that both pure monetary rewards and charitable donations activated the mesolimbic reward pathway, a primitive part of the brain that usually responds to food and sex. However, when volunteers generously placed the interests of others before their own by making charitable donations, another brain circuit was selectively activated: the subgenual cortex/septal region. These structures are intimately related to social attachment and bonding in other species. Altruism, the experiment suggested, was not a superior moral faculty that suppresses basic selfish urges but rather was basic to the brain, hard-wired and pleasurable. One brain region, the subgenual anterior cingulate cortex/basal forebrain, contributes to learning altruistic behavior, especially in those with trait empathy. The same study has shown a connection between giving to charity and the promotion of social bonding.\n\nIn fact, in an experiment published in March 2007 at the University of Southern California neuroscientist Antonio R. Damasio and his colleagues showed that subjects with damage to the ventromedial prefrontal cortex lack the ability to empathically feel their way to moral answers, and that When confronted with moral dilemmas, these brain-damaged patients coldly came up with \"end-justifies-the-means\" answers, leading Damasio to conclude that the point was not that they reached immoral conclusions, but that when they were confronted by a difficult issue - in this case as whether to shoot down a passenger plane hijacked by terrorists before it hits a major city - these patients appear to reach decisions without the anguish that afflicts those with normally functioning brains. According to Adrian Raine, a clinical neuroscientist also at the University of Southern California, one of this study's implications is that society may have to rethink how it judges immoral people: \"Psychopaths often feel no empathy or remorse. Without that awareness, people relying exclusively on reasoning seem to find it harder to sort their way through moral thickets. Does that mean they should be held to different standards of accountability?\"\n\nIn another study, in the 1990s, Dr. Bill Harbaugh, a University of Oregon economist, concluded people are motivated to give for reasons of personal prestige and in a similar fMRI scanner test in 2007 with his psychologist colleague Dr. Ulrich Mayr, reached the same conclusions of Jorge Moll and Jordan Grafman about giving to charity, although they were able to divide the study group into two groups: \"egoists\" and \"altruists\". One of their discoveries was that, though rarely, even some of the considered \"egoists\" sometimes gave more than expected because that would help others, leading to the conclusion that there are other factors in cause in charity, such as a person's environment and values.\n\nThe International Encyclopedia of the Social Sciences defines \"psychological altruism\" as \"a motivational state with the goal of increasing another’s welfare.\" Psychological altruism is contrasted with \"psychological egoism,\" which refers to the motivation to increase one's own welfare.\n\nThere has been some debate on whether or not humans are truly capable of psychological altruism. Some definitions specify a self-sacrificial nature to altruism and a lack of external rewards for altruistic behaviors. However, because altruism ultimately benefits the self in many cases, the selflessness of altruistic acts is brought to question. The social exchange theory postulates that altruism only exists when benefits to the self outweigh costs to the self. Daniel Batson is a psychologist who examined this question and argues against the social exchange theory. He identified four major motives for altruism: altruism to ultimately benefit the self (egoism), to ultimately benefit the other person (altruism), to benefit a group (collectivism), or to uphold a moral principle (principlism). Altruism that ultimately serves selfish gains is thus differentiated from selfless altruism, but the general conclusion has been that empathy-induced altruism can be genuinely selfless. The \"empathy-altruism hypothesis\" basically states that psychological altruism does exist and is evoked by the empathic desire to help someone who is suffering. Feelings of empathic concern are contrasted with feelings of personal distress, which compel people to reduce their own unpleasant emotions. People with empathic concern help others in distress even when exposure to the situation could be easily avoided, whereas those lacking in empathic concern avoid helping unless it is difficult or impossible to avoid exposure to another's suffering. Helping behavior is seen in humans at about two years old, when a toddler is capable of understanding subtle emotional cues.\nIn psychological research on altruism, studies often observe altruism as demonstrated through prosocial behaviors such as helping, comforting, sharing, cooperation, philanthropy, and community service. Research has found that people are most likely to help if they recognize that a person is in need and feel personal responsibility for reducing the person's distress. Research also suggests that the number of bystanders witnessing distress or suffering affects the likelihood of helping (the \"Bystander effect\"). Greater numbers of bystanders decrease individual feelings of responsibility. However, a witness with a high level of empathic concern is likely to assume personal responsibility entirely regardless of the number of bystanders.\n\nMany studies have observed the effects of volunteerism (as a form of altruism) on happiness and health and have consistently found a strong connection between volunteerism and current and future health and well-being. In a study of older adults, those who volunteered were higher on life satisfaction and will to live, and lower in depression, anxiety, and somatization. Volunteerism and helping behavior have not only been shown to improve mental health, but physical health and longevity as well, attributable to the activity and social integration it encourages. One study examined the physical health of mothers who volunteered over a 30-year period and found that 52% of those who did not belong to a volunteer organization experienced a major illness while only 36% of those who did volunteer experienced one. A study on adults ages 55+ found that during the four-year study period, people who volunteered for two or more organizations had a 63% lower likelihood of dying. After controlling for prior health status, it was determined that volunteerism accounted for a 44% reduction in mortality. Merely being aware of kindness in oneself and others is also associated with greater well-being. A study that asked participants to count each act of kindness they performed for one week significantly enhanced their subjective happiness. It is important to note that, while research supports the idea that altruistic acts bring about happiness, it has also been found to work in the opposite direction—that happier people are also kinder. The relationship between altruistic behavior and happiness is bidirectional. Studies have found that generosity increases linearly from sad to happy affective states.\n\nStudies have also been careful to note that feeling over-taxed by the needs of others has conversely negative effects on health and happiness. For example, one study on volunteerism found that feeling overwhelmed by others' demands had an even stronger negative effect on mental health than helping had a positive one (although positive effects were still significant). Additionally, while generous acts make people feel good about themselves, it is also important for people to appreciate the kindness they receive from others. Studies suggest that gratitude goes hand-in-hand with kindness and is also very important for our well-being. A study on the relationship happiness to various character strengths showed that \"a conscious focus on gratitude led to reductions in negative affect and increases in optimistic appraisals, positive affect, offering emotional support, sleep quality, and well-being.\".\n\n\"Sociologists have long been concerned with how to build the good society\" (\"Altruism, Morality, and Social Solidarity\". American Sociological Association.). The structure of our societies and how individuals come to exhibit charitable, philanthropic, and other pro-social, altruistic actions for the common good is a largely researched topic within the field. The American Sociology Association (ASA) acknowledges Public sociology saying, \"The intrinsic scientific, policy, and public relevance of this field of investigation in helping to construct 'good societies' is unquestionable\" (\"Altruism, Morality, and Social Solidarity\" ASA). This type of sociology seeks contributions that aid grassroots and theoretical understandings of what motivates altruism and how it is organized, and promotes an altruistic focus in order to benefit the world and people it studies. How altruism is framed, organized, carried out, and what motivates it at the group level is an area of focus that sociologists seek to investigate in order to contribute back to the groups it studies and \"build the good society\". The motivation of altruism is also the focus of study; some publications link the occurrence of moral outrage to the punishment of perpetrators and compensation of victims.\n\nPathological altruism is when altruism is taken to an unhealthy extreme, and either harms the altruistic person, or well-intentioned actions cause more harm than good.\n\nThe term \"pathological altruism\" was popularised by the book \"Pathological Altruism\".\n\nExamples include depression and burnout seen in healthcare professionals, an unhealthy focus on others to the detriment of one's own needs, hoarding of animals, and ineffective philanthropic and social programs that ultimately worsen the situations they are meant to aid.\n\nMost, if not all, of the world's religions promote altruism as a very important moral value. Buddhism, Christianity, Hinduism, Islam, Jainism, Judaism, and Sikhism, etc., place particular emphasis on altruistic morality.\n\nAltruism figures prominently in Buddhism. Love and compassion are components of all forms of Buddhism, and are focused on all beings equally: love is the wish that all beings be happy, and compassion is the wish that all beings be free from suffering. \"Many illnesses can be cured by the one medicine of love and compassion. These qualities are the ultimate source of human happiness, and the need for them lies at the very core of our being\" (Dalai Lama).\n\nStill, the notion of altruism is modified in such a world-view, since the belief is that such a practice promotes our own happiness: \"The more we care for the happiness of others, the greater our own sense of well-being becomes\" (Dalai Lama).\n\nIn the context of larger ethical discussions on moral action and judgment, Buddhism is characterized by the belief that negative (unhappy) consequences of our actions derive not from punishment or correction based on moral judgment, but from the law of karma, which functions like a natural law of cause and effect. A simple illustration of such cause and effect is the case of experiencing the effects of what one causes: if one causes suffering, then as a natural consequence one would experience suffering; if one causes happiness, then as a natural consequence one would experience happiness.\n\nThe fundamental principles of Jainism revolve around the concept of altruism, not only for humans but for all sentient beings. Jainism preaches the view of \"Ahimsa\" – to live and let live, thereby not harming sentient beings, i.e. uncompromising reverence for all life. It also considers all living things to be equal. The first Tirthankara, Rishabhdev, introduced the concept of altruism for all living beings, from extending knowledge and experience to others to donation, giving oneself up for others, non-violence and compassion for all living things.\n\nJainism prescribes a path of non-violence to progress the soul to this ultimate goal. A major characteristic of Jain belief is the emphasis on the consequences of not only physical but also mental behaviors. One's unconquered mind with anger, pride (ego), deceit, greed and uncontrolled sense organs are the powerful enemies of humans. Anger spoils good relations, pride destroys humility, deceit destroys peace and greed destroys everything. Jainism recommends conquering anger by forgiveness, pride by humility, deceit by straightforwardness and greed by contentment.\n\nJains believe that to attain enlightenment and ultimately liberation, one must practice the following ethical principles (major vows) in thought, speech and action. The degree to which these principles are practiced is different for householders and monks. They are:\nThe \"great vows\" (Mahavrata) are prescribed for monks and \"limited vows\" (Anuvrata) are prescribed for householders. The house-holders are encouraged to practice the above-mentioned five vows. The monks have to observe them very strictly. With consistent practice, it will be possible to overcome the limitations gradually, accelerating the spiritual progress.\n\nThe principle of non-violence seeks to minimize karmas which limit the capabilities of the soul. Jainism views every soul as worthy of respect because it has the potential to become \"Siddha\" (God in Jainism). Because all living beings possess a soul, great care and awareness is essential in one's actions. Jainism emphasizes the equality of all life, advocating harmlessness towards all, whether the creatures are great or small. This policy extends even to microscopic organisms. Jainism acknowledges that every person has different capabilities and capacities to practice and therefore accepts different levels of compliance for ascetics and householders.\n\nAltruism is central to the teachings of Jesus found in the Gospel, especially in the Sermon on the Mount and the Sermon on the Plain. From biblical to medieval Christian traditions, tensions between self-affirmation and other-regard were sometimes discussed under the heading of \"disinterested love\", as in the Pauline phrase \"love seeks not its own interests.\" In his book \"Indoctrination and Self-deception,\" Roderick Hindery tries to shed light on these tensions by contrasting them with impostors of authentic self-affirmation and altruism, by analysis of other-regard within creative individuation of the self, and by contrasting love for the few with love for the many. Love confirms others in their freedom, shuns propaganda and masks, assures others of its presence, and is ultimately confirmed not by mere declarations from others, but by each person's experience and practice from within. As in practical arts, the presence and meaning of love becomes validated and grasped not by words and reflections alone, but in the making of the connection.\n\nSt Thomas Aquinas interprets 'You should love your neighbour as yourself' as meaning that love for ourselves is the exemplar of love for others. Considering that \"the love with which a man loves himself is the form and root of friendship\" and quotes Aristotle that \"the origin of friendly relations with others lies in our relations to ourselves,\" he concluded that though we are not bound to love others more than ourselves, we naturally seek the common good, the good of the whole, more than any private good, the good of a part. However, he thinks we should love God more than ourselves and our neighbours, and more than our bodily life—since the ultimate purpose of loving our neighbour is to share in eternal beatitude: a more desirable thing than bodily well being. In coining the word Altruism, as stated above, Comte was probably opposing this Thomistic doctrine, which is present in some theological schools within Catholicism.\n\nMany biblical authors draw a strong connection between love of others and love of God. 1 John 4 states that for one to love God one must love his fellowman, and that hatred of one's fellowman is the same as hatred of God. Thomas Jay Oord has argued in several books that altruism is but one possible form of love. An altruistic action is not always a loving action. Oord defines altruism as acting for the other's good, and he agrees with feminists who note that sometimes love requires acting for one's own good when the other's demands undermine overall well-being.\n\nGerman philosopher Max Scheler distinguishes two ways in which the strong can help the weak. One way is a sincere expression of Christian love, \"motivated by a powerful feeling of security, strength, and inner salvation, of the invincible fullness of one’s own life and existence\". Another way is merely \"one of the many modern substitutes for love, ... nothing but the urge to turn away from oneself and to lose oneself in other people’s business.\" At its worst, Scheler says, \"love for the small, the poor, the weak, and the oppressed is really disguised hatred, repressed envy, an impulse to detract, etc., directed against the opposite phenomena: wealth, strength, power, largesse.\"\n\nIn Islam, the concept 'ithaar' (إيثار) (altruism) is the notion of 'preferring others to oneself'. For Sufis, this means devotion to others through complete forgetfulness of one's own concerns, where concern for others is rooted to be a demand made by ALLAH on the human body, considered to be property of ALLAH alone. The importance lies in sacrifice for the sake of the greater good; Islam considers those practicing Eyaar as abiding by the highest degree of nobility.\nThis is similar to the notion of chivalry, but unlike that European concept, in i'thar attention is focused on everything in existence. A constant concern for ALLAH (i.e. God) results in a careful attitude towards people, animals, and other things in this world.\nThis concept was emphasized by Sufis of Islam like Rabia al-Adawiyya who paid attention to the difference between dedication to ALLAH (i.e. God) and dedication to people. Thirteenth-century Turkish Sufi poet Yunus Emre explained this philosophy as \"Yaratılanı severiz, Yaratandan ötürü\" or \"We love the creature, because of The Creator.\" For many Muslims, i'thar must be practiced as a religious obligation during specific Islamic holidays. However, i'thar is also still an Islamic ideal to which all Muslims should strive to adhere at all times.\n\nJudaism defines altruism as the desired goal of creation. The famous Rabbi Abraham Isaac Kook stated that love is the most important attribute in humanity. This is defined as bestowal, or giving, which is the intention of altruism. This can be altruism towards humanity that leads to altruism towards the creator or God. Kabbalah defines God as the force of giving in existence. Rabbi Moshe Chaim Luzzatto in particular focused on the 'purpose of creation' and how the will of God was to bring creation into perfection and adhesion with this upper force.\n\nModern Kabbalah developed by Rabbi Yehuda Ashlag, in his writings about the future generation, focuses on how society could achieve an altruistic social framework. Ashlag proposed that such a framework is the purpose of creation, and everything that happens is to raise humanity to the level of altruism, love for one another. Ashlag focused on society and its relation to divinity.\n\nAltruism is essential to the Sikh religion. The central faith in Sikhism is that the greatest deed any one can do is to imbibe and live the godly qualities like love, affection, sacrifice, patience, harmony, truthfulness. The concept of \"seva,\" or selfless service to the community for its own sake is an important concept in Sihkism. \n\nThe fifth Nanak, Guru Arjun Dev, sacrificed his life to uphold 22 carats of pure truth, the greatest gift to humanity, the Guru Granth. The ninth Guru, Tegh Bahadur, sacrificed his head to protect weak and defenseless people against atrocity. In the late seventeenth century, Guru Gobind Singh Ji (the tenth guru in Sikhism), was in war with the Mughal rulers to protect the people of different faiths when a fellow Sikh, Bhai Kanhaiya, attended the troops of the enemy. He gave water to both friends and foes who were wounded on the battlefield. Some of the enemy began to fight again and some Sikh warriors were annoyed by Bhai Kanhaiya as he was helping their enemy. Sikh soldiers brought Bhai Kanhaiya before Guru Gobind Singh Ji, and complained of his action that they considered counter-productive to their struggle on the battlefield.\"What were you doing, and why?\" asked the Guru. \"I was giving water to the wounded because I saw your face in all of them,\" replied Bhai Kanhaiya. The Guru responded, \"Then you should also give them ointment to heal their wounds. You were practicing what you were coached in the house of the Guru.\"\n\nIt was under the tutelage of the Guru that Bhai Kanhaiya subsequently founded a volunteer corps for altruism. This volunteer corps still to date is engaged in doing good to others and trains new volunteering recruits for doing the same.\n\nIn Hinduism Selflessness (Atmatyag), Love (Prema), Kindness (Daya) and Forgiveness (Kshama) are considered as the highest acts of humanity or \"Manushyattva\". Giving alms to the beggers or poor people is considered as a divine act or \"Punya\" and Hindus believe it will free their souls from guilt or \"Paapa\" and will led them to heaven or \"Swarga\" in afterlife. Altruism is also the central act of various Hindu mythology and religious poems and songs.\n\nSwami Vivekananda, the legendary Hindu monk, has said -\"Jive prem kare jeijon, Seijon sebiche Iswar\" (Whoever loves any living being, is serving god.). Mass donation of clothes to poor people (Vastraseva), or blood donation camp or mass food donation (Annaseva) for poor people is common in various Hindu religious ceremonies.\n\nSwami Sivananda, an Advaita scholar, reiterates the views in his commentary synthesising Vedanta views on the Brahma Sutras, a Vedantic text. In his commentary on Chapter 3 of the Brahma Sutras, Sivananda notes that karma is insentient and short-lived, and ceases to exist as soon as a deed is executed. Hence, karma cannot bestow the fruits of actions at a future date according to one's merit. Furthermore, one cannot argue that karma generates apurva or punya, which gives fruit. Since apurva is non-sentient, it cannot act unless moved by an intelligent being such as a god. It cannot independently bestow reward or punishment.\n\nHowever the very well known and popular text, the Bhagavad Gita supports the doctrine of karma yoga (achieving oneness with God through action) & \"nishkaama karma\" or action without expectation / desire for personal gain which can be said to encompass altruism. Altruistic acts are generally celebrated and very well received in Hindu literature and is central to Hindu morality.\n\nThere exists a wide range of philosophical views on humans' obligations or motivations to act altruistically. Proponents of ethical altruism maintain that individuals are morally obligated to act altruistically. The opposing view is ethical egoism, which maintains that moral agents should always act in their own self-interest. Both ethical altruism and ethical egoism contrast with utilitarianism, which maintains that each agent should act in order to maximise the efficacy of their function and the benefit to both themselves and their co-inhabitants.\n\nA related concept in descriptive ethics is psychological egoism, the thesis that humans always act in their own self-interest and that true altruism is impossible. Rational egoism is the view that rationality consists in acting in one's self-interest (without specifying how this affects one's moral obligations).\n\nThe genes OXTR, CD38, COMT, DRD4, DRD5, IGF2, GABRB2 have been found to be candidate genes for altruism.\n\n"}
{"id": "91869", "url": "https://en.wikipedia.org/wiki?curid=91869", "title": "Ascribed characteristics", "text": "Ascribed characteristics\n\nAscribed characteristics, as used in the social sciences, refers to properties of an individual attained at birth, by inheritance, or through the aging process. The individual has very little, if any, control over these characteristics. Typical examples include race, ethnicity, gender, caste, height, and appearance. The term is apt for describing characteristics chiefly caused by \"nature\" (e.g. genetics) and for those chiefly caused by \"nurture\" (e.g. parenting during early childhood), See: Nature versus Nurture.\n\nDemography being the statistical study of populations requires the ability to differentiate between populations. Most populations self-ascribe themselves as being different than another by the creation of a country. This allows Demographers to draw lines between countries and compare them. However useful, countries have significant complex populations that require more exacting definitions. Commonly seen are uses of Race, Gender and Ethnicity. In the scope of academic demography, all of these are Social Constructs, ASCRIBED to groups or individuals for Stratification . With these theories in place, many new theories can be formed and data collected to either prove or disprove them. Ascribed characteristics can have large by-products weather perceived or not. Discussed below are Race, Gender, Social Status / Caste and Hiring / Promotion.\n\nAscribed characteristics are not always used for academic purposes. People with certain ascribed characteristics can be systematically treated with prejudice. Thus, the study of racism can be seen, at least superficially, as the study of the ways that people with a certain skin color and cultural background are systematically treated differently by society at large. Race does not entirely have a \n\nFrank van Tubergen studied the how ascribed characteristics and achieved characteristics effect their Social Capital in the article \"Personal networks in Saudi Arabia: The role of ascribed and achieved characteristics\". The article highlighted that women have less social capital than men in Saudi Arabia citing specifically that it was \"due to fewer non-family connections\". There are many arguments that stem from disagreements over the definition of what is a fact when it comes to gender, showing the fluidity of ascribed characteristics. For example, people who find homosexuality morally objectionable may attempt to justify this by insisting that homosexuals make a conscious decision about the nature of the sexual desire they experience however, it would difficult to condemn homosexuality if homosexuality was predetermined, either genetically or from early childhood. (See Sexual orientation.). Both Groups do however use the term all the same, simply ascribing different definitions to those individuals.\n\nMany different societies have had varying types of social stratification both historically and in the modern era. One of the most obvious examples is India, and its Caste System. In its essence, it was a system that ascribed sweepers the lowest status, making this one group literally untouchable. Although India officially states that discrimination against lower castes is illegal. \n\nRonald P. Dore was a British sociologist that was a specialist in the Japanese Economy. His view will be expressed here. In his largest work, \"British Factory, Japanese Factory\" , Dore investigates whether decisions on hiring and promotion, in the Japanese firm Hitachi, over a particular time were based chiefly on \"achievement\" or chiefly on \"ascribed characteristics\". The context of the discussion implied that achievement-based decisions are good, while those based on ascribed characteristics are bad. His discussion admits explicitly and, implicitly, that there are several complications to moral judgement that include:\n\n\nDore also points out that what counts as an ascribed characteristic can vary depending on context. In evaluating the fairness of hiring standards, he viewed an applicant's success in the educational system as a good approximation of achievement. Thus, he noted that hiring decisions at Hitachi, during the time of his study, were \"regulated by very strict qualification standards\" and not very significantly influenced by ascribed characteristics. When he turned to evaluate opportunities for advancement within the firm, however, Dore noted that \"educational qualifications...limit the range of posts which one can achieve\". Meaning even if one's level of achievement increases, one may still be kept down by a relative lack of achievement in the educational system. Therefore, in investigating opportunities for promotions, educational achievement \" the two become another form of ascribed characteristic.\" These additional forms of ascribed characteristics expand on the definition of an ascribed characteristic allowing for it to have more applications. \n"}
{"id": "51505200", "url": "https://en.wikipedia.org/wiki?curid=51505200", "title": "Black Girl Magic", "text": "Black Girl Magic\n\nBlack Girl Magic (#BlackGirlMagic) is a concept and movement that was popularized by CaShawn Thompson in 2013. The concept was born as a way to \"celebrate the beauty, power and resilience of black women\", as described by Julee Wilson from \"HuffPost\", and to congratulate black women on their accomplishments. Referring to a speech made by Michelle Obama at the Black Girls Rock Awards, Thompson explains that black women around the world persevering despite adversity inspired her to spread the concept of Black Girl Magic. With these women in mind, Thompson created the social media hashtag, clothing campaign, and rallying cry \"Black Girl Magic\", in the hopes of counteracting negativity society places on black women.\n\nThough born online, the movement has inspired many organizations across the world to host events using the title. The movement has also seen celebrity support, as singers Corinne Bailey Rae, Janelle Monáe and Solange Knowles have invoked the concept, and ballerina Misty Copeland and President Barack Obama discussed the idea in an interview with Maya Rhodan for \"Time\" and \"Essence\" magazines.\n\nSince being popularized, the concept has also gained traction in cultural criticism, invoked in analysis of music and film. As its usage has grown, the expression has drawn criticism as well as staunch defenders.\n\nIn 2016, poet Mahogany L. Browne created a slam poetry piece entitled \"Black Girl Magic\".\n\nIn 2013, Thompson coined the phrase via the hashtag #BlackGirlsAreMagic— now shortened to #BlackGirlMagic—to create an online dialogue centered around the achievements of black women, in a society that has historically recognized very few of these achievements. Since being popularized, one can find the hashtag being used on Twitter, Instagram, Facebook, and other social media and mainstream media platforms, celebrating positive messages and images of black women all across the globe.\n\nIn January 2014, Thompson began selling t-shirts sporting the \"Black Girls Are Magic\" logo that she created with her friend. At the time, Thompson only meant to sell these shirts to her friends and family, but has since sold over 3,000 T-shirts through her Teespring account. While the popularity of the shirts started among active social media users, Thompson was quickly surprised to see that people were also buying them as gifts for granddaughters, daughters, and nieces, as they felt that the message was encouraging for young girls too.\n\nThe shirts have also gained some celebrity recognition, with prominent young black women such as Willow Smith and Amandla Stenberg posting pictures of themselves on various social media platforms wearing their Black Girls Are Magic gear.\n\n\"I say 'magic' because it's something that people don't always understand,\" Thompson told \"The Los Angeles Times\". She went on to explain how \"Sometimes our accomplishments might seem to come out of thin air, because a lot of times, the only people supporting us are other black women.\" At its core, the purpose of this movement is to create a platform where women of color can stand together against the stereotyping, colourism, misogynoir and racism that is often their lived experience.\n\nAs its usage has grown, the concept has also drawn criticism.\n\nWhile many black women support the concept of Black Girl Magic, some feel it reinforces the \"strong black woman\" archetype that black women often confront. In an article for Elle Magazine, Linda Chavers argued that the movement suggests that black women are superhuman, or something other than human. She goes on to explain how, historically, black women have been seen and treated as subhuman beings, and how the image of black women persevering despite her suffering, is the epitome of the strong black woman type that is often celebrated while simultaneously being criticized in today's culture.\n\nIn 2016, Black Girl Magic Ltd was established as an nonprofit organization in an attempt to create a legacy and continuity throughout the UK and Europe.\n\n"}
{"id": "4491426", "url": "https://en.wikipedia.org/wiki?curid=4491426", "title": "Buttered cat paradox", "text": "Buttered cat paradox\n\nThe buttered cat paradox is a common joke based on the tongue-in-cheek combination of two adages:\n\n\nThe paradox arises when one considers what would happen if one attached a piece of buttered toast (butter side up) to the back of a cat, then dropped the cat from a large height. The buttered cat paradox, submitted by artist John Frazee of Kingston, New York, won a 1993 \"OMNI\" magazine competition about paradoxes. The basic premise, stating the conditions of the cat and bread and posed as a question, was presented in a routine by comic and juggler Michael Davis, appearing on \"The Tonight Show with Johnny Carson\", July 22, 1988.\n\nSome people jokingly maintain that the experiment will produce an anti-gravity effect. They propose that as the cat falls towards the ground, it will slow down and start to rotate, eventually reaching a steady state of hovering a short distance from the ground while rotating at high speed as both the buttered side of the toast and the cat’s feet attempt to land on the ground.\nIn June 2003, Kimberly Miner won a Student Academy Award for her film \"Perpetual Motion\". Miner based her film on a paper written by a high-school friend that explored the potential implications of the cat and buttered toast idea.\n\nThe faux paradox has captured the imagination of science-oriented humorists. In May 1992, the Usenet Oracle Digest #441 included a question from a supplicant asking about the paradox. Testing the theory is the main theme in an episode of the comic book strip \"Jack B. Quick\", the title character seeks to test this theory, leading to the cat hovering above the ground, with the cat's wagging tail providing propulsion. The March 31, 2005, strip of the webcomic \"Bunny\" also explored the idea in the guise of a plan for a \"Perpetual Motion MoggieToast 5k Power Generator\", based on Sod's law. In \"Science Askew\", Donald E. Simanek comments on this phenomenon.\n\nThe idea appeared on the British panel game \"QI\", where the idea was discussed. As well as talking about the idea, they also brought up other questions regarding the paradox. These included:\n\nThe paradox also appeared in the episode \"Gravitational Anarchy\" of the scientific podcast \"Radiolab\". Later, a humoristic explainer animation was put together by the animated production company Barq, based on an extracted audio clip from the \"Radiolab\" episode.\n\nBrazilian energy drink brand Flying Horse has released an award-winning commercial that simulates the recreation of this phenomenon, which is then used to create perpetual energy.\n\nIt also appeared in a comics series called \"Kid Paddle\" where Kid tells the story to his gullible friend Horace while at the dinner table. The comic is fairly popular in France and Belgium.\n\nIn reality, cats possess the ability to turn themselves right side up in mid-air if they should fall upside-down, known as the cat righting reflex. This enables them to land on their feet if dropped from sufficient height, about .\n\nToast, being an inanimate object, lacks both the ability and the desire to right itself. A study at Manchester Metropolitan University involving dropping 100 slices under laboratory conditions established that toast typically lands on the floor butter-side-down as a result of the manner in which it is typically dropped from a table, and the aerodynamic drag caused by the air pockets within the bread. The toast is typically butter-side-up when dropped. As it falls, it rotates; given the typical speed of rotation and the typical height of a table, a slice of toast that began butter-side-up on the table will land butter-side-down on the floor in 81% of cases.\n\n\n"}
{"id": "8902379", "url": "https://en.wikipedia.org/wiki?curid=8902379", "title": "Charles Otis Whitman", "text": "Charles Otis Whitman\n\nCharles Otis Whitman (December 6, 1842 – December 14, 1910) was an American zoologist, who was influential to the founding of classical ethology. A dedicated educator who preferred to teach a few research students at a time, he made major contributions in the areas of evolution and embryology of worms, comparative anatomy, heredity, and animal behaviour. He was known as the \"Father of Zoology\" in Japan.\n\nWhitman was born in Woodstock, Maine. His parents were Adventist pacifists and prevented his efforts to enlist in the Union army in 1862. He worked as a part-time teacher and converted to Unitarianism. He graduated from Bowdoin College in 1868. Following graduation, Whitman became principal of the Westford Academy, a small Unitarian-oriented college preparatory school outside Lowell, Massachusetts. In 1872 he moved to Boston and after becoming a member of the Boston Society of Natural History in 1874, he decided to study zoology full-time. In 1875, he took a leave of absence and went to the University of Leipzig in Germany to complete a Ph.D. which he obtained in 1878.\n\nA year later he received a postdoctoral fellowship at the Johns Hopkins University, but immediately gave it up when after recommended by noted biologist Edward Sylvester Morse, he was hired by the Japanese government to succeed Morse as professor at the Tokyo Imperial University from 1879-1881. Influenced by his training in Germany, he introduced systematic methods of biological research, including the use of the microscope.\n\nAfter leaving Japan, Whitman performed research at the Naples Zoological Station (1882), became an assistant at the Museum of Comparative Zoology, Harvard University (1883–5), then directed the Allis Lake Laboratory, in Milwaukee (1886–9), where he founded the \"Journal of Morphology\" (1887).\n\nIn 1884, Whitman married Emily Nunn. He moved to Clark University (Worcester, Massachusetts) (1889–92), then became a professor and curator of the Zoological Museum at the University of Chicago (1892–1910), while concurrently serving as founding director of the Marine Biological Laboratory, Woods Hole, Massachusetts (1888–1908). During the 1880s, Whitman established himself as the central figure of academic biology in the United States. He systematized the procedures that European anatomists and zoologists had gradually developed over the past two decades.\n\nOver the course of his career, Whitman worked with more than 700 species of pigeons, studying the relationship between phenotypic variation and heredity. By the turn of the 20th century, the last group of passenger pigeons, all descended from the same pair, was kept by Whitman at the University of Chicago. The last attempt to breed the remaining specimens was done by Whitman and the Cincinnati Zoo, which included attempts at making a rock dove foster Passenger Pigeon eggs. Whitman sent Martha, which was to be the last known specimen, to Cincinnati Zoo in 1902.\n\nIn December 1910, however, he caught a chill and died a few days later.\n\nWhitman was a non-Darwinian evolutionist. Stephen Jay Gould wrote that Whitman did not believe in Lamarckism, Darwinism or mutationism, instead Whitman was an advocate of orthogenesis. Whitman only wrote one book on orthogenesis which was published nine years after his death in 1919 titled \"Orthogenetic evolution in pigeons\" the book was published in a three volume set titled \"Posthumous Works of Charles Otis Whitman\", Gould claims that the book was written \"too late, to win any potential influence\".\n\n\n\n"}
{"id": "25707018", "url": "https://en.wikipedia.org/wiki?curid=25707018", "title": "Concision", "text": "Concision\n\nConcision (alternatively brevity, laconicism, terseness, or conciseness) is the cutting out of unnecessary words while conveying an idea. It aims to enhance communication by eliminating redundancy without omitting important information. Concision has been described as one of the elementary principles of writing. The related concept of succinctness is the opposite of verbosity.\n\nConcision means to be economical with words, expressing what's needed using the fewest words necessary. That may involve removing redundant or unnecessary phrases or replacing them with shorter ones. It is described in \"The Elements of Style\" by Strunk and White as follows:\n\nConcision has also been described as \"eliminat[ing] words that take up space without saying much.\" Simple examples include replacing \"\" with \"because\" or \"at this point in time\" with \"now\" or \"currently.\"\n\nAn example sentence, with explanation:\n\nThe following example is taken from:\n\nThe source suggests this replacement:\n\nIn the second quote, the same information is communicated in less than half the length. However, it could be more concisely rewritten and communicate the same information:\n\nConcise expression, particularly in writing, is considered one of the basic goals of teaching the English language. Techniques to achieve concise writing are taught for students at all levels, from the introduction to writing to the preparation of PhD dissertations, and legal writing for law students.\n\nIt has been argued that although \"in expository prose English places a high value on conciseness... [t]he value placed on conciseness... is not shared by all cultures\", with, for example, the Thai culture as one where redundancy is prized as an opportunity to use additional words to demonstrate the writer's command of the language. This may lead to a tendency for people from those cultures to use repetitive or redundant phrasing when learning English.\n\nThe related concept of succinctness is a characteristic of speech, writing, data structure, algorithmic games, and thought in general, exhibiting both clarity and brevity. It is the opposite of verbosity, in which there is an excess of words.\n\nBrevity in succinctness is not achieved by shortening original material by coding or compressing it, but rather by omitting redundant material from it.\n\n\n"}
{"id": "55838656", "url": "https://en.wikipedia.org/wiki?curid=55838656", "title": "Culture of violence theory", "text": "Culture of violence theory\n\nThe culture of violence theory addresses the pervasiveness of specific violent patterns within in a societal dimension. The concept of violence being ingrained in Western society and culture has been around for at least the past century. Developed from structural violence, as research progressed the notion that a culture can sanction violent acts developed into what we know as culture of violence theory today. Two prominent examples of culture legitimizing violence can be seen in rape myths and victim blaming. Rape myths lead to misconstrued notions of blame; it is common for the responsibility associated with the rape to be placed on the victim rather than the offender.\n\nFurthermore, the culture of violence theory potentially accounts for inter-generational theories of violence and domestic violence. Childhood exposure to violence in the household may later lead to similar patterns in marital relations. Similarly, early experience with domestic violence is likely to increase an individual's potential for development of clinical symptoms. Additionally, presence of a preexisting mental disorder may heighten the chances of becoming involved in an abusive relationship.\n\nThere are many factors which contribute to the persistence of violence among individuals and on a societal level; gender is one relevant factor to understanding the culture of violence theory. In the United States, a majority of reported rapes involve female victims. However, there is a growing body of evidence to support the notion that women can perpetuate relational cycles of violence. While a culture of violence has an impact on people as a whole, for individuals who have experienced trauma in their lives the impact can be much larger.\n\nAs mentioned previously the culture of violence theory addresses the pervasiveness of specific violent patterns within in a societal dimension. Specifically, culture of violence theory explains how cultures and societies can sanction violent acts. While related to structural violence, cultural violence theory is different by explaining why direct acts of violence or violence built into systems of society exists and how they are legitimized. Research suggests that cultures can encourage and permit violence to exist as a response to various environmental obstacles, such as widespread resource impoverishment. This can be seen within various aspects of culture, such as film, television, music, language, art, and propaganda.\n\nRape myths refer to the inaccurate views and stereotypes of forced sexual acts, and the victims and perpetuators of them. These notions are prevalent among the general population and often suggest that the victims of non-consensual sexual acts have bad reputations, are promiscuous, dress provocatively, or are fabricating assault when they regret the consensual acts after the fact. These views are often legitimized by the status quo of men dominating women across domains such as family, education, work, and many others. Rape myth acceptance can lead to poor assault/rape prevention measures, decrease in reporting of assaults/rapes, increases of assaults/rapes, and re-victimization.\n\nViolence in relationships, commonly referred to as intimate partner violence (IPV), is impacted by various factors including the presence of mental illness or use of substances. Specifically, individuals with depression, generalized anxiety (GAD), or panic disorder are potentially at risk for physical violence towards a partner; findings are consistent for both men and women regarding the connection between psychiatric diagnoses and perpetuation of relationship violence. Additionally, propensity to engage in specific behaviors such as gambling or endorsement of violent pornography have also been associated with increased risk for relationship violence occurrence. Individual factors have also been suggested to be associated with relationship violence including anger, aggressiveness, and adverse emotional internalization. Contrarily, exposure to relationship violence is also linked to the later development of mental health symptoms or diagnoses.\n\nThe prevalence of legitimization of violence may be facilitated by its presence in various media. There is evidence to suggest that sex-related crimes account for nearly 10% of all dialogue on television related to sex, most of which is found on fictional programs. Additionally, research has also found a positive relationship between pornography consumption and attitudes supporting violence against women, especially when the pornography in question is violent in nature. However, consideration of individual differences is necessary in evaluating exposure to violent media and overall outcomes. Factors which influence media content exposure and subsequent outcomes include gender and personality traits. Individuals who are male, hostile, impulsive, and are low on empathy are more likely to be susceptible to violent media exposure.\n\nPublic justification of violence arise when those not necessarily directly involved in the violent act will not react negatively to the violence because they believe it is warranted. Examples of public justification of violence are most evident in rape myths and victim blaming, as discussed above. However, the common belief regarding legitimate violence tends to place responsibility on victims or potential victims of violence. Another example that is not as often noted, is the pervasive notion of the \"chosen one,\" within some extremist religious language and various nationalism propaganda that will function as a means to perpetuate the undermining of the other and allowance of violence against the other.\n\nEarly childhood trauma, specifically exposure to abuse or violence, is linked to mental health disorder development. Witnessing domestic violence during childhood places individuals at a greater risk for developing psychological disorders such as depression. General functioning may be impacted in behavioral or emotional domains as a result of exposure to domestic violence as a child. Post-traumatic stress disorder (PTSD) is a widely supported potential resultant diagnosis of childhood trauma. More specifically, interpersonal trauma places children and adolescents at an increased risk for developing PTSD, with girls being the most susceptible. Furthermore, exposure to bullying has also been shown to induce symptoms of PTSD or be connected to a PTSD diagnosis. Bullying and trauma responses have been observed in both child and adult populations as well as across various environments including academic or professional settings.\n\nFor individuals who have experienced rape or sexual violence, interventions are implemented to address symptoms of trauma. Various types of therapeutic interventions, including cognitive-processing therapy, prolonged exposure therapy, and eye movement desensitization, have been utilized for trauma responses.\n"}
{"id": "913118", "url": "https://en.wikipedia.org/wiki?curid=913118", "title": "Decidability (logic)", "text": "Decidability (logic)\n\nIn logic, a true/false decision problem is decidable if there exists an effective method for deriving the correct answer (instead of looping indefinitely, crashing, returning \"don't know\", or returning a wrong answer). Logical systems such as propositional logic are decidable if membership in their set of logically valid formulas (or theorems) can be effectively determined. A theory (set of sentences closed under logical consequence) in a fixed logical system is decidable if there is an effective method for determining whether arbitrary formulas are included in the theory. Many important problems are undecidable, that is, it has been proven that no effective method for determining membership (returning a correct answer after finite, though possibly very long, time in all cases) can exist for them.\n\nAs with the concept of a decidable set, the definition of a decidable theory or logical system can be given either in terms of \"effective methods\" or in terms of \"computable functions\". These are generally considered equivalent per Church's thesis. Indeed, the proof that a logical system or theory is undecidable will use the formal definition of computability to show that an appropriate set is not a decidable set, and then invoke Church's thesis to show that the theory or logical system is not decidable by any effective method (Enderton 2001, pp. 206\"ff.\").\n\nEach logical system comes with both a syntactic component, which among other things determines the notion of provability, and a semantic component, which determines the notion of logical validity. The logically valid formulas of a system are sometimes called the theorems of the system, especially in the context of first-order logic where Gödel's completeness theorem establishes the equivalence of semantic and syntactic consequence. In other settings, such as linear logic, the syntactic consequence (provability) relation may be used to define the theorems of a system.\n\nA logical system is decidable if there is an effective method for determining whether arbitrary formulas are theorems of the logical system. For example, propositional logic is decidable, because the truth-table method can be used to determine whether an arbitrary propositional formula is logically valid.\n\nFirst-order logic is not decidable in general; in particular, the set of logical validities in any signature that includes equality and at least one other predicate with two or more arguments is not decidable. Logical systems extending first-order logic, such as second-order logic and type theory, are also undecidable.\n\nThe validities of monadic predicate calculus with identity are decidable, however. This system is first-order logic restricted to signatures that have no function symbols and whose relation symbols other than equality never take more than one argument.\n\nSome logical systems are not adequately represented by the set of theorems alone. (For example, Kleene's logic has no theorems at all.) In such cases, alternative definitions of decidability of a logical system are often used, which ask for an effective method for determining something more general than just validity of formulas; for instance, validity of sequents, or the consequence relation {(Г, \"A\") | Г ⊧ \"A\"} of the logic.\n\nA theory is a set of formulas, which here is assumed to be closed under logical consequence. The question of decidability for a theory is whether there is an effective procedure that, given an arbitrary formula in the signature of the theory, decides whether the formula is a member of the theory or not. This problem arises naturally when a theory is defined as the set of logical consequences of a fixed set of axioms. Examples of decidable first-order theories include the theory of real closed fields, and Presburger arithmetic, while the theory of groups and Robinson arithmetic are examples of undecidable theories.\n\nThere are several basic results about decidability of theories. Every inconsistent theory is decidable, as every formula in the signature of the theory will be a logical consequence of, and thus a member of, the theory. Every complete recursively enumerable first-order theory is decidable. An extension of a decidable theory may not be decidable. For example, there are undecidable theories in propositional logic, although the set of validities (the smallest theory) is decidable.\n\nA consistent theory that has the property that every consistent extension is undecidable is said to be essentially undecidable. In fact, every consistent extension will be essentially undecidable. The theory of fields is undecidable but not essentially undecidable. Robinson arithmetic is known to be essentially undecidable, and thus every consistent theory that includes or interprets Robinson arithmetic is also (essentially) undecidable.\n\nSome decidable theories include (Monk 1976, p. 234):\n\nMethods used to establish decidability include quantifier elimination, model completeness, and Vaught's test.\n\nSome games have been classified as to their decidability:\n\nSome undecidable theories include (Monk 1976, p. 279):\nThe interpretability method is often used to establish undecidability of theories. If an essentially undecidable theory \"T\" is interpretable in a consistent theory \"S\", then \"S\" is also essentially undecidable. This is closely related to the concept of a many-one reduction in computability theory.\n\nA property of a theory or logical system weaker than decidability is semidecidability. A theory is semidecidable if there is an effective method which, given an arbitrary formula, will always tell correctly when the formula is in the theory, but may give either a negative answer or no answer at all when the formula is not in the theory. A logical system is semidecidable if there is an effective method for generating theorems (and only theorems) such that every theorem will eventually be generated. This is different from decidability because in a semidecidable system there may be no effective procedure for checking that a formula is \"not\" a theorem.\n\nEvery decidable theory or logical system is semidecidable, but in general the converse is not true; a theory is decidable if and only if both it and its complement are semi-decidable. For example, the set of logical validities \"V\" of first-order logic is semi-decidable, but not decidable. In this case, it is because there is no effective method for determining for an arbitrary formula \"A\" whether \"A\" is not in \"V\". Similarly, the set of logical consequences of any recursively enumerable set of first-order axioms is semidecidable. Many of the examples of undecidable first-order theories given above are of this form.\n\nDecidability should not be confused with completeness. For example, the theory of algebraically closed fields is decidable but incomplete, whereas the set of all true first-order statements about nonnegative integers in the language with + and × is complete but undecidable. \nUnfortunately, as a terminological ambiguity, the term \"undecidable statement\" is sometimes used as a synonym for independent statement.\n\n\n"}
{"id": "2426628", "url": "https://en.wikipedia.org/wiki?curid=2426628", "title": "Empathy gap", "text": "Empathy gap\n\nA hot-cold empathy gap is a cognitive bias in which people underestimate the influences of visceral drives on their own attitudes, preferences, and behaviors.\n\nThe most important aspect of this idea is that human understanding is \"state-dependent\". For example, when one is angry, it is difficult to understand what it is like for one to be calm, and vice versa; when one is blindly in love with someone, it is difficult to understand what it is like for one not to be, (or to imagine the possibility of not being blindly in love in the future). Importantly, an inability to minimize one's gap in empathy can lead to negative outcomes in medical settings (e.g., when a doctor needs to accurately diagnose the physical pain of a patient), and in workplace settings (e.g., when an employer needs to assess the need for an employee's bereavement leave).\n\nHot-cold empathy gaps can be analyzed according to their direction:\n\nThey can also be classified in regards to their relation with time (past or future) and whether they occur intra- or inter-personally:\n\nThe term \"hot-cold empathy gap\" was coined by Carnegie Mellon University psychologist, George Loewenstein. Hot-cold empathy gaps are one of Loewenstein's major contributions to behavioral economics.\n\nVisceral factors are an array of influences which include hunger, thirst, sexual arousal, drug cravings for the drugs one is addicted to, physical pain, and strong emotions. These drives have a disproportionate effect on decision making and behavior: the mind, when affected (i.e., in a hot state), tends to ignore all other goals in an effort to placate these influences. These states can lead a person to feel \"out of control\" and act impulsively.\n\nHot-cold empathy gap is also dependent on the person's memory of visceral experience. As such, it is very common to underestimate visceral state due to restrictive memory. In general, people are more likely to underestimate the effect of pain in a cold state as compared to those in the hot state.\n\nNordgren, van der Pligt and van Harreveld (2006) assessed the impact of pain on the subjects performance on a memory test. In the assessment process, participants were questioned how pain and other factors affected their performance.\n\nThe results revealed that those participants in the pain free or cold state undervalued the impact of pain on their performance. Whereas, participants undergoing pain, accurately measured the effect of pain on performance.\n\nImplications of the empathy gap were explored in the realm of sexual decision-making, where young men in an unaroused \"cold state\" failed to predict that in an aroused \"hot state\" they will be more likely to make risky sexual decisions, (e.g., not using a condom).\n\nThe empathy gap has also been an important idea in research about the causes of bullying. In one study examining a central theory that, \"only by identifying with a victim’s social suffering can one understand its devastating effects,\" researchers created five experiments. The first four examined the degree to which participants in a game who were not excluded could estimate the social pain of participants who were excluded. The findings were that those who were not socially excluded consistently underestimated the pain felt by those who were excluded. A survey included in the study directed at teachers' opinions of school policy toward bullying found that those with an experience of social pain, caused by bullying, often rated the pain experienced by those facing bullying or social exclusion as higher than teachers who did not have such experience, and further, that teachers who had experienced social pain were more likely to punish students for bullying.\n\nGeorge F. Loewenstein explored visceral factors related to addictions like smoking. The factors have to do with drive states which are essential for living – for example, sleepiness and hunger. Somehow, addiction is miscategorized as an essential living drive state due to a behavior disorder. From the findings emerged new discoveries about hot and cold empathy gap and its important role in drug addictions, such as smoking.\n\nA study done in year 2008 explored empathy gap within smokers.\n\n98 smokers, from the age of 18 to 40 who smoked at least 10 cigarettes per day for their past 12 days and are not interested in quitting, were selected through paper advertisements. For the experiments, smokers were asked to shun smoking for two days. The participants started off from session one and then moved on to session two.\n\n\n\"Willingness to accept craving\" (WTAC) is a measurement based on the money that participants received for previous smoking research. The results indicate that the compensation demand increased from first session to the second session for those in controlled cue and decreased for those in the hot cue.\n\nThe cold cue participants underpredicted their monetary compensation to delay smoking whereas the hot cue participants overpredicted their monetary compensation. This shows the gap both groups in different stages of empathy. It can also lead to a prediction that they will be misinformed about high risk situations. For example, many smokers in parties will probably underestimate their consumption of smoking, however, the consumption may be higher than predicted for the smoker. Those who would like to quit smoking may find quitting easy, however during the time of quitting smoking, they might find it incredibly difficult to control the urge to smoke. High craving situations will lead to a higher chances for a person to smoke, whereas those who are not in the state of craving smoke will have no idea about how it is like to intensely crave smoking.\n\nThe bargaining games conclude that if one is completely powerless, the one proposing the offer to the powerless will lack strategy. Thus, the powerless will ironically receive higher outcomes. This is because of the egocentric empathy gap.\n\nIn general, people have difficulty taking perspectives of a typical situation and decision making. Often, attribution bias of false consensus is the reason why the overestimation of similar perspective occurs.\n\nVan Boven divided students in the experiment into sellers and buyers of the coffee mug. He collected the price that buyers are willing to pay and collected the price at which the sellers are willing to sell the mug. To test the empathy gap, the buyers of the coffee mug were also asked to predict the price that the sellers would have to offer.\n\nPrediction of the sellers and buyers were close to their own price proposed and depended on their own evaluation of the mug. This leads to a conclusion that empathy gap does exist since both parties were unable to evaluate the other party. When being the weaker party, an encouragement of being more strategic occurs since the weaker party fears that their outcome will be in threat. In fact, this leads to a decision more in favor of the weaker party.\n\nFurther conclusion can be made about empathy gap and power: the weaker party often doesn't realize that being in a weaker party can actually give them more power to strategically think and make decision, leading to better outcomes. The weaker party has no idea what they are capable of doing. They convince themselves that being more powerful is often more advantageous. Whereas the powerful party lacks strategy and leads to a poor outcome.\nTrial of experiments that held different combinations of the ultimatum game was done. The trials lead to an ultimate conclusion that participants like to be more powerful than to be powerless. Since the reasoning is not necessarily true due to attribution bias of false consensus, the powerful ones didn't abuse their power. Even though an assumption of being powerful leads to power's abuse, in reality it calls for more pro-social behavior and responsibility.\n\n\n"}
{"id": "16950168", "url": "https://en.wikipedia.org/wiki?curid=16950168", "title": "Essential dimension", "text": "Essential dimension\n\nIn mathematics, essential dimension is an invariant defined for certain algebraic structures such as algebraic groups and quadratic forms. It was introduced by J. Buhler and Z. Reichstein \nand in its most generality defined by A. Merkurjev.\nBasically, essential dimension measures the complexity of algebraic structures via their fields of definition. For example, a quadratic form q : V → K over a field K, where V is a K-vector space, is said to be defined over a subfield L of K if there exists a K-basis e...,e of V such that q can be expressed in the form formula_1 with all coefficients a belonging to L. If K has characteristic different from 2, every quadratic form is diagonalizable. Therefore, q has a field of definition generated by n elements. Technically, one always works over a (fixed) base field k and the fields K and L in consideration are supposed to contain k. The essential dimension of q is then defined as the least transcendence degree over k of a subfield L of K over which q is defined.\n\nFix an arbitrary field k and let \"Fields/k\" denote the category of finitely generated field extensions of k with inclusions as morphisms. Consider a (covariant) functor F : Fields/k → Set.\nFor a field extension K/k and an element \"a\" of F(K/k) a \"field of definition of a\" is an intermediate field K/L/k such that \"a\" is contained in the image of the map F(L/k) → F(K/k) induced by the inclusion of L in K.\n\nThe \"essential dimension of a\", denoted by \"ed(a)\", is the least transcendence degree (over k) of a field of definition for \"a\". The essential dimension of the functor F, denoted by \"ed(F)\", is the supremum of \"ed(a)\" taken over all elements \"a\" of F(K/k) and objects K/k of Fields/k.\n\n\n"}
{"id": "5517556", "url": "https://en.wikipedia.org/wiki?curid=5517556", "title": "Extension topology", "text": "Extension topology\n\nIn topology, a branch of mathematics, an extension topology is a topology placed on the disjoint union of a topological space and another set.\n\nThere are various types of extension topology, described in the sections below.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose open sets are of the form: A ∪ Q, where A is an open set of X and Q is a subset of P.\n\nNote that the closed sets of X ∪ P are of the form: B ∪ Q, where B is a closed set of X and Q is a subset of P.\n\nFor these reasons this topology is called the extension topology of X plus P, with which one extends to X ∪ P the open and the closed sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote the similitude of this extension topology construction and the Alexandroff one-point compactification, in which case, having a topological space X which one wishes to compactify by adding a point ∞ in infinity, one considers the closed sets of X ∪ {∞} to be the sets of the form: K, where K is a closed compact set of X, or B ∪ {∞}, where B is a closed set of X.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose open sets are of the form: X ∪ Q, where Q is a subset of P, or A, where A is an open set of X.\n\nFor this reason this topology is called the open extension topology of X plus P, with which one extends to X ∪ P the open sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nNote that the closed sets of X ∪ P are of the form: Q, where Q is a subset of P, or B ∪ P, where B is a closed set of X.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote that the open extension topology of X ∪ P is smaller than the extension topology of X ∪ P.\n\nBeing Z a set and p a point in Z, one obtains the excluded point topology construction by considering in Z the discrete topology and applying the open extension topology construction to Z - {p} plus p.\n\nLet X be a topological space and P a set disjoint from X. Consider in X ∪ P the topology whose closed sets are of the form: X ∪ Q, where Q is a subset of P, or B, where B is a closed set of X.\n\nFor this reason this topology is called the closed extension topology of X plus P, with which one extends to X ∪ P the closed sets of X. Note that the subspace topology of X as a subset of X ∪ P is the original topology of X, while the subspace topology of P as a subset of X ∪ P is the discrete topology.\n\nNote that the open sets of X ∪ P are of the form: Q, where Q is a subset of P, or A ∪ P, where A is an open set of X.\n\nBeing Y a topological space and R a subset of Y, one might ask whether the extension topology of Y - R plus R is the same as the original topology of Y, and the answer is in general no.\n\nNote that the closed extension topology of X ∪ P is smaller than the extension topology of X ∪ P.\n\nBeing Z a set and p a point in Z, one obtains the particular point topology construction by considering in Z the discrete topology and applying the closed extension topology construction to Z - {p} plus p.\n"}
{"id": "20310489", "url": "https://en.wikipedia.org/wiki?curid=20310489", "title": "Grinold and Kroner Model", "text": "Grinold and Kroner Model\n\nThe Grinold and Kroner Model is used to calculate expected returns for a stock, stock index or the market as whole. It is a part of a larger framework for making forecasts about market expectations.\n\nThe model states that:\n\nformula_1\n\nWhere\nOne offshoot of this discounted cash flow analysis is the Fed Model. Under the Fed model, the earnings yield is compared to the 10-year treasury bonds. If the earnings yield is lower than that of the bonds, the investor would shift their money into the less risky T-bonds.\n\nGrinold, Kroner, and Siegel (2011) estimated the inputs to the Grinold and Kroner model and arrived at a then-current equity risk premium estimate between 3.5% and 4%. The equity risk premium is the difference between the expected total return on a capitalization-weighted stock market index and the yield on a riskless government bond (in this case one with 10 years to maturity).\n"}
{"id": "47525166", "url": "https://en.wikipedia.org/wiki?curid=47525166", "title": "Information Coding Classification", "text": "Information Coding Classification\n\nThe Information Coding Classification (ICC) is a classification system covering almost all extant 6500 knowledge fields (knowledge domains). Its conceptualization goes beyond the scope of the well known library classification systems, such as Dewey Decimal Classification (DCC), Universal Decimal Classification (UDC), and Library of Congress Classification (LCC), by extending also to knowledge systems that so far have not afforded to classify literature. ICC actually presents a flexible universal ordering system for both literature and other kinds of information, set out as knowledge fields. From a methodological point of view, ICC differs from the above-mentioned systems along the following three lines:\nRespective knowledge fields permit to step down by the same principle to a third and forth level, and even further to a fifth and sixth level. Finally, knowledge field subdivisions will have to conform to said digital position scheme.\nHence, for a given knowledge field identical codes will mark identical categories under respective numbers of the coding system. This mnemotechnical aspect of the system helps memorizing and straightaway retrieving the whereabouts of respective interdisciplinary and transdisciplinary fields.\n\nThe first two hierarchical levels may be regarded as a top- or upper ontology for ontologies and other applications.\n\nThe terms of the first three hierarchical levels were set out in German and English in \"Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft\", on pp. 82 to 100. It was published in 2014 and available so far only in German. In the meantime, also the French terms of the knowlwdge fields have been collected.\nCompetence for maintenance and further development rests with the German Chapter of the \nInternational Society for Knowledge Organization (ISKO) e.V.\n\nAt the end of 1970, Prof. Alwin Diemer, Univ.of Düsseldorf proposed to Ingetraut Dahlberg to undertake a philosophical dissertation on \"The universal classification system of knowledge, its ontological, epistemological, and information theoretical foundations\". Diemer had in mind an innovating ontological approach for such a system based on the whole spectrum of kinds of being and complying with epistemological requirements. The third requirement had already been taken up somehow in the Indian Colon Classification, yet it still called for explanations and additions. In 1974, the dissertation was published in German entitled \"Grundlagen universaler Wissensordnung\". It started with conceptual clarifications, and why and how the term „universal“ was linked to knowledge, including knowledge fields, such as commodity science, artefacts, statistics, patents, standardization, communication, utility services et al. In chapter 3, six universal classification systems (DDC, UDC, LCC, BC, CC and BBK) were presented, analyzed and compared.\n\nWhile preparing the dissertation, Dahlberg started with elaborating the new universal system by first gleaning a lot of extant designations of knowledge fields from whatever available reference works. This was funded by the German Documentation Society (DGD) (1971-2) under the title of \"Order system of knowledge fields\". In addition, the syllabuses of German universities and polytechniques were explored for relevant terms and documented (1975). Thereafter, it seemed necessary to add definitions from special dictionaries and encyclopediae; it soon appeared that the 12.500 terms included numerous synonyms, so that the whole collection boiled down to about 6.500 concept designations (Project Logstruktur, supported by the German Science Foundation (DFG) 1976-78).\n\nThe outcome of this work was the formulation of 30 theses which ended up in 12 principles for the new system, published 40 years later under. These principles refer not only to theoretical foundations but also to structure and other organizational aspects of the whole array of knowledge fields. In 1974, the digital position scheme for field subdivision had already been developed to allow for classifying classification literature in the bibliographical section of the first issue of the Journal International Classification. In 1977, the entire ICC was ready for presentation at a seminar in Bangalore, India. A publication of the first three hierarchical levels appeared however only in 1982. It was applied to the bibliography of classification systems and thesauri in vol.1 of the International Classification and Indexing Bibliography; it has been updated.\n\nThese were published in full length in the book \"Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft\" and the article \"Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches\", hence it suffices to just mention their topics with some necessary additions.\n\n\n\nAn example of its application is the structure of the classification system for knowledge organization literature Gliederung der Klassifikationsliteratur. (A simplified version with an additional introduction is given in, p. 71)\n\n\nThe first two levels of ICC can be represented by following matrix.\n\nThe first hierarchical level of the 9 subject categories results from the first vertical array under codes 1-9. The second hierarchical level of subject categories is structured by the 9 functionally ordered form categories, listed in the first horizontal line under codes 01-09. Some exceptions are mentioned in principle 7.\n\nFor classifying web documents as conceived by Jens Hartmann, University of Karlsruhe, Prof.Walter Koch, University of Graz, has explored in his Institute for Applied Information Technology Research Society (AIT) the application of ICC to automatically classifying metadata of some 350.000 documents. This was facilitated by data generated within the framework of an EU-supported project \"EuropeanaLocal\" . For this exploration, three ICC hierarchical levels have been used for some 5000 terms. The result is described in the report of Christoph Mak. Prof.Koch regarded a classification degree of almost 50% as a good result, considering that only a shortened version of ICC had been used. In order to reach a better result one would have needed 1–2 years. Also an index of all terms with their codes could be achieved under these explorations.\n\nMotivated by the work of an Italian research Group in Trento on \"Revising the Wordnet Domains Hierarchy: semantics, coverage and balancing\", by which the DDC codes were used, Prof. Ernesto DeLuca et al. showed in a study that for such case the use of ICC could lead to essentially better results. This was shown in two contributions: \"Including knowledge domains from the ICC into the Multilingual Lexical Linked Data Cloud (LLD)\" and \"Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung?\", in which the LLD was used in a meta-model which contains all resources with the possibility of retrieval and navigation of data from different aspects. By this, the existing work about many thousand knowledge fields (of ICC) can be combined with the Multilingual Lexical Linked Data Cloud, based on RDF/OWL representation of EuroWordNet and similar integrated lexical resources (MultiWordNet, MEMODATA and the Hamburg Metapher BD).\n\nIn October 2013, the computer scientist Hermann Bense, Dortmund, explored the possibilities for structuring the Semanic Web with ICC codes. He developed two approaches for a pictorial presentation of knowledge fields with their possible subdivisions. A graphic representation of those knowledge fields pertaining to the first two levels can be found under Ontology4. The inclusion of the third hierarchical level has been envisaged as the next step.\n"}
{"id": "41698769", "url": "https://en.wikipedia.org/wiki?curid=41698769", "title": "K-anonymity", "text": "K-anonymity\n\n\"k\"-anonymity is a property possessed by certain anonymized data. The concept of \"k\"-anonymity was first introduced by Latanya Sweeney and Pierangela Samarati in a paper published in 1998 as an attempt to solve the problem: \"Given person-specific field-structured data, produce a release of the data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful.\" A release of data is said to have the \"k\"-anonymity property if the information for each person contained in the release cannot be distinguished from at least formula_1 individuals whose information also appear in the release.\n\nK-anonymity received wide-spread media coverage in 2018 when British computer scientist Junade Ali used the property alongside cryptographic hashing to create a communication protocol to anonymously verify if a password was leaked without disclosing the searched password. This protocol was implemented as a public API in Troy Hunt's Have I Been Pwned? service and is consumed by multiple services including Password Managers and browser extensions.\n\nIn the context of \"k\"-anonymization problems, a database is a table with \"n\" rows and \"m\" columns. Each row of the table represents a record relating to a specific member of a population and the entries in the various rows need not be unique. The values in the various columns are the values of attributes associated with the members of the population. The following table is a nonanonymized database consisting of the patient records of some fictitious hospital in Kochi. \n\nThere are 6 attributes and 10 records in this data. There are two common methods for achieving \"k\"-anonymity for some value of \"k\".\n\n\nThe next table shows the anonymized database.\n\nThis data has 2-anonymity with respect to the attributes 'Age', 'Gender' and 'State of domicile' since for any combination of these attributes found in any row of the table there are always at least 2 rows with those exact attributes. The attributes available to an adversary are called quasi-identifiers. Each quasi-identifier tuple occurs in at least \"k\" records for a dataset with \"k\"-anonymity.\n\nMeyerson and Williams (2004) demonstrated that optimal \"k\"-anonymity is an NP-hard problem, however heuristic methods such as \"k\"-Optimize as given by Bayardo and Agrawal (2005) often yields effective results. A practical approximation algorithm that enables solving the \"k\"-anonymization problem with an approximation guarantee of formula_2 was presented by Kenig and Tassa.\n\nBecause k-anonymization does not include any randomization, attackers can still make inferences about data sets that may harm individuals. For example, if the 19-year-old John from Kerala is known to be in the database above, then it can be reliably said that he has either cancer, a heart-related disease, or a viral infection.\n\n\"K\"-anonymization is not a good method to anonymize high-dimensional datasets. For example, researchers showed that, given 4 locations, the unicity of mobile phone timestamp-location datasets (formula_3, \"k\"-anonymity when formula_4) can be as high as 95%.\n\nIt has also been shown that \"k\"-anonymity can skew the results of a data set if it disproportionately suppresses and generalizes data points with unrepresentative characteristics. The suppression and generalization algorithms used to \"k\"-anonymize datasets can be altered, however, so that they do not have such a skewing effect.\n\n"}
{"id": "37604", "url": "https://en.wikipedia.org/wiki?curid=37604", "title": "Kanji", "text": "Kanji\n\nKanji (; ] ) are the adopted logographic Chinese characters that are used in the Japanese writing system. They are used alongside the Japanese syllabic scripts \"hiragana\" and \"katakana\". The Japanese term \"kanji\" for the Chinese characters literally means \"Han characters\". It is written with the same characters in the Chinese language to refer to the character writing system, \"hànzì\" ().\n\nChinese characters first came to Japan on official seals, letters, swords, coins, mirrors, and other decorative items imported from China. The earliest known instance of such an import was the King of Na gold seal given by Emperor Guangwu of Han to a Yamato emissary in 57 AD. Chinese coins from the first century AD have been found in Yayoi-period archaeological sites. However, the Japanese of that era probably had no comprehension of the script, and would remain illiterate until the fifth century AD. According to the \"Nihon Shoki\" and \"Kojiki\", a semi-legendary scholar called Wani () was dispatched to Japan by the Kingdom of Baekje during the reign of Emperor Ōjin in the early fifth century, bringing with him knowledge of Confucianism and Chinese characters.\n\nThe earliest Japanese documents were probably written by bilingual Chinese or Korean officials employed at the Yamato court. For example, the diplomatic correspondence from King Bu of Wa to Emperor Shun of Liu Song in 478 has been praised for its skillful use of allusion. Later, groups of people called \"fuhito\" were organized under the monarch to read and write Classical Chinese. During the reign of Empress Suiko (593–628), the Yamato court began sending full-scale diplomatic missions to China, which resulted in a large increase in Chinese literacy at the Japanese court.\n\nIn ancient times paper was so rare that people stenciled kanji onto thin, rectangular strips of wood. These wooden boards were used for communication between government offices, tags for goods transported between various countries, and the practice of writing. The oldest written kanji in Japan discovered so far was written in ink on wood as a wooden strip dated to the 7th century. It is a record of trading for cloth and salt.\n\nThe Japanese language had no written form at the time Chinese characters were introduced, and texts were written and read only in Chinese. Later, during the Heian period (794–1185), however, a system known as \"kanbun\" emerged, which involved using Chinese text with diacritical marks to allow Japanese speakers to restructure and read Chinese sentences, by changing word order and adding particles and verb endings, in accordance with the rules of Japanese grammar.\n\nChinese characters also came to be used to write Japanese words, resulting in the modern kana syllabaries. Around 650 AD, a writing system called \"man'yōgana\" (used in the ancient poetry anthology \"Man'yōshū\") evolved that used a number of Chinese characters for their sound, rather than for their meaning. Man'yōgana written in cursive style evolved into \"hiragana\", or \"onna-de\", that is, \"ladies' hand,\" a writing system that was accessible to women (who were denied higher education). Major works of Heian-era literature by women were written in hiragana. \"Katakana\" emerged via a parallel path: monastery students simplified \"man'yōgana\" to a single constituent element. Thus the two other writing systems, hiragana and katakana, referred to collectively as \"kana\", are descended from kanji.\n\nIn modern Japanese, kanji are used to write parts of the language (usually content words) such as nouns, adjective stems, and verb stems, while hiragana are used to write inflected verb and adjective endings and as phonetic complements to disambiguate readings (\"okurigana\"), particles, and miscellaneous words which have no kanji or whose kanji is considered obscure or too difficult to read or remember. Katakana are mostly used for representing onomatopoeia, non-Japanese loanwords (except those borrowed from ancient Chinese), the names of plants and animals (with exceptions), and for emphasis on certain words.\n\nIn 1946, following World War II and under the Allied Occupation of Japan, the Japanese government, guided by the Supreme Commander of the Allied Powers instituted a series of orthographic reforms. This was done with the goal of facilitating learning for children and simplifying kanji use in literature and periodicals.\nThe number of characters in circulation was reduced, and formal lists of characters to be learned during each grade of school were established.\nSome characters were given simplified glyphs, called . Many variant forms of characters and obscure alternatives for common characters were officially discouraged.\n\nThese are simply guidelines, so many characters outside these standards are still widely known and commonly used; these are known as .\n\nThe are 1,006 characters that Japanese children learn in elementary school. Originally the list only contained 881 characters. This was expanded to 996 characters in 1977. It was not until 1982 the list was expanded to its current size. The grade-level breakdown of these kanji is known as the , or the \"gakushū kanji\". ()\n\nThe are 2,136 characters consisting of all the \"Kyōiku kanji\", plus 1,130 additional kanji taught in junior high and high school. In publishing, characters outside this category are often given \"furigana\". The \"jōyō kanji\" were introduced in 1981, replacing an older list of 1,850 characters known as the , introduced in 1946. Originally numbering 1,945 characters, the \"jōyō kanji\" list was extended to 2,136 in 2010. Some of the new characters were previously \"Jinmeiyō kanji\"; some are used to write prefecture names: , , , , , , , , , and .\n\nSince September 27, 2004, the consist of 3,119 characters, containing the \"jōyō kanji\" plus an additional 983 kanji found in people's names. There were only 92 kanji in the original list published in 1952, but new additions have been made frequently. Sometimes the term \"jinmeiyō kanji\" refers to all 3,119, and sometimes it only refers to the 983 that are only used for names.\n\n are any kanji not contained in the \"jōyō kanji\" and \"jinmeiyō kanji\" lists. These are generally written using traditional characters, but extended shinjitai forms exist.\n\nThe Japanese Industrial Standards for kanji and kana define character code-points for each kanji and kana, as well as other forms of writing such as the Latin alphabet, Cyrillic script, Greek alphabet, Hindu-Arabic numerals, etc. for use in information processing. They have had numerous revisions. The current standards are:\n\n are kanji that are not represented in existing Japanese encoding systems. These include variant forms of common kanji that need to be represented alongside the more conventional glyph in reference works, and can include non-kanji symbols as well.\n\n\"Gaiji\" can be either user-defined characters or system-specific characters. Both are a problem for information interchange, as the code point used to represent an external character will not be consistent from one computer or operating system to another.\n\n\"Gaiji\" were nominally prohibited in JIS X 0208-1997, and JIS X 0213-2000 used the range of code-points previously allocated to \"gaiji\", making them completely unusable. Nevertheless, they persist today with NTT DoCoMo's \"i-mode\" service, where they are used for emoji (pictorial characters).\n\nUnicode allows for optional encoding of \"gaiji\" in private use areas, while Adobe's SING (Smart INdependent Glyphlets) technology allows the creation of customized gaiji.\n\nThe Text Encoding Initiative uses a <g> element to encode any non-standard character or glyph, including gaiji. (The g stands for \"gaiji\")\n\nThere is no definitive count of kanji characters, just as there is none of Chinese characters generally. The \"Dai Kan-Wa Jiten\", which is considered to be comprehensive in Japan, contains about 50,000 characters. The \"Zhonghua Zihai\", published in 1994 in China contains about 85,000 characters; however, the majority of these are not in common use in any country, and many are obscure variants or archaic forms.\n\nApproximately 2,000 to 3,000 characters are commonly used in Japan, a few thousand more find occasional use, and a total of 13,108 characters can be encoded in various Japanese Industrial Standards for kanji.\n\nBecause of the way they have been adopted into Japanese, a single kanji may be used to write one or more different words—or, in some cases, morphemes—and thus the same character may be pronounced in different ways. From the point of view of the reader, kanji are said to have one or more different \"readings\". Although more than one reading may become activated in the brain, deciding which reading is appropriate depends on recognizing which word it represents, which can usually be determined from context, intended meaning, whether the character occurs as part of a compound word or an independent word, and sometimes location within the sentence. For example, is usually read \"kyō\", meaning \"today\", but in formal writing is instead read \"konnichi\", meaning \"nowadays\"; this is understood from context. Nevertheless, some cases are ambiguous and require a \"furigana\" gloss, which are also used simply for difficult readings or to specify a non-standard reading.\n\nKanji readings are categorized as either \"on'yomi\" (literally \"sound reading\", from Chinese) or \"kun'yomi\" (literally \"meaning reading\", native Japanese), and most characters have at least two readings, at least one of each. However, some characters have only a single reading, such as or ; \"kun\"-only are common for Japanese-coined kanji (\"kokuji\"). Some common kanji have ten or more possible readings; the most complex common example is , which is read as \"sei, shō, nama, ki, o-u, i-kiru, i-kasu, i-keru, u-mu, u-mareru, ha-eru\", and \"ha-yasu\", totaling 8 basic readings (first 2 are \"on\", rest are \"kun\"), or 12 if related verbs are counted as distinct; see okurigana: 生 for details.\n\nMost often, a character will be used for both sound and meaning, and it is simply a matter of choosing the correct reading based on which word it represents. In other cases, a character is used only for sound (\"ateji\"). In this case, pronunciation is still based on a standard reading, or used only for meaning (broadly a form of \"ateji\", narrowly \"jukujikun\"). Therefore, only the full compound—not the individual character—has a reading. There are also special cases where the reading is completely different, often based on an historical or traditional reading.\n\nThe analogous phenomenon occurs to a much lesser degree in Chinese varieties, where there are literary and colloquial readings of Chinese characters—borrowed readings and native readings. In Chinese these borrowed readings and native readings are etymologically related, since they are between Chinese varieties (which are related), not from Chinese to Japanese (which are not related). They thus form doublets and are generally similar, analogous to different on'yomi, reflecting different stages of Chinese borrowings into Japanese.\n\nThe , the Sino-Japanese reading, is the modern descendant of the Japanese approximation of the base Chinese pronunciation of the character at the time it was introduced. It was often previously referred to as translation reading, as it was recreated readings of the Chinese pronunciation but was not the Chinese pronunciation or reading itself, similar to the English pronunciation of Latin loanwords. Old Japanese scripts often stated that \"on'yomi\" readings were also created by the Japanese during their arrival and re-borrowed by the Chinese as their own. There also exist kanji created by the Japanese and given an \"on'yomi\" reading despite not being a Chinese-derived or a Chinese-originating character. Some kanji were introduced from different parts of China at different times, and so have multiple \"on'yomi\", and often multiple meanings. \"Kanji\" invented in Japan would not normally be expected to have \"on'yomi\", but there are exceptions, such as the character \"to work\", which has the \"kun'yomi\" \"\"hataraku\" and the \"on'yomi\" \"dō\"\", and \"gland\", which has only the \"on'yomi\" \"\"sen\"—in both cases these come from the \"on'yomi\" of the phonetic component, respectively \"dō\" and \"sen\"\".\n\nGenerally, \"on'yomi\" are classified into four types according to their region and time of origin:\n\nThe most common form of readings is the \"kan-on\" one, and use of a non-\"kan-on\" reading in a word where the \"kan-on\" reading is well-known is a common cause of reading mistakes or difficulty, such as in (\"go-on\"), where is usually instead read as \"kai\". The \"go-on\" readings are especially common in Buddhist terminology such as , as well as in some of the earliest loans, such as the Sino-Japanese numbers. The \"tō-on\" readings occur in some later words, such as , , and . The go-on, kan-on, and tō-on readings are generally cognate (with rare exceptions of homographs; see below), having a common origin in Old Chinese, and hence form linguistic doublets or triplets, but they can differ significantly from each other and from modern Chinese pronunciation.\n\nIn Chinese, most characters are associated with a single Chinese sound, though there are distinct literary and colloquial readings. However, some homographs ( ) such as (' or ') (Japanese: \"an, gō, gyō\") have more than one reading in Chinese representing different meanings, which is reflected in the carryover to Japanese as well. Additionally, many Chinese syllables, especially those with an entering tone, did not fit the largely consonant-vowel (CV) phonotactics of classical Japanese. Thus most \"on'yomi\" are composed of two morae (beats), the second of which is either a lengthening of the vowel in the first mora (to \"ei\", \"ō\", or \"ū\"), the vowel \"i\", or one of the syllables \"ku\", \"ki\", \"tsu\", \"chi\", \"fu\" (historically, later merged into \"ō\"), or moraic \"n\", chosen for their approximation to the final consonants of Middle Chinese. It may be that palatalized consonants before vowels other than \"i\" developed in Japanese as a result of Chinese borrowings, as they are virtually unknown in words of native Japanese origin, but are common in Chinese.\n\n\"On'yomi\" primarily occur in words, many of which are the result of the adoption, along with the kanji themselves, of Chinese words for concepts that either did not exist in Japanese or could not be articulated as elegantly using native words. This borrowing process is often compared to the English borrowings from Latin, Greek, and Norman French, since Chinese-borrowed terms are often more specialized, or considered to sound more erudite or formal, than their native counterparts (occupying a higher linguistic register). The major exception to this rule is family names, in which the native \"kun'yomi\" are usually used (though \"on'yomi\" are found in many personal names, especially men's names).\n\nThe , the native reading, is a reading based on the pronunciation of a native Japanese word, or \"yamato kotoba\", that closely approximated the meaning of the Chinese character when it was introduced. As with \"on'yomi\", there can be multiple \"kun'yomi\" for the same kanji, and some kanji have no \"kun'yomi\" at all.\n\nFor instance, the character for east, , has the \"on'yomi\" \"tō\", from Middle Chinese \"\". However, Japanese already had two words for \"east\": \"higashi\" and \"azuma\". Thus the kanji had the latter readings added as \"kun'yomi\". In contrast, the kanji , denoting a Chinese unit of measurement (about 30 mm or 1.2 inch), has no native Japanese equivalent; it only has an \"on'yomi\", \"sun\", with no native \"kun'yomi\". Most \"kokuji\", Japanese-created Chinese characters, only have \"kun'yomi\", although some have back-formed a pseudo-\"on'yomi\" by analogy with similar characters, such as \"dō\", from \"dō\", and there are even some, such as \"sen\" \"gland\", that have only an \"on'yomi\".\n\n\"Kun'yomi\" are characterized by the strict (C)V syllable structure of \"yamato kotoba\". Most noun or adjective \"kun'yomi\" are two to three syllables long, while verb \"kun'yomi\" are usually between one and three syllables in length, not counting trailing hiragana called \"okurigana\". \"Okurigana\" are not considered to be part of the internal reading of the character, although they are part of the reading of the word. A beginner in the language will rarely come across characters with long readings, but readings of three or even four syllables are not uncommon. This contrasts with \"on'yomi\", which are monosyllabic, and is unusual in the Chinese family of scripts, which generally use one character per syllable—not only in Chinese, but also in Korean, Vietnamese, and Zhuang; polysyllabic Chinese characters are rare and considered non-standard.\n\n\nLonger readings exist for non-Jōyō characters and non-kanji symbols, where a long gairaigo word may be the reading (this is classed as \"kun'yomi\"—see single character gairaigo, below)—the character has the seven kana reading \"senchimētoru\" \"centimeter\", though it is generally written as \"cm\" (with two half-width characters, so occupying one space); another common example is '%' (the percent sign), which has the five kana reading \"pāsento\". Further, some Jōyō characters have long non-Jōyō readings (students learn the character, but not the reading), such as \"omonpakaru\" for .\n\nIn a number of cases, multiple kanji were assigned to cover a single Japanese word. Typically when this occurs, the different kanji refer to specific shades of meaning. For instance, the word , \"naosu\", when written , means \"to heal an illness or sickness\". When written it means \"to fix or correct something\". Sometimes the distinction is very clear, although not always. Differences of opinion among reference works is not uncommon; one dictionary may say the kanji are equivalent, while another dictionary may draw distinctions of use. As a result, native speakers of the language may have trouble knowing which kanji to use and resort to personal preference or by writing the word in hiragana. This latter strategy is frequently employed with more complex cases such as もと \"moto\", which has at least five different kanji: , and , the first three of which have only very subtle differences. Another notable example is \"sakazuki\" \"sake cup\", which may be spelt as at least five different kanji: , and ; of these, the first two are common—formally is a small cup and a large cup.\n\nLocal dialectical readings of kanji are also classified under \"kun'yomi\", most notably readings for words in Ryukyuan languages. Further, in rare cases gairaigo (borrowed words) have a single character associated with them, in which case this reading is formally classified as a \"kun'yomi\", because the character is being used for meaning, not sound. This is discussed under single character gairaigo, below.\n\nThere are many kanji compounds that use a mixture of \"on'yomi\" and \"kun'yomi\", known as or words (depending on the order), which are themselves examples of this kind of compound (they are autological words): the first character of \"jūbako\" is read using \"on'yomi\", the second \"kun'yomi\" (\"on-kun\"). \nIt is the other way around with \"yutō\" (\"kun-on\").\n\nFormally, these are referred to as and . Note that in both these words, the \"on'yomi\" has a long vowel; long vowels in Japanese generally come from Chinese, hence distinctive of \"on'yomi\". These are the Japanese form of hybrid words. Other examples include , and .\n\n\"Ateji\" often use mixed readings. For instance the city of Sapporo, whose name derives from the Ainu language and has no meaning in Japanese, is written with the \"on-kun\" compound (which includes \"sokuon\" as if it were a purely \"on\" compound).\n\n and are readings of kanji combinations that have no direct correspondence to the characters' individual \"on'yomi\" or \"kun'yomi\". From the point of view of the character, rather than the word, this is known as a , and these are listed in kanji dictionaries under the entry for the character.\n\n\"Gikun\" are when non-standard kanji are used, generally for effect, such as using with reading \"fuyu\" (\"winter\"), rather than the standard character .\n\n\"Jukujikun\" are when the standard kanji for a word are related to the meaning, but not the sound. The word is pronounced as a whole, not corresponding to sounds of individual kanji. For example, (\"this morning\") is jukujikun, and read neither as *\"ima'asa\", the \"kun'yomi\" of the characters, nor \"konchō\", the \"on'yomi\" of the characters, nor any combination thereof. Instead it is read as \"kesa\", a native bisyllabic Japanese word that may be seen as a single morpheme, or as a fusion of \"kyō\" (previously \"kefu\"), \"today\", and \"asa\", \"morning\". Likewise, (\"tomorrow\") is jukujikun, and read neither as \"akari(no)hi\", the \"kun'yomi\" of the characters, nor \"meinichi\", the \"on'yomi\" of the characters, nor any combination thereof. Instead it is read as \"ashita\", a native multisyllabic Japanese word that may be seen as a single morpheme.\n\nJukujikun are primarily used for some native Japanese words, such as Yamato ( or , the name of a Japanese province as well as ancient name for Japan), and for some old borrowings, such as from Ainu, from Portuguese, or \"bīru\" (, wheat alcohol) from Dutch, especially if the word was borrowed before the Meiji Period. Words whose kanji are jukujikun are often usually written as hiragana (if native), or katakana (if borrowed); some old borrowed words are also written as hiragana, especially Portuguese loanwords such as \"karuta\" () from Portuguese \"carta\" (Eng: card), \"tempura\" () from Portuguese \"tempora\" (Eng: time), and \"pan\" () from Spanish \"pan\" (Eng: bread), as well as \"tabako\" ().\n\nSometimes, jukujikun can even have more kanji than there are syllables, examples being \"kera\" (, woodpecker), \"gumi\" (, silver berry/oleaster), and \"Hozumi\" (, a surname).\n\nJukujikun are quite varied. Often the kanji compound for jukujikun is idiosyncratic and created for the word, and where the corresponding Chinese word does not exist; in other cases a kanji compound for an existing Chinese word is reused, where the Chinese word and \"on'yomi\" may or may not be used in Japanese; for example, is jukujikun for \"tonakai\", from Ainu, but the \"on'yomi\" reading of \"junroku\" is also used. In some cases Japanese coinages have subsequently been borrowed back into Chinese, such as .\n\nThe underlying word for jukujikun is a native Japanese word or foreign borrowing, which either does not have an existing kanji spelling (either \"kun'yomi\" or \"ateji\") or for which a new kanji spelling is produced. Most often the word is a noun, which may be a simple noun (not a compound or derived from a verb), or may be a verb form or a fusional pronunciation; for example is originally from the verb , while is fusional. In rare cases jukujikun is also applied to inflectional words (verbs and adjectives), in which case there is frequently a corresponding Chinese word.\n\nExamples of jukujikun for inflectional words follow. The most common example of a jukujikun adjective is , originally \"kawayu-i;\" the word is used in Chinese, but the corresponding \"on'yomi\" is not used in Japanese. By contrast, \"appropriate\" can be either or are both used; the \"-shii\" ending is because these were formerly a different class of adjectives. A common example of a verb with jukujikun is , corresponding to \"on'yomi\" . A sample jukujikun deverbal (noun derived from a verb form) is , from , spelling from . See and for many more examples. Note that there are also compound verbs and, less commonly, compound adjectives, and while these may have multiple kanji without intervening characters, they are read using usual \"kun'yomi;\" examples include and .\n\nTypographically, the furigana for jukujikun are often written so they are centered across the entire word, or for inflectional words over the entire root—corresponding to the reading being related to the entire word—rather than each part of the word being centered over its corresponding character, as is often done for the usual phono-semantic readings.\n\nBroadly speaking, jukujikun can be considered a form of \"ateji\", though in narrow usage \"ateji\" refers specifically to using characters for sound and not meaning (sound-spelling), rather than meaning and not sound (meaning-spelling), as in jukujikun.\n\nMany jukujikun (established meaning-spellings) began life as gikun (improvised meaning-spellings). Occasionally a single word will have many such kanji spellings; an extreme example is , which may be spelt in a great many ways, including , , , , , , , , , and —many of these variant spellings are particular to haiku poems.\n\nIn some rare cases, an individual kanji has a reading that is borrowed from a modern foreign language (gairaigo), though most often these words are written in katakana. Notable examples include , , , and . See list of single character gairaigo for more. These are classed as \"kun'yomi\" of a single character, because the character is being used for meaning only (without the Chinese pronunciation), rather than as ateji, which is the classification used when a gairaigo term is written as a compound (2 or more characters). However, unlike the vast majority of other \"kun'yomi\", these readings are not native Japanese, but rather borrowed, so the \"kun'yomi\" label can be misleading. The readings are also written in katakana, unlike the usual hiragana for native \"kun'yomi\". Note that most of these characters are for units, particularly SI units, in many cases using new characters (kokuji) coined during the Meiji period, such as .\n\nSome kanji also have lesser-known readings called \"nanori\" (), which are mostly used for names (often given names) and in general, are closely related to the \"kun'yomi\". Place names sometimes also use \"nanori\" or, occasionally, unique readings not found elsewhere.\n\nFor example, there is the surname (literally, \"little birds at play\") that implies there are no predators, such as hawks, present. Pronounced, \"kotori asobu\". The name then can also mean (\"taka ga inai\", literally, \"no hawks around\") and it can be shortened to be pronounced as \"Takanashi\".\n\nAlthough there are general rules for when to use \"on'yomi\" and when to use \"kun'yomi\", the language is littered with exceptions, and it is not always possible for even a native speaker to know how to read a character without prior knowledge (this is especially true for names, both of people and places); further, a given character may have multiple \"kun'yomi\" or \"on'yomi\". When reading Japanese, one primarily recognizes \"words\" (multiple characters and okurigana) and their readings, rather than individual characters, and only guess readings of characters when trying to \"sound out\" an unrecognized word.\n\nHomographs exist, however, which can sometimes be deduced from context, and sometimes cannot, requiring a glossary. For example, may be read either as \"kyō\" \"today (informal)\" (special fused reading for native word) or as \"konnichi\" \"these days (formal)\" (\"on'yomi\"); in formal writing this will generally be read as \"konnichi\". In some cases multiple readings are common, as in \"pork soup\", which is commonly pronounced both as \"ton-jiru\" (mixed \"on-kun\") and \"buta-jiru\" (\"kun-kun\"), with \"ton\" somewhat more common nationally. Inconsistencies abound—for example \"gyū-niku\" \"beef\" and \"yō-niku\" \"mutton\" have \"on-on\" readings, but \"buta-niku\" \"pork\" and \"tori-niku\" \"poultry\" have \"kun-on\" readings.\n\nThe main guideline is that a single kanji followed by \"okurigana\" (hiragana characters that are part of the word)—as used in native verbs and adjectives—\"always\" indicates \"kun'yomi\", while kanji compounds (kango) usually use \"on'yomi\", which is usually \"kan-on;\" however, other \"on'yomi\" are also common, and \"kun'yomi\" are also commonly used in kango. For a kanji in isolation without okurigana, it is typically read using their \"kun'yomi\", though there are numerous exceptions. For example, \"iron\" is usually read with the \"on'yomi\" \"tetsu\" rather than the \"kun'yomi\" \"kurogane\". Chinese \"on'yomi\" which are not the common \"kan-on\" reading are a frequent cause of difficulty or mistakes when encountering unfamiliar words or for inexperienced readers, though skilled natives will recognize the word; a good example is (\"go-on\"), where is usually instead read as \"kai\".\n\nOkurigana are used with \"kun'yomi\" to mark the inflected ending of a native verb or adjective, or by convention. Note that Japanese verbs and adjectives are closed class, and do not generally admit new words (borrowed Chinese vocabulary, which are nouns, can form verbs by adding at the end, and adjectives via \"-no\" or \"-na\", but cannot become native Japanese vocabulary, which inflect). For example: \"aka-i\" \"red\", \"atara-shii\" \"new\", \"mi-ru\" \"(to) see\". Okurigana can be used to indicate which \"kun'yomi\" to use, as in \"ta-beru\" versus \"ku-u\" (casual), both meaning \"(to) eat\", but this is not always sufficient, as in , which may be read as \"a-ku\" or \"hira-ku\", both meaning \"(to) open\". is a particularly complicated example, with multiple \"kun\" and \"on'yomi\"—see okurigana: 生 for details. Okurigana is also used for some nouns and adverbs, as in \"nasake\" \"sympathy\", \"kanarazu\" \"invariably\", but not for \"kane\" \"money\", for instance. Okurigana is an important aspect of kanji usage in Japanese; see that article for more information on \"kun'yomi\" orthography\nKanji occurring in are generally read using \"on'yomi\", especially for four-character compounds (\"yojijukugo\"). Though again, exceptions abound, for example, \"jōhō\" \"information\", \"gakkō\" \"school\", and \"shinkansen\" \"bullet train\" all follow this pattern. This isolated kanji versus compound distinction gives words for similar concepts completely different pronunciations. \"north\" and \"east\" use the \"kun'yomi\" \"kita\" and \"higashi\", being stand-alone characters, but \"northeast\", as a compound, uses the \"on'yomi\" \"hokutō\". This is further complicated by the fact that many kanji have more than one \"on'yomi\": is read as \"sei\" in \"sensei\" \"teacher\" but as \"shō\" in \"isshō\" \"one's whole life\". Meaning can also be an important indicator of reading; is read \"i\" when it means \"simple\", but as \"eki\" when it means \"divination\", both being \"on'yomi\" for this character.\n\nThese rules of thumb have many exceptions. \"Kun'yomi\" compound words are not as numerous as those with \"on'yomi\", but neither are they rare. Examples include \"tegami\" \"letter\", \"higasa\" \"parasol\", and the famous \"kamikaze\" \"divine wind\". Such compounds may also have okurigana, such as (also written ) \"karaage\" \"Chinese-style fried chicken\" and \"origami\", although many of these can also be written with the okurigana omitted (for example, or ).\n\nSimilarly, some \"on'yomi\" characters can also be used as words in isolation: \"ai\" \"love\", \"Zen\", \"ten\" \"mark, dot\". Most of these cases involve kanji that have no \"kun'yomi\", so there can be no confusion, although exceptions do occur. Alone may be read as \"kin\" \"gold\" or as \"kane\" \"money, metal\"; only context can determine the writer's intended reading and meaning.\n\nMultiple readings have given rise to a number of homographs, in some cases having different meanings depending on how they are read. One example is , which can be read in three different ways: \"jōzu\" (skilled), \"uwate\" (upper part), or \"kamite\" (stage left/house right). In addition, has the reading \"umai\" (skilled). More subtly, has three different readings, all meaning \"tomorrow\": \"ashita\" (casual), \"asu\" (polite), and \"myōnichi\" (formal). Furigana (reading glosses) is often used to clarify any potential ambiguities.\n\nConversely, in some cases homophonous terms may be distinguished in writing by different characters, but not so distinguished in speech, and hence potentially confusing. In some cases when it is important to distinguish these in speech, the reading of a relevant character may be changed. For example, (privately established, esp. school) and (city established) are both normally pronounced \"shi-ritsu;\" in speech these may be distinguished by the alternative pronunciations \"watakushi-ritsu\" and \"ichi-ritsu\". More informally, in legal jargon \"preamble\" and \"full text\" are both pronounced \"zen-bun\", so may be pronounced \"mae-bun\" for clarity, as in \"Have you memorized the preamble [not 'whole text'] of the constitution?\". As in these examples, this is primarily using a \"kun'yomi\" for one character in a normally \"on'yomi\" term.\n\nAs stated above, \"jūbako\" and \"yutō\" readings are also not uncommon. Indeed, all four combinations of reading are possible: \"on-on\", \"kun-kun\", \"kun-on\" and \"on-kun\".\n\nSeveral famous place names, including those of Japan itself ( \"Nihon\" or sometimes \"Nippon\"), those of some cities such as Tokyo ( \"Tōkyō\") and Kyoto ( \"Kyōto\"), and those of the main islands Honshu ( \"Honshū\"), Kyushu ( \"Kyūshū\"), Shikoku ( \"Shikoku\"), and Hokkaido ( \"Hokkaidō\") are read with \"on'yomi\"; however, the majority of Japanese place names are read with \"kun'yomi\": \"Ōsaka\", \"Aomori\", \"Hakone\". Names often use characters and readings that are not in common use outside of names. When characters are used as abbreviations of place names, their reading may not match that in the original. The Osaka () and Kobe () baseball team, the Hanshin () Tigers, take their name from the \"on'yomi\" of the second kanji of \"Ōsaka\" and the first of \"Kōbe\". The name of the Keisei () railway line—linking Tokyo () and Narita ()—is formed similarly, although the reading of from is \"kei\", despite \"kyō\" already being an \"on'yomi\" in the word \"Tōkyō\".\n\nJapanese family names are also usually read with \"kun'yomi\": \"Yamada\", \"Tanaka\", \"Suzuki\". Japanese given names often have very irregular readings. Although they are not typically considered \"jūbako\" or \"yutō\", they often contain mixtures of \"kun'yomi\", \"on'yomi\" and \"nanori\", such as \"Daisuke\" [\"on-kun\"], \"Natsumi\" [\"kun-on\"]. Being chosen at the discretion of the parents, the readings of given names do not follow any set rules, and it is impossible to know with certainty how to read a person's name without independent verification. Parents can be quite creative, and rumours abound of children called \"Āsu\" (\"Earth\") and \"Enjeru\" (\"Angel\"); neither are common names, and have normal readings \"chikyū\" and \"tenshi\" respectively. Some common Japanese names can be written in multiple ways, e.g. Akira can be written as , , , , , , , , , , , , , , , , and many other characters and kanji combinations not listed, Satoshi can be written as , , , , , , , , , , , , , , , , , , etc., and Haruka can be written as , , , , , , , , and several other possibilities. Common patterns do exist, however, allowing experienced readers to make a good guess for most names. To alleviate any confusion on how to pronounce the names of other Japanese people, most official Japanese documents require Japanese to write their names in both kana and kanji.\n\nChinese place names and Chinese personal names appearing in Japanese texts, if spelled in kanji, are almost invariably read with \"on'yomi\". Especially for older and well-known names, the resulting Japanese pronunciation may differ widely from that used by modern Chinese speakers. For example, Mao Zedong's name is pronounced as in Japanese, and the name of the legendary Monkey King, Sun Wukong, is pronounced \"Son Gokū\" () in Japanese.\n\nToday, Chinese names that are not well known in Japan are often spelled in katakana instead, in a form much more closely approximating the native Chinese pronunciation. Alternatively, they may be written in kanji with katakana furigana. Many such cities have names that come from non-Chinese languages like Mongolian or Manchu. Examples of such not-well-known Chinese names include:\n\nInternationally renowned Chinese-named cities tend to imitate the older English pronunciations of their names, regardless of the kanji's \"on'yomi\" or the Mandarin or Cantonese pronunciation, and can be written in either katakana or kanji. Examples include:\nNotes: \n\nIn some cases the same kanji can appear in a given word with different readings. Normally this occurs when a character is duplicated and the reading of the second character has voicing (\"rendaku\"), as in \"hito-bito\" \"people\" (more often written with the iteration mark as ), but in rare cases the readings can be unrelated, as in .\n\nBecause of the ambiguities involved, kanji sometimes have their pronunciation for the given context spelled out in ruby characters known as \"furigana\", (small \"kana\" written above or to the right of the character) or \"kumimoji\" (small \"kana\" written in-line after the character). This is especially true in texts for children or foreign learners. It is also used in newspapers and \"manga\" (comics) for rare or unusual readings, or for situations like the first time a character's name is given, and for characters not included in the officially recognized set of essential kanji. Works of fiction sometimes use \"furigana\" to create new \"words\" by giving normal kanji non-standard readings, or to attach a foreign word rendered in katakana as the reading for a kanji or kanji compound of the same or similar meaning.\n\nConversely, specifying a given kanji, or spelling out a kanji word—whether the pronunciation is known or not—can be complicated, due to the fact that there is not a commonly used standard way to refer to individual kanji (one does not refer to \"kanji #237\"), and that a given reading does not map to a single kanji—indeed there are many homophonous \"words\", not simply individual characters, particularly for \"kango\" (with \"on'yomi\"). Easiest is to write the word out—either on paper or tracing it in the air—or look it up (given the pronunciation) in a dictionary, particularly an electronic dictionary; when this is not possible, such as when speaking over the phone or writing implements are not available (and tracing in air is too complicated), various techniques can be used. These include giving \"kun'yomi\" for characters—these are often unique—using a well-known word with the same character (and preferably the same pronunciation and meaning), and describing the character via its components. For example, one may explain how to spell the word via the words , , and —the first two use the \"kun'yomi\", the third is a well-known compound—saying \"\"kaori\", \"karai\", \"ryō\" as in \"inryō\".\"\n\nIn dictionaries, both words and individual characters have readings glossed, via various conventions. Native words and Sino-Japanese vocabulary are glossed in hiragana (for both \"kun\" and \"on\" readings), while borrowings (\"gairaigo\")—including modern borrowings from Chinese—are glossed in katakana; this is the standard writing convention also used in furigana. By contrast, readings for individual characters are conventionally written in katakana for \"on\" readings, and hiragana for \"kun\" readings. Kun readings may further have a separator to indicate which characters are okurigana, and which are considered readings of the character itself. For example, in the entry for , the reading corresponding to the basic verb may be written as (\"ta.beru\"), to indicate that \"ta\" is the reading of the character itself. Further, kanji dictionaries often list compounds including irregular readings of a kanji.\n\nSince kanji are essentially Chinese \"hànzì\" used to write Japanese, the majority of characters used in modern Japanese still retain their Chinese meaning, physical resemblance with some of their modern traditional Chinese characters counterparts, and a degree of similarity with Classical Chinese pronunciation imported to Japan from 5th to 9th century. Nevertheless, after centuries of development, there is a notable number of kanji used in modern Japanese which have different meaning from \"hanzi\" used in modern Chinese. Such differences are the result of:\n\nLikewise, the process of character simplification in mainland China since the 1950s has resulted in the fact that Japanese speakers who have not studied Chinese may not recognize some simplified characters.\n\nIn Japanese, refers to Chinese characters made outside of China. Specifically, kanji made in Japan are referred to as . They are primarily formed in the usual way of Chinese characters, namely by combining existing components, though using a combination that is not used in China. The corresponding phenomenon in Korea is called \"gukja\" (), a cognate name; there are however far fewer Korean-coined characters than Japanese-coined ones. Other languages using the Chinese family of scripts sometimes have far more extensive systems of native characters, most significantly Vietnamese chữ Nôm, which comprises over 20,000 characters used throughout traditional Vietnamese writing, and Zhuang sawndip, which comprises over 10,000 characters, which are still in use.\n\nSince kokuji are generally devised for existing native words, these usually only have native \"kun\" readings. However, they occasionally have a Chinese \"on\" reading, derived from a phonetic, as in , \"dō\", and in rare cases only have an \"on\" reading, as in , \"sen\", from , which was derived for use in technical compounds ( means \"gland\", hence used in medical terminology).\n\nThe majority of kokuji are ideogrammatic compounds (), meaning that they are composed of two (or more) characters, with the meaning associated with the combination. For example, is composed of (person radical) plus (action), hence \"action of a person, work\". This is in contrast to kanji generally, which are overwhelmingly phono-semantic compounds. This difference is because kokuji were coined to express Japanese words, so borrowing existing (Chinese) readings could not express these—combining existing characters to logically express the meaning was the simplest way to achieve this. Other illustrative examples (below) include \"sakaki\" tree, formed as \"tree\" and \"god\", literally \"divine tree\", and \"tsuji\" \"crossroads, street\" formed as () \"road\" and \"cross\", hence \"cross-road\".\n\nIn terms of meanings, these are especially for natural phenomena (esp. flora and fauna species) that were not present in ancient China, including a very large number of fish, such as (sardine), (codfish), (seaperch), and (sillago), and trees, such as (evergreen oak), (Japanese cedar), (birch, maple) and (spindle tree). In other cases they refer to specifically Japanese abstract concepts, everyday words (like ), or later technical coinages (such as ).\n\nThere are hundreds of \"kokuji\" in existence. Many are rarely used, but a number have become commonly used components of the written Japanese language. These include the following:\n\nJōyō kanji has about 9 kokuji; there is some dispute over classification, but generally includes these:\n\"jinmeiyō kanji\"\nHyōgaiji:\n\nSome of these characters (for example, , \"gland\") have been introduced to China. In some cases the Chinese reading is the inferred Chinese reading, interpreting the character as a phono-semantic compound (as in how \"on\" readings are sometimes assigned to these characters in Chinese), while in other cases (such as ), the Japanese \"on\" reading is borrowed (in general this differs from the modern Chinese pronunciation of this phonetic). Similar coinages occurred to a more limited extent in Korea and Vietnam.\n\nHistorically, some kokuji date back to very early Japanese writing, being found in the \"Man'yōshū\", for example— \"iwashi\" \"sardine\" dates to the Nara period (8th century)—while they have continued to be created as late as the late 19th century, when a number of characters were coined in the Meiji era for new scientific concepts. For example, some characters were produced as regular compounds for some (but not all) SI units, such as ( \"meter\" + \"thousand, kilo-\") for kilometer, ( \"liter\" + \"thousand, kilo-\") for kiloliter, and ( \"gram\" + \"thousand, kilo-\") for kilogram—see Chinese characters for SI units for details. However, SI units in Japanese today are almost exclusively written using rōmaji or katakana such as or for km, for kl, and or for kg.\n\nIn Japan the kokuji category is strictly defined as characters whose \"earliest\" appearance is in Japan. If a character appears earlier in the Chinese literature, it is not considered a kokuji even if the character was independently coined in Japan and unrelated to the Chinese character (meaning \"not borrowed from Chinese\"). In other words, kokuji are not simply characters that were made in Japan, but characters that were \"first\" made in Japan. An illustrative example is . This spelling was created in Edo period Japan from the ateji (phonetic kanji spelling) for the existing word \"ankō\" by adding the radical to each character—the characters were \"made in Japan\". However, is not considered kokuji, as it is found in ancient Chinese texts as a corruption of (魚匽). is considered kokuji, as it has not been found in any earlier Chinese text. Casual listings may be more inclusive, including characters such as . Another example is , which is sometimes not considered kokuji due to its earlier presence as a corruption of Chinese .\n\nIn addition to \"kokuji\", there are kanji that have been given meanings in Japanese different from their original Chinese meanings. These are not considered \"kokuji\" but are instead called \"kokkun\" () and include characters such as the following:\n\nHan-dynasty scholar Xu Shen in his 2nd-century dictionary \"Shuowen Jiezi\" classified Chinese characters into six categories ( \"liùshū\", Japanese: \"rikusho\"). The traditional classification is still taught but is problematic and no longer the focus of modern lexicographic practice, as some categories are not clearly defined, nor are they mutually exclusive: the first four refer to structural composition, while the last two refer to usage.\n\n\"Shōkei\" (Mandarin: \"xiàngxíng\") characters are pictographic sketches of the object they represent. For example, is an eye, while is a tree. The current forms of the characters are very different from the originals, though their representations are more clear in oracle bone script and seal script. These pictographic characters make up only a small fraction of modern characters.\n\n\"Shiji\" (Mandarin: \"zhǐshì\") characters are ideographs, often called \"simple ideographs\" or \"simple indicatives\" to distinguish them and tell the difference from compound ideographs (below). They are usually simple graphically and represent an abstract concept such as \"up\" or \"above\" and \"down\" or \"below\". These make up a tiny fraction of modern characters.\n\n\"Kaii\" (Mandarin: \"huìyì\") characters are compound ideographs, often called \"compound indicatives\", \"associative compounds\", or just \"ideographs\". These are usually a combination of pictographs that combine semantically to present an overall meaning. An example of this type is (rest) from (person radical) and (tree). Another is the \"kokuji\" (mountain pass) made from (mountain), (up) and (down). These make up a tiny fraction of modern characters.\n\n\"Keisei\" (Mandarin: \"xíngshēng\") characters are phono-semantic or radical-phonetic compounds, sometimes called \"semantic-phonetic\", \"semasio-phonetic\", or \"phonetic-ideographic\" characters, are by far the largest category, making up about 90% of the characters in the standard lists; however, some of the most frequently used kanji belong to one of the three groups mentioned above, so \"keisei moji\" will usually make up less than 90% of the characters in a text. Typically they are made up of two components, one of which (most commonly, but by no means always, the left or top element) suggests the general category of the meaning or semantic context, and the other (most commonly the right or bottom element) approximates the pronunciation. The pronunciation relates to the original Chinese, and may now only be distantly detectable in the modern Japanese \"on'yomi\" of the kanji; it generally has no relation at all to \"kun'yomi\". The same is true of the semantic context, which may have changed over the centuries or in the transition from Chinese to Japanese. As a result, it is a common error in folk etymology to fail to recognize a phono-semantic compound, typically instead inventing a compound-indicative explanation.\n\n\"Tenchū\" (Mandarin: \"zhuǎnzhù\") characters have variously been called \"derivative characters\", \"derivative cognates\", or translated as \"mutually explanatory\" or \"mutually synonymous\" characters; this is the most problematic of the six categories, as it is vaguely defined. It may refer to kanji where the meaning or application has become extended. For example, is used for 'music' and 'comfort, ease', with different pronunciations in Chinese reflected in the two different \"on'yomi\", \"gaku\" 'music' and \"raku\" 'pleasure'.\n\n\"Kasha\" (Mandarin: \"jiǎjiè\") are rebuses, sometimes called \"phonetic loans\". The etymology of the characters follows one of the patterns above, but the present-day meaning is completely unrelated to this. A character was appropriated to represent a similar-sounding word. For example, in ancient Chinese was originally a pictograph for \"wheat\". Its syllable was homophonous with the verb meaning \"to come\", and the character is used for that verb as a result, without any embellishing \"meaning\" element attached. The character for wheat , originally meant \"to come\", being a \"keisei moji\" having 'foot' at the bottom for its meaning part and \"wheat\" at the top for sound. The two characters swapped meaning, so today the more common word has the simpler character. This borrowing of sounds has a very long history.\n\nThe iteration mark () is used to indicate that the preceding kanji is to be repeated, functioning similarly to a ditto mark in English. It is pronounced as though the kanji were written twice in a row, for example and . This mark also appears in personal and place names, as in the surname Sasaki (). This symbol is a simplified version of the kanji , a variant of .\n\nAnother abbreviated symbol is , in appearance a small katakana \"ke\", but actually a simplified version of the kanji , a general counter. It is pronounced \"ka\" when used to indicate quantity (such as , \"rokkagetsu\" \"six months\") or \"ga\" in place names like .\n\nThe way how these symbols may be produced on a computer depends on the operating system. In OS X, typingwill reveal the symbol as well as , and . To produce , type . Under Windows, typingwill reveal some of these symbols, while in Google IME,may be used.\n\nKanji, whose thousands of symbols defy ordering by conventions such as those used for the Latin script, are often collated using the traditional Chinese radical-and-stroke sorting method. In this system, common components of characters are identified; these are called radicals. Characters are grouped by their primary radical, then ordered by number of pen strokes within radicals. For example, the kanji character , meaning \"cherry\", is sorted as a ten-stroke character under the four-stroke primary radical meaning \"tree\". When there is no obvious radical or more than one radical, convention governs which is used for collation.\n\nOther kanji sorting methods, such as the SKIP system, have been devised by various authors.\n\nModern general-purpose Japanese dictionaries (as opposed to specifically character dictionaries) generally collate all entries, including words written using kanji, according to their kana representations (reflecting the way they are pronounced). The gojūon ordering of kana is normally used for this purpose.\n\nJapanese school children are expected to learn 1006 basic kanji characters, the \"kyōiku kanji\", before finishing the sixth grade. The order in which these characters are learned is fixed. The \"kyōiku kanji\" list is a subset of a larger list, originally of 1945 kanji characters, in 2010 extended to 2136, known as the \"jōyō kanji\"—characters required for the level of fluency necessary to read newspapers and literature in Japanese. This larger list of characters is to be mastered by the end of the ninth grade. Schoolchildren learn the characters by repetition and radical.\n\nStudents studying Japanese as a foreign language are often required by a curriculum to acquire kanji without having first learned the vocabulary associated with them. Strategies for these learners vary from copying-based methods to mnemonic-based methods such as those used in James Heisig's series \"Remembering the Kanji\". Other textbooks use methods based on the etymology of the characters, such as Mathias and Habein's \"The Complete Guide to Everyday Kanji\" and Henshall's \"A Guide to Remembering Japanese Characters\". Pictorial mnemonics, as in the text \"Kanji Pict-o-graphix\", are also seen.\n\nThe Japanese government provides the \"Kanji kentei\" ( \"Nihon kanji nōryoku kentei shiken\"; \"Test of Japanese Kanji Aptitude\"), which tests the ability to read and write kanji. The highest level of the \"Kanji kentei\" tests about six thousand kanji.\n\n\n\n\n"}
{"id": "14695685", "url": "https://en.wikipedia.org/wiki?curid=14695685", "title": "Kapala", "text": "Kapala\n\nA kapala (Sanskrit for \"skull\") or skullcup is a cup made from a human skull and used as a ritual implement (bowl) in both Hindu Tantra and Buddhist Tantra (Vajrayana). Especially in Tibet, they are often carved or elaborately mounted with precious metals and jewels. \n\n'Kapala' () is a loan word into Tibetan from Sanskrit \"kapāla\" (Devanagari: कपाल) referring to the skull or forehead, usually of a human. By association, it refers to the ritual skullcup fashioned out of a human cranium.\n\nMany of the deities of Vajrayana, including mahasiddhas, dakinis and dharmapalas, are depicted as carrying the kapala, usually in their left hand. Some deities such as the Hindu Chinnamasta and the related Buddhist Vajrayogini are depicted as drinking blood from the kapala.\n\nHindu deities that may be depicted with the kapala include Durga, Kālī and Shiva, especially in his Bhairava form. Even Ganesha, when adopted into Tibetan Buddhism as Maharakta Ganapati, is shown with a kapala filled with blood. \n\nSome of the Hindu deities pictured thus are:\n\na) Kālī, pictured in the most common four-armed iconographic image, shows each hand carrying variously a sword, a trishula (trident), a severed head, and a bowl or skullcup (kapala) catching the blood of the severed head.\n\nb) The Chamunda, a form of Durga, seen in the Halebidu temple built by the Hoysala, is described as wearing a garland of severed heads or skulls (Mundamala). She is described as having four, eight, ten, or twelve arms, holding a damaru (drum), trishula (trident), sword, a snake (nāga), skull-mace (khatvanga), thunderbolt (vajra), a severed head and panapatra (drinking vessel, wine cup) or skullcup (kapala), filled with blood.\n\nIn Tibetan monasteries a kapala is used symbolically to hold bread or dough cakes, torma, and wine instead of blood and flesh as offerings to wrathful deities, such as the ferocious Dharmapāla (\"defender of the faith\"). The dough cakes are shaped to resemble human eyes, ears and tongues. The \"kapala\" is made in the form of a skull, specially collected and prepared. It is elaborately anointed and consecrated before use. The cup is also elaborately decorated and kept in a triangular pedestal. The heavily embossed cup is usually made of silver-gilt bronze with lid shaped like a skull and with a handle made in the form of a thunderbolt.\n\nKapalas are used mainly for esoteric purposes such as rituals. Among the rituals using kapalas are higher tantric meditation to achieve a transcendental state of thought and mind within the shortest possible time; libation to gods and deities to win their favor; by Tibetan Lamas as an offering bowl on the altar, being filled with wine or blood as a gift to the Yidam Deity or all the Deities; and the Vajrayana empowerment ceremony.\n\nThe kapala is one of several charnel ground implements made from human bone found by tantrics at sky burial sites.\nThe charnel ground, an ancient Tibetan burial custom, is distinctly different from the customs of graveyards and cremation, but all three of them have been a part of the home ground of tantric practitioners’ such as the yogis and yoginis, Shaiva Kapalikas and Aghoris, shamans and sadhus. The charnel ground, often referred to as \"sky burial\" by Western sources, is an area demarcated specifically in Tibet, defined by the Tibetan word Jhator (literal meaning is ’giving alms to the birds’), a way of exposing the corpse to nature, where human bodies are disposed as it were or in a chopped (chopped after the rituals) condition in the open ground as a ritual that has great religious meaning of the ascent of the mind to be reincarnated into another circle of life. Such a practice results in finding human bones, half or whole skeletons, more or less putrefying corpses and disattached limbs lying scattered around. Items made from human skulls or bones are found in the sky burial grounds by the Sadhus and Yogins of the tantric cult. The charnel grounds are also known by the epithets the \"field of death\" or the \"valley of corpses\". In Tibet, a class distinction in the burial practices is also noted. The dead High Lamas are buried in Stupas or cremated but the dead commoners are disposed of in the Charnel ground or in a Sky burial.\n\nThe products from the charnel ground are the charnel ground ornaments such as the i) Crown of five skulls, ii) Bone necklace, iii) Bone armlets, iv) Bone bracelets, v) Bone skirt and vi) Bone anklets which decorate many images of dakinis, yoginis, dharmapalas and a few other deities (as may be seen in some of the pictures and stone images depicted in the gallery here), and other products such as the Bone trumpet, the Skull cup and Skull drum used by the tantric practitioners. Kapala or the skull cup is thus a produce from the Charnel ground.\n\n\n"}
{"id": "15777247", "url": "https://en.wikipedia.org/wiki?curid=15777247", "title": "Lambert v. California", "text": "Lambert v. California\n\nLambert v. California, 355 U.S. 225 (1957), was a United States Supreme Court case regarding the defense of ignorance of the law when there is no legal notice. The court held that, when one is required to register one's presence, failure to register may only be punished when there is a probability that the accused party had knowledge of the law before committing the crime of failing to register.\n\nLambert had previously been convicted of forgery, a felony in California. She was unaware that a Los Angeles city ordinance required that she, being a felon, register if she remained in the city for more than five days. The ordinance stipulated that she, as a convicted criminal, could be fined $500 and sentenced to up to six months in jail for every day she remained in the city after the five-day limit. When she was arrested on suspicion of committing another offense she was convicted for failure to register. As Lambert was not allowed to use her lack of knowledge as a defense, she was convicted, fined $250 and sentenced to three years probation. Lambert appealed her case, arguing that she had no knowledge that she had to register her name and that convicting her would deprive her of due process under the Fourteenth Amendment.\n\nThe Supreme Court reversed Lambert's conviction, holding that knowledge or probability of knowledge of a statute is required to convict someone of a notice offense. Justice William Douglas, who delivered the majority opinion for the court, wrote:\nHowever, the court did not overturn the right of states and municipalities to force occupants to register for a given purpose. The court held that because the ordinance that forced convicted felons to register was not accompanied by any action, nor were there circumstances that would lead a felon to be aware of his or her duty to register, the ordinance was unconstitutional. The Justice continued:\nThis case is an exception to the legal principle \"ignorantia legis non excusat\"—that the ignorance of the law is not a suitable excuse for breaking it. Because it deals with the motives (or lack thereof) for committing a crime, it addresses \"mens rea\", the degree of legal culpability that arises from the motivation of a criminal.\n\n"}
{"id": "5498909", "url": "https://en.wikipedia.org/wiki?curid=5498909", "title": "Letterlike Symbols", "text": "Letterlike Symbols\n\nLetterlike Symbols is a Unicode block containing 80 characters which are constructed mainly from the glyphs of one or more letters. In addition to this block, Unicode includes full styled mathematical alphabets, although Unicode does not explicitly categorise these characters as being \"letterlike\".\n\nThe Letterlike Symbols block contains two emoji:\nU+2122 and U+2139.\n\nThe block has four standardized variants defined to specify emoji-style (U+FE0F VS16) or text presentation (U+FE0E VS15) for the\ntwo emoji, both of which default to a text presentation.\n\nThe following Unicode-related documents record the purpose and process of defining specific characters in the Letterlike Symbols block:\n\n"}
{"id": "946975", "url": "https://en.wikipedia.org/wiki?curid=946975", "title": "Line (geometry)", "text": "Line (geometry)\n\nThe notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. Lines are an idealization of such objects. Until the 17th century, lines were defined in this manner: \"The [straight or curved] line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which […] will leave from its imaginary moving some vestige in length, exempt of any width. […] The straight line is that which is equally extended between its points.\"\n\nEuclid described a line as \"breadthless length\" which \"lies equally with respect to the points on itself\"; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry).\n\nIn modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.\n\nWhen a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.\n\nAll definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition. In geometry, it is frequently the case that the concept of line is taken as a primitive. In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.\n\nIn a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a \"description\" or \"mental image\" of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The \"definition\" of line in Euclid's Elements falls into this category. Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.\n\nWhen geometry was first formalised by Euclid in the \"Elements\", he defined a general line (straight or curved) to be \"breadthless length\" with a straight line being a line \"which lies evenly with the points on itself\". These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms, but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.\n\nIn an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians), a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point. In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.\n\nAny collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.\n\nLines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. \n\nIn two dimensions, the equation for non-vertical lines is often given in the \"slope-intercept form\":\nwhere:\n\nThe slope of the line through points formula_2 and formula_3, when formula_4, is given by formula_5 and the equation of this line can be written formula_6.\n\nIn formula_7, every line formula_8 (including vertical lines) is described by a linear equation of the form\n\nwith fixed real coefficients \"a\", \"b\" and \"c\" such that \"a\" and \"b\" are not both zero. Using this form, vertical lines correspond to the equations with \"b\" = 0.\n\nThere are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.\n\nThe equation of the line passing through two different points formula_10 and formula_11 may be written as\nIf \"x\" ≠ \"x\", this equation may be rewritten as\nor\n\nIn three dimensions, lines can \"not\" be described by a single linear equation, so they are frequently described by parametric equations:\nwhere:\n\nThey may also be described as the simultaneous solutions of two linear equations\nsuch that formula_20 and formula_21 are not proportional (the relations formula_22 imply formula_23). This follows since in three dimensions a single linear equation typically describes a plane and a line is what is common to two distinct intersecting planes.\n\nThe \"normal form\" (also called the \"Hesse normal form\", after the German mathematician Ludwig Otto Hesse), is based on the \"normal segment\" for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:\nwhere \"θ\" is the angle of inclination of the normal segment (the oriented angle from the unit vector of the \"x\" axis to this segment), and \"p\" is the (positive) length of the normal segment. The normal form can be derived from the general form formula_25 by dividing all of the coefficients by\n\nUnlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, \"θ\" and \"p\", to be specified. If \"p\" > 0, then \"θ\" is uniquely defined modulo 2. On the other hand, if the line is through the origin (\"c\" = 0, \"p\" = 0), one drops the term to compute sin\"θ\" and cos\"θ\", and \"θ\" is only defined modulo .\n\nIn polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:\nwhere \"m\" is the slope of the line and b is the \"y\"-intercept. When \"θ\" = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:\nIn polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,\nwhere formula_30 and formula_31 represent the \"x\" and \"y\" intercepts respectively.\nThe above equation is not applicable for vertical and horizontal lines because in these cases one of the intercepts does not exist. Moreover, it is not applicable on lines passing through the pole since in this case, both \"x\" and \"y\" intercepts are zero (which is not allowed here since formula_30 and formula_31 are denominators).\nA vertical line that doesn't pass through the pole is given by the equation\nSimilarly, a horizontal line that doesn't pass through the pole is given by the equation\nThe equation of a line which passes through the pole is simply given as:\nwhere \"m\" is the slope of the line.\n\nThe vector equation of the line through points A and B is given by formula_37 (where λ is a scalar).\n\nIf a is vector OA and b is vector OB, then the equation of the line can be written: formula_38.\n\nA ray starting at point \"A\" is described by limiting λ. One ray is obtained if λ ≥ 0, and the opposite ray comes from λ ≤ 0.\n\nIn three-dimensional space, a first degree equation in the variables \"x\", \"y\", and \"z\" defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in \"n\"-dimensional space \"n\"-1 first-degree equations in the \"n\" coordinate variables define a line under suitable conditions.\n\nIn more general Euclidean space, R (and analogously in every other affine space), the line \"L\" passing through two different points \"a\" and \"b\" (considered as vectors) is the subset\nThe direction of the line is from \"a\" (\"t\" = 0) to \"b\" (\"t\" = 1), or in other words, in the direction of the vector \"b\" − \"a\". Different choices of \"a\" and \"b\" can yield the same line.\n\nThree points are said to be \"collinear\" if they lie on the same line. Three points \"usually\" determine a plane, but in the case of three collinear points this does \"not\" happen.\n\nIn affine coordinates, in \"n\"-dimensional space the points \"X\"=(\"x\", \"x\", ..., \"x\"), \"Y\"=(\"y\", \"y\", ..., \"y\"), and \"Z\"=(\"z\", \"z\", ..., \"z\") are collinear if the matrix\nhas a rank less than 3. In particular, for three points in the plane (\"n\" = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.\n\nEquivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, \"k\" points in a plane are collinear if and only if any (\"k\"–1) pairs of points have the same pairwise slopes.\n\nIn Euclidean geometry, the Euclidean distance \"d\"(\"a\",\"b\") between two points \"a\" and \"b\" may be used to express the collinearity between three points by:\nHowever, there are other notions of distance (such as the Manhattan distance) for which this property is not true.\n\nIn the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.\n\nIn a sense, all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:\n\nIn the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.\n\nFor more general algebraic curves, lines could also be:\nWith respect to triangles we have:\n\nFor a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.\n\nFor a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.\n\nParallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other—every point that is on either one of them is also on the other.\n\nPerpendicular lines are lines that intersect at right angles.\n\nIn three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.\n\nIn many models of projective geometry, the representation of a line rarely conforms to the notion of the \"straight curve\" as it is visualised in Euclidean geometry. In elliptic geometry we see a typical example of this. In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.\n\nGiven a line and any point \"A\" on it, we may consider \"A\" as decomposing this line into two parts.\nEach such part is called a ray (or half-line) and the point \"A\" is called its \"initial point\". The point A is considered to be a member of the ray. Intuitively, a ray consists of those points on a line passing through \"A\" and proceeding indefinitely, starting at \"A\", in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.\n\nGiven distinct points \"A\" and \"B\", they determine a unique ray with initial point \"A\". As two points define a unique line, this ray consists of all the points between \"A\" and \"B\" (including \"A\" and \"B\") and all the points \"C\" on the line through \"A\" and \"B\" such that \"B\" is between \"A\" and \"C\". This is, at times, also expressed as the set of all points \"C\" such that \"A\" is not between \"B\" and \"C\". A point \"D\", on the line determined by \"A\" and \"B\" but not in the ray with initial point \"A\" determined by \"B\", will determine another ray with initial point \"A\". With respect to the \"AB\" ray, the \"AD\" ray is called the \"opposite ray\".\nThus, we would say that two different points, \"A\" and \"B\", define a line and a decomposition of this line into the disjoint union of an open segment and two rays, \"BC\" and \"AD\" (the point \"D\" is not drawn in the diagram, but is to the left of \"A\" on the line \"AB\"). These are not opposite rays since they have different initial points.\n\nIn Euclidean geometry two rays with a common endpoint form an angle.\n\nThe definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.\n\nIn topology, a ray in a space \"X\" is a continuous embedding R → \"X\". It is used to define the important concept of end of the space.\n\nA line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.\n\nThe \"shortness\" and \"straightness\" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.\n\n\n\n"}
{"id": "2250567", "url": "https://en.wikipedia.org/wiki?curid=2250567", "title": "Line infantry", "text": "Line infantry\n\nLine infantry was the type of infantry that composed the basis of European land armies from the middle of the 17th century to the middle of the 19th century. For both battle and parade drill, it consisted of two to four ranks of foot soldiers drawn up side by side in rigid alignment, and thereby maximizing the effect of their firepower. By extension, the term came to be applied to the regular regiments \"of the line\" as opposed to Foot Guards, light infantry, skirmishers, militia, support personnel, plus some other special categories of infantry not focused on heavy front line combat.\n\nLine infantry mainly used three formations in its battles: the line, the square and the column.\n\nWith the massive proliferation of small arms (firearms that could be carried by hand, as opposed to cannon) in the infantry units from the middle of 17th century, the battlefield was dominated by linear tactics, according to which the infantry was aligned into long thin lines and fired volleys. A line consisted of 2, 3 or 4 ranks of soldiers.\n\nAccording to the instructions, the soldiers were supposed to fire at volleys at the command of officers. But in a real battle, this happened only in the first minutes of the battle. After one or two volleys, each soldier charged a musket and fired at his own discretion, without hearing the commands of the officers. Which of course brought confusion to the system, and the smoke interfered with the accurate shooting. Such a shootout in a puff of smoke could occur for a very long period of time and inflicted huge losses on both sides, and the result was unpredictable. In addition, at the time of the “hot” shootout, the soldiers were so busy and focused on shooting that they could not notice the attack of cavalry from the flank. Therefore, experienced officers tried to avoid such costly shootouts and restrained their soldiers from premature firing, in order to get closer to the enemy’s line as close as possible and solve the battle with several crushing volleys at a short distance. In some cases, it was possible to overturn the enemy with just one volley at a short distance. \nThe line was considered as the fundamental battle formation as it allowed for the largest deployment of firepower. Troops in skirmish formation, though able to take cover and use initiative, were highly vulnerable to cavalry and could not hold ground against advancing infantry columns. Line infantry provided an 'anchor' for skirmishers and cavalry to retreat to if threatened.\n\nAgainst surrounding enemy cavalry, line infantry could swiftly adopt square formations to provide protection. Such squares were hollow (consisting of four lines), unlike the pikemen' and old-style musketeers' square.\n\nMovement in line formation was very slow, and unless the battalion was superbly trained, a breakdown in cohesion was virtually assured, especially in any kind of uneven or wooded terrain. As a result, line was mostly used as a stationary formation, with troops moving in column formations and then deploying to line at their destination. Usually, columns would be adopted for movement and melee attacks.\n\nLine infantry was trained in the manual of arms evolutions, the main objectives of which were fast deployment of a line, rapid shooting and manoeuvre.\n\nLine tactics required a strict discipline and simple movements, practiced to the point where they became second-nature. During training, the drill and corporal punishments were widely used.\n\nLine infantry quickly became the most common type of infantry in European countries. Musketeers and grenadiers, formerly elite troops, gradually became part of the line infantry, switching to linear tactics.\n\nOver time the use of line infantry tactics spread outside of Europe, often as a result of European imperialism. In European colonies and settlements with small populations from the home country, line infantry forces were often raised from the local population, with the British East India Company's sepoys perhaps being the most historically significant example.\n\nDuring 1814, the War of the Six Coalition, regular French line infantry recruits were trained very infirmitively due to the fierce attack of the Coalition Forces. A recruit was trained by firing two cartridges and four blanks. There was also light training of forming several formations. By these examples, forming a massive extent of well trained, elite line infantry was a very complicated process.\n\nIn the middle of the 16th century, the matchlock muskets of some line infantry were equipped with bayonets. Bayonets were attached to the muzzles of muskets and were used when line troops entered melee combat. They also helped to defend against cavalry.\n\nAt the end of the 17th century, a flaw within the design of matchlock muskets became more apparent. Since the matchlock musket used a slow burning piece of twine know as a slow match, the twine sometimes would accidentally set fire to the gunpowder reservoir in the musket prematurely setting off all of the gunpowder and bringing serious injury and death to the operator. During this time, matchlock muskets began to be replaced by lighter and cheaper infantry fusils with flintlocks, weighing 5 kg with a caliber of 17.5 mm, first in France and then in other countries. In many countries, the new fusils retained the name \"musket\". Both muskets and fusils were smoothbore, which lessened their accuracy and range, but made for faster loading, lesser amount of bore fouling and more robust, less complicated firearms.\n\nThe range and accuracy of smooth-bore muskets was in the range of 300-400 yards against the line of infantry / cavalry. In a single enemy could get from a distance of no more than 50-100 yards. It should be borne in mind that ordinary linear infantrymen were poorly trained in aimed shooting, due to the saving of gunpowder and lead (modern reenactors achieve much better results by firing smooth-bore muskets). But the line infantrymen were well trained in fast reloading muskets. The recruit should have done at least 3 rounds a minute, and an experienced soldier could have done up to 6 rounds per minute.\n\nThe bulk of the line infantry had no protective equipment, as armor that could provide protection from musket fire were considered too expensive and heavy. Only the former elite troops could keep by tradition some elements of protection, for example, the copper mitre caps of grenadiers.\n\nInitially, soldiers equipped with firearms formed only a small part of the infantry branch of most armies, because of their vulnerability to hostile cavalry. Pikemen formed the majority of infantrymen and were known as heavy infantry. A significant part of infantry consisted of old-style musketeers, who did not use the linear tactics, instead skirmishing in open formation. However, by the middle of the 17th century, musketeers deployed in line formation already provided about half of the foot troops in most Western European armies. Maurice of Nassau was noted as the first large scale user of linear tactic, introducing the 'counter-march' to enable his formations of musketeers to maintain a continuous fire. After the invention of the bayonet, musketeers could finally defend themselves from the enemy's horsemen, and the percentage of pikemen fell gradually. In 1699, the Austrian army got rid of their pikes. In 1703, the French army did the same, in 1704 the British and 1708 the Dutch. In 1699–1721, Peter I converted almost all Russian foot-regiments to line infantry. The abandonment of the pike, together with the faster firing rate made possible by the introduction of the new flintlock musket and paper cartridge, resulted in the abandonment of the deeper formations of troops more ideal for the melee-oriented pikemen. Instead, military thinking switched to shallower lines that maximized the firepower of an infantry formation.\n\nBesides regular line infantry, there were elite troops (royal guards and other designated elite regiments) and the light infantry. Light infantry operated in extended order (also known as skirmish formation) as opposed to the close order (tight formations) used by line infantry. Since the late 18th century, light infantry in most European countries mostly consisted of riflemen (such as the German Jäger), armed with rifled carbines and trained in aimed shooting and use of defilades. In England, much of the light infantry was armed with smooth-bore muskets, only a few regiments used rifled muskets. In the Russian Empire, light infantry was forming at a very fast pace; by the end of the 18th century, light infantry regiments numbered 40,000 soldiers (jaeger). In the Russian army, light infantry weapons were significantly different from line infantry. Light infantrymen were armed with short, high-quality manufacturing muskets. In addition, each light infantryman was armed with a pistol with a flintlock. Line infantry, whose muskets with bayonets were heavier than carbines, became known as heavy infantry and were used as the main deciding force.\n\nIn France, during the Revolutionary and Napoleonic Wars, the division into the Guard, while line infantry and light infantry formally continued to exist, line and \"light\" regiments had identical weaponry (smooth-bore fusils) and tactics. (Napoleon preferred smooth-bore weaponry for their faster reload speeds.) However, each battalion in both line and \"light\" regiments included a company of voltigeurs, who were expected to act as skirmishers as well being able to deploy into line.\n\nAfter the unsuccessful army reforms of Paul I, the number of light infantry in the Russian army was significantly reduced and made up only 8% of the entire field infantry. But soon the Russian army returned to the trend of increasing the number of light infantry, begun in the 18th century. By 1811, 50 light infantry regiments were formed in the Russian army. In addition, in each linear battalion it was required to have 100 of the best shooting soldiers who fought in a loose ranks and covered their battalions from the enemy skirmishers. The total number of light infantry reached 40% of the entire field infantry. Unfortunately, the sharp increase in the number of light infantry greatly influenced their quality of training and equipment. The Russian infantry of 1854 comprised 108 regiments, of which 42 were line infantry. The remainder were specialized or elite units such as Guards, Grenadiers and Jägers. Only part of the Russian light infantry were equipped with the M1854 rifle, the remainder retaining smoothbore percussion muskets.\n\nIn the second half of the 19th century, the coming of mass production and new technologies, such as the Minie ball, allowed European armies to gradually equip all their infantrymen with rifled weapons, and the percentage of line infantry equipped with muskets fell. In the American Civil War, both Northern and Confederate armies had only a few line regiments equipped with the old-style smooth-bore muskets. However, France, due to Napoleon III, who admired Napoleon I, had 300 line battalions (comprising an overwhelming majority) even in 1870. Although the French line infantry received Chassepot rifles in 1866, it was still being trained in the use of close formations (line, column and square), which was changed only after the dethronement of Napoleon III. This was common practice in all conventional Western armies until the late 19th century, as infantry tactics and military thinking had yet to catch up with the new technological development.\n\nIn the years after the Napoleonic Wars, line infantry continued to be deployed as the main battle force, while light infantry provided fire support and covered the movement of units. In Russia, Great Britain, France, Prussia and some other states, linear tactics and formation discipline were maintained into the late 19th century.\n\nWith the invention of new weaponry, the concept of line infantry began to wane. The Minié ball (an improved rifle ammunition), allowed individual infantrymen to shoot more accurately and over greatly increased range. Men walking in formation line-abreast became easy targets, as evidenced in the American Civil War. The Austro-Prussian War in 1866 showed that breech-loading rifles, which gave the individual shooter a greatly increased rate of fire, were greatly superior to muzzleloaded rifles. In the 1860s, most German states and Russia converted their line infantry and riflemen into 'united' infantry, which used rifles and skirmish tactics. After the Franco-Prussian War, both the German Empire and the French Third Republic did the same. However, Great Britain retained the name \"line infantry\", although it used rifled muskets from 1853, breech loading rifles from 1867, and switched from closed lines to extended order during Boer wars.\n\nThe growing accuracy and rate of fire of rifles, together with the invention of the Maxim machine gun in 1883, meant that close order line infantry would suffer huge losses before being able to close with their foe, while the defensive advantages given to line infantry against cavalry became irrelevant with the effective removal of offensive cavalry from the battlefield in the face of the improved weaponry. With the turn of the 20th Century, this slowly led to infantry increasingly adopting skirmish style light infantry tactics in battle, while retaining line infantry drill for training.\n\nWhile, as detailed above, linear battle tactics had become obsolete by the second half of the nineteenth century, regiments in a number of European armies continued to be classified as \"line infantry\" (or cavalry). This designation had come to mean the regular or numbered regiments of an army, as opposed to specialist or elite formations. Accordingly, the distinction had become one of traditional title or classification without significance in respect of armament or tactics. As an example, the \nBelgian Army of 1914 comprised 14 regiments of \"Infanterie de Ligne\" (line infantry), three of \"Chasseurs a pied\" (light infantry), one of \"Grenadiers\" and one of \"Carabiniers\". Similar differentiations were made in the majority of European armies of the period, although English-speaking authors sometimes use the designation \"line infantry\" when referring to the ordinary infantry of some other countries where the exact term was not in use.\n\nThe term was also used by US units during the Second World War, as shown by this quote from a report of the 782nd Tank Battalion in late April 1945:\nOn the 22nd of April, the Battalion moved from Oberkotzau, Germany to Wunsiedel, Germany. Here the attachment of the line companies to the Regimental Combat Teams of the 97th Division was completed. We separated, not coming together again until the war was over. Company \"A\" joined the 303rd at Rehau, Germany: Company \"B\" joined the 386th at Arzburg, Germany: and Company \"C\" the 387th at Waldsassen, Germany.\n\nThe modern British Army retains the traditional distinction between \"Guards\", \"Line Infantry\" and \"the Rifles\" on ceremonial occasions for historical reasons. It is linked to the order of precedence within the British Army and regimental pride, so for example Colonel Patrick Crowley states in the \"introduction\" in \"A Brief History of The Princess of Wales’s Royal Regiment\" (2015):\n\nInfantry of most 21st-century armies are still trained in formation manoeuvre and drill, as a way of instilling discipline and unit cohesion. Members of the US Army utilize the term \"line company\" (informally) in light infantry battalions to differentiate those companies (generally A–D) that perform the traditional infantry role from the support companies (generally F and HHC) charged with supporting the \"line companies\". The Marine Corps does the same for all its infantry units. In this vein, officers assigned to the rifle companies are referred to as \"line officers\" while billeted to positions such as Platoon Leaders and Commanding and Executive Officers.\n\n"}
{"id": "17950868", "url": "https://en.wikipedia.org/wiki?curid=17950868", "title": "Locally discrete collection", "text": "Locally discrete collection\n\nIn mathematics, particularly topology, collections of subsets are said to be locally discrete if they look like they have precisely one element from a local point of view. The study of locally discrete collections is worthwhile as Bing's metrization theorem shows.\n\nLet \"X\" be a topological space. A collection {G} of subsets of \"X\" is said to be locally discrete, if each point of the space has a neighbourhood intersecting at most one element of the collection. A collection of subsets of \"X\" is said to be countably locally discrete, if it is the countable union of locally discrete collections.\n\n1. Locally discrete collections are always locally finite. See the page on local finiteness.\n\n2. If a collection of subsets of a topological space X is locally discrete, it must satisfy the property that each point of the space belongs to at most one element of the collection. This means that only collections of pairwise disjoint sets can be locally discrete. \n\n3. A Hausdorff space cannot have a locally discrete basis unless it is itself discrete. The same property holds for a T space.\n\n4. The following is known as Bing's metrization theorem:\n\nA space \"X\" is metrizable iff it is regular and has a basis that is countably locally discrete.\n\n5. A countable collection of sets is necessarily countably locally discrete. Therefore, if X is a metrizable space with a countable basis, one implication of Bing's metrization theorem holds. In fact, Bing's metrization theorem is almost a corollary of the Nagata-Smirnov theorem.\n\n\n"}
{"id": "325499", "url": "https://en.wikipedia.org/wiki?curid=325499", "title": "Marginal cost", "text": "Marginal cost\n\nIn economics, marginal cost is the change in the opportunity cost that arises when the quantity produced is incremented by one unit, that is, it is the cost of producing one more unit of a good. Intuitively, marginal cost at each level of production includes the cost of any additional inputs required to produce the next unit. At each level of production and time period being considered, marginal costs include all costs that vary with the level of production, whereas other costs that do not vary with production are considered fixed. For example, the marginal cost of producing an automobile will generally include the costs of labor and parts needed for the additional automobile and not the fixed costs of the factory that have already been incurred. In practice, marginal analysis is segregated into short and long-run cases, so that, over the long run, all costs (including fixed costs) become marginal.\n\nIf the cost function formula_1 is differentiable, the marginal cost formula_2 is the first derivative of the cost function with respect to the quantity formula_3.\nThe marginal cost can be a function of quantity if the cost function is non-linear. If the cost function is not differentiable, the marginal cost can be expressed as follows.\nwhere formula_6 denotes an incremental change of one unit.\n\nIn the simplest case, the total cost function and its derivative are expressed as follows, where Q represents the production quantity, VC represents variable costs, FC represents fixed costs and TC represents total costs.\n\nSince (by definition) fixed costs do not vary with production quantity, it drops out of the equation when it is differentiated. The important conclusion is that marginal cost \"is not related to\" fixed costs. This can be compared with average total cost or ATC, which is the total cost divided by the number of units produced and \"does\" include fixed costs.\n\nFor discrete calculation without calculus, marginal cost equals the change in total (or variable) cost that comes with each additional unit produced. In contrast, incremental cost is the composition of total cost from the surrogate of contributions, where any increment is determined by the contribution of the cost factors, not necessarily by single units.\n\nFor instance, suppose the total cost of making 1 shoe is $30 and the total cost of making 2 shoes is $40. The marginal cost of producing the second shoe is $40 – $30 = $10.\n\nMarginal cost is not the cost of producing the \"next\" or \"last\" unit. As Silberberg and Suen note, the cost of the last unit is the same as the cost of the first unit and every other unit. In the short run, increasing production requires using more of the variable input — conventionally assumed to be labor. Adding more labor to a fixed capital stock reduces the marginal product of labor because of the diminishing marginal returns. This reduction in productivity is not limited to the additional labor needed to produce the marginal unit – the productivity of every unit of labor is reduced. Thus the costs of producing the marginal unit of output has two components:\nthe cost associated with producing the marginal unit\nand the increase in average costs for all units produced due to the \"damage\" to the entire productive process (∂AC/∂q)q. \nThe first component is the per unit or average cost. The second unit is the small increase in costs due to the law of diminishing marginal returns which increases the costs of all units of sold.\n\nMarginal costs can also be expressed as the cost per unit of labor divided by the marginal product of labor.\n\nBecause formula_11 is the change in quantity of labor that affects a one unit change in output, this implies that this equals formula_12.\n\nwhere MPL is the ratio of increase in the quantity produced per unit increase in labour i.e. ΔQ/ΔL.\nTherefore, formula_13 Since the wage rate is assumed constant, marginal cost and marginal product of labor have an inverse relationship—if marginal cost is increasing (decreasing) the marginal product of labor is decreasing (increasing).\n\nEconomies of scale applies to the long run, a span of time in which all inputs can be varied by the firm so that there are no fixed inputs or fixed costs. Production may be subject to economies of scale (or diseconomies of scale). Economies of scale are said to exist if an additional unit of output can be produced for less than the average of all previous units— that is, if long-run marginal cost is below long-run average cost, so the latter is falling. Conversely, there may be levels of production where marginal cost is higher than average cost, and the average cost is an increasing function of output. For this generic case, minimum average cost occurs at the point where average cost and marginal cost are equal (when plotted, the marginal cost curve intersects the average cost curve from below); this point will \"not\" be at the minimum for marginal cost if fixed costs are greater than 0.\n\nThe portion of the marginal cost curve above its intersection with the average variable cost curve is the supply curve for a firm operating in a perfectly competitive market. (the portion of the MC curve below its intersection with the AVC curve is not part of the supply curve because a firm would not operate at price below the shutdown point) This is not true for firms operating in other market structures. For example, while a monopoly \"has\" an MC curve it does not have a supply curve. In a perfectly competitive market, a supply curve shows the quantity a seller's willing and able to supply at each price – for each price, there is a unique quantity that would be supplied. The one-to-one relationship simply is absent in the case of a monopoly. With a monopoly, there could be an infinite number of prices associated with a given quantity. It all depends on the shape and position of the demand curve and its accompanying marginal revenue curve.\n\nIn perfectly competitive markets, firms decide the quantity to be produced based on marginal costs and sale price. If the sale price is higher than the marginal cost, then they supply the unit and sell it. If the marginal cost is higher than the price, it would not be profitable to produce it. So the production will be carried out until the marginal cost is equal to the sale price. In other words, firms refuse to sell if the marginal cost is greater than the market price.\n\nMarginal costs are not affected by changes in fixed cost. Marginal costs can be expressed as ∆C(q)∕∆Q. Since fixed costs do not vary with (depend on) changes in quantity, MC is ∆VC∕∆Q. Thus if fixed cost were to double, the cost of MC would not be affected, and consequently, the profit-maximizing quantity and price would not change. This can be illustrated by graphing the short run total cost curve and the short-run variable cost curve. The shapes of the curves are identical. Each curve initially increases at a decreasing rate, reaches an inflection point, then increases at an increasing rate. The only difference between the curves is that the SRVC curve begins from the origin while the SRTC curve originates on the y-axis. The distance of the origin of the SRTC above the origin represents the fixed cost – the vertical distance between the curves. This distance remains constant as the quantity produced, Q, increases. MC is the slope of the SRVC curve. A change in fixed cost would be reflected by a change in the vertical distance between the SRTC and SRVC curve. Any such change would have no effect on the shape of the SRVC curve and therefore its slope at any point – MC.\n\nOf great importance in the theory of marginal cost is the distinction between the marginal \"private\" and \"social\" costs. The marginal private cost shows the cost associated to the firm in question. It is the marginal private cost that is used by business decision makers in their profit maximization goals. Marginal social cost is similar to private cost in that it includes the cost of private enterprise but \"also\" any other cost (or offsetting benefit) to society to parties having no direct association with purchase or sale of the product. It incorporates all negative and positive externalities, of both production and consumption. Examples might include a social cost from air pollution affecting third parties or a social benefit from flu shots protecting others from infection.\n\nExternalities are costs (or benefits) that are not borne by the parties to the economic transaction. A producer may, for example, pollute the environment, and others may bear those costs. A consumer may consume a good which produces benefits for society, such as education; because the individual does not receive all of the benefits, he may consume less than efficiency would suggest. Alternatively, an individual may be a smoker or alcoholic and impose costs on others. In these cases, production or consumption of the good in question may differ from the optimum level.\n\nMuch of the time, private and social costs do not diverge from one another, but at times social costs may be either greater or less than private costs. When marginal social costs of production are greater than that of the private cost function, we see the occurrence of a negative externality of production. Productive processes that result in pollution are a textbook example of production that creates negative externalities.\n\nSuch externalities are a result of firms externalizing their costs onto a third party in order to reduce their own total cost. As a result of externalizing such costs, we see that members of society will be negatively affected by such behavior of the firm. In this case, we see that an increased cost of production in society creates a social cost curve that depicts a greater cost than the private cost curve.\n\nIn an equilibrium state, we see that markets creating negative externalities of production will overproduce that good. As a result, the socially optimal production level would be lower than that observed.\n\nWhen marginal social costs of production are less than that of the private cost function, we see the occurrence of a positive externality of production. Production of public goods are a textbook example of production that create positive externalities. An example of such a public good, which creates a divergence in social and private costs, includes the production of education. It is often seen that education is a positive for any whole society, as well as a positive for those directly involved in the market.\n\nExamining the relevant diagram we see that such production creates a social cost curve that is less than that of the private curve. In an equilibrium state, we see that markets creating positive externalities of production will underproduce that good. As a result, the socially optimal production level would be greater than that observed.\n"}
{"id": "32401141", "url": "https://en.wikipedia.org/wiki?curid=32401141", "title": "Maxine Syjuco", "text": "Maxine Syjuco\n\nMaxine Syjuco (born December 1, 1984) is an artist, poet and model from Manila, Philippines. She is the youngest daughter of avant-garde artists Cesare Syjuco and Jean Marie Syjuco, and is the vocalist and songwriter of the experimental Art Rock band Jack of None.\n\nWorking mainly with photography, digital collage, painting and installation art, her work possesses an intimate, philosophical, and often twisted voice in storytelling. Manipulating faces, unveiling poignant surrealities, and probing into abandoned rooms of the subconscious, Maxine authors dark, thought-provoking introspections of the human condition and its fragility—conjuring a netherworld where ghostly images exist neither here nor there.\n\nHer first book of poetry, \"A Secret Life,\" was published in 2008 and received critical acclaim. Her poems have been translated into Polish and French, and have seen print in several international anthologies including the Asia Literary Review, The Poet’s Guild Quarterly, Chopin with Cherries, and the 2011 Rhino International Poetry Anthology.\n\nIn 2016, she was awarded the Independent Music Award in New York City, U.S.A., for Best Album Art, Photography and Design. In the same competition, her band, Jack of None, received 2 category nominations (Best Spoken Word Song, and Best Album of the Year) for their debut album titled \"Who's Listening to Van Gogh's Ear?\".\n\nMaxine's exhibits and poetry-performances have taken place in venues as varied as the Cultural Center of the Philippines, the Metropolitan Museum of Manila, the Tokiwa Museum of Japan, the Hangaram Museum of Korea, the Art Takes Miami Exhibition in the U.S.A., and others.\n"}
{"id": "31384378", "url": "https://en.wikipedia.org/wiki?curid=31384378", "title": "Misplaced loyalty", "text": "Misplaced loyalty\n\nMisplaced loyalty (or mistaken loyalty, misguided loyalty or misplaced trust) is loyalty placed in other persons or organisations where that loyalty is not acknowledged or respected; is betrayed or taken advantage of. It can also mean loyalty to a malignant or misguided cause.\n\nSocial psychology provides a partial explanation for the phenomenon in the way \"[T]he norm of social commitment directs us to honor our agreements. [...] People usually stick to the deal even though it has changed for the worse\". Humanists point out that \"[M]an inherits the capacity for loyalty, but not the use to which he shall put it [...] may unselfishly devote himself to what is petty or vile, as he may to what is generous and noble\".\n\nPart of the conventional therapeutic wisdom is 'that those of us who were unlucky enough to be raised by bad parents also get to be burdened as adults by their demands...we maintain a sense of misguided loyalty'. Under the rubric - 'Misplaced Loyalty: The Codependency Factor' - the self-help movement would strongly challenge such loyalty: 'in either individual therapy or self-help groups, the goal is to seek out and replace our misguided loyalty and attachment to our failed parents with attachment to healthier peers'.\n\nPsychoanalysis would highlight the accompanying paradox that 'the child, it should be remembered, always defends the bad parent more ferociously than the good'. The paradox may help account for what have been called 'trauma bonds...the misplaced loyalties found in exploitive cults, incest families, or hostage and kidnapping situations, or codependents who live with alcoholics, compulsive gamblers or sex addicts'.\n\n'Institutions develop powerful instruments of defence for their protection and perpetuation...develop misguided loyalty to committee and boards. To criticize forcibly rather than to cover up is to rock the boat'. Similarly, there are 'examples where misguided loyalty on the part of a business owner or manager has led to a decline in a business's performance'.\n\nSometimes, however, institutions are torn by conflicting codes of loyalty. Thus in the police, in-force loyalty, which 'has sometimes caused officers to lie and cheat on behalf of others...is now regarded as misplaced loyalty': in partial palliation, 'it must be understood that this \"looking after one's mates\" is a critical element of loyalty for those who face combat'.\n\nThe charge of misplaced loyalty is often used as a weapon in analytic disputes. Lacan for example criticised Ernest Kris for the way 'he accredits this interpretation to \"ego psychology\" \"à la\" Hartmann, whom he believed he was under some obligation to support'.\n\nSimilarly, Neville Symington's 'criticism of Melanie Klein is that...she maintained the concept of the death instinct in order to remain loyal to Freud's instinct theory, but it only muddles her otherwise clear formulations'.\n\n\n\n\n"}
{"id": "2220463", "url": "https://en.wikipedia.org/wiki?curid=2220463", "title": "Missing white woman syndrome", "text": "Missing white woman syndrome\n\nMissing white woman syndrome is a phenomenon noted by social scientists and media commentators of the extensive media coverage, especially in television, of missing person cases involving young, white, upper-middle-class women or girls. The phenomenon is defined as the Western media's undue focus on upper-middle-class white women who disappear, with the disproportionate degree of coverage they receive being compared to cases of missing women of color and missing men and women of lower social classes. Although the term was coined to describe disproportionate coverage of missing person cases, it is sometimes used to describe similar disparities in news coverage of other violent crimes. Instances have been cited in the United States, Canada, the United Kingdom and South Africa.\n\nPBS news anchor Gwen Ifill is said to be the originator of the phrase. Charlton McIlwain, a professor at New York University, defines the syndrome as white women perpetually occupying a privileged role as violent crime victims in news media reporting, and concludes that missing white woman syndrome functions as a type of racial hierarchy in the cultural imagery of the West. Professor Eduardo Bonilla-Silva categorizes the racial component of missing white woman syndrome as a form of racial grammar, through which white supremacy is normalized by implicit or even invisible standards.\n\nMissing white woman syndrome has led to a number of right-wing tough on crime measures that were named for white women who disappeared and were subsequently found harmed. Moody, Dorris and Blackwell (2008) concluded that in addition to race and class, factors such as supposed attractiveness, body size and youthfulness function as unfair criteria in the determination of newsworthiness in coverage of missing women. Also noteworthy was that news coverage of missing black women was more likely to focus on the victim's baggage, such as abusive boyfriends or a troubled past, while coverage of white women tends to focus on their roles as mothers or daughters.\n\nWith regard to missing children, statistical research which compares national media reports with FBI data shows that there is marked under-representation of African American children in media reports relative to non-African American children. A subsequent study found that girls from minority groups were the most under-represented in these missing-children news reports by a very large margin.\n\nZach Sommers, a sociologist at Northwestern University, noted that while there is a sizable body of research that shows that white people are more likely than people of color to appear in news coverage as victims of violent crime there is relatively little when it comes to missing persons cases. In 2013, Sommers cross-referenced the missing persons coverage of four national and local media outlets against the FBI's missing persons database. Sommers found black people received disproportionately less coverage than whites and men received disproportionately less coverage than women; Sommers could not directly assess the number of missing white women in the FBI files due to how the data was structured but concluded that there was circumstantial—although not statistically conclusive—evidence that white women received disproportionate coverage. In the same study, professor Eduardo Bonilla-Silva elucidated that the subtle standard of placing a premium on white lives in the news helps to maintain and reinforce a racial hierarchy with whites at the top. For example, black women are members of both a marginalized racial group and a marginalized gender group. Crucially, though, black women have an “intersectional experience [that] is greater than the sum of racism and sexism.” In other words, like white women, black women are subject to sexism, but the form of that sexism differs for black women because of the compounding effects of racial discrimination; with missing white woman syndrome being a pertinent manifestation of this social phenomenon. Sociologists note that the tone of media coverage for black female victims differs markedly from coverage of white female victims in that the former are more likely to be blamed for purportedly putting themselves in harm's way, either knowingly or unknowingly. Victim blaming in this context reinforces the notion that black female victims are not only less innocent, but also less worthy of rescue relative to white women. Other observers note the lack of publicity given to black female victims of police brutality in news coverage, attributing the silence to a tradition of “sexism and patriarchy” in American society.\n\nA report that aired on CNN noted the differences between the level of media coverage given to Caucasian women like Laci Peterson and Natalee Holloway, who disappeared in 2002 and 2005 respectively, and LaToyia Figueroa, a pregnant Black Hispanic woman. Figueroa disappeared in Philadelphia the same year Holloway disappeared. Figueroa and her unborn daughter were found murdered. The San Francisco Chronicle published an article detailing the disparity between the coverage of the Peterson case and that of Evelyn Hernandez, a Hispanic woman who was nine months pregnant when she disappeared in 2002.\n\nKym Pasqualini, president of the National Center for Missing Adults, observed that media outlets tend to focus on \"damsels in distress\" – typically, affluent young white women and teenagers.\n\nIn a 2016 \"Esquire\" article about the disappearance of Tiffany Whitton, journalist Tom Junod observed that women of lower social status such as Whitton, a 26-year-old unemployed drug addict who was on parole, do not get much media attention as \"media outlets are ruthlessly selective, and they tend to prefer women who are white, pretty, and, above all, innocent\". Whitton's mother stated that producers of shows like \"Nancy Grace\" told her they weren't interested in her daughter's case. Dr. Cory L. Armstrong wrote in the \"Washington Post\" that \"the pattern of choosing only young, white, middle-class women for the full damsel treatment says a lot about a nation that likes to believe it has consigned race and class to irrelevance\". \n\nAccording to a study published in \"The Law and Society Association\", aboriginal women who go missing in Canada receive 27 times less news coverage than white women; they also receive \"dispassionate and less-detailed, headlines, articles, and images.\"\n\nUniversity of Leicester Criminology Professor Yvonne Jewkes cites the murder of Milly Dowler, the murder of Sarah Payne, and the Soham murders as examples of \"eminently newsworthy stories\" about girls from \"respectable\" middle-class families and backgrounds whose parents used the news media effectively. She writes that, in contrast, the street murder of Damilola Taylor, a 10-year- old boy from Nigeria, initially received little news coverage, with reports initially concentrating upon street crime levels and community policing, and largely ignoring the victim. Even when Damilola's father flew into the UK from Nigeria to make press statements and television appearances, the level of public outcry did not, Jewkes asserts, reach \"the near hysterical outpourings of anger and sadness that accompanied the deaths of Sarah, Milly, Holly, and Jessica\".\n\nIn January 2006, London Police Commissioner Ian Blair described the media as institutionally racist. As an example, he had referred to the murder of two young girls in Soham in 2002. He said \"almost nobody\" understood why it became such a big story. Two cases of missing white girl syndrome that have been given as contrasting examples: the murder of Hannah Williams and the murder of Danielle Jones. It was suggested that Jones received more coverage than Williams because Jones was a middle-class schoolgirl, whilst Williams was from a working-class background with a stud in her nose and estranged parents.\n\nSandile Memela, chief director for social cohesion at South Africa's Department of Arts and Culture, noted amidst the Oscar Pistorius trial that there existed substantial differences between how media outlets reported on the murders of Reeva Steenkamp and Zanele Khumalo; two South African models, respectively white and black, who had been murdered by their boyfriends under nearly identical circumstances. Memela asserted that the discrepancy between the media coverage of the Steenkamp and Khumala murders amounted to \"structural racism\" within South African society, and stated: \"As a country we seem to have chosen to ignore the agony, pain and suffering of the Khumalo family for no other reason than that they are black.\"\n\nOn September 11, 2014, the South African news network SABC3 aired an investigative report which raised concerns around the “Missing White Woman Syndrome”; where the death of Steenkamp was juxtaposed with the death of Zanele Khumalo.\n\nSocial commentaries pointed to media bias in the coverage of soldier Jessica Lynch versus that of her fellow soldiers, Shoshana Johnson and Lori Piestewa. All three were ambushed in the same attack during the Iraq War on March 23, 2003, with Piestewa being killed and Lynch and Johnson being injured and taken prisoner. Lynch, a young, blonde, white woman, received far more media coverage than Johnson (a black woman and a single mother) and Piestewa (a Hopi from an impoverished background, and also a single mother), with media critics suggesting that the media gave more attention to the woman with whom audiences supposedly more readily identify.\nLynch herself leveled harsh criticism at this disproportionate coverage that focused only on her, stating in a congressional testimony before the United States House Committee on Oversight and Government Reform:\n\nIn October 2013, a girl estimated to be about 4 years of age was found in the custody of a Roma couple in Greece and was presumed to have been abducted. The story about the \"blonde angel\" and the search for her biological parents received international media coverage. A Romani rights activist commented on the case to say \"imagine if the situation were reversed and the children were brown and the parents were white\". The child was later identified as Maria Ruseva. Her biological mother was a Bulgarian Roma who gave Maria up for adoption.\n\nCritics have also cited excessive media coverage of murder trials where the defendant is female, white, young and attractive, and included them along with Missing White Woman Syndrome instances in an all-encompassing narrative nicknamed the \"woman in jeopardy\" or \"damsel in distress\" genre. In such cases, the media will focus on the accused, rather than the victim as in Missing White Woman Syndrome cases, and they will be more ambiguous about their guilt than in other criminal cases regardless of evidence. Cited examples include Amanda Knox, Jodi Arias and Casey Anthony.\n\nThe following missing person cases have been cited as instances of missing white woman syndrome; media commentators on the phenomenon regard them as garnering a disproportionate level of media coverage relative to contemporary cases involving missing girls or women of non-white ethnicities, and missing males of all ethnicities. The date of death or disappearance is given in parentheses.\n\nThe following missing person cases have been expressly compared to contemporary missing person cases labeled as examples of \"Missing White Woman Syndrome\", in order to hightlight differences in coverage between them. The date of death or disappearance is given in parentheses.\n\n\n\n"}
{"id": "28040030", "url": "https://en.wikipedia.org/wiki?curid=28040030", "title": "Moon Museum", "text": "Moon Museum\n\nMoon Museum is a small ceramic wafer three-quarters of an inch by half an inch in size, containing artworks by six prominent artists from the late 1960s. The artists with works in the \"museum\" are Robert Rauschenberg, , John Chamberlain, Claes Oldenburg, Forrest Myers and Andy Warhol. \n\nThis wafer was supposedly covertly attached to a leg of the Lunar Module \"Intrepid\", and subsequently left on the Moon during Apollo 12. \"Moon Museum\" is considered the first Space Art object. While it is impossible to tell if \"Moon Museum\" is actually on the Moon without sending another mission to look, technicians have admitted to placing personal effects onto the Apollo landers, hidden in the layers of gold blankets that wrapped parts of the spacecraft which remained on the Moon after the astronauts departed.\n\nThe concept for \"Moon Museum\" was brainstormed by Forrest \"Frosty\" Myers. He stated that \"My idea was to get six great artists together and make a tiny little museum that would be on the moon.\" Myers attempted several times to get his project sanctioned by NASA. He claims the agency gave him the runaround and, Myers states, \"They never said no, I just could not get them to say anything.\" Instead of going through the official channels, he was forced to take the back route and try to smuggle it on board.\nMyers contacted Experiments in Art and Technology (E.A.T.), a non-profit group that was linking artists with engineers to create new works. Through E.A.T., Myers was introduced to some scientists from Bell Laboratories, specifically Fred Waldhauer. Using techniques normally used to produce telephone circuits, the scientists etched the drawings Myers had gathered onto small ceramic wafers. Either 16 or 20 of these wafers were created, with one going on the lunar lander and the rest, copies of the original, handed out to the artists and others involved in the project.\n\nWhen NASA dithered whether the wafer would be allowed onto the module, Waldhauer devised another plan. Waldhauer knew a Grumman Aircraft engineer who was working on the Apollo 12 lander module, and he proved willing to place the wafer on it. Myers asked Waldhauer how he would know if the art actually made it onto the lander, and was told that the person who worked for Grumman would send him a telegram when the wafer was in place. At 3:35 p.m. on November 12, 1969, less than two days before Apollo 12 took off, Myers received a telegram at his house from Cape Canaveral, Florida stating \"YOUR ON' A.O.K. ALL SYSTEMS GO,\" and signed \"JOHN F.\"\n\nThe existence of the work was not revealed until Myers informed \"The New York Times\" which ran an article on the story two days after Apollo 12 left the Moon and two days before they splashed down in the Pacific Ocean.\n\nThere are six artworks located on the ceramic tile, each one in black and white. Starting in the top center is a single line by Robert Rauschenberg. To its right is a black square with thin white lines intersecting, resembling a piece of circuitry, by David Novros. Below it is John Chamberlain's contribution, a template pattern which also resembles circuitry. In the lower middle is a geometric variation on Mickey Mouse, by Claes Oldenburg, a popular motif for the artist at that time. Forrest Myers created the work in the lower left, a computer-generated drawing of a \"linked symbol\" called \"Interconnection\". Finally, the last drawing in the upper left is by Andy Warhol. He created a stylized version of his initials which, when viewed at certain angles, can appear as a rocket ship or a penis. \"He was being the terrible bad boy,\" said Forrest Myers in an interview. Warhol's work is covered by a thumb in the image often associated with \"Moon Museum\", but other images with the drawing visible can be found.\n\nBoth John Chamberlain and Claes Oldenburg have confirmed through representatives that they contributed drawings to \"Moon Museum\".\n\n\n"}
{"id": "58519746", "url": "https://en.wikipedia.org/wiki?curid=58519746", "title": "Motherhood Studies", "text": "Motherhood Studies\n\nMotherhood Studies is a recognized field of study coined by Dr. Andrea O'Reilly. It is related to maternal feminism.\n\nThe idea of motherhood studies was influenced by Adrienne Rich’s \"Of Woman Born: Motherhood as Experience and Institution\" in 1976. It is consists of three interconnected categories of inquiry: motherhood as institution, motherhood as experience, and motherhood as identity or subjectivity. Motherhood studies is often referred to as a feminist practice. Feminist mothering critiques the sexist and patriarchal values that contemporary society upholds. Part of feminist mothering is maternal activism.\n\nSamira Kawash argues that \"the marginalization of motherhood in feminist thought was not only a political rejection of maternalist politics construed as a conservative backlash to feminism\".\n\nOne of the leading portals of mother studies is \"The m/other voices foundation\", a non-profit organization in the USA. It emerged in 2014 from Deirdre M. Donoghue's research project \"(m)other voices: the maternal as an attitude, maternal thinking and the  production of time and knowledge\" at Witte de With Centre for Contemporary Art.  The project's purpose was to initiate discourse and the doing of maternal theory within arts and other fields of cultural production and to reflect on the maternal figure as a thinker and a producer of knowledge.\n\nMore and more organized initiatives within academic communities are starting such as the \"Motherhood Initiative for Research and Community Involvement\", a feminist scholarly and activist organization on mothering-motherhood based in Toronto, Canada.\n\nThe \"Journal of the Motherhood Initiative\" re-launched in 2010 as a continuation of the \"Journal of the Association for Research on Mothering\" that started in 1999.\n\nKinser, Amber, ed. \"Mothering in the Third Wave\". Toronto: Demeter Press, 2008.\n\nRich, Adrienne. \"Of Woman Born: Mother- hood as Experience and Institution\", 1976.\n\nRoss Haller Baggesen, Lise. \"Mothernism.\"\n\nUmansky, Laurie. \"Motherhood Re- conceived\", 1996.\n\nHays, Sharon. \"The Cultural Contradictions of Motherhood\", 1996.\n\nMuseum of Motherhood\n\nManifesto for Maintenance Art, 1969\n\nMary Kelly\n\nMaternal Feminism\n"}
{"id": "36347820", "url": "https://en.wikipedia.org/wiki?curid=36347820", "title": "National Motor Freight Classification", "text": "National Motor Freight Classification\n\nThe National Motor Freight Traffic Association publishes the National Motor Freight Classification® (NMFC®), a standard that provides a comparison of commodities moving in commerce. The NMFC® is developed and maintained by the Commodity Classification Standards Board (CCSB). \n\nThe NMFC® is a voluntary standard that provides a comparison of commodities moving in interstate, intrastate and foreign commerce. It is similar in concept to the groupings or grading systems that serve many other industries. Commodities are grouped into one of 18 classes—from a low of class 50 to a high of class 500—based on an evaluation of four transportation characteristics: density, stowability, handling and liability. Together, these characteristics establish a commodity’s “transportability.”\n\nThe NMFC® also specifies minimum packaging requirements to ensure that goods are adequately protected in the motor carrier environment and can be handled and stowed in a manner that is reasonably safe and practicable. It contains various rules that govern and otherwise relate to the classification and/or packaging of commodities as well as procedures for the filing and disposition of claims, and procedures governing interline settlements. It also contains the Uniform Straight Bill of Lading, including its terms and conditions.\n\n\n"}
{"id": "19777897", "url": "https://en.wikipedia.org/wiki?curid=19777897", "title": "Nonviolent video game", "text": "Nonviolent video game\n\nNonviolent video games are video games characterized by little or no violence. As the term is vague, game designers, developers, and marketers that describe themselves as non-violent video game makers, as well as certain reviewers and members of the non-violent gaming community, often employ it to describe games with \"comparatively\" little or no violence. The definition has been applied flexibly to games in such purposive genres as the Christian video game. However, a number of games at the fringe of the \"non-violence\" label can only be viewed as objectively violent.\n\nThe purposes behind the development of the nonviolent genre are primarily reactionary in nature. As video quality and level of gaming technology have increased, the violent nature of some video games has gained worldwide attention from moral, political, gender, and medical/psychological quarters. The popularity of violent video games and increases in youth violence have led to much research into the degree to which video games may be blamed for societally negative behaviors. Despite the inconclusive nature of the scientific results, a number of groups have rejected violent video games as offensive and have promoted the development of non-violent alternatives. The existence of a market for such games has in turn led to the manufacture and distribution of a number of games specifically designed for the nonviolent gaming community. Video game reviewers have additionally identified a number of games belonging to traditionally violent gameplay genres as \"nonviolent\" in comparison to a typical game from the violent genre. Despite the fact that some of these games contain mild violence, many of them have entered the argot of nonviolent gamers as characteristic non-violent games.\n\nControversies surrounding the negative influences of video games are nearly as old as the medium itself. In 1964, Marshall McLuhan, a noted media theorist, suggested in his book, \"\", that \"[t]he games people play reveal a great deal about them. \" This was built upon in the early 1980s in an anti-video-game crusade spearheaded by the former Long Island PTA president, Ronnie Lamm, who spoke about her cause on the MacNeil/Lehrer NewsHour in December 1982. The same year, Surgeon General, C. Everett Koop, had suggested that games had no merit and offered little in the way of anything constructive to young people. Despite early general claims of the negative effects of video games, however, effects of these concerns were relatively minor prior to the early 1990s.\n\nDiscussion of the impact of violence in video games came as early as 1976's \"Death Race\" arcade game (a game with black and white graphics which involved running over screaming zombies). In the 1980s titles such as Exidy's \"Chiller\" (1986), Namco's \"Splatterhouse\" (1987) and Midway's \"NARC\" (1988) raised concerns about video game violence in the arcade. Home games such as Palace's \"Barbarian\" (1987) featured the ability to decapitate opponents. It was not until graphic capabilities increased and a wave of new ultra-violent titles were released in the early 1990s that the mainstream news began to pay significant attention to the phenomenon. In 1992, with Midway's release of the first \"Mortal Kombat\" video game, and then in 1993 with id's \"Doom\", genuine controversy was first ignited as the wide and growing popularity of violent video games came into direct conflict with the moral and religious ethics of concerned citizens. Protests and game-bannings followed the publicizing of these conflicts, and controversies would erupt periodically throughout the 1990s with the releases of such games as \"Dreamweb\" (1992), \"Wolfenstein 3D\" (1992), \"Mortal Kombat II\" (1993), \"Phantasmagoria\" (1995), \"Duke Nukem 3D\" (1996), \"Blood\" (1997), \"Grand Theft Auto\" (1997), \"Carmageddon\" (1997), \"Postal\" (1997), \"Mortal Kombat 3\" (1997), \"\" (1998), \"\" (1998), \"Grand Theft Auto 2\" (1999), and \"\" (1999) among others.\nIn April 1999, the fears of the media and violence-watch groups were legitimated in their eyes as investigations into the lives of Eric Harris and Dylan Klebold, the shooters in the Columbine High School massacre, revealed that they had been fans of the video game, \"Doom\", and had even created levels for it today dubbed \"the Harris levels\". A great deal of discussion of violence in video games followed this event with strong arguments made on both sides, and research into the phenomenon which had begun during the 1980s received renewed support and interest.\n\nIn December 2001, Surgeon General David Satcher, led a study on violence in youth and determined that while the impact of video games on violent behavior has yet to be determined, \"findings suggest that media violence has a relatively small impact on violence,\" and that \"meta-analysis [had demonstrated that] the overall effect size for both randomized and correlational studies was small for physical aggression and moderate for aggressive thinking.\"\n\nDespite this, the controversies and debate have persisted, and this has been the catalyst for the emergence of the non-violent video game genre. Non-violent video games are defined in the negative by a \"Modus tollendo ponens\" disjunctive argument. In other words, in order to recognize a non-violent game, an identifier must recognize the violent game as a distinct class. This has led to a degree of ambiguity in the term as it relies upon a definition of violence which for different identifiers may mean different things. In general, violence may be placed into at least three distinct categories:\n\nThe majority of these games have not been scientifically tested to see whether children learn the skills the games claim to teach.In another study performed in Chile, educational video games were put into some first and second grade classrooms. Children who had the games in their classroom showed more progress in math, reading comprehension, and spelling than the children who did not use games in their classrooms.\n\nThe history of video game development shares approximate contemporaneity with media violence research in general. In the early 1960, studies were conducted on the effects of violence in cartoons, and throughout the 1970s and 1980s a number of studies were conducted on how televised violence influenced viewers (especially younger viewers). The focus of many of these studies was on the effects of exposure of children to violence, and these studies frequently employed the social learning theory framework developed by Albert Bandura to explore violent behavioral modeling.\nWith advancements in video technology and the rise of video games containing graphic violence in the late 1980s and early 1990s, media violence research shifted to a great degree from televised violence to video game violence. Although under current debate, a number of researchers have claimed that violent games may cause more intense feelings of aggression than nonviolent games, and may trigger feelings of anger and hostility. Theoretical explanations for these types of effects have been explained in myriad theories including social cognitive theory, excitation transfer theory, priming effect and the General Aggression Model. However recent scholarship has suggested that social cognitive theories of aggression are outdated and should be retired.\n\nOne difference between video games and television which nearly all media violence studies recognize is that video games are primarily interactive while television is primarily passive in nature. Video game players identify with the character they control in the video games and there have been suggestions that the interactivity available in violent video games narrows the gap between the theory and practice of youth violence in a manner that goes beyond the effects of televised violence. Acknowledgment of the fact that, for better or worse, video games are likely to remain a part of modern society has led to a brace of comparative studies between violent games and non-violent games. As technology has advanced, such studies have adapted to include the effects of violent games and non-violent games in new media methods such as immersive virtual reality simulations.\n\nResults have varied, with some research indicating correlation between violence in video games and violence in players of the games, and other research indicating minimal if any relationship. Despite the lack of solid conclusion on the issue, the suggestion that violent games cause youth violence together with the clear popularity of violent video game genres such as the first-person shooter have led some game designers to publish non-violent alternatives.\n\nAs research supporting the view that video game violence leads to youth violence has been produced, there have been a number of lawsuits initiated by victims to gain compensation for loss alleged to have been caused by video-game-related violence. Similarly, in the US Congress and the legislatures of states and other countries, a number of legislative actions have been taken to mandate rating systems and to curb the distribution of violent video games. At times, individual games considered too violent have been censored or banned in such countries as Australia, Greece, etc.\nIn 1997, Christian conservative activist and (now former) attorney Jack Thompson brought suit against Atari, Nintendo, Sega, and Sony Computer Entertainment on behalf of the victims of Heath High School shooting in \"James v. Meow Media\". The suit was dismissed in 2000, absolving the companies of responsibility for the shooter's actions based on a lack of remedy under Kentucky tort law. In 2002, the Sixth Circuit Court of Appeals upheld the dismissal, and in 2003, the U.S. Supreme Court denied certiorari, refusing to review the case because it was not dismissed on 1st Amendment grounds.\n\nIn 2000, the County Council for St. Louis, Missouri enacted Ordinance 20,193 that barred minors from purchasing, renting, or playing violent video games deemed to contain any visual depiction or representation of realistic injury to a human or a \"human-like being\" that appealed to minors' \"morbid interest in violence.\" This ordinance was challenged in 2001 by the Interactive Digital Software Association (IDSA) as violative of freedom of expression as guaranteed by the first amendment. The IDSA cited the 7th Circuit case of \"American Amusement Machine Association v. Kendrick\" as precedent suggesting that video game content was a form of freedom of expression, however in 2002 the Eastern District Court of Missouri ultimately issued the controversial ruling that \"video games are not a form of expression protected by the First Amendment\" in \"Interactive Digital Software Association v. St. Louis County\".\n\nIn the aftermath of the Columbine High School massacre, a $5 billion lawsuit was filed in 2001 against a number of video game companies and id Software, the makers of the purportedly violent video game, \"Doom\" by victims of the tragedy. Also named in the suit were Acclaim Entertainment, Activision, Capcom, Eidos Interactive, GT Interactive Software, Interplay Entertainment, Nintendo, Sony Computer Entertainment, Square Co., Midway Games, Apogee Software, Atari Corporation, Meow Media, and Sega. Violent video games mentioned by name included \"Doom\", \"Quake\", \"Redneck Rampage\", and \"Duke Nukem\". The suit was dismissed by Judge Babcock in March 2002 in a ruling suggesting that a decision against the game makers would have a chilling effect on free speech. Babcock noted that \"it is manifest that there is social utility in expressive and imaginative forms of entertainment, even if they contain violence.\"\n\nIn 2003, Washington State enacted a statute banning the sale or rental to minors of video games containing \"aggressive conflict in which the player kill, injures, or otherwise causes physical harm to a human form in the game who is depicted by dress or other recognizable symbols as a public law enforcement officer.\" In 2004, this statute was subsequently declared an unconstitutional violation of the first amendment right to free speech in the Federal District Court case of \"Video Software Dealers Ass'n v. Maleng\".\n\nIn 2005, Jack Thompson brought suit against Sony Computer Entertainment and \"Grand Theft Auto\" in representation of the victims of the Devin Moore shooting incident. On 7 November 2005, Thompson withdrew from \"Strickland v. Sony\", stating, \"It was my idea [to leave the case].\" He was quick to mention that the case would probably do well with or without his presence. This decision followed scrutiny from Judge James Moore, however Thompson claimed he received no pressure to withdraw. At the same time, Judge James Moore had taken the motion to revoke Thompson's license under advisement. Jack Thompson appeared in court to defend his \"pro hac vice\" right to practice law in Alabama, following accusations that he violated legal ethics. Shortly thereafter, the case was dismissed and Thompson's license was revoked following a denial of his \"pro hac vice\" standing by Judge Moore who noted that \"Mr. Thompson's actions before this Court suggest that he is unable to conduct himself in a manner befitting practice in this state.\" In March 2006, the Alabama Supreme Court upheld Judge Moore's ruling against the dismissal of the case.\n\nIn 2005, California State Senator, Leland Yee introduced California Assembly Bills 1792 & 1793 which barred ultra-violent video games and mandated the application of ESRB ratings for video games. Yee, a former child psychologist has publicly criticized such games as \"\" and \"Manhunt 2\", and opposes the U.S. Army's \"Global Gaming League\". Both of these bills were passed by the assembly and signed by Gov. Arnold Schwarzenegger in October 2005. By December 2005, both bills had been struck down in court by Judge Ronald Whyte as unconstitutional, thereby preventing either from going into effect on 1 January 2006. Similar bills were subsequently filed in such states as Michigan and Illinois, but to date all have been ruled to be unconstitutional.\n\nIn 2005, in reaction to such controversial games as \"\", Senator Hillary Clinton along with Senators Joe Lieberman and Evan Bayh, introduced the Family Entertainment Protection Act (S.2126), intended to protect children from inappropriate content found in video games by imposing a federal mandate for inclusion of ESRB ratings. All three senators have actively sought restrictions on video game content with Sen. Lieberman denouncing the violence contained in video games and attempting to regulate sales of violent video games to minors, arguing that games should have to be labeled based upon age-appropriateness. Regarding \"Grand Theft Auto\", Lieberman has stated, \"The player is rewarded for attacking a woman, pushing her to the ground, kicking her repeatedly and then ultimately killing her, shooting her over and over again. I call on the entertainment companies—they've got a right to do that, but they have a responsibility not to do it if we want to raise the next generation of our sons to treat women with respect.\"\n\nIn June 2006, the Louisiana case of \"Entertainment Software Association v. Foti\" struck down a state statute that sought to bar minors from purchasing video games with violent content. The statute was declared an unconstitutional violation of the 1st Amendment. Amici filing briefs included Jack Thompson.\n\nOn 27 September 2006, Senator Sam Brownback (R-KS) introduced the United States Truth in Video Game Rating Act (S.3935). The act would require the ESRB to have access to the full content of and hands-on time with the games it was to rate, rather than simply relying on the video demonstrations submitted by developers and publishers. Two days later, Congressman Fred Upton introduced the Video Game Decency Act (H.R.6120) to the House.\n\nVideo game rating boards exist in a number of countries, typically placing restrictions (suggested or under force of law) for content that is violent or sexual in nature. About 5% of games fall into a category rated \"mature\" and recommended to those 17 years old and older. Those games account for about a quarter of all video game sales. Gamers seeking violence find themselves increasingly age restricted as identified violence level increases. This means that non-violent games, which are the least restricted, are available to all players at any age. This moral or legislative public policy against violence has the indirect effect of encouraging players of all ages and especially younger players to play non-violent games, however it also produces something of a forbidden fruit effect. For this and other reasons, the effectiveness of rating systems such as the ESRB to actually curb violent gameplay in youth gaming has been characterized as futile.\n\nTable of violence ratings\n\nA number of studies have been conducted specifically analyzing the differences between male and female preference in video game styles. Studies have vacillated between findings that the gender effect on violence preference in games is significant and insignificant, however no firm conclusions have been achieved to date. The number of studies in this field has blossomed contemporaneously with greater gender studies, and a degree of tension exists in the field between the traditional stereotype of violence as a male-dominant characteristic and the realities of the marketing data for violent games.\n\nIn 2008, an example of such studies was funded by the Office of Juvenile Justice and Delinquency Prevention, U.S. Department of Justice to the Center for Mental Health and Media. These studies were released in the book, \"Grand Theft Childhood\", wherein it was found that among girls, nine of the \"top ten [most popular video games] were nonviolent games such as \"Mario\" titles, \"Dance Dance Revolution\" or simulation games\" compared to a majority of violent games in the top ten favorites of boys. Ultimately, the conclusion reached in \"Grand Theft Childhood\" was that \"focusing on such easy but minor targets as violent video games causes parents, social activists and public-policy makers to ignore the much more powerful and significant causes of youth violence that have already been well established, including a range of [non-gender-linked] social, behavioral, economic, biological and mental-health factors.\" This conclusion supports Surgeon General Satcher's 2001 study (\"supra\").\n\nDespite this conclusion, general awareness of the issue together with traditional stereotyping has led a number of game developers and designers to create non-violent video games specifically for female audiences. Advertisement placement and other marketing techniques have in the past targeted women as more receptive to non-violent video game genres such as life simulation games, strategy games, or puzzle video games. Although these genres often contain certain degrees of violence, they lack the emphasis on graphic violence characterized for instance by the first-person shooter genre.\n\nCriticism for the violent aspects of video game culture has come from a number of anti-violence groups, and perhaps the most vocal of these are the numerous religious opposition groups. The moral codes of nearly all major religions contain prohibitions against murder and violence in general. In some cases this prohibition even extends to aggression, wrath, and anger. Violent video games, while merely vicarious in nature, have been the focus of religious disapproval or outrage in various circles. Notable anti-violent-video-game crusader, former attorney Jack Thompson is a self proclaimed Christian conservative, and his legal actions against violent video games have been intimately linked to his religious views. As groups like the fundamentalist Christian population have increased in number of adherents, new marketing opportunities have developed contemporaneously. Several religion-centric games forums such as GameSpot's \"Religion and Philosophy\" forum have developed within the greater gaming community in reaction to this growing niche.\n\nThere has been a rapid increase in Christian video games in the last decade, however as Christian games have striven to compete with their more popular secular progenitors, there has been an increasing number of games released that blur the lines between Christian and non-Christian values. Jack Thompson, for instance, has publicly decried such Christian games as \"\", stating \"It's absurd, ... you can be the Christians blowing away the infidels, and if that doesn't hit your hot button, you can be the Antichrist blowing away all the Christians.\" (The game reviewers IGN, Ars Technica and GameSpy have disagreed that \"Left Behind: Eternal Forces\" is overtly violent.) Similarly, James Dobson, PhD., founder of the Focus on the Family group, has advised parents in relation to video games to \"avoid the violent ones altogether.\nAlthough Christian games have been around since Sparrow Records' \"Music Machine\" for the Atari 2600, there have been few genres as unassailably violent as that of the first-person shooter (FPS). The majority of games that have been banned for violence have been FPS games, and for this reason, Christian games in the FPS genre have struggled to overcome the blurring effects of the violence inherent to the genre. Games such as \"Revelation 7\" and \"Xibalba\", for instance, have attempted to avoid claims of violence by using \"off the wall\" absurdist humor with enemies such as flying, bat-winged clown heads (modeled after the biblical Jezebel) that shoot rays out of their nose, or alien Nazis (a mocking reference to Raëlian religious beliefs) Other Christian FPS games such as \"Eternal War\" have avoided the issue by expressing the view that justified violence is morally acceptable.\n\nSome of these games, despite containing objectively violent content, have been affirmatively labeled \"non-violent video games\" by marketers and faith-based non-violent gaming communities. In direct response to the Columbine High School massacre (alleged to have been caused by the shooters' obsession with the game, \"Doom\"), Rev. Ralph Bagley began production on \"Catechumen\", a Christian first-person game produced by N'Lightening Software involving holy swords instead of guns. In \"Catechumen\", the player fights inhuman demons using holy armament. When \"sent back\" the demons produce no blood or gore, and for this reason it has been described as a non-violent game. The intent of \"Catechumen\", according to Rev. Bagley, is \"to build the genre of Christian gaming. People are tired of having these violent, demonic games dictating to their kids.\" Among Christian FPS games, a lack of gore has often been used as the minimum standard for non-violence. Christian game reviewers have at times characterized non-Christian games such as \"Portal\" and \"Narbacular Drop\" as comparatively non-violent games despite their lack of a Christian focus.\n\nAn example of a notable Christian video game organization is the Christian Game Developers Foundation, focusing on family-friendly gameplay and Biblical principles. Another well known Christian video game creator and distributor is Wisdom Tree which is best known for its unlicensed Christian video games on the Nintendo Entertainment System.\n\nIn 2006, Escapist magazine reported that a Hindu first-person shooter entitled \"My Hindu Shooter\" was in the works. In \"My Hindu Shooter\", a game based on the Unreal Engine, the player employs the Vedic abilities of astrology, Ayurvedic healing, breathing (meditation), herbalism, Gandharva Veda music, architecture (which let you purify demonic areas) and yagyas (rituals). Gameplay involved acquisition of the siddhis of clairvoyance, levitation, invisibility, shrinking and strength, and the ultimate goal of the game was to achieve pure consciousness by removing karma through completion of quests and cleansing the six chakras in ascending order. The only way to actually win the game is to complete it without harming or killing any other living creature. Despite the violence-free requirements of the main character, however, a player could die and be reincarnated in a number of different forms like a human, a pig, a dog, or a worm. Whatever form you came back as would limit the way in which you could interact with other characters in the game. Like the majority of games that have been labeled non-violent, violence in the game that is applied to the character rather than that the character applies is not considered to make the game a violent game.\n\nAccording to Buddhist morality, the first of the Five Precepts of Śīla is a personal rule of not killing. This moral guideline extends to human as well as non-human life. \nThere are five conditions to violate the first precept:\n\nAs the first precept requires an actual living being to be killed to be considered as violated, Buddhists can still enjoy video games with violence because there is no real being that is dying or being hurt. The fuller extent of the first precept is to maintain a harmless attitude towards all. The main problem is the mind, which is the main focus of Buddhism. Violent video games tend to create ill-will and tension, thus it is not conducive for meditation practice. Other than that, the action of the mind also creates \"kamma\" (action) which will bear its fruit when the conditions are right.\n\nAlthough primarily browser games, a number of stand-alone video games eschewing violence, such as the 2007 Thai game \"Ethics Game\" have been created that promote the Five Precepts and Buddhism generally. The Buddhist concept of dharma has been emphasized in a number of Buddhist games as a reaction to perceptions of the adharmic state of modern games. The concept of zen has also influenced a number of nonviolent video games such as \"Zen of Sudoku\", and The Game Factory's \"Zenses\" series.\n\nWhile the earliest games to feature a Jewish main character (the \"Wolfenstein\" series' William \"B.J.\" Blazkowicz) are characterized by militant anti-Nazism, a number of non-violent Jewish games (such as the \"Avner\" series by Torah Educational Software (TES)) primarily aimed at younger audiences has emerged with the intention of promoting Jewish religious concepts related to the Torah. One notable non-violent game that explores Jewish themes is \"The Shivah\", a puzzle-adventure game featuring a non-violent battle between Rabbis that takes the form of an insult swordfight.\n\nDespite notoriety in the Western press for controversial violent games such as \"Under Ash\", and \"Under Siege\", Muslim developer Afkar Media has also produced at least one non-violent game entitled \"Road Block Buster\". In \"Road Block Buster\" the hero must \"jump[] around [] doing tricks to soldiers ... [attempt] to get over any barrier or road block implanted by Israeli Defense forces without using violence, [and] earn respect by helping surrounded people whom can't get through the separation walls.\"\n\nExceptionally rare, the few Sikh games (e.g. \"Sikh Game\") in existence are primarily browser-based. \"Sarbloh Warriors\", planned to contain mild violence, and characterized by the BBC as Anti-Muslim, entered production as a major Sikh game, but seems to have left active production around 2007.\n\nIn January 2007, artist Chris Nelson produced a non-violent art game called \"The Seven Valleys\" which he exhibited during the ACSW conference at the University of Ballarat. Based on the \"Unreal Tournament\" engine, \"The Seven Valleys\" was designed with the intention of \"illustrat[ing] the unillustratable, and 'subvert[ing]' the image of violent video games in the process.\" Nelson was interviewed by ABC Radio on the subject in February of the same month.\n\nVideo game publisher, Destineer's non-violent puzzle video game, \"WordJong\" for the Nintendo DS has been considered a Taoist puzzle game.\n\nNon-profit organization Heartseed set out to produce several non-violent games, drawing inspiration from a claimed agreement between several religions under the sign of nonviolence: \"Christianity, Hinduism, Islam, Buddhism, Taoism, Judaism and others, – because throughout all of them can be found a common thread of decency meant to propel us toward spiritual enlightenment\". This project seems to have been discontinued.\n\nNon-violent video games as a genre are characterized as a genre by purpose. Unlike genres described by style of gameplay, non-violent video games span a wide number of gameplay genres. Defined in the negative, the purpose of non-violent games is to provide the player with an experience that \"lacks\" violence. Many traditional gameplay genres naturally lack violence and application of the term \"non-violent video games\" to titles that fall under these categories raises no questions regarding accuracy. For game developers and designers who self-identify as non-violent video game makers, however, the challenge has been to expand the concepts of non-violence into such traditionally violent gameplay genres as action games, role-playing games, strategy games, and the first-person shooter.\n\nEmergent and more recent gameplay genres such as music video games are for the most part naturally non-violent. Purposive video game genres such as educational games also are primarily non-violent in nature. Other electronic game genres like audio games are also most frequently non-violent. The indisputably non-violent nature of these games are often considered self-evident by members of both the non-violent gaming community and the gaming community at large. As such they are often not explicitly identified as such. Typically, explicit identification is applied counterintuitively to titles where there might otherwise be a question concerning non-violence. This has led such categorization to be viewed with mistrust, hostility, and mockery by those who fail to recognize the comparative nature of the definition or who disagree fundamentally with the underlying purpose of the genre.\n\nAmong traditionally non-violent games are included maze games, adventure games, life simulation games, construction and management simulation games, and some vehicle simulation games among others. These games are generally less frequently described as non-violent due to the self-evident nature of the descriptive term. Prior to the development of games specifically designed for non-violence, non-violent gamers were limited to these traditionally non-violent genres, however a number of games even under the traditionally non-violent umbrella may be considered arguably violent. \"Minesweeper\", for instance, is an abstraction of a scenario that often leads to a patently violent result.\n\nOther common traditionally non-violent genres include adventure games, puzzle games, music video games, programming games, party games, and traditional games.\n\nExamples of traditionally non-violent games include:\n\nAction games have typically been among the most violent of video games genres with the liberal employment of enemies to thwart the actions of the player-character, and an emphasis on killing these enemies to neutralize them. As action games have developed they have become progressively more violent over the years as advances in graphic capabilities allowed for more realistic enemies and death sequences. Nevertheless, some companies like Nintendo have tended to shy away from this kind of realism in favor of cartoon and fantasy violence, a concept also implicated in the increase of youth violence by media violence researchers. This has created a spectrum of violence in action games. Non-violent video game proponents have labeled a number of games containing comparatively low-level violence as non-violent as well as games such as the anti-violent serious game, \"Food Force\".\n\nStudies have also shown that there are tangible benefits to violence in action games such as increased ability to process visual information quickly and accurately. This has led to support for the development of action games that are non-violent which will allow players to retain the positive visual processing benefits without the negatives associated with violence.\n\nExamples of non-violent action games include:\n\nThe application of the term \"non-violent\" to genres such as the first-person shooter (FPS), that many players consider \"inherently\" or \"definitionally\" violent, has at times generated vociferous arguments that the concept is inconceivable and at best oxymoronic. This argument typically derives from a strict definition of violence as \"extreme, destructive, or uncontrollable force especially of natural events; intensity of feeling or expression, \" a definition by which the vast majority of video games may be described as violent. Despite arguments to the contrary, however, such characterizations have been employed as a marketing tool by makers and distributors of non-violent video games, and the degree of popularity enjoyed by games so described may be attributed to the comparative violence of other more violent members of the supergenre.\n\nNon-violent first-person shooter developers have expressed the notion that it is the challenge of making a non-violent game in a violent genre that motivates them in part. Rarely, FPS games such as \"Garry's Mod\" and \"Portal\" that have been developed without the specific intent of non-violence have been identified by reviewers and the non-violent gaming community as non-violent FPSes. Such characterizations have led to the concern that parents may allow their children to play these games, not realizing that there are still some elements of violence including violent deaths. Further ambiguities arise when determining whether a game is a first-person \"shooter\", as in certain games such as \"Narbacular Drop\", the player doesn't shoot anything at all, but merely clicks walls with the cursor. Similarly, in games such as \"realMyst\", the player merely interacts with the environment by touching things in the first-person and the term \"shooter\" is seen to be objectively inaccurate. Despite this, non-violent first-person games have often been characterized oxymoronically as shooters because of all comparable genres, these games are most closely similar to the FPS and often employ engines designed for the FPS genre.\n\nOften modeled upon violent FPS games, non-violent FPSes such as \"Chex Quest\" or the newer Sherlock Holmes games, may bear striking resemblance to the violent game whose engine they are using. Developers of such games often have done little to change the game other than replacing violent or scary imagery and recasting the storyline to describe \"zorching\", \"slobbing\", or otherwise non-lethally incapacitating enemies. Examples of simple changes intended to reduce violence for non-violent FPSes include the alteration of the red shroud from the death-sequence in \"Doom\" to become the green shroud from the slime-sequence in \"Chex Quest\" or the removal of the red shroud from the death-sequence in \"Half-Life 2\" for the deaths in \"Portal\". At times, similarities between violent progenitors and their non-violent descendants have proved strong enough that the non-violent developers have cast their game as a spoof of the violent version. This is apparent in \"Chex Quest\". Off-beat and absurdist humor have been employed in a number of games order to tone down the serious content by makers of non-violent FPSes.\n\nOne subgenre of the FPS that typically is not characterized as non-violent despite the fact that gameplay revolves to a great degree around avoidance of battle, is that of stealth games. Although much of the gameplay characteristic to stealth games accords closely with the requirements of the non-violent genre, stealth games most frequently simply delay chaotic violence to focus instead upon controlled precision violence. When stealth and violence are both present as options, stealth is often presented as the superior option for being morally superior or requiring greater skill.\n\nExamples of non-violent FPSes include:\n\nThough infrequently regarded as explicitly violent, role-playing video games (RPGs) have traditionally focused on the adventures of a party of travelers as they spend days and months leveling-up to fight greater and greater foes. Fighting in these games is highly stylized and often turn-based, however the actions of the player-characters and the enemies that attack them are distinctly violent. There have been some attempts made to reduce this violence by rendering it in cartoonish format as in some members of the \"Final Fantasy\" series or by recasting the enemies' deaths as \"fainting,\" \"sleeping,\" or becoming \"stunned\" as in the \"Pokémon\" series, however neither of these series has been explicitly labeled non-violent.\n\nOne rare example of an RPG that was designed as a non-violent video game is \"Spiritual Warfare\", a game with enemies featuring a non-violent main-character wherein the player wanders about converting the denizens of his town to Christianity while fending off the attacks of wild animals with holy food. Another example is \"A Tale in the Desert\", an MMORPG based on economic development.\n\nSecular examples include Capcom's \"Ace Attorney\" series and Victor's \"Story of Seasons\" series – a simulation/RPG game. Additionally, such RPGs as \"\", \"\", and \"Deus Ex\" have all been identified as containing certain modes of play that are mostly non-violent. This concept is also explored in the higher difficulty levels of the \"Thief\" series.\n\nIn 2015 the fan-funded indie RPG \"Undertale\" allowed players to choose whether or not to use violent methods, in a story that includes both explicit and subtextual commentary on the nature of violence in videogames.\n\nAs with role-playing games, strategy games have traditionally focused on the development and expansion of a group as they defend themselves from the attacks of enemies. Strategy games tend not to be explicitly described as violent, however they nearly universally contain violent content in the form of battles, wars, and other skirmishes. Additionally, strategy games tend to more often strive for realistic scenarios and depictions of the battles that result. Counteracting this violence, however, is the fact that strategy games tend to be set at a distant third party perspective and as such the violence of the battles tends to be minute and highly stylized.\n\nFor the most part, non-violent groups have not explored this genre of violent game. Gender-marketers have designed strategy games for both male and female audiences, however gender-linked treatment of violence has not occurred in this genre and as such, male- and female-oriented strategy games tend to contain equal degrees of violence. Christian developers have made various attempts at the genre including \"\" in which the player attempts to convert as many civilians in an apocalyptic future as possible by raising their spirit level and shielding them from the corrupting influences of rock-and-roll music and general secularism. The game has been criticized by such anti-violent video game personalities as former attorney Jack Thompson for strategy involving the slaying of infidels and non-believers, and for the ability of players engaged in multi-play modes to play as the Antichrist on the side of the Forces of Satan.\n\nSecular non-violent video game designers have emerged from the serious games movement and include such anti-violence titles as \"PeaceMaker\", a game where the player tries to foster peace between Israelis and Palestinians, and \"A Force More Powerful\", a nonviolence-themed game designed by Steve York, a documentary filmmaker and director of Bringing Down a Dictator, a non violent resistance film featuring Ivan Marovic, a resistance leader against Slobodan Milošević who was instrumental in bringing him down in 2000. Though these games are anti-violent, however, failure on the part of the player leads to violence. Thus the goal of the game is merely shifted to active violence-prevention while the degree of violence used to challenge the player remains consistent with violent strategy games.\n\nSome examples of relatively nonviolent strategy games include:\n\nAccording to the Funk and Buchman method for classifying video games, there are six categories into which games may be divided:\n\nThe separation of violent and nonviolent sports here illustrates a phenomenon also recognizable in the vehicle simulation game genre. With both sport and vehicle simulation games, gameplay has traditionally been highly bipolar with nearly as many violent titles as non-violent. Just as there are non-violent and \"violent\" sports games, so too are there flight and \"combat\" flight simulators, space flight and space \"combat\" simulators, and racing games and vehicular \"combat\" games. Distinction of games as violent or non-violent here serves a purely practical purpose as gameplay may differ considerably beyond the merely aesthetic.\n\nSuch sports as Ping-pong, golf, and billiards, have in the past been identified as nonviolent. A common example of a nonviolent game that has been employed in a number of studies and government committee reports is \"\".\n\nAn example of a notable non-violent sports game company is Kush Games.\n\n"}
{"id": "68282", "url": "https://en.wikipedia.org/wiki?curid=68282", "title": "Peasant", "text": "Peasant\n\nA peasant is a pre-industrial agricultural laborer or farmer, especially one living in the Middle Ages under feudalism and paying rent, tax, fees, or services to a landlord. In Europe, peasants were divided into three classes according to their personal status: slave, serf, and free tenant. Peasants either hold title to land in fee simple, or hold land by any of several forms of land tenure, among them socage, quit-rent, leasehold, and copyhold.\n\nThe word \"peasant\" is—and long has been—often used pejoratively to refer to poor or landless farmers and agricultural workers, especially in the poorer countries of the world in which the agricultural labor force makes up a large percentage of the population. The implication of the term is that the \"peasant\" is uneducated, ignorant, and unfamiliar with the more sophisticated mannerisms of the urban population.\n\nThe word peasantry is also commonly used in a non-pejorative sense as a collective noun for the rural population in the poor and under-developed countries of the world.\n\nThe word \"peasant\" is derived from the 15th century French word \"païsant\" (compare Italian \"paesano\"), meaning one from the \"pays\", or countryside; ultimately from the Latin \"pagus\", or outlying administrative district.\n\nPeasants typically made up the majority of the agricultural labour force in a pre-industrial society. The majority of the people in the Middle Ages were peasants.\n\nThough \"peasant\" is a word of loose application, once a market economy had taken root, the term \"peasant proprietors\" was frequently used to describe the traditional rural population in countries where smallholders farmed much of the land. More generally, the word \"peasant\" is sometimes used to refer pejoratively to those considered to be \"lower class\", perhaps defined by poorer education and/or a lower income.\n\nThe open field system of agriculture dominated most of northern Europe during medieval times and endured until the nineteenth century in many areas. Under this system, peasants lived on a manor presided over by a lord or a bishop of the church. Peasants paid rent or labor services to the lord in exchange for their right to cultivate the land. Fallowed land, pastures, forests, and wasteland were held in common. The open field system required cooperation among the peasants of the manor. It was gradually replaced by individual ownership and management of land.\n\nThe relative position of peasants in Western Europe improved greatly after the Black Death had reduced the population of medieval Europe in the mid-14th century: resulting in more land for the survivors and making labor more scarce. In the wake of this disruption to the established order, later centuries saw the invention of the printing press, the development of widespread literacy and the enormous social and intellectual changes of the Enlightenment.\n\nThe evolution of ideas in an environment of relatively widespread literacy laid the groundwork for the Industrial Revolution, which enabled mechanically and chemically augmented agricultural production while simultaneously increasing the demand for factory workers in cities, who became what Karl Marx called the proletariat. The trend toward individual ownership of land, typified in England by Enclosure, displaced many peasants from the land and compelled them, often unwillingly, to become urban factory-workers, who came to occupy the socio-economic stratum formerly the preserve of the medieval peasants.\n\nThis process happened in an especially pronounced and truncated way in Eastern Europe. Lacking any catalysts for change in the 14th century, Eastern European peasants largely continued upon the original medieval path until the 18th and 19th centuries. Serfdom was abolished in Russia in 1861, and while many peasants would remain in areas where their family had farmed for generations, the changes did allow for the buying and selling of lands traditionally held by peasants, and for landless ex-peasants to move to the cities. Even before emancipation in 1861, serfdom was on the wane in Russia. The proportion of serfs within the empire had gradually decreased \"from 45-50 percent at the end of the eighteenth century, to 37.7 percent in 1858.\"\n\nIn Germany, peasants continued to center their lives in the village well into the 19th century. They belonged to a corporate body and helped to manage the community resources and to monitor community life. In the East they had the status of serfs bound permanently to parcels of land. A peasant is called a \"Bauer\" in German and \"Bur\" in Low German (pronounced in English like \"boor\").\n\nIn most of Germany, farming was handled by tenant farmers who paid rents and obligatory services to the landlord—typically a nobleman. Peasant leaders supervised the fields and ditches and grazing rights, maintained public order and morals, and supported a village court which handled minor offenses. Inside the family the patriarch made all the decisions, and tried to arrange advantageous marriages for his children. Much of the villages' communal life centered on church services and holy days. In Prussia, the peasants drew lots to choose conscripts required by the army. The noblemen handled external relationships and politics for the villages under their control, and were not typically involved in daily activities or decisions.\n\nIn his seminal book \"Peasants into Frenchmen: the Modernization of Rural France, 1880–1914\" (1976), historian Eugen Weber traced the modernization of French villages and argued that rural France went from backward and isolated to modern and possessing a sense of French nationhood during the late 19th and early 20th centuries. He emphasized the roles of railroads, republican schools, and universal military conscription. He based his findings on school records, migration patterns, military-service documents and economic trends. Weber argued that until 1900 or so a sense of French nationhood was weak in the provinces. Weber then looked at how the policies of the Third Republic created a sense of French nationality in rural areas. The book was widely praised, but some argued that a sense of Frenchness existed in the provinces before 1870.\n\nFarmers in China have been sometimes referred to as \"peasants\" in English-language sources. However, the traditional term for farmer, \"nongfu\" (农夫), simply refers to \"farmer\" or \"agricultural worker\". In the 19th century, Japanese intellectuals reinvented the Chinese terms \"fengjian\" (封建) for \"feudalism\" and \"nongmin\" (农民), or \"farming people,\" terms used in the description of feudal Japanese society. These terms created a negative image of Chinese farmers by making a class distinction where one had not previously existed. Anthropologist Myron Cohen considers these terms to be neologisms that represented a cultural and political invention. He writes:\n\nModern Western writers often continue to use the term \"peasant\" for Chinese farmers, typically without ever defining what the term means. This Western use of the term suggests that China is stagnant, \"medieval\", underdeveloped, and held back by its rural population. Cohen writes that the \"imposition of the historically burdened Western contrasts of town and country, shopkeeper and peasant, or merchant and landlord, serves only to distort the realities of the Chinese economic tradition\".\n\nA bur is presented by the Rambam (Maimonides) as a person having neither (ethical) torah education nor virtues of manners nor the ability to acquire them. Maimonides gives five definitions of Hebrew terms found in Jewish scripture, that discuss foolishness and wisdom, they are, in ascending order: bur, am ha'aretz, golem, chacham, and chasid. The definition of the Hebrew term \"bur\" is extracted by Maimonides from the phrase \"sedeh bur\", which translates as an \"uncultivated field\". The Talmud and Mishnah (Pirke Avot II:4) also have this term.\n\nCommonly, \"bur\" would be translated into English as \"boor\".\n\nSince the literate classes have left the most records, and these tended to dismiss peasants as figures of coarse appetite and rustic comedy, the term \"peasant\" may have a pejorative rather than descriptive connotation in historical memory. Society was theorized as being organized into three \"estates\": those who work, those who pray, and those who fight. The Annales School of French historians emphasized the importance of peasants. Its leader Fernand Braudel devoted the first volume—called \"The Structures of Everyday Life\"—of his major work, \"Civilization and Capitalism 15th–18th Century\" to the largely silent and invisible world that existed below the market economy.\n\nOther research in the field of peasant studies was promoted by Florian Znaniecki and Fei Xiaotong, and in the post-1945 studies of the \"great tradition\" and the \"little tradition\" in the work of Robert Redfield. In the 1960s, anthropologists and historians began to rethink the role of peasant revolt in world history and in their own disciplines. Peasant revolution was seen as a Third World response to capitalism and imperialism.\n\nThe anthropologist Eric Wolf, for instance, drew on the work of earlier scholars in the Marxist tradition such as Daniel Thorner, who saw the rural population as a key element in the transition from feudalism to capitalism. Wolf and a group of scholars criticized both Marx and the field of modernization theorists for treating peasants as lacking the ability to take action. James C. Scott's field observations in Malaysia convinced him that villagers were active participants in their local politics even though they were forced to use indirect methods. Many of these activist scholars looked back to the peasant movement in India and to the theories of the revolution in China led by Mao Zedong starting in the 1920s. The anthropologist Myron Cohen, however, asked why the rural population in China were called \"peasants\" rather than \"farmers\", a distinction he called political rather than scientific. One important outlet for their scholarly work and theory was the \"Journal of Peasant Studies\".\n\n\n\n\n"}
{"id": "158741", "url": "https://en.wikipedia.org/wiki?curid=158741", "title": "Power-to-weight ratio", "text": "Power-to-weight ratio\n\nPower-to-weight ratio (or specific power or power-to-mass ratio) is a calculation commonly applied to engines and mobile power sources to enable the comparison of one unit or design to another. Power-to-weight ratio is a measurement of actual performance of any engine or power source. It is also used as a measurement of performance of a vehicle as a whole, with the engine's power output being divided by the weight (or mass) of the vehicle, to give a metric that is independent of the vehicle's size. Power-to-weight is often quoted by manufacturers at the peak value, but the actual value may vary in use and variations will affect performance.\n\nThe inverse of power-to-weight, weight-to-power ratio (power loading) is a calculation commonly applied to aircraft, cars, and vehicles in general, to enable the comparison of one vehicle's performance to another. Power-to-weight ratio is equal to thrust per unit mass multiplied by the velocity of any vehicle.\n\nThe power-to-weight ratio (Specific Power) formula for an engine (power plant) is the power generated by the engine divided by the mass. (\"Weight\" in this context is a colloquial term for \"mass\". To see this, note that what an engineer means by the \"power to weight ratio\" of an electric motor is not infinite in a zero gravity environment.)\n\nA typical turbocharged V8 diesel engine might have an engine power of and a mass of , giving it a power-to-weight ratio of 0.65 kW/kg (0.40 hp/lb).\n\nExamples of high power-to-weight ratios can often be found in turbines. This is because of their ability to operate at very high speeds. For example, the Space Shuttle's main engines used turbopumps (machines consisting of a pump driven by a turbine engine) to feed the propellants (liquid oxygen and liquid hydrogen) into the engine's combustion chamber. The original liquid hydrogen turbopump is similar in size to an automobile engine (weighing approximately ) and produces 72,000 hp (53.6 MW) for a power-to-weight ratio of 153 kW/kg (93 hp/lb).\n\nIn classical mechanics, instantaneous power is the limiting value of the average work done per unit time as the time interval Δ\"t\" approaches zero.\n\nThe typically used metrical unit of the power-to-weight ratio is formula_2 which equals formula_3. This fact allows one to express the power-to-weight ratio purely by SI base units.\n\nIf the work to be done is rectilinear motion of a body with constant mass formula_4, whose center of mass is to be accelerated along a straight line to a speed formula_5 and angle formula_6 with respect to the centre and radial of a gravitational field by an onboard powerplant, then the associated kinetic energy to be delivered to the body is equal to\n\nwhere:\n\nThe instantaneous mechanical pushing/pulling power delivered to the body from the powerplant is then\n\nwhere:\n\nIn propulsion, power is only delivered if the powerplant is in motion, and is transmitted to cause the body to be in motion. It is typically assumed here that mechanical transmission allows the powerplant to operate at peak output power. This assumption allows engine tuning to trade power band width and engine mass for transmission complexity and mass. Electric motors do not suffer from this tradeoff, instead trading their high torque for traction at low speed. The power advantage or power-to-weight ratio is then\n\nwhere:\n\nThe actual useful power of any traction engine can be calculated using a dynamometer to measure torque and rotational speed, with peak power sustained when the transmission and/or operator keeps the product of torque and rotational speed maximised. For jet engines there is e be usefully calculated there, for rockets there is typically no cruise speed, so it is less meaningful.\n\nPeak power of a traction engine occurs at a rotational speed higher than the speed when torque is maximised and at or below the maximum rated rotational speed – Max RPM. A rapidly falling torque curve would correspond with sharp torque and power curve peaks around their maxima at similar rotational speed, for example a small, lightweight engine with a large turbocharger. A slowly falling or near flat torque curve would correspond with a slowly rising power curve up to a maximum at a rotational speed close to Max RPM, for example a large, heavy multi-cylinder engine suitable for cargo/hauling. A falling torque curve could correspond with a near flat power curve across rotational speeds for smooth handling at different vehicle speeds, such as a traction electric motor.\n\nThermal energy is made up from molecular kinetic energy and latent phase energy. Heat engines are able to convert thermal energy in the form of a temperature gradient between a hot source and a cold sink into other desirable mechanical work. Heat pumps take mechanical work to regenerate thermal energy in a temperature gradient. Care should be made when interpreting propulsive power, especially for jet engines and rockets, deliverable from heat engines to a vehicle.\n\nAn electric motor uses electrical energy to provide mechanical work, usually through the interaction of a magnetic field and current-carrying conductors. By the interaction of mechanical work on an electrical conductor in a magnetic field, electrical energy can be generated.\n\nFluids (liquid and gas) can be used to transmit and/or store energy using pressure and other fluid properties. Hydraulic (liquid) and pneumatic (gas) engines convert fluid pressure into other desirable mechanical or electrical work. Fluid pumps convert mechanical or electrical work into movement or pressure changes of a fluid, or storage in a pressure vessel.\nA variety of effects can be harnessed to produce thermoelectricity, thermionic emission, pyroelectricity and piezoelectricity. Electrical resistance and ferromagnetism of materials can be harnessed to generate thermoacoustic energy from an electric current.\n\nAll electrochemical cell batteries deliver a changing voltage as their chemistry changes from \"charged\" to \"discharged\". A nominal output voltage and a cutoff voltage are typically specified for a battery by its manufacturer. The output voltage falls to the cutoff voltage when the battery becomes \"discharged\". The nominal output voltage is always less than the open-circuit voltage produced when the battery is \"charged\". The temperature of a battery can affect the power it can deliver, where lower temperatures reduce power. Total energy delivered from a single charge cycle is affected by both the battery temperature and the power it delivers. If the temperature lowers or the power demand increases, the total energy delivered at the point of \"discharge\" is also reduced.\n\nBattery discharge profiles are often described in terms of a factor of battery capacity. For example, a battery with a nominal capacity quoted in ampere-hours (Ah) at a C/10 rated discharge current (derived in amperes) may safely provide a higher discharge current – and therefore higher power-to-weight ratio – but only with a lower energy capacity. Power-to-weight ratio for batteries is therefore less meaningful without reference to corresponding energy-to-weight ratio and cell temperature. This relationship is known as Peukert's law.\n\nCapacitors store electric charge onto two electrodes separated by an electric field semi-insulating (dielectric) medium. Electrostatic capacitors feature planar electrodes onto which electric charge accumulates. Electrolytic capacitors use a liquid electrolyte as one of the electrodes and the electric double layer effect upon the surface of the dielectric-electrolyte boundary to increase the amount of charge stored per unit volume. Electric double-layer capacitors extend both electrodes with a nanopourous material such as activated carbon to significantly increase the surface area upon which electric charge can accumulate, reducing the dielectric medium to nanopores and a very thin high permittivity separator.\n\nWhile capacitors tend not to be as temperature sensitive as batteries, they are significantly capacity constrained and without the strength of chemical bonds suffer from self-discharge. Power-to-weight ratio of capacitors is usually higher than batteries because charge transport units within the cell are smaller (electrons rather than ions), however energy-to-weight ratio is conversely usually lower.\nFuel cells and flow cells, although perhaps using similar chemistry to batteries, have the distinction of not containing the energy storage medium or fuel. With a continuous flow of fuel and oxidant, available fuel cells and flow cells continue to convert the energy storage medium into electric energy and waste products. Fuel cells distinctly contain a fixed electrolyte whereas flow cells also require a continuous flow of electrolyte. Flow cells typically have the fuel dissolved in the electrolyte.\n\nPower-to-weight ratios for vehicles are usually calculated using curb weight (for cars) or wet weight (for motorcycles), that is, excluding weight of the driver and any cargo. This could be slightly misleading, especially with regard to motorcycles, where the driver might weigh 1/3 to 1/2 as much as the vehicle itself. In the sport of competitive cycling athlete's performance is increasingly being expressed in VAMs and thus as a power-to-weight ratio in W/kg. This can be measured through the use of a bicycle powermeter or calculated from measuring incline of a road climb and the rider's time to ascend it.\n\nMost vehicles are designed to meet passenger comfort and cargo carrying requirements. Different designs trade off power-to-weight ratio to increase comfort, cargo space, fuel economy, emissions control, energy security and endurance. Reduced drag and lower rolling resistance in a vehicle design can facilitate increased cargo space without increase in the (zero cargo) power-to-weight ratio. This increases the role flexibility of the vehicle. Energy security considerations can trade off power (typically decreased) and weight (typically increased), and therefore power-to-weight ratio, for fuel flexibility or drive-train hybridisation. Some utility and practical vehicle variants such as hot hatches and sports-utility vehicles reconfigure power (typically increased) and weight to provide the perception of sports car like performance or for other psychological benefit.\n\nA locomotive generally must be very heavy in order to develop enough adhesion on the rails to start a train. As the coefficient of friction between steel wheels and rails seldom exceeds 0.25 in most cases, improving a locomotive's power-to-weight ratio is often counterproductive. However, the choice of power transmission system, such as variable-frequency drive versus direct current drive, may support a higher power-to-weight ratio by better managing propulsion power.\n\nIncreased engine performance is a consideration, but also other features associated with luxury vehicles. Longitudinal engines are common. Bodies vary from hot hatches, sedans (saloons), coupés, convertibles and roadsters. Mid-range dual-sport and cruiser motorcycles tend to have similar power-to-weight ratios.\n\nPower-to-weight ratio is an important vehicle characteristic that affects the acceleration of sports vehicles.\n\nAircraft depend on high power-to-weight ratio to generate sufficient thrust to achieve sustained flight, and then to fly fast. \n\nJet aircraft produce thrust directly.\n\nPower to weight ratio is important in cycling, since it determines acceleration and the speed during hill climbs. Since a cyclist's power to weight output decreases with fatigue, it is normally discussed with relation to the length of time that he or she maintains that power. A professional cyclist can produce over 20 W/kg as a 5-second maximum. A 225-pound touring cyclist coasting down an exhilarating 10-degree mountain slope at 35 miles per hour rates 62 pounds per horsepower. The calculation is demonstrative and does not imply terminal speed. The power figure is 3.6 horsepower. In ISO units, this is 27 W/kg. In 60 seconds of such coasting, therefore, 39 kcal are lost, or 45 watt-hours. Climbing is the major impediment to progress on road when cycling.\n\n\n"}
{"id": "23799", "url": "https://en.wikipedia.org/wiki?curid=23799", "title": "Power set", "text": "Power set\n\nIn mathematics, the power set (or powerset) of any set is the set of all subsets of , including the empty set and itself, variously denoted as (), 𝒫(), ℘() (using the \"Weierstrass p\"), , , or, identifying the powerset of with the set of all functions from to a given set of two elements, . In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set.\n\nAny subset of () is called a \"family of sets\" over .\n\nIf is the set , then the subsets of are\n\nand hence the power set of is .\n\nIf is a finite set with elements, then the number of subsets of is . This fact, which is the motivation for the notation , may be demonstrated simply as follows,\n\nCantor's diagonal argument shows that the power set of a set (whether infinite or not) always has strictly higher cardinality than the set itself (informally the power set must be larger than the original set). In particular, Cantor's theorem shows that the power set of a countably infinite set is uncountably infinite. The power set of the set of natural numbers can be put in a one-to-one correspondence with the set of real numbers (see Cardinality of the continuum).\n\nThe power set of a set , together with the operations of union, intersection and complement can be viewed as the prototypical example of a Boolean algebra. In fact, one can show that any \"finite\" Boolean algebra is isomorphic to the Boolean algebra of the power set of a finite set. For \"infinite\" Boolean algebras this is no longer true, but every infinite Boolean algebra can be represented as a subalgebra of a power set Boolean algebra (see Stone's representation theorem).\n\nThe power set of a set forms an abelian group when considered with the operation of symmetric difference (with the empty set as the identity element and each set being its own inverse) and a commutative monoid when considered with the operation of intersection. It can hence be shown (by proving the distributive laws) that the power set considered together with both of these operations forms a Boolean ring.\n\nIn set theory, is the set of all functions from to . As \"2\" can be defined as (see natural number), (i.e., ) is the set of all functions from to {0,1}. By identifying a function in with the corresponding preimage of , we see that there is a bijection between and (), where each function is the characteristic function of the subset in () with which it is identified. Hence and () could be considered identical set-theoretically. (Thus there are two distinct notational motivations for denoting the power set by : the fact that this function-representation of subsets makes it a special case of the notation and the property, mentioned above, that .)\n\nThis notion can be applied to the example above in which to see the isomorphism with the binary numbers\nfrom 0 to with being the number of elements in the set.\nIn , a \"1\" in the position corresponding to the location in the enumerated set indicates the presence of the element. So .\n\nFor the whole power set of we get:\nSuch bijective mapping of \"S\" to integers is arbitrary, so this representation of subsets of \"S\" is not unique, but the sort order of the enumerated set does not change its cardinality.\n\nHowever, such finite binary representation is only possible if \"S\" can be enumerated (this is possible even if \"S\" has an infinite cardinality, such as the set of integers or rationals, but not for example if \"S\" is the set of real numbers, in which we cannot enumerate all irrational numbers to assign them a defined finite location in an ordered set containing all irrational numbers).\n\nThe power set is closely related to the binomial theorem. The number of subsets with elements in the power set of a set with elements is given by the number of combinations, , also called binomial coefficients.\n\nFor example, the power set of a set with three elements, has:\n\nUsing this relationship we can compute formula_3 using the formula:\n\nformula_4\n\nTherefore one can deduce the following identity, assuming formula_5:\n\nformula_6\n\nIf is a finite set, there is a recursive algorithm to calculate ().\n\nDefine the operation }.\n\nIn English, return the set with the element added to each set in .\n\n\nIn other words, the power set of the empty set is the set containing the empty set and the power set of any other set is all the subsets of the set containing some specific element and all the subsets of the set not containing that specific element.\n\nThe set of subsets of of cardinality less than or equal to is sometimes denoted by or , and the set of subsets with cardinality strictly less than is sometimes denoted or . Similarly, the set of non-empty subsets of might be denoted by or .\n\nA set can be regarded as an algebra having no nontrivial operations or defining equations. From this perspective the idea of the power set of as the set of subsets of generalizes naturally to the subalgebras of an algebraic structure or algebra.\n\nNow the power set of a set, when ordered by inclusion, is always a complete atomic Boolean algebra, and every complete atomic Boolean algebra arises as the lattice of all subsets of some set. The generalization to arbitrary algebras is that the set of subalgebras of an algebra, again ordered by inclusion, is always an algebraic lattice, and every algebraic lattice arises as the lattice of subalgebras of some algebra. So in that regard subalgebras behave analogously to subsets.\n\nHowever, there are two important properties of subsets that do not carry over to subalgebras in general. First, although the subsets of a set form a set (as well as a lattice), in some classes it may not be possible to organize the subalgebras of an algebra as itself an algebra in that class, although they can always be organized as a lattice. Secondly, whereas the subsets of a set are in bijection with the functions from that set to the set {0,1} = 2, there is no guarantee that a class of algebras contains an algebra that can play the role of 2 in this way.\n\nCertain classes of algebras enjoy both of these properties. The first property is more common, the case of having both is relatively rare. One class that does have both is that of multigraphs. Given two multigraphs and , a homomorphism consists of two functions, one mapping vertices to vertices and the other mapping edges to edges. The set of homomorphisms from to can then be organized as the graph whose vertices and edges are respectively the vertex and edge functions appearing in that set. Furthermore, the subgraphs of a multigraph are in bijection with the graph homomorphisms from to the multigraph definable as the complete directed graph on two vertices (hence four edges, namely two self-loops and two more edges forming a cycle) augmented with a fifth edge, namely a second self-loop at one of the vertices. We can therefore organize the subgraphs of as the multigraph , called the power object of .\n\nWhat is special about a multigraph as an algebra is that its operations are unary. A multigraph has two sorts of elements forming a set of vertices and of edges, and has two unary operations giving the source (start) and target (end) vertices of each edge. An algebra all of whose operations are unary is called a presheaf. Every class of presheaves contains a presheaf that plays the role for subalgebras that 2 plays for subsets. Such a class is a special case of the more general notion of elementary topos as a category that is closed (and moreover cartesian closed) and has an object , called a subobject classifier. Although the term \"power object\" is sometimes used synonymously with exponential object , in topos theory is required to be .\n\nIn category theory and the theory of elementary topoi, the universal quantifier can be understood as the right adjoint of a functor between power sets, the inverse image functor of a function between sets; likewise, the existential quantifier is the left adjoint.\n\n\n\n"}
{"id": "2384652", "url": "https://en.wikipedia.org/wiki?curid=2384652", "title": "Proofs involving covariant derivatives", "text": "Proofs involving covariant derivatives\n\nThis article contains proof of formulas in Riemannian geometry that involve the Christoffel symbols.\n\nStart with the Bianchi identity\n\nContract both sides of the above equation with a pair of metric tensors:\n\nThe first term on the left contracts to yield a Ricci scalar, while the third term contracts to yield a mixed Ricci tensor,\nThe last two terms are the same (changing dummy index \"n\" to \"m\") and can be combined into a single term which shall be moved to the right,\nwhich is the same as\nSwapping the index labels \"l\" and \"m\" yields\n\nThe last equation in the proof above can be expressed as\nwhere δ is the Kronecker delta. Since the mixed Kronecker delta is equivalent to the mixed metric tensor,\nand since the covariant derivative of the metric tensor is zero (so it can be moved in or out of the scope of any such derivative), then\nFactor out the covariant derivative\nthen raise the index \"m\" throughout\nThe expression in parentheses is the Einstein tensor, so \n\nthis means that the covariant divergence of the Einstein tensor vanishes.\n\nStarting with the local coordinate formula for a covariant symmetric tensor field formula_16, the Lie derivative along a vector field formula_17 is\nhere, the notation formula_19 means taking the partial derivative with respect to the coordinate formula_20.      \"Q.E.D.\"     (\"return to article\")\n\n\n"}
{"id": "509403", "url": "https://en.wikipedia.org/wiki?curid=509403", "title": "Punch (combat)", "text": "Punch (combat)\n\nA punch is a striking blow with the fist. It is used in most martial arts and combat sports, most notably Boxing, where it is the only type of offensive technique allowed. In sports, hand wraps or other padding such as gloves may be used to protect athletes and practitioners from injuring themselves.\n\nThe use of punches varies between different martial arts and combat sports. Styles such as Boxing, Suntukan or Russian fist fighting use punches alone, while others such as Kickboxing, Muay Thai, Lethwei or Karate may use both punches and kicks. Others such as wrestling and judo (punches and other striking techniques, atemi, are present in judo kata, but are forbidden in competitions) do not use punches at all. There are many types of punches and as a result, different styles encompass varying types of punching techniques.\n\nThis is not a comprehensive list of all punches, due to the large diversity of schools of practice whose techniques, employing arm, shoulder, hip and leg work, may invariably differ.\n\nIn boxing, punches are classified according to the motion and direction of the strike; contact is always made with the knuckles. There are four primary punches in boxing: the jab, cross, hook, and uppercut. \n\nPunching techniques in Karate are called \"tsuki\" or \"zuki\". Contact is made with the first two knuckles (\"seiken\"). If any other part of the hand is used to strike with, such as the back of the fist (\"uraken\") or the bottom of the fist (\"tetsui\"), then the blow is classified as a strike (uchi).\n\nKarate punches include the thrust punch \"oi-zuki\" made using the lead-hand, straight punch \"choku-zuki\", reverse punch \"gyaku-zuki\", made from the opposite (lead) hand, and many other variations.\n\n"}
{"id": "4556097", "url": "https://en.wikipedia.org/wiki?curid=4556097", "title": "Raewyn Connell", "text": "Raewyn Connell\n\nRaewyn Connell (born 3 January 1944) (also known as R.W. Connell, formerly Robert before her transition) is an Australian sociologist. She gained prominence as an intellectual of the Australian New Left. She is currently Professor Emerita at the University of Sydney and known for the concept of hegemonic masculinity and her book, \"Southern Theory\".\n\nConnell was born Robert William Connell on 3 January 1944 in Sydney, Australia. Her father, William Fraser (Bill) Connell (OBE), was a Professor of Education at the University of Sydney for many years, where he focused on educational research and teaching. Her mother, Margaret Lloyd Connell (nee Peck) was a high school science teacher. Connell has two sisters, Patricia Margaret Selkirk and Helen Connell.\n\nConnell was educated at Manly and North Sydney High Schools, and has degrees from the University of Melbourne and University of Sydney. She has held academic positions at universities in Australia, including being the founding professor of sociology at Macquarie University 1976–1991. \n\nIn the United States Connell was visiting professor of Australian studies at Harvard University 1991–1992, and professor of sociology at University of California Santa Cruz 1992–1995. She was a rank-and-file member of the Australian Labor Party (before the party shifted to the right in the early 1980s), and a trade unionist, currently in the National Tertiary Education Union.\n\nConnell's sociology emphasises the historical nature of social reality and the transformative character of social practice. Her writing tries to combine empirical detail, structural analysis, critique, and relevance to practice. Much of her empirical work uses biographical (life-history) interviewing, in education, family life and workplaces. She has written or co-written twenty-one books and more than 150 research papers. Her work is translated into 16 languages.\n\nConnell serves on the editorial board or advisory board of numerous academic journals, including \"Signs\", \"Sexualities\", \"The British Journal of Sociology\", \"Theory and Society\", and \"The International Journal of Inclusive Education\".\n\nConnell is a trans woman, who completed her gender transition late in life. Almost all her earlier work was published under the gender-neutral name \"R. W. Connell\", up to the second edition of \"Masculinities\" in 2005. A few publications are under the names Bob or Robert. Since 2006 all her work has appeared under the name Raewyn Connell. Connell has also written about transsexualism.\n\nConnell first became known for research on large-scale class dynamics (\"Ruling Class, Ruling Culture\", 1977 and \"Class Structure in Australian History\", 1980), and the ways class and gender hierarchies are re-made in the everyday life of schools (\"Making the Difference\", 1982).\n\nIn the late 1980s she developed a social theory of gender relations (\"Gender and Power\", 1987), which emphasised that gender is a large-scale social structure not just a matter of personal identity. In applied fields she has worked on poverty and education (\"Schools and Social Justice\", 1993), sexuality and AIDS prevention, and labour movement strategy (\"Socialism & Labor\", 1978).\n\nConnell is best known outside Australia for studies of the social construction of masculinity. She was one of the founders of this research field, and her book \"Masculinities\" (1995, 2005) is the most-cited in the field. The concept of hegemonic masculinity has been particularly influential and has attracted much debate. She has been an advisor to UNESCO and UNO initiatives relating men, boys and masculinities to gender equality and peacemaking.\n\nConnell has developed a sociology of intellectuals that emphasises the collective character of intellectual labour, and the importance of its social context. Her 2007 book \"Southern Theory\" extended this to the global dynamics of knowledge production, critiquing the \"Northern\" bias of mainstream social science which is predominately produced in \"metropolitan\" universities. In doing so, she argues, metropolitan social theory fails to adequately explain social phenomena in the Southern experience.\n\nShe analysed examples of theoretical work deriving from the global South: including the work of Paulin Hountondji, Ali Shariati, Veena Das, Ashis Nandy and Raúl Prebisch. Connell has also examined Southern theories of neoliberalism and gender.\n\n"}
{"id": "1468089", "url": "https://en.wikipedia.org/wiki?curid=1468089", "title": "Sentence diagram", "text": "Sentence diagram\n\nIn pedagogy and theoretical syntax, a sentence diagram or \"parse tree\" is a pictorial representation of the grammatical structure of a sentence. The term \"sentence diagram\" is used more in pedagogy, where sentences are \"diagrammed\". The term \"parse tree\" is used in linguistics (especially computational linguistics), where sentences are \"parsed\". Their purposes are to demonstrate the structure of sentences. The model is informative about the relations between words and the nature of syntactic structure and is thus used as a tool to help predict which sentences are and are not possible.\n\nMost methods of diagramming in pedagogy are based on the work of Alonzo Reed and Brainerd Kellogg in their book \"Higher Lessons in English,\" first published in 1877, though the method has been updated with recent understanding of grammar. Reed and Kellogg were preceded, and their work probably informed, by W. S. Clark, who published his \"balloon\" method of depicting grammar in his 1847 book \"A Practical Grammar: In Which Words, Phrases & Sentences are Classified According to Their Offices and Their Various Relationships to Each Another.\"\n\nSome schoolteachers continue to use the Reed–Kellogg system in teaching grammar, but others have discouraged it in favor of more modern tree diagrams. However, these modern tree structures draw on techniques that were already present in Reed–Kellogg diagrams. Reed and Kellogg defend their system in the preface to their grammar:\n\n\"The Objections to the Diagram.\"--The fact that the pictorial diagram groups the parts of a sentence according to their offices and relations, and not in the order of speech, has been spoken of as a fault. It is, on the contrary, a merit, for it teaches the pupil to look through the literary order and discover the logical order. He thus learns what the literary order really is, and sees that this may be varied indefinitely, so long as the logical relations are kept clear.\n\nThe assertion that correct diagrams can be made mechanically is not borne out by the facts. It is easier to avoid precision in oral analysis than in written. The diagram drives the pupil to a most searching examination of the sentence, brings him face to face with every difficulty, and compels a decision on every point.\n\nThese statements bear witness to the fact that Reed–Kellogg diagrams abstract away from actual word order in order to focus more intently on how words in sentences function and relate to each other.\n\nSimple sentences in the Reed–Kellogg system are diagrammed according to these forms:\nThe diagram of a simple sentence begins with a horizontal line called the \"base\". The subject is written on the left, the predicate on the right, separated by a vertical bar which extends through the base. The predicate must contain a verb, and the verb either requires other sentence elements to complete the predicate, permits them to do so, or precludes them from doing so. The verb and its object, when present, are separated by a line that ends at the baseline. If the object is a direct object, the line is vertical. If the object is a predicate noun or adjective, the line looks like a backslash, \\, sloping toward the subject.\n\nModifiers of the subject, predicate, or object are placed below the base line:\n\nModifiers, such as adjectives (including articles) and adverbs, are placed on slanted lines below the word they modify. Prepositional phrases are also placed beneath the word they modify; the preposition goes on a slanted line and the slanted line leads to a horizontal line on which the object of the preposition is placed.\n\nThese basic diagramming conventions are augmented for other types of sentence structures, e.g. for coordination and subordinate clauses.\n\nThe connections to modern principles for constructing parse trees are present in the Reed–Kellogg diagrams, although Reed and Kellogg understood such principles only implicitly. The principles are now regarded as the constituency relation of phrase structure grammars and the dependency relation of dependency grammars. These two relations are illustrated here adjacent to each other for comparison:\n\nConstituency is a one-to-one-or-more relation; every word in the sentence corresponds to one or more nodes in the tree diagram. Dependency, in contrast, is a one-to-one relation; every word in the sentence corresponds to exactly one node in the tree diagram. Both parse trees employ the convention where the category acronyms (e.g. N, NP, V, VP) are used as the labels on the nodes in the tree. The one-to-one-or-more constituency relation is capable of increasing the amount of sentence structure to the upper limits of what is possible. The result can be very \"tall\" trees, such as those associated with X-bar theory. Both constituency-based and dependency-based theories of grammar have established traditions.\n\nReed–Kellogg diagrams employ both of these modern tree generating relations. The constituency relation is present in the Reed–Kellogg diagrams insofar as subject, verb, object, and/or predicate are placed equi-level on the horizontal base line of the sentence and divided by a vertical or slanted line. In a Reed–Kellogg diagram, the vertical dividing line that crosses the base line corresponds to the binary division in the constituency-based tree (S → NP + VP), and the second vertical dividing line that does not cross the baseline (between verb and object) corresponds to the binary division of VP into verb and direct object (VP → V + NP). Thus the vertical and slanting lines that cross or rest on the baseline correspond to the constituency relation. The dependency relation, in contrast, is present insofar as modifiers dangle off of or appear below the words that they modify.\n\nOne can render Reed–Kellogg diagrams according to modern tree conventions. When one does so, the result is a hybrid dependency-constituency tree. The Reed–Kellogg diagrams above appear as the following trees:\n\nA mixing of labeling conventions (i.e. category label vs. actual word) helps draw attention to the presence of both constituency and dependency. The S and VP in these trees mark the constituency relation and the words themselves mark the dependency relation. A major difference between these hybrid trees and the Reed–Kellogg diagrams, however, is that the hybrid trees encode actual word order, whereas the Reed–Kellogg diagrams are abstracting away from actual word order in order to focus more on function.\n\nA sentence may also be broken down by functional parts: subject, object, adverbial, verb (predicator). The subject is the owner of an action, the verb represents the action, the object represents the recipient of the action, and the adverbial qualifies the action. The various parts can be phrases rather than individual words.\n\n\n\n"}
{"id": "14650242", "url": "https://en.wikipedia.org/wiki?curid=14650242", "title": "Serkan Özkaya", "text": "Serkan Özkaya\n\nSerkan Özkaya (1973) is a Turkish-American conceptual artist whose work deals with topics of appropriation and reproduction. He typically operates outside of traditional art spaces and often makes multiple versions of even his own work including his most noted work David (inspired by Michelangelo). Özkaya's artworks are held in the permanent collections of the İstanbul Modern and Borusan Contemporary Art in Istanbul, as well as three different 21c Museum Hotels locations—Louisville, Kentucky; Bentonville, Arkansas; and Nashville, Tennessee.\n\nÖzkaya grew up in Istanbul, Turkey. At a young age, he began learning about Western art through the study of reproductions of major artworks found in books. It is here his interests in the concepts originality, replication and emulation began. He attended Istanbul University, graduating with both a bachelor and a master's degree in arts. He continued his graduate studies at Bard College. After leaving Bard with a master's degree in fine art, he received an art fellowship at MacDowell Colony. Özkaya then returned to Istanbul University to study for his Ph.D. in German Language and Literature. In 2001, he published \"Genius and Creativity in the Arts\", a comparative study of the German works Moses und Aron (an opera by Arnold Schoenberg), Doctor Faustus (a novel by Thomas Mann) and (a book by Theodor W. Adorno).\n\n\"Dear Sir or Madam\" (1996–2009) is a collection of letters and other correspondences between Özkaya and cultural institutions, dignitaries and curators. It is his own paper trail of bureaucracy in inaction. In the collection, among others, are his letters to the Louvre asking that he be allowed to hang the Mona Lisa upside down and a letter to the German Bundestag that they permit him to re-wrap the Reichstag. Often, the letters do not deserve a response but curiously, they have usually been treated with official but unthinking courtesy, read perhaps but not comprehended—answered and then often passed along the bureaucratic chain. In 2009, \"Dear Sir or Madam\" was exhibited at the Slag Gallery in New York along with works from the same period. Among them was \"Proletarier Aller Laender\" (1998-2009) or \"Proletariat of all Countries\", as it is also known in English, a collection of red plastic foam figurines glued to the floor. In viewing this work, the audience is most likely forced to trample on the little figures as if to trample on the working classes. However, the proletariat, which the figurines represent, in their resilience and ultimate power always springing back, indestructible. Other works in the exhibition included the golden sculptures \"Levitation by Defecation\" (2008), \"Shit on a Stick\" (2008), and \"Goldenboy\" (2006), \"a fiberglass figure painted in gold acrylic and dangling a couple of feet off the ground by a noose\".\n\nIn 2000, Özkaya collected roughly 30,000 slides from artists, galleries, and institutions and showed them on one of the largest galleries in the main pedestrian street in Istanbul, at the Kazim Taskent Art Gallery. \"What A Museum Should Really Look Like (Large Glass)\" presented a giant mosaic of individual images. During the day the slides were readable from the inside of the gallery and at night, with the lights on, they became a scene for the street. This piece was later initiated in Utrecht, the Netherlands in a much larger scale with 100,000 slides. The collage of thousands of pieces of artwork was collected through an online advertisement. All of the works were arranged in the mosaic with no coherent order and no editorial or curatorial control from Özkaya.\n\nIn 2003, Özkaya turned the front and back covers of the Turkish language newspaper, \"Radikal\" into drawings. This act enabled the reader to acquire a limited edition artwork for the price of the newspaper. In early 2004, it was part of a group exhibition named \"The Poetics of Proximity\" at the Guggenheim Gallery at Chapman University. In similar fashion, in subsequent years, some pages of several other newspapers were also hand-rendered and appeared as a print of the drawings. The overall piece, entitled \"Today Could Be A Day Of Historical Importance\" was executed in Sweden with \"Aftonbladet\", in Germany with Freitag, and in the US with \"The New York Times\" and \"The Courier-Journal\". The last of which was created in collaboration with the non-profit artwithoutwalls. \"The New York Times\" version in the \"Weekend Arts\" section appeared \"mise en abyme\" with Özkaya's drawing appearing as an inset in the page and an even smaller drawing appearing as an inset of that, and so on, creating a Droste effect.\n\nIn 2005, Özkaya created \"David (inspired by Michelangelo)\", a double-sized golden replica of Michelangeloʼs \"David\" based on a 3D computer model by Marc Levoy of Stanford University. The sculpture was first intended to be part of the 9th International Istanbul Biennial. Unfortunately, the statue crashed and was destroyed while in the process of being installed at Şişli Square. Özkaya created two new versions of the replica at a studio in Eskişehir, one of which was to be placed in the local Sazova Science, Art and Culture Park. The other was acquired by 21c Museum Hotels in Louisville, Kentucky. On its way to Louisville, the piece made a stop in New York City. Although just a mere copy, the statue made quite a spectacle, as the artist intended, attracting the attention of tourists and residents as it was transported uncovered lying horizontally on a semi-trailer truck. The giant 30-foot statue is currently featured in front of the museum hotel on West Main Street in Louisville.\n\nIn \"A Sudden Gust of Wind\" (2007 –2013), Özkaya depicted the journey of a stack of papers blown away by the wind. The original work was made with basic materials: standard A4 paper, thread, and glue. The piece was inspired by \"Ejiri in Suruga Province (A Sudden Gust of Wind)\", a 19th-century Japanese woodcut by Hokusai, and \"A Sudden Gust of Wind (after Hokusai)\" (1993) by Jeff Wall. One version of the installation was first exhibited at the Boots Contemporary Art Space in St. Louis, Missouri. Another version of the work was acquired by 21c Museum Hotel and installed at their third location in Bentonville, Arkansas. This version utilizes about 400 metal sheets scattered in a large gallery space.\n\n\"Bring me the head of…\" (2007–2015) is the edible form of a teddy bear's head on a plate. Instead of working with museums or galleries, in this series, Özkaya collaborated with restaurants and chefs and they decided the ingredients—that is the material of the sculpture. This work was made in collaboration with M on the Bund in Shanghai, offered at Freemans' Restaurant in New York in 2007 as a part of the Performa Biennial, at Changa in Istanbul in 2008, as street food in Izmir in 2009, at Capital M in Beijing in 2012, and at the Hive in Bentonville, Arkansas in 2014.\n\nIn \"One and Three Pasta\" (2012), Özkaya collaborated with the architect George L. Legendre, a professor at Harvard University. They created a series of replicas using computer models of 92 types of pasta (based on Legendreʼs mathematical equations). The actual pieces of pasta were displayed next to their ideal replicas in nylon along with the generative equations for the shape. One and Three Pasta references to canonical works like Joseph Kosuthʼs One and Three Chairs and Donald Juddʼs Untitled stacks.\n\n\"An Attempt at Exhausting A Place…\" is a series of works in which the artist \"dissolves the white cube\" and the usual constraints of a traditional art space. Four projectors are used to display a live feed of the scene beyond the wall on which they are projected. The work was exhibited as part of Özkaya's solo exhibition at the Postmasters Gallery in New York, running from May 14 to June 18, 2016. In a similar installation at İstanbul Modern, a projection on one its interior walls treated the view to a scene of the Bosphorus Strait, making the wall seem transparent. A version of the work is also presented in a permanent exhibition at the lobby of the 21c Museum Hotel in Nashville, Tennessee.\n\n\"Atlas\" (2011) is a contribution to a walking museum wherein Özkaya constructed a rock to be strapped to the curator's back and promenaded daily throughout the streets of New York. The idea was to make \"the museum\" itself wander around the streets of the city with Özkaya's new piece, a giant rock.\n\n\"Radisson/Picasso\" (2012), a pair of Radisson Hotel matchboxes with the text of one of which has been changed to read \"Picasso\" instead of \"Radisson\" and was displayed alongside the original.\n\n\"Mirage\" (2013) was installed at the Postmasters Gallery in New York and consisted of a shadow of a passenger airplane that crossed the room for 45 seconds every four minutes. The shadow here was considered not as the mere absence of light but rather as a material in and of itself which could then be sculpted. The work was also exhibited as part of the Borusan Contemporary Art Collection's Overture: New Acquisitions (2015) at Perili Köşk in Istanbul.\n\n\"MyMoon\" (2015 – present) is another artwork by Özkaya, a large round rock floating in the sky, that is visible only through a smartphone via the MyMoon app. Using the phone's GPS and compass, MyMoon app detects the object in the sky and makes it visible. MyMoon rotates in close proximity to the earth's moon in the sky. Yet unlike the earth's satellite, it is visible outdoors and indoors.\n\nIn October 2017, Özkaya presented his latest major work \"En attendons\" (\"We Will Wait\") at the Postmasters Gallery. The work is a recreation of Marcel Duchamp's \"Étant donnés\" from which Özkaya's also gets its name in anagram form. Özkaya claims to have discovered a secret hidden within Duchamp's piece. He proposes that it is not only a peephole into a diorama but that, under proper lighting, it also functions as a camera obscura that projects the French artist's self-portrait on a surface opposite the peephole. According to Özkaya, there is more to \"Étant Donnés\" than previously thought; that the work, which took Duchamp more than 20 years to create, projects an image of Rrose Sélavy, \"Duchamp’s female alter ego.\"\n\n\n\n"}
{"id": "53395676", "url": "https://en.wikipedia.org/wiki?curid=53395676", "title": "Situational theory of problem solving", "text": "Situational theory of problem solving\n\nThe situational theory of problem solving attempts to explain why and how an individual communicates during a problematic situation. The Situational Theory of Problem Solving (STOPS) was proposed by Jeong-Nam Kim and James E. Grunig in 2011 though their article “problem solving and communicative action: A situational theory of problem solving.” The theory was developed from the situational theory of publics (STP) and claimed it is “an extended and generalized version” of STP. This theory has an assumption that “the more one commits to problem resolution, the more one becomes acquisitive of information pertaining to the problem, selective in dealing with information, and transmissive in giving it to others.”\n\nSTP has been heavily used in the field of public relations to understand why and how publics communicate. The original situational theory uses three independent variables (problem recognition, constraint recognition, and involvement recognition) to predict the dependent variable of information seeking and processing.\n\nSTOPS was proposed to overcome four limitations of STP:\nAlterations in existed variables (problem recognition, constraint recognition, involvement recognition, and reference criterion) were done to explain communicative action in problem solving variable. STOPS also expanded the focus of the theory from \"decisions\" to a more general concept of life \"problems.\" A new variable, situational motivation in problem solving, was added to mediate the effects of predictive variables of communicative behavior.\n\n\nThe extent to which an individual wants to know more about a problem. This concept mediates the effect of problem recognition, constraint recognition, and involvement recognition. Referent criterion would be independent of this variable because it is more cognitive than perceptual.\n\nWhen an individual tries to solve a problem, his or her communicative activeness increases in three domains of communication action: information acquisition, selection, and transmission.\n\nThe communicative action that is relevant to the degrees of information searching for problem solving.\n\nThe communicative action that is relevant to the extent of an individual's directedness in acquiring and sharing information.\n\nThe communicative action that is relevant to the degrees of educating others to utilize collective behaviors for problem solving.\n\nAn individual’s perception toward a problematic life situation, motivation to solve the problem, and activation of cognitive frames influence an individual’s activeness in six information behaviors – information forefending, information permitting, information forwarding, information sharing, information seeking, and information attending.\nSTOPS has more than 200 research bibliographies in academic databases such as \"Communication & Mass Media Complete, Business Source Premier\", and \"Academic Search Premier\". Some of the applications of this theory are in the fields of health communication, crisis communication, organizational communication, and nonprofit communication.\n\n"}
{"id": "33849471", "url": "https://en.wikipedia.org/wiki?curid=33849471", "title": "Social citizenship", "text": "Social citizenship\n\nSocial citizenship was a term first coined by T.H. Marshall, who argued that the ideal citizenship experience entails access to political, civil and social rights in a state. According to Marshall, social citizenship includes “the whole range from the right to a modicum of economic welfare and security to the right to share to the full in the social heritage and to live the life of a civilized being according to the standards prevailing in society”. Marshall’s concept of social policy has been critiqued by some scholars for being idealistic and only applicable to the participation of able-bodied white males.\n\nUnder the Elizabethan Poor Law, social rights were not part of citizenship status. The Poor Law Act of 1601 placed the responsibility of caring for the disabled on the family; the state was not legally obligated to care for those who were unable to work. By the middle of the seventeenth century, small pensions were allocated to the poor, which often included disabled persons who were barred from wage labor, although these provisions were meager and not substantial enough to live off of.\n\nIn 1834, the Poor Law Amendment Act was created in order to reduce the number of able-bodied poor who received a pension. While the disabled were able to access monetary assistance, they could only receive indoor assistance through institutionalization. This structure of monetary provision stigmatized the disabled for their lack of participation in the labor force and ostracized them from the community, a result that reflects the cultural emphasis placed upon the ability and willingness to participate in the labor force.\n\nBeginning in the early 1900s, a “residual approach” to social policy was enacted in Britain where financial assistance would be provided to those without support at a minimal level in order to encourage active participation in the labor force. Post-1945, additions were made to this social welfare approach that encompassed political-economic, social, and organizational reform. Political-economic motivation emphasized reform encouraging high rates of employment, lower taxes, and minimal welfare dependents. Social reform placed women, children, the elderly and the disabled as dependents of a male wage-earner which cast further distinctions between impaired and able-bodied citizens. Finally, organizational reforms gave professionals increasing state support and decision making power in the realm of social services, especially in the case of health services.\n\nThe current model of the welfare state, whereby social citizenship is obtained, encourages citizens to enter a “welfare market” in which they become consumers of welfare benefits, especially benefits linked to health and social care. Beginning in 1979, the conservative administration led by Margaret Thatcher encouraged this model, arguing that market based access to social citizenship allows for the empowerment of citizens. This capitalist model, advocates claim, allows citizens to obtain full social citizenship by becoming “competent members of society,” which according to citizenship theorists Turner and Marshall is a key aspect of being a member of the state. Whereas barred access to full social citizenship was combated by strengthening the private sector under Thatcher, the New Labour Government elected in 1997 focused on the expansion of public services in order to raise employment, and combat poverty. Under the New Labour government, social inclusion became one of the primary rights of social citizenship in Britain. Social inclusion is multifaceted and includes the right of a citizen to participate in society and the economy.\n\nThe Disability Discrimination Act of 1995 was the first social policy created to prevent exclusion and discrimination in matters of the economy, though the act did not address prevalent inequalities in education and transportation.\n\nIn 2001, the Special Educational Needs and Disability Act (SENDA) made discrimination against special needs students illegal and called for 'reasonable adjustments' to be made in order to ensure equal access to education, a crucial aspect of social citizenship.\n\nThe Disability Equality Duty (DED) created in 2005 called for institutions to begin planning changes to infrastructure in order to further include people with disabilities.\n\nDisability Rights movements in Europe have primarily been centered on making changes in social welfare policy. Organizations advocating for disability rights in Britain have argued for the elimination of stereotypes that cast people with disabilities as vulnerable and in need of community care. In the current system, health and service providers have been in control of assessing the needs of the disabled. Many disability rights organizations in Britain have pushed against this system in which the practitioner’s expertise takes precedence over the experience and assessment of the patient. Biehal and colleagues have stated, “Many users of social services are excluded from full citizenship. Their right to treatment as equals may be limited by poverty, racism, assumptions about gender, age and disability”.\n\nPolicies created to ensure the social participation of British citizens with disabilities have been critiqued for not being strict enough. The 1995 Disability Discrimination Act has been accused of providing only limited protection as it did not include protection from discrimination in education or transportation. In addition, 90% of employers were exempt from following the Act due to the fact that they had fewer than 20 employees. The policies that were designed to answer these critiques have also been accused of being inadequate as they are difficult to enforce.\n"}
{"id": "420088", "url": "https://en.wikipedia.org/wiki?curid=420088", "title": "Social mobility", "text": "Social mobility\n\nSocial mobility is the movement of individuals, families, households, or other categories of people within or between social strata in a society. It is a change in social status relative to one's current social location within a given society.\n\nSocial mobility is defined as the movement of individuals, families, households, or other categories of people within or between layers or tiers in an open system of social stratification. Open stratification systems are those in which at least some value is given to achieved status characteristics in a society. The movement can be in a \"downward\" or \"upward\" direction.\n\nMobility is most often quantitatively measured in terms of change in economic mobility such as changes in income or wealth. Occupation is another measure used in researching mobility, which usually involves both quantitative and qualitative analysis of data, but other studies may concentrate on social class. Mobility may be \"intragenerational\", within the same generation, or \"intergenerational\", between one or more generations. Intragenerational mobility is less frequent, representing \"rags to riches\" cases in terms of upward mobility. Intergenerational upward mobility is more common, where children or grandchildren are in economic circumstances better than those of their parents or grandparents. In the US, this type of mobility is described as one of the fundamental features of the \"American Dream\" even though there is less such mobility than almost all other OECD countries.\n\nSocial mobility is highly dependent on the overall structure of social statuses and occupations in a given society. The extent of differing social positions and the manner in which they fit together or overlap provides the overall social structure of such positions. Add to this the differing dimensions of status, such as Max Weber's delineation of economic stature, prestige, and power and we see the potential for complexity in a given social stratification system. Such dimensions within a given society can be seen as independent variables that can explain differences in social mobility at different times and places in different stratification systems. In addition, the same variables that contribute as intervening variables to the valuation of income or wealth and that also affect social status, social class, and social inequality do affect social mobility. These include sex or gender, race or ethnicity, and age.\n\nEducation provides one of the most promising chances of upward social mobility into a better social class and attaining a higher social status, regardless of current social standing in the overall structure of society. However, the stratification of social classes and high wealth inequality directly affects the educational opportunities people are able to obtain and succeed in, and the chance for one's upward social mobility. In other words, social class and a family's socioeconomic status directly affect a child's chances for obtaining a quality education and succeeding in life. By age five, there are significant developmental differences between low, middle, and upper class children's cognitive and noncognitive skills. \nAmong older children, evidence suggests that the gap between high- and low-income primary- and secondary-school students has increased by almost 40 percent over the past thirty years. These differences persist and widen into young adulthood and beyond. Just as the gap in K–12 test scores between high- and low-income students is growing, the difference in college graduation rates between the rich and the poor is also growing. Although the college graduation rate among the poorest households increased by about 4 percentage points between those born in the early 1960s and those born in the early 1980s, over this same period, the graduation rate increased by almost 20 percentage points for the wealthiest households.\n\nAverage family income, and social status, have both seen a decrease for the bottom third of all children between 1975-2011. The 5th percentile of children and their families have seen up to a 60% decrease in average family income. The wealth gap between the rich and the poor, the upper and lower class, continues to increase as more middle-class people get poorer and the lower-class get even poorer. As the socioeconomic inequality continues to increase in the United States, being on either end of the spectrum makes a child more likely to remain there, and never become socially mobile.\nA child born to parents with income in the lowest quintile is more than ten times more likely to end up in the lowest quintile than the highest as an adult (43 percent versus 4 percent). And, a child born to parents in the highest quintile is five times more likely to end up in the highest quintile than the lowest (40 percent versus 8 percent).\n\nThis is due to lower- and working-class parents (where at least one has at most a high school diploma) spending less time on average with their children in their earliest years of life and not being as involved in their children's education and time out of school. This parenting style, known as \"accomplishment of natural growth\" differs from the style of middle-class and upper-class parents (with at least one parent having higher education), known as \"cultural cultivation\". More affluent social classes are able to spend more time with their children at early ages, and children receive more exposure to interactions and activities that lead to cognitive and non-cognitive development: things like verbal communication, parent-child engagement, and being read to daily. These children's parents are much more involved in their academics and their free time; placing them in extracurricular activities which develop not only additional non-cognitive skills but also academic values, habits, and abilities to better communicate and interact with authority figures. Lower class children often attend lower quality schools, receive less attention from teachers, and ask for help much less than their higher class peers. The chances for social mobility are primarily determined by the family a child is born into. Today, the gaps seen in both access to education and educational success (graduating from a higher institution) is even larger. Today, while college applicants from every socioeconomic class are equally qualified, 75% of all entering freshmen classes at top-tier American institutions belong to the uppermost socioeconomic quartile. A family's class determines the amount of investment and involvement parents have in their children's educational abilities and success from their earliest years of life, leaving low-income students with less chance for academic success and social mobility due to the effects that the (common) parenting style of the lower and working-class have on their outlook on and success in education.\n\nThese differing dimensions of social mobility can be classified in terms of differing types of capital that contribute to changes in mobility. Cultural capital, a term first coined by French sociologist Pierre Bourdieu is the process of distinguishing between the economic aspects of class and powerful cultural assets. Bourdieu described three types of capital that place a person in a certain social category: economic capital; social capital; and cultural capital. Economic capital includes economic resources such as cash, credit, and other material assets.\n\nSocial capital includes resources one achieves based on group membership, networks of influence, relationships and support from other people. Cultural capital is any advantage a person has that gives them a higher status in society, such as education, skills, or any other form of knowledge. Usually, people with all three types of capital have a high status in society. Bourdieu found that the culture of the upper social class is oriented more toward formal reasoning and abstract thought. The lower social class is geared more towards matters of facts and the necessities of life. He also found that the environment in which person develops has a large effect on the cultural resources that a person will have.\n\nThe cultural resources a person has obtained can heavily influence a child's educational success. It has been shown that students raised under the concerted cultivation approach have \"an emerging sense of entitlement\" which leads to asking teachers more questions and being a more active student, causing teachers to favor students raised in this manner. This childrearing approach which creates positive interactions in the classroom environment is in contrast with the natural growth approach to childrearing. In this approach, which is more common amongst working-class families, parents do not focus on developing the special talents of their individual children, and they speak to their children in directives. Due to this, it is more rare for a child raised in this manner to question or challenge adults and conflict arises between childrearing practices at home and school. Children raised in this manner are less inclined to participate in the classroom setting and are less likely to go out of their way to positively interact with teachers and form relationships.\n\nIn the United States, links between minority underperformance in our schools have been made with a lacking in the cultural resources of cultural capital, social capital, and economic capital, yet inconsistencies persist even when these variables are accounted for. \"Once admitted to institutions of higher education, African Americans and Latinos continued to underperform relative to their white and Asian counterparts, earning lower grades, progressing at a slower rate, and dropping out at higher rates. More disturbing was the fact that these differentials persisted even after controlling for obvious factors such as SAT scores and family socioeconomic status\". The theory of capital deficiency is among the most recognized explanations for minority underperformance academically—that for whatever reason they simply lack the resources to find academic success. One of the largest factors for this, asides from the social, economic, and cultural capital mentioned earlier, is human capital. This form of capital, identified by social scientists only in recent years, has to do with the education and life preparation of children. \"Human capital refers to the skills, abilities, and knowledge possessed by specific individuals\". This allows college-educated parents who have large amounts of human capital to invest in their children in certain ways to maximize future success—from reading to them at night to possessing a better understanding of the school system which causes them to be less differential to teachers and school authorities. Research also shows that well-educated black parents are less able to transmit human capital to their children when compared to their white counterparts, due to a legacy of racism and discrimination.\n\nWhile it is generally accepted that some level of mobility in society is desirable, there is no consensus agreement upon \"how much\" social mobility is \"good\" or \"bad\" for a society. There is no international \"benchmark\" of social mobility, though one can compare measures of mobility across regions or countries or within a given area over time. While cross-cultural studies comparing differing types of economies are possible, comparing economies of similar type usually yields more comparable data. Such comparisons typically look at intergenerational mobility, examining the extent to which children born into different families have different life chances and outcomes.\n\nIn a study for which the results were first published in 2009, conduct an exhaustive analysis of social mobility in developed countries. In addition to other correlations with negative social outcomes for societies having high inequality, they found a relationship between high social inequality and low social mobility. Of the eight countries studied—Canada, Denmark, Finland, Sweden, Norway, Germany, the UK and the US, the US had both the highest economic inequality and lowest economic mobility. In this and other studies, in fact, the USA has very low mobility at the lowest rungs of the socioeconomic ladder, with mobility increasing slightly as one goes up the ladder. At the top rung of the ladder, however, mobility again decreases.\n\nOne study comparing social mobility between developed countries found that the four countries with the lowest \"intergenerational income elasticity\", i.e. the highest social mobility, were Denmark, Norway, Finland, and Canada with less than 20% of advantages of having a high income parent passed on to their children.\nStudies have also found \"a clear negative relationship\" between income inequality and intergenerational mobility. Countries with low levels of inequality such as Denmark, Norway and Finland had some of the greatest mobility, while the two countries with the high level of inequality—Chile and Brazil—had some of the lowest mobility.\n\nIn Britain, much debate on social mobility has been generated by comparisons of the 1958 National Child Development Study (NCDS) and the 1970 Birth Cohort Study BCS70, which compare intergenerational mobility in earnings between the 1958 and the 1970 UK cohorts, and claim that intergenerational mobility decreased substantially in this 12-year period. These findings have been controversial, partly due to conflicting findings on social class mobility using the same datasets, and partly due to questions regarding the analytical sample and the treatment of missing data. UK Prime Minister Gordon Brown has famously said that trends in social mobility \"are not as we would have liked\".\n\nAlong with the aforementioned \"Do Poor Children Become Poor Adults?\" study, \"The Economist\" also stated that \"evidence from social scientists suggests that American society is much 'stickier' than most Americans assume. Some researchers claim that social mobility is actually declining.\" A German study corroborates these results. In spite of this low mobility Americans have had the highest belief in meritocracy among middle- and high-income countries. A study of social mobility among the French corporate class has found that class continues to influence who reaches the top in France, with those from the upper-middle classes tending to dominate, despite a longstanding emphasis on meritocracy.\n\nThomas Piketty (2014) finds that wealth-income ratios, today, seem to be returning to very high levels in low economic growth countries, similar to what he calls the \"classic patrimonial\" wealth-based societies of the 19th century wherein a minority lives off its wealth while the rest of the population works for subsistence living.\n\nSocial mobility can also be influenced by differences that exist within education. The contribution of education to social mobility often gets neglected in social mobility research although it really has the potential to transform the relationship between origins and destinations. Recognizing the disparities between strictly location and its educational opportunities highlights how patterns of educational mobility are influencing the capacity for individuals to experience social mobility. There is some debate regarding how important educational attainment is for social mobility. A substantial literature argues that there is a direct effect of social origins (DESO) which cannot be explained by educational attainment. However, other evidence suggests that, using a sufficiently fine-grained measure of educational attainment, taking on board such factors as university status and field of study, education fully mediates the link between social origins and access to top class jobs.\n\nThe patterns of educational mobility that exist between inner city schools versus schools in the suburbs is transparent. Graduation rates supply a rich context to these patterns. In the 2013–14 school year, Detroit Public Schools observed a graduation rate of 71% whereas Grosse Pointe High School (Detroit suburb) observed an average graduation rate of 94%. A similar phenomena was observed in Los Angeles, California as well as in New York City. Los Angeles Senior High School (inner city) observed a graduation rate of 58% and San Marino High School (suburb) observed a graduation rate of 96%. New York City Geographic District Number Two (inner city) observed a graduation rate of 69% and Westchester School District (suburb) observed a graduation rate of 85%. These patterns were observed across the country when assessing the differences between inner city graduation rates and suburban graduation rates.\n\nLack of education frequently leads to lack of success in the future for many individuals. They do not possess the degrees required to even apply for a plethora of jobs. Therefore, these individuals may get stuck in communities that are at a stand still. Ultimately, the social classes remain stagnant because nothing is changing within each social construct and education is at the forefront in terms of its contribution to the future issues.\n\nSocial status attainment and therefore social mobility in adulthood are of interest to psychologists, sociologists, political scientists, economists, epidemiologists and many more. The reason behind the interest is because it indicates access to material goods, educational opportunities, healthy environments, and nonetheless the economic growth.\n\nResearchers did a study that encompassed a wide range of data of individuals in lifetime (in childhood and during mid-adulthood). Most of the Scottish children which were born in 1921 participated in the Scottish Mental Survey 1932, which was conducted under the auspices of the Scottish Council for Research in Education (SCRE) and obtained the data of psychometric intelligence of Scottish pupils. The number of children who took the mental ability test (based on the Moray House tests) was 87,498. They were between age 10 and 11. The tests covered general, spatial and numerical reasoning.\n\nAt mid-life period, a subset of the subjects participated in one of the studies, which were large health studies of adults and were carried out in Scotland in the 1960s and 1970s. The particular study they took part in was the collaborative study of 6022 men and 1006 women, conducted between 1970 and 1973 in Scotland. Participants completed a questionnaire (participant's address, father's occupation, the participant's own first regular occupation, the age of finishing full-time education, number of siblings, and if the participant was a regular car driver) and attended a physical examination (measurement of height). Social class was coded according to the Registrar General's Classification for the participant's occupation at the time of screening, his first occupation and his father's occupation. Researchers separated into six social classes were used.\n\nA correlation and structural equation model analysis was conducted. In the structural equation models, social status in the 1970s was the main outcome variable. The main contributors to education (and first social class) were father's social class and IQ at age 11, which was also found in a Scandinavian study. This effect was direct and also mediated via education and the participant's first job.\n\nParticipants at midlife did not necessarily end up in the same social class as their fathers. There was social mobility in the sample: 45% of men were upwardly mobile, 14% were downward mobile and 41% were socially stable. IQ at age 11 had a graded relationship with participant's social class. The same effect was seen for father's occupation. Men at midlife social class I and II (the highest, more professional) also had the highest IQ at age 11. Height at midlife, years of education and childhood IQ were significantly positively related to upward social mobility, while number of siblings had no significant effect. For each standard deviation increase in IQ score at the age 11, the chances of upward social mobility increases by 69% (with a 95% confidence). After controlling the effect of independent variables, only IQ at age 11 was significantly inversely related to downward movement in social mobility. More years of education increase the chance that a father's son will surpass his social class, whereas low IQ makes a father's son prone to falling behind his father's social class.\nHigher IQ at age 11 was also significantly related to higher social class at midlife, higher likelihood car driving at midlife, higher first social class, higher father's social class, fewer siblings, higher age of education, being taller and living in a less deprived neighbourhood at midlife. IQ was significantly more strongly related to the social class in midlife than the social class of the first job.\n\nFinally, height, education and IQ at age 11 were predictors of upward social mobility and only IQ at age 11 and height were significant predictors of downward social mobility. Number of siblings was not significant in neither of the models.\n\nAnother research looked into the pivotal role of education in association between ability and social class attainment through three generations (fathers, participants and offspring) using the SMS1932 (Lothian Birth Cohort 1921) educational data, childhood ability and late life intellectual function data. It was proposed that social class of origin acts as a ballast restraining otherwise meritocratic social class movement, and that education is the primary means through which social class movement is both restrained and facilitated—therefore acting in a pivotal role.\n\nIt was found that social class of origin predicts educational attainment in both the participant's and offspring generations. Father's social class and participant's social class held the same importance in predicting offspring educational attainment—effect across two generations. Educational attainment mediated the association of social class attainments across generations (father's and participants social class, participant's and offspring's social class). There was no direct link social classes across generations, but in each generation educational attainment was a predictor of social class, which is consistent with other studies. Also, participant's childhood ability moderately predicted their educational and social class attainment (.31 and .38). Participant's educational attainment was strongly linked with the odds of moving downward or upward on the social class ladder. For each SD increase in education, the odds of moving upward on the social class spectrum were 2.58 times greater (the downward ones were .26 times greater). Offspring's educational attainment was also strongly linked with the odds of moving upward or downward on the social class ladder. For each SD increase in education, the odds of moving upward were 3.54 times greater (the downward ones were .40 times greater). In conclusion, education is very important, because it is the fundamental mechanism functioning both to hold individuals in their social class of origin and to make it possible for their movement upward or downward on the social class ladder.\n\nIn the Cohort 1936 it was found that regarding whole generations (not individuals) the social mobility between father's and participant's generation is: 50.7% of the participant generation have moved upward in relation to their fathers, 22.1% had moved downwards, and 27.2% had remained stable in their social class. There was a lack of social mobility in the offspring generation as a whole. However, there was definitely individual offspring movement on the social class ladder: 31.4% had higher social class attainment than their participant parents (grandparents), 33.7% moved downward, and 33.9% stayed stable. Participant's childhood mental ability was linked to social class in all three generations. A very important pattern has also been confirmed: average years of education increased with social class and IQ.\n\nThere were some great contributors to social class attainment and social class mobility in the twentieth century: Both social class attainment and social mobility are influenced by pre-existing levels of mental ability, which was in consistence with other studies. So, the role of individual level mental ability in pursuit of educational attainment—professional positions require specific educational credentials. Furthermore, educational attainment contributes to social class attainment through the contribution of mental ability to educational attainment. Even further, mental ability can contribute to social class attainment independent of actual educational attainment, as in when the educational attainment is prevented, individuals with higher mental ability manage to make use of the mental ability to work their way up on the social ladder. This study made clear that intergenerational transmission of educational attainment is one of the key ways in which social class was maintained within family, and there was also evidence that education attainment was increasing over time. Finally, the results suggest that social mobility (moving upward and downward) has increased in recent years in Britain. Which according to one researcher is important because an overall mobility of about 22% is needed to keep the distribution of intelligence relatively constant from one generation to the other within each occupational category.\n\nResearchers looked into the effects elitist and non-elitist education systems have on social mobility. Education policies are often critiqued based on their impact on a single generation, but it is important to look at education policies and the effects they have on social mobility. In the research, elitist schools are defined as schools that focus on providing its best students with the tools to succeed, whereas an egalitarian school is one that predicates itself on giving equal opportunity to all its students to achieve academic success.\n\nWhen private education supplements were not considered, it was found that the greatest amount of social mobility was derived from a system with the least elitist public education system. It was also discovered that the system with the most elitist policies produced the greatest amount of utilitarian welfare. Logically, social mobility decreases with more elitist education systems and utilitarian welfare decreases with less elitist public education policies.\n\nWhen private education supplements are introduced, it becomes clear that some elitist policies promote some social mobility and that an egalitarian system is the most successful at creating the maximum amount of welfare. These discoveries were justified from the reasoning that elitist education systems discourage skilled workers from supplementing their children's educations with private expenditures.\n\nThe authors of the report showed that they can challenge conventional beliefs that elitist and regressive educational policy is the ideal system. This is explained as the researchers found that education has multiple benefits. It brings more productivity and has a value, which was a new thought for education. This shows that the arguments for the regressive model should not be without qualifications. Furthermore, in the elitist system, the effect of earnings distribution on growth is negatively impacted due to the polarizing social class structure with individuals at the top with all the capital and individuals at the bottom with nothing.\n\nEducation is very important in determining the outcome of one's future. It is almost impossible to achieve upward mobility without education. Education is frequently seen as a strong driver of social mobility. The quality of one's education varies depending on the social class that they are in. The higher the family income the better opportunities one is given to get a good education. The inequality in education makes it harder for low-income families to achieve social mobility. Research has indicated that inequality is connected to the deficiency of social mobility. In a period of growing inequality and low social mobility, fixing the quality of and access to education has the possibility to increase equality of opportunity for all Americans.\n\n\"One significant consequence of growing income inequality is that, by historical standards, high-income households are spending much more on their children's education than low-income households.\" With the lack of total income, low-income families can't afford to spend money on their children's education. Research has shown that over the past few years, families with high income has increased their spending on their children's education. High income families were paying $3,500 per year and now it has increased up to nearly $9,000, which is seven times more than what low income families pay for their kids' education. The increase in money spent on education has caused an increase in college graduation rates for the families with high income. The increase in graduation rates is causing an even bigger gap between high income children and low-income children. Given the significance of a college degree in today's labor market, rising differences in college completion signify rising differences in outcomes in the future.\n\nFamily income is one of the most important factors in determining the mental ability(intelligence) of their children. With such bad education that urban school are offering, parents of high income are moving out of these areas to give their children a better opportunity to succeed. As urban school systems worsen, high income families move to rich suburbs because that is where they feel better education is; if they do stay in the city, they put their children to private schools. Low income families don't have a choice but to settle for the bad education because they cannot afford to relocate to rich suburbs. The more money and time parents invest in their child plays a huge role in determining their success in school. Research has showed that higher mobility levels are perceived for locations where there are better schools.\n\n\n\n"}
{"id": "1448018", "url": "https://en.wikipedia.org/wiki?curid=1448018", "title": "Teleonomy", "text": "Teleonomy\n\nTeleonomy is the quality of apparent purposefulness and goal-directedness of structures and functions in living organisms brought about by the exercise, augmentation, and, improvement of reasoning. The term derives from two Greek words, τέλος \"telos\" (\"end, purpose\") and νόμος \"nomos\" (\"law\"), and means \"end-directed\" (literally \"purpose-law\"). Teleonomy is sometimes contrasted with teleology, where the latter is understood as a purposeful goal-directedness brought about through human or divine intention. Teleonomy is thought to derive from evolutionary history, adaptation for reproductive success, and/or the operation of a program. Teleonomy is related to programmatic or computational aspects of purpose.\n\nColin Pittendrigh, who coined the term in 1958, applied it to biological phenomena that appear to be end-directed, hoping to limit the much older term teleology to actions planned by an agent who can internally model alternative futures with intention, purpose and foresight:\nIn 1965 Ernst Mayr cited Pittendrigh and criticized him for not making a \"clear distinction between the two teleologies of Aristotle\"; evolution involves Aristotle's material causes and formal causes rather than efficient causes. Mayr adopted Pittendrigh's term, but supplied his own definition:\n\nRichard Dawkins described the properties of \"archeo-purpose\" (by natural selection) and \"neo-purpose\" (by evolved adaptation) in his talk on the \"Purpose of Purpose\". Dawkins attributes the brain's flexibility as an evolutionary feature in adapting or subverting goals to making neo-purpose goals on an overarching evolutionary archeo-purpose. Language allows groups to share neo-purposes, and cultural evolution - occurring much faster than natural evolution - can lead to conflict or collaborations.\n\nIn behavior analysis, Hayne Reese made the adverbial distinction between purposefulness (having an internal determination) and purposiveness (serving or effecting a useful function). Reese implies that non-teleological statements are called teleonomic when they represent an \"if A then C\" phenomenon's antecedent; where, teleology is a consequent representation. The concept of purpose, as only being the teleology final cause, requires supposedly impossible time reversal; because, the future consequent determines the present antecedent. Purpose, as being both in the beginning and the end, simply rejects teleology, and addresses the time reversal problem. In this, Reese sees no value for teleology and teleonomic concepts in behavior analysis; however, the concept of purpose preserved in process can be useful, if not reified. A theoretical time-dimensional tunneling and teleological functioning of temporal paradox would also fit this description without the necessity of a localized intelligence. Whereas the concept of a teleonomic process, such as evolution, can simply refer to a system capable of producing complex products without the benefit of a guiding foresight.\n\nIn 1966 George C. Williams approved of the term in the last chapter of his \"Adaptation and Natural Selection; a critique of some current evolutionary thought\". In 1970, Jacques Monod, in \"Chance and Necessity, an Essay on the Natural Philosophy of Modern Biology\", suggested teleonomy as a key feature that defines life:\n\nIn 1974 Ernst Mayr illustrated the difference in the statements:\n\nSubsequently, philosophers like Ernest Nagel further analysed the concept of goal-directedness in biology and by 1982, philosopher and historian of science David Hull joked about the use of teleology and teleonomy by biologists:\n\nThe concept of teleonomy was largely developed by Mayr and Pittendrigh to separate biological evolution from teleology. Pittendrigh's purpose was to enable biologists who had become overly cautious about goal-oriented language to have a way of discussing the goals and orientations of an organism's behaviors without inadvertently invoking teleology. Mayr was even more explicit, saying that while teleonomy certainly operates on the level of organisms, the process of evolution itself is necessarily non-teleonomic.\n\nEvolution largely hoards hindsight, as variations unwittingly make \"predictions\" about structures and functions which could successfully cope with the future, and which participate in a process of natural selection that culls the unfit, leaving the fit to the next generation. Information accumulates about functions and structures that are successful, exploiting feedback from the environment via the selection of fitter coalitions of structures and functions. Robert Rosen has described these features as an anticipatory system which builds an internal model based on past and possible future states.\n\nIn 1962, Grace A. de Laguna's \"The Role of Teleonomy in Evolution\" attempted to show how different stages of evolution were characterized by different types of teleonomy. de Laguna points out that humans have oriented teleonomy so that the teleonomic goal is not restricted to the reproduction of humans, but also to cultural ideals.\n\nIn recent years, a few biologists believe that the separation of teleonomy from the process of evolution has gone too far. Peter Corning notes that behavior, which is a teleonomic trait, is responsible for the construction of biological niches, which is an agent of selection. Therefore, it would be inaccurate to say that there was no role for teleonomy in the process of evolution, since teleonomy dictates the fitness landscape according to which organisms are selected. Corning calls this phenomenon \"teleonomic selection\".\n\nIn teleology, Kant's positions as expressed in Critique of Judgment, were neglected for many years because in the minds of many scientists they were associated with vitalist views of evolution. Their recent rehabilitation is evident in teleonomy, which bears a number of features, such as the description of organisms, that are reminiscent of the Aristotelian conception of final causes as essentially recursive in nature. Kant's position is that, even though we cannot know whether there are final causes in nature, we are constrained by the peculiar nature of the human understanding to view organisms teleologically. Thus the Kantian view sees teleology as a necessary principle for the study of organisms, but only as a regulative principle, and with no ontological implications.\n\nTalcott Parsons, in the later part of his working with a theory of social evolution and a related theory of world-history, adopted the concept of teleonomy as the fundamental organizing principle for directional processes and his theory of societal development in general. In this way, Parsons tried to find a theoretical compromise between voluntarism as a principle of action and the idea of a certain directionality in history.\n\nTeleonomy is closely related to concepts of emergence, complexity theory, and self-organizing systems. It has extended beneath biology to be applied in the context of chemistry. Some philosophers of biology resist the term and still employ \"teleology\" when analyzing biological function and the language used to describe it, while others endorse it.\n\n\n\n"}
{"id": "43829217", "url": "https://en.wikipedia.org/wiki?curid=43829217", "title": "The Light Between Oceans (film)", "text": "The Light Between Oceans (film)\n\nThe Light Between Oceans is a 2016 romantic period drama film written and directed by Derek Cianfrance and based on the 2012 novel of the same name by M. L. Stedman. An international co-production between the United States, Australia, the United Kingdom, and New Zealand, the film stars Michael Fassbender, Alicia Vikander, Rachel Weisz, Bryan Brown, and Jack Thompson. The film tells the story of a lighthouse keeper and his wife who rescue and adopt an infant girl adrift at sea. Years later, the couple discover the child's true parentage and are faced with the moral dilemma of their actions.\n\n\"The Light Between Oceans\" had its world premiere at the 73rd Venice International Film Festival on September 1, 2016, where it competed for the Golden Lion. The film was released by Touchstone Pictures in North America on September 2, 2016, being the last DreamWorks Pictures film distributed by Walt Disney Studios Motion Pictures through their 2011 output deal. The film was released in the United Kingdom on November 1, 2016, by Entertainment One Films. It received mixed reviews and grossed $26 million worldwide.\n\nTom Sherbourne, a traumatised and withdrawn hero of World War I, is hired as a lightkeeper at Janus Rock, a lighthouse off the coast of Western Australia. He falls in love with a local girl, Isabel Graysmark, and they marry in 1921. Isabel loses two pregnancies in three years and fears she may never become a mother.\n\nShortly after Isabel's second miscarriage, a rowboat containing a dead man and a newborn baby girl washes ashore near the lighthouse. Tom knows that regulations require him to report the discovery. However, Isabel fears that the baby will almost certainly be sent to an orphanage and persuades Tom to pass the baby off as their own. Tom grudgingly agrees. He buries the man on the island, and the couple names the girl Lucy.\n\nAs Tom and Isabel are about to have Lucy christened on the mainland, Tom sees a woman, Hannah Roennfeldt, kneeling in front of a grave bearing the names of Frank Roennfeldt and his baby daughter Grace Ellen, who were lost at sea on the day they found Lucy, 26 April 1923. Tom fears that Lucy might very well be Hannah's missing daughter. He writes anonymously to Hannah to tell her that her husband is dead but that her baby is safe and well cared-for.\n\nFour years later, Tom, Isabel, and Lucy, who have enjoyed an idyllic life together, attend a ceremony for the anniversary of Tom's lighthouse, and they strike up a conversation with Hannah and her sister, Gwen Potts. They learn that Frank was a German, that Hannah's marrying him so soon after the First World War was controversial, and that he had been accosted in the street by a drunken crowd. He jumped into a rowboat and fled with his baby daughter. \n\nTormented by his conscience, Tom sends Hannah a small rattle that was found with Lucy on the boat. One of Tom's co-workers recognizes the rattle on a reward poster and reports him to the police.\n\nTom takes full responsibility, claiming he bullied Isabel into complying. Isabel is enraged that Tom is willing to give Lucy away and breaks off contact with him after his arrest. Lucy is returned to her birth family but initially rejects and hates them, having no memory of them. She refuses to answer to \"Grace\" and even runs away in an effort to return to the lighthouse.\n\nThe police accuse Tom of murdering Frank and are unable to draw an answer from the distraught Isabel as to whether or not Frank was dead when they discovered him. Lucy runs away to look for the lighthouse, and a search team is sent to rescue her. She is found and returned to Hannah, but the events lead Hannah to realize that Lucy now belongs to Isabel. \n\nHannah promises to return Lucy to Isabel as soon as Isabel testifies against Tom. Just as Tom is about to be taken by boat to Albany for trial, Isabel reads a letter which Tom had sent her, confiding he had not deserved his happiness with Lucy and how carrying the blame will assuage his guilt for surviving the war. \n\nIsabel jumps on the boat and confesses everything. Moved by their gesture and reminded by Frank's words to always forgive others, Hannah offers to speak on their behalf at trial. Lucy has at last begun to bond with her biological mother and grandfather, who agrees to call her \"Lucy-Grace\" as a compromise.\n\nIn 1950, an adult Lucy-Grace Rutherford, accompanied by her baby son Christopher, tracks Tom down. She has not been in contact with the Sherbournes for over twenty years, as they had agreed not to contact her for the rest of her childhood. Isabel has recently died, still tormented with guilt for her actions, and Tom gives Lucy-Grace a letter that Isabel had written in case Lucy ever made contact. An emotional Lucy-Grace thanks Tom, the only father she knew, for rescuing and raising her for the few years on Janus, and she asks if she can visit again. She and Tom embrace before she leaves.\n\n\nDreamWorks acquired the rights to the novel on November 27, 2012, with David Heyman and Jeffrey Clifford producing through Heyday Films. DreamWorks approached Derek Cianfrance at the behest of Steven Spielberg, who was impressed by Cianfrance's \"Blue Valentine\". In September 2013, Cianfrance was announced to direct the film. In May 2014, Michael Fassbender was announced in the film. In June 2014, Alicia Vikander joined the cast of the film, followed by Rachel Weisz in July 2014. Participant Media joined the production in August 2014.\n\nPrincipal photography started in September 2014, with filming locations in New Zealand and Australia. Filming took place in Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough. Filming sites included the former Dunedin Prison in Dunedin and Stuart Street at the former King Edward Technical College building.\n\nFootage aboard a steam train was filmed in October inside a refurbished wooden \"bird cage\" passenger carriage from the Pleasant Point Railway in South Canterbury. Mainline Steam Heritage Trust used Ja 1240 \"Jessica\" for the movie and it was transferred from the trust's Christchurch depot to Dunedin for filming to take place. While the scene in which the locomotive is used was set in 1918, locomotive Ja 1240 was built in 1947 and was the second New Zealand Railways JA class locomotive to be built. Built at New Zealand Railways Hillside workshops in Dunedin it ran exclusively in the South Island of New Zealand from 1947 until 1971.\n\nIn November the production moved to Australia and filming began in Stanley, Tasmania where the crews transformed some locations in the town including the pier, which was refurbished, and the road, which was covered in gravel. Production wrapped on November 24, 2014.\n\nCianfrance spent a year editing the film, with little breaks in between with the first cut of the film ending up at 2 hours and 20 minutes.\n\n\"The Light Between Oceans\" held its world premiere at the 73rd Venice International Film Festival on September 1, 2016. The film was distributed by Walt Disney Studios Motion Pictures through the Touchstone Pictures banner, being the last DreamWorks film to be released under the original agreement with Walt Disney Studios. Disney released the film in the United States on September 2, 2016. Disney opted not to give the film a limited release, a method often used by studios for adult dramas, and instead issued the film in general wide release at 1,500 locations with focus on upscale venues.\n\nDisney also distributed \"The Light Between Oceans\" overseas, except for territories in Europe, the Middle East, Africa and Japan, where distribution was handled by Mister Smith Entertainment through other third-party film distributors; Entertainment One Films in the United Kingdom, Reliance in India, Arthouse in Russia, and Phantom Film in Japan.\n\n\"The Light Between Oceans\" was released by Touchstone Home Entertainment on Blu-ray, DVD, and digital download on January 24, 2017.\n\n\"The Light Between Oceans\" grossed $12.5 million in North America and $13.3 million in other territories for a worldwide total of $26 million, against a budget of $20 million.\n\nIn the United States, the film was released on September 2, 2016, alongside \"Morgan\", and was projected to gross $6–9 million from around 1,500 theaters in its opening weekend. It grossed $1.4 million on its first day and $4.8 million in its opening weekend, finishing 6th at the box office.\n\n\"The Light Between Oceans\" received mixed reviews from critics. On review aggregation website Rotten Tomatoes, the film has an approval rating of 60%, based on 214 reviews, with an average rating of 6.2/10. The website's critical consensus reads, \"\"The Light Between Oceans\" presents a well-acted and handsomely mounted adaptation of its bestselling source material, but ultimately tugs on the heartstrings too often to be effective.\" On Metacritic, the film has an average score of 60 out of 100, based on 44 critics, indicating \"mixed or average reviews\". According to CinemaScore, audiences gave the film an average grade of \"B+\", on an A+ to F scale.\n\n"}
{"id": "37711130", "url": "https://en.wikipedia.org/wiki?curid=37711130", "title": "The Sticking Place", "text": "The Sticking Place\n\nThe Sticking Place is a 2012 interactive web documentary by Moosestash Films. The film explores the journey of elite athlete, Leah Callahan, a freestyle wrestler with a dream to compete in the Olympic Games.\n\nFollowing Leah Callahan, the web documentary blends together some of the most candid moments in the wrestler's push to the Olympic Games, presenting her journey through collage, video, animation, photos, audio clips, and user activated/generated content, housed within a collection of vibrant, responsive web-screens. Audiences are invited to explore some of the most intimate details of Leah's day-to-day life, watch her train, read her personal journal entries and emails, share in her childhood home videos, and spend moments with friends and family, all the while examining the realities of what it takes to pursue an Olympic-sized dream.\n\nVancouver-based filmmakers Josephine Anderson and Brittany Baxter co-directed/produced the film, following and filming Callahan over the span of a year in Edmonton, Calgary, Winnipeg, Mackenzie and Prince George. The Sticking Place was produced with assistance from the National Film Board of Canada's Filmmaker Assistance Program, and with funds generated from a crowd-funding campaign on Kickstarter.\n\n\"The Sticking Place\" went live online on June 27, 2012. On February 24, 2013, The Sticking Place was awarded a Pixel Award for Best Sports Project, also garnering the People's Champ Award. On November 15, 2012, The Sticking Place was nominated for Best Web Series: Non-Fiction at the Digi Awards in Toronto, formerly known as the Canadian New Media Awards.\n\n"}
{"id": "10351785", "url": "https://en.wikipedia.org/wiki?curid=10351785", "title": "Timeline of racial tension in Omaha, Nebraska", "text": "Timeline of racial tension in Omaha, Nebraska\n\nThe timeline of racial tension in Omaha, Nebraska lists events in African-American history in Omaha. These included racial violence, but also include many firsts as the African-American community built its institutions. Omaha has been a major industrial city on the edge of what was a rural, agricultural state. It has attracted a more diverse population than the rest of the state. Its issues were common to other major industrial cities of the early 20th century, as it was a destination for 19th and 20th century European immigrants, and internal white and African-American migrants from the South in the Great Migration. Many early 20th century conflicts arose out of labor struggles, postwar social tensions and economic problems, and hiring of later immigrants and black migrants as strikebreakers in the meatpacking and stockyard industries. Massive job losses starting in the 1960s with the restructuring of the railroad, stockyards and meatpacking industries contributed to economic and social problems for workers in the city.\n\nAccording to several prominent Omaha historians, racial discrimination was a significant issue in Omaha from the 1950s through the 2000s (decade). Analyzing race relations in Omaha during the period they commented, \"1968 rivals 1919 as probably the worst year in the history of twentieth-century America from the standpoint of violence and internal tension.\" In 1969 three days of rioting swept the Near North Side, and in 1970 a policeman was killed by a suitcase bomb while answering a disturbance call at a house in North Omaha. However, as the 1966 Oscar-nominated documentary \"A Time for Burning\" and the 1970s books of Lois Mark Stalvey illustrated, the violence apparently served a purpose as lines of communication were opened between the \"West Omaha matron and the black laborer.\"\n\n\n"}
{"id": "8526082", "url": "https://en.wikipedia.org/wiki?curid=8526082", "title": "Unfolding Object", "text": "Unfolding Object\n\nUnfolding Object is a 2002 work of internet art created by John Simon after a commission from the Solomon R. Guggenheim Museum in New York City. Along with \"net.flag\" by Mark Napier, it was among the first pieces of internet art to be collected by a major museum.\n\nSimon has described \"Unfolding Object\" as \"an endless book that rewrites itself and whose use dictates its content.\" It begins as a blank square visible on a web page hosted on the museum's website, but responds when clicked by visitors to the site. Gradually the square unfolds, click by click, until it reaches a certain point, after which it begins to close. The \"pages\" of the design are programmed to respond differently as they receive more clicks; vertical, horizontal, and diagonal lines denote different numbers of previous clicks. The colors of the square and of its background are both programmed to change hourly.\n\n"}
{"id": "43874041", "url": "https://en.wikipedia.org/wiki?curid=43874041", "title": "United South End Settlements", "text": "United South End Settlements\n\nThe United South End Settlements (USES) consist of four settlement houses, founded as part of the Settlement movement to provide services such as daycare, education, and healthcare to improve the lives of the poor, and a children's museum dating back to 1891. With their slogan of \"neighbors helping neighbors since 1891\", the United South End Settlements continues to serve Boston's South End and Lower Roxbury community today. USES has grown and evolved over time to remain relevant to the South End.\n\nThe history of USES began in 1891 when William J. Tucker founded what was to become known as the South End House at 6 Rollins Street. The South End House would be the first of its kind \"in Boston and the fourth one in the United States.\" Before the incorporation of the final five organizations into USES, the Federation of South End Settlements was formed in 1950. The Federation of South End Settlements was made up of the South End House, Lincoln House (1892), Hale House (1895), Harriet Tubman House (1904), Ellis Memorial House, and Eldridge House. This federation was founded in order to create a larger pool of funds for the organizations to share. Eventually the Ellis Memorial and Eldridge Houses would disassociate themselves in 1959, and the four remaining houses and the Children's Art Centre would unite to become the United South End Settlements in January 1960.\n\nUSES originally focused on providing services to \"[improve] housing, public health, and sanitation, developing day care programs that included medical care for children, and creating mental health programs.\" Specifically, USES' \"residents established milk stations, public baths, dispensaries, and services, such as emergency loan and stamp savings programs\" USES also founded \"specialized schools for industrial, vocational, and employment training for both women and men\" to create more opportunity in the South End neighborhood. Culture and arts were not forgotten in the South End either as USES provided \"free concerts, art exhibitions, reading rooms, and a variety of social, drama, and literary clubs\" for its residents. As of 2014, USES still provides many of the same services that it has all along, incorporating the technology of today into its programs and curriculums for the residents of the South End and Lower Roxbury community.\n\n\n\nIndividuals interested in volunteering with the organization must submit an application (available online). All volunteers and interns are given the opportunity to list their preferred programs and hours of availability.\n\nDonating to USES is possible online. Donors that give at least one-thousand dollars annually are named members of the Harriet Tubman Society and their names can be featured on the USES website.\n"}
{"id": "2685907", "url": "https://en.wikipedia.org/wiki?curid=2685907", "title": "Universality (philosophy)", "text": "Universality (philosophy)\n\nIn philosophy, universality is the idea that universal facts exist and can be progressively discovered, as opposed to relativism. In certain theologies, universalism is the quality ascribed to an entity whose existence is consistent throughout the universe, whose being is independent of and unconstrained by the events and conditions that compose the universe, such as entropy and physical locality. \n\nThis article also discusses Kantian and Platonist notions of \"universal\", which are considered by most philosophers to be separate notions.\n\nWhen used in the context of ethics, the meaning of \"universal\" refers to that which is true for \"all similarly situated individuals.\" Rights, for example in natural rights, or in the 1789 Declaration of the Rights of Man and of the Citizen, for those heavily influenced by the philosophy of the Enlightenment and its conception of a human nature, could be considered universal. The 1948 Universal Declaration of Human Rights is inspired by such principles.\n\nIn logic, or the consideration of valid arguments, a proposition is said to have universality if it can be conceived as being true in all possible contexts without creating a contradiction. Some philosophers have referred to such propositions as universalizable. A truth is considered to be universal if it is logically valid (logical) in and also beyond all times and places. Hence a universal truth is considered logically to transcend the state of the physical universe, whose order is derived from such truths. In this case, such a truth is seen as eternal or as absolute. The patterns and relations expressed by mathematics in ways that are consistent with the fields of logic and mathematics are typically considered truths of universal scope. This is not to say that universality is limited to mathematics, since it is also used in philosophy, theology, and other pursuits.\n\nThe relativist conception denies the existence of some or all universal truths, particularly ethical ones (as moral relativism). Though usage of the word \"truth\" has various domains of application, relativism does not necessarily apply to all of them.\n\nIn metaphysics, a universal is a type, a property, or a relation. The noun \"universal\" contrasts with \"individual\", while the adjective \"universal\" contrasts with \"particular\" or sometimes with \"concrete\". The latter meaning, however, may be confusing since Hegelian and neo-Hegelian (e.g. British idealist) philosophies speak of \"concrete universals\".\n\nA universal may have instances, known as its \"particulars\". For example, the type \"dog\" (or \"doghood\") is a universal, as are the property \"red\" (or \"redness\") and the relation \"betweenness\" (or \"being between\"). Any particular dog, red thing, or object that is between other things is not a universal, however, but is an \"instance\" of a universal. That is, a universal type (\"doghood\"), property (\"redness\"), or relation (\"betweenness\") \"inheres\" a particular object (a specific dog, red thing, or object between other things).\n\nPlatonic realism holds universals to be the referents of general terms, i.e. the \"abstract\", nonphysical entities to which words like \"doghood\", \"redness\", and \"betweenness\" refer. By contrast, particulars are the referents of proper names, like \"Fido\", or of definite descriptions that identify single objects, like the phrase, \"that apple on the table\". By contrast, other metaphysical theories merely use the terminology of universals to describe physical entities.\n\n\"The problem of universals\" is an ancient problem in metaphysics concerning the nature of universals, or whether they exist. Part of the problem involves the implications of language use and the complexity of relating language to ontological theory.\n\nUniversal truth is regarded as ontic, i.e. expressing the order of being itself. A universal truth is epistemic only to the extent that its ontic expression is apprehended or discerned in a veridical way, which cannot affect its being in any case. Most ontological frameworks do not consider classes to be universals, although some prominent philosophers, such as John Bigelow, do. \n\n"}
{"id": "28734467", "url": "https://en.wikipedia.org/wiki?curid=28734467", "title": "World Health Organisation Composite International Diagnostic Interview", "text": "World Health Organisation Composite International Diagnostic Interview\n\nThe World Health Organisation Composite International Diagnostic Interview (CIDI) is a structured interview for psychiatric disorders. As the interview is designed for epidemiological studies, it can be administered by those who are not clinically trained and can be completed in a short amount of time. Versions of the CIDI were used in two important studies, the National Comorbidity Survey (NCS)\nand National Comorbidity Survey Replication (NCS-R)\nwhich are often used as a reference for estimates of the rates of psychiatric illness in the USA.\nThe first version of the CIDI was published in 1988, and has been periodically updated to reflect the changing diagnostic criteria of DSM and ICD.\n\nThe Composite International Diagnostic Interview – Short Form (CIDI-SF) was first published by Ronald C. Kessler and colleagues in 1998,\nand the PhenX Toolkit uses this as its adult protocol for general psychiatric assessment. However, the CIDI-SF is no longer supported. According to a 2007 memo by Kessler, this decision was based on decreased need for the CIDI-SF following the introduction of other short interviews (specifically, PRIME-MD and MINI) and a lack of funding to refine the instrument.\n\n"}
