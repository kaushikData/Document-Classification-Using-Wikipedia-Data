{"id": "54336810", "url": "https://en.wikipedia.org/wiki?curid=54336810", "title": "Amargosa River pupfish", "text": "Amargosa River pupfish\n\nThe Amargosa River pupfish (\"Cyprinodon nevadensis amargosae\") is a member of a pupfish species complex which inhabits the watershed of ancient Lake Manly (present day Death Valley in California, USA). Currently, the species inhabits two disjunct perennial reaches of the lower Amargosa River. The upstream portion is near Tecopa and passes through the Amargosa Canyon. The lower portion is northwest of Saratoga Springs, just at the head (southern inlet) of Death Valley, where the Amargosa River turns north to enter the valley.\n\nThese dimunitive fish subsist on cyanobacteria and algae. They have a life history adapted to the vagaries of the intermittent nature of their environment. They have a very short generation time (<1 year and usually just a few months), which allows for rapid exploitation of flooded portions of the streambed in years of high flow.\n"}
{"id": "22553934", "url": "https://en.wikipedia.org/wiki?curid=22553934", "title": "Atheist feminism", "text": "Atheist feminism\n\nAtheist feminism is a branch of feminism that advocates atheism. Atheist feminists also oppose religion as a main source of female oppression and inequality, believing that the majority of the religions are sexist and oppressive to women.\n\nThe first known feminist who was also an atheist was Ernestine Rose, born in Poland on January 13, 1810. Her open confession of disbelief in Judaism when she was a teenager brought her into conflict with her father (who was a rabbi) and an unpleasant relationship developed. In order to force her into the obligations of the Jewish faith, her father, without her consent, betrothed her to a friend and fellow Jew when she was sixteen. Instead of arguing her case in a Jewish court (since her father was the local rabbi who ruled on such matters), she went to a secular court, pleaded her own case, and won. In 1829 she went to England, and in 1835 she was one of the founders of the British atheist organization Association of All Classes of All Nations, which \"called for human rights for all people, regardless of sex, class, color, or national origin\". She lectured in England and America (moving to America in May 1836) and was described by Samuel P. Putnam 3 as \"one of the best lecturers of her time\". He wrote that \"no orthodox [meaning religious] man could meet her in debate\".\n\nIn the winter of 1836, Judge Thomas Hertell, a radical and freethinker, submitted a married women's property act in the legislature of the state of New York to investigate ways of improving the civil and property rights of married women, and to permit them to hold real estate in their own name, which they were not then permitted to do in New York. Upon hearing of the resolution, Ernestine Rose drew up a petition and began the soliciting of names to support the resolution in the state legislature, sending the petition to the legislature in 1838. This was the first petition drive done by a woman in New York. Ernestine continued to increase both the number of the petitions and the names until such rights were finally won in 1848, with the passing of the Married Women's Property Act. Others who participated in the work for the bill included Susan B. Anthony, Elizabeth Cady Stanton, Lucretia Mott and Frances Wright, who were all anti-religious. Later when Susan B. Anthony and Elizabeth Cady Stanton analyzed the influences which led to the Seneca Falls Convention on women's rights in 1848, they identified three causes, the first two being the radical ideas of Frances Wright and Ernestine Rose on religion and democracy, and the initial reforms in women's property law in the 1830s and 1840s.\n\nErnestine later joined a group of freethinkers who had organized a Society for Moral Philanthropists, at which she often lectured. In 1837, she took part in a debate that continued for thirteen weeks, where her topics included the advocacy of abolition of slavery, women's rights, equal opportunities for education, and civil rights. In 1845 she was in attendance at the first national convention of infidels [meaning atheists]. Ernestine Rose also introduced \"the agitation on the subject of women's suffrage\" in Michigan in 1846. In a lecture in Worcester, Massachusetts in 1851, she opposed calling upon the Bible to underwrite the rights of women, claiming that human rights and freedom of women were predicated upon \"the laws of humanity\" and that women, therefore, did not require the written authority of either Paul or Moses, because \"those laws and our claim are prior\" to both.\n\nShe attended the Women's Rights Convention in the Tabernacle, New York City, on September 10, 1853, and spoke at the Hartford Bible Convention in 1854. It was in March of that year, also, that she took off with Susan B. Anthony on a speaking tour to Washington, D.C. Susan B. Anthony arranged the meetings and Ernestine Rose did all of the speaking; after this successful tour, Susan B. Anthony embarked on her own first lecture tour.\n\nLater, in October 1854, Ernestine Rose was elected president of the National Women's Rights Convention at Philadelphia, overcoming the objection that she was unsuitable because of her atheism. Susan B. Anthony supported her in this fight, declaring that every religion—and none—should have an equal right on the platform. In 1856 she spoke at the Seventh National Woman's [Rights] Convention saying in part, \"And when your minister asks you for money for missionary purposes, tell him there are higher, and holier, and nobler missions to be performed at home. When he asks for colleges to educate ministers, tell him you must educate woman, that she may do away with the necessity of ministers, so that they may be able to go to some useful employment.\"\n\nShe appeared again in Albany, New York, for the State Women's Rights Convention in early February 1861, the last one to be held until the end of the Civil War. On May 14, 1863, she shared the podium with Elizabeth Cady Stanton, Susan B. Anthony, Lucy Stone, and Antoinette Blackwell when the first Women's National Loyal League met to call for equal rights for women, and to support the government in the Civil War \"in so far as it makes a war for freedom\".\n\nShe was in attendance at the American Equal Rights Association meeting in which there was a schism and on May 15, 1869, she joined with Elizabeth Cady Stanton, Susan B. Anthony, and Lucy Stone to form a new organization, the National Woman Suffrage Association, which fought for both male and female suffrage, taking a position on the executive committee. She died at Brighton, England, on August 4, 1892, at age eighty-two.\n\nThe most prominent other people to publicly advocate for feminism and to challenge Christianity in the 1800s were Elizabeth Cady Stanton and Matilda Joslyn Gage. In 1885 Stanton wrote an essay entitled \"Has Christianity Benefited Woman?\" arguing that it had in fact hurt women's rights, and stating, \"All religions thus far have taught the headship and superiority of man, [and] the inferiority and subordination of woman. Whatever new dignity, honor, and self-respect the changing theologies may have brought to man, they have all alike brought to woman but another form of humiliation\". In 1893 Matilda Joslyn Gage wrote the book for which she is best known, \"Woman, Church, and State\", which was one of the first books to draw the conclusion that Christianity is a primary impediment to the progress of women, as well as civilization. In 1895 Elizabeth Cady Stanton wrote \"The Woman's Bible\", revised and continued with another book of the same name in 1898, in which she criticized religion and stated \"the Bible in its teachings degrades women from Genesis to Revelation.\" She died in 1902.\n\nAtheist feminist Anne Nicol Gaylor cofounded the Freedom From Religion Foundation in 1976 with her daughter, Annie Laurie Gaylor, and was also editor of \"Freethought Today\" from 1984 to 2009, when she became executive editor. Aside from promoting atheism in general, her atheist feminist activities include writing the book \"Woe To The Women: The Bible Tells Me So\", first published in 1981, which is now in its 4th printing. This book exposes and discusses sexism in the Bible. Furthermore, her 1997 book, \"Women Without Superstition: \"No Gods, No Masters\"\", was the first collection of the writings of historic and contemporary female freethinkers. She has also written several articles on religion's harm to women.\n\nOther notable atheist feminists active today include Ayaan Hirsi Ali, Ophelia Benson, Amanda Marcotte, and Taslima Nasrin. and Sikivu Hutchinson author of \"Moral Combat, Black Atheists, Gender Politics and the Values Wars\", the first book by an African American woman on atheism, racial politics, gender justice and feminism. African American feminist atheists like Hutchinson espouse an intersectional approach to feminist organizing, activism and scholarship that is rooted in the lived experiences and social history of communities of color with respect to racism, white supremacy, sexism/misogynoir, heterosexism and capitalist oppression. Black feminist atheist praxis differs from atheist feminist approaches that confine critique of religion to dogma and gender oppression rather than looking at how religious hierarchies are also informed by imperialism, capitalism and segregation. Feminist activist from FEMEN Inna Shevchenko speaks out against organised religions as one of the major historical obstacles for women's liberation and feminism. At the Secular Conference 2017 in London,speaking on compatibility of feminism and religion, she said \"I am looking forward for a day, when imams, rabbis, priests, religious fanatics, sexistst and mysoginists fed by monotheist dogmas will go down on their knees but not to pray for support of their god, they will go on their knees in front of women of the world to pray for their forgiveness. It is only then they can be proud of their gods\".\n\nIn 2012 the first \"Women in Secularism\" conference was held, from May 18–20 at the Crystal City Marriott at Reagan National Airport in Arlington, Virginia.\n\nIn August 2012 Jennifer McCreight founded a movement known as Atheism Plus that \"applies skepticism to everything, including social issues like sexism, racism, politics, poverty, and crime.\" Atheism Plus had a website that was active from 2012 to 2016.\n\nIn July 2014 a joint statement by atheist activists Ophelia Benson and Richard Dawkins was issued stating, \"It’s not news that allies can’t always agree on everything. People who rely on reason rather than dogma to think about the world are bound to disagree about some things. Disagreement is inevitable, but bullying and harassment are not. If we want secularism and atheism to gain respect, we have to be able to disagree with each other without trying to destroy each other. In other words we have to be able to manage disagreement ethically, like reasonable adults, as opposed to brawling like enraged children who need a nap. It should go without saying, but this means no death threats, rape threats, attacks on people’s appearance, age, race, sex, size, haircut; no photoshopping people into demeaning images, no vulgar epithets.\" Dawkins added, \"I’m told that some people think I tacitly endorse such things even if I don’t indulge in them. Needless to say, I’m horrified by that suggestion. Any person who tries to intimidate members of our community with threats or harassment is in no way my ally and is only weakening the atheist movement by silencing its voices and driving away support.\"\n\n"}
{"id": "52777879", "url": "https://en.wikipedia.org/wiki?curid=52777879", "title": "Base change theorems", "text": "Base change theorems\n\nIn mathematics, the base change theorems relate the direct image and the pull-back of sheaves. More precisely, they are about the base change map, given by the following natural transformation of sheaves:\n\nwhere\n\nis a Cartesian square of topological spaces and formula_3 is a sheaf on \"X\".\n\nSuch theorems exist in different branches of geometry: for (essentially arbitrary) topological spaces and proper maps \"f\", in algebraic geometry for (quasi-)coherent sheaves and \"f\" proper or \"g\" flat, similarly in analytic geometry, but also for étale sheaves for \"f\" proper or \"g\" smooth.\n\nA simple base change phenomenon arises in commutative algebra when \"A\" is a commutative ring and \"B\" and \"A' \"are two \"A\"-algebras. Let formula_4. In this situation, given a \"B\"-module \"M\", there is an isomorphism (of \"A' \"-modules):\n\nHere the subscript indicates the forgetful functor, i.e., formula_6 is \"M\", but regarded as an \"A\"-module.\nIndeed, such an isomorphism is obtained by observing \n\nThus, the two operations, namely forgetful functors and tensor products commute in the sense of the above isomorphism.\nThe base change theorems discussed below are statements of a similar kind.\n\nThe base change theorems presented below all assert that (for different types of sheaves, and under various assumptions on the maps involved), that the following \"base change map\"\n\nis an isomorphism, where\n\nare continuous maps between topological spaces that form a Cartesian square and formula_3 is a sheaf on \"X\". Here formula_11 denotes the higher direct image of formula_12 under \"f\", i.e., the derived functor of the direct image (also known as pushforward) functor formula_13.\n\nThis map exists without any assumptions on the maps \"f\" and \"g\". It is constructed as follows: since formula_14 is left adjoint to formula_15, there is a natural map (called unit map)\nand so \n\nThe Grothendieck spectral sequence then gives the first map and the last map (they are edge maps) in:\n\nCombining this with the above yields\n\nUsing the adjointness of formula_20 and formula_21 finally yields the desired map.\n\nThe above-mentioned introductory example is a special case of this, namely for the affine spectra formula_22 and, consequently, formula_23, and the quasi-coherent sheaf formula_24 associated to the \"B\"-module \"M\".\n\nIt is conceptually convenient to organize the above base change maps, which only involve only a single higher direct image functor, into one which encodes all formula_25 at a time. In fact, similar arguments as above yield a map in the derived category of sheaves on \"S':\"\n\nwhere formula_27 denotes the (total) derived functor of formula_13.\n\nIf \"X\" is a Hausdorff topological space, \"S\" is a locally compact Hausdorff space and \"f\" is universally closed (i.e., formula_29 is a closed map for any continuous map formula_30), then\nthe base change map\n\nis an isomorphism. Indeed, we have: for formula_32,\n\nand so for formula_34\n\nTo encode all individual higher derived functors of formula_13 into one entity, the above statement may equivalently be rephrased by saying that the base change map\n\nis a quasi-isomorphism.\n\nThe assumptions that the involved spaces be Hausdorff have been weakened by .\n\nIf the map \"f\" is not closed, the base change map need not be an isomorphism, as the following example shows (the maps are the standard inclusions) :\n\nOne the one hand formula_39 is always zero, but if formula_12 is a local system on formula_41 corresponding to a representation of the fundamental group formula_42 (which is isomorphic to Z), then formula_43 can be computed as the invariants of the monodromy action of formula_44 on the stalk formula_45 (for any formula_46), which need not vanish.\n\nTo obtain a base-change result, the functor formula_13 (or its derived functor) has to be replaced by the direct image with compact support formula_48. For example, if formula_49 is the inclusion of an open subset, such as in the above example, formula_50 is the extension by zero, i.e., its stalks are given by\n\nIn general, there is a map formula_52, which is a quasi-isomorphism if \"f\" is proper, but not in general. The proper base change theorem mentioned above has the following generalization: there is a quasi-isomorphism\n\n\"Proper base change theorems\" for quasi-coherent sheaves apply in the following situation: formula_49 is a proper morphism between noetherian schemes, and formula_3 is a coherent sheaf which is flat over \"S\" (i.e., formula_45 is flat over formula_57). In this situation, the following statements hold:\n\nAs the stalk of the sheaf formula_73 is closely related to the cohomology of the fiber of the point under \"f\", this statement is paraphrased by saying that \"cohomology commutes with base extension\".\n\nThese statements are proved using the following fact, where in addition to the above assumptions formula_74: there is a finite complex formula_75 of finitely generated projective \"A\"-modules and a natural isomorphism of functors\non the category of formula_77-algebras.\n\nThe base change map\nis an isomorphism for a quasi-coherent sheaf formula_12 (on formula_80), provided that the map formula_81 is \"flat\" (together with a number of technical conditions: \"f\" needs to be a separated morphism of finite type, the schemes involved need to be Noetherian).\n\nA far reaching extension of flat base change is possible when considering the base change map\nin the derived category of sheaves on \"S',\" similarly as mentioned above. Here formula_83 is the (total) derived functor of the pullback of formula_84-modules (because formula_85 involves a tensor product, formula_20 is not exact when is not flat and therefore is not equal to its derived functor formula_83).\nThis map is a quasi-isomorphism provided that the following conditions are satisfied:\n\nOne advantage of this formulation is that the flatness hypothesis has been weakened. However, making concrete computations of the cohomology of the left- and right-hand sides now requires the Grothendieck spectral sequence.\n\nDerived algebraic geometry provides a means to drop the flatness assumption, provided that the pullback formula_119 is replaced by the homotopy pullback. In the easiest case when \"X\", \"S\", and formula_95 are affine (with the notation as above), the homotopy pullback is given by the derived tensor product\nThen, assuming that the schemes (or, more generally, derived schemes) involved are quasi-compact and quasi-separated, the natural transformation\nis a quasi-isomorphism for any quasi-coherent sheaf, or more generally a complex of quasi-coherent sheaves.\nThe afore-mentioned flat base change result is in fact a special case since for \"g\" flat the homotopy pullback (which is locally given by a derived tensor product) agrees with the ordinary pullback (locally given by the underived tensor product), and since the pullback along the flat maps \"g\" and \"g' \"are automatically derived (i.e., formula_123). The auxiliary assumptions related to the Tor-independence or Tor-amplitude in the preceding base change theorem also become unnecessary.\n\nIn the above form, base change has been extended by to the situation where \"X\", \"S\", and \"S' \"are (possibly derived) stacks, provided that the map \"f\" is a perfect map (which includes the case that \"f\" is a quasi-compact, quasi-separated map of schemes, but also includes more general stacks, such as the classifying stack \"BG\" of an algebraic group in characteristic zero).\n\nProper base change also holds in the context of complex manifolds.\nThe theorem on formal functions is a variant of the proper base change, where the pullback is replaced by a completion operation.\n\nThe see-saw principle and the theorem of the cube, which are foundational facts in the theory of abelian varieties, are a consequence of proper base change.\n\nA base-change also holds for D-modules: if \"X\", \"S\", \"X',\" and \"S' \"are smooth varieties (but \"f\" and \"g\" need not be flat or proper etc.), there is a quasi-isomorphism\nwhere formula_125 and formula_126 denote the inverse and direct image functors for \"D\"-modules.\n\nFor étale torsion sheaves formula_12, there are two base change results referred to as \"proper\" and \"smooth base change\", respectively: base change holds if formula_128 is proper. It also holds if \"g\" is smooth, provided that \"f\" is quasi-compact and provided that the torsion of formula_12 is prime to the characteristic of the residue fields of \"X\".\n\nClosely related to proper base change is the following fact (the two theorems are usually proved simultaneously): let \"X\" be a variety over a separably closed field and formula_3 a constructible sheaf on formula_131. Then formula_132 are finite in each of the following cases:\n\nUnder additional assumptions, extended the proper base change theorem to non-torsion étale sheaves.\n\nIn close analogy to the topological situation mentioned above, the base change map for an open immersion \"f\", \nis not usually an isomorphism. Instead the extension by zero functor formula_135 satisfies an isomorphism\nThis fact and the proper base change suggest to define the \"direct image functor with compact support\" for a map \"f\" by\nwhere formula_138 is a \"compactification\" of \"f\", i.e., a factorization into an open immersion followed by a proper map.\nThe proper base change theorem is needed to show that this is well-defined, i.e., independent (up to isomorphism) of the choice of the compactification.\nMoreover, again in analogy to the case of sheaves on a topological space, a base change formula for formula_21 vs. formula_48 does hold for non-proper maps \"f\".\n\nFor the structural map formula_141 of a scheme over a field \"k\", the individual cohomologies of formula_142, denoted by formula_143 referred to as cohomology with compact support. It is an important variant of usual étale cohomology.\n\nSimilar ideas are also used to construct an analogue of the functor formula_48 in A-homotopy theory.\n\n\n\n"}
{"id": "53887749", "url": "https://en.wikipedia.org/wiki?curid=53887749", "title": "Biosurveillance", "text": "Biosurveillance\n\nBiosurveillance is an aspect of biodefense relating to the detection of bioterrorism threats.\n\nBioWatch is a US government program to detect biological agents present in the environment through a system of filters.\n"}
{"id": "44065971", "url": "https://en.wikipedia.org/wiki?curid=44065971", "title": "Blockchain", "text": "Blockchain\n\nA blockchain, originally block chain, is a growing list of records, called \"blocks\", which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash).\n\nBy design, a blockchain is resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain.\n\nBlockchain was invented by Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin. The invention of the blockchain for bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications, and blockchains which are readable by the public are widely used by cryptocurrencies. Blockchain is considered a type of payment rail. Private blockchains have been proposed for business use. Sources such as the \"Computerworld\" called the marketing of such blockchains without a proper security model \"snake oil\".\n\nThe first work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system where document timestamps could not be tampered with. In 1992, Bayer, Haber and Stornetta incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block.\n\nThe first blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to add blocks to the chain without requiring them to be signed by a trusted party. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.\n\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.\n\nThe words \"block\" and \"chain\" were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, \"blockchain,\" by 2016. The term \"blockchain 2.0\" refers to new applications of the distributed blockchain database, first emerging in 2014. \"The Economist\" described one implementation of this second-generation programmable blockchain as coming with \"a programming language that allows users to write more sophisticated smart contracts, thus creating invoices that pay themselves when a shipment arrives or share certificates which automatically send their owners dividends if profits reach a certain level.\"\n\n, blockchain 2.0 implementations continue to require an off-chain oracle to access any \"external data or events based on time or market conditions [that need] to interact with the blockchain.\"\n\nIBM opened a blockchain innovation research center in Singapore in July 2016. A working group for the World Economic Forum met in November 2016 to discuss the development of governance models related to blockchain.\n\nAccording to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.\n\nIn May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ‘planning or [looking at] active experimentation with blockchain’.\n\nIn November 2018, Conservative MEP Emma McClarkin’s plan to utilise blockchain technology to boost trade was backed by the European Parliament’s Trade Committee. \n\nA blockchain is a decentralized, distributed and public digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending. A blockchain has been described as a \"value-exchange protocol\". This blockchain-based exchange of value can be completed quicker, safer and cheaper than with traditional systems. A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.\n\nBlocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block.\n\nSometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher value can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, in a blockchain using the proof-of-work system, the chain with the most cumulative proof-of-work is always considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.\n\nThe \"block time\" is the average time it takes for the network to generate one extra block in the blockchain. Some blockchains create a new block as frequently as every five seconds. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is 10 minutes.\n\nBy storing data across its peer-to-peer network, the blockchain eliminates a number of risks that come with data being held centrally. The decentralized blockchain may use ad-hoc message passing and distributed networking.\n\nPeer-to-peer blockchain networks lack centralized points of vulnerability that computer crackers can exploit; likewise, it has no central point of failure. Blockchain security methods include the use of public-key cryptography. A \"public key\" (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A \"private key\" is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.\n\nEvery node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication and computational trust. No centralized \"official\" copy exists and no user is \"trusted\" more than any other. Transactions are broadcast to the network using software. Messages are delivered on a best-effort basis. Mining nodes validate transactions, add them to the block they are building, and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Alternative consensus methods include proof-of-stake. Growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.\n\nOpen blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases. Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain. Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision. Nikolai Hampton of \"Computerworld\" said that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\"\n\nThe great advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed. This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.\n\nBitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include a proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\".\n\nFinancial companies have not prioritised decentralized blockchains.\nIn 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. , bitcoin has the highest market capitalization.\n\nPermissioned blockchains use an access control layer to govern who has access to the network. In contrast to public blockchain networks, validators on private blockchain networks are vetted by the network owner. They do not rely on anonymous nodes to validate transactions nor do they benefit from the network effect. Permissioned blockchains can also go by the name of 'consortium' or 'hybrid' blockchains.\n\nThe \"New York Times\" noted in both 2016 and 2017 that many corporations are using blockchain networks \"with private blockchains, independent of the public system.\"\n\nNikolai Hampton pointed out in \"Computerworld\" that \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\" This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others, and \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power—it's time consuming and expensive.\" He also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\"\n\nBlockchain technology can be integrated into multiple areas. The primary use of blockchains today is as a distributed ledger for cryptocurrencies, most notably bitcoin. There are a few operational products maturing from proof of concept by late 2016.\n\n, some observers remain skeptical. Steve Wilson, of Constellation Research, believes the technology has been hyped with unrealistic claims. To mitigate risk, businesses are reluctant to place blockchain at the core of the business structure.\n\nMost cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are blockchain-based. On May 8, 2018 Facebook confirmed that it is opening a new blockchain group which will be headed by David Marcus who previously was in charge of Messenger. According to The Verge Facebook is planning to launch its own cryptocurrency for facilitating payments on the platform.\n\nBlockchain-based smart contracts are proposed contracts that could be partially or fully executed or enforced without human interaction. One of the main objectives of a smart contract is automated escrow. An IMF staff discussion reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general. But \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use their legal status is unclear.\n\nMajor portions of the financial industry are implementing distributed ledgers for use in banking, and according to a September 2016 IBM study, this is occurring faster than expected.\n\nBanks are interested in this technology because it has potential to speed up back office settlement systems.\n\nBanks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.\n\nBerenberg, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories.\n\nSome video games are based on blockchain technology. The first such game, \"Huntercoin\", was released in February, 2014. Another blockchain game is \"CryptoKitties\", launched in November 2017. The game made headlines in December 2017 when a cryptokitty character - an-in game virtual pet - was sold for US$100,000. \"CryptoKitties\" illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network with about 30% of all Ethereum transactions being for the game.\n\nCryptokitties also demonstrated how blockchains can be used to catalog game assets (digital assets).\n\nWithin the video game industry, while blockchain use is seen as part of a marketplace mechanism, such as with Robot Cache, blockchain is also postulated as a way to share video game assets between various games. The Blockchain Game Alliance was formed in September 2018 to explore alternative uses of blockchains in video gaming with support of Ubisoft and Fig, among others.\n\nBlockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users or musicians. In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution. Imogen Heap's Mycelia service has also been proposed as blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\" Everledger is one of the inaugural clients of IBM's blockchain-based tracking service.\n\nNew distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain. The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers. Online voting is another application of the blockchain.\n\nOther designs include:\n\nIn September 2018, IBM and a start-up Hu-manity.co launched a blockchain-based app that let patients sell anonymized data to pharmaceutical companies.\n\nCurrently, there are three types of blockchain networks - public blockchains, private blockchains and consortium blockchains.\n\nA public blockchain has absolutely no access restrictions. Anyone with an internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol). Usually, such networks offer economic incentives for those who secure them and utilize some type of a Proof of Stake or Proof of Work algorithm.\n\nSome of the largest, most known public blockchains are Bitcoin and Ethereum.\n\nA private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access is restricted.\n\nThis type of blockchains can be considered a middle-ground for companies that are interested in the blockchain technology in general but are not comfortable with a level of control offered by public networks. Typically, they seek to incorporate blockchain into their accounting and record-keeping procedures without sacrificing autonomy and running the risk of exposing sensitive data to the public internet.\n\nA consortium blockchain is often said to be semi-decentralized. It, too, is permissioned but instead of a single organization controlling it, a number of companies might each operate a node on such a network. The administrators of a consortium chain restrict users' reading rights as they see fit and only allow a limited set of trusted nodes to execute a consensus protocol.\n\nIn October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.\n\nThe Bank for International Settlements has criticized the public proof-of-work blockchains for high energy consumption.\n\nNicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley examines blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases finds it grossly inadequate.\n\nIn September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, \"Ledger\", was announced. The inaugural issue was published in December 2016. The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies such as bitcoin.\n\nThe journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address in the first page of their papers.\n\n\n\n"}
{"id": "22634639", "url": "https://en.wikipedia.org/wiki?curid=22634639", "title": "Burning of Cork", "text": "Burning of Cork\n\nThe Burning of Cork by British forces took place on the night of 11–12 December 1920, during the Irish War of Independence. It followed an Irish Republican Army (IRA) ambush of a British Auxiliary patrol in the city, which wounded twelve Auxiliaries, one fatally. In retaliation, the Auxiliaries, Black and Tans and British soldiers looted and burnt numerous buildings in the city centre. Many civilians reported being beaten, shot at, and robbed by British forces. Firefighters testified that British forces hindered their attempts to tackle the blazes through intimidation, cutting their hoses and shooting at them.\n\nMore than 40 business premises, 300 residential properties, the City Hall and Carnegie Library were destroyed by the fire. Over £3 million worth of damage (equivalent to €172 million today) was wrought, 2,000 were left jobless and many more became homeless. Two unarmed IRA volunteers were shot dead in the north of the city.\n\nBritish forces carried out other reprisals on Irish civilians during the war, but the burning of Cork was one of the most significant. The British government at first denied that its forces had started the fires, and blamed the IRA. Later, a British Army inquiry concluded that a company of Auxiliaries were responsible.\n\nThe Irish War of Independence began in 1919, following the declaration of an Irish Republic and its parliament, Dáil Éireann. The army of the new republic, the Irish Republican Army (IRA), waged guerrilla warfare against British forces: the British Army and the Royal Irish Constabulary (RIC). To help fight the IRA, the British Government formed the Auxiliary Division; a paramilitary unit composed of ex-soldiers from Britain which specialized in counter-insurgency. It also recruited thousands of British ex-soldiers into the RIC, who became known as \"Black and Tans\". IRA intelligence officer Florence O'Donoghue claimed the subsequent burning and looting of Cork was \"not an isolated incident, but rather the large-scale application of a policy initiated and approved, implicitly or explicitly, by the British government\".\nCounty Cork was an epicenter of the war. On 23 November 1920, a \"Black and Tan\" in civilian dress threw a grenade into a group of IRA volunteers who had just left a brigade meeting on St Patrick Street in Cork. Three IRA volunteers of the 1st Cork Brigade were killed: Paddy Trahey, Patrick Donohue and Seamus Mehigan. \"The New York Times\" reported that sixteen people were injured.\n\nOn 28 November 1920, the IRA's 3rd Cork Brigade ambushed an Auxiliary patrol at Kilmichael, killing 17 Auxiliaries. This was the biggest loss of life for the British in County Cork. On 10 December, British forces declared martial law in counties Cork (including the city), Kerry, Limerick, and Tipperary. It also imposed a military curfew on Cork city, which began at 10 each night. IRA volunteer Seán Healy later recalled that \"at least 1,000 troops would pour out of Victoria Barracks at this hour and take over complete control of the city\".\n\nIRA intelligence established that an Auxiliary patrol usually left Victoria Barracks (in the north of the city) each night at 8 and made its way to the city centre via Dillon's Cross. On 11 December, IRA commander Seán O'Donoghue received intelligence that two lorries of Auxiliaries would be leaving the barracks that night and travelling with them would be British Army Intelligence Corps Captain James Kelly. \n\nThat evening, a unit of six IRA volunteers commanded by O'Donoghue took up position between the barracks and Dillon's Cross. Their goal was to destroy the patrol and capture or kill Captain Kelly. Five of the volunteers hid behind a stone wall while one, Michael Kenny, stood across the road dressed as an off-duty British officer. When the lorries neared he was to beckon the driver of the first lorry to slow down or stop. The neighbourhood was close (within a \"couple of hundred yards\") of a military barracks.\n\nAt 8, two lorries each carrying 13 Auxiliaries emerged from the barracks. The first lorry slowed when the driver spotted Kenny and, as it did so, the IRA unit attacked with grenades and revolvers. The official British report said that 12 members of the Auxiliary Division were wounded and that one, Temporary Cadet Spencer Chapman, a former Officer in the 4th Battalion London Regiment (Royal Fusiliers), died from his wounds shortly after. As the IRA unit made its escape, some of the Auxiliaries managed to fire their rifles in the direction of the volunteers while others dragged the wounded to the nearest cover: O'Sullivan's pub.\n\nThe Auxiliaries broke into the pub with weapons drawn. They ordered everyone to put their hands over their heads to be searched. Backup and an ambulance were sent from the nearby barracks. One witness described a number of young men being rounded-up and forced to lie on the ground. The Auxiliaries dragged one of them to the middle of the crossroads, stripped him naked and forced him to sing \"God Save the King\" until he collapsed on the road.\n\nAngered by an attack so near their headquarters and seeking retribution for the deaths of their colleagues at Kilmichael, the Auxiliaries gathered to wreak their revenge. Charles Schulze, a member of the Auxiliaries and a former British Army Captain in the Dorsetshire Regiment during World War I, organized a group of Auxiliaries to burn the centre of Cork.\n\nAt 9:30, lorries of Auxiliaries and British soldiers left the barracks and alighted at Dillon's Cross, where they broke into a number of houses and herded the occupants on to the street. They then set the houses on fire and stood guard as they were razed to the ground. Those who tried to intervene were fired upon and some were badly beaten. Seven buildings were set alight at the crossroads. When one was found to be owned by Protestants, the Auxiliaries quickly doused the fire.\n\nAt about the same time, a group of armed and uniformed Auxiliaries surrounded a tram at Summerhill, smashed its windows, and forced all the passengers out. A number of the passengers (including at least three women) were repeatedly kicked, hit with rifle butts, threatened, and verbally abused. The Auxiliaries then forced the passengers to line-up against a wall and searched them, while continuing the physical and verbal abuse. Some had their money and belongings stolen. Another tram was set alight near Father Mathew's statue. Meanwhile, witnesses reported seeing a group of 14–18 Black and Tans firing wildly for upwards of 20 minutes on nearby MacCurtain Street.\n\nSoon after, witnesses reported groups of armed men on St Patrick's Street, the city's main shopping area. Some were uniformed or partially uniformed members of the Auxiliaries and British Army while others wore no uniforms. They were seen firing into the air, smashing shop windows and setting buildings alight. Many reported hearing bombs exploding. A group of Auxiliaries were seen throwing a bomb into the ground floor of the Munster Arcade, which housed both shops and flats. It exploded under the residential quarters while people were inside the building. They managed to escape unharmed but were detained by the Auxiliaries.\nThe fire brigade was informed of the fire at Dillon's Cross shortly before 10 pm and were sent to deal with it at once. However, on finding that Grant's department store on St Patrick's Street was ablaze, they decided to tackle it first. The fire brigade's Superintendent, Alfred Hutson, called Victoria Barracks and asked them to tackle the fire at Dillon's Cross so that he could focus on the city centre. However, the barracks took no heed of his request. As he did not have enough resources to deal with all the fires at once, \"he would have to make choices – some fires he would fight, others he could not\".\n\nHutson oversaw the operation on St Patrick's Street, and met \"Cork Examiner\" reporter Alan Ellis. He told Ellis \"that all the fires were being deliberately started by incendiary bombs, and in several cases he had seen soldiers pouring cans of petrol into buildings and setting them alight\". \n\nA number of firemen later testified that British forces hindered their attempts to tackle the blazes by intimidating them, cutting their hoses and/or driving lorries over the hoses. They also said that firemen were shot at and that at least two were wounded by gunfire. Shortly after 3, \"Cork Examiner\" reporter Alan Ellis came upon a unit of the fire brigade pinned down by gunfire near City Hall. The firemen said that they were being shot at by Black and Tans who had broken into the building. They also claimed to have seen uniformed men carrying cans of petrol into the building from nearby Union Quay barracks. \n\nAt about 4 a large explosion was heard and City Hall and the neighbouring Carnegie Library went up in flames, resulting in the loss of many of the city's public records. When more firefighters arrived, British forces fired at them and refused them access to water. The last act of arson took place at about 6 when a group of policemen looted and burnt the Murphy Brothers' clothes shop on Washington Street.\n\nAfter the ambush at Dillon's Cross, IRA commander Seán O'Donoghue and volunteer James O'Mahony made their way to the Delaney farmhouse at Dublin Hill in the north of the city. Brothers Cornelius and Jeremiah Delaney were members of F Company, 1st Battalion, 1st Cork Brigade IRA.\n\nO'Donoghue hid a number of unused grenades on the farm and the two men went their separate ways. At about 2 at least eight armed men entered the house and went upstairs into the brothers' bedroom. The brothers got up and stood at the bedside and were asked their names. When they answered, the gunmen opened fire. Both brothers were shot dead and their elderly relative, William Dunlea, was wounded by gunfire. According to Daniel Delaney, the father of the brothers, the gunmen wore long overcoats and spoke with English accents. It is thought that, while searching the site of the ambush, the Auxiliaries had found a cap belonging to one of the volunteers and had used bloodhounds to follow the scent to the Delaney home.\n\nOver 40 business premises and 300 residential properties had been destroyed. This amounted to over five acres of the city. Over £3 million worth of damage (1920 value) was suffered, although the value of property looted by British forces was not clear. Many people became homeless and 2,000 were left jobless. Fatalities included an Auxiliary killed by the IRA, two IRA volunteers killed by Auxiliaries, and a woman who died from a heart-attack when Auxiliaries burst into her house. A number of people, including firefighters, had reportedly been assaulted or otherwise wounded.\n\nFlorence O'Donoghue, intelligence officer of the 1st Cork Brigade IRA at the time, described the scene in Cork on the morning of the 12th:\"Many familiar landmarks were gone forever – where whole buildings had collapsed here and there a solitary wall leaned at some crazy angle from its foundation. The streets ran with sooty water, the footpaths were strewn with broken glass and debris, ruins smoked and smoldered and over everything was the all-pervasive smell of burning.\"\n\nAt midday mass in the North Cathedral the Bishop of Cork, Daniel Cohalan, condemned the arson but said the burning of the city was a result of the \"murderous ambush at Dillon's Cross\" and vowed, \"I will certainly issue a decree of excommunication against anyone who, after this notice, shall take part in an ambush or a kidnapping or attempted murder or arson\". No excommunications, however, were issued, and the bishop's edict was largely ignored by pro-IRA priests and chaplains.\n\nA meeting of Cork Corporation was held that afternoon at the Corn Exchange. Councillor J.J. Walsh condemned the bishop for his comments, which he claimed held the Irish people up as the \"evil-doers\". Walsh said that while the people of Cork had been suffering, \"not a single word of protest was uttered [by the bishop], and today, after the city has been decimated, he saw no better course than to add insult to injury\". Councillor Michael Ó Cuill, alderman Tadhg Barry and the Lord Mayor, Daniel O'Callaghan, agreed with Walsh's sentiments. The members resolved that the Lord Mayor should send a telegram asking for the intervention of the European governments and the USA.\n\nThree days after the fire, on 15 December, two lorry-loads of Auxiliaries were travelling from Dunmanway to Cork for the funeral of Spencer Chapman, a comrade killed at Dillon's Cross. When they met two men (an elderly priest, Canon Magner, walking with a parishioner, a farmer's son) helping a resident magistrate fix his car, an Auxiliary, Vernon Anwyl Hart, got out and began questioning them, then shot Magner and his companion, Timothy (or Tadhg) Crowley. Both died of their injuries. A military court of inquiry heard that he had been a friend of Chapman and had been \"drinking steadily\" since his death. Hart was found guilty of murder, but insane. At a subsequent investigation, one of the reasons given for his murder was that he refused to have the parish church bells tolled after the Kilmichael ambush, in which 17 Auxiliaries were killed.\n\nIrish nationalists called for an open and impartial inquiry. In the British House of Commons, Sir Hamar Greenwood, the Chief Secretary for Ireland, refused demands for such an inquiry. He denied that British forces had any involvement and accused the IRA of starting the fires. When asked about reports of firefighters being hampered by British forces he said \"Every available policeman and soldier in Cork was turned out at once and without their assistance the fire brigade could not have gone through the crowds and did the work that they tried to do\".\n\nBonar Law said \"in the present condition of Ireland, we are much more likely to get an impartial inquiry in a military court than in any other\". The British military launched its own inquiry, the \"Strickland Report\", but Cork Corporation instructed its employees and other corporate officials to take no part. The report pointed the finger of blame at members of the Auxiliaries' K Company, based at Victoria Barracks. The Auxiliaries, it was claimed, set the fires in reprisal for the IRA attack at Dillon's Cross. However, the British Government refused to publish the report.\n\nThe Irish Labour Party and Trades Union Congress published a pamphlet in January 1921 entitled \"Who burned Cork City?\" The work drew on evidence from hundreds of eyewitness which suggested that the fires had been set by British forces and that British forces had prevented firefighters from tackling the blaze. The material was compiled by the President of University College Cork, Alfred O'Rahilly.\n\nK Company Auxiliary Charles Schulze, a former British Army Captain, wrote in a letter to his girlfriend in England that it was \"sweet revenge\", while in a letter to his mother he wrote: \"Many who had witnessed scenes in France and Flanders say that nothing they had experienced was comparable with the punishment meted out in Cork\". After the fire, K Company was moved to Dunmanway and began wearing burnt corks in their caps in reference to the burning of the city. For their part in the arson and looting, K Company was disbanded on 31 March 1921.\n\nThere has been debate over whether British forces at Victoria Barracks had planned to burn the city before the ambush at Dillon's Cross, whether the British Army itself was involved, and whether those who set the fires were being commanded by superior officers. Florence O'Donoghue, who was intelligence officer of the 1st Cork Brigade IRA at the time, wrote:\"What appears more probable is that the ambush provided the excuse for an act which was long premeditated and for which all arrangements had been made. The rapidity with which supplies of petrol and Verey lights were brought from Cork barracks to the centre of the city, and the deliberate manner in which the work of firing the various premises was divided amongst groups under the control of officers, gives evidence of organisation and pre-arrangement. Moreover, the selection of certain premises for destruction and the attempt made by an Auxiliary officer to prevent the looting of one shop by Black and Tans: 'You are in the wrong shop; that man is a Loyalist' and the reply, 'We don't give a damn; this is the shop that was pointed out to us', is additional proof that the matter had been carefully planned beforehand.\"\n\n\n"}
{"id": "7830", "url": "https://en.wikipedia.org/wiki?curid=7830", "title": "Convention (norm)", "text": "Convention (norm)\n\nA convention is a set of agreed, stipulated, or generally accepted standards, norms, social norms, or criteria, often taking the form of a custom.\n\nCertain types of rules or customs may become law and regulatory legislation may be introduced to formalize or enforce the convention (for example, laws that define on which side of the road vehicles must be driven). In a social context, a convention may retain the character of an \"unwritten law\" of custom (for example, the manner in which people greet each other, such as by shaking each other's hands).\n\nIn physical sciences, numerical values (such as constants, quantities, or scales of measurement) are called conventional if they do not represent a measured property of nature, but originate in a convention, for example an average of many measurements, agreed between the scientists working with these values.\n\nA convention is a selection from among two or more alternatives, where the rule or alternative is agreed upon among participants. Often the word refers to unwritten customs shared throughout a community. For instance, it is conventional in many societies that strangers being introduced shake hands. Some conventions are explicitly legislated; for example, it is conventional in the United States and in Germany that motorists drive on the right side of the road, whereas in New Zealand and the United Kingdom motorists drive on the left. The standardization of time is a human convention based on the solar cycle or calendar. The extent to which justice is conventional (as opposed to natural or objective) is historically an important debate among philosophers.\n\nThe nature of conventions has raised long-lasting philosophical discussion. Quine, Davidson, and David Lewis published influential writings on the subject. Lewis's account of convention received an extended critique in Margaret Gilbert's \"On Social Facts\" (1989), where an alternative account is offered. Another view of convention comes from Ruth Millikan's \"Language: A Biological Model\" (2005), once more against Lewis.\n\nAccording to David Kalupahana, The Buddha described conventions—whether linguistic, social, political, moral, ethical, or even religious—as arising dependent on specific conditions. According to his paradigm, when conventions are considered absolute realities, they contribute to dogmatism, which in turn leads to conflict. This does not mean that conventions should be absolutely ignored as unreal and therefore useless. Instead, according to Buddhist thought, a wise person adopts a middle way without holding conventions to be ultimate or ignoring them when they are fruitful.\n\nIn sociology a \"social rule\" refers to any social convention commonly adhered to in a society. These \"rules\" are not written in law or otherwise formalized. In social constructionism there is a great focus on social rules. It is argued that these rules are socially constructed, that these rules act upon every member of a society, but at the same time, are re-produced by the individuals.\n\nSociologists representing symbolic interactionism argue that social rules are created through the interaction between the members of a society. The focus on active interaction highlights the fluid, shifting character of social rules. These are specific to the social context, a context that varies through time and place. That means a social rule changes over time within the same society. What was acceptable in the past may no longer be the case. Similarly, rules differ across space: what is acceptable in one society may not be so in another.\n\nSocial rules reflect what is \"acceptable\" or \"normal\" behaviour in any situation. Michel Foucault's concept of discourse is closely related to social rules as it offers a possible explanation how these rules are shaped and change. It is the social rules that tell people what is \"normal\" behaviour for any specific category. Thus, social rules tell a woman how to behave in a womanly manner, and a man, how to be manly. Other such rules are as follows:\n\n\nIn government, convention is a set of unwritten rules that participants in the government must follow. These rules can be ignored only if justification is clear, or can be provided. Otherwise, consequences follow. Consequences may include ignoring some other convention that has until now been followed. According to the traditional doctrine (Dicey), conventions cannot be enforced in courts, because they are non-legal sets of rules. Convention is particularly important in the Westminster System of government, where many of the rules are unwritten.\n\nThe term \"convention\" is also used in international law to refer to certain formal statements of principle such as the Convention on the Rights of the Child. Conventions are adopted by international bodies such as the International Labour Organization and the United Nations. Conventions so adopted usually apply only to countries that ratify them, and do not automatically apply to member states of such bodies. These conventions are generally seen as having the force of international treaties for the ratifying countries. The best known of these are perhaps the several Geneva Conventions.\n\n\n"}
{"id": "29567568", "url": "https://en.wikipedia.org/wiki?curid=29567568", "title": "Delusions of Gender", "text": "Delusions of Gender\n\nDelusions of Gender: How Our Minds, Society, and Neurosexism Create Difference is a 2010 book by Cordelia Fine, written to debunk the idea that men and women are hardwired with different interests. The author criticizes claimed evidence of the existence of innate biological differences between men and women's minds as being faulty and exaggerated, and while taking a position of agnosticism with respect to inherent differences relating to interest/skill in 'understanding the world' versus 'understanding people', reviews literature demonstrating how cultural and societal beliefs contribute to sex differences.\n\nIn the first part of the book, \"'Half Changed World', Half Changed Minds\", Fine argues that social and environmental factors strongly influence the mind, challenging a 'biology as fallback' view that, since society is equal now for the sexes, persistent inequalities must be due to biology. She also discusses the history and impact of gender stereotypes and the ways that science has been used to justify sexism.\n\nIn the second part of the book, \"Neurosexism\", Fine criticizes the current available arguments and studies supporting sex differences in the mind, focusing on methodological weaknesses and implicit assumptions. Within neuroscientific investigations, these include small samples that give rise to unreliable, spurious results, and poorly justified 'reverse inferences' (claims of stereotype-consistent psychological differences between the sexes on the basis of brain differences). Fine also demonstrates how already weak neuroscientific conclusions are then grossly overblown by popular writers. Fine also discusses non-neuroimaging evidence cited as support for innate differences between the sexes. For example, she explains weaknesses in the work done by a student of Simon Baron-Cohen that has been widely cited (by the Gurian Institute, by Leonard Sax, by Peter Lawrence, and by Baron-Cohen himself): one-and-a-half-day-old babies were tested for preference in sequence rather than being given a choice; were tested in different viewing positions, some horizontal on their backs and some held in a parent's lap, which could affect their perception; inadequate efforts were made to ensure the sex of the subject was unknown to the tester at the time of the test; the authors assume, without justification, that newborn looking preferences are a reliable 'flag' for later social skills that are the product of a long and complex developmental process.\n\nIn the third part of the book, \"Recycling Gender\", Fine discusses the highly gendered society in which children develop, and the contribution of that to the group identity processes that motivate children to 'self-socialize'. This challenges the common belief of parents that they tried gender-neutral parenting, but it didn't work. An overall thesis of the work is the negative impact for sex equality of neurosexism (popular or academic neuroscientific claims that reinforce or justify gender stereotypes in ways that are not scientifically justified).\n\nIn the UK, the book received positive reviews in \"Nature\", \"The Independent\", \"The Times Literary Supplement\", \"New Scientist\", \"Metro\" and \"The Belfast Telegraph\". \"The Guardian\" and \"London Evening Standard\" each chose it as a Book of the Year. It was Book of the Week in \"Times Higher Education\".\n\nIn Australia, the book received positive reviews in \"The Age\", \"The Australian\" and \"The West Australian\".\n\n\"Delusions of Gender\" received positive reviews in the United States in \"The New York Times\", \"The Washington Post\", \"USA Today\", \"Newsweek\", \"Jezebel\" and \"Kirkus Reviews\". \"Publishers Weekly\" chose it for a starred review and as a Pick of the Week.\n\nMore positive reviews came from \"Frankfurter Allgemeine Zeitung\", \"The Globe and Mail\", \"Socialist Worker\", \"Out in Perth\", \"The Fat Quarter\", \"Erotic Review\", \"The F Word\", \"Counterfire\", \"Neuroskeptic\" (at \"Discover\" magazine). \"Ms.\" magazine and \"Elle\" singled the book out for their readers.\n\n\nSimon Baron-Cohen reviewed the book in \"The Psychologist\". In it, he accused Fine of \"fusing science with politics,\" writing, \"Where I – and I suspect many other contemporary scientists – would part ways with Fine is in her strident, extreme denial of the role that biology might play in giving rise to any sex differences in the mind and brain. ...(she) ignores that you can be a scientist interested in the nature of sex differences while being a clear supporter of equal opportunities and a firm opponent of all forms of discrimination in society.\" Fine responded in a published letter to \"The Psychologist,\" stating \"The thesis of my book (no veils required) is that while social effects on sex differences are well-established, spurious results, poor methodologies and untested assumptions mean we don’t yet know whether, on average, males and females are born differently predisposed to systemizing versus empathising.\"\n\nFormer APA President Diane F. Halpern, co-author of the article \"The Science of Sex Differences in Mathematics and Science\" that Fine criticizes in \"Delusions of Gender,\" reviewed the book and concluded that it was \"strongest in exposing research conclusions that are closer to fiction than science...[but] weakest in failing to also point out differences that are supported by a body of carefully conducted and well-replicated research\", noting that Fine largely ignores the latter body of research.\n\nStanford neurobiologist Ben Barres stated in a review for the Public Library of Science Biology that Delusions of Gender \"should be required reading for every neurobiology student, if not every human being.\"\n\nLewis Wolpert, a developmental biologist who is the author of \"Why can't a woman be more like a man?\", in a video lecture stated that Fine \"[hasn't] got a clue about biology\". However, he offered no specific criticism of Fine's claims.\n\nMcCarthy and Ball (2011) reviewed the book in the journal \"Biology of Sex Differences.\" They acknowledged that \"Prompting laypeople to adopt a more critical view of overly simplistic views of complex data sets is a goal any scientist can support, and for that we applaud (Fine's) efforts.\" However, their overall review is not positive, and they suggest that Fine's book presents an oversimplified and seriously distorted characterization of neuroscience as applied to the study of sex differences. They expressed disappointment that Fine's book \"...can be vexing in the ways the scientific study of sex differences in brain and behavior is portrayed and (how) the current state-of-the-art is presented.\" However, later work by Fine published in the journal \"Neuroethics\" identified systematic issues in the way human neuroimaging investigations of sex differences tend to be investigated, contra the notion of a few 'bad apples'. Many of the criticisms of such work made in \"Delusions of Gender\" were noted in the article \"Perils and pitfalls of reporting sex differences\" by Donna L. Maney, as part of a \"Philosophical Transactions of the Royal Society\" Theme Issue \"Multifaceted origins of sex differences in the brain', compiled and edited by McCarthy in 2016.\n\nEvolutionary biologist Marlene Zuk, reviewing the book with Rebecca Jordan-Young's \"Brain Storm\", in the \"Quarterly Review of Biology\" wrote: \"It is important to emphasize that neither author advocates throwing the gender-neutral baby out with its pink or blue bathwater ... The books are good ammunition for arguments with people who think science has incontrovertibly shown biological bases for gender differences such as mathematical ability. At the same time, they are not simply claiming that “it is all culture” or that science can play no role in understanding gender. Both Fine and Jordan-Young want better science, not less of it.\"\n\n\n"}
{"id": "43017519", "url": "https://en.wikipedia.org/wiki?curid=43017519", "title": "Direct clustering algorithm", "text": "Direct clustering algorithm\n\nDirect clustering algorithm (DCA) is a methodology for identification of cellular manufacturing structure within an existing manufacturing shop. The DCA was introduced in 1982 by H.M. Chan and D.A. Milner The algorithm restructures the existing machine / component (product) matrix of a shop by switching the rows and columns in such a way that a resulting matrix shows component families (groups) with corresponding machine groups. See Group technology. The algorithm is executable in manual way but was already suitable for computer use of the time.\nThe cellular manufacturing structure consists of several machine groups (production cells) where corresponding product groups (products with similar technology) are being exclusively manufactured. In aim of identification of possible cellular manufacturing structure within an existing manufacturing shop the DCA methodology roughly provides following procedure:\nThe DCA methodology would give a perfect result in an ideal case where there are no overlapping machines or products between the groups. The overlapping in most real cases represents further challenge for the methodology users. The \"Formation of Machine Cells/ Part Families in Cellular Manufacturing Systems Using an ART-Modified Single Linkage Clustering Approach – A Comparative Study\" by M. Murugan and V. Selladurai shows the comparison of DCA to some other methodologies of the same purpose.\n"}
{"id": "26703329", "url": "https://en.wikipedia.org/wiki?curid=26703329", "title": "Figurative analogy", "text": "Figurative analogy\n\nA figurative analogy is a comparison about two things that are not alike but share only some common property. On the other hand, a literal analogy is about two things that are nearly exactly alike.\n\nThe two things compared in a figurative analogy are not obviously comparable in most respects. Metaphors and similes are two types of figurative analogies.\n\nIn the course of analogical reasoning, figurative analogies become weak if the disanalogies of the entities being compared are relevant—in the same way that literal analogies become weak. Consider the disanalogies involving two cars in a literal analogy (the same principle concerning disanalogies is true for a figurative analogy): The day they were purchased isn't relevant whereas the previous accidents of the two cars would be relevant. If car A has been in five accidents while car B has been in no accidents, then the conclusion drawn about the future performance of the two cars is affected.\n\n"}
{"id": "4030560", "url": "https://en.wikipedia.org/wiki?curid=4030560", "title": "Flag protocol", "text": "Flag protocol\n\nA flag protocol defines the proper placement, handling, use, and disposal of flags. Some countries have added certain protocols into their legal system while others prefer to have \"guidelines\" without civil or criminal consequences attached.\nGeneral guidelines are accepted practically universally.\n\nThe flag of honor, which is the nation's flag in most cases, is flown on the center mast if possible. It is also correct to fly the flag on its own right. To an observer it would be on the far left. If more than three flags are used, the proper position is as far left from the point of view of an observer. An additional flag may be placed on the right side, but is not necessary.\n\nWhen two poles are crossed, the position of honor is the flag that ends on the left side from the point of view of an observer (the pole will therefore end on the right).\n\nIn a semicircle, the position of honor is the center. If a full circle is used outside an entrance to an arena or stadium, the position of honor is directly over the entrance. If used to line the walls of the arena, the flag should be placed directly opposite the entrance.\n\nWhen flown horizontally, as from a flag pole, the flag should be oriented so that the canton is closest to the top of the pole. If hung against a wall, the canton should be placed in the upper-left corner from the point of view of the observer.\n\nWhen hung vertically, flags should be rotated so the canton is again closest to the top of the pole. If the flag is displayed against a wall, the canton should again appear in the upper-left corner, which requires that the flag be both rotated and \"flipped\" from its horizontal orientation.\n\nOn a vehicle the flag should be put on a window or affixed securely to the front of the chassis, on the nearside of the vehicle, i.e. the one opposite the driver. (In other words, in countries that drive on the right hand side of the road, a flag is on the right of the vehicle.) On a vehicle where a visiting Head of State or Government is sharing a car with the host Head of State or Government, the host's flag takes the nearside position, the guest's flag on the offside.\n\nWhen placed with a podium or at a place of worship, the flag should hang directly behind or on a pole to the right of the speaker, from the point of view of the flag.\n\nWhen carried in single file, the flag of honor leads.\n\nWhen flags of many nations are flown the flag of the hosting country should be placed on the right with the rest following in alphabetical order in the language of the host country. The flag of any sovereign nation should not be displayed over that of another, except when the nations are at war with each other.\n\nSometimes in a ceremonial flypast, a flag is flown from a weighted rope dangling from beneath a helicopter.\n\nMembers of the royal family and the nobility each have their own flags. The Standard of the Sultan must be flown only over Istana Nurul Iman. The same goes for the Crown Prince (Deputy Sultan)'s flag, but other non-royal title holders fly the national flag. As in many other countries, Bruneians consider it taboo for the flag to touch the ground.\n\nWhen a French vessel meets another French ship, it is to lower and raise its ensign as a greeting. A merchant ship meeting a ship of the French Navy will greet three times.\n\nThe flag of India has a very distinctive protocol and is governed by the Flag Code of India, 2002 the Emblems and Names (Prevention of Improper Use) Act, 1950; and the Prevention of Insults to National Honour Act, 1971.\n\nInsults to the national flag, are punishable by law with imprisonment up to three years, or a fine, or both.\n\nOfficial regulation states that the flag must never touch the ground or water, or be used as a drapery in any form.\n\nDisposal of damaged flags is also covered by the flag code. Damaged or soiled flags may not be cast aside or disrespectfully destroyed; they have to be destroyed as a whole in private, preferably by burning or by any other method consistent with the dignity of the flag.\n\nBecause the flag of Saudi Arabia bears the Shahada, it is never flown at half-mast.\n\nThe Department for Communities and Local Government in November 2012 released the \"Plain English guide to flying flags\" for England, a \"summary of the new, more liberalised, controls over flag flying that were introduced on 12 October 2012\". In England, the statute governing the flying of flags are The Town and Country Planning (Control of Advertisements) (England) (Amendment) Regulations 2007 and 2012.\n\nWhen displayed either horizontally or vertically against a wall, the union should be uppermost and to the flag's own right, that is, to the observer's left. When displayed in a window, the flag should be displayed in the same way, with the union or blue field to the left of the observer in the street.\n\nThe flag should be to the speaker's right (also described as the flag's own right or audience's left), that is to the left of the podium or pulpit as the speaker is facing the audience. Old guidelines had a distinction whether the flag was at the level of the speaker on a stage or the level of the audience. That distinction has been eliminated and the rule simplified.\n\nWhen the flag is displayed at half-staff, it is customary to raise it briskly to the top the flag pole, then lower it slowly to the half-way mark. This is also done when lowering the flag. The flag is only displayed at half-staff by presidential decree or act of Congress, except on two days: On Pearl Harbor Remembrance Day, the flag can be displayed at half-staff until sundown.; on Memorial Day, the flag is flown at half-staff until noon, and then raised to full staff for the remainder of the day.\n\nWhen displaying the US flag, it is customary for it to be above a state's flag when flown on the same pole. When flown separately, a state's flag may be at the same height as the US flag, with the US flag to the left of the state flag, from the perspective of the viewer. When flown with several state flags, the US flag should be at the same height and to the flag's own right (viewer's left), or at the center of and higher than a grouping of state flags. The idea that only the Texas and Hawaii flags—having been the national flags of the Republic of Texas and the Kingdom of Hawaii—may be flown at an equal height to the US flag is a legend. In fact, \"any\" other flag may be flown at an equal height to the US flag provided the US flag is at the leftmost staff from the perspective of the viewer.\n\nThe flag of the United States is used to drape the coffins of deceased veterans of the armed forces. When it is so used, the Union (white stars on blue background) is placed above the deceased's left shoulder.\n\nAccording to United States Code found in Title 4, Chapter 1 pertaining to patriotic customs and observances:\nThese laws were supplemented by executive orders and presidential proclamations.\n\nNational flags cannot be adulterated on any way, nor be used with other intention than as national symbols as stated by law. It is also prohibited for buildings to raise flags other than national flags. The public loyalty oath to the flag must be taken once by every citizen and is celebrated on 19 June at learning institutes. Disposal of damaged flags is done by the Uruguayan Army. Each year on 24 September damaged flags are burnt as an official act.\n\n\n"}
{"id": "1928465", "url": "https://en.wikipedia.org/wiki?curid=1928465", "title": "Flavour (particle physics)", "text": "Flavour (particle physics)\n\nIn particle physics, flavour or flavor refers to the \"species\" of an elementary particle. The Standard Model counts six flavours of quarks and six flavours of leptons. They are conventionally parameterized with \"flavour quantum numbers\" that are assigned to all subatomic particles. They can also be described by some of the family symmetries proposed for the quark-lepton generations.\n\nIn classical mechanics, a force acting on a point-like particle can only alter the particle's dynamical state, i.e., its momentum, angular momentum, etc. Quantum field theory, however, allows interactions that can alter other facets of a particle's nature described by non dynamical, discrete quantum numbers. In particular, the action of the weak force is such that it allows the conversion of quantum numbers describing mass and electric charge of both quarks and leptons from one discrete type to another. This is known as a flavour change, or flavour transmutation. Due to their quantum description, flavour states may also undergo quantum superposition.\n\nIn atomic physics the principal quantum number of an electron specifies the electron shell in which it resides, which determines the energy level of the whole atom. Analogously, the five flavour quantum numbers (isospin, strangeness, charm, bottomness or topness) can characterize the quantum state of quarks, by the degree to which it exhibits six distinct flavours (u, d, s, c, b, t).\n\nComposite particles can be created from multiple quarks, forming hadrons, such as mesons and baryons, each possessing unique aggregate characteristics, such as different masses, electric charges, and decay modes. A hadron's overall flavour quantum numbers depend on the numbers of constituent quarks of each particular flavour.\n\nAll of the various charges discussed above are conserved by the fact that the corresponding charge operators can be understood as \"generators of symmetries\" that commute with the Hamiltonian. Thus, the eigenvalues of the various charge operators are conserved.\n\nAbsolutely conserved flavour quantum numbers are:\n\nIn some theories, the individual baryon and lepton number conservation can be violated, if the difference between them () is conserved (see chiral anomaly). All other flavour quantum numbers are violated by the electroweak interactions. Strong interactions conserve all flavours.\n\nIf there are two or more particles which have identical interactions, then they may be interchanged without affecting the physics. Any (complex) linear combination of these two particles give the same physics, as long as the combinations are orthogonal, or perpendicular, to each other.\n\nIn other words, the theory possesses symmetry transformations such as formula_1, where and are the two fields (representing the various \"generations\" of leptons and quarks, see below), and is any unitary matrix with a unit determinant. Such matrices form a Lie group called SU(2) (see special unitary group). This is an example of flavour symmetry.\n\nIn quantum chromodynamics, flavour is a conserved global symmetry. In the electroweak theory, on the other hand, this symmetry is broken, and flavour changing processes exist, such as quark decay or neutrino oscillations.\n\nAll leptons carry a lepton number . In addition, leptons carry weak isospin, , which is − for the three charged leptons (i.e. electron, muon and tau) and + for the three associated neutrinos. Each doublet of a charged lepton and a neutrino consisting of opposite are said to constitute one generation of leptons. In addition, one defines a quantum number called weak hypercharge, , which is −1 for all left-handed leptons. Weak isospin and weak hypercharge are gauged in the Standard Model.\n\nLeptons may be assigned the six flavour quantum numbers: electron number, muon number, tau number, and corresponding numbers for the neutrinos. These are conserved in strong and electromagnetic interactions, but violated by weak interactions. Therefore, such flavour quantum numbers are not of great use. A separate quantum number for each generation is more useful: electronic lepton number (+1 for electrons and electron neutrinos), muonic lepton number (+1 for muons and muon neutrinos), and tauonic lepton number (+1 for tau leptons and tau neutrinos). However, even these numbers are not absolutely conserved, as neutrinos of different generations can mix; that is, a neutrino of one flavour can transform into another flavour. The strength of such mixings is specified by a matrix called the Pontecorvo–Maki–Nakagawa–Sakata matrix (PMNS matrix).\n\nAll quarks carry a baryon number . They also all carry weak isospin, . The positive- quarks (up, charm, and top quarks) are called \"up-type quarks\" and negative- quarks (down, strange, and bottom quarks) are called \"down-type quarks\". Each doublet of up and down type quarks constitutes one generation of quarks.\n\nFor all the quark flavour quantum numbers listed below, the convention is that the flavour charge and the electric charge of a quark have the same sign. Thus any flavour carried by a charged meson has the same sign as its charge. Quarks have the following flavour quantum numbers:\n\nThese five quantum numbers, together with baryon number (which is not a flavour quantum number), completely specify numbers of all 6 quark flavours separately (as , i.e. an antiquark is counted with the minus sign). They are conserved by both the electromagnetic and strong interactions (but not the weak interaction). From them can be built the derived quantum numbers:\n\nThe terms \"strange\" and \"strangeness\" predate the discovery of the quark, but continued to be used after its discovery for the sake of continuity (i.e. the strangeness of each type of hadron remained the same); strangeness of anti-particles being referred to as +1, and particles as −1 as per the original definition. Strangeness was introduced to explain the rate of decay of newly discovered particles, such as the kaon, and was used in the Eightfold Way classification of hadrons and in subsequent quark models. These quantum numbers are preserved under strong and electromagnetic interactions, but not under weak interactions.\n\nFor first-order weak decays, that is processes involving only one quark decay, these quantum numbers (e.g. charm) can only vary by 1, that is, for a decay involving a charmed quark or antiquark either as the incident particle or as a decay byproduct, ; likewise, for a decay involving a bottom quark or antiquark . Since first-order processes are more common than second-order processes (involving two quark decays), this can be used as an approximate \"selection rule\" for weak decays.\n\nA special mixture of quark flavours is an eigenstate of the weak interaction part of the Hamiltonian, so will interact in a particularly simple way with the W bosons. (Charged weak interactions violate flavor). On the other hand, a fermion of a fixed mass (an eigenstate of the kinetic and strong interaction parts of the Hamiltonian) is an eigenstate of flavour. The transformation from the former basis to the flavor-eigenstate/mass-eigenstate basis for quarks underlies the Cabibbo–Kobayashi–Maskawa matrix (CKM matrix). This matrix is analogous to the PMNS matrix for neutrinos, and quantifies flavour changes under charged weak interactions of quarks.\n\nThe CKM matrix allows for CP violation if there are at least three generations.\n\nFlavour quantum numbers are additive. Hence antiparticles have flavour equal in magnitude to the particle but opposite in sign. Hadrons inherit their flavour quantum number from their valence quarks: this is the basis of the classification in the quark model. The relations between the hypercharge, electric charge and other flavour quantum numbers hold for hadrons as well as quarks.\n\nQuantum chromodynamics (QCD) contains six flavours of quarks. However, their masses differ and as a result they are not strictly interchangeable with each other. The up and down flavours are close to having equal masses, and the theory of these two quarks possesses an approximate SU(2) symmetry (isospin symmetry).\n\nUnder some circumstances, the masses of quarks do not meaningfully contribute to the system's behavior, and can be ignored. The simplified behavior of flavour transformations can then be successfully modeled as acting independently on the left- and right-handed parts of each quark field. This approximate description of the flavour symmetry is described by a chiral group .\n\nIf all quarks had non-zero but equal masses, then this chiral symmetry is broken to the \"vector symmetry\" of the \"diagonal flavour group\" , which applies the same transformation to both helicities of the quarks. This reduction of symmetry is a form of \"explicit symmetry breaking\". The amount of explicit symmetry breaking is controlled by the current quark masses in QCD.\n\nEven if quarks are massless, chiral flavour symmetry can be spontaneously broken if the vacuum of the theory contains a chiral condensate (as it does in low-energy QCD). This gives rise to an effective mass for the quarks, often identified with the valence quark mass in QCD.\n\nAnalysis of experiments indicate that the current quark masses of the lighter flavours of quarks are much smaller than the QCD scale, Λ, hence chiral flavour symmetry is a good approximation to QCD for the up, down and strange quarks. The success of chiral perturbation theory and the even more naive chiral models spring from this fact. The valence quark masses extracted from the quark model are much larger than the current quark mass. This indicates that QCD has spontaneous chiral symmetry breaking with the formation of a chiral condensate. Other phases of QCD may break the chiral flavour symmetries in other ways.\n\nSome of the historical events that led to the development of flavour symmetry are discussed in the article on isospin.\n\n\n\n"}
{"id": "452577", "url": "https://en.wikipedia.org/wiki?curid=452577", "title": "Free body diagram", "text": "Free body diagram\n\nIn physics and engineering, a free body diagram (force diagram, or FBD) is a graphical illustration used to visualize the applied forces, movements, and resulting reactions on a body in a given condition. They depict a body or connected bodies with all of the applied forces and moments, as well as reactions, that act on that/those body(ies). The body may consist of multiple internal members, for example, a truss, or be a compact body such as a beam. A series of free bodies and other diagrams may be necessary to solve complex problems.\n\nFree body diagrams are used to visualize the forces and moments applied to a body and calculate the resulting reactions, in many types of mechanics problems. Most free body diagrams are used both to determine the loading of individual structural components as well as calculating internal forces within the structure in almost all engineering disciplines from Biomechanics to Structural.\nIn the educational environment, learning to draw a free body diagram is an important step in understanding certain topics in physics, such as statics, dynamics and other forms of classical mechanics.\n\nA free body diagram is not meant to be a scaled drawing. It is a diagram that is modified as the problem is solved. There is an art and flexibility to the process. The iconography of a free body diagram, not only how it is drawn but also how it is interpreted, depends upon how a body is modeled.\n\nFree body diagrams consist of:\n\nThe number of forces and moments shown in a free body diagram depends on the specific problem and the assumptions made; common assumptions are neglecting air resistance, friction and assuming rigid bodies. In statics all forces and moments must balance to zero; the physical interpretation of this is that if the forces and moments do not sum to zero the body is accelerating and the principles of statics do not apply. In dynamics the resultant forces and moments can be non-zero.\n\nFree body diagrams may not represent an entire physical body. Using what is known as a \"cut\" only portions of a body are selected for modeling. This technique exposes internal forces, making them external, therefore allowing analysis. This technique is often used several times, iteratively to peel back forces acting on a physical body. For example, a gymnast performing the iron cross: analyzing the ropes and the person lets you know the total force (body weight, neglecting rope weight, breezes, buoyancy, electrostatics, relativity, rotation of the earth, etc..). Then cut the person out and only show one rope. You get force direction. Then only look at the person, now you can get hand forces. Now only look at the arm to get the shoulder forces and moments, and on and on until the component you intend to analyze is exposed.\n\nA body may be modeled \nin three ways:\n\n\nConsider a body in free fall in a uniform gravitational field. The body may be\n\n\nAn FBD represents the body of interest and the external forces on it.\n\n\nTypically, however, a provisional free body sketch is drawn before all these things are known. After all, the whole point of the diagram is to help to determine magnitude, direction, and point of application of the external loads! Thus when a force arrow is originally drawn its length may not be meant to indicate the unknown magnitude. Its line may not correspond to the exact line of action. Even its direction may turn out to be wrong. Very often the original direction of the arrow may be directly opposite to the true direction. External forces known to be small that are known to have negligible effect on the result of the analysis, are sometimes omitted, but only after careful consideration or after other analysis proving it (e.g. buoyancy forces of the air in the analysis of a chair, or atmospheric pressure on the analysis of a frying pan).\n\nThe external; forces acting on the object include friction, gravity, normal force, drag, tension, or a human force due to pushing or pulling. When in a non-inertial reference frame (see coordinate system, below), fictitious forces, such as centrifugal pseudoforce are appropriate.\n\nA coordinate system is sometimes included, and is chosen according to convenience (or advantage). Savvy selection of coordinate frame may make defining the vectors simpler when writing the equations of motion. The \"x\" direction might be chosen to point down the ramp in an inclined plane problem, for example. In that case the friction force only has an \"x\" component, and the normal force only has a \"y\" component. The force of gravity will still have components in both the \"x\" and \"y\" direction: \"mg\"sin(\"θ\") in the \"x\" and \"mg\"cos(\"θ\") in the \"y\", where \"θ\" is the angle between the ramp and the horizontal.\n\nThere are some things that a free body diagram explicitly excludes. Although other sketches that include these things may be helpful in visualizing a problem, a proper free body diagram should \"not\" show:\n\nA free body diagram is analyzed by summing all of the forces, often accomplished by summing the forces in each of the axis directions. When the net force is zero, the body must be at rest or must be moving at a constant velocity (constant speed and direction), by Newton's first law. If the net force is not zero, then the body is accelerating in that direction according to Newton's second law.\n\nDetermining the sum of the forces is straightforward if all they are aligned with the coordinate frame's axes, but it is somewhat more complex if some forces are not aligned. It is often convenient to analyze the components of the forces, in which case the symbols ΣF and ΣF are used instead of ΣF. Forces that point at an angle to the diagram's coordinate axis can be broken down into two parts (or three, for three dimensional problems)—each part being directed along one of the axes—horizontally (\"F\") and vertically (\"F\").\n\nA simple free body diagram, shown above, of a block on a ramp illustrates this.\n\nSome care is needed in interpreting the diagram.\n\n"}
{"id": "49399421", "url": "https://en.wikipedia.org/wiki?curid=49399421", "title": "General Rules for the Interpretation of the Harmonized System", "text": "General Rules for the Interpretation of the Harmonized System\n\nThe General Rules for the Interpretation of the Harmonized System (\"GRI\") are the rules that govern the classification of goods under the Harmonized Commodity Description and Coding System (HS). \n\nThere are 6 General Rules in all, which must be applied in consecutive order.\n\n\n"}
{"id": "2941715", "url": "https://en.wikipedia.org/wiki?curid=2941715", "title": "Hex sign", "text": "Hex sign\n\nHex signs are a form of Pennsylvania Dutch folk art, related to fraktur, found in the Fancy Dutch tradition in Pennsylvania Dutch Country. Barn paintings, usually in the form of \"stars in circles\", began to appear on the landscape in the early 19th century, and became widespread decades later when commercial paint became readily available. By the 1950s commercialized hex signs, aimed at the tourist market, became popular and these often include stars, compass roses, stylized birds known as distelfinks, hearts, tulips, or a tree of life. Two schools of thought exist on the meaning of hex signs. One school ascribes a talismanic nature to the signs, the other sees them as purely decorative, or \"Chust for nice\" in the local dialect. Both schools recognize that there are sometimes superstitions associated with certain hex sign themes, and neither ascribes strong magical power to them. The Amish do not use hex signs.\n\nPainted barn stars in circular borders are a common sight on Pennsylvania Dutch barns in central and southeastern Pennsylvania, especially in Berks County, Lancaster County and Lehigh County. However, the modern decoration of barns is a late development in Pennsylvania Dutch folk art. Prior to the 1830s, the cost of paint meant that most barns were unpainted. As paint became affordable, the Pennsylvania Dutch began to decorate their barns much like they decorated items in their homes. Barn decorating reached its peak in the early 20th century, at which time there were many artists who specialized in barn decorating. Drawn from a large repertoire of designs barn painters combined many elements in their decorations. The geometric patterns of quilts can be seen in the patterns of many hex signs. Hearts and tulips seen on barns are commonly found on elaborately lettered and decorated birth, baptism and marriage certificates known as fraktur.\nThroughout the 20th century, hex signs were often produced as commodities for the tourist industry in Pennsylvania. These signs could be bought and then mounted onto barns and used as household decorations. Jacob Zook of Paradise, Pennsylvania claimed to have originated the modern mountable sign in 1942, based on traditional designs, to be sold in souvenir gift shops to tourists along the Lincoln Highway. William Schuster, Milton Hill, Johnny Ott, and Johnny and Eric Claypoole also contributed to this hex sign revival or adaptation. Modern artists may stress the symbolic meanings, for example, a horse head is used to protect animals from disease and the building from lightning, and a dove represents peace and contentment. An unusual use is the official logo of the Pennsylvania Bureau of Radiation Protection, which incorporates the international symbol for radiation into its yellow-and-red adaptation of a traditional hex sign design.\n\nThere are two opposing schools of belief regarding the derivation of the name. The term \"hex\" with occult connotations may derive from the Pennsylvanian German word \"hex\" (German \"Hexe\", Dutch \"Heks\"), meaning \"witch.\" However the term \"hex sign\" was not used until the 20th Century, after 1924 when Wallace Nutting's book \"Pennsylvania Beautiful\" was published. Nutting, who was not a Pennsylvania native, interviewed farmers about their distinctive barn decoration. Before this time there was no standardized term and many Pennsylvania German farmers simply called the signs \"blumme\" or \"sterne\" (meaning flowers or stars). However one farmer used the term \"Hexefoos\" in his description. The term became popular with Pennsylvania Germans themselves during the blossoming tourist trade of Southeastern Pennsylvania.\n\nIn recent years, hex signs have come to be used by non-Pennsylvania Dutch persons as talismans for folk magic rather than as items of decoration. Some believe that both the Pennsylvania German barn design and hex designs originate with the Alpine Germans. They note that hexes are of pre-Christian Germanic origin; for instance, a circled rosette is called the \"Sun of the Alps\" in Padania (Po Valley). Based on this history, Neopagans or Germanic heathens have taken up the practice of creating hex signs, incorporating other pre-Christian signs and symbols into the hex work. Gandee, in his book \"Strange Experience, Autobiography of a Hexenmeister\", described hex signs as \"painted prayers\".\n\nSome view the designs as decorative symbols of ethnic identification, possibly originating in reaction to 19th century attempts made by the government to suppress the Pennsylvania German language.\nAnabaptist sects (like the Amish and Mennonites) in the region have a negative view of hex signs. It is not surprising that hex signs are rarely, and perhaps never, seen on an Amish or Mennonite household or farm. \n\n\n\n"}
{"id": "29996600", "url": "https://en.wikipedia.org/wiki?curid=29996600", "title": "Hope (given name)", "text": "Hope (given name)\n\nHope is a feminine name derived from the Middle English \"hope,\" ultimately from the Old English word \"hopian\" referring to a positive expectation or to the theological virtue of hope. It was used as a virtue name by the Puritans. Puritans also used Hope as an element in phrase names, such as Hope-for, Hopeful, and Hope-still.\n\nThe name is also the usual English translation of the Greek name of Saint Hope, an early Christian child martyr who was tortured to death along with her sisters Faith and Charity. She is known as Elpis in Greek and Spes in Church Latin and her name is translated differently in other languages.\n\nFaith, Hope and Charity, the three theological virtues, are names traditionally given to triplet girls, just as Faith and Hope remain common names for twin girls. There were 40 sets of twins named Faith and Hope born in the United States in 2009, the second most common name combination for twin girls. One example were the American triplets Faith, Hope and Charity Cardwell, who were born in 1899 in Texas and were recognized in 1994 by the Guinness Book of World Records as the world's longest lived triplets.\n\nHope has been among the top 1,000 names given to girls born in the United States since 1880 and has been among the top 500 since 1909. It was ranked as the 231st most popular name for girls born in 2011 in the United States, down from its peak ranking of No. 144 in 1999.\n\n\n\n"}
{"id": "14863512", "url": "https://en.wikipedia.org/wiki?curid=14863512", "title": "Human rights violations by the CIA", "text": "Human rights violations by the CIA\n\nThis article deals with the activities of the Central Intelligence Agency, an agency of the Federal government of the United States, that violate human rights.\n\nIn 2003, Patricia Derian, who served as Assistant Secretary of State for Human Rights in the Carter Administration wrote, \"Through these U.S. military and intelligence agencies the United States government is sending a dangerous and double message. If this continues, it will subvert our entire human rights policy.\"\n\nIn understanding the CIA's role in human rights, there are challenging problems of ethics. John R. Stockwell, a CIA officer who left the Agency and became a public critic, said of the CIA field officers: \"They do not meet the death squads on the streets where they are actually chopping up people or laying them down on the street and running trucks over their heads. The CIA people in San Salvador meet the police chiefs, and the people who run the death squads, and they do liaise with them, they meet them beside the swimming pool of the villas. And it is a sophisticated, civilized kind of relationship. And they talk about their children, who are going to school at UCLA or Harvard and other schools, and they do not talk about the horrors of what is being done. They pretend like it is not true.\"\n\nFlorencio Caballero, a former Honduran Army interrogator, said that he had been trained by the Central Intelligence Agency, which the \"New York Times\" confirmed with US and Honduran officials. Much of his account was confirmed by three American and two Honduran officials, and may be the fullest given of how army and police units were authorized to organize death squads that seized, interrogated and killed suspected leftists. He said that while Argentine and Chilean trainers taught the Honduran Army kidnapping and elimination techniques, the CIA explicitly forbade the use of physical torture or assassination.\n\nCaballero described the CIA role as ambiguous. \"Caballero said his superior officers ordered him and other members of army intelligence units to conceal their participation in death squads from CIA advisers. He added that he was sent to Houston for six months in 1979 to be trained by CIA instructors in interrogation techniques. \"They prepared me in interrogation to end the use of physical torture in Honduras - they taught psychological methods\", Mr. Caballero said of his American training. \"So when we had someone important, we hid him from the Americans, interrogated him ourselves and then gave him to a death squad to kill.\"\n\nIt is widely accepted that the CIA has both directly engaged in the torture of detainees at CIA-run black sites and has sent detainees to be tortured by friendly governments in a manner contravening both US and international law.\n\nThe existence of black sites was first published by \"The Washington Post\" in November 2005 following reports by human rights NGOs. US President George W. Bush acknowledged the existence of secret prisons operated by the CIA during a speech on 6 September 2006.\nAdministration officials such as Deputy Assistant Attorney General John Yoo(as well as prominent legal scholars such as Alan Dershowitz) publicly defended and justified the use of torture against terrorism suspects.\n\nThere are balancing assertions that CIA personnel attempted to minimize abuses by foreign governments who practiced torture before any involvement with CIA. There is evidence that CIA has funded academic research, unwitting in some cases, which was then used to develop harsh interrogation techniques.\n\nFrom 2002-2007, as part of the War on Terror, CIA personnel employed so-called \"Enhanced interrogation techniques\", a variety of coercive and abusive interrogation techniques that are widely judged to constitute torture. Various officials of the George W. Bush administration have argued that \"harsh\" techniques such as waterboarding are not torture, or the end objectives of the War against Terror justify those means. There has been considerable domestic and international protest against these practices. The United Nations' Special Rapporteur on Torture, Human Rights Watch, and American legal scholars have called for the prosecution of Bush administration officials who ordered torture, conspired to provide legal cover for torture, and CIA and DoD personnel and contract workers who carried it out.\n\nTorture and harsh imprisonment accusations (as well as documented examples) were not limited to the recent War on Terror. Yuri Nosenko, a Soviet KGB defector, was held in stark conditions of solitary confinement in a clandestine CIA facility in the continental US from 1966 to 1969.\n\nWaterboarding is a method that gives the subject the sensation of drowning, and may cause permanent damage to the lungs and other parts of the body. Some individuals being waterboarded, who may have had preexisting cardiac or respiratory disease, died under the method.\n\nFor example, the CIA used waterboarding, and other interrogation techniques against three suspected Al Quaeda members, Khalid Sheikh Mohammed, Abu Zubaydah and Abd al-Rahim al-Nashiri.\n\nIn 2007, Red Cross investigators concluded in a secret report that the Central Intelligence Agency's interrogation methods for high-level al Qaeda prisoners constituted torture which could make the Bush administration officials who approved them guilty of war crimes, according to the book \"Dark Side: The Inside Story of How the War on Terror Turned Into a War on American Ideals,\" by Jane Mayer a journalist for \"The New Yorker\".\n\nAccording to the book, the report of the International Committee of the Red Cross found that the methods used on Abu Zubaydah, the first major al Qaeda figure captured by the United States, were \"categorically\" torture, which is illegal under both United States law and international conventions to which the U.S. is a party. A copy of the report was given to the CIA in 2007. For example, the book states that Abu Zubaydah was confined in a box \"so small he said he had to double up his limbs in the fetal position\" and was one of several prisoners to be \"slammed against the walls,\" according to the Red Cross report. The CIA has admitted that Abu Zubaydah and two other prisoners were waterboarded, a practice in which water is poured on the nose and mouth to create the sensation of suffocation and drowning.\n\nOn 24 January 1997, two CIA manuals were declassified in response to a Freedom of Information Act (FOIA) request filed by the \"Baltimore Sun\" in 1994. The first manual, \"KUBARK Counterintelligence Interrogation\", dated July 1963, is the source of much of the material in the second manual. The second manual, \"Human Resource Exploitation Training Manual - 1983\", was used in at least seven U.S. training courses conducted in Latin American countries, including Honduras, between 1982 and 1987. Both manuals deal exclusively with interrogation and have an entire chapter devoted to \"coercive techniques.\" These manuals recommend arresting suspects early in the morning by surprise, blindfolding them, and stripping them naked. Interrogation rooms should be windowless, soundproof, dark and without toilets. Suspects should be held incommunicado and should be deprived of any kind of normal routine in eating and sleeping. The manuals describe coercive techniques to be used \"to induce psychological regression in the subject by bringing a superior outside force to bear on his will to resist.\"\n\nWhile the US manuals contained coercive measures, they did not rise to the level of what is generally defined as torture. Torture, however, has been culturally a part of authoritarian South American governments, especially in 1973-1983 (Dirty War). Argentina had learned such methods from the French in Operation Charly and its secret services trained the personnel of other South American countries. It is not clear to what extent field personnel and leadership of agencies of the Federal government of the United States (notably the CIA, Defense and State Departments, AID) were aware of this, condoned it, or actively assisted it.\n\nIn 2007, the chief Argentinian interrogator, Ernesto Guillermo Barreiro, was arrested in the United States. It is not clear whether he will be deported, held, or extradited. The other two arrested were Peruvians, Telmo Ricardo Hurtado and Juan Manuel Rivera Rondon, accused of having participated in the massacre of 69 peasants in an Andean village in 1985, when President Alan García was trying to suppress the Maoist Shining Path guerrilla movement. Garcia was the President of Peru again from 2006 to 2011.\n\nCIA involvement ranged from no knowledge, to knowledge but no participation, to knowledge with the suggestion that less brutal techniques were appropriate, to participation or observation.\n\nExtraordinary rendition is the process of clandestinely moving a prisoner from where he was captured, to an interrogation center in a country not subject to US law regarding extreme methods of interrogation. Some reports indicate CIA personnel operated the prison and performed the interrogations, while others state that while the CIA delivered the prisoner, third-country intelligence personnel did the actual interrogation. The Wikipedia details the list of known or suspected black sites where CIA prisoners may be detained. Known or suspected aircraft used to transport prisoners are in Rendition aircraft.\n\nA claim that the black sites existed was made by \"The Washington Post\" in November 2005 and before by human rights NGOs. US President George W. Bush acknowledged the existence of secret prisons operated by the CIA during a speech on 6 September 2006.\n\nA 50-page Human Rights Watch report, \"Ghost Prisoner: Two Years in Secret CIA Detention,\" contains a detailed description of a secret CIA prison from a Palestinian former detainee who was released from custody the previous year. Human Rights Watch has also sent a public letter to U.S. President George W. Bush requesting information about the fate and whereabouts of the missing detainees.\n\"President Bush told us that the last 14 CIA prisoners were sent to Guantanamo, but there are many other prisoners 'disappeared' by the CIA whose fate is still unknown,\" said Joanne Mariner, terrorism and counterterrorism director at Human Rights Watch. \"The question is: what happened to these people and where are they now?\"\nIn early September 2006, 14 detainees were transferred from secret CIA prisons to military custody at Guantanamo Bay. In a televised speech on 6 September 2006, President Bush announced that with those 14 transfers, no prisoners were left in CIA custody.\n\nBefore 1973, Administrations were principally concerned with a Communist threat of subversion, to be met with host country internal security forces such as the police. Police and military roles were blurred. US assistance to foreign police began in the 1950s, and increased in the early 1960s when the Kennedy administration became concerned about growing communist insurgent activities and established a public safety program within the Agency for International Development (AID) to train foreign police. By 1968 the United States was spending $60 million a year to train police in 34 countries in areas such as criminal investigation, patrolling, interrogation and counterinsurgency techniques, riot control, weapon use, and bomb disposal The United States also provided weapons, telecommunications, transportation, and other equipment. In the early 1970s, the Congress became concerned over the apparent absence of clear policy guidelines and the use of program funds to support repressive regimes that committed human rights' abuses. As a result, \"the Congress determined that it was inadvisable for the United States to continue supporting any foreign police organizations\". Both the CIA and military intelligence with Counterintelligence Force Protection Source Operations may have intelligence collecting relationships with local police.\n\nThe Bureau of Diplomatic Security, under the Department of State representatives at any embassy is expected to be aware of all contacts between US personnel and local law enforcement organizations. The 1973 legislation significantly limited police training of all sorts, partially in response to human rights concerns.\n\nThe majority of this training went to Latin America, but some did go to countries elsewhere in the world, according to Senator Alan Cranston, especially those with narcotics or terrorism problems. Cranston cited six reasons why the Congress, in 1974, banned police training, after learning of training and equipping \"police in Iran, Vietnam, Brazil, and other countries were involved in torture, murder, and the suppression of legitimate political activity\":\n\nCranston went on to say,\n\nIn principle, training and assistance to police, as opposed to military organization, is against US law, but the relevant law allows Presidential waiver of most provisions. A GAO report\n\n... provides information on (1) the legislative authority for providing assistance to foreign law enforcement agencies and personnel, (2) the extent and cost of U.S. activities, and (3) experts' opinions on the management of these programs.\n\nIn 1973 and 1974, the Congress enacted legislation forbidding U.S. agencies from using foreign economic or military assistance funds to assist foreign police, but it subsequently granted numerous exemptions to permit assistance in some countries and in various aspects of police force development, including material and weapons support, force management, narcotics control, and counterterrorism tactics. The 1974 prohibition did not apply to the use of other funds by agencies such as the United States Department of Justice (DOJ) or United States Department of Transportation (DOT) to train or assist foreign law enforcement personnel.\n\nNo government agency is in charge of calculating the cost. However, the General Accounting Office identified 125 countries that received U.S. training and assistance for their police forces during fiscal year 1990 at a cost of at least $117 million.\n\nThe prohibition did not apply to Drug Enforcement Administration DEA and Federal Bureau of Investigation FBI assistance to combat international narcotics trafficking.\n\nDuring fiscal year 1990 at a cost of about $117 million, the GAO estimated the major programs were: U.S. programs providing :\n\nCurrent and former State Department and other government officials, and academic experts who have been involved in assistance to foreign police forces, stated that the U.S. government lacks:\na clear policy, or program objectives, on the role of U.S. assistance to police forces in the new and emerging democracies, a focal point for coordination and decision-making, and a means for determining whether individual programs and activities support U.S. policy or contribute to overall U.S. interests.\n\nThey noted that each program is managed individually, and the only place that coordination is occurring is at the U.S. embassy in the country.\n\nThe US General Accounting Office obtained information on CIA training and assistance provided to foreign law enforcement personnel, reviewed the legislative authority for providing this training and assistance, and identified efforts to coordinate these activities. They did not review program implementation in recipient countries. They interviewed officials and obtained records from AID and the Departments of State, Justice, and Defense, in Washington, D.C.; reviewed legislation and agency legal opinions on foreign police assistance; interviewed academic and legal experts on current U.S. assistance to foreign police; and reviewed literature published on foreign police assistance and AID's public safety program.\n\nCongressionally approved exemptions generally authorize activities that benefit a specific U.S. goal, such as countering the terrorist threat to U.S. citizens overseas or combating drug trafficking.\nExemptions were also extended by:\nIt also expanded the judicial reform program and the police training exemption to countries in Latin America and the Caribbean. In 1988 the Congress further expanded the judicial reform program to allow police assistance to promote investigative and forensic skills, develop law enforcement training curricula, and improve administration and management of law enforcement organizations. This act specifically prohibited the Department of Defense (DOD) and the U.S. armed forces from providing training under this program.\n\nMaritime subjects were exempted from the section 660 prohibition, as well as the prohibition for any country that has a long-standing democratic tradition, does not have armed forces, and does not engage in a consistent pattern of gross violations of human rights. The act permitted such countries to receive any type of police assistance.\n\nThis series of acts approved certain police assistance activities in Latin America and the Caribbean for narcotics control purposes. The 1988 act expanded DOD's role and allowed it to provide training and weapons and ammunition in fiscal years 1989 and 1990 to foreign police units that are specifically organized for narcotics enforcement in eligible countries in Latin America and the Caribbean. This act also allowed economic support funds to be provided to Colombian police for the protection of judges, government officials, and members of the press against narco-terrorist attacks. The 1989 Act extended DOD's authority to train police units in Bolivia, Colombia, and Peru in fiscal year 1990, if the units are only for narcotics enforcement. It also allowed DOD to provide, in addition to weapons and ammunition, other defense articles such as helicopters, vehicles, radios, and personnel gear.\nThe 1990 act authorized DOD to continue to train and equip police forces in the Andean region.\n\nIn addition to the exemptions previously discussed, there are other authorities that waive the prohibition on assistance to police forces of foreign countries. For example, the President may authorize foreign assistance when `it is important to the security interests of the United States'. This allows the President to waive any provision of the Foreign Assistance Act of 1961, including section 660.\n\nThis aims to improve foreign governments' antiterrorist capabilities to better protect U.S. citizens and interests. In 1990, the U.S. provided assistance to 49 countries at a cost of nearly $10 million. Sixty-two percent of the funds were spent in Latin America, the Caribbean, and Europe, and less than $500,000 was used to purchase equipment. The Department of State manages the program and contracts with other U.S. government agencies, state or local police departments, and private firms to conduct the training. The Federal Aviation Administration, U.S. Customs Service, the Immigration and Naturalization Service, the Federal Law Enforcement Training Center, and the U.S. Marshals Service are regular trainers. In compliance with legislative requirements, most training takes place in the United States.\n\nOne of the objectives of the Department of State Bureau of International Narcotics Matters (INM) international narcotics control training program is to strengthen host country enforcement and interdiction capabilities. During fiscal year 1990, INM provided a minimum of $45 million in training and equipment to foreign police, principally in Mexico, Jamaica, Colombia, Ecuador, Peru, Bolivia, Brazil, Venezuela, Pakistan, Thailand, and Turkey. These are all narcotics producing and trafficking countries.\n\nINM reimburses other U.S. government agencies, primarily the Drug Enforcement Administration (DEA), Customs, and Coast Guard, to conduct the actual training. DEA provides narcotics investigative training, Customs teaches air, sea and land port search procedures, and Coast Guard teaches courses in maritime interdiction. Other agencies may also be requested to train on a reimbursable basis in areas where they have specific expertise. For example, DOD provides helicopter training to police in drug trafficking countries. Training is conducted both overseas and in the United States and is reviewed and approved by INM.\n\nIn addition, DOD used military assistance funds to train and equip narcotics enforcement police in several drug producing and trafficking countries. Documents provided by DOD show that in fiscal year 1990, DOD provided training and equipment with a value of at least $17 million to Mexico, $1.3 million to Bolivia, $10 million to Colombia, $1 million to Ecuador, and $1 million to Peru. DOD officials informed us that training and equipment valued at more than these amounts may also have been provided. However, documentation was not available at the Washington, D.C., agency headquarters level that specified the amounts for law enforcement activities. The equipment provided consisted of UH-1 helicopters and spare parts, ammunition, small arms, riot control equipment, radios, and miscellaneous personal gear.\n\nIn FY1986, AID transferred funds to DOJ to design, develop, and implement projects to improve and enhance the investigative capabilities of law enforcement agencies in the Latin America and the Caribbean region. This was part of AID's effort to reform judicial systems. Using these funds, DOJ established the International Criminal Investigative Training Assistance Program (ICITAP). Operating under State Department oversight, ICITAP has conducted criminal justice sector needs assessments in the region and has expanded its training to include basic police management and police academy development. In fiscal year 1990, ICITAP received $7 million from the Department of State for its regional program. It trained more than 1,000 students from the Caribbean, Central and South America and sponsored 7 conferences. Training includes police management, criminal investigation, crime scene search, and forensic medicine courses. Except for students sent to training programs in the United States, ICITAP training takes place overseas.\n\nThe Federal Bureau of Investigation (FBI) also provides limited training for foreign law enforcement officials. Each year approximately 100 international police officials attend the 11-week college level course at the FBI National Academy that includes studies on management and forensic sciences. The FBI pays for the training and subsistence, but does not pay for the students' transportation. Over the last 10 years, more than 1,100 foreign police officials from 89 countries have graduated from this course.\n\nUsing its own funds, the FBI created two training courses:\n\nThe FBI also provides other training and assistance to foreign police as requested, but the cost is unknown. For example, the National Center for the Analysis of Violent Crime provided training to Canadian police. The Criminal Investigative Division conducted a training seminar for officers from Italy's three national law enforcement agencies on the use of sensitive investigative techniques such as the operation of confidential sources, undercover operations, and electronic surveillance. The FBI also furnishes on-the-job assistance to governments who request help during particularly difficult or sensitive investigations.\n\nDOD supplies a limited amount of military training and assistance to police officials. During fiscal years 1986 and 1987, DOD trained and equipped the El Salvadoran and Honduran police to counter urban terrorist activities.\n\nThis assistance was authorized in response to the murder of U.S. Marines by terrorists in El Salvador and was managed and delivered by the U.S. Army Military Police. The assistance consisted of training in counterterrorism techniques and the supply of police vehicles, communications, weapons, and other equipment. This effort cost $19.8 million, of which $17 million was provided to El Salvador.\n\nIn fiscal year 1990, DOD spent $6.4 million in previously authorized but unused military assistance funds to purchase needed equipment and weapons for Panama's newly formed national police force. Items procured included police vehicles, communications equipment, small arms, and personal gear. This assistance was a one-time, emergency program.\n\nDOD has an ongoing military assistance program to support Costa Rican police. In fiscal year 1990, DOD supplied $431,000 in military equipment and $232,000 in military training to the Costa Rican Civil Guard to help them carry out their responsibility to protect the border regions of the country. DOD provided equipment such as vehicles, personnel gear, and radios, and military training in areas such as coastal operations. Additionally, DOD conducted technical training courses in equipment maintenance and medical skills among others.\n\nDOD, along with the United Kingdom, supports the Eastern Caribbean Regional Security System that was formed after the U.S. intervention in Grenada. The Security System is composed of a few permanently assigned military officers, but largely depends upon island nation police officers who can be called up for military duty in case of emergency. The United States equips and trains these personnel to prepare them for such an eventually. In fiscal year 1990, DOD provided $4.2 million in military assistance funds that were used to purchase equipment such as jeeps, small arms, uniforms, and communications gear. DOD also provided $300,000 for training in special operations, rural patrol, field survival, and surveillance, as well as technical courses in communications, navigation, maintenance, and medicine.\n\nSince some agencies fund assistance out of their own budgets, without necessarily having a line item for police assistance, the GAO \"could not accurately determine the extent or cost of assistance to foreign police\" In the GAO estimates, \"some double counting of students may be occurring and agencies may not be differentiating between assistance provided to police and assistance provided to the military.\" This might happen when \"the agency supplying the training and the agency paying for the training may both include the trainees in their reporting systems, such as when ICITAP pays for students attending the FBI academy.\"\n\nWhile the DOJ was asked to calculate information on its work with foreign police, it could not assign a dollar value to items including \"travel expenses, salaries, and expendable items such as course materials.\" H. \n\"Also, GAO could not always determine whether a student was a police officer or a military member because some agencies do not collect such data, DOD officials informed us that once they receive permission to train police in a specific activity they do not provide a further accounting breakdown. For example, training provided to the Eastern Caribbean Regional Security System was for law enforcement personnel, although a few trainees may have belonged to military organizations.\"\n\nProject MKULTRA, or MK-ULTRA, was the code name for a CIA mind-control research program that began in 1950, involved primarily with the experimentation of drugs and other \"chemical, biological and radiological\" stimuli on both willing and uninformed subjects.\n\nIn December 1974, \"The New York Times\" reported that the CIA had conducted illegal domestic activities, including experiments on U.S. citizens, during the 1960s. The report prompted investigations by both the U.S. Congress (in the form of the Church Committee) and a presidential commission (known as the Rockefeller Commission). The congressional investigations and the Rockefeller Commission report revealed that the CIA and the Department of Defense had in fact conducted experiments to influence and control human behavior through the use of psychoactive drugs such as LSD and mescaline and other chemical, biological, and psychological means. Experiments were often conducted without the subjects' knowledge or consent.\n\nMK-ULTRA was started on the order of CIA director Allen Dulles, largely in response to alleged Soviet, Chinese, and North Korean use of mind-control techniques on U.S. prisoners of war in Korea. The goal of the experiments was to study mind-control in order to develop methods of interrogation and behavior modification and manipulation, as well as to develop a possible truth drug.\n\nMK-ULTRA research ultimately proved useless to the CIA and they have abandoned the program. Because most MK-ULTRA records were deliberately destroyed in 1973 by order of then CIA Director Richard Helms, it is difficult if not impossible to have a complete understanding of the more than 150 individually funded research sub-projects sponsored by MK-ULTRA and related CIA programs.\n\nFollowing the recommendations of the Church Committee, President Gerald Ford in 1976 issued the first Executive Order on Intelligence Activities which, among other things, prohibited \"experimentation with drugs on human subjects, except with the informed consent, in writing and witnessed by a disinterested party, of each such human subject\" and in accordance with the guidelines issued by the National Commission. Subsequent orders by Presidents Carter and Reagan expanded the directive to apply to any human experimentation.\n\nAt least since World War II, a distinction has been drawn between assassination of civilian leaders, and targeted killings of leaders of fighting organizations. Some cases were blurry, such as the British-Czech Operation Anthropoid, the killing of uniformed SS officer Reinhard Heydrich, the acting Reichsprotektor of The Protectorate of Bohemia and Moravia. A failed attempt, by British troops, to kill Field Marshal Erwin Rommel was clearly aimed at a military leader, as was the successful shooting down of Admiral Isoroku Yamamoto.\n\nCIA has admitted being involved in assassination attempts against foreign leaders. Recently, there have been targeted killings of suspected terrorists, typically with missiles fired from unmanned aerial vehicles, in a manner that a number of legal authorities believe was a legitimate act as opposed to a prohibited assassination.\n\nOf the cases cited, it appears that no CIA personnel or even directly controlled foreign agents personally killed any leader, but there certainly were cases where the CIA knew of, or supported, plots to overthrow foreign leaders. In the cases of Lumumba in 1960/61 and Castro in the 50s and 60s, the CIA was involved in preparing to kill the individual. In other cases, such as Diem, the Agency knew of a plot but did not warn him, and communications at White House level indicated that the Agency had, with approval, told the plotters the US didn't object to their plan. The gun or poison, however, was not in the hands of a CIA officer.\n\nCIA personnel were involved in attempted assassinations of foreign government leaders such as Fidel Castro. They provided support to those that killed Patrice Lumumba. In yet another category was noninterference in the Republic of Vietnam (South Vietnam) coup in which President Ngo Dinh Diem was killed.\n\nA distinction has been drawn between political assassinations and \"targeted killing\" of leaders of non-state belligerents.\n\nPerhaps the best-documented account of CIA-sponsored assassination plans were against President Fidel Castro of Cuba.\n\nAccording to columnist Jack Anderson, the first attempt was part of the Bay of Pigs Invasion operation, but five more teams were sent, the last apprehended on a rooftop within rifle range of Castro, at the end of February or beginning of March 1963. Anderson speculated that President Fidel Castro may have become aware of it, and somehow recruited Lee Harvey Oswald to retaliate against President John F. Kennedy.\n\nMaheu was identified as the team leader, who recruited John Roselli, a gambler with contacts in the American and Cuban underworlds. The CIA assigned two operations officers, William King Harvey and James O'Connell, to accompany Roselli to Miami to recruit the actual teams.\n\nAnderson's story appears to be confirmed by two CIA documents, the first referring to an Inspector General report of investigation of allegations that the Agency conspired to assassinate Fidel Castro. The story first appeared in Drew Pearson's column and has since appeared in Jack Anderson's column. \"While the columns contained many factual errors, the allegations are basically true. Second, a declassified memo from Howard Osborne, director of the CIA Office of Security, dated 15 February 1972, in the \"CIA Family Jewels\" series, from to the Executive Director, speaks of John Roselli, then serving time in a Federal penitentiary in Seattle, Washington, with deportation scheduled at the end of his sentence. While the CIA was aware \"Roselli intended to expose his participation in the plot should we not intervene in his behalf. The DCI at the time, John McCone, decided to take a calculated risk and accept the consequences of possible disclosure. Two articles by Jack Anderson discuss the plot, as well the \"Washington Post\" Sunday magazine, \"Parade\"\n\nIndividuals who were aware of this project were: Director of Central Intelligence Allen Dulles, Richard M. Bissell Jr. (Deputy Director for Plans (DDP)) Colonel J.C. King (Chief, Western Hemisphere Division, DDP), Colonel Sheffield Edwards, William Harvey, and James P. O'Connell. Also included were Robert A. Maheu (former FBI agent, public relations agent who did work for the CIA, and later an aide to Howard Hughes), and his attorneys Edward P. Morgan and Edward Bennett Williams.\n\nOn 26 February 1971, Osborne arranged with the Commissioner of the U.S. Immigration and Naturalization Service to flag any deportation. INS confirmed they did this again for 1972.\n\nIn summary, CIA, Embassy, and other US personnel, up to White House level, were aware of yet another coup being planned against President Ngo Dinh Diem of South Vietnam. While the US had no direct participation in the coup, the plotters were told, in a deniable way, that the US did not object to it. No documentary evidence has surfaced that the US knew that Diem and his brother were to be killed, and it is unclear that all the Vietnamese plotters knew or agreed to it. President John F. Kennedy was aware of the coup plans, but apparently had not considered the hazard to Diem.\n\nAccording to the \"Pentagon Papers\", the final US loss of confidence in Diem began when his government violently suppressed a protest on Buddha's Birthday, 8 May 1963. Up to that point, the majority Buddhists had not been very politically active, even though Diem had given preference to the Catholic minority. Quickly, however, the Buddhists put a \"cohesive and disciplined [political] organization\" into action. By June, the situation moved from dissidence from a religious group to a \"grave crisis of public confidence\".\n\nThen-Ambassador Frederick Nolting had tried to persuade Diem to moderate government action against Buddhists, but with no success. While Nolting was on leave, President John F. Kennedy appointed Henry Cabot Lodge, Jr. as the new Ambassador. In June 1963, senior leaders began, for the first time, to discuss the effect of a coup to remove Diem. Nolting and the US military in Vietnam, however, argued that Diem was keeping chaos at bay. Nolting left permanently in mid-August, but the assurances from Diem died with multiple August 21 night raids on Buddhist temples in many parts of Vietnam. Two days later, a US representative was approached by generals considering a coup. On 23 August, the first contact with a U.S. representative was made by generals who had begun to plan a coup against Diem. They were told that the U.S. had determined that Diem's brother, who had led the raids on the Buddhists, could not stay in any kind of power, and that, \"then, we must face the possibility that Diem himself cannot be preserved.\"\n\nAn 8 May 1973 memorandum states that \"An Inspector General report of investigation of allegations that the Agency was instrumental in bringing about the assassination of President Ngo Dinh Diem of South Vietnam. The allegations were determined to be without foundation.\"\n\nNevertheless, the Pentagon Papers observed,\n\nFor the military coup d'etat against Ngo Dinh Diem, the U.S. must accept its full share of responsibility. Beginning in August 1963 we variously authorized, sanctioned and encouraged the coup efforts of the Vietnamese generals and offered full support for a successor government. In October we cut off aid to Diem in a direct rebuff, giving a green light to the generals. We maintained clandestine contact with them throughout the planning and execution of the coup and sought to review their operational plans and proposed new government. Thus, as the nine-year rule of Diem came to a bloody end, our complicity in his overthrow heightened our responsibilities and our commitment in an essentially leaderless Vietnam.\n\nThe Church Committee concluded it had \"solid evidence of a plot to assassinate Patrice Lumumba [the first elected Prime Minister of the Republic of Congo]. Strong hostility to Lumumba, voiced at the very highest levels of government may have been intended to initiate an assassination operation; at the least it engendered such an operation. The evidence indicates that it is likely that President Eisenhower's expression of strong concern about Lumumba at a meeting of the National Security Council on 18 August 1960, was taken by (Director of Central Intelligence) Allen Dulles as authority to assassinate Lumumba. There is, however, testimony by Eisenhower Administration officials, and ambiguity and lack of clarity in the records of high-level policy meetings, which tends to contradict the evidence that the President intended an assassination effort against Lumumba. In a footnote, the Committee cited an unnamed official as saying he had heard Eisenhower order the assassination.\"\n\nThe week after the August 18 NSC meeting, a presidential advisor reminded the Special Group of the \"necessity for very straightforward action\" against Lumumba and prompted a decision not to rule out consideration of \"any particular kind of activity which might contribute to getting rid of Lumumba.\" The Special Group is one of the many names for the often-reorganized committee that approved CIA covert action proposals. It has been called the 303 committee, Special Group (counterinsurgency), Operations Advisory Group, 5412 committee, and Forty Committee. \"The following day, Dulles cabled a CIA Station Officer in Leopoldville, Republic of the Congo,* that \"in high quarters\" the \"removal\" of Lumumba was \"an urgent and prime objective.\"\n\nShortly thereafter the CIA's clandestine service formulated a plot to assassinate Lumumba. The plot proceeded to the point that lethal substances and instruments specifically intended for use in an assassination were delivered by the CIA to the Congo Station. There is no evidence that these instruments of assassination were actually used against Lumumba.\n\nIn the meantime, Lumumba was dismissed from his post by Congolese President Joseph Kasa-Vubu, an act of dubious legality; in retaliation, Lumumba attempted to dismiss Kasa-Vubu from the presidency, an act of even more dubious legality. On 14 September, a coup d'état endorsed by the CIA and organized by Colonel Joseph Mobutu removed Lumumba from office.\n\nLumumba was killed, in 1961, by forces under the control of the President, Moise Tshombe of Katanga, a province that had declared its independence of the Republic of the Congo. Lumumba was taken by Katangan soldiers commanded by Belgians, and eventually shot by a Katangan firing squad under Belgian leadership.\n\nThe independent Republic of the Congo was declared on 30 June 1960, with Joseph Kasa-Vubu as President and Patrice Lumumba as Prime Minister. It shared a name with the neighboring Republic of the Congo to the west, a French colony that also gained independence in 1960, and the two were normally differentiated by also stating the name of the relevant capital city, so Congo (Léopoldville) versus Congo (Brazzaville).\n\nLarry Devlin became Chief of Station in Congo in July 1960, a mere 10 days after the country's independence from Belgium and shortly before Prime Minister Patrice Lumumba's two-month term in office, dismissal from power and ultimate execution. In his memoir, Devlin reveals that late in 1960, he received instructions from an agent (\"Joe from Paris\") who was relaying instructions from CIA headquarters that he (Devlin) was to effect the assassination of Lumumba. Various poisons, including one secreted in a tube of toothpaste, were proffered. The directive had come from the CIA Deputy Chief of Plans Dick Bissell, but Devlin wanted to know if it had originated at a higher level and if so, how high. \"Joe\" had been given to understand that it had come from President Dwight D. Eisenhower, but Devlin to this day does not know for sure. Devlin writes (and has recently said in public speaking engagements) that he felt an assassination would have been \"morally wrong\" and likely to backfire and work against U.S. interests. In the event, he temporized, neglecting to act, and Lumumba was ultimately murdered by his enemies in Katanga, with Belgian government participation. U.S. intelligence was kept apprised.\n\nThe United Nations Security Council was called into session on 7 December 1960 to consider Soviet demands that the U.N. seek Lumumba's immediate release, the immediate restoration of Lumumba as head of the Congo government, the disarming of the forces of Mobutu, and the immediate evacuation of Belgians from the Congo. Soviet Representative Valerian Zorin refused U.S. demands that he disqualify himself as Security Council President during the debate. Dag Hammarskjöld, answering Soviet attacks against his Congo operations, said that if the U.N. forces were withdrawn from the Congo \"I fear everything will crumble.\"\n\nFollowing a U.N. report that Lumumba had been mistreated by his captors, his followers threatened (on 9 December 1960) to seize all Belgians and \"start cutting off the heads of some of them\" unless Lumumba was released within 48 hours.\n\nAccording to the Church Committee report:\n\nIn February 1960, CIA's Near East Division sought the endorsement of what the Division Chief <nowiki>[</nowiki>James H. Critchfield<nowiki>]</nowiki> called the \"Health Alteration Committee\" for its proposal for a \"special operation\" to \"incapacitate\" an Iraqi Colonel believed to be \"promoting Soviet bloc political interests in Iraq.\" The Division sought the Committee's advice on a technique, \"which while not likely to result in total disablement would be certain to prevent the target from pursuing his usual activities for a minimum of three months,\" adding: \"We do not consciously seek subject's permanent removal from the scene; we also do not object should this complication develop.\" ... In April [1962], the [Health Alteration] Committee unanimously recommended to the DDP [Deputy Director for Plans, Richard M. Bissell Jr.] that a \"disabling operation\" be undertaken, noting that the Chief of Operations advised that it would be \"highly desirable.\" Bissell's deputy, Tracy Barnes, approved on behalf of Bissell ... The approved operation was to mail a monogrammed handkerchief containing an incapacitating agent to the colonel from an Asian country. [James] Scheider [Science Advisor to Bissell] testified that, while he did not now recall the name of the recipient, he did remember mailing from the Asian country, during the period in question, a handkerchief \"treated with some kind of material for the purpose of harassing that person who received it.\" ... During the course of this Committee's investigation, the CIA stated that the handkerchief was \"in fact never received (if, indeed, sent).\" It added that the colonel: \"Suffered a terminal illness before a firing squad in Baghdad (an event we had nothing to do with) not very long after our handkerchief proposal was considered.\"\n\nAlthough some sources depict this operation as an assassination attempt on Iraqi Prime Minister Abd al-Karim Qasim, other sources note that this interpretation is inaccurate or unsupported by evidence, as the notion that the CIA sought the target's assassination is refuted by the plain meaning of the text itself. In addition, it is unlikely that Qasim was the intended recipient of the handkerchief, as CIA officials would likely have remembered an attack on the Iraqi head of state. While Qasim was not a colonel but a brigadier general and did not openly promote Soviet interests in Iraq, the pro-Soviet head of Iraq's \"People's Court,\" Colonel Fahdil Abbas al-Mahdawi, fits the above description perfectly. Qasim effectively banned the ICP in January 1960, but Mahdawi remained a crucial conduit between Qasim's government and several communist-front groups—including the \"Peace Partisans,\" which was allowed to operate in public despite being formally outlawed in May 1961—and was known for his outspoken praise for Fidel Castro as well as his trips throughout the Soviet Union, the Eastern Bloc, and China. In 1991, former high-ranking U.S. diplomat Hermann Eilts told journalist Elaine Sciolino that Mahdawi had been the target.\n\nMahdawi and some of his family members \"were\" stricken with a serious case of what Mahdawi dubbed \"influenza\" in 1962, but it is unknown whether this ailment was related to the CIA's plan to poison him; Nathan J. Citino observes that \"the timing of the illness does not correspond exactly to that of the 'incapacitating' operation as described in the cited testimony.\"\n\nAn Inspector General report of investigation of allegations that the Agency was instrumental in bringing about the\nassassination of Rafael Trujillo, dictator of the Dominican Republic. Trujillo was effective head of government at the time of his assassination in 1961.\n\nConditions leading to a desire, by Dominicans, appeared to begin Johnny Abbes, took control the Intelligence Military Service (the secret police), and the country developed more internal violence and increasingly isolated from other nations. This isolation compounded Trujillo's fears, prompting him to worsen his foreign interventionism.\n\nTo be sure, Trujillo did have cause to resent the leaders of some nations, such as Cuba's Fidel Castro, who assisted a small, abortive invasion attempt by dissident Dominicans in 1959. Trujillo, however, expressed greater concern over Venezuela's president Rómulo Betancourt (1959–64). An established and outspoken opponent of Trujillo, Betancourt had been associated with some individual Dominicans who had plotted against the dictator. Trujillo developed an obsessive personal hatred towards Betancourt and supported numerous plots of Venezuelan exiles to overthrow him. This pattern of intervention led the Venezuelan government to take its case against Trujillo to the Organization of American States (OAS).\n\nThis development infuriated Trujillo, who ordered his foreign agents to plant a bomb inside Betancourt's car. The assassination attempt, carried out on 24 June 1960, injured but did not kill the Venezuelan president.The firestorm caused from the incident inflamed world opinion against Trujillo. The members of the OAS, expressing this outrage, voted unanimously to sever diplomatic relations and to impose strong economic sanctions on the Dominican Republic.\n\nFinally on the night of the 30 May 1961, Rafael Trujillo was shot to death on San Cristobal Avenue, Santo Domingo. He was the victim of an ambush plotted by a number of Dominicans. According to American reporter Bernard Diederich, the United States Central Intelligence Agency (CIA) had supplied some of the guns used to kill the president.\n\nIn a report to the Deputy Attorney General of the United States, CIA officials described the agency as having \"no active part\" in the assassination and only a \"faint connection\" with the groups that planned the killing. but the internal CIA investigation, by its Inspector General, \"disclosed quite extensive Agency involvement with the plotters.\"\n\nAccording to an 86-page portion of the Rockefeller Commission investigating the CIA's domestic activities that was removed at the behest of the Ford administration and did not become public until 2016:\n\n[Richard] Bissell [Deputy Director of Plans for the CIA] also testified that there was discussion within the Agency of the possibility of an attempt on the life of President Achmed Sukarno of Indonesia which \"progressed as far as the identification of an asset who it was felt might be recruited for this purpose. The plan was never reached, was never perfected to the point where it seemed feasible.\" He said the Agency had \"absolutely nothing\" to do with the death of Sukarno. ... He stated that no assassination plans would have been undertaken without authorization outside the Agency, and that no such authorization was undertaken for plans against either Lumumba or Sukarno. ... Since no evidence was found to bring the Sukarno and Lumumba matters within the scope of the Commission's investigative authority, no further investigation in these two areas was undertaken.\nIn 1984, a CIA manual for training the Nicaraguan Contras in psychological operations and unconventional warfare, entitled \"Psychological Operations in Guerrilla War\", became public. The manual recommended \"selective use of violence for propagandistic effects\" and to \"neutralize\" (i.e., kill) government officials. Nicaraguan Contras were taught to lead:\n\nThe manual also recommended:\n\nThe CIA claimed that the purpose of the manual was to \"moderate\" activities already being done by the Contras.\n\nIn May 2018, The European Court of Human Rights ruled that the countries of Romania and Lithuania were involved in torture activities perpetrated by the United States Central Intelligence Agency (CIA). The two countries were found to have run \"black site\" torture rooms and detention facilities therefore committing grave human rights abuses. In Romania, the secret facility was located at Northern Bucharest. In Lithuania, the black site was a guesthouse in the capital city of Vilnius which operated as early as 2002. In December 2014, the US Senate Select Committee on Intelligence filed a report which included torture details involving Saudi Arabian nationals Abd al-Rahim al-Nashiri suspected of masterminding the bombing of the US navy guided-missile destroyer USS \"Cole\" in Yemen and Abu Zubaydah, an alleged Al Qaeda terrorist. The European tribunal said the \"two high-value prisoners\" were \"subjected to maltreatment and arbitrary incarceration at the CIA black site. Hence, the Lithuania and Romanian governments violated the Eastern Convention of Human Rights' ban on torture. Authorities in Lithuania also permitted the transport of Zubaydah to a secret prison in Afghanistan for additional abuse. In a separate decision, the European court learned Romania was knowledgeable of Nashiri's torture by the CIA in its own country.\n\n"}
{"id": "19327282", "url": "https://en.wikipedia.org/wiki?curid=19327282", "title": "Hyperinteger", "text": "Hyperinteger\n\nIn non-standard analysis, a hyperinteger \"n\" is a hyperreal number that is equal to its own integer part. A hyperinteger may be either finite or infinite. A finite hyperinteger is an ordinary integer. An example of an infinite hyperinteger is given by the class of the sequence in the ultrapower construction of the hyperreals.\n\nThe standard integer part function: \nis defined for all real \"x\" and equals the greatest integer not exceeding \"x\". By the transfer principle of non-standard analysis, there exists a natural extension: \ndefined for all hyperreal \"x\", and we say that \"x\" is a hyperinteger if: \nThus the hyperintegers are the image of the integer part function on the hyperreals.\n\nThe set formula_4 of all hyperintegers is an internal subset of the hyperreal line formula_5. The set of all finite hyperintegers (i.e. formula_6 itself) is not an internal subset. Elements of the complement\n\nare called, depending on the author, \"non-standard\", \"unlimited\", or \"infinite\" hyperintegers. The reciprocal of an infinite hyperinteger is always an infinitesimal.\n\nNonnegative hyperintegers are sometimes called \"hypernatural\" numbers. Similar remarks apply to the sets formula_8 and formula_9. Note that the latter gives a non-standard model of arithmetic in the sense of Skolem.\n\n"}
{"id": "89532", "url": "https://en.wikipedia.org/wiki?curid=89532", "title": "Identity (philosophy)", "text": "Identity (philosophy)\n\nIn philosophy, identity, from (\"sameness\"), is the relation each thing bears only to itself. The notion of identity gives rise to many philosophical problems, including the identity of indiscernibles (if \"x\" and \"y\" share all their properties, are they one and the same thing?), and questions about change and personal identity over time (what has to be the case for a person \"x\" at one time and a person \"y\" at a later time to be one and the same person?).\n\nThe philosophical concept of identity is distinct from the more well-known notion of identity in use in psychology and the social sciences. The philosophical concept concerns a \"relation\", specifically, a relation that \"x\" and \"y\" stand in if, and only if they are one and the same thing, or \"identical to\" each other (i.e. if, and only if \"x\" = \"y\"). The sociological notion of identity, by contrast, has to do with a person's self-conception, social presentation, and more generally, the aspects of a person that make them unique, or qualitatively different from others (e.g. cultural identity, gender identity, national identity, online identity and processes of identity formation).\n\nMetaphysicians, and sometimes philosophers of language and mind, ask other questions:\nThe law of identity originates from classical antiquity. The modern formulation of identity is that of Gottfried Leibniz, who held that \"x\" is the same as \"y\" if and only if every predicate true of \"x\" is true of \"y\" as well.\n\nLeibniz's ideas have taken root in the philosophy of mathematics, where they have influenced the development of the predicate calculus as Leibniz's law. Mathematicians sometimes distinguish identity from equality. More mundanely, an \"identity\" in mathematics may be an \"equation\" that holds true for all values of a variable. Hegel argued that things are inherently self-contradictory and that the notion of something being self-identical only made sense if it were not also not-identical or different from itself and did not also imply the latter. In Hegel's words, \"Identity is the identity of identity and non-identity.\" More recent metaphysicians have discussed trans-world identity—the notion that there can be the same object in different possible worlds. An alternative to trans-world identity is the counterpart relation in Counterpart theory. It is a similarity relation that rejects trans-world individuals and instead defends an objects counterpart - the most similar object.\n\nSome philosophers have denied that there is such a relation as identity. Thus Ludwig Wittgenstein writes (\"Tractatus\" 5.5301): \"That identity is not a relation between objects is obvious.\" At 5.5303 he elaborates: \"Roughly speaking: to say of two things that they are identical is nonsense, and to say of one thing that it is identical with itself is to say nothing.\" Bertrand Russell had earlier voiced a worry that seems to be motivating Wittgenstein's point (\"The Principles of Mathematics\" §64): \"[I]dentity, an objector may urge, cannot be anything at all: two terms plainly are not identical, and one term cannot be, for what is it identical with?\" Even before Russell, Gottlob Frege, at the beginning of \"On Sense and Reference,\" expressed a worry with regard to identity as a relation: \"Equality gives rise to challenging questions which are not altogether easy to answer. Is it a relation?\" More recently, C. J. F. Williams has suggested that identity should be viewed as a second-order relation, rather than a relation between objects, and Kai Wehmeier has argued that appealing to a binary relation that every object bears to itself, and to no others, is both logically unnecessary and metaphysically suspect.\n\nKind-terms, or sortals give a criterion of identity and non-identity among items of their kind.\n\n\n\n"}
{"id": "27362878", "url": "https://en.wikipedia.org/wiki?curid=27362878", "title": "Isadora (software)", "text": "Isadora (software)\n\nIsadora is a proprietary graphic programming environment for Mac OS X and Microsoft Windows, with emphasis on real-time manipulation of digital video. It was first released in 2002. It has support for Open Sound Control and MIDI. Isadora was designed by Mark Coniglio.\n\n"}
{"id": "3210520", "url": "https://en.wikipedia.org/wiki?curid=3210520", "title": "Japanese aesthetics", "text": "Japanese aesthetics\n\nThe modern study of Japanese aesthetics only started a little over two hundred years ago in the West. The Japanese aesthetic is a set of ancient ideals that include \"wabi\" (transient and stark beauty), \"sabi\" (the beauty of natural patina and aging), and \"yūgen\" (profound grace and subtlety). These ideals, and others, underpin much of Japanese cultural and aesthetic norms on what is considered tasteful or beautiful. Thus, while seen as a philosophy in Western societies, the concept of aesthetics in Japan is seen as an integral part of daily life. Japanese aesthetics now encompass a variety of ideals; some of these are traditional while others are modern and sometimes influenced by other cultures.\n\nShinto is considered to be at the fountain-head of Japanese culture. With its emphasis on the wholeness of nature and character in ethics, and its celebration of the landscape, it sets the tone for Japanese aesthetics. Nevertheless, Japanese aesthetic ideals are most heavily influenced by Japanese Buddhism. In the Buddhist tradition, all things are considered as either evolving from or dissolving into nothingness. This \"nothingness\" is not empty space. It is rather a space of potentiality. If the seas represent potential then each thing is like a wave arising from it and returning to it. There are no permanent waves. At no point is a wave complete, even at its peak. Nature is seen as a dynamic whole that is to be admired and appreciated. This appreciation of nature has been fundamental to many Japanese aesthetic ideals, \"arts,\" and other cultural elements. In this respect, the notion of \"art\" (or its conceptual equivalent) is also quite different from Western traditions (see Japanese art).\n\nWabi and sabi refers to a mindful approach to everyday life. Over time their meanings overlapped and converged until they are unified into \"Wabi-sabi\", the aesthetic defined as the beauty of things \"imperfect, impermanent, and incomplete\". Things in bud, or things in decay, as it were, are more evocative of wabi-sabi than things in full bloom because they suggest the transience of things. As things come and go, they show signs of their coming or going, and these signs are considered to be beautiful. In this, beauty is an altered state of consciousness and can be seen in the mundane and simple. The signatures of nature can be so subtle that it takes a quiet mind and a cultivated eye to discern them. In Zen philosophy there are seven aesthetic principles for achieving Wabi-Sabi.\n\nFukinsei (不均斉): asymmetry, irregularity;\n\nKanso (簡素): simplicity;\n\nKoko (考古): basic, weathered;\n\nShizen (自然): without pretense, natural;\n\nYugen (幽玄): subtly profound grace, not obvious;\n\nDatsuzoku (脱俗): unbounded by convention, free;\n\nSeijaku (静寂): tranquility, silence.\nEach of these things are found in nature but can suggest virtues of human character and appropriateness of behaviour. This, in turn suggests that virtue and civility can be instilled through an appreciation of, and practice in, the arts. Hence, aesthetic ideals have an ethical connotation and pervades much of the Japanese culture.\n\nMiyabi (雅) is one of the oldest of the traditional Japanese aesthetic ideals, though perhaps not as prevalent as Iki or Wabi-sabi. In modern Japanese, the word is usually translated as \"elegance,\" \"refinement,\" or \"courtliness\" and sometimes referred to as \"heart-breaker\".\n\nThe aristocratic ideal of Miyabi demanded the elimination of anything that was absurd or vulgar and the \"polishing of manners, diction, and feelings to eliminate all roughness and crudity so as to achieve the highest grace.\" It expressed that sensitivity to beauty which was the hallmark of the Heian era. Miyabi is often closely connected to the notion of Mono no aware, a bittersweet awareness of the transience of things, and thus it was thought that things in decline showed a great sense of miyabi.\n\nShibui (渋い) (adjective), shibumi (渋み) (noun), or shibusa (渋さ) (noun) are Japanese words which refer to a particular aesthetic or beauty of simple, subtle, and unobtrusive beauty. Originating in the Muromachi period (1336–1392) as shibushi, the term originally referred to a sour or astringent taste, such as that of an unripe persimmon. Shibui maintains that literal meaning still, and remains the antonym of amai (甘い), meaning 'sweet'. Like other Japanese aesthetic terms, such as iki and wabi-sabi, shibui can apply to a wide variety of subjects, not just art or fashion. Shibusa includes the following essential qualities. (1) Shibui objects appear to be simple overall but they include subtle details, such as textures, that balance simplicity with complexity. (2) This balance of simplicity and complexity ensures that one does not tire of a shibui object but constantly finds new meanings and enriched beauty that cause its aesthetic value to grow over the years. (3) Shibusa is not to be confused with wabi or sabi. Though many wabi or sabi objects are shibui, not all shibui objects are wabi or sabi. Wabi or sabi objects can be more severe and sometimes exaggerate intentional imperfections to such an extent that they can appear to be artificial. Shibui objects are not necessarily imperfect or asymmetrical, though they can include these qualities. (4) Shibusa walks a fine line between contrasting aesthetic concepts such as elegant and rough or spontaneous and restrained.\n\nIki (いき, often written 粋) is a traditional aesthetic ideal in Japan. The basis of iki is thought to have formed among urbane mercantile class (Chōnin) in Edo in the Tokugawa period (1603–1868). Iki is an expression of simplicity, sophistication, spontaneity, and originality. It is ephemeral, straightforward, measured, and unselfconscious. Iki is not overly refined, pretentious, complicated. Iki may signify a personal trait, or artificial phenomena exhibiting human will or consciousness. Iki is not used to describe natural phenomena, but may be expressed in human appreciation of natural beauty, or in the nature of human beings. The phrase \"iki\" is generally used in Japanese culture to describe qualities that are aesthetically appealing and when applied to a person, what they do, or have, constitutes a high compliment. \"Iki\" is not found in nature. While similar to \"wabi-sabi\" in that it disregards perfection, \"iki\" is a broad term that encompasses various characteristics related to refinement with flair. The tasteful manifestation of sensuality can be \"iki\". Etymologically, iki has a root that means pure and unadulterated. However, it also carries a connotation of having an appetite for life.\n\nJo-ha-kyū (序破急) is a concept of modulation and movement applied in a wide variety of traditional Japanese arts. Roughly translated to \"beginning, break, rapid\", it implies a tempo that begins slowly, accelerates, and then ends swiftly. This concept is applied to elements of the Japanese tea ceremony, to kendō, to the traditional theatre, to Gagaku, and to the traditional collaborative linked verse forms renga and renku (haikai no renga).\n\n is an important concept in traditional Japanese aesthetics. The exact translation of the word depends on the context. In the Chinese philosophical texts the term was taken from, yūgen meant \"dim\", \"deep\" or \"mysterious\". In the criticism of Japanese waka poetry, it was used to describe the subtle profundity of things that are only vaguely suggested by the poems, and was also the name of a style of poetry (one of the ten orthodox styles delineated by Fujiwara no Teika in his treatises).\n\nYūgen suggests that which is beyond what can be said, but it is not an allusion to another world. It is about this world, this experience. All of these are portals to yūgen:\n\n\"To watch the sun sink behind a flower clad hill.\nTo wander on in a huge forest without thought of return.\nTo stand upon the shore and gaze after a boat that disappears behind distant islands.\nTo contemplate the flight of wild geese seen and lost among the clouds.\nAnd, subtle shadows of bamboo on bamboo.\" Zeami Motokiyo\n\nZeami was the originator of the dramatic art form Noh theatre and wrote the classic book on dramatic theory (Kadensho). He uses images of nature as a constant metaphor. For example, \"snow in a silver bowl\" represents \"the Flower of Tranquility\". Yūgen is said to mean \"a profound, mysterious sense of the beauty of the universe ... and the sad beauty of human suffering\". It is used to refer to Zeami's interpretation of \"refined elegance\" in the performance of Noh.\n\n refers to the various traditional Japanese arts disciplines: (theater), (Japanese flower arrangement), (Japanese calligraphy), (Japanese tea ceremony), and (Japanese pottery). All of these disciplines carry an ethical and aesthetic connotation and teach an appreciation of the process of creation. To introduce discipline into their training, Japanese warriors followed the example of the arts that systematized practice through prescribed forms called kata - think of the tea ceremony. Training in combat techniques incorporated the way of the arts (Geidō), practice in the arts themselves, and instilling aesthetic concepts (for example, yugen) and the philosophy of arts (geido ron). This led to combat techniques becoming known as the martial arts (even today, David Lowry shows, in the 'Sword and Brush: the spirit of the martial arts', the affinity of the martial arts with the other arts). All of these arts are a form of tacit communication and we can, and do, respond to them by appreciation of this tacit dimension.\n\nEnsō (円相) is a Japanese word meaning \"circle\". It symbolizes the Absolute, enlightenment, strength, elegance, the Universe, and the void; it also may be taken to symbolize the Japanese aesthetic itself. Zen Buddhist calligraphists may \"believe that the character of the artist is fully exposed in how she or he draws an ensō. Only a person who is mentally and spiritually complete can draw a true ensō. Some artists will practice drawing an ensō daily, as a kind of spiritual exercise.\"\n\nBecause of its nature, Japanese aesthetics has a wider relevance than is usually accorded to aesthetics in the West. In her pathmaking book, Eiko Ikegami reveals a complex history of social life in which aesthetic ideals become central to Japan's cultural identities. She shows how networks in the performing arts, the tea ceremony, and poetry shaped tacit cultural practices and how politeness and politics are inseparable. She contends that what in Western cultures are normally scattered, like art and politics, have been, and are, distinctly integrated in Japan.\n\nAfter the introduction of Western notions in Japan, Wabi Sabi aesthetics ideals have been re-examined with Western values, by both Japanese and non-Japanese. Therefore, recent interpretations of the aesthetics ideals inevitably reflect Judeo-Christian perspectives and Western philosophy.\n\nMany traditional Japanese aesthetic criteria are manifest in, and discussed as part of, diverse elements of Japanese cuisine; see kaiseki for a refined expression.\n\nA modern phenomenon, since the 1970s cuteness or in Japanese has become a prominent aesthetic of Japanese popular culture, entertainment, clothing, food, toys, personal appearance, behavior, and mannerisms.\n\nAs a cultural phenomenon, cuteness is increasingly accepted in Japan as a part of Japanese culture and national identity. Tomoyuki Sugiyama, author of \"Cool Japan\", believes that \"cuteness\" is rooted in Japan's harmony-loving culture, and Nobuyoshi Kurita, a sociology professor at Musashi University in Tokyo, has stated that \"cute\" is a \"magic term\" that encompasses everything that's acceptable and desirable in Japan.\n\n"}
{"id": "22369048", "url": "https://en.wikipedia.org/wiki?curid=22369048", "title": "Judicial corporal punishment", "text": "Judicial corporal punishment\n\nJudicial corporal punishment (JCP) is the infliction of corporal punishment as a result of a sentence by a court of law. The punishments include caning, bastinado, birching, whipping, or strapping. The practice was once commonplace in many countries, but it is no longer practised in any European country, and it has now been abolished in most Western countries, but remains a legal punishment in some Asian, African and Middle Eastern countries.\n\nThe Singaporean official punishment of caning became much discussed around the world in 1994 when an American teenager, Michael Fay, was sentenced to six strokes of the cane for vandalism.\n\nOther former British colonies with judicial caning currently on their statute books include Barbados, Botswana, Brunei, Swaziland, Tonga, Trinidad & Tobago, and Zimbabwe.\n\nMany Muslim-majority countries use judicial corporal punishment, such as United Arab Emirates, Qatar, Saudi Arabia, Iran, northern Nigeria, Sudan and Yemen, employ judicial whipping or caning for a range of offences. In Indonesia (Aceh province only) it has recently been introduced for the first time.\n\nA list of 33 countries that use lawful, official JCP today is as follows:\n\n\nThe above list does not include countries where a \"blind eye\" is sometimes turned to unofficial JCP by local tribes, authorities, etc. including Bangladesh, and Colombia.\n\nThe last birching sentence in Jersey was carried out in 1966, and abandoned as a policy in 1969 but lingered on the statute books. Obsolete references to corporal punishment were removed from remaining statutes by the \"Criminal Justice (Miscellaneous Provisions) (No. 2) (Jersey) Law 2007\"\n\nThe last birching sentence was carried out in 1968. The \"Corporal Punishment (Guernsey) Law, 1957\" was finally repealed by the \"Criminal Justice (Miscellaneous Provisions) (Bailiwick of Guernsey) Law, 2006\"\n\nIt was abolished in the Isle of Man after the judgment in \"Tyrer v. UK\" by the European Court of Human Rights. Judicial birching was abolished in 2000 (the last was in January 1976; the last caning was a 13-year-old boy, who was convicted of robbing another child of 10p, was the last recorded juvenile case in May 1971)\n\nIn 1854 judicial corporal punishment was abolished in the Netherlands with the exception of whipping. Whipping itself was abolished in 1870.\n\nIn the Wetboek van Strafrecht, article 9, this kind of punishment is not listed as primary or secondary punishment. Mainly because of human rights and/or human dignity, corporal punishment has been abolished and does not exist at this time.\n\nThe Constitutional Court decided in 1995 in the case of \"S v Williams and Others\" that caning of juveniles was unconstitutional. Although the ruling in \"S v Williams\" was limited to the corporal punishment of males under the age of 21, Justice Langa mentioned in \"dicta\" that there was a consensus that corporal punishment of adults was also unconstitutional.\n\nThe Abolition of Corporal Punishment Act, 1997 abolished judicial corporal punishment.\n\nIn the United Kingdom, JCP generally was abolished in 1948; however, it persisted in prisons as a punishment for prisoners committing serious assaults on prison staff (ordered by prison's visiting justices) until it was abolished by s 65 (Abolition of corporal punishment in prison) of the Criminal Justice Act 1967 (the last ever prison flogging was in 1962).\n\nAmerican colonies judicially punished in a variety of forms, including whipping, stocks, the pillory and the ducking stool. In the seventeenth and eighteenth centuries, whipping posts were considered indispensable in American and English towns. Starting in 1776, Gen. George Washington strongly advocated and utilized JCP in the Continental Army, with due process protection, obtaining in 1776 authority from the Continental Congress to impose 100 lashes, more than the previous limit of 39. In his 1778 Bill for Proportioning Crimes and Punishments, Thomas Jefferson provided up to 15 lashes for individuals pretending to witchcraft or prophecy, at the jury’s discretion; castration for men guilty of rape, polygamy or sodomy, and a minimum half-inch hole bored in the nose cartilage of women convicted of those sex crimes. In 1781, Washington requested legal authority from the Continental Congress to impose up to 500 lashes, as there was still a punishment gap between 100 lashes and the death penalty. The Founders believed whipping and other forms of corporal punishment effectively promoted pro-social and discouraged anti-social behavior. Two later presidents, Abraham Lincoln and Theodore Roosevelt, advocated judicial corporal punishment as punishment for wife-beating.\n\nIn the United States judicial flogging was last used in 1952 in Delaware when a wife beater got 20 lashes. In Delaware, the criminal code permitted floggings to occur until 1972. One of the major objections to judicial corporal punishment in the United States was that it was unpleasant to administer.\n\nJudicial corporal punishment has never been held unconstitutional in the United States. In fact, the Fifth Amendment specifically mentions amputation of limbs as a possible sentence for offenders, perhaps referencing Thomas Jefferson's proposed legislation punishing rapists.\n\nIt was removed from the statute book in Canada in 1972, in India in the 1950s, in New Zealand in 1941, and in Australia at various times in the 20th century according to State.\n\nIt has been abolished in recent decades in Hong Kong, Jamaica, Kenya, Sri Lanka, and Zambia.\n\nOther countries that were neither former British territories nor Islamic states that have used JCP in the more distant past include China, Germany, Korea, Sweden and Vietnam.\n\n"}
{"id": "26784161", "url": "https://en.wikipedia.org/wiki?curid=26784161", "title": "Manhattan address algorithm", "text": "Manhattan address algorithm\n\nThe Manhattan address algorithm refers to the formulas used to estimate the closest east–west cross street for building numbers on north–south avenues in the New York City borough of Manhattan.\n\nTo find the approximate number of the closest cross street, divide the building number by a divisor (generally 20) and add (or subtract) the \"magic number\" from the table below:\n"}
{"id": "5461188", "url": "https://en.wikipedia.org/wiki?curid=5461188", "title": "McMahon system tournament", "text": "McMahon system tournament\n\nA McMahon system tournament is a tournament design for games such as go and chess that improves upon the Swiss system tournament's rules. It can be understood as a generalization of the Swiss system. \n\nAs in a Swiss tournament, all players compete in the same number of rounds against various other players. Unlike Swiss, the players do not all start with zero points, but are awarded initial points based on their rating prior to the tournament. This gives starting advantage to higher rated players, but they will play tougher opponents from the very start. The system features an \"upper bar\", set to a specific rating, so that all players rated above that are considered to have a chance to win the tournament, and start with the same number of points.\n\nMcMahon pairing matches players in each round against opponents that have equal or almost equal numbers of points so far. Players gain a point for each round they win or half a point for a draw. The player with the highest number of points after the last round is the tournament winner. Some set of tie-breaking rules must be chosen in advance to be applied if two or more players achieve the same number of points.\n\nThe system is named after Lee McMahon of Bell Labs, and was originally used as a club ranking system at the New York Go Club. It was then adopted for Go tournaments in Britain, and has since become the most popular tournament system used in Go. Use of the McMahon system does not determine policy on other tournament questions such as whether to pair players from the same club, whether to use accelerated pairings, whether Go games should be even or handicap, etc.\n\nThe advantage of the McMahon system over the Swiss system is that it requires fewer rounds to find a winner, and that it avoids extreme match-ups (very strong players against very weak players) in the earlier rounds. By matching up possible tournament winners earlier, the system allows for more games amongst this group, and thus improves sampling. In other words, the sorting effect of wins and losses propelling players up and down is applied all along to refine what is believed to be a pretty good initial ordering, rather than being wasted merely to bring similarly skilled players together into roughly sorted order, and thus the McMahon system achieves a more accurate final sorted order. It also delivers better playing experiences to players at all levels.\n\n"}
{"id": "36690595", "url": "https://en.wikipedia.org/wiki?curid=36690595", "title": "Melvin L. Morse", "text": "Melvin L. Morse\n\nMelvin L. Morse is an American medical doctor who specialized in pediatrics. He was voted by his peers as one of \"America's Best Doctors\" in 1997–1998, 2001–2002, and 2005–2006. He has published numerous scientific articles in medical journals over the course of his thirty-year career. As the author of several books, Morse has appeared on many talk show and television programs to discuss his extensive research on near-death experiences in children.\n\nHis 1991 book \"Closer to the Light\" was a bestseller. Oprah Winfrey interviewed Morse about this book in 1992. Larry King interviewed Morse in 2010. The PBS show \"Upon Reflection\" produced a half-hour episode devoted to Morse. He was the subject of an article in the \"Rolling Stone\" magazine in 2004 entitled \"In search of the Dead Zone\".\n\nIn 2012, Morse and his second wife were charged with felony child endangerment based on allegations made by his eleven-year-old step-daughter. During Morse's 2014 trial, he was dubbed as the \"waterboarding doctor\" by the media. Trial testimony did not substantiate any instances of \"waterboarding\" as the term is generally understood; however, Morse was convicted of reckless endangerment and was sentenced to serve three years in prison.\n\nMorse was released from Sussex County Correctional Institution (SSCI) in 2016. According to Kahlil Peterkin, Clinical Supervisor of the prison's therapeutic Key program and Morse's therapist of thirteen months, Morse underwent a transformation while incarcerated. Peterkin also reported Morse was well-respected by his fellow inmates and considered a leader in prison. He taught fellow inmates meditation techniques which Peterkin also described as 'transformative\" in the lives of the men who learned and practiced them.\n\nFollowing his release, Morse co-founded \"The Recidivism Prevention Group\", a company dedicated to assisting addicts and former inmates in developing spiritual understandings to re-enter society as productive members. The group uses meditation techniques to accomplish these goals. Morse now resides in Washington, DC.\n\nMorse graduated from Johns Hopkins University in Baltimore, Maryland in 1975 with a Bachelor of Arts degree in Natural Science. Morse earned a medical degree from George Washington University in Washington, D.C. in 1980. He interned in Pediatrics at the University of California at San Francisco, and then completed a residency in Pediatrics at Seattle Children's Hospital. He subsequently completed a two-year fellowship in Hematology/Oncology and a one-year fellowship in Behavioral Pediatrics.\n\nMorse practiced Pediatrics in Renton, Washington for 20 years. He was an Associate Professor of Pediatrics at the University of Washington in Seattle. In 1986, Morse worked for a year as a pediatrician at Fort Hall, Idaho for the Indian Health Service. He retired from the full-time practice of Pediatrics in 2006 before moving to Delaware in 2007. Prior to his arrest, he was working as a pediatrician at an office in Milton, Delaware. After his arrest on child endangerment charges in 2012, his Delaware license was suspended.\n\nIn 2007, Morse became the Research Director of the Institute for the Scientific Study of Consciousness (ISSC) founded by Charles Tart in 1979. While Director of ISSC, he was awarded the Warcollier International Prize for consciousness research in 2011.\n\n\nMorse has been married twice, and has six children, five of whom are adopted. He is in regular contact all his children. Delaware Family Court has not restricted his access to his daughter Melody.\n\nIn August 2012, Morse and his wife Pauline were arrested for felony child endangerment based on allegations of \"waterboarding\" made by his eleven-year-old step-daughter. He was also accused by Delaware State Police of force-feeding the eleven-year-old girl until she threw up, among other child abuses.\n\nMorse's trial started on January 28, 2014 at the Sussex County Superior Court in Delaware. The accusation made by the State Police that Morse had force-fed his step-daughter was not substantiated at trial. The step-daughter testified at trial that she vomited due to overeating, not force feeding.\n\nMorse placed her in the bathtub and ran water over her head to rinse the vomit out of her hair. This was the testimony which spawned the term \"waterboarding\" by the prosecution and the media. The trial transcripts contain no testimony regarding any instances of \"waterboarding\" in the context of torture as it is known today.\n\nMorse was convicted of one felony count of first-degree reckless endangerment (for holding his step-daughter's head under the faucet of a bathtub) and five misdemeanor charges. He was sentenced to a five-year sentence, with three years to be served in prison and two on probation. His wife pled guilty to misdemeanor charges.\n"}
{"id": "42542157", "url": "https://en.wikipedia.org/wiki?curid=42542157", "title": "Murder of Stacey Mitchell", "text": "Murder of Stacey Mitchell\n\nStacey Mitchell (1990 – 18 December 2006) was a British-born girl living in Australia who was murdered at the age of 16, on 18 December 2006, by lesbian couple Jessica Stasinowsky (born 1985) and Valerie Parashumti (born 1987). She was bludgeoned with a concrete block and strangled with a chain. Her corpse was found in a wheelie bin shortly afterwards. Stasinowsky and Parashumti had known Mitchell for three days, and claimed they murdered her because they found her irritating.\n\nStacey Mitchell was born in Dorset, England, but emigrated to Australia at the age of 10. At the age of 16, she ran away from her family home and stayed with 19-year-old Parashumti and 21-year-old Stasinowsky. \n\nParashumti was accused by Stasinowsky of flirting with Mitchell. She planned to kill Mitchell to prove to Stasinowsky she was not attracted to her. The three of them spent the evening drinking whisky before Parashumti hit Mitchell over the back of the head with a concrete block. Stasinowsky then strangled her with a dog chain. As she lay dying, the two women kissed over her body and filmed her on a mobile phone. Earlier attempts to kill Mitchell by Stasinowsky included putting broken glass in her drink and spilling oil on the bathroom floor.\n\nThe two women were tried at the Perth Supreme Court. During the trial they \"smiled, giggled and whispered at each other\". Parashumti's lawyer stated that she had a severe personality disorder and had grown up with an abusive father. She had also had an obsession with the vampire sub-culture, and a penchant for drinking people's blood since the age of 10. Parashumti's father had previously been in prison for beating his wife. \n\nBoth women pleaded guilty, and revealed that after Mitchell's death, they had visited a hardware store to look for a chainsaw and some spades. In 2008, both women were sentenced to strict security life imprisonment, with a 24-year minimum term. Stasinowsky subsequently appealed her sentence. Her appeal was unanimously rejected by the WA Court of Appeal. David Ross John Haynes, who also lived with them, was sentenced to two years' imprisonment for being an accessory. Although he had been fully aware of the plan to kill Mitchell, he agreed to go to his bedroom and listen to music.\n\nIt was discovered in 2009 that the two women were continuing their relationship in prison, reportedly spending up to 90 minutes together on weekdays, and seven hours a day on weekends. Parashumti was consequently moved to a separate prison. They attempted to maintain contact by sending letters, using fellow inmate Catherine Birnie as a go-between. In 2012, three prison officers were injured, following their intervention in a fight involving Parashumti and another inmate. In 2013 Parashumti attempted to escape from prison, but failed to leave the grounds. It was reported she would consequently face a police charge.\n\nIn 2014, 20 charges against Stasinowsky, which pre-dated the murder, were dropped.\n"}
{"id": "5504424", "url": "https://en.wikipedia.org/wiki?curid=5504424", "title": "Number Forms", "text": "Number Forms\n\nNumber Forms is a Unicode block containing characters that have specific meaning as numbers, but are constructed from other characters. They consist primarily of vulgar fractions and Roman numerals. In addition to the characters in the Number Forms block, three fractions were inherited from ISO-8859-1, which was incorporated whole as the Latin-1 supplement block.\n\nThe following Unicode-related documents record the purpose and process of defining specific characters in the Number Forms block:\n\n"}
{"id": "23398732", "url": "https://en.wikipedia.org/wiki?curid=23398732", "title": "PC power management", "text": "PC power management\n\nPC power management refers to the mechanism for controlling the power use of personal computer hardware. This is typically through the use of software that puts the hardware into the lowest power demand state available. It is an aspect of Green computing.\n\nA typical office PC might use on the order of 90 watts when active (approximately 50 watts for the base unit, and 40 watts for a typical LCD screen); and three to four watts when ‘asleep’. Up to 10% of a modern office’s electricity demand might be due to PCs and monitors.\n\nWhile some PCs allow low power settings, there are many situations, especially in a networked environment, where processes running on the computer will prevent the low power settings from taking effect. This can have a dramatic effect on energy use that is invisible to the user. The monitor may have gone into standby mode, and the PC may appear to be idle, but operational testing has shown that on any given day an average of over 50% of an organisation's computers would fail to go to sleep, and over time this happened to over 90% of the machines.\n\nThe Windows power management system is based upon an idle timer. If the computer is idle for longer than the preset timeout then the PC may be configured to sleep or hibernate. The user may configure the timeout using the Control Panel. Windows uses a combination of user activity and CPU activity to determine when the computer is idle.\n\nApplications can temporarily inhibit this timer by using the \"SetThreadExecutionState\" API. There are legitimate reasons why this may be necessary such as burning a DVD or playing a video. However, in many cases applications can unnecessarily prevent power management from working. This is commonly known as Windows 'Insomnia' and can be a significant barrier to successfully implementing power management.\n\nCommon causes of 'insomnia' include:\n\n\nOperating systems have built-in settings to control power use. Microsoft Windows supports predefined power plans and custom sleep and hibernation settings through a Control Panel Power Options applet. Apple's OS X includes idle and sleep configuration settings through the Energy Saver System Preferences applet. Likewise, Linux distributions include a variety of power management settings and tools.\n\nThere is a significant market in third-party PC power management software offering features beyond those present in the Windows operating system. Products are targeted at enterprise environments offering Active Directory integration and per-user/per-machine settings with the more advanced offering multiple power plans, scheduled power plans, anti-insomnia features and enterprise power usage reporting. Notable vendors include 1E NightWatchman, Data Synergy PowerMAN (Software), Faronics Power Save, Verdiem SURVEYOR. and EnviProt Auto Shutdown Manager\n\nUsing this type of energy management tool on an organisation's network has been demonstrated to save on average 200 kg of CO emissions per PC per year, and generate $36 per PC per year in energy savings. An organisation such as a large office, with 4,000 employees, would be able to make CO emissions savings of over 800 tonnes and about $140,000 per year in energy savings. A large corporation with 50,000 employees could save 10,000 tonnes of CO and about $1.7 million per year in energy costs.\n\n"}
{"id": "4927850", "url": "https://en.wikipedia.org/wiki?curid=4927850", "title": "Panrationalism", "text": "Panrationalism\n\nPanrationalism (or comprehensive rationalism) holds two premises true:\n\n\nThe first problem that needs to be dealt with is: what is the rational criterion or authority to which they appeal? Here the panrationalists diverge into two groups:\n\nDescartes is considered the founder of rationalism and gave the illustration \"cogito ergo sum\" as the paradigm to demonstrate what he believed.\n\nThe problem of both these appeals is that:\n\nIn his \"The Critique of Pure Reason\" Kant sought to reconcile both appeals.\n\n\n"}
{"id": "14107803", "url": "https://en.wikipedia.org/wiki?curid=14107803", "title": "Participation (philosophy)", "text": "Participation (philosophy)\n\nIn philosophy, participation is the inverse of inherence.\n\nAccidents are said to \"inhere\" in substance. Substances, in turn, \"participate\" in their accidents. For example, the color red is said to inhere in the red apple. Conversely, the red apple participates in the color red.\n\nParticipation also is predicated by analogy to a dependence relations between accidents. Thus an act may be said to participate in time in the sense that every act must occur at some time. In a similar way, color may be said to inhere in space, meaning that a color occurs only on the surface of a body—and thus only in space.\n\nInherence, on the other hand, would not normally be predicated analogously of accidents.\n\n"}
{"id": "63149", "url": "https://en.wikipedia.org/wiki?curid=63149", "title": "Pastry War", "text": "Pastry War\n\nThe Pastry War (, ), also known as the First French intervention in Mexico or the First Franco-Mexican War (1838–1839), began in November 1838 with the naval blockade of some Mexican ports and the capture of the fortress of San Juan de Ulúa in Veracruz by French forces sent by King Louis-Philippe. It ended several months later in March 1839 with a British-brokered peace. The intervention followed many claims by French nationals of losses due to unrest in Mexico.\n\nThis incident was the first and lesser of Mexico's two 19th-century wars with France, being followed by the French invasion of 1861–67 which supported the short reign of Emperor Maximilian I of Mexico who was executed by firing squad at the end of that later conflict.\n\nDuring the early years of the new Mexican republic there was widespread civil disorder as factions competed for control of the country. The fighting often resulted in the destruction or looting of private property. Average citizens had few options for claiming compensation as they had no representatives to speak on their behalf. Foreigners whose property was damaged or destroyed by rioters or bandits were usually also unable to obtain compensation from the Mexican government and they began to appeal to their own governments for help and compensation.\n\nCommercial relationships between France and Mexico existed prior to France's recognition of Mexico's independence in 1830, and after the establishment of diplomatic relationships France rapidly became Mexico's third largest trade partner. However, France had yet to secure trade agreements similar to those that the United States and Great Britain (then Mexico's two largest trade partners) had, and as a result of this French goods were subject to higher taxes\n\nIn a complaint to King Louis-Philippe, a French pastry chef known only as Monsieur Remontel claimed that in 1832 Mexican officers looted his shop in Tacubaya (then a town on the outskirts of Mexico City). Remontel demanded 60,000 pesos as reparations for the damage (his shop was valued at less than 1,000 pesos).\n\nIn view of Remontel's complaint (which gave its name to the ensuing conflict) and of other complaints from French nationals (among them the looting in 1828 of French shops at the Parian market and the execution in 1837 of a French citizen accused of piracy) in 1838 prime minister Louis-Mathieu Molé demanded from Mexico the payment of 600,000 pesos (3 million Francs) in damages, an enormous sum for the time, when the typical daily wage in Mexico City was about one peso (8 Mexican reals).\n\nWhen president Anastasio Bustamante made no payment, the King of France ordered a fleet under Rear Admiral Charles Baudin to declare and carry out a blockade of all Mexican ports on the Atlantic coast from Yucatán to the Rio Grande, to bombard the Mexican fortress of San Juan de Ulúa, and to seize the city of Veracruz, which was the most important port on the Gulf coast. French forces captured Veracruz by December 1838 and Mexico declared war on France.\n\nWith trade cut off, the Mexicans began smuggling imports via Corpus Christi, Republic of Texas and into Mexico. Fearing that France would blockade the Republic's ports as well, a battalion of Texan forces began patrolling Corpus Christi Bay to stop Mexican smugglers. One smuggling party abandoned their cargo of about a hundred barrels of flour on the beach at the mouth of the bay, thus giving Flour Bluff its name. The United States, ever watchful of its relations with Mexico, sent the schooner \"Woodbury\" to help the French in their blockade \n\nMeanwhile, acting without explicit government authority, Antonio López de Santa Anna, known for his military leadership, came out of retirement from his hacienda near Xalapa and surveyed the defenses of Veracruz. He offered his services to the government, which ordered him to fight the French by any means necessary. He led Mexican forces against the French. In a skirmish with the rear guard of the French, Santa Anna was wounded in the leg by French grapeshot. His leg was amputated and buried with full military honors. Exploiting his wounds with eloquent propaganda, Santa Anna catapulted back to power.\n\nThe French forces withdrew on 9 March 1839 after a peace treaty was signed. As part of said treaty the Mexican government agreed to pay 600,000 pesos as damages to French citizens while France received promises for future trade commitments in place of war indemnities. However, this amount was never paid and that was later used as one of the justifications for the second French intervention in Mexico of 1861.\n\nFollowing the Mexican victory in 1867 and the collapse of the second French empire in 1870, Mexico and France would not resume diplomatic relationships until 1880 when both countries left behind claims related to the wars.\n"}
{"id": "30871572", "url": "https://en.wikipedia.org/wiki?curid=30871572", "title": "Peace Brigades International", "text": "Peace Brigades International\n\nPeace Brigades International (PBI) is a non-governmental organization founded in 1981 which \"protects human rights and promotes non-violent transformation of conflicts\". It primarily does this by sending international volunteers to areas of conflict, who then provide protective, non-violent accompaniment to members of human rights organizations, unions, peasant groups and others that are threatened by political violence. PBI also facilitates other peace-building initiatives within conflict countries. They are a “nonpartisan” organization that does not interfere with the affairs of those they accompany.\n\nCurrently, in 2015, PBI has field projects in Colombia, Guatemala, Honduras, Indonesia, Kenya, Mexico and Nepal.\n\nInspired by the work of Shanti Sena in India, Peace Brigades International was founded in 1981 by a group of nonviolence activists, including Narayan Desai, George Willoughby, Charles Walker, Raymond Magee, Jamie Diaz and Murray Thomson. In 1983, during the Contra war, PBI sent a short-term peace team to Jalapa, Nicaragua positioning themselves between warring factions. This project was continued and expanded by Witness for Peace. The first long term PBI project was started that same year in Guatemala (1983–1999, re-initiated in 2003), followed by El Salvador (1987–1992), Sri Lanka (1989–1998), North America (1992–1999, in Canada and the USA), Colombia (since 1994), the Balkans (1994–2001, joint with other organizations), Haiti (1995–2000), Mexico (since 1998), Indonesia (1999–2011, and since 2015), Nepal (2005-2014), Kenya (since 2013) and Honduras (since 2013).\n\nIn 1989, PBI volunteers escorted Nobel Peace Prize winner Rigoberta Menchú on her first visit back to Guatemala from exile.\nOther individuals that PBI has protected include Amílcar Méndez, Nineth Montenegro and Frank LaRue in Guatemala; and Mario Calixto and Claudia Julieta Duque in Colombia.\n\nThe international protective accompaniment work that PBI developed and pioneered, has inspired similar work by many other organizations, including Witness for Peace, the Christian Peacemaker Teams, the Muslim Peacemaker Teams, Nonviolent Peaceforce, Protection International, the International Peace Observers Network and the Meta Peace Team.\n\nPBI is a team-based organization that uses consensus decision making. It is non-hierarchical in structure. There are three different aspects to the overall PBI structure, which are the Country Groups, the Field Projects, and the International Level (which consists of the PBI General Assembly, the International Council (IC), and the International Operations Council (IOC)). An international meeting is held every three years, that is attended by members from across the organization, to analyze and modify the direction of each country's program.\n\nPBI attracts volunteers from diverse backgrounds for its work in the field projects. Argentina, Australia, Austria, Bangladesh, Belgium, Bolivia, Brazil, Chile, Colombia, Czech Republic, Finland, France, Germany, Greece, Holland, Ireland, Italy, Mexico, Netherlands, Norway, Poland, Portugal, Romania, Slovenia, Spain, Sweden, Switzerland, the United Kingdom, and the United States — among many other countries — have all been represented among PBI's volunteer pool. Potential volunteers must be strongly committed to non-violence, and all applicants must attend in-depth training where they learn the philosophy of non-violence, non-violent strategies, and team dynamics. All volunteers must be fluent in Spanish for the Mexican, Guatemalan and Colombian projects, and all volunteers for the Nepalese program must be fluent in English and have a basic understanding of Nepali. An applicant may not be a citizen of the country they desire to work in, and must be able to make a minimum commitment of one year.\n\nApart from getting involved in the field projects, there is also the possibility for individuals to be able to volunteer in PBI’s country groups.\n\nPeace Brigades International has received a number of awards for its work, including \nthe Memorial Per la Pau \"Josep Vidal I Llecha\" (1989),\nthe Friedrich Siegmund-Schultze Förderpreis (1995, PBI-Germany),\nthe Memorial de la Paz y la Solidaridad Entre los Pueblos (1995),\nthe International Pfeffer Peace Prize (1996)\nthe Aachener International Peace Prize (1999),\nthe Medalla Comemorativa de la Paz (1999),\nthe Martin Ennals Award for Human Rights Defenders (2001, Colombia project),\nand the Jaime Brunet Prize (2011).\n\n\n\n\n"}
{"id": "27309055", "url": "https://en.wikipedia.org/wiki?curid=27309055", "title": "Proteak", "text": "Proteak\n\nProteak is a forestry company that cultivates teak trees on plantations located on reclaimed ranch lands in the dry tropical regions of Mexico and Latin America. Based out of Mexico City, Mexico, Proteak has satellite offices in Wimberley, Texas and Tepic, Mexico. At their manufacturing facilities, Proteak produces a range of teak products including: cutting boards, butcher blocks, decking, flooring and lumber.\n\nProteak Renewable Forestry, SAPIB was founded in 2000 by a group of investors in the US and Mexico. Forestry management operations began in 2000 with the planting of teak forests along the Pacific coast of Mexico. In 2006, Proteak began to offer its first commercial product lines.\n\nIn 2010, Proteak reported that having over 8,000 acres under cultivation on reclaimed ranch lands, while protecting roughly 2,000 of virgin tropical forests adjacent to those plantations. As part of the company's operating guidelines, Proteak preserves standing forests on the properties that it acquires. Proteak's sustainable practices have earned its plantations a Forest Stewardship Council certificate.\n\nProteak's IPO closed on June 30, 2010, selling approximately 79 million shares on the Mexican Stock Exchange.\n\nDue to the high demand for teak lumber, commercial forestry operations began to grow teak in dry tropical climates around the world throughout the 20th century. The deforestation of teak's natural range in Southeast Asia and trade sanctions imposed by the US Treasury Department, have not only served to drive the cost of teak higher, but have also led consumers to look for environmentally sustainable sources of tropical hardwood lumber. Plantation Teak can be cultivated sustainably and is typically priced substantially lower than old growth teak.\n\nAlthough the world's largest remaining teak forests are located inside Burma, the human rights violations perpetrated by the government of Myanmar have cause some consumers to refer to teak from the region as \"conflict teak.\"\n\nSome rumors suggest that teak trees grown on a plantation exhibit lower densities than those grown in Southeast Asia. However, studies conducted by the USDA have found no significant correlation between growth rate and its density.\n\nProteak's most visible product line include cutting boards, butcher blocks, counter tops and other kitchen accessories. The timber used to manufacture these products comes entirely from Proteak's FSC certified plantations.\n\nTeak is often used in kitchen applications because of its highly resilient characteristics compared to other hardwoods. Equipped with a naturally high level of oily resins called tectoquinones, teak wood has a unique ability to repel moisture, fungi, warping and rot. Plantation teak kitchen products are often popular with environmentally conscious chefs who prefer the material for its relatively small ecological footprint.\n\nCompared with other sustainable options like bamboo, teak cutting boards and butcher blocks are often considered to result in less wear on knife blades. End grain cutting boards showcase the tree's natural rings and hide knife marks and wear. Other manufacturing styles, such as edge grain or face grain are generally more affordable thanks to the efficiency of their construction.\n\nTeak lumber and decking make up another prominent portion of Proteak's commercial offerings through it subsidiary Teakyard. Boards cut from teak logs exhibit an attractive golden or brown complexion with a tight grain pattern. Often used in the construction of boats, patios, and outdoor furniture, teak lumber is known for its ability to resist rot, warping and cracking.\n\nPlantation teak lumber is also available at a significantly lower price than teak imported from Burma's old-growth forests. Likewise, it's often preferred by consumers who are looking for an ethical or environmentally friendly teak option for their construction project.\n\n\"Slow growth\" attempts to duplicate old-growth teak with the proper growing conditions. Proteak employs the slow growth process in the cultivation of its plantation teak by selecting plantation sites that mimic the soil and precipitation characteristics of Southeast Asia.\n\nProteak produces its timber without the use of fertilizers or irrigation, both of which speed up the growth of the wood and alter the teak's famous grain pattern.\n\n"}
{"id": "841441", "url": "https://en.wikipedia.org/wiki?curid=841441", "title": "Psychonautics", "text": "Psychonautics\n\nPsychonautics (from the Ancient Greek ' [\"soul\", \"spirit\" or \"mind\"] and ' [\"sailor\" or \"navigator\"] – \"a sailor of the soul\") refers both to a methodology for describing and explaining the subjective effects of altered states of consciousness, especially an important subgroup called holotropic states, including those induced by meditation or mind-altering substances, and to a research paradigm in which the researcher voluntarily immerses themselves into an altered mental state in order to explore the accompanying experiences.\n\nThe term has been applied diversely, to cover all activities by which altered states are induced and utilized for spiritual purposes or the exploration of the human condition, including shamanism, lamas of the Tibetan Buddhist tradition, sensory deprivation, and archaic/modern drug users who use entheogenic substances in order to gain deeper insights and spiritual experiences. A person who uses altered states for such exploration is known as a psychonaut.\n\nThe term \"psychonautics\" derives from the prior term \"psychonaut\", usually attributed to German author Ernst Jünger who used the term in describing Arthur Heffter in his 1970 essay on his own extensive drug experiences \"Annäherungen: Drogen und Rausch\" (literally: \"Approaches: Drugs and Inebriation\"). In this essay, Jünger draws many parallels between drug experience and physical exploration—for example, the danger of encountering hidden \"reefs.\"\n\nPeter J. Carroll made \"Psychonaut\" the title of a 1982 book on the experimental use of meditation, ritual and drugs in the experimental exploration of consciousness and of psychic phenomena, or \"chaos magic\". The term's first published use in a scholarly context is attributed to ethnobotanist Jonathan Ott, in 2001.\n\nClinical psychiatrist Jan Dirk Blom describes psychonautics as denoting \"the exploration of the psyche by means of techniques such as lucid dreaming, brainwave entrainment, sensory deprivation, and the use of hallucinogenics or entheogens\", and a psychonaut as one who \"seeks to investigate their mind using intentionally induced altered states of consciousness\" for spiritual, scientific, or research purposes.\n\nPsychologist Dr. Elliot Cohen of Leeds Metropolitan University and the UK Institute of Psychosomanautics defines psychonautics as \"the means to study and explore consciousness (including the unconscious) and altered states of consciousness; it rests on the realization that to study consciousness is to transform it.\" He associates it with a long tradition of historical cultures worldwide. Leeds Metropolitan University is currently the only university in the UK to offer a module in Psychonautics.\n\nAmerican Buddhist writer Robert Thurman depicts the Tibetan Buddhist master as a psychonaut, stating that \"Tibetan lamas could be called psychonauts, since they journey across the frontiers of death into the in-between realm.\"\n\nThe aims and methods of psychonautics, when state-altering substances are involved, is commonly distinguished from recreational drug use by research sources. Psychonautics as a means of exploration need not involve drugs, and may take place in a religious context with an established history. Cohen considers psychonautics closer in association to wisdom traditions and other transpersonal and integral movements.\n\nHowever, there is considerable overlap with modern drug use and due to its modern close association with psychedelics and other drugs it is also studied in the context of drug abuse from a perspective of addiction, the drug abuse market and online psychology, and studies into existing and emerging drugs within toxicology.\n\n\nThese may be used in combination; for example, traditions such as shamanism may combine ritual, fasting, and hallucinogenic substances.\n\nOne of the best known psychonautical works is Aldous Huxley's \"The Doors of Perception\". In addition to Ernst Jünger, who coined the term, the American physician, neuroscientist, psychoanalyst, philosopher, writer and inventor John C. Lilly is another well-known psychonaut. Lilly was interested in the nature of consciousness and, amongst other techniques, he used isolation tanks in his research.\n\nPhilosophical- and Science-fiction author Philip K. Dick has also been described as a psychonaut for several of his works such as \"The Three Stigmata of Palmer Eldritch\". Another influential psychonaut is the psychologist and writer Timothy Leary. Leary is known for controversial talks and research on the subject; he wrote several books including \"The Psychedelic Experience\". Another widely known psychonaut is the American philosopher, ethnobotanist, lecturer, and author Terence McKenna. McKenna spoke and wrote about subjects including psychedelic drugs, plant-based entheogens, shamanism, metaphysics, alchemy, language, culture, technology, and the theoretical origins of human consciousness.\n\n"}
{"id": "43854", "url": "https://en.wikipedia.org/wiki?curid=43854", "title": "Reality", "text": "Reality\n\nReality is the sum or aggregate of all that is real or existent, as opposed to that which is merely imaginary. The term is also used to refer to the ontological status of things, indicating their existence. In physical terms, reality is the totality of the universe, known and unknown. Philosophical questions about the nature of reality or existence or being are considered under the rubric of ontology, which is a major branch of metaphysics in the Western philosophical tradition. Ontological questions also feature in diverse branches of philosophy, including the philosophy of science, philosophy of religion, philosophy of mathematics, and philosophical logic. These include questions about whether only physical objects are real (i.e., Physicalism), whether reality is fundamentally immaterial (e.g., Idealism), whether hypothetical unobservable entities posited by scientific theories exist, whether God exists, whether numbers and other abstract objects exist, and whether possible worlds exist. \n\nA common colloquial usage would have \"reality\" mean \"perceptions, beliefs, and attitudes toward reality\", as in \"My reality is not your reality.\" This is often used just as a colloquialism indicating that the parties to a conversation agree, or should agree, not to quibble over deeply different conceptions of what is real. For example, in a religious discussion between friends, one might say (attempting humor), \"You might disagree, but in my reality, everyone goes to heaven.\"\n\nReality can be defined in a way that links it to worldviews or parts of them (conceptual frameworks): Reality is the totality of all things, structures (actual and conceptual), events (past and present) and phenomena, whether observable or not. It is what a world view (whether it be based on individual or shared human experience) ultimately attempts to describe or map.\n\nCertain ideas from physics, philosophy, sociology, literary criticism, and other fields shape various theories of reality. One such belief is that there simply and literally \"is\" no reality beyond the perceptions or beliefs we each have about reality. Such attitudes are summarized in the popular statement, \"Perception is reality\" or \"Life is how you perceive reality\" or \"reality is what you can get away with\" (Robert Anton Wilson), and they indicate anti-realism – that is, the view that there is no objective reality, whether acknowledged explicitly or not.\n\nMany of the concepts of science and philosophy are often defined culturally and socially. This idea was elaborated by Thomas Kuhn in his book \"The Structure of Scientific Revolutions\" (1962). \"The Social Construction of Reality\", a book about the sociology of knowledge written by Peter L. Berger and Thomas Luckmann, was published in 1966. It explained how knowledge is acquired and used for the comprehension of reality. Out of all the realities, the reality of everyday life is the most important one since our consciousness requires us to be completely aware and attentive to the experience of everyday life.\n\nPhilosophy addresses two different aspects of the topic of reality: the nature of reality itself, and the relationship between the mind (as well as language and culture) and reality.\n\nOn the one hand, ontology is the study of being, and the central topic of the field is couched, variously, in terms of being, existence, \"what is\", and reality. The task in ontology is to describe the most general categories of reality and how they are interrelated. If a philosopher wanted to proffer a positive definition of the concept \"reality\", it would be done under this heading. As explained above, some philosophers draw a distinction between reality and existence. In fact, many analytic philosophers today tend to avoid the term \"real\" and \"reality\" in discussing ontological issues. But for those who would treat \"is real\" the same way they treat \"exists\", one of the leading questions of analytic philosophy has been whether existence (or reality) is a property of objects. It has been widely held by analytic philosophers that it is \"not\" a property at all, though this view has lost some ground in recent decades.\n\nOn the other hand, particularly in discussions of objectivity that have feet in both metaphysics and epistemology, philosophical discussions of \"reality\" often concern the ways in which reality is, or is not, in some way \"dependent upon\" (or, to use fashionable jargon, \"constructed\" out of) mental and cultural factors such as perceptions, beliefs, and other mental states, as well as cultural artifacts, such as religions and political movements, on up to the vague notion of a common cultural world view, or .\n\nThe view that there is a reality independent of any beliefs, perceptions, etc., is called realism. More specifically, philosophers are given to speaking about \"realism \"about\"\" this and that, such as realism about universals or realism about the external world. Generally, where one can identify any class of object, the existence or essential characteristics of which is said not to depend on perceptions, beliefs, language, or any other human artifact, one can speak of \"realism \"about\"\" that object.\n\nOne can also speak of \"anti\"-realism about the same objects. \"Anti-realism\" is the latest in a long series of terms for views opposed to realism. Perhaps the first was idealism, so called because reality was said to be in the mind, or a product of our \"ideas\". Berkeleyan idealism is the view, propounded by the Irish empiricist George Berkeley, that the objects of perception are actually ideas in the mind. In this view, one might be tempted to say that reality is a \"mental construct\"; this is not quite accurate, however, since, in Berkeley's view, perceptual ideas are created and coordinated by God. By the 20th century, views similar to Berkeley's were called phenomenalism. Phenomenalism differs from Berkeleyan idealism primarily in that Berkeley believed that minds, or souls, are not merely ideas nor made up of ideas, whereas varieties of phenomenalism, such as that advocated by Russell, tended to go farther to say that the mind itself is merely a collection of perceptions, memories, etc., and that there is no mind or soul over and above such mental events. Finally, anti-realism became a fashionable term for \"any\" view which held that the existence of some object depends upon the mind or cultural artifacts. The view that the so-called external world is really merely a social, or cultural, artifact, called social constructionism, is one variety of anti-realism. Cultural relativism is the view that social issues such as morality are not absolute, but at least partially cultural artifact.\n\nA correspondence theory of knowledge about what exists claims that \"true\" knowledge of reality represents accurate correspondence of statements about and images of reality with the actual reality that the statements or images are attempting to represent. For example, the scientific method can verify that a statement is true based on the observable evidence that a thing exists. Many humans can point to the Rocky Mountains and say that this mountain range exists, and continues to exist even if no one is observing it or making statements about it.\n\nThe nature of being is a perennial topic in metaphysics. For, instance Parmenides taught that reality was a single unchanging Being, whereas Heraclitus wrote that all things flow. The 20th century philosopher Heidegger thought previous philosophers have lost sight the question of Being (qua Being) in favour of the questions of beings (existing things), so that a return to the Parmenidean approach was needed. An ontological catalogue is an attempt to list the fundamental constituents of reality. The question of whether or not existence is a predicate has been discussed since the Early Modern period, not least in relation to the ontological argument for the existence of God. Existence, \"that\" something is, has been contrasted with \"essence\", the question of \"what\" something is.\nSince existence without essence seems blank, it associated with nothingness by philosophers such as Hegel. Nihilism represents an extremely negative view of being, the absolute a positive one.\n\nThe question of direct or \"naïve\" realism, as opposed to indirect or \"representational\" realism, arises in the philosophy of perception and of mind out of the debate over the nature of conscious experience; the epistemological question of whether the world we see around us is the real world itself or merely an internal perceptual copy of that world generated by neural processes in our brain. Naïve realism is known as \"direct\" realism when developed to counter \"indirect\" or representative realism, also known as epistemological dualism, the philosophical position that our conscious experience is not of the real world itself but of an internal representation, a miniature virtual-reality replica of the world.\n\nTimothy Leary coined the influential term Reality Tunnel, by which he means a kind of representative realism. The theory states that, with a subconscious set of mental filters formed from their beliefs and experiences, every individual interprets the same world differently, hence \"Truth is in the eye of the beholder\". His ideas influenced the work of his friend Robert Anton Wilson.\n\nThe status of abstract entities, particularly numbers, is a topic of discussion in mathematics.\n\nIn the philosophy of mathematics, the best known form of realism about numbers is Platonic realism, which grants them abstract, immaterial existence. Other forms of realism identify mathematics with the concrete physical universe.\n\nAnti-realist stances include formalism and fictionalism.\n\nSome approaches are selectively realistic about some mathematical objects but not others. Finitism rejects infinite quantities. Ultra-finitism accepts finite quantities up to a certain amount. Constructivism and intuitionism are realistic about objects that can be explicitly constructed, but reject the use of the principle of the excluded middle to prove existence by reductio ad absurdum.\n\nThe traditional debate has focused on whether an abstract (immaterial, intelligible) realm of numbers has existed \"in addition to\" the physical (sensible, concrete) world. A recent development is the mathematical universe hypothesis, the theory that \"only\" a mathematical world exists, with the finite, physical world being an illusion within it.\n\nAn extreme form of realism about mathematics is the mathematical multiverse hypothesis advanced by Max Tegmark. Tegmark's sole postulate is: \"All structures that exist mathematically also exist physically\". That is, in the sense that \"in those [worlds] complex enough to contain self-aware substructures [they] will subjectively perceive themselves as existing in a physically 'real' world\". The hypothesis suggests that worlds corresponding to different sets of initial conditions, physical constants, or altogether different equations should be considered real. The theory can be considered a form of Platonism in that it posits the existence of mathematical entities, but can also be considered a mathematical monism in that it denies that anything exists except mathematical objects.\n\nThe problem of universals is an ancient problem in metaphysics about whether universals exist. Universals are general or abstract qualities, characteristics, properties, kinds or relations, such as being male/female, solid/liquid/gas or a certain colour, that can be predicated of individuals or particulars or that individuals or particulars can be regarded as sharing or participating in. For example, Scott, Pat, and Chris have in common the universal quality of \"being human\" or \"humanity\".\n\nThe realist school claims that universals are real – they exist and are distinct from the particulars that instantiate them. There are various forms of realism. Two major forms are Platonic realism and Aristotelian realism. \"Platonic realism\" is the view that universals are real entities and they exist independent of particulars. \"Aristotelian realism\", on the other hand, is the view that universals are real entities, but their existence is dependent on the particulars that exemplify them.\n\nNominalism and conceptualism are the main forms of anti-realism about universals.\n\nA traditional realist position in ontology is that time and space have existence apart from the human mind. Idealists deny or doubt the existence of objects independent of the mind. Some anti-realists whose ontological position is that objects outside the mind do exist, nevertheless doubt the independent existence of time and space.\n\nKant, in the \"Critique of Pure Reason\", described time as an \"a priori\" notion that, together with other \"a priori\" notions such as space, allows us to comprehend sense experience. Kant denies that either space or time are substance, entities in themselves, or learned by experience; he holds rather that both are elements of a systematic framework we use to structure our experience. Spatial measurements are used to quantify how far apart objects are, and temporal measurements are used to quantitatively compare the interval between (or duration of) events. Although\nspace and time are held to be \"transcendentally ideal\" in this sense, they are also \"empirically real\", i.e. not mere illusions.\n\nIdealist writers such as J. M. E. McTaggart in \"The Unreality of Time\" have argued that time is an illusion.\n\nAs well as differing about the reality of time as a whole, metaphysical theories of time can differ in their ascriptions of reality to the past, present and future separately.\n\nTime, and the related concepts of process and evolution are central to the system-building metaphysics of A. N. Whitehead and Charles Hartshorne.\n\nThe term \"possible world\" goes back to Leibniz's theory of possible worlds, used to analyse necessity, possibility, and similar modal notions. Modal realism is the view, notably propounded by David Kellogg Lewis, that all possible worlds are as real as the actual world. In short: the actual world is regarded as merely one among an infinite set of logically possible worlds, some \"nearer\" to the actual world and some more remote. Other theorists may use the Possible World framework to express and explore problems without committing to it ontologically.\nPossible world theory is related to alethic logic: a proposition is \"necessary\" if it is true in all possible worlds, and \"possible\" if it is true in at least one. The many worlds interpretation of quantum mechanics is a similar idea in science.\n\nThe philosophical implications of a physical TOE are frequently debated. For example, if philosophical physicalism is true, a physical TOE will coincide with a philosophical theory of everything.\n\nThe \"system building\" style of metaphysics attempts to answer \"all\" the important questions in a coherent way, providing a complete picture of the world. Plato and Aristotle could be said to be early examples of comprehensive systems. In the early modern period (17th and 18th centuries), the system-building \"scope\" of philosophy is often linked to the rationalist \"method\" of philosophy, that is the technique of deducing the nature of the world by pure \"a priori\" reason. Examples from the early modern period include the Leibniz's Monadology, Descartes's Dualism, Spinoza's Monism. Hegel's Absolute idealism and Whitehead's Process philosophy were later systems.\n\nOther philosophers do not believe its techniques can aim so high. Some scientists think a more mathematical approach than philosophy is needed for a TOE, for instance Stephen Hawking wrote in \"A Brief History of Time\" that even if we had a TOE, it would necessarily be a set of equations. He wrote, \"What is it that breathes fire into the equations and makes a universe for them to describe?\"\n\nOn a much broader and more subjective level, private experiences, curiosity, inquiry, and the selectivity involved in personal interpretation of events shapes reality as seen by one and only one individual and hence is called phenomenological. While this\nform of reality might be common to others as well, it could at times also be so unique to oneself as to never be experienced or agreed upon by anyone else. Much of the kind of experience deemed spiritual occurs on this level of reality.\n\nPhenomenology is a philosophical method developed in the early years of the twentieth century by Edmund Husserl and a circle of followers at the universities of Göttingen and Munich in Germany. Subsequently, phenomenological themes were taken up by philosophers in France, the United States, and elsewhere, often in contexts far removed from Husserl's work.\n\nThe word \"phenomenology\" comes from the Greek \"phainómenon\", meaning \"that which appears\", and \"lógos\", meaning \"study\". In Husserl's conception, phenomenology is primarily concerned with making the structures of consciousness, and the phenomena which appear in acts of consciousness, objects of systematic reflection and analysis. Such reflection was to take place from a highly modified \"first person\" viewpoint, studying phenomena not as they appear to \"my\" consciousness, but to any consciousness whatsoever. Husserl believed that phenomenology could thus provide a firm basis for all human knowledge, including scientific knowledge, and could establish philosophy as a \"rigorous science\".\n\nHusserl's conception of phenomenology has been criticised and developed not only by himself, but also by his student and assistant Martin Heidegger, by existentialists, such as Maurice Merleau-Ponty, Jean-Paul Sartre, and by other philosophers, such as Paul Ricoeur, Emmanuel Levinas, and Dietrich von Hildebrand.\n\nSkeptical hypotheses in philosophy suggest that reality is very different from what we think it is; or at least that we cannot prove it is not. Examples include:\n\n\nJain philosophy postulates that seven tattva (truths or fundamental principles) constitute reality. These seven \"tattva\" are:\n\nScientific realism is, at the most general level, the view that the world described by science (perhaps ideal science) is the real world, as it is, independent of what we might take it to be. Within philosophy of science, it is often framed as an answer to the question \"how is the success of science to be explained?\" The debate over what the success of science involves centers primarily on the status of entities that are not directly observable discussed by scientific theories. Generally, those who are scientific realists state that one can make reliable claims about these entities (viz., that they have the same ontological status) as directly observable entities, as opposed to instrumentalism. The most used and studied scientific theories today state more or less the truth.\n\n\"Realism\" in the sense used by physicists does not equate to realism in metaphysics.\nThe latter is the claim that the world is mind-independent: that even if the results of a measurement do not pre-exist the act of measurement, that does not require that they are the creation of the observer. Furthermore, a mind-independent property does not have to be the value of some physical variable such as position or momentum. A property can be \"dispositional\" (or potential), i.e. it can be a tendency: in the way that glass objects tend to break, or are disposed to break, even if they do not \"actually\" break. Likewise, the mind-independent properties of quantum systems could consist of a tendency to respond to particular measurements with particular values with ascertainable probability. Such an ontology would be metaphysically realistic, without being realistic in the physicist's sense of \"local realism\" (which would require that a single value be produced with certainty).\n\nA closely related term is counterfactual definiteness (CFD), used to refer to the claim that one can meaningfully speak of the definiteness of results of measurements that have not been performed (i.e. the ability to assume the existence of objects, and properties of objects, even when they have not been measured).\n\nLocal realism is a significant feature of classical mechanics, of general relativity, and of electrodynamics; but quantum mechanics has shown that quantum entanglement is possible. This was rejected by Einstein, who proposed the EPR paradox, but it was subsequently quantified by Bell's inequalities. If Bell's inequalities are violated, either local realism \"or\" counterfactual definiteness must be incorrect; but some physicists dispute that experiments have demonstrated Bell's violations, on the grounds that the sub-class of inhomogeneous Bell inequalities has not been tested or due to experimental limitations in the tests. Different interpretations of quantum mechanics violate different parts of local realism and/or counterfactual definiteness.\n\nThe quantum mind–body problem refers to the philosophical discussions of the mind–body problem in the context of quantum mechanics. Since quantum mechanics involves quantum superpositions, which are not perceived by observers, some interpretations of quantum mechanics place conscious observers in a special position.\n\nThe founders of quantum mechanics debated the role of the observer, and of them, Wolfgang Pauli and Werner Heisenberg believed that it was the observer that produced collapse. This point of view, which was never fully endorsed by Niels Bohr, was denounced as mystical and anti-scientific by Albert Einstein. Pauli accepted the term, and described quantum mechanics as \"lucid mysticism\".\n\nHeisenberg and Bohr always described quantum mechanics in logical positivist terms. Bohr also took an active interest in the philosophical implications of quantum theories such as his complementarity, for example. He believed quantum theory offers a complete description of nature, albeit one that is simply ill-suited for everyday experiences – which are better described by classical mechanics and probability. Bohr never specified a demarcation line above which objects cease to be quantum and become classical. He believed that it was not a question of physics, but one of philosophy.\n\nEugene Wigner reformulated the \"Schrödinger's cat\" thought experiment as \"Wigner's friend\" and proposed that the consciousness of an observer is the demarcation line which precipitates collapse of the wave function, independent of any realist interpretation. Commonly known as \"consciousness causes collapse\", this interpretation of quantum mechanics states that observation by a conscious observer is what makes the wave function collapse.\n\nThe multiverse is the hypothetical set of multiple possible universes (including the historical universe we consistently experience) that together comprise everything that exists: the entirety of space, time, matter, and energy as well as the physical laws and constants that describe them. The term was coined in 1895 by the American philosopher and psychologist William James. In the many-worlds interpretation (MWI), one of the mainstream interpretations of quantum mechanics, there are an infinite number of universes and every possible quantum outcome occurs in at least one universe.\n\nThe structure of the multiverse, the nature of each universe within it and the relationship between the various constituent universes, depend on the specific multiverse hypothesis considered. Multiverses have been hypothesized in cosmology, physics, astronomy, religion, philosophy, transpersonal psychology and fiction, particularly in science fiction and fantasy. In these contexts, parallel universes are also called \"alternative universes\", \"quantum universes\", \"interpenetrating dimensions\", \"parallel dimensions\", \"parallel worlds\", \"alternative realities\", \"alternative timelines\", and \"dimensional planes\", among others.\n\nA theory of everything (TOE) is a putative theory of theoretical physics that fully explains and links together all known physical phenomena, and predicts the outcome of \"any\" experiment that could be carried out \"in principle\". The theory of everything is also called the final theory. Many candidate theories of everything have been proposed by theoretical physicists during the twentieth century, but none have been confirmed experimentally. The primary problem in producing a TOE is that general relativity and quantum mechanics are hard to unify. This is one of the unsolved problems in physics.\n\nInitially, the term \"theory of everything\" was used with an ironic connotation to refer to various overgeneralized theories. For example, a great-grandfather of Ijon Tichy, a character from a cycle of Stanisław Lem's science fiction stories of the 1960s, was known to work on the \"General Theory of Everything\". Physicist John Ellis claims to have introduced the term into the technical literature in an article in \"Nature\" in 1986. Over time, the term stuck in popularizations of quantum physics to describe a theory that would unify or explain through a single model the theories of all fundamental interactions and of all particles of nature: general relativity for gravitation, and the standard model of elementary particle physics – which includes quantum mechanics – for electromagnetism, the two nuclear interactions, and the known elementary particles.\n\nCurrent candidates for a theory of everything include string theory, M theory, and loop quantum gravity.\n\nVirtual reality (VR) is a computer-simulated environment that can simulate physical presence in places in the real world, as well as in imaginary worlds.\n\nThe Virtuality Continuum is a continuous scale ranging between the completely virtual, a Virtuality, and the completely real: Reality. The reality-virtuality continuum therefore encompasses all possible variations and compositions of real and virtual objects. It has been described as a concept in new media and computer science, but in fact it could be considered a matter of anthropology. The concept was first introduced by Paul Milgram.\n\nThe area between the two extremes, where both the real and the virtual are mixed, is the so-called Mixed reality. This in turn is said to consist of both Augmented Reality, where the virtual augments the real, and Augmented virtuality, where the real augments the virtual.\nCyberspace, the world's computer systems considered as an interconnected whole, can be thought of as a virtual reality; for instance, it is portrayed as such in the cyberpunk fiction of William Gibson and others. Second life and MMORPGs such as \"World of Warcraft\" are examples of artificial environments or virtual worlds (falling some way short of full virtual reality) in cyberspace.\n\nOn the Internet, \"real life\" refers to life in the real world. It generally references life or consensus reality, in contrast to an environment seen as fiction or fantasy, such as virtual reality, lifelike experience, dreams, novels, or movies. Online, the acronym \"IRL\" stands for \"in real life\", with the meaning \"not on the Internet\". Sociologists engaged in the study of the Internet have determined that someday, a distinction between online and real-life worlds may seem \"quaint\", noting that certain types of online activity, such as sexual intrigues, have already made a full transition to complete legitimacy and \"reality\". The abbreviation \"RL\" stands for \"real life\". For example, one can speak of \"meeting in RL\" someone whom one has met in a chat or on an Internet forum. It may also be used to express an inability to use the Internet for a time due to \"RL problems\".\n\n\n\n"}
{"id": "2940858", "url": "https://en.wikipedia.org/wiki?curid=2940858", "title": "Rejuvenation", "text": "Rejuvenation\n\nRejuvenation is a medical discipline focused on the practical reversal of the aging process.\n\nRejuvenation is distinct from life extension. Life extension strategies often study the causes of aging and try to oppose those causes in order to slow aging. Rejuvenation is the \"reversal\" of aging and thus requires a different strategy, namely repair of the damage that is associated with aging or replacement of damaged tissue with new tissue. Rejuvenation can be a means of life extension, but most life extension strategies do not involve rejuvenation.\n\nVarious myths tell the stories about the quest for rejuvenation. It was believed that magic or intervention of a supernatural power can bring back the youth and many mythical adventurers set out on a journey to do that, for themselves, their relatives or some authority that sent them.\n\nAn ancient Chinese emperor actually sent out ships of young men and women to find a pearl that would rejuvenate him. This led to a myth among modern Chinese that Japan was founded by these people.\n\nIn some religions, people were to be rejuvenated after death prior to placing them in heaven.\n\nThe stories continued well into the 16th century. The Spanish explorer Juan Ponce de León led an expedition around the Caribbean islands and into Florida to find the Fountain of Youth. Led by the rumors, the expedition continued the search and many perished. The Fountain was nowhere to be found as locals were unaware of its exact location.\n\nSince the emergence of philosophy, sages and self-proclaimed wizards always made enormous efforts to find the secret of youth, both for themselves and for their noble patrons and sponsors. It was widely believed that some potions may restore the youth.\n\nAnother commonly cited approach was attempting to transfer the essence of youth from young people to old. Some examples of this approach were sleeping with virgins or children (sometimes literally sleeping, not necessarily having sex), bathing in or drinking their blood.\n\nThe quest for rejuvenation reached its height with alchemy. All around Europe, and also beyond, alchemists were looking for the Philosopher's Stone, the mythical substance that, as it was believed, could not only turn lead into gold, but also prolong life and restore youth. Although the set goal was not achieved, alchemy paved the way to the scientific method and so to the medical advances of today.\n\nSerge Abrahamovitch Voronoff was a French surgeon born in Russia who gained fame for his technique of grafting monkey testicle tissue on to the testicles of men while working in France in the 1920s and 1930s. This was one of the first medically accepted rejuvenation therapies (before he was proved to be wrong around 1930–1940). The technique brought him a great deal of money, although he was already independently wealthy. As his work fell out of favor, he went from being a highly respected surgeon to a subject of ridicule. By the early 1930s, over 500 men had been treated in France by his rejuvenation technique, and thousands more around the world, such as in a special clinic set up in Algiers. Noteworthy people who had the surgery included Harold McCormick, chairman of the board of International Harvester Company, and the aging premier of Turkey.\n\nSwiss doctor Paul Niehans, who was one of the fathers of cellular therapy, developed in 1931–1949 years the so-called Fresh cell therapy. Fresh cell therapy is mainly the use of live animal embryo organs cells which are injected into the patient with the purpose of achieving a revitalizing effect. These cells are generally extracted from sheep’s fetuses because in comparison to other animals, like pigs, rabbits and cows, sheep are clean animals and rarely contract diseases. Of course animal cells are not able to be included in human tissue, but they can secrete factors for rejuvenating. That's why this rejuvenation technology, despite the harsh criticism is practiced to this day.\n\nIn fiction, there is an increasing amount of work being done on possibilities of rejuvenation treatments, and the effect this would have on society. Misspent Youth as well as the Commonwealth Saga by Peter F. Hamilton are one of the most well known examples of this, dealing with the short- and long-term effects of a near perfect 80-year-old to 20-year-old body change with mind intact. Also the Mars trilogy deals with a much more imperfect type of rejuvenation, including problems such as long-term memory loss and sheer boredom that comes with such age. Also the post-mortal characters in the Revelation Space series often illustrate this issue with long-term or essentially infinite lifespans, sheer boredom induces them to undertake activities of extreme risk.\n\nAging is an accumulation of damage to macromolecules, cells, tissues and organs. If any of that damage can be repaired, the result is rejuvenation.\n\nThere have been many experiments which have been shown to increase the maximum life span of laboratory animals, thereby achieving life extension. A few experimental methods such as replacing hormones to youthful levels have had considerable success in partially rejuvenating laboratory animals and humans. A recent experiment involved breeding genetically manipulated mice that lacked an enzyme called telomerase, causing the mice to age prematurely and suffer ailments. When the mice were given injections to reactivate the enzyme, it repaired the damaged tissues and reversed the signs of aging. There are at least eight important hormones that decline with age: 1. human growth hormone (HGH); 2. the sexual hormones: testosterone or oestrogen/progesterone; 3. erythropoietin (EPO); 4. insulin; 5. DHEA; 6. melatonin; 7. thyroid; 8. pregnenolone. In theory, if all or some of these hormones are replaced, the body will respond to them as it did when it was younger, thus repairing and restoring many body functions. In line with this, recent experiments show that heterochronic parabiosis, i.e. connecting the circulatory systems of young and old animal, leads to the rejuvenation of the old animal, including restoration of proper stem cell function. Similar experiments show that grafting old muscles into young hosts leads to their complete restoration, whereas grafting young muscles into old hosts does not. These experiments show that aging is mediated by systemic environment, rather than being an intrinsic cell property. Clinical trials based on transfusion of young blood were scheduled to begin in 2014.\n\nMost attempts at genetic repair have traditionally involved the use of a retrovirus to insert a new gene into a random position on a chromosome. But by attaching zinc fingers (which determine where transcription factors bind) to endonucleases (which break DNA strands), homologous recombination can be induced to correct and replace defective (or undesired) DNA sequences. The first applications of this technology are to isolate stem cells from the bone marrow of patients having blood disease mutations, to correct those mutations in laboratory dishes using zinc finger endonucleases and to transplant the stem cells back into the patients.\n\nStem cell regenerative medicine uses three different strategies:\n\nA salamander can not only regenerate a limb, but can regenerate the lens or retina of an eye and can regenerate an intestine. For regeneration the salamander tissues form a blastema by de-differentiation of mesenchymal cells, and the blastema functions as a self-organizing system to regenerate the limb.\n\nYet another option involves cosmetic changes to the individual to create the appearance of youth. These are generally superficial and do little to make the person healthier or live longer, but the real improvement in a person's appearance may elevate their mood and have positive side effects normally correlated with happiness. Cosmetic surgery is a large industry offering treatments such as removal of wrinkles (\"face lift\"), removal of extra fat (liposuction) and reshaping or augmentation of various body parts (abdomen, breasts, face).\n\nThere are also, as commonly found throughout history, many fake rejuvenation products that have been shown to be ineffective. Chief among these are powders, sprays, gels, and homeopathic substances that claim to contain growth hormones. Authentic growth hormones are only effective when injected, mainly due to the fact that the 191-amino acid protein is too large to be absorbed through the mucous membranes, and would be broken up in the stomach if swallowed.\n\nThe Mprize scientific competition is under way to deliver on the mission of extending healthy human life. It directly accelerates the development of revolutionary new life extension therapies by awarding two cash prizes: one to the research team that breaks the world record for the oldest-ever mouse; and one to the team that develops the most successful late-onset rejuvenation. Current Mprize winner for rejuvenation is Steven Spindler. Caloric restriction (CR), the consumption of fewer calories while avoiding malnutrition, was applied as a robust method of decelerating aging and the development of age-related diseases.\n\nThe biomedical gerontologist Aubrey de Grey has initiated a project, Strategies for Engineered Negligible Senescence (SENS), to study how to reverse the damage caused by aging. He has proposed seven strategies for what he calls the seven deadly sins of aging:\nIn 2009, Aubrey de Grey co-founded the SENS Foundation to expedite progress in the above-listed areas.\n\n\n"}
{"id": "25606638", "url": "https://en.wikipedia.org/wiki?curid=25606638", "title": "Search neutrality", "text": "Search neutrality\n\nSearch neutrality is a principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance. This means that when a user queries a search engine, the engine should return the most relevant results found in the provider's domain (those sites which the engine has knowledge of), without manipulating the order of the results (except to rank them by relevance), excluding results, or in any other way manipulating the results to a certain bias. \nSearch neutrality is related to network neutrality in that they both aim to keep any one organization from limiting or altering a user's access to services on the Internet. Search neutrality aims to keep the organic search results (results returned because of their relevance to the search terms, as opposed to results sponsored by advertising) of a search engine free from any manipulation, while network neutrality aims to keep those who provide and govern access to the Internet from limiting the availability of resources to access any given content.\n\nThe term \"search neutrality\" in context of the internet appears as early as March 2009 in an academic paper by Andrew Odlyzko titled, \"Network Neutrality, Search Neutrality, and the Never-ending Conflict between Efficiency and Fairness in Markets\". In this paper, Odlykzo predicts that if net neutrality were to be accepted as a legal or regulatory principle, then the questions surrounding search neutrality would be the next controversies. Indeed, in December 2009 the New York Times published an opinion letter by Foundem co-founder and lead complainant in an anti-trust complaint against Google, Adam Raff, which likely brought the term to the broader public. According to Raff in his opinion letter, search neutrality ought to be \"the principle that search engines should have no editorial policies other than that their results be comprehensive, impartial and based solely on relevance\". On October 11, 2009, Adam and his wife Shivaun launched SearchNeutrality.org, an initiative dedicated to promoting investigations against Google's search engine practices . There, the Raffs note that they chose to frame their issue with Google as \"search neutrality\" in order to benefit from the focus and interest on net neutrality.\n\nIn contrast to net neutrality, answers to such questions, as \"what is search neutrality?\" or \"what are appropriate legislative or regulatory principles to protect search neutrality?\", appear to have less consensus. The idea that neutrality means equal treatment, regardless of the content, comes from debates on net neutrality . Neutrality in search is complicated by the fact that search engines, by design and in implementation, are not intended to be neutral or impartial. Rather, search engines and other information retrieval applications are designed to collect and store information (indexing), receive a query from a user, search for and filter relevant information based on that query (searching/filtering), and then present the user with only a subset of those results, which are ranked from most relevant to least relevant (ranking). \"Relevance\" is a form of bias used to favor some results and rank those favored results. Relevance is defined in the search engine so that a user is satisfied with the results and is therefore subject to the user's preferences. And because relevance is so subjective, putting search neutrality into practice has been so contentious.\n\nSearch neutrality became a concern after search engines, most notably Google, were accused of search bias by other companies. Competitors and companies claim search engines systematically favor some sites (and some kind of sites) over others in their lists of results, disrupting the objective results users believe they are getting. \n\nThe call for search neutrality goes beyond traditional search engines. Sites like Amazon.com and Facebook are also accused of skewing results. Amazon’s search results are influenced by companies that pay to rank higher in their search results while Facebook filters their newsfeed lists to conduct social experiments.\n\nIn order to find information on the Web, most users make use of search engines, which crawl the web, index it and show a list of results ordered by relevance. The use of search engines to access information through the web has become a key factor for online businesses companies, which depend on the flow of users visiting their pages. One of these companies is Foundem. Foundem provides a \"vertical search\" service to compare products available on online markets for the U.K. Many people see these \"vertical search\" sites as spam. Beginning in 2006 and for three and a half years following, Foundem’s traffic and business dropped significantly due to what they assert to be a penalty deliberately applied by Google. It is unclear, however, whether their claim of a penalty was self-imposed via their use of iframe HTML tags to embed the content from other websites. At the time at which Foundem claims the penalties were imposed, it was unclear whether web crawlers crawled beyond the main page of a website using iframe tags without some extra modifications. The former SEO director OMD UK, Jaamit Durrani, among others, offered this alternative explanation, stating that “Two of the major issues that Foundem had in summer was content in iFrames and content requiring javascript to load – both of which I looked at in August, and they were definitely in place. Both are huge barriers to search visibility in my book. They have been fixed somewhere between then and the lifting of the supposed ‘penalty’. I don’t think that’s a coincidence.” \n\nMost of Foundem’s accusations claim that Google deliberately applies penalties to other vertical search engines because they represent competition. Foundem is backed by a Microsoft proxy group, the 'Initiative for Competitive Online Marketplace'.\n\nThe following table details Foundem's chronology of events as found on their website:\n\nGoogle's large market share (85%) has made them a target for search neutrality litigation via antitrust laws. In February 2010, Google released an article on the Google Public Policy blog expressing their concern for fair competition, when other companies at the UK joined Foundem's cause (eJustice.fr, and Microsoft's Ciao! from Bing) also claiming being unfairly penalized by Google.\n\nAfter two years of looking into claims that Google “manipulated its search algorithms to harm vertical websites and unfairly promote its own competing vertical properties,” the Federal Trade Commission (FTC) voted unanimously to end the antitrust portion of its investigation without filing a formal complaint against Google. The FTC concluded that Google’s “practice of favoring its own content in the presentation of search results” did not violate U.S. antitrust laws. The FTC further determined that even though competitors might be negatively impacted by Google's changing algorithms, Google did not change its algorithms to hurt competitors, but as a product improvement to benefit consumers.\n\nThere are a number of arguments for and against search neutrality.\n\n\n\nGoogle’s \"Universal Search\" system has been identified as one of the least neutral search engine practices, and following the implementation of Universal Search websites, such as MapQuest, the company experienced a massive decline in web traffic. This decline has been attributed to Google linking to its own services rather than the services offered at external websites. Despite these claims, Google actually displays Google content on the first page, while rival search engines do not considerably less often than Microsoft's Bing which displays Microsoft content when rivals do not. Bing displays Microsoft content in first place more than twice as often as Google shows Google content in first place. This indicates that as far as there is any 'bias', Google is less biased than its principal competitor.\n"}
{"id": "21647661", "url": "https://en.wikipedia.org/wiki?curid=21647661", "title": "Self model", "text": "Self model\n\nThe self-model is the central concept in the theory of consciousness called the self-model theory of subjectivity (SMT). This concept comprises experiences of ownership, of first person perspective, and of a long-term unity of beliefs and attitudes. These features are instantiated in the prefrontal cortex. This theory is an interdisciplinary approach to understanding and explaining the phenomenology of consciousness and the self. This theory has two core contents, the phenomenal self-model (PSM) and the phenomenal model of the intentionality relation (PMIR). Thomas Metzinger advanced the theory in his 1993 book \"Subjekt und Selbstmodell\" (Subject and self-model).\n\nThe PSM is an entity that “actually exists, not only as a distinct theoretical entity but something that will be empirically discovered in the future- for instance, as a specific stage of the global neural dynamics in the human brain”. Involved in the PSM are three phenomenal properties that must occur in order to explain the concept of the self. The first is mineness, “a higher order property of particular forms of phenomenal content,” or the idea of ownership. The second is perspectivalness, which is “a global, structural property of phenomenal space as a whole”. More simply, it is what is commonly referred to as the ecological self, the immovable center of perception. The third phenomenal property is selfhood, which is “the phenomenal target property” or the idea of the self over time. It is the property of phenomenal selfhood that plays the most important role in creating the fictional self and the first person perspective. Metzinger defines the first person perspective as the “existence of single coherent and temporally stable model of reality which is representationally centered around or on a single coherent and temporally stable phenomenal subject”. The first-person perspective can be non-conceptual and is autonomously active due to the constant reception of perceptual information by the brain. The brain, specifically the brainstem and hypothalamus, processes this information into representational content, namely linguistic reflections. The PSM then uses this representational content to attribute phenomenal states to our perceived objects and ourselves. We are thus what Metzinger calls naïve realists, who believe we are perceiving reality directly when in actuality we are only perceiving representations of reality. The data structures and transport mechanisms of the data are “transparent” so that we can introspect on our representations of perceptions, but cannot introspect on the data or mechanisms themselves. These systemic representational experiences are then connected by subjective experience to generate the phenomenal property of selfhood. Subjective experience is the result of the Phenomenal Model of Intentionality Relationship (PMIR). The PMIR is a “conscious mental model, and its content is an ongoing, episodic subject-object relation”. The model is a result of the combination of our unique set of sensory receptors that acquire input, our unique set of experiences that shape connections within the brain, and our unique positions in space that give our perception perspectivalness.\n\nThe prefrontal cortex is implicated in all the functions of the human self model. The following functions all require communication with the prefrontal cortex; agency and association areas of the cortex; spatial perspectivity and the parietal lobes, unity and the temporal lobes.\n\nDisorders of the self model are implicated in several disorders including schizophrenia, autism, and depersonalization. According to this theory, long-term unity is impaired in autism, similar to theory of mind deficits and weak central coherence theory. Individuals with autism are thought to be impaired in assigning mental states to other people, an ability that probably codevelops with long-term unity of self. Weak central coherence, that is, the inability to assemble information into a cohesive whole, reflects the same problems with creating a unified sense of self and benific sense extreme in narcissism.\n\n"}
{"id": "268524", "url": "https://en.wikipedia.org/wiki?curid=268524", "title": "Ship of Theseus", "text": "Ship of Theseus\n\nIn the metaphysics of identity, the ship of Theseus — or Theseus's paradox — is a thought experiment that raises the question of whether a ship—standing for an object in general—that has had all of its components replaced remains fundamentally the same object.\n\nFirst, suppose that the famous ship sailed by the hero Theseus in a great battle has been kept in a harbour as a museum piece. As the years go by some of the wooden parts begin to rot and are replaced by new ones. After a century or so, all of the parts have been replaced. Is the \"restored\" ship still the same object as the original?\n\nSecond, suppose that each of the removed pieces were stored in a warehouse, and after the century, technology develops to cure their rotting and enable them to be put back together to make a ship. Is this \"reconstructed\" ship the original ship? And if so, is the restored ship in the harbour still the original ship too?\n\nThis theory states that two ships, while identical in all other ways, are not identical if they exist at two different times. Each ship-at-time is a unique \"event\". So even without replacement of parts, the ships in the harbour are different at each time. This theory is extreme in its denial of the everyday concept of identity, which is relied on by most people in everyday use.\n\nThe concept of identity might then be replaced with some other metaphysical device to fill its role. For example, we might consider \"Ship Of Theseusness\" to be a property or class which is applied to all the events in the harbour as well as to the reconstructed ship-events.\n\nThis solution was first introduced by the Greek philosopher Heraclitus who attempted to solve the paradox by introducing the idea of a river where water replenishes it. Arius Didymus quoted him as saying \"upon those who step into the same rivers, different and again different waters flow\". Plutarch disputed Heraclitus' claim about stepping twice into the same river, citing that it cannot be done because \"it scatters and again comes together, and approaches and recedes\".\n\nAccording to the philosophical system of Aristotle and his followers, four causes or reasons describe a thing; these causes can be analyzed to get to a solution to the paradox. The formal cause or 'form' (perhaps best parsed as the cause of an object's form or of its having that form) is the design of a thing, while the material cause is the matter of which the thing is made. Another of Aristotle's causes is the 'end' or final cause, which is the intended purpose of a thing. The ship of Theseus would have the same ends, those being, mythically, transporting Theseus, and politically, convincing the Athenians that Theseus was once a living person, though its material cause would change with time. The efficient cause is how and by whom a thing is made, for example, how artisans fabricate and assemble something; in the case of the ship of Theseus, the workers who built the ship in the first place could have used the same tools and techniques to replace the planks in the ship.\n\nAccording to Aristotle, the \"what-it-is\" of a thing is its formal cause, so the ship of Theseus is the 'same' ship, because the formal cause, or design, does not change, even though the matter used to construct it may vary with time. In the same manner, for Heraclitus's paradox, a river has the same formal cause, although the material cause (the particular water in it) changes with time, and likewise for the person who steps in the river.\n\nThis argument's validity and soundness as applied to the paradox depend on the accuracy not only of Aristotle's expressed premise that an object's formal cause is not only the primary or even sole determiner of its defining characteristic(s) or essence (\"what-it-is\") but also of the unstated, stronger premise that an object's formal cause is the sole determiner of its \"identity\" or \"\"which\"-it-is\" (\"i.e.\", whether the previous and the later ships or rivers are the \"same\" ship or river). This latter premise is subject to attack by indirect proof using arguments such as \"Suppose two ships are built using the same design and exist at the same time until one sinks the other in battle. Clearly the two ships are not the same ship even before, let alone after, one sinks the other, and yet the two have the same formal cause; therefore, formal cause cannot by itself suffice to determine an object's identity\" or \" [...] therefore, two objects' or object-instances' having the same formal cause does not by itself suffice to make them the same object or prove that they are the same object.\"\n\nIn this theory, both the reconstructed and restored ships claim identity with the original, as they can both trace their histories back to it. As such they are both identical with the original. As identity is a transitive relation, the two ships are therefore also identical with each other, and are a single ship existing in two locations at the same time.\n\nIn this theory, the reconstructed and restored ships are considered to be separate objects, both are identical to the original, but they are not identical to each other. This requires us to drop the transitivity of identity, ie. deny that \"A = B\" and \"B = C\" entails \"A = C\".\n\nA basic principle of logical atomism is that facts in the world exist independently of one another. Only if we deny this principle then we can claim the following: the restored ship claims continuity of parts with the original over time and so, in the absence of other arguments, claims identity with the original. However when the reconstructed ship is completed and announced to the world, it presents a better claim on continuity, which changes the status of the restored ship making it lose its identity with the original. As a theory of observer-independent reality, this is hard to conceive; it involves both action at a distance and a violation of logical atomism. However it is more acceptable to Kantian style metaphysicists who view their subject as a theory of psychology rather than reality, as it described what biological humans are likely to believe in practice. (For example, if these were real ships on display to the public for a fee, it seems likely that the public would pay to see the reconstructed rather than restored ship.)\n\nOne common argument found in the philosophical literature is that in the case of Heraclitus' river one is tripped up by two different definitions of \"the same\", in other words the vagueness of the term. In one sense, things can be \"qualitatively identical\", by sharing some properties. In another sense, they might be \"numerically identical\" by being \"one\". As an example, consider two different marbles that look identical. They would be qualitatively, but not numerically, identical; a marble can be numerically identical only to itself.\n\nAs the parts of the ship are replaced, the identity of the ship gradually changes, as the name \"Theseus' Ship\" is a truthful description only when the historical memory of Theseus' use of the ship - his physical contact with, and control of, its matter - is accurate. For example, the museum curator, prior to any restoration, may say with perfect truthfulness that the bed in the captain's cabin is the same bed in which Theseus himself once slept; but once the bed has been replaced, this is no longer true, and the claim would then be an imposture, because a different description would be more accurate, i.e.; \"a replica of Theseus' bed.\" The new bed would be as foreign to Theseus as a completely new ship. This is true of every other piece of the original boat. As the parts are replaced, the new boat becomes exactly that: a new boat. Hobbes' proposed restored boat built from the original parts will be the original ship, as its parts are the actual pieces of matter that participated in Theseus' journeys.\n\nTed Sider and others have proposed that considering objects to extend across time as four-dimensional causal series of three-dimensional \"time-slices\" could solve the ship of Theseus problem because, in taking such an approach, all four-dimensional objects remain numerically identical to themselves while allowing individual time-slices to differ from each other. The aforementioned river, therefore, comprises different three-dimensional time-slices of itself while remaining numerically identical to itself across time; one can never step into the same river-time-slice twice, but one can step into the same (four-dimensional) river twice.\n\nA 2010 psychology study reported that 20 members of the public considered the restored ship to be the original while 24 considered the reconstructed to be the original.\n\nThe paradox had been discussed by other ancient philosophers such as Heraclitus and Plato prior to Plutarch's writings, and more recently by Thomas Hobbes and John Locke. Several variants are known, including the grandfather's axe, which has had both head and handle replaced, and the similar idea \"Trigger's Broom\".\n\nThis particular version of the paradox was first introduced in Greek legend as reported by the historian, biographer, and essayist Plutarch:\n\nPlutarch thus questions whether the ship would remain the same if it were entirely replaced, piece by piece. Centuries later, the philosopher Thomas Hobbes introduced a further puzzle, wondering what would happen if the original planks were gathered up after they were replaced, and used to build a second ship. Hobbes asked which ship, if either, would be the original Ship of Theseus.\n\nThe paradox appears in several more applied fields of philosophy. \n\nIn philosophy of mind, the ship is replaced by a person whose identity over time is called into question.\n\nIn both philosophy of law and practical law, the paradox appears when the ownership of an object or of the rights to its name are disagreed in court. For example, groups of people such as companies, sports teams, and musical bands may all change their parts and see their old members re-form into rivals, leading to legal actions between the old and new entities. Also, texts and computer programs may be edited gradually but so heavily that none of the original remains, posing the legal question of whether the owners of the original have any claim on the result.\n\nIn ontological engineering such as the design of practical databases and AI systems, the paradox appears regularly when data objects change over time.\n\nThere are many more examples of the paradox found in real-life and in fiction.\n\n"}
{"id": "54135531", "url": "https://en.wikipedia.org/wiki?curid=54135531", "title": "Social influence bias", "text": "Social influence bias\n\nThe social influence bias is an asymmetric herding effect on online social media platforms which makes users overcompensate for negative ratings but amplify positive ones. Positive social influence can accumulate and result in a rating bubble, while negative social influence is neutralized by crowd correction. This phenomenon was first described in a paper written by Lev Muchnik, Sinan Aral and Sean J. Taylor in 2014, then the question was revisited by Cicognani et al., who's experiment reinforced Munchnik's and his co-authors' results.\n\nOnline customer reviews are trusted sources of information in various contexts such as online marketplaces, dining, accommodation, movies or digital products. However, these online ratings are not immune to herd behaviour, which means that subsequent reviews are not independent from each other. As on many such sites, preceding opinions are visible to a new reviewer, he or she can be heavily influenced by the antecedent evaluations in his or her decision about the certain product, service or online content. This form of herding behaviour inspired Muchnik, Aral and Taylor to conduct their experiment on influence in social contexts.\n\nMuchnik, Aral and Taylor designed a large-scale randomized experiment to measure social influence on user reviews. The experiment was conducted on the social news aggregation website like Reddit. The study lasted for 5 months, the authors randomly assigned 101 281 comments to one of the following treatment groups: up-treated (4049), down-treated (1942) or control (the proportions reflect the observed ratio of up- and down-votes. Comments which fell to the first group were given an up-vote upon the creation of the comment, the second group got a down-vote upon creation, the comments in the control group remained untouched. A vote is equivalent of a single rating (+1 or -1). As other users are unable to trace a user’s votes, they were unaware of the experiment. Due to randomization, comments in the control and the treatment group was not different in terms of expected rating. The treated comments were viewed more than 10 million times and rated 308 515 times by successive users.\n\nThe up-vote treatment increased the probability of up-voting by the first viewer by 32% over the control group (Figure 1A), while the probability of down-voting did not change compared to the control group, which means that users did not correct the random positive rating. The upward bias remained inplace for the observed 5-month period. The accumulating herding effect increased the comment’s mean rating by 25% compared to the control group comments (Figure 1C). Positively manipulated comments did receive higher ratings at all parts of the distribution, which means that they were also more likely to collect extremely high scores. The negative manipulation created an asymmetric herd effect: although the probability of subsequent down-votes was increased by the negative treatment, the probability of up-voting also grew for these comments. The community performed a correction which neutralized the negative treatment and resulted non-different final mean ratings from the control group. The authors also compared the final mean scores of comments across the most active topic categories on the website. The observed positive herding effect was present in the “politics,” “culture and society,” and “business” subreddits, but was not applicable for “economics,” “IT,” “fun,” and “general news”.\n\nThe skewed nature of online ratings makes review outcomes different to what it would be without the social influence bias. In a 2009 experiment by Hu, Zhang and Pavlou showed that the distribution of reviews of a certain product made by unconnected individuals is approximately normal, however, the rating of the same product on Amazon followed a J-Shaped distribution with twice as much five-star ratings than others. Cicognani, Figini and Magnani came to similar conclusions after their experiment conducted on a tourism services website: positive preceding ratings influenced raters' behavior more than mediocre ones. Positive crowd correction makes community-based opinions upward-biased.\n"}
{"id": "49270163", "url": "https://en.wikipedia.org/wiki?curid=49270163", "title": "Solo status", "text": "Solo status\n\nSolo status is a social psychology term first popularized by Lord and Saenz (1985), to classify the situation when only one member of a particular social category (race, gender, culture) is present in a group setting. While the term has been used interchangeably with \"token\", solo status “does not imply that a person has been preferentially selected for a position by virtue of his or her social category”, but is the singular representative based on mere chance or circumstance.\n\nAs the only member from the particular social category, individuals (especially women and minorities) frequently feel a sense of responsibility to properly represent their entire category, with fear of not perpetuating a negative stereotype. This self-censoring places a greater strain on the individual’s cognitive resources, leading to poorer performance on the task. Solo status can impact a person’s performance even without the direct reference or relation to a stereotype, in spite of the person having equal or higher education or training than the remaining group.\n\nThe field of psychology is consistently confronted with the struggle of ecological validity and the applicability of their laboratory research in the real world. Due to the methods of testing solo status in both the field and the lab, psychologists understanding of solo status has evolved over time. Lord and Saenz (1985) completed the first solo status experiment in a lab, where they claimed that solo status was equally detrimental to men and women on their cognitive performance. Unfortunately, these results did not align with Yoder's and Aniakudo's (1997) findings that women who experienced solo status in their offices were negatively affected in their job performances and productivity when compared to their male counterparts. Due to the discrepancies found in the research, Sekaquaptewa and Thompson (2002) compared the differences in methods and determined that laboratory studies focused on solo status’ impact on the learning task, while field studies focused on the task performance. The primary findings on the studies’ approach is explored below:\n\nUsing Lord and Sanez (1985), as the primary case, participants were required to engage in a conversation with individuals online after receiving solo status during the learning phase. The article found that solo status individuals recalled less of the discussion, with the memory loss being justified by the participant splitting their attention between focusing on how they are portraying themselves and the material being learned. Lord and Sanez (1985), found no difference between gender and race. These results were supported by Cohen and Swim (1995) and Stangor, Carr, and Kiang (1998) who found a mirroring trend.\n\nIn field studies, individuals were reminded of their solo status during the performance task, where they become the sole focus of the group. In these studies, the individual had to speak in front of a group or engage in discussions. Based on these studies, a differing level of solo status has been demonstrated between high-status and low-status individuals. Low status individuals are often concerned that a negative performance will reflect poorly on their social category, while high status individuals do not share this concern.\n\nIn most cases, white males are classified as the high-status due to the privilege men receive in society. Due in part to this privilege, white males do not show any implications of solo status in field cases. In fact, Heikes (1991) found that white male nurses who experienced solo status reported a positive experience and were privy to special treatment that frequently lead to an earlier promotion than their female counterparts.\n\nIn psychological studies, low-status individuals are represented by women or minorities (ethnic or race), who consistently perform worse in recall or when answering questions in group situations. Kanter (1977) found that observers remembered what solo status individuals’ stated better than nonsolos, and were judged more critically than their majority counterparts. These critical judgments were worsened by the circumstance, as solo status is most frequently seen in contexts that impact the solo status individuals’ social and economic welfare.\n"}
{"id": "21780446", "url": "https://en.wikipedia.org/wiki?curid=21780446", "title": "Species", "text": "Species\n\nIn biology, a species is the basic unit of classification and a taxonomic rank, as well as a unit of biodiversity, but it has proven difficult to find a satisfactory definition. Scientists and conservationists need a species definition which allows them to work, regardless of the theoretical difficulties. If as Carl Linnaeus thought, species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change continually, and to grade into one another. A species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. While this definition is often adequate, when looked at more closely it is problematic. For example, with hybridisation, in a species complex of hundreds of similar microspecies, or in a ring species, the boundaries between closely related species become unclear. Among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Problems also arise when dealing with fossils, since reproduction cannot be examined; the concept of the chronospecies is therefore used in palaeontology. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche.\n\nAll species are given a two-part name, a \"binomial\". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, \"Boa constrictor\" is one of four species of the genus \"Boa\".\n\nSpecies were seen from the time of Aristotle until the 18th century as fixed kinds that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book \"The Origin of Species\" explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.\n\nAs a practical matter, species concepts may be used to define species that are then used to measure biodiversity, though whether this is a good measure is disputed, as other measures are possible.\n\nIn his biology, Aristotle used the term γένος (génos) to mean a kind, such as a bird or fish, and εἶδος (eidos) to mean a specific form within a kind, such as (within the birds) the crane, eagle, crow, or sparrow. These terms were translated into Latin as \"genus\" and \"species\", though they do not correspond to the Linnean terms thus named; today the birds are a class, the cranes are a family, and the crows a genus. A kind was distinguished by its attributes; for instance, a bird has feathers, a beak, wings, a hard-shelled egg, and warm blood. A form was distinguished by being shared by all its members, the young inheriting any variations they might have from their parents. Aristotle believed all kinds and forms to be distinct and unchanging. His approach remained influential until the Renaissance.\n\nWhen observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray, an English naturalist, was the first to attempt a biological definition of species in 1686, as follows:\n\nIn the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences. He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships. At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the \"scala naturae\" or great chain of being. However, whether or not it was supposed to be fixed, the \"scala\" (a ladder) inherently implied the possibility of climbing.\n\nFaced with evidence of hybridisation, Linnaeus came to accept that species could change, and the struggle for survival, but not that new species could freely evolve. By the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 \"Zoological Philosophy\", described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.\n\nIn 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals. This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:\n\nThe commonly used names for kinds of organisms are often ambiguous: \"cat\" could mean the domestic cat, \"Felis catus\", or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean \"Puma concolor\" in various parts of America, while \"panther\" may also mean the jaguar (\"Panthera onca\") of Latin America or the leopard (\"Panthera pardus\") of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in \"Puma\", and the specific epithet as in \"concolor\".\n\nA species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens. Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are \"appropriate, compact, euphonious, memorable, and do not cause offence\".\n\nBooks and articles sometimes intentionally do not identify species fully and use the abbreviation \"sp.\" in the singular or \"spp.\" (standing for \"species pluralis\", the Latin for multiple species) in the plural in place of the specific name or epithet (e.g. \"Canis\" sp.) This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology. Authors may also use \"spp.\" as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. Abbreviations such as \"sp.\" should not be italicised. When a species identity is not clear a specialist may use \"cf.\" before the epithet to indicate that confirmation is required. The abbreviations \"nr.\" (near) or \"aff.\" (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.\n\nWith the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:\n\nThe naming of a particular species, including which genus (and higher taxa) it is placed in, is a \"hypothesis\" about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two named species are discovered to be of the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called \"synonymisation\". Dividing a taxon into multiple, often new, taxa is called \"splitting\". Taxonomists are often referred to as \"lumpers\" or \"splitters\" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.\n\nThe nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with \"sensu stricto\" (\"in the narrow sense\") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym \"sensu lato\" (\"in the broad sense\") denotes a wider usage, for instance including other subspecies. Other abbreviations such as \"auct.\" (\"author\"), and qualifiers such as \"non\" (\"not\") may be used to further clarify the sense in which the specified authors delineated or described the species.\n\nMost modern textbooks make use of Ernst Mayr's 1942 definition, known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as\n\nIt has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection. Mayr's use of the adjective \"potentially\" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.\n\nIt is difficult to define a species in a way that applies to all organisms. The debate about species delimitation is called the species problem. The problem was recognized even in 1859, when Darwin wrote in \"On the Origin of Species\":\n\nA simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:\n\n\nSpecies identification is made difficult by discordance between molecular and morphological investigations; these can be categorized as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages). In addition, horizontal gene transfer (HGT) makes it difficult to define a species. All species definitions assume that an organism acquires its genes from one or two parents very like the \"daughter\" organism, but that is not what happens in HGT. There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes, including some crustaceans and echinoderms.\n\nThe evolutionary biologist James Mallet concludes that\n\nThe species concept is further weakened by the existence of microspecies, groups of organisms, including many plants, with very little genetic variability, usually forming species aggregates. For example, the dandelion \"Taraxacum officinale\" and the blackberry \"Rubus fruticosus\" are aggregates with many microspecies—perhaps 400 in the case of the blackberry and over 200 in the dandelion, complicated by hybridisation, apomixis and polyploidy, making gene flow between populations difficult to determine, and their taxonomy debatable. Species complexes occur in insects such as \"Heliconius\" butterflies, vertebrates such as \"Hypsiboas\" treefrogs, and fungi such as the fly agaric.\n\nNatural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow \"Corvus corone\" and the hooded crow \"Corvus cornix\" appear and are classified as separate species, yet they hybridise freely where their geographical ranges overlap.\n\nA ring species is a connected series of neighbouring populations, each of which can sexually interbreed with adjacent related populations, but for which there exist at least two \"end\" populations in the series, which are too distantly related to interbreed, though there is a potential gene flow between each \"linked\" population. Such non-breeding, though genetically connected, \"end\" populations may co-exist in the same region thus closing the ring. Ring species thus present a difficulty for any species concept that relies on reproductive isolation. However, ring species are at best rare. Proposed examples include the herring gull-lesser black-backed gull complex around the North pole, the \"Ensatina eschscholtzii\" group of 19 populations of salamanders in America, and the greenish warbler in Asia, but many so-called ring species have turned out to be the result of misclassification leading to questions on whether there really are any ring species.\n\nBiologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test. Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others. Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts, and the philosopher of science John Wilkins counted 26. Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.\n\nA typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists. The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a \"classical\" method of determining species, such as with Linnaeus early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged \"Drosophila\" born to a two-winged mother is not a different species). Species named in this manner are called \"morphospecies\".\n\nIn the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on this, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms. It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.\n\nA mate-recognition species is a group of sexually reproducing organisms that recognize one another as potential mates. Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridize successfully, they are still distinct cohesion species if the amount of hybridization is insufficient to completely mix their respective gene pools. A further development of the recognition concept is provided by the biosemiotic concept of species.\n\nIn microbiology, genes can move freely even between distantly related bacteria, possibly extending to the whole bacterial domain. As a rule of thumb, microbiologists have assumed that kinds of Bacteria or Archaea with 16S ribosomal RNA gene sequences more similar than 97% to each other need to be checked by DNA-DNA hybridisation to decide if they belong to the same species or not. This concept was narrowed in 2006 to a similarity of 98.7%.\n\nDNA-DNA hybridisation is outdated, and results have sometimes led to misleading conclusions about species, as with the pomarine and great skua. Modern approaches compare sequence similarity using computational methods.\n\nDNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use. The so-called barcode is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data Systems (BOLD) contains DNA barcode sequences from over 190,000 species. However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently. Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.\n\nA phylogenetic or cladistic species is an evolutionarily divergent lineage, one that has maintained its hereditary integrity through time and space. A cladistic species is the smallest group of populations that can be distinguished by a unique set of morphological or genetic traits. Molecular markers may be used to determine genetic similarities in the nuclear or mitochondrial DNA of various species. For example, in a study done on fungi, studying the nucleotide characters using cladistic species produced the most accurate results in recognising the numerous fungi species of all the concepts studied. Versions of the Phylogenetic Species Concept may emphasize monophyly or diagnosability.\n\nUnlike the Biological Species Concept, a cladistic species does not rely on reproductive isolation, so it is independent of processes that are integral in other concepts. It works for asexual lineages, and can detect recent divergences, which the Morphological Species Concept cannot. However, it does not work in every situation, and may require more than one polymorphic locus to give an accurate result. The concept may lead to splitting of existing species, for example of Bovidae, into many new ones.\n\nAn evolutionary species, suggested by George Gaylord Simpson in 1951, is \"an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies\". This differs from the biological species concept in embodying persistence over time. Wiley and Mayden state that they see the evolutionary species concept as \"identical\" to Willi Hennig's species-as-lineages concept, and assert that the biological species concept, \"the several versions\" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggest that the concept works for both asexual and sexually-reproducing species.\n\nAn ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.\n\nA genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation. In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).\n\nAn evolutionarily significant unit (ESU) or \"wildlife species\" is a population of organisms considered distinct for purposes of conservation.\n\nIn palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the phyletically extinct one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.\n\nViruses have enormous populations, are doubtfully living since they consist of little more than a string of DNA or RNA in a protein coat, and mutate rapidly. All of these factors make conventional species concepts largely inapplicable. A viral quasispecies is a group of genotypes related by similar mutations, competing within a highly mutagenic environment, and hence governed by a mutation–selection balance. It is predicted that a viral quasispecies at a low but evolutionarily neutral and highly connected (that is, flat) region in the fitness landscape will outcompete a quasispecies located at a higher but narrower fitness peak in which the surrounding mutants are unfit, \"the quasispecies effect\" or the \"survival of the flattest\". There is no suggestion that a viral quasispecies resembles a traditional biological species.\n\nSpecies are subject to change, whether by evolving into new species, exchanging genes with other species, merging with other species or by becoming extinct.\n\nThe evolutionary process by which biological populations evolve to become distinct or reproductively isolated as species is called speciation. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book \"The Origin of Species\". Speciation depends on a measure of reproductive isolation, a reduced gene flow. This occurs most easily in allopatric speciation, where populations are separated geographically and can diverge gradually as mutations accumulate. Reproductive isolation is threatened by hybridisation, but this can be selected against once a pair of populations have incompatible alleles of the same gene, as described in the Bateson–Dobzhansky–Muller model. A different mechanism, phyletic speciation, involves one lineage gradually changing over time into a new and distinct form, without increasing the number of resultant species.\n\nHorizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.\n\nLouis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped \"into communities that regularly swap genes\", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.\n\nA species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface, and waters. Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a \"compilospecies\".\n\nBiologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; this was termed \"taxonomic inflation\", which could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties. Some observers claim that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise. \n\nConservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between lawmakers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridizes with the unprotected California spotted owl and the barred owl; this has led to legal debates. It has been argued that the species problem is created by the varied uses of the concept of species, and that the solution is to abandon it and all other taxonomic ranks, and use unranked monophyletic groups instead. It has been argued, too, that since species are not comparable, counting them is not a valid measure of biodiversity; alternative measures of phylogenetic biodiversity have been proposed.\n\n\n"}
{"id": "14698865", "url": "https://en.wikipedia.org/wiki?curid=14698865", "title": "Sponsored mobility", "text": "Sponsored mobility\n\nSponsored mobility refers to a system of social mobility where elite individuals in society select (either directly or through agents) recruits to induct into high status groups. This norm functions in opposition to contest mobility, in which everyone is seen as having equal opportunity to attain high status. \n\nThe definitive research article on the subject was published in 1960 by Ralph H. Turner. Turner compared the American and British systems of secondary education and found the two to be markedly different. He links the idea of sponsored mobility with the British system.\n\nIn a system of sponsored mobility, elite status is not earned, but given on the basis of some objective criterion and cannot be taken. Individuals must be sponsored by one or more members already in the elite circle in order to gain access. Those who are already members judge potential entrants on the extent to which they possess characteristics they wish to see in their future peers.\n\nSponsored mobility favors a much greater degree of social control than contest mobility. Individual effort means nothing; only qualities that grant sponsorship matter. In this system, credentials exist not to identify elite members to outsiders, but only to each other. These are things that only the elite are trained to recognize (i.e. Discrimination in cultural tastes) therefore making it difficult for interlopers to gain access to membership.\nOne instance where a system of sponsored mobility may develop into a contest mobility scenario is when there is no one dominant elite. Without a monopoly, elites must compete amongst themselves because no one group can control the recruitment process. \n\nIn comparing contest mobility with sponsored mobility, Deborah Abowitz surveyed American college students and examined effects of family class position, income, gender, and other status variables in determining attitudes about the norm that exists in the U.S. \n\nAbowitz studied college students instead of secondary students precisely because, “What brings college students' beliefs into question…is the effect of radical critiques of the system that they are often exposed to in college -- in courses which focus not on the rhetoric of an open class system but on evidence of social reproduction and the rising levels of inequality in wealth and income among Americans today.”\n\nHer study concluded that students do in fact subscribe to the achievement ideology enforced by contest mobility, and reject the sponsored mobility notion that background is a more important determinant of future status than effort.\n\nTurner, Ralph, H. (1960). Sponsored and Contest Mobility and the School System. \"American Sociological Review, 25(6)\", 855-862.\n\nAbowitz, Deborah A., (2005). SOCIAL Mobility and the American Dream: What Do College Students Believe? \"College Student Journal, 39(4)\"\n"}
{"id": "102913", "url": "https://en.wikipedia.org/wiki?curid=102913", "title": "Sunk cost", "text": "Sunk cost\n\nIn economics and business decision-making, a sunk cost is a cost that has already been incurred and cannot be recovered (also known as retrospective cost).\n\nSunk costs are sometimes contrasted with \"prospective costs\", which are future costs that may be incurred or changed if an action is taken. In that regard, both retrospective and prospective costs could be either fixed costs (continuous for as long as the business is in operation and unaffected by output volume) or variable costs (dependent on volume). However, many economists consider it a mistake to classify sunk costs as \"fixed\" or \"variable.\" For example, if a firm sinks $400 million on an enterprise software installation, that cost is \"sunk\" because it was a one-time expense and cannot be recovered once spent. A \"fixed\" cost would be monthly payments made as part of a service contract or licensing deal with the company that set up the software. The upfront irretrievable payment for the installation should \"not\" be deemed a \"fixed\" cost, with its cost spread out over time. Sunk costs should be kept separate. The \"variable costs\" for this project might include data centre power usage, for example.\n\nIn traditional microeconomic theory, only prospective (future) costs are relevant to an investment decision. The fields of traditional economics propose that economic actors should not let sunk costs influence their decisions. Doing so would not be rationally assessing a decision exclusively on its own merits. Alternatively, a decision-maker might make rational decisions according to their own incentives, outside of efficiency or profitability. This is considered to be an \"incentive problem\" and is distinct from a sunk cost problem. Evidence from behavioral economics suggests this theory may fail to predict real-world behavior. Sunk costs do, in fact, influence actors' decisions because humans are prone to loss aversion and framing effects. \n\nSunk costs should not affect the rational decision-maker's best choice. However, until a decision-maker irreversibly commits resources, the prospective cost is an avoidable future cost and is properly included in any decision-making processes.\n\nFor instance, if someone is considering preordering movie tickets, but has not actually purchased them yet, the cost remains avoidable. However, if the price of the tickets rises to an amount that requires him or her to pay more than the value he or she places on them, they should figure out the change in terms of prospective cost that goes into the decision-making process and re-evaluate his or her decision hence.\n\nThe sunk cost is distinct from economic loss. For example, when a new car is purchased, it can subsequently be resold; however, it will probably not be resold for the original purchase price. The economic loss is the difference (including transaction costs). The sum originally paid should not affect any rational future decision-making about the car, regardless of the resale value: if the owner can derive more value from selling the car than not selling it, then it should be sold, regardless of the price paid. In this sense, the sunk cost is not a precise quantity, but an economic term for a sum paid, in the past, which is no longer relevant to decisions about the future; it may be used inconsistently in quantitative terms as the original cost or the expected economic loss. It may also be used as shorthand for an error in analysis due to the \"sunk cost fallacy\", irrational decision-making or, most simply, as irrelevant data.\n\nEconomists argue that sunk costs are not taken into account when making rational decisions. In the case of a baseball game ticket that has already been purchased, the ticket-buyer can choose between the following two end results if he realizes that he doesn't like the game:\n\n\nIn either case, the ticket-buyer has \"paid the price of the ticket\" so \"that\" part of the decision no longer affects the future. If the ticket-buyer regrets buying the ticket, the current decision should be based on whether he wants to see the game at all, regardless of the price, just as if he were to go to a free baseball game. The economist will suggest that, since the second option involves suffering in only one way (spent money), while the first involves suffering in two (spent money plus wasted time), option two is obviously preferable.\n\nSunk costs may cause cost overrun. In business, an example of sunk costs may be investment into a factory or research that now has a lower value or no value whatsoever. For example, $20 million has been spent on building a power plant; the value at present is zero because it is incomplete (and no sale or recovery is feasible). The plant can be completed for an additional $10 million, or abandoned and a different but equally valuable facility built for $5 million. It should be obvious that abandonment and construction of the alternative facility is the more rational decision, even though it represents a total loss of the original expenditure—the original sum invested is a sunk cost. If decision-makers are irrational or have the wrong incentives, the completion of the project may be chosen. For example, politicians or managers may have more incentive to avoid the appearance of a total loss. In practice, there is considerable ambiguity and uncertainty in such cases, and decisions may in retrospect appear irrational that were, at the time, reasonable to the economic actors involved and in the context of their own incentives.\nBehavioral economics recognizes that sunk costs often affect economic decisions due to loss aversion: the price paid becomes a benchmark for the value, whereas the price paid should be irrelevant. This is considered irrational behavior (as rationality is defined by classical economics). Economic experiments have shown that the sunk cost fallacy and loss aversion are common; hence economic rationality—as assumed by much of economics—is limited. This has enormous implications for finance, economics, and securities markets in particular. Daniel Kahneman won the Nobel Prize in Economics in part for his extensive work in this area with his collaborator, Amos Tversky.\n\nTwo specific features characterizing the sunk cost heuristic worth mentioning are:\n\nIn 1968 Knox and Inkster, in what is perhaps the classic sunk cost experiment, approached 141 horse bettors: 72 of the people had just finished placing a $2.00 bet within the past 30 seconds, and 69 people were about to place a $2.00 bet in the next 30 seconds. Their hypothesis was that people who had just committed themselves to a course of action (betting $2.00) would reduce post-decision dissonance by believing more strongly than ever that they had picked a winner. Knox and Inkster asked the bettors to rate their horse's chances of winning on a 7-point scale. What they found was that people who were about to place a bet rated the chance that their horse would win at an average of 3.48 which corresponded to a \"fair chance of winning\" whereas people who had just finished betting gave an average rating of 4.81 which corresponded to a \"good chance of winning\". Their hypothesis was confirmed: after making a $2.00 commitment, people became more confident their bet would pay off. Knox and Inkster performed an ancillary test on the patrons of the horses themselves and managed (after normalization) to repeat their finding almost identically.\n\nAdditional evidence of inflated probability estimations can be found in Arkes and Blumer (1985) and Arkes and Hutzel (2000).\n\nIn a study of 96 business students in 1976, Staw and Fox gave the subjects a choice between making an R&D investment either in an underperforming company department, or in other sections of the hypothetical company. Staw and Fox divided the participants into two groups: a low responsibility condition and a high responsibility condition. In the high responsibility condition, the participants were told that they, as manager, had made an earlier, disappointing R&D investment. In the low responsibility condition, subjects were told that a former manager had made a previous R&D investment in the underperforming division and were given the same profit data as the other group. In both cases subjects were then asked to make a new $20 million investment. There was a significant interaction between assumed responsibility and average investment, with the high responsibility condition averaging $12.97 million and the low condition averaging $9.43 million.\n\nSimilar results have been obtained in earlier studies by Staw (1974, 1976) and by Arkes and Blumer (1985) and Whyte (1986).\n\nMany people have strong misgivings about \"wasting\" resources (loss aversion). In the above example involving a non-refundable sporting event ticket, many people, for example, would feel obliged to go to the event despite not really wanting to, because doing otherwise would be \"wasting\" the ticket worth. They usually feel they've passed the \"point of no return\". This is sometimes referred to as the \"sunk cost fallacy\". Economists would label this behavior as \"irrational\". It is inefficient because it misallocates resources by depending on information that is irrelevant to the decision being made.\n\nThis line of thinking, in turn, may reflect a non-standard measure of utility, which is ultimately subjective and unique to the consumer. A ticket-buyer who purchases a ticket to an event they won't enjoy in advance makes a semi-public commitment to watching it. To leave early is to make this lapse of judgment manifest to strangers, an appearance they might otherwise choose to avoid. Alternatively, they may take a sense of pride in having recognized the opportunity cost of the alternative use of time.\n\nThe idea of sunk costs is often employed when analyzing business decisions. A common example of a sunk cost for a business is the promotion of a brand name. This type of marketing incurs costs that cannot normally be recovered. It is not typically possible to later \"demote\" one's brand names in exchange for cash. A second example is R&D costs. Once spent, such costs are sunk and should have no effect on future pricing decisions. So a pharmaceutical company’s attempt to justify high prices because of the need to recoup R&D expenses is fallacious. The company will charge market prices whether R&D had cost one dollar or one million dollars. However, R&D costs, and the ability to recoup those costs, are a factor in deciding whether to spend the money on R&D or not. While this example is a fallacy, however, raising prices in order to finance future R&D is not considered to be one. \n\nSome research has also noted circumstances where the \"sunk cost fallacy\" is reversed; that is, where individuals appear irrationally eager to write off earlier investments in order to take up a new endeavor.\n\nThe \"sunk cost fallacy\" is in game theory sometimes known as the \"Concorde Fallacy\", referring to the fact that the British and French governments continued to fund the joint development of Concorde even after it became apparent that there was no longer an economic case for the aircraft. The project was regarded privately by the British government as a \"commercial disaster\" which should never have been started and was almost cancelled, but political and legal issues had ultimately made it impossible for either government to pull out.\n\nThe \"bygones principle\" is an economic theory used in business. Economists stress the \"extra\" or \"marginal\" costs and benefits of every decision. The theory emphasizes the importance of ignoring past costs and only taking into account the future costs and benefits when making decisions. It states that when making a decision, one should make a hard-headed calculation of the extra costs one will incur and weigh these against its extra advantages.\n\n"}
{"id": "52558881", "url": "https://en.wikipedia.org/wiki?curid=52558881", "title": "Tetens equation", "text": "Tetens equation\n\nThe Tetens equation is an equation to calculate the saturation vapour pressure of water over liquid and ice. It is named after its creator, O. Tetens who was an early German meteorologist. He published his equation in 1930, and while the publication itself is rather obscure, the equation is widely known among meteorologists and climatologists because of its ease of use and relative accuracy at temperatures within the normal ranges of natural weather conditions.\n\nMonteith and Unsworth (2008) provide Tetens' formula for temperatures above 0 °C:\nwhere temperature  is in degrees Celsius (°C) and saturation vapor pressure  is in kilopascals (kPa). According to Monteith and Unsworth, \"Values of saturation vapour pressure from Tetens' formula are within 1 Pa of exact values up to 35 °C.\"\n\nMurray (1967) provides Tetens' equation for temperatures below 0 °C:\n\n"}
{"id": "13474705", "url": "https://en.wikipedia.org/wiki?curid=13474705", "title": "Tilted large deviation principle", "text": "Tilted large deviation principle\n\nIn mathematics — specifically, in large deviations theory — the tilted large deviation principle is a result that allows one to generate a new large deviation principle from an old one by \"tilting\", i.e. integration against an exponential functional. It can be seen as an alternative formulation of Varadhan's lemma.\n\nLet \"X\" be a Polish space (i.e., a separable, completely metrizable topological space), and let (\"μ\") be a family of probability measures on \"X\" that satisfies the large deviation principle with rate function \"I\" : \"X\" → [0, +∞]. Let \"F\" : \"X\" → R be a continuous function that is bounded from above. For each Borel set \"S\" ⊆ \"X\", let\n\nand define a new family of probability measures (\"ν\") on \"X\" by\n\nThen (\"ν\") satisfies the large deviation principle on \"X\" with rate function \"I\" : \"X\" → [0, +∞] given by\n"}
{"id": "33861351", "url": "https://en.wikipedia.org/wiki?curid=33861351", "title": "UK Kindness Movement", "text": "UK Kindness Movement\n\nUK Kindness Movement is a non-monetary, independent organisation that promotes kindness campaigns, initiatives, and resources. It also serves as a think tank on kindness research.\n\nIt was founded in 2005 by Louise Burfitt-Dons as an associate campaign of children’s charity Act Against Bullying to promote their Cool to be Kind campaign and initiate other pro-social projects and initiatives. Since then it has designed and produced media campaigns across a range of sectors emphasising the importance of ethical behaviour in modern life.\n\nThe UK Kindness Movement co-founded Kindness Day UK . It was originally celebrated on November 13, the date decreed for World Kindness Day. but the date for the UK National Day of Kindness has been moved to March 31. Kindness Day has received a great amount of support from celebrities, enterprise, politicians and religious leaders. Supporters include Dame Barbara Stocking DBE, Chief Executive of Oxfam, The Rt HON Sir John Major KG CH, Russell Brand, Holly Willoughby and Sir Stuart Rose, Chairman of Marks and Spencer, The Rt Hon David Blunkett, Gary Lineker, Sir Alex Ferguson, Vanessa Feltz, Camilla Dallerup, Peter Snow, Jo Brand, Billy Murray, Patsy Kensit, Alan Titchmarsh, Brian Blessed, Jilly Cooper, Noel Edmonds, Charles Kennedy MP, Arlene Phillips, and Ruby Wax\n\nThe UK Kindness Movement has been researching the link between kindness and civility. The benefits of kindness are being welcomed since the global economic crisis and findings that more altruistic methods of doing business can be rewarding . Acts of kindness are linked to increased serotonin in the recipient of the kindness and the one being kind In April 2012 the UKKM published the Good Returns Report, which was the first comprehensive study of the fiscal cost of decreasing kindness on the economy.\n\nThere are many nationwide campaigns to promote kindness. Transport for London have been collecting stories from people on London Tubes and they are exhibited on trains and in stations \n\nIn April 25, 2009 the UKKM inspired Kindival, created for the award-winning documentary series Battlefront and Channel 4 led by Tom Robbins. The event took place at the Chelsea College of Art boosting The Kindness Offensive as its headline act.\n\nOn November 10, 2011, the UK Kindness Movement organised a public conference with the Big Society Network at Somerset House to debate the level of kindness in the City. This followed from concern that the London Riots indicated a decline of kindness and civility in modern Britain. A Young Foundation report suggested changes in national and local policy to counteract the drop in moral standards, including a better balance between punitive responses and actions that actively encourage civility.\n\nOther UK organisations, which promote acts of kindness include The Kindness Offensive and Kindness UK.\n\n"}
{"id": "1137466", "url": "https://en.wikipedia.org/wiki?curid=1137466", "title": "Underclass", "text": "Underclass\n\nThe underclass is the segment of the population that occupies the lowest possible position in a class hierarchy, below the core body of the working class.\n\nThe general idea that a class system includes a population \"under\" the working class has a long tradition in the social sciences (for example, lumpenproletariat). However, the specific term, \"underclass\", was popularized during the last half of the 20th century, first by social scientists of American poverty, and then by American journalists.\n\nThe underclass concept has been a point of controversy among social scientists. Definitions and explanations of the underclass, as well as proposed solutions for managing or fixing the \"underclass problem\" have been highly debated. \n\nGunnar Myrdal is generally credited as the first proponent of the term \"underclass.\" Writing in the early 1960s on economic inequality in the U.S., Myrdal's underclass refers to a \"class of unemployed, unemployables, and underemployed, who are more and more hopelessly set apart from the nation at large, and do not share in its life, its ambitions, and its achievements\". However, this general conception of a class or category of people below the core of the working class has a long tradition in the social sciences, such as through the work of Henry Mayhew, whose \"London Labour and the London Poor\" sought to describe the hitherto invisible world of casual workers, prostitutes, and street-people.\n\nThe specific concept of an underclass in the U.S. underwent several transformations during the decades following Myrdal's introduction of the term. According to sociologist Herbert Gans, while Myrdal's structural conceptualization of the underclass remained relatively intact through the writings of William Julius Wilson and others, in several respects the \"structural\" definition was abandoned by many journalists and academics, and replaced with a \"behavioral\" conception of the underclass, which fuses Myrdal's term with Oscar Lewis's and others' conception of a \"culture of poverty\".\n\nVarious definitions of the underclass have been set forth since the term's initial conception; however, all of these definitions are basically different ways of imagining a category of people beneath the working class. The definitions vary by which particular dimensions of this group are highlighted. A few popular descriptions of the underclass are considered as follows.\n\nErik Olin Wright defines the underclass as a \"category of social agents who are economically oppressed but not consistently exploited within a given class system\". The underclass occupies the lowest possible rung on a class ladder. According to Wright, the underclass are oppressed because they are generally denied access to the labor market, and thus they are \"not consistently exploited\" because the opportunity for their economic exploitation is minimal.\n\nUnlike the working class, which is routinely exploited for their labor power, the underclass, generally speaking, do not hold the labor power worthy of exploitation. Wright argues,\n\nThis quote partly concerns the spaces and locations for the underclass.\n\nThe underclass generally occupies specific zones in the city. Thus, the notion of an underclass is popular in Urban Sociology, and particularly in accounts of urban poverty. The term, \"underclass\", and the phrase, \"urban underclass\", are, for the most part, used interchangeably. Studies concerning the post-civil rights African American ghetto often include a discussion of the urban underclass. Many writings concerning the underclass, particularly in the U.S., are urban-focused.\n\nWilliam Julius Wilson's books,\" The Declining Significance of Race\" (1978) and \"The Truly Disadvantaged\" (1987), are popular accounts of the black urban underclass. Wilson defines the underclass as \"a massive population at the very bottom of the social ladder plagued by poor education and low-paying jobs.\" He generally limits his discussion to those trapped in the post-civil-rights ghetto in the American rust belt (see \"Potential Causes and Proposed Solutions\" section of this entry for a more detailed summary of Wilson on the underclass).\n\nElijah Anderson's, \"Streetwise\" (1990), employs ethnographic methods to study a gentrifying neighborhood, \"The Village\" (pseudonym), bordering a black ghetto, \"Northton\" (pseudonym), in an American city. Anderson provides the following description of the underclass in this ghetto:\n\nLawrence M. Mead defines the underclass as a group that is poor and behaviorally deficient. He describes the underclass as \"dysfunctional.\" He provides the following definition in his 1986 book, \"Beyond Entitlement\",\n\nKen Auletta, often credited as the primary journalist who brought the underclass term to the forefront of the American consciousness, describes the American underclass as non-assimilated Americans, and he suggests that the underclass may be subcategorized into four distinct groups:\n\nEach of the above definitions are said to conceptualize the same general group – the American underclass – but they provide somewhat competing imagery. While Wright, Wilson, and Anderson each position the underclass in reference to the labor market, Auletta's definition is simply \"non-assimilation\" and his examples, along with Mead's definition, highlight underclass members' participation in deviant behavior and their adoption of an antisocial outlook on life. These controversies are elaborated further in the next section (\"Characteristics of the Underclass\").\n\nAs evident with Mead and Auletta's framing, some definitions of the underclass significantly diverge from the initial notion of an \"economic group\" beneath the working class. A few writings on the underclass distinguish between various types of underclass, such as the social underclass, the impoverished underclass, the reproductive underclass, the educational underclass, the violent underclass, and the criminal underclass, with some expected horizontal mobility between these groups. Even more divergent from the initial notion of an underclass are the recent journalistic accounts of a so-called \"genetic underclass\", referring to a genetic inheritance of a predisposition to addiction and other personality traits traditionally associated with behavioral definitions of the underclass. However, such distinctions between criminal, social, impoverished, and other specified underclass terms still refer to the same general group—those beneath the working class. And, despite recent journalistic accounts of a \"genetic underclass\", the underclass concept is primarily, and has traditionally been, a social science term.\n\nThe underclass is located by a collection of identifying characteristics, such as high levels of joblessness, out-of-wedlock births, female-headed households, crime, violence, substance abuse, and high school dropout rates. The underclass harbors these traits to a greater degree than the general population, and other classes more specifically.\n\nJoel Rogers and James Wright identify four general themes by which these characteristics are organized within academic and journalistic accounts of the underclass: economic, social-psychological, behavioral, and ecological (spatial concentration).\n\nThe economic dimension is the most basic and least contested theme of the underclass – the underclass is overwhelmingly poor. The underclass experiences high levels of joblessness, and what little employment its members hold in the formal economy is best described as precarious labor. However, it is important to note that simply being poor is not synonymous with being part of the underclass. The underclass is \"persistently poor\" and, for most definitions, the underclass live in areas of concentrated poverty. Some scholars, such as Ricketts and Sawhill, argue that being poor is not a requirement for underclass membership, and thus there are individuals who are non-poor members of the underclass because they live in \"underclass areas\" and embody other characteristics of the underclass, such as being violent, criminal, and anti-social (e.g., gang leaders).\n\nMany writers often highlight the social-psychological dimensions of the underclass. The underclass is often framed as holding beliefs, attitudes, opinions, and desires that are inconsistent with those held by society at large. The underclass is frequently described as a \"discouraged\" group with members who feel \"cut off\" from mainstream society. Linked to this discussion of the underclass being psychologically deviant, the underclass is also said to have low levels of cognition and literacy. Thus, the underclass is often seen as being mentally disconnected from the rest of society. Consider the following:\n\nNot only is the underclass frequently said to think differently, they are also said to behave differently. Some believe that the underclass concept was meant to capture the coincidence of a number of social ills including poverty, joblessness, crime, welfare dependence, fatherless families, and low levels of education or work related skills. These behavioral characteristics, coupled with arguments that the underclass is psychologically disconnected from mainstream society, are occasionally highlighted as evidence that the underclass live in a subculture of poverty. From this point of view, members of the underclass embody a distinct set of thoughts, perceptions, and actions – a \"style of life\" - that are transmitted across generations. However, just as the conceptualization of a \"culture of poverty\" in general is debated, so too are the attempts to frame the underclass as members of such a culture.\n\nThe ecological dimension, a fourth theme in the literature on the underclass, is often used as both a description and an explanation for the underclass. The underclass is concentrated in specific areas. Although there are some writings on the \"rural underclass\", in general the underclass is framed as an urban phenomenon and the phrases \"ghetto poverty\" and \"inner-city poverty\" are often used synonymously with the underclass term. However, many scholars are careful not to equate concentrated poverty with the underclass. Living in areas of concentrated poverty is more or less framed as a common (and often necessary) condition of the underclass, but it is generally not considered a sufficient condition since many conceptualizations of the underclass highlight behavioral and psychological deviancy that may not necessarily persist in high-poverty areas. In Wilson's writings on the underclass – a term he eventually replaces with \"ghetto poverty\" (see section titled \"Critiques of the Underclass Concept\")– the underclass is described as a population that is physically and socially isolated from individuals and institutions of mainstream society, and this isolation is one of a collection of causes to concentrated poverty and why the \"social dislocations\" (e.g., crime, school dropouts, out of wed-lock pregnancy, etc.) of the underclass emerge.\n\nThus, the underclass is defined and identified by multiple characteristics. Members are persistently poor and experience high levels of joblessness. However, these trends are generally not seen as sufficient identifiers of the underclass, because, for many, the underclass concept also captures dimensions of psychological and behavioral deviancy. Furthermore, the underclass is generally identified as an urban phenomenon with its members typically living in areas of concentrated poverty.\n\nSimilar to issues of defining and identifying the underclass, the outlining of potential causes and proposed solutions for the \"underclass problem\" have also been points of contestation. Debates concerning the diagnosis of, and prescription for, the underclass often mirror debates concerning first world poverty more generally. However, in many writings on the specific notion of the underclass, some particular causes and solutions have been set forth.\n\nA few of these propositions are outlined below, including those developed by William Julius Wilson, Douglas Massey and Nancy Denton, Lawrence M. Mead, and Ken Auletta. The work by these authors' certainly do not compile an exhaustive list of suggested causes or solutions for the underclass, but they are arguably the most read proposals among social scientists. The contrasting causes and solutions highlighted by Wilson and Mead in particular have been popular points for debate. However, because prescription is dependent on diagnosis, much of the debates between Wilson and Mead have been on the causes and conditions of the underclass. Wilson highlights social isolation and the disappearance of quality work (for example, via deindustrialization and offshore labor outsourcing) for ghetto residents, while Mead highlights an overgenerous and permissive welfare state. Massey and Denton link the creation of the underclass to racial residential segregation and advocate for policies encouraging desegregation. Auletta provides a different policy framework discussion by highlighting two extreme positions (the wholesale option and the laissez-faire option) and one middle-of-the-road position (the retail option), but these are more discussions concerning the amount of public resources that should be dedicated to fixing, or attempting to fix, the underclass problem, rather than specific strategies. Auletta seems to support the retail option, which would provide aid to underclass members deserving and hopeful and withhold aid to members undeserving and hopeless.\n\nFor Wilson, the cause of the underclass is structural. In \"The Truly Disadvantaged\", Wilson highlights a conglomerate of factors in the last half of the twentieth century leading to a growing urban underclass. The factors listed include but are not limited to the shift from a goods-producing economy to a service-producing economy (including deindustrialization) and the offshore outsourcing of labor not only in the industrial sector but also in substantial portions of the remaining service sector. These factors are aggravated by the exodus of the middle and upper classes from the inner city (first the well-known \"white flight\" but later the less-studied departure of the black middle class), which creates a \"spatial mismatch\" between where low-income people live (inner-city neighborhoods) and where low-skill service-sector jobs are available (the suburbs). The result is the transformation of the post-civil-rights-era inner city into a \"ghetto\" whose residents are isolated from mainstream institutions.\n\nWilson proposes a comprehensive social and economic program that is primarily universal, but nevertheless includes targeted efforts to improve the life chances of the ghetto underclass and other disadvantaged groups. Wilson lists multiple examples of what this universal program would include, such as public funding of training, retraining, and transitional employment benefits that would be available to all members of society. With respect to the diagnosis of concentration and isolation, Wilson suggests the promotion of social mobility, through programs that will increase employment prospects for the underclass, will lead to geographic mobility. Wilson describes his proposed program as having a \"hidden agenda\" for policy makers \"to improve the life chances of truly disadvantaged groups such as the ghetto underclass by emphasizing programs to which the more advantaged groups of all races and class backgrounds can positively relate\". Universal programs are more easily accepted within the US' political climate than targeted programs, yet the underclass would likely experience the most benefit from universal programs. Wilson notes that some means-tested programs are still necessary, but recommends that they be framed as secondary to universal programming efforts. The following quote summarizes his policy call:\n\nIn their 1993 book, \"American Apartheid\", sociologists Douglas Massey and Nancy Denton concur with much of Wilson's suggested causes and proposed solutions, but introduce racial residential segregation (as an outcome of both institutionalized and individual-level discrimination) as an explanatory factor. Massey and Denton argue that racial residential segregation is primarily an outcome of institutionalized racism in real estate and banking, coupled with, and significantly motivated by, individual-level prejudice and discrimination. They provide the following summary,\nGiven the prominent role of segregation in the construction and maintenance of the urban underclass, Massey and Denton call for policies that promote desegregation. They provide a detailed list of policy suggestions in the closing of their book. They argue that policies aimed at desegregation need to target the private housing market, where an overwhelming majority of housing is allocated. In doing this, the authors call upon the federal government to dedicate more resources to the upholding of the Fair Housing Act, including speedy judicial action against violators (to strengthen deterrent effects of the legislation).\n\nMead argues that the core cause of the underclass problem (or at least the perpetuation of the underclass problem) is welfare. Mead argues that most welfare programs encourage social dysfunctions, including welfare dependency, illegitimate births, joblessness, and crime. For Mead, welfare is too permissive and provides benefits to the underclass without requirements for its members to change their behavior and lifestyle.\n\nMead's diagnosis that permissive welfare is a primary cause of the underclass problem is followed by a prescription for a more authoritative welfare program that combines benefits with requirements. This proposal is often called \"workfare\", which requires welfare recipients to work in order to receive aid. For Mead, such a program design would evoke behavioral change since permissiveness is replaced with authority. Mead summarizes his call to replace permissive welfare with authoritative welfare:\n\nKen Auletta closes his book, \"The Underclass\" (1982), by highlighting three typologies of solutions: \"the wholesale option\", \"the laissez-faire option\", and \"the retail option\".\n\nThe \"wholesale option\" includes both conservatives and liberals who are optimistic that government action can solve the underclass problem. According to Auletta, left-wing wholesale proponents call for increased public aid while right-wing wholesale proponents call for government to reduce taxes to increase jobs (inspired by trickle-down economic theory) and charge the government to \"get tough\" on underclass crime and welfare dependency.\n\nThe \"laissez-faire option\" is pessimistic and its proponents are extremely wary of proposed solutions to a problem they see as unsolvable. Proponents of this perspective call for a drastic withdrawal of public aid for the underclass and are concerned with \"quarantining the patient\" instead of hunting for what they believe is an imaginary cure. In other words, the laissez-faire option assumes that the underclass is generally hopeless, and thus the only public effort given to them should be the bare minimum.\n\nThe \"retail option\" includes those in between optimism and pessimism, what Auletta calls \"skeptics\". The retail option advocates for targeted efforts, recognizing the limits of government intervention, but is also aware of the positive impact social policy can have on efforts to fix specific problems of the underclass. This middle ground perspective requests that aid be given to members of the underclass considered to be deserving of aid, but withheld from members considered to be undeserving. However, proponents of the retail option often disagree on which members of the underclass are considered deserving and which are not. This appears to be the approach embraced by Auletta as he closes his book with reflections on some of the people he interviews throughout preceding pages. He says, \"I have no difficulty giving up on violent criminals like the Bolden brothers or street hustlers like Henry Rivera. But knowing how a government helping hand made it possible for Pearl Dawson and William Mason to succeed, would you be willing to write them off?\"\n\nSocial scientists often point to journalism as a primary institution conceptualizing the underclass for a mass audience. Many suggest that the underclass terminology employed by American journalists in the last quarter of the twentieth-century were partial to behavioral and cultural—as opposed to a structural—definitions of the underclass.\n\nWhile journalists' use of the underclass term is vast, a few popular sources are frequently cited in the academic literature on the underclass and journalism. Ken Auletta employed the underclass term in three articles published in The New Yorker in 1981, and in book form a year later. Auletta is arguably the most read journalist of the underclass and many of his ideas, including his definition of the underclass, are included in this Wikipedia entry.\n\nAnother notable journalist is Nicholas Lemann who published a handful of articles on the underclass in the \"Atlantic Monthly\" during the late 1980s and early 1990s. His 1986 writings on \"The Origins of the Underclass\" argue that the underclass was created by two migrations, the great migration of Southern blacks to the North and West during the early to mid twentieth century and the exodus of middle class blacks out of the ghetto during the 1970s through the early 90s. In 1991 Lemann also published an article titled \"The Other Underclass\", which details Puerto Ricans, and particularly Puerto Ricans residing in South Bronx, as members of the urban underclass in the US.\n\nFollowing the popularization of the underclass concept in both academic and journalistic writings, some academics began to overtly criticize underclass terminology. Those in opposition to the underclass concept generally argue that, on the one hand, \"underclass\" is a homogenizing term that simplifies a heterogeneous group, and on the other hand, the term is derogatory and demonizes the urban poor.\n\nMany who refute the underclass concept suggest that the \"underclass\" term has been transformed into a codeword to refer to poor inner-city blacks. For example, Hilary Silver highlights a moment when David Duke, former Grand Wizard of the KKK, campaigned for Louisiana Governor by complaining about the \"welfare underclass\". The underclass concept has been politicized, with those from the political left arguing that joblessness and insufficient welfare provided are causes of underclass conditions while the political right employ the underclass term to refer to welfare dependency and moral decline. Many sociologists suggest that this latter rhetoric – the right-wing perspective – became dominant in mainstream accounts of the underclass during the later decades of the twentieth-century.\n\nHerbert Gans is one of the most vocal critics of the underclass concept. Gans suggests that American journalists, inspired partly by academic writings on the \"culture of poverty\", reframed \"underclass\" from a structural term (in other words, defining the underclass in reference to conditions of social/economic/political structure) to a behavioral term (in other words, defining the underclass in reference to rational choice and/or in reference to a subculture of poverty). Gans suggests that the word \"underclass\" has become synonymous with impoverished blacks that behave in criminal, deviant, or \"just non-middle-class ways\".\n\nLoïc Wacquant deploys a relatively similar critique by arguing that \"underclass\" has become a blanket term that frames urban blacks as behaviorally and culturally deviant. Wacquant notes that underclass status is imposed on urban blacks from outside and above them (e.g., by journalists, politicians, and academics), stating that \"underclass\" is a derogatory and \"negative label that nobody claims or invokes except to pin it on to others\". And, although the underclass concepts is homogenizing, Wacquant argues that underclass imagery differentiates on gender lines, with the underclass male being depicted as a violent \"gang banger\", a physical threat to public safety, and the underclass female being generalized as \"welfare mother\" (also see welfare queen), a \"moral assault on American values\".\n\nThe concept of 'the ghetto' and 'underclass' has also faced criticism empirically. Research has shown significant differences in resources for neighborhoods with similar populations both across cities and over time. This includes differences in the resources of neighborhoods with predominantly low income and/or racial minority populations. The cause of these differences in resources across similar neighborhoods has more to do with dynamics outside of the neighborhood. To a large extent the problem with the 'ghetto' and 'underclass' concepts stem from the reliance on case studies (in particular case studies from Chicago), which confine social scientist understandings of socially disadvantaged neighborhoods.\n\nThe charges against underclass terminology have motivated replacement terms. For example, William Julius Wilson, sympathetic to criticisms brought against underclass terminology (particularly those criticisms posited by Gans), begins to replace his use of the term underclass with \"ghetto poor\" during the early 1990s. For Wilson, this replacement terminology is simply an attempt to revamp the framing of inner-city poverty as being structurally rooted. He states, \"I will substitute the term 'ghetto poor' for the term 'underclass' and hope that I will not lose any of the subtle theoretical meaning that the latter term has had in my writings.\"\n"}
{"id": "42271443", "url": "https://en.wikipedia.org/wiki?curid=42271443", "title": "Wen and wu", "text": "Wen and wu\n\nWén 文 and wǔ 武 - a conceptual pair in Chinese philosophy and political culture describing opposition and complementarity of civil ① and military ② realms of government. Differentiation between \"wen\" and \"wu\" was engaged in discussions on criminal punishment, administrative control, creation and reproduction of social order, education and moral transformation.\n\nThe concept was formed during the Chunqiu and Warring States periods, and best articulated in the 3rd or 2nd century BCE. However, until recently it was not much discussed by the Western scholars because of ① their aberrated perception of the importance of Confucianism in the pre-imperial and early imperial era, and ② their understanding of Confucianism as pacifist in its nature. An example of the last is provided by John K. Fairbank: “Warfare was disesteemed in Confucianism... The resort to warfare (\"wu\") was an admission of bankruptcy in the pursuit of \"wen\" [civility or culture]. Consequently, it should be a last resort... Herein lies the pacifist bias of the Chinese tradition... Expansion through \"wen\"... was natural and proper; whereas expansion by \"wu\", brute force and conquest, was never to be condoned.”\n\nThe posthumous names of the Zhou dynasty (1046-256 BCE) founders, King Wen and King Wu, represent the two terms as standing in the \"father-and-son\" relationship. Since the conquest of Shang and creation of the Zhou imperial order were the most discussed events of the classical era, the two impersonated terms had very broad currency. However, their origin is presently impossible to pinpoint.\n\n\"Shuoyuan\", compiled by Liu Xiang (77-6 BCE, Han dynasty), gives a classical example of the terms' balancing against each other: \n\nKing Cheng enfeoffed Bo Qin [the Duke of Zhou's son] as the Duke of Lu. Summoning him, he addressed him, saying: \"Do you know the Way of acting as the ruler over the people? ... Should you possess the civil but lack the martial, you will have no means to awe those below. Should you possess the martial but lack the civil, the people will fear you but not draw close. If the civil and martial are implemented together, then your awe-inspiring virtue will be achieved.\"\n\n\n"}
