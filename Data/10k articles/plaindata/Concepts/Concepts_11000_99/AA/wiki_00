{"id": "60487", "url": "https://en.wikipedia.org/wiki?curid=60487", "title": "Abscissa and ordinate", "text": "Abscissa and ordinate\n\nIn mathematics, the abscissa (; plural \"abscissae\" or \"abscissæ\" or \"abscissas\") and the ordinate are respectively the first and second coordinate of a point in a coordinate system.\n\nThe abscissa of a point is the signed measure of its projection on the primary axis, whose absolute value is the distance between the projection and the origin of the axis, and whose sign is given by the location on the projection relative to the origin (before: negative; after: positive).\n\nThe ordinate of a point is the signed measure of its projection on the secondary axis, whose absolute value is the distance between the projection and the origin of the axis, and whose sign is given by the location on the projection relative to the origin (before: negative; after: positive).\n\nUsually these are the horizontal and vertical coordinates of a point in a two-dimensional rectangular Cartesian coordinate system. The terms can also refer to the horizontal and vertical axes respectively (typically \"x\"-axis and \"y\"–axis) of a two-dimensional graph. An ordered pair consists of two terms—the abscissa (horizontal, usually \"x\") and the ordinate (vertical, usually \"y\")—which define the location of a point in two-dimensional rectangular space. \n\nThough the word \"abscissa\" (Latin; \"linea abscissa\", \"a line cut off\") has been used at least since \"De Practica Geometrie\" published in 1220 by Fibonacci (Leonardo of Pisa), its use in its modern sense may be due to Venetian mathematician Stefano degli Angeli in his work \"Miscellaneum Hyperbolicum, et Parabolicum\" of 1659.\n\nIn his 1892 work \"Vorlesungen über Geschichte der Mathematik, Volume 2,\" (\"Lectures on history of mathematics\") German historian of mathematics Moritz Cantor writes\n\nIn a somewhat obsolete variant usage, the abscissa of a point may also refer to any number that describes the point's location along some path, e.g. the parameter of a parametric equation. Used in this way, the abscissa can be thought of as a coordinate-geometry analog to the independent variable in a mathematical model or experiment (with any ordinates filling a role analogous to dependent variables).\n\n"}
{"id": "44746918", "url": "https://en.wikipedia.org/wiki?curid=44746918", "title": "Adhikarivada", "text": "Adhikarivada\n\nAdhikārivāda is the doctrine of special rights and privileges with regard to the right to universal knowledge of the Upanishads. \n\nVivekananda rejected this doctrine which, in his view, was the outcome of pure selfishness and ignored both the infinite possibilities of the human soul and the fact that all men are capable of receiving knowledge if it is imparted in their own respective languages. Therefore, he claimed, there is no reason for access to the universal knowledge of the Upanishads to be restricted on the basis of caste or religion.\n\n \n"}
{"id": "296639", "url": "https://en.wikipedia.org/wiki?curid=296639", "title": "Arrow of time", "text": "Arrow of time\n\nThe Arrow of Time, or Time's Arrow, is the concept of the \"one-way direction\" or \"asymmetry\" of time. It was\ndeveloped in 1927 by the British astronomer Arthur Eddington, and is an unsolved general physics question. This direction, according to Eddington, can be determined by studying the organization of atoms, molecules, and bodies, and might be drawn upon a four-dimensional relativistic map of the world (\"a solid block of paper\").\n\nPhysical processes at the microscopic level are believed to be either entirely or mostly time-symmetric: if the direction of time were to reverse, the theoretical statements that describe them would remain true. Yet at the macroscopic level it often appears that this is not the case: there is an obvious direction (or \"flow\") of time.\n\nIn the 1928 book \"The Nature of the Physical World\", which helped to popularize the concept, Eddington stated:\n\nLet us draw an arrow arbitrarily. If as we follow the arrow we find more and more of the random element in the state of the world, then the arrow is pointing towards the future; if the random element decreases the arrow points towards the past. That is the only distinction known to physics. This follows at once if our fundamental contention is admitted that the introduction of randomness is the only thing which cannot be undone. I shall use the phrase ‘time's arrow’ to express this one-way property of time which has no analogue in space.\n\nEddington then gives three points to note about this arrow:\n\n\nAccording to Eddington the arrow indicates the direction of progressive increase of the random element. Following a lengthy argument upon the nature of thermodynamics he concludes that, so far as physics is concerned, time's arrow is a property of entropy alone.\n\nThe symmetry of time (T-symmetry) can be understood by a simple analogy: if time were perfectly symmetrical, a video of real events would seem realistic whether played forwards or backwards. An obvious objection to this notion is gravity: things fall down, not up. Yet a ball that is tossed up, slows to a stop, and falls into the hand is a case where recordings would look equally realistic forwards and backwards. The system is T-symmetrical, but while going \"forward\", kinetic energy is dissipated and entropy is increased. Entropy may be one of the few processes that is not time-reversible. According to the statistical notion of increasing entropy, the \"arrow\" of time is identified with a decrease of free energy.\n\nThe arrow of time is the \"one-way direction\" or \"asymmetry\" of time. The thermodynamic arrow of time is provided by the second law of thermodynamics, which says that in an isolated system, entropy tends to increase with time. Entropy can be thought of as a measure of microscopic disorder; thus the second law implies that time is asymmetrical with respect to the amount of order in an isolated system: as a system advances through time, it becomes more statistically disordered. This asymmetry can be used empirically to distinguish between future and past, though measuring entropy does not accurately measure time. Also, in an open system, entropy can decrease with time.\n\nBritish physicist Sir Alfred Brian Pippard wrote, \"There is thus no justification for the view, often glibly repeated, that the Second Law of Thermodynamics is only statistically true, in the sense that microscopic violations repeatedly occur, but never violations of any serious magnitude. On the contrary, no evidence has ever been presented that the Second Law breaks down under any circumstances.\" However, there are a number of paradoxes regarding violation of the second law of thermodynamics, one of them due to the Poincaré recurrence theorem.\n\nThis arrow of time seems to be related to all other arrows of time and arguably underlies some of them, with the exception of the weak arrow of time.\n\nHarold Blum's 1951 book \"Time's Arrow and Evolution\" \"explored the relationship between time's arrow (the second law of thermodynamics) and organic evolution.\" This influential text explores \"irreversibility and direction in evolution and order, negentropy, and evolution.\" Blum argues that evolution followed specific patterns predetermined by the inorganic nature of the earth and its thermodynamic processes.\n\nThe cosmological arrow of time points in the direction of the universe's expansion. It may be linked to the thermodynamic arrow, with the universe heading towards a heat death \"(Big Chill)\" as the amount of usable energy becomes negligible. Alternatively, it may be an artifact of our place in the universe's evolution (see the Anthropic bias), with this arrow reversing as gravity pulls everything back into a Big Crunch.\n\nIf this arrow of time is related to the other arrows of time, then the future is \"by definition\" the direction towards which the universe becomes bigger. Thus, the universe expands — rather than shrinks — by definition.\n\nThe thermodynamic arrow of time and the second law of thermodynamics are thought to be a consequence of the initial conditions in the early universe. Therefore they ultimately result from the cosmological set-up.\n\nWaves, from radio waves to sound waves to those on a pond from throwing a stone, expand outward from their source, even though the wave equations accommodate solutions of convergent waves as well as radiative ones. This arrow has been reversed in carefully worked experiments that created convergent waves, so this arrow probably follows from the thermodynamic arrow in that meeting the conditions to produce a convergent wave requires more order than the conditions for a radiative wave. Put differently, the probability for initial conditions that produce a convergent wave is much lower than the probability for initial conditions that produce a radiative wave. In fact, normally a radiative wave increases entropy, while a convergent wave decreases it, making the latter contradictory to the second law of thermodynamics in usual circumstances.\n\nA cause precedes its effect: the causal event occurs before the event it affects. Birth, for example, follows a successful conception and not vice versa. Thus causality is intimately bound up with time's arrow.\n\nAn epistemological problem with using causality as an arrow of time is that, as David Hume maintained, the causal relation per se cannot be perceived; one only perceives sequences of events. Furthermore, it is surprisingly difficult to provide a clear explanation of what the terms cause and effect really mean, or to define the events to which they refer. However, it does seem evident that dropping a cup of water is a cause while the cup subsequently shattering and spilling the water is the effect.\n\nPhysically speaking, the perception of cause and effect in the dropped cup example is a phenomenon of the thermodynamic arrow of time, a consequence of the second law of thermodynamics. Controlling the future, or causing something to happen, creates correlations between the doer and the effect, and these can only be created as we move forwards in time, not backwards.\n\nCertain subatomic interactions involving the weak nuclear force violate the conservation of both parity and charge conjugation, but only very rarely. An example is the kaon decay. According to the CPT Theorem, this means they should also be time irreversible, and so establish an arrow of time. Such processes should be responsible for matter creation in the early universe.\n\nThat the combination of parity and charge conjugation is broken so rarely means that this arrow only \"barely\" points in one direction, setting it apart from the other arrows whose direction is much more obvious. This arrow had not been linked to any large scale temporal behaviour until the work of Joan Vaccaro, who showed that T violation could be responsible for conservation laws and dynamics.\n\nAccording to the Copenhagen interpretation of quantum mechanics, quantum evolution is governed by the Schrödinger equation, which is time-symmetric, and by wave function collapse, which is time irreversible. As the mechanism of wave function collapse is philosophically obscure, it is not completely clear how this arrow links to the others. Despite the post-measurement state being entirely stochastic in formulations of quantum mechanics, a link to the thermodynamic arrow has been proposed, noting that the second law of thermodynamics amounts to an observation that nature shows a bias for collapsing wave functions into higher entropy states versus lower ones, and the claim that this is merely due to more possible states being high entropy runs afoul of Loschmidt's paradox. According to one physical view of wave function collapse, the theory of quantum decoherence, the quantum arrow of time is a consequence of the thermodynamic arrow of time.\n\nRelational quantum mechanics proposes that there is no such thing as an absolute wave function collapse, and that what an observer sees as wave function collapse is in fact the observer becoming entangled with the measured state. The thermodynamic arrow is an increase in entanglement over time; in this way, relational quantum mechanics relates the quantum arrow to the thermodynamic arrow.\n\nPhysicists say that quantum uncertainty gives rise to entanglement, the putative source of the arrow of time. The idea that entanglement might explain the arrow of time was proposed by Seth Lloyd in the 1980s. Lloyd argues that quantum uncertainty, and the way it spreads as particles become increasingly entangled, could replace human uncertainty in the old classical proofs as the true source of the arrow of time. According to Lloyd; “The arrow of time is an arrow of increasing correlations.”\n\nA related mental arrow arises because one has the sense that one's perception is a continuous movement from the known (past) to the unknown (future). Anticipating the unknown forms the psychological future, which always seems to be something one is moving towards. However, like a projection in a mirror, it makes what is actually already a part of memory, such as desires, dreams, and hopes, seem ahead of the observer.\n\nThe association of \"behind ⇔ past\" and \"ahead ⇔ future\" is itself culturally determined. For example, the Aymara language associates \"ahead ⇔ past\" and \"behind ⇔ future\". Similarly, the Chinese term for \"the day after tomorrow\" 後天 (\"hòu tiān\") literally means \"after (or behind) day\", whereas \"the day before yesterday\" 前天 (\"qián tiān\") is literally \"preceding (or in front) day.\"\n\nThe words \"yesterday\" and \"tomorrow\" both translate to the same word in Hindi: कल (\"kal\"), meaning \"[one] day remote from today.\" The ambiguity is resolved by verb tense. परसों (\"parsoⁿ\") is used for both \"day before yesterday\" and \"day after tomorrow\", or \"two days from today\". नरसों (\"narsoⁿ\") is used for \"three days from today.\"\n\nThe other side of the psychological passage of time is in the realm of volition and action. We plan and often execute actions intended to affect the course of events in the future. From the Rubaiyat:\n\n<poem>\nThe Moving Finger writes; and, having writ,\n  Moves on: nor all thy Piety nor Wit\nShall lure it back to cancel half a Line,\n  Nor all thy Tears wash out a Word of it.\n</poem>\n- Omar Khayyám (translation by Edward Fitzgerald).\n\n\n"}
{"id": "38143077", "url": "https://en.wikipedia.org/wiki?curid=38143077", "title": "Arya (Buddhism)", "text": "Arya (Buddhism)\n\nArya (Sanskrit, also \"ārya\"; Pāli: \"ariya\") is a term frequently used in Buddhism that can be translated as \"noble\", \"not ordinary\", \"valuable\", \"precious\", \"pure\", etc. Arya in the sense of \"noble\" or \"exalted\" is frequently used in Buddhist texts to designate a spiritual warrior or hero.\n\nThe term is used in the following contexts:\n\nIn the context of the four noble truths (Sanskrit: \"arya satya\"; Pali: \"ariya sacca\"), contemporary scholars explain the meaning of \"arya\" as follows:\n\nBhikkhu Bodhi explains:\n\nIn Chinese Buddhist texts, \"\" is translated as 聖 (approximately, \"holy, sacred\", pinyin \"shèng\", on'yomi \"sei\").\n\nThe spiritual character of the use of the term ārya in Buddhist texts can also be seen in the Mahavibhasa and in the Yogacarabhumi. The Mahāvibhasa states that only the noble ones (āryas) realize all four of the four noble truths (āryasatyāni) and that only a noble wisdom understands them fully. The same text also describes the āryas as the ones who \"have understood and realized about the [truth of] suffering, (impermanence, emptiness, and no-self)\" and who \"understand things as they are\". In another text, the Yogācārabhūmi (Taishō 1579, vol. xx, 364b10-15), the āryas are described as being free from the \"viparyāsa\"s.\n\nSeveral Buddhist texts show that the \"\" was taught to everybody, including the āryas, Dasyus, Devas, Gandharvas and Asuras. The (from the Mūlasarvāstivādavinaya) describes a story of Buddha teaching his dharma to the Four Heavenly Kings () of the four directions. In this story, the guardians of the east () and the south () are āryajatiya (āryas) who speak Sanskrit, while the guardians of the west () and the north () are dasyujatiya (Dasyus) who speak Dasyu languages. In order to teach his Dharma, Buddha has to deliver his discourse in Aryan and Dasyu languages. This story describes Buddha teaching his Dharma to the āryas and Dasyus alike. The (a Mahāyāna sūtra) describes how Avalokiteśvara taught the ārya Dharma to the asuras, s and s.\n\nIn many parts of the South India, if somebody (new) is supposed to be addressed respectably, the prefix \"Ayya\", derived from \"Arya\" is used. South Indians used to call them \"Arya\" which is now transformed to \"Ayya\". This term is used even today.\n\n"}
{"id": "7257", "url": "https://en.wikipedia.org/wiki?curid=7257", "title": "Caste", "text": "Caste\n\nCaste is a form of social stratification characterized by endogamy, hereditary transmission of a lifestyle which often includes an occupation, status in a hierarchy, customary social interaction, and exclusion. It is an extreme evolution of a system of legally-entrenched social classes, also endogamous and hereditary, such as that of feudal Europe. Although caste systems exist in various regions, its paradigmatic ethnographic example is the division of Indian society into rigid social groups, with roots in India's ancient history and persisting until today; it is sometimes used as an analogical basis for the study of caste-like social divisions existing outside India. In biology, the term is applied to role stratification in eusocial animals like ants and termites, though the analogy is imperfect as these also involve extremely stratified reproduction.\n\nThe origins of the term 'caste' are attributed to the Spanish and Portuguese \"casta\", which, according to the John Minsheu's Spanish dictionary (1599), means \"race, lineage, or breed\". When the Spanish colonized the New World, they used the word to mean a \"clan or lineage\". It was, however, the Portuguese who first employed \"casta\" in the primary modern sense of the English word ‘caste’ when they applied it to the thousands of endogamous, hereditary Indian social groups they encountered upon their arrival in India in 1498, as a direct extension of the concept of ‘casta’ in contemporary Portugal. The use of the spelling \"caste\", with this latter meaning, is first attested in English in 1613.\n\nModern India's caste system is based on the social groupings called \"jāti\" and the theoretical \"varna\". The system of \"varnas\" appears in Hindu texts dating back to 1000 BCE and envisages the society divided into four classes: Brahmins (scholars and priests), Kshatriyas (warriors and nobles), Vaishyas (farmers, merchants, and artisans) and Shudras (laborers/service providers). The texts do not mention any separate, untouchable category in \"varna\" classification. Scholars believe that the system of \"varnas\" was a theoretical classification envisioned by the Brahmins, but never truly operational in society. The practical division of the society had always been in terms of \"jātis\" (birth groups), which are not based on any specific principle, but could vary from ethnic origins to occupations. The \"jātis\" have been endogamous groups without any fixed hierarchy but subject to vague notions of rank articulated over time based on lifestyle and social, political or economic status. In many instances, as in Bengal, historically the kings and rulers had been called upon, when required, to mediate on the ranks of \"jātis\", which might number in thousands all over the subcontinent and vary by region. In practice, the \"jātis\" may or may not fit into the \"varna\" classes and many prominent \"jatis\", for example the Jats and Yadavs, straddled two varnas i.e. Kshatriyas and Vaishyas, and the \"varna\" status of \"jātis\" itself was subject to articulation over time.\n\nStarting with the British colonial Census of 1901 led by Herbert Hope Risley, all the \"jātis\" were grouped under the theoretical \"varnas\" categories. According to political scientist Lloyd Rudolph, Risley believed that \"varna\", however ancient, could be applied to all the modern castes found in India, and \"[he] meant to identify and place several hundred million Indians within it.\" The terms \"varna\" (conceptual classification based on occupation) and \"jāti\" (groups) are two distinct concepts: while \"varna\" is the idealised four-part division envisaged by the Twice-Borns, \"jāti\" (community) refers to the thousands of actual endogamous groups prevalent across the subcontinent. The classical authors scarcely speak of anything other than the \"varnas\", as it provided a convenient shorthand; but a problem arises when even Indologists sometimes confuse the two.\n\nUpon independence from Britain, the Indian Constitution listed 1,108 castes across the country as Scheduled Castes in 1950, for positive discrimination. The Untouchable communities are sometimes called \"Scheduled Castes\", \"Dalit\" or \"Harijan\" in contemporary literature. In 2001, Dalits were 16.2% of India's population. Most of the 15 million bonded child workers are from the lowest castes.\n\nIndependent India has witnessed caste-related violence. In 2005, government recorded approximately 110,000 cases of reported violent acts, including rape and murder, against Dalits. For 2012, the government recorded 651 murders, 3,855 injuries, 1,576 rapes, 490 kidnappings, and 214 cases of arson.\n\nThe socio-economic limitations of the caste system are reduced due to urbanization and affirmative action. Nevertheless, the caste system still exists in endogamy and patrimony, and thrives in the politics of democracy, where caste provides ready made constituencies to politicians. The globalization and economic opportunities from foreign businesses has influenced the growth of India's middle-class population. Some members of the Chhattisgarh Potter Caste Community (CPCC) are middle-class urban professionals and no longer potters unlike the remaining majority of traditional rural potter members. The co-existence of the middle-class and traditional members in the CPCC has created intersectionality between caste and class. There is persistence of caste in Indian politics. Caste associations have evolved into caste-based political parties. Political parties and the state perceive caste as an important factor for mobilization of people and policy development. It is not politics that gets caste-ridden; it is caste that gets politicized.\n\nStudies by Bhatt and Beteille have shown changes in status, openness, mobility in the social aspects of Indian society. As a result of the modern social pressures on the country, India is experiencing a change in their social sphere dynamic as well as economically in the caste system. While arranged marriages are still the most common practice in India, the internet has helped the younger generation of Indians to have more control over their marriages through the use of dating apps. Hypergamy is still a common practice in India and Hindu culture. Men can marry a lower caste or a woman of an equal one. If the girl marries up, then her children will take the status of their father. If the girl marries down, her entire family is reduced to the social status of their son in law. Geographical factors also determine the loosening of the caste system. Many Northern villages are more likely to experience exogamous marriages, due to a lack of people within the same caste. Women in North India have been found to be less likely to leave or divorce their husbands since they are of a relatively lower caste system, and have higher restrictions on their freedoms. On the other hand, Pahari women have much more freedom to leave their husbands without stigma.\n\nThe Nepalese caste system resembles that of the Indian \"jāti\" system with numerous \"jāti\" divisions with a \"varna\" system superimposed for a rough equivalence. But since the culture and the society is different some of the things are different. Inscriptions attest the beginnings of a caste system during the Licchavi period. Jayasthiti Malla (1382–1395) categorized Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506–1575). The Hindu social code was later set up in Gorkha by Ram Shah (1603–1636).\n\nMcKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.\n\nThe caste system in Sri Lanka is a division of society into strata, influenced by the textbook \"varnas\" and \"jāti\" system found in India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the British/Kandyan period Kadayimpoth – Boundary books as well, indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.\n\nBalinese caste structure has been described in early 20th-century European literature to be based on three categories – triwangsa (thrice born) or the nobility, \"dwijāti\" (twice born) in contrast to \"ekajāti\" (once born) the low folks. Four statuses were identified in these sociological studies, spelled a bit differently from the caste categories for India:\n\nThe Brahmana caste was further subdivided by these Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five: Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher-caste Brahmana men with lower-caste women. The other castes were similarly further sub-classified by these 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to \"castas\" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.\n\nDuring the period of Yuan Dynasty, ruler Kublai Khan enforced a \"Four Class System\", which was a legal caste system. The order of four classes of people was maintained by the information of the descending order were:-\n\nToday, the Hukou system is considered by various sources as the current caste system of China.\n\nThere is also significant controversy over the social classes of Tibet, especially with regards to the serfdom in Tibet controversy.\n\nIn Japan's history, social strata based on inherited position rather than personal merit, were rigid and highly formalized in a system called \"mibunsei\" (身分制). At the top were the Emperor and Court nobles (kuge), together with the Shōgun and daimyō. Below them, the population was divided into four classes: samurai, peasants, craftsmen and merchants. Only samurai were allowed to bear arms. A samurai had a right to kill any peasants, craftsman or merchant who he felt were disrespectful. Merchants were the lowest caste because they did not produce any products. The castes were further sub-divided; for example, peasants were labelled as \"furiuri\", \"tanagari\", \"mizunomi-byakusho\" among others. As in Europe, the castes and sub-classes were of the same race, religion and culture.\n\nHowell, in his review of Japanese society notes that if a Western power had colonized Japan in the 19th century, they would have discovered and imposed a rigid four-caste hierarchy in Japan.\n\nDe Vos and Wagatsuma observe that Japanese society had a systematic and extensive caste system. They discuss how alleged caste impurity and alleged racial inferiority, concepts often assumed to be different, are superficial terms, and are due to identical inner psychological processes, which expressed themselves in Japan and elsewhere.\n\nEndogamy was common because marriage across caste lines was socially unacceptable.\n\nJapan had its own untouchable caste, shunned and ostracized, historically referred to by the insulting term \"Eta\", now called \"Burakumin\". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the Buraku or Burakumin underclasses. The Burakumin are regarded as \"ostracised\". The burakumin are one of the main minority groups in Japan, along with the Ainu of Hokkaidō and those of residents of Korean and Chinese descent.\n\nThe baekjeong (백정) were an \"untouchable\" outcaste of Korea. The meaning today is that of butcher. It originates in the Khitan invasion of Korea in the 11th century. The defeated Khitans who surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time, their ethnic origin was forgotten, and they formed the bottom layer of Korean society.\n\nIn 1392, with the foundation of the Confucian Joseon dynasty, Korea systemised its own native class system. At the top were the two official classes, the Yangban, which literally means \"two classes\". It was composed of scholars (\"munban\") and warriors (\"muban\"). Scholars had a significant social advantage over the warriors. Below were the \"jung-in\" (중인-中人: literally \"middle people\". This was a small class of specialized professions such as medicine, accounting, translators, regional bureaucrats, etc. Below that were the \"sangmin\" (상민-常民: literally 'commoner'), farmers working their own fields. Korea also had a serf population known as the \"nobi\". The nobi population could fluctuate up to about one third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5% of the total population of Korea. The hereditary nobi system was officially abolished around 1886–87 and the rest of the nobi system was abolished with the Gabo Reform of 1894, but traces remained until 1930.\n\nThe opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the \"baekjeong\". However, everyone was not equal under the Christian congregation, and even so protests erupted when missionaries tried to integrate \"baekjeong\" into worship, with non-\"baekjeong\" finding this attempt insensitive to traditional notions of hierarchical advantage. Around the same time, the \"baekjeong\" began to resist open social discrimination. They focused on social and economic injustices affecting them, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by upper class, authorities, and \"commoners\", and the use of degrading language against children in public schools.\n\nWith the Gabo reform of 1896, the class system of Korea was officially abolished. Following the collapse of the Gabo government, the new cabinet, which became the Gwangmu government after the establishment of the Korean Empire, introduced systematic measures for abolishing the traditional class system. One measure was the new household registration system, reflecting the goals of formal social equality, which was implemented by the loyalists' cabinet. Whereas the old registration system signified household members according to their hierarchical social status, the new system called for an occupation.\n\nWhile most Koreans by then had surnames and even bongwan, although still substantial number of cheonmin, mostly consisted of serfs and slaves, and untouchables did not. According to the new system, they were then required to fill in the blanks for surname in order to be registered as constituting separate households. Instead of creating their own family name, some cheonmins appropriated their masters' surname, while others simply took the most common surname and its bongwan in the local area. Along with this example, activists within and outside the Korean government had based their visions of a new relationship between the government and people through the concept of citizenship, employing the term \"inmin\" (\"people\") and later, \"kungmin\" (\"citizen\").\n\nThe Committee for Human Rights in North Korea reported that \"Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life.\" Regarded as \"Songbun\", Barbara Demick describes this \"class structure\" as an updating of the hereditary \"caste system\", combining Confucianism and Stalinism. She claims that a bad family background is called \"tainted blood\", and that by law this \"tainted blood\" lasts for three generations.\n\nHeidi Fjeld has put forth the argument that pre-1950s Tibetan society was functionally a caste system, in contrast to previous scholars who defined the Tibetan social class system as similar to European feudal serfdom, as well as non-scholarly western accounts which seek to romanticize a supposedly 'egalitarian' ancient Tibetan society.\n\nYezidi society is hierarchical. The secular leader is a hereditary emir or prince, whereas a chief sheikh heads the religious hierarchy. The Yazidi are strictly endogamous; members of the three Yazidi castes, the murids, sheikhs and pirs, marry only within their group.\n\nPre-Islamic Sassanid society was immensely complex, with separate systems of social organization governing numerous different groups within the empire. Historians believe society comprised four\nsocial classes:\n\n\nIn Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.\n\nVarious sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa, however the following features are common – it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the \"Nupe\" of Nigeria, the \"Beni Amer\" of East Africa, and the \"Tira\" of Sudan, the exclusionary principle has been driven by evolving social factors.\n\nAmong the Igbo of Nigeria – especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country – Obinna finds Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracized, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.\n\nThe \"osu\" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the \"Osus\" people as \"owned by deities\" and outcasts.\n\nThe Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.\n\nIn a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Ivory Coast, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.\n\nSimilarly, the Mandé societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the \"jonow\" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the \"geer\" (freeborn/nobles), \"jaam\" (slaves and slave descendants) and the underclass \"neeno\". In various parts of West Africa, Fulani societies also have class divisions. Other castes include \"Griots\", \"Forgerons\", and \"Cordonniers\".\n\nTamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the \"Malinke\" people no later than 14th century, and was present among the \"Wolof\" and \"Soninke\", as well as some \"Songhay\" and \"Fulani\" populations, no later than 16th century. Tamari claims that wars, such as the \"Sosso-Malinke\" war described in the \"Sunjata\" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.\n\nAs West Africa evolved over time, sub-castes emerged that acquired secondary specializations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.\n\nEthel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility.\n\nIn a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited. Alula Pankhurst has published a study of caste groups in SW Ethiopia.\n\nAmong the Kafa, there were also traditionally groups labeled as castes. \"Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manno). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves.\"\n\nThe Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.\n\nThe traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as \"sab\") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.\n\nFor centuries, through the modern times, the majority regarded Cagots who lived primarily in the Basque region of France and Spain as an inferior caste, the untouchables. While they had the same skin color and religion as the majority, in the churches they had to use segregated doors, drink from segregated fonts, and receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.\n\nIn July 2013, the UK government announced its intention to amend the Equality Act 2010 to \"introduce legislation on caste, including any necessary exceptions to the caste provisions, within the framework of domestic discrimination law\". Section 9(5) of the Equality Act 2010 provides that \"a Minister may by order amend the statutory definition of race to include caste and may provide for exceptions in the Act to apply or not to apply to caste\".\n\nFrom September 2013 to February 2014, Meena Dhanda led a project on \"Caste in Britain\" for the UK Equality and Human Rights Commission (EHRC).\n\nIn view of W. Lloyd Warner relationship between Blacks and Whites in USA historically showed many features of caste like residential segregation, marriage restrictions. Discrimination based upon socio-economic factors are historically prevalent within the country. \n\nAccording to Gerald D. Berreman, in the two systems, there are rigid rules of avoidance and certain types of contacts are defined as contaminating. In India, there are complex religious features which make up the system, whereas in the United States race and color are the basis of differentiation. The caste system in India and in the United States has higher groups which desire to retain their position for themselves and thus perpetuate the system.\n\nAs a result of the increased immigration to the United States, many Indian Americans have created a similar system. A survey commissioned by Equality Labs finds that caste discrimination is playing out in the United States as well. 2/3 of the members of the lowest caste, called Dalits, said that they have faced workplace discrimination due to their caste. 41% have experienced discrimination in education because of it in the United States.\n\n\n\n\n"}
{"id": "55725063", "url": "https://en.wikipedia.org/wiki?curid=55725063", "title": "Chauffeur (criminal)", "text": "Chauffeur (criminal)\n\nA chauffeur, (French, \"heater upper\") was a type of French criminal active from the 18th to the 20th centuries. In gangs, they carried out Home invasions on remote, rural dwellings, then tortured the householders into revealing the hiding places of savings or valuables. This was often done by burning victims' feet in the fireplace, which gave rise to the name for them.\n\nA notable gang was the Chauffeurs de la Drome who were responsible for eighteen murders between 1905 and 1908.\n\nThese attacks continued after the Second World War. \"Le gang des Romanis\" was active in Bourgogne and \"Le Gang d’Albert\" was active in Picardy until their leaders were separately captured and executed in 1952.\n"}
{"id": "27477807", "url": "https://en.wikipedia.org/wiki?curid=27477807", "title": "Constant conjunction", "text": "Constant conjunction\n\nConstant conjunction is a phrase used in philosophy as a variant or near synonym for causality and induction. It can be construed to contradict a more common phrase: correlation is not causation. \n\nThe philosopher David Hume used the phrase frequently in his discussion of the limits of empiricism to explain our ideas of causation and inference. In \"An Enquiry concerning Human Understanding\" and \"A Treatise of Human Nature\" Hume proposed that the origin of our knowledge of necessary connections arises out of observation of the \"constant conjunction\" of certain impressions across many instances. A more modern conception would argue that scientific law is distinguishable from a principle that arises merely accidentally because of the constant conjunction of one thing and another, but there is considerable controversy over what this distinguishing feature might be.\n\nAlthough British empiricism and associationist philosophers elaborated on Hume's fundamental idea in many diverse ways, and metaphysicians like Immanuel Kant tried to dissipate the position, the force of his arguments has remained remarkably robust, and they have found unexpected support in three scientific discoveries of the 20th century: Ivan Pavlov's laws of conditioning; Hebbian neural networks; and spike-timing-dependent plasticity (STDP).\n\nIn Pavlov's framework, an unconditioned stimulus can follow in constant conjunction a conditioning/conditioned stimulus within a timeframe of milliseconds to several seconds, and result in the conditioned stimulus having many of the properties of the unconditioned stimulus. Donald Hebb explained this as an intrinsic property of cell assemblies within the nervous system to form connections within large cliques of cells whenever those cells fire together within a reasonably short period of time. (A modern shorthand for his ideas states: \"Cells that fire together, wire together\".) Modern neuroscience has confirmed this insight as a product of the activity of synapses and STDP, so structured to strengthen connections between cells that fire within very short periods (10s of milliseconds) of each other. The longer time periods of classical conditioning are presumably a large-number effect of cliques of these synapses and cells.\n"}
{"id": "652102", "url": "https://en.wikipedia.org/wiki?curid=652102", "title": "Corner case", "text": "Corner case\n\nIn engineering, a corner case (or pathological case) involves a problem or situation that occurs only outside of normal operating parameters—specifically one that manifests itself when multiple environmental variables or conditions are simultaneously at extreme levels, even though each parameter is within the specified range for that parameter.\n\nFor example, a loudspeaker might distort audio, but only when played at maximum volume, maximum bass, and in a high-humidity environment. Or a computer server may be unreliable, but only with the maximum complement of 64 processors, 512 GB of memory, and 10,000 signed-on users.\n\nContrast a corner case with an edge case, the latter an issue that occurs only at a (single) maximum or minimum parameter. For example, a speaker may distort audio at maximum volume, even in the absence of other extreme settings or conditions.\n\nCorner cases form part of an engineer's lexicon—especially an engineer involved in testing or debugging a complex system. Corner cases are often harder and more expensive to reproduce, test, and optimize because they require maximal configurations in multiple dimensions. They are frequently less-tested, given the belief that few product users will, in practice, exercise the product at multiple simultaneous maximum settings. Expert users of systems therefore routinely find corner case anomalies, and in many of these, errors.\n\nThe term \"corner case\" comes about by physical analogy with \"edge case\" as an extension of the \"flight envelope\" metaphor to a set of testing conditions whose boundaries are determined by the 2 combinations of extreme (minimum and maximum) values for the number \"n\" of variables being tested, \"i.e.\", the total parameter space for those variables. Where an edge case involves pushing one variable to a minimum or maximum, putting users at the \"edge\" of the configuration space, a corner case involves doing so with multiple variables, which would put users at a \"corner\" of a multidimensional configuration space.\n\n"}
{"id": "33790621", "url": "https://en.wikipedia.org/wiki?curid=33790621", "title": "Counterconditioning", "text": "Counterconditioning\n\nCounterconditioning (also called stimulus substitution) is functional analytic principle that is part of behavior analysis, and involves the conditioning of an unwanted behavior or response to a stimulus into a wanted behavior or response by the association of positive actions with the stimulus. For example, when training a dog, a person would create a positive response by petting or calming the dog when the dog reacts anxiously or nervously to a stimulus. Therefore, this will associate the positive response with the stimulus.\n\nMary Cover Jones was the first to show the effectiveness of the counter conditioning process in her rabbit experiments. She was able to eliminate the fear of rabbits from a young boy. The rabbit was first kept away from the boy and then moved closer and closer, while the boy was able to eat his favorite foods. The boy was allowed to touch the rabbit and then was able to eat his food to reduce the nervousness touching the rabbit induced. Eventually the boy was able to pet the rabbit without any sign of fear because of the unpleasant and feared stimulus of the rabbit was now replaced by the pleasant stimulus of the food. But Jones was not the only one working on this process of conditioning, J.B. Watson and R. Rayner suggested a process similar to that of Jones and also shortly after the rabbit experiments were published Ivan Pavlov used a similar procedure for a dog that was agitated by his experiments.\n\nCounterconditioning is very similar to extinction seen in classical conditioning. It is the process of getting rid of an unwanted response. But in counterconditioning, the unwanted response does not just disappear, it is replaced by a new, wanted response. \"The conditioned stimulus is presented with the unconditioned stimulus\". \nThis also can be thought of as stimulus substitution. The weaker stimulus will be replaced by the stronger stimulus. \nWhen counterconditioning is successful, the process can not just be explained by simply substitution of a stimulus. It usually is explained by things such as conditioned inhibition, habituation, or extinction.\n\nIt is a common treatment for aggression, fears, and phobias.\nThe use of counter conditioning is widely used for treatment in humans as well as animals. The most common goal is to decrease or increase the want or desire to the stimulus. One of the most widely used types of counter conditioning is systematic desensitization. This technique uses muscle relaxation instead of food as the positive counter stimulus. The main goal in this treatment is to reduce fear to a certain feared stimulus.\n\n"}
{"id": "267487", "url": "https://en.wikipedia.org/wiki?curid=267487", "title": "Discipline", "text": "Discipline\n\nDiscipline is action or inaction that is regulated to be in accordance (or to achieve accord) with a particular system of governance. Discipline is commonly applied to regulating human and animal behavior, and furthermore, it is applied to each activity-branch in all branches of organized activity, knowledge, and other fields of study and observation. Discipline can be a set of expectations that are required by any governing entity including the self, groups, classes, fields, industries, or societies.\n\nChildren being educated to use public litter bins is a form of disciplinary education that is expected by some societies. If a child cannot use a litter bin the lack of discipline can result in a reaction from observant people in public. Many people observe a form of disciplinary effort in their daily lives. Discipline has a very important role in student's life.\nDiscipline is a moral obligation among many groups of people. Disciplined behavior is required by some laws and other legal obligations. Commercial entities can also put in place strict disciplinary requirements for the length of the commercial agreement. Airlines enforce strict disciplinary and conduct rules upon flight passengers. \n\nIn the military, discipline regards the efforts made by superiors to punish the serviceperson. In academia, discipline can also regard the educators' responses and efforts that are designed to punish the student(s). Not only in military, in every parts of life of an individual discipline plays the most important role. Discipline shows the actual face of an individual. In animal husbandry and training, the animals may be disciplined to perform specific task and activities without errors. Additionally, animals can discipline their young through numerous methods; including nips, bites, and grips.\n\nDisciplinarians have been involved in many societies throughout history. The Victorian era resulted in the popular use of disciplinarian governance over children. Edward VIII had a disciplinarian father, and the English had modelled the royal families during this era. Edward's grandmother was Queen Victoria who had championed the role of the family unit during her reign. A disciplinarian will enforce a stricter set of rules that are aimed at developing children according to theories of order and discipline. Disciplinarians have also been linked to child abuse in numerous cases and biographies.\n\n\n\n"}
{"id": "37664004", "url": "https://en.wikipedia.org/wiki?curid=37664004", "title": "Dyad (philosophy)", "text": "Dyad (philosophy)\n\nThe Dyad is a title used by the Pythagoreans for the number two, representing the principle of \"twoness\" or \"otherness\".\n\nNumenius of Apamea, a Neopythagorean philosopher in the latter 2nd century CE, said that Pythagoras gave the name of Monad to God, and the name of Dyad to matter. Aristotle equated matter as the formation of the elements (energies) into the material world as the static material was formed by the energies being acted upon by force or motion. Later Neoplatonic Philosophers and idealists like Plotinus treated the dyad as a second cause (demiurge), which was the divine mind (nous) that via a reflective nature () causes matter to \"appear\" or become perceivable.\n\n"}
{"id": "9567", "url": "https://en.wikipedia.org/wiki?curid=9567", "title": "Egoism", "text": "Egoism\n\nEgoism is an ethical theory that treats self-interest as the foundation of morality.\n\nLatin word \"ego\" meaning \"I\".\n\n\nThe terms \"egoism\" and \"egotism\" may also refer to:\n\n\n"}
{"id": "44945962", "url": "https://en.wikipedia.org/wiki?curid=44945962", "title": "El Paquete Semanal", "text": "El Paquete Semanal\n\nEl Paquete Semanal (\"The Weekly Package\") or El Paquete is a one terabyte collection of digital material distributed since around 2008 on the underground market in Cuba as a substitute for broadband Internet. In 2015 it was the primary source of entertainment for millions of Cubans. Lower tiered packages are also available for a lower price at sizes ranging from 5-100gb. \n\nThe most popular content is TV series, soap operas, music and the illegal classifieds but it also contains films, video clips, Spanish language news websites, computer technology websites, instructional videos and advertisements for local Cuban businesses. Most buyers request only certain parts of the Package which may sell for as little as $1 US.\n\nSince a 2011 legal property reform regarding private enterprise, a Cuban advertising firm called Etres has used the new regulations surrounding advertising to legally charge local businesses a small fee to arrange for a short clip or poster promoting their establishment to feature in the Package.\n\nIn May 2016 it was still unknown who compiled the material or from where it was obtained. Some have theorized that the lack of pornographic material and lack of anti-government views in the package may indicate the Cuban government is involved in its production.\n\n"}
{"id": "1704142", "url": "https://en.wikipedia.org/wiki?curid=1704142", "title": "Escalation of commitment", "text": "Escalation of commitment\n\nEscalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from some decision, action, or investment nevertheless continues the same behavior rather than alter course. The actor maintains behaviors that are irrational, but align with previous decisions and actions.\n\nEconomists and behavioral scientists use a related term, \"sunk-cost fallacy\", to describe the justification of increased investment of money, time, lives, etc. in a decision, based on the cumulative prior investment (\"sunk cost\") despite new evidence suggesting that the cost, beginning immediately, of continuing the decision outweighs the expected benefit.\n\nIn sociology, \"irrational escalation of commitment\" or \"commitment bias\" describe similar behaviours. The phenomenon and the sentiment underlying it are reflected in such proverbial images as \"throwing good money after bad\", \"in for a penny, in for a pound\", and \"if at first you don't succeed, try, try again\".\n\nEscalation of commitment was first described by Barry M. Staw in his 1976 paper, \"Knee deep in the big muddy: A study of escalating commitment to a chosen course of action\".\n\nResearchers, inspired by the work of Staw, conducted studies that tested factors, situations and causes of escalation of commitment. The research introduced other analyses of situations and how people approach problems and make decisions. Some of the earliest work stemmed from events in which this phenomenon had an effect and help explain the phenomenon.\n\nOver the past few decades, researchers have followed and analyzed many examples of the escalation of commitment to a situation. The heightened situations are explained in three elements. Firstly, a situation has a costly amount of resources such as time, money and people invested in the project. Next, past behavior leads up to an apex in time where the project has not met expectations or could be in a cautious state of decline. Lastly, these problems all force a decision-maker to make choices that include the options of continuing to pursue a project until completion by adding additional costs, or canceling the project altogether.\n\nResearchers have also developed an argument regarding how escalation of commitment is observed in two different categories. Many researchers believe that the need to escalate resources is linked to expectancy theory. \"According to such a viewpoint, decision makers assess the probability that additional resource allocations will lead to goal attainment, as well as the value of goal attainment (i.e., rewards minus costs), and thereby generate a subjective expected utility associated with the decision to allocate additional resources.\" The next phase of the escalation process is self-justification and rationalizing if the decision the leader made used resources well, if the resources being used were used to make positive change, and assuring themselves that the decision they chose was right. Leaders must balance costs and benefits of any problem to produce a final decision. What matters most often in assuring the leader that they made the right choice regardless of the final outcome is if their decision is what others believed in.\n\nResearch conducted on the topic has been taken from many other forms and theories of psychology. Many believe that what researchers have done thus far to explain this behavior is best analyzed and tested through situational development and resource allocation.\n\nOther research has identified circumstances that lead to the opposite of escalation of commitment, namely deescalation of commitment. This research explains the factors that influence whether escalation or deescalation of commitment is more likely to occur through the role of budgeting and mental accounting.\n\nEscalation of commitment can many times cause behavior change by means of locking into resources. One of the first examples of escalation of commitment was described by George Ball, who wrote to President Lyndon Johnson to explain to him the predictions of the war outcome:\n\nSelf-justification thought process is a part of commitment decisions of leaders and managers of a group and can therefore cause a rise in commitment levels. This attitude provides \"one explanation for why people escalate commitment to their past investments.\" Managers make decisions that reflect previous behavior. Managers tend to recall and follow information that is aligned to their behavior to create consistency for their current and future decisions. If a group member or outside party recognizes inconsistent decision making, this can alter the leadership role of the manager. Managers have external influence from society, peers, and authority, which can significantly alter a manager's perception on what factors realistically matter when making decisions.\n\nProspect theory helps to describe the natural reactions and processes involved in making a decision in a risk-taking situation. Prospect theory makes the argument for how levels of wealth, whether that is people, money, or time, affect how a decision is made. Researchers were particularly interested in how one perceives a situation based on costs and benefits. The framing to how the problem is introduced is crucial for understanding and thinking of the probability that the situation will either work in favor or against you and how to prepare for those changes. \"Whyte (1986) argued that prospect theory provides the psychological mechanism by which to explain escalating commitment to a failing course of action without the need to invoke self-justification processes. (Fiegenbaum & Thomas, 1988: 99)\" Prospect theorists believe that one's use of this process is when there is a negative downfall in the stakes that will affect the outcome of the project. To ensure they will not fail, the individual may add more resources to assure them that they will succeed. Although this theory seems realistic, researchers \"Davis and Bobko (1986) found no effect of personal responsibility on continued commitment to the previous course of action in the positive frame condition.\" Which means that escalation of commitment will be lower in the higher responsibility situation.\n\nThe attribution theory, originating from Fritz Heider, \"attempts to find causal explanations for events and human behaviors.\" This theory approaches two methods of inquiry including locus of causality and stability. Locus of causality reflects on internal characteristics of an individual, such as intelligence levels and attention seeking, with the relationship of the external space such as weather forecasts and task difficulty. Aspects of control become a significant factor in how a manager justifies a decision made. Managers will use the relationship between internal and external factors to describe why they made a decision to a single point of view. Managers may justify their actions by explaining that this was out of their personal control of the event, or they could believe that the decision could not be controlled by anyone else. Research suggests that \"the type of attribution made by an employee across these dimensions is likely to impact an employee's tendency to engage in the negative emotional activity referred to as escalation of commitment.\"\n\nIdentity is a large part of how we move through the world. Private thoughts and opinions as well as the effect of others create the social identity theory. People make connections between their use of groups and their own view of themselves, which researchers have discovered motivates people to keep their social status and to defend it whenever it is endangered.\n\nGroups engage in temporal comparisons, which means that you compare actions and behaviors at \"different points in time.\" This is a form of social identity scenario. This type of comparison can be made when a decision seems unproductive and forces team members to consider any threat to the group.\n\n\"The aggregate model's emphasis is upon the accumulation and balance of forces rather than the ordering of effects over time.\" The model is general and can provide an ideal view as to how. The effects whether positive or negative are defined by micro and macro forces. This model goes by situation rather than what researchers have defined as the norm. There is no process to follow, which makes it very useful for researchers because they can understand a situation more clearly as well as see the bigger picture of the situation.\n\nThe main drivers of the tendency to continue investing in losing propositions are project, psychological, social, and structural.\n\nProject determinants are those that refer to the original commitments and decisions made at a project's beginning. This includes general project characteristics and initial financial costs. Among them, decision risk, opportunity cost information, and information acquisition have been found to have negative relationships with escalation of commitment. Decision uncertainty, positive performance trend information, and expressed preference for initial decision have been found to have positive relationships.\n\nHigh costs of ending a project or changing its course, potential financial gain upon completion, and extensive structure can factor in to escalation of commitment, making it difficult to walk away from the project. Preventing future monetary loss, potential savings, and having clearly available alternatives can allow for avoidance of the behavior. In studies by Teger and later Ross and Staw, situations where ending an action costs more than completing it resulted in decision makers being trapped in their current, costly behaviors.\n\nPsychological determinants are those that refer to internal views on the actions and information involved in a project. This can include cognitive factors, personality, and emotions as they relate to project elements. Of these, sunk costs, time investment, decision maker experience and expertise, self-efficacy and confidence, personal responsibility for the initial decision, ego threat, and proximity to project completion have been found to have positive relationships with escalation of commitment, while anticipated regret and positive information framing have been found to have negative relationships.\n\nOptimism and belief that one has control over what will still be positive outcomes lead to continued self-justification of commitment to actions. People add to their initial personal investments in the hope they will overcome currently negative results. This was illustrated in a case study by Staw, where providing business students with manipulated responsibilities for initial decisions and their outcomes resulted in the greatest commitment to increased actions and resources when the initial decision assigned was made directly by the student with poor outcomes. In these instances, people take further risk in an attempt to avoid further problems. This is even more likely when subjects view current issues as having unstable reasoning rather than stable reasoning, or when the individual is unwilling to admit mistakes. They then believe the situation will stabilize or turn around. Confirmation bias can also lead to escalation of commitment as individuals are then less likely to recognize the negative results of their decisions. On the other hand, if the results are recognized, they can be blamed on unforeseeable events occurring during the course of the project.\n\nThe effect of sunk costs is often seen escalating commitment. When the amount of investment is greater and can not be recovered, the desire to avoid complete loss of those resources and keeping with impression management prompts continued investment over pulling out. Relatedly, as invested resources can include time, closeness to completion of a project yields similar results. More value is placed on project when they are early or late in their process due to the clarity of completion goals at those points. It's more likely that risks will be taken at these points than in a project closer to a visible midpoint.\n\nSocial determinants are those that refer to the expectations and influence of other individuals or groups on project actions. Included in these, group identity or cohesive strength has been found to have the most influence on escalation of commitment while public evaluation of decision and resistance to decision from others has little significance in relation.\n\nIndividuals present themselves cautiously to others in the environment. They don't concentrate on themselves and what they believe is accurate and competent—they are only interested in their organization appearing competent. Escalation of commitment is about decision making that usually comes in a form of a group or individual deciding a course of action. Managers have a responsibility to choose the fate of what a group of people have been working on. A manager who decides to back a team out of a project isn't concerned that the project failed, they are concerned that team members may think the manager is incompetent. Studies that tested this phenomenon included factors such as policy resistance, job insecurity (Fox & Staw 1979), and audience size (Rubin & Lang 1981). All showed a spike in commitment when these realistic factors are present. This mental and emotional response is referred to as the face-saving effect. Individuals who are responsible for others are constantly checking themselves to assure that their actions and beliefs are parallel to the expectations for their viewers. One's social identity to the public can decide your fate. For example, a team can identify a level of commitment and personal connection to an idea or project. Team members consistently use statements like \"that project is Bob's baby,\" or \"oh, we had the same idea.\" Both ends of the spectrum are crucial to how others view and analyze a situation, especially something that failed.\n\nLeaders are responsible for guiding a team through the difficult problems into a solution. Although many times, the negative aspects of commitments can discussed in more detail and importance, there are many social rewards that are positive. One example of this phenomenon is persistence. A project on the verge of failure is a manager's responsibility to revive, but with persistence, a manager can get rewarded to turning a bad project into something great. Rewards are earned for turning around a team to produce something successful. When managers stick to their goals, and get their team to produce responsibly.\n\nStructural determinants are those that refer to the cultural and environmental features and resources of the project's overall organization. The minimal research available on them indicates that agency problems most influence escalation of commitment.\n\nThere are macro-level variables that affect the organizational structure of a team and how the make decisions. Decisions are made based on individuals in a group setting with specific roles and responsibilities and effect decisions that are a product of interaction in the team. The determinant that affects escalation of commitment is institutional inertia. This phenomenon is used to describe how individual attitudes and behavior changes frequently, so therefore, a group's stature is also unpredictable and controllable. \"Organizations have very imperfect sensory systems, making them relatively impervious to changes in their environments.\"\n\nThis is one factor that plays a role in how issues are addressed. When there are a group of individuals involved in communication, decisions, and change, with the lack of consistency in the group, most of the tasks fail to complete. This phenomenon occurs in situations such as policy change, rulings and procedures.\n\nThis issue can also cause havoc for an organization when they are too attached to a policy, thought process or identity because they are not free to change. \"On occasion, a project, product, or policy can become so closely tied to the values and purposes of the organization that it becomes almost unthinkable to consider withdrawal.\" One primary example of this phenomenon is the downfall of the Pan American World Airways company, commonly known as Pan Am. Pan Am was a well known airlines and hotel with hundreds of employees. With the turn of industry, airline policies were deregulated and caused financial downfall for Pan Am. The company over time made cuts to their company to stay afloat. The company believed that their image of being an airline was more important than being a successful company that they removed all of the assets that were in fact making them the largest amount of revenue only to save the image they thought they needed to remain to be Pan Am.\n\nIn groups, it can be more difficult to attribute issues to a single, simpler determinant. While determinants are still applicable, oftentimes there are more complex factors when organizational goals and actions misalign. Groups, especially as they grow larger, can be resistant to changing course.\n\nEven if the need to change course or cease action is recognized, communication speed, policies, and business politics can be hindrances. A larger organization, especially one with a spread of subgroups, has to communicate the argument and decision to go against previous actions across the appropriate levels. If this communication does not occur in a timely manner or is blocked, the final decision may not be made and acted upon. A decision that goes against existing rules and processes may also reach resistance even with support from those within the organization. Individuals and groups that are directly employed due to a project, have financial stake in it may provide enough opposition to prevent changes from being made as well. They feel personally responsible for the parts they've worked on and can also feel that they too are being replaced or terminated. Escalation of commitment can then occur in any of these situations. External groups can play an even larger part in escalating commitment if their power is greater than that of the group taking action and they use that power to directly lead and influence.\n\nWith a larger number of decision makers included, groups have the opportunity for greater productivity than single individuals, but they also have the opportunity for greater losses and escalation. Members can eliminate some of the escalation potential if they come to a better decision earlier on in the process and avoid the need to change course dramatically. Yet they can also hold onto a larger base of support for their initial actions to the point where escalation potential is increased. In this case, groupthink assists in keeping with the original decision and pressures towards conformity rather than dividing the group with other options. Also, a group whose members are not cohesive can have decreased escalation potential due to conflicts and varying levels of inclusion in the process.\n\nOrganizations that are family businesses are especially prone to escalation of commitment due to the added level of going through the family structure in addition to the business structure, allowing for further conflicts between the two. Business reputation, customer and share loss, and financial loss become risks.\n\n\n"}
{"id": "26209432", "url": "https://en.wikipedia.org/wiki?curid=26209432", "title": "Filipino values", "text": "Filipino values\n\nThe Filipino value system or Filipino values refers to the set of values or the value system that a majority of the Filipino have historically held important in their lives. This Philippine values system includes their own unique assemblage of consistent ideologies, moral codes, ethical practices, etiquette and cultural and personal values that are promoted by their society. As with any society though, the values that an individual holds sacred can differ on the basis of religion, upbringing and other factors.\n\nAs a general description, the distinct value system of Filipinos is rooted primarily in personal alliance systems, especially those based in kinship, obligation, friendship, religion (particularly Christianity) and commercial relationships.\n\nFilipino values are, for the most part, centered at maintaining social harmony, motivated primarily by the desire to be accepted within a group. The main sanction against diverging from these values are the concepts of \"\"Hiya\", roughly translated as 'a sense of shame', and \"Amor propio\"\" or 'self-esteem'. Social approval, acceptance by a group, and belonging to a group are major concerns. Caring about what others will think, say or do, are strong influences on social behavior among Filipinos.\n\nAccording to the anthropologist Leonardo Mercado, the Filipino worldview is basically 'nondualistic'. Based on his linguistic analyses of Filipino value terms like \"loob\" (Cebuano \"buot\"), he concludes that Filipinos desire harmony, not only in interpersonal relationships, but also with nature and religion, while still remaining nondichotomous.\n\nFlorentino Timbreza, a cultural philosopher, concludes in his book \"Pilosopiyang Pilipino\" (1982) that Filipino values are based on the significance of the world to man. Life experiences dictate the philosophy of the Filipino, augmented by other sources like proverbs, folk sayings, folk tales, and the like.\n\nF. Landa Jocano identified two models of the Filipino value system. The first is the exogenous model or the foreign model, while the second is the indigenous model or the traditional model. The foreign model is described to be a \"legal and formal\" model. The indigenous model is described as a \"traditional and non-formal\" model or guide but is deeply embedded in the subconscious of the Filipinos.\n\nThe foreign model was inherited by Filipinos from Western cultures, particularly from the Spaniards and the Americans. An example of a foreign or exogenous influence is bureaucracy exhibited in the government of the Philippines.\n\nBased on studies, surveys, opinions, anecdotes, and other literatures made by experts and researchers in relation to Filipino social values or Filipino core values, along with the Filipino character or Filipino identity of a person or an individual known as the Filipino, the Filipino value system are found to possess inherent key elements.\n\nOne can note how \"Hiya\" (propriety/dignity)\", Pakikisama\"(companionship/esteem)\",\" and \"Utang na loob\"(gratitude/solidarity), are merely Surface Values—readily seen and observed values exhibited and esteemed by many Filipinos. These three values are considered branches from a single origin—the actual Core Value of the Filipino Personality—\"Kapwa\". It means 'togetherness', and refers to community, or not doing things alone. \"Kapwa\" has two categories, \"Ibang Tao\" (other people) and \"Hindi Ibang Tao\" (not other people). The Surface Values spin off of the Core Value through the Pivotal Aspect of \"Pakikiramdam,\" or shared inner perception (\"Feeling for another\").\n\nOther notable key elements or motivations are optimism about the future, pessimism with regards to present situations and events, the concern and care for other people, the existence of friendship and friendliness, the habit of being hospitable, religious nature, respectfulness to self and others, respect for the female members of society, the fear of God, and abhorrence of acts of cheating and thievery.\n\nThe values of Filipinos specifically upholds the following items: solidarity of the family unit, security of the Philippine economy, orientation to small-groups, personalism, the concepts of \"\"loob\" or \"kalooban\"\" (meaning \"what’s inside the self\", the \"inner-self\", or the \"actual personal feelings of the self\"), existence and maintenance of smooth interpersonal relationships, and the sensing of the feelings or needs of others (known as \"pakikiramdam\"). In a larger picture, these values are grouped into general clusters or \"macroclusters\": namely, the relationship cluster, the social cluster, the livelihood cluster, the inwardness cluster, and the optimism cluster.\n\nThis is the shared sense of identity and consciousness of the 'other'. It means treating others with respect and dignity as an equal, not someone below the individual.\n\nThe basic and most important unit of a Filipino's life is the family. Unlike in Western countries, young Filipinos who turn 18 are not expected to move out of their parents' home. When a Filipino's parents are old and cannot take care of themselves, they are cared for in their children's homes and are very rarely brought by their children to Homes for the Aged. The practice of separating the elderly from the rest of the family, while common in Western countries, is often looked down upon in Filipino society. Family lunches with the whole clan with up to 50 people, extending until the line of second cousins, are not unusual. The Filipino puts a great emphasis on the value of family and being close to one's family members.\n\nThis famous trait is the ability of Filipinos to find humour in everything. It sheds light on the optimism and positivity of Filipinos in whatever situation they are in so as to remain determined in going through struggles or challenges. It serves as a coping technique, the same way a child who has fallen laughs at himself/herself to hide his/her embarrassment.\n\nStudies show that Filipinos often have an aversion to a set of standardised rules or procedures; They are known to follow a \"natural clock\" or organic sense of time—doing things in the time they feel is right. They are present-oriented: which means that one attends to a task or requirement at the time it is needed and does not worry much about future engagements. This allows the Filipino to adapt and be flexible in doing the tasks at times not bound to a particular schedule or timeframe. This allows them think on their feet and be creative in facing whatever challenge or task they have even when it is already right in front of them.\n\nThe Philippines is approximately 85 percent Christians (mostly Roman Catholics ), 10 percent Muslim, and 5 percent 'other' religions, including the Taoist-Buddhist religious beliefs of Chinese and the 'indigenous' animistic beliefs of some peoples in upland areas that resisted 300 years of Spanish colonial rule. This is a reflection of the Filipinos' strong faith in God as seen in their various practices. This includes the numerous church holidays they observe, the customary (and obligatory) Sunday Mass, the individual's basis of their moral standpoints, the influence of the Church on the minds, actions, and opinions of the majority, importance of the Sacraments, praying at almost any possible time of the day, the extreme practices during Holy Week,\n\nThe Filipinos as a people who have been constantly under the rule of numerous powerful countries has over time, developed a sense of resourcefulness or the ability to survive with whatever they have. They have the extraordinary ability to make something out of almost nothing. If a Filipino was given just a screwdriver, plastic bagseut, and some tape, he would still be able to build a bird tree, especially for the sake of survival, and provided that he be allowed to hunt for some needed surrounding material.\n\nWith resourcefulness comes hard work. Filipinos are very determined and persevering in accomplishing whatever they set their minds to. Filipinos over the years have proven time and time again that they are a people with an industrious attitude. Sadly, this is seen by others as Filipinos being only useful as domestic helpers, working abroad to help their families in the country. This is also present in the country’s workforce particularly the farmers. Even with little support, technological weaknesses and the country’s seasonal typhoons, the Filipino farmer still strives to earn their daily meal.\n\nForeigners who come to visit the Philippines speak of Filipinos going out of their way to help them when lost, or the heartwarming generosity of a Filipino family hosting a visitor in their poverty-stricken home. Meanwhile, most foreigners who attend Filipino gatherings abroad (which are frequently organized for hundreds of reasons) testify to the warmth and friendliness of Filipinos as they experience that feeling of “belongingness.” Indeed, the legendary Filipino hospitality is not limited to the Philippines. It is everywhere wherever there are Filipinos.\n\nIn relation to parenthood, bearing male and female children depends on the preferences of the parents based on the expected roles that each gender would assume once grown up. Both genders are expected to become responsible members of the family and their society. Women in the Philippines are expected to become caring and nurturing mothers for their own children.\n\nFemale Filipinos are also expected to lend a hand in household work. They are even anticipated to offer assistance after being married. On the other hand, Filipino men are expected to assume the role of becoming the primary source of income and financial support of his family.\n\n\nGeneral:\n\n"}
{"id": "13269942", "url": "https://en.wikipedia.org/wiki?curid=13269942", "title": "First-year composition", "text": "First-year composition\n\nFirst-year composition (sometimes known as freshman composition or freshman writing) is an introductory core curriculum writing course in American colleges. This course focuses on improving students' abilities to write in a university setting and introduces students to writing practices in the disciplines and professions. These courses are traditionally required of incoming students, thus the previous name, \"Freshman Composition\". Scholars working within the field of Composition-Rhetoric often have teaching First-year composition (FYC) courses as the practical focus of their scholarly work. \n\nFYC courses are structured in a variety of ways. Some institutions of higher education require only one term of FYC, while others require two or three courses. There are a number of identifiable pedagogies associated with FYC, including: current-traditional, expressivist, social-epistemic, process, post-process and Writing about Writing (WAW). Each of these pedagogies can generate a multitude of curricula. \n\nComposition professionals, including those with degrees in Writing Studies and Rhetoric and Composition, often focus on a rhetorical approach to help students learn how to apply an understanding of audience, purpose, context, invention, and style to their writing processes. This rhetorical approach has shown that real writing, rather than existing as isolated modes, has more to do with a writer choosing from among many approaches to perform rhetorical tasks. In addition to a focus on rhetoric, many first year composition courses also emphasize writing process, where students are encouraged to interact with classmates and receive feedback to be used for revision. These practices can take the form of essay peer review or workshopping. Portfolios are a common way of assessing revised student work.\n\nSince the late nineteenth century, college courses on composition have become increasingly common in American higher education. Although a longstanding course offering at many colleges, first-year composition remains controversial and marginalized.\n\nThe requirement for a first-year composition course has been debated in composition studies. This debate centers around how effective the first-year composition course is and the changes that need to be made to develop the field of composition. While most schools do require some form of the first-year composition course, there are some schools that have decided to abolish the first-year composition requirement. Some scholars, such as Sharon Crowley in \"Composition in the University: Historical and Polemical Essays,\" argue that this requirement should be abolished. Crowley does not suggest the course itself be removed, only the requirement that all freshmen take the course. She states that students would still be interested in the course if the requirement was abolished and that removing the requirement would strengthen the field of composition. She implies that composition studies is marginalized within the university because of the view of the first-year composition course as a skill course. Removing the requirement, she states, would remove the association of composition studies with introductory courses, giving more acknowledgement to the field. Crowley's opinion initiated a debate in the composition field, but she is not the only critic who advocates for the removal of this requirement. Scholars Douglas Downs and Elizabeth Wardle also dislike the requirement and instead argue for a writing studies curriculum. However, there are scholars who do not agree with this stance, argue that the first-year composition course is needed in the university, and believe that the requirement should remain. Scholar David Smit is one critic who favors keeping the requirement. He suggests that the requirement can be kept and the curriculum and structure of the first-year composition course altered for improvement. Smit explains that many of the developmental goals of those who favor abolishing the requirement can still be achieved by offering more writing experiences. He proposes more genre writing in composition courses with a \"scaffolding\" progression of discipline writing. If this was done, he suggests, the concerns over the status of composition studies in the university would still be solved, as the course would no longer be seen as skills based. There has been no consensus reached in composition studies regarding the status of the first-year composition course requirement. The benefits of the course, as well as the drawbacks, continue to be debated and the scholars noted above are only a few of the voices and perspectives involved in this discussion. Despite the debate about the requirement, it remains in effect at a majority of universities.\n\nFirst-year composition is designed to meet the goals for successful completion set forth by the Council of Writing Program Administrators. To reach these goals, students must learn rhetorical conventions, critical thinking skills, information literacy, and the process of writing an academic paper. While there is no American standard curriculum for first-year composition, curriculum is developed at several levels, including the state, institution, department, and writing program.\n\nWith the publication of James Kinneavy's \"Theory of Discourse\" in 1971, English departments began incorporating rhetoric into their composition classrooms. In doing this, composition instructors have placed more emphasis on teaching audience analysis, Aristotle's three appeals (ethos, pathos, and logos), and teaching Kinneavy's modes of discourse.\nAccording to Brian Sutton in \"Writing in the Disciplines, First-Year Composition, and the Research Paper,\" since 1980, there has been an increasing debate in academic circles as to whether the \"generic\" approach to writing in first year composition is useful for students whose future writing will be discipline specific (46).\n\nFirst described by Mina Shaughnessy in the 1970s, Basic writing is a division of composition studies that strives to bring disadvantaged students entering college to a more complete understanding of the rhetorical aspects of the writing process.\n\n\n"}
{"id": "27633793", "url": "https://en.wikipedia.org/wiki?curid=27633793", "title": "Impedance analogy", "text": "Impedance analogy\n\nThe impedance analogy is a method of representing a mechanical system by an analogous electrical system. The advantage of doing this is that there is a large body of theory and analysis techniques concerning complex electrical systems, especially in the field of filters. By converting to an electrical representation, these tools in the electrical domain can be directly applied to a mechanical system without modification. A further advantage occurs in electromechanical systems: Converting the mechanical part of such a system into the electrical domain allows the entire system to be analysed as a unified whole. \n\nThe mathematical behaviour of the simulated electrical system is identical to the mathematical behaviour of the represented mechanical system. Each element in the electrical domain has a corresponding element in the mechanical domain with an analogous constitutive equation. Every law of circuit analysis, such as Kirchhoff's laws, that apply in the electrical domain also applies to the mechanical impedance analogy. \n\nThe impedance analogy is one of the two main mechanical-electrical analogies used for representing mechanical systems in the electrical domain, the other being the mobility analogy. The roles of voltage and current are reversed in these two methods, and the electrical representations produced are the dual circuits of each other. The impedance analogy preserves the analogy between electrical impedance and mechanical impedance whereas the mobility analogy does not. On the other hand, the mobility analogy preserves the topology of the mechanical system when transferred to the electrical domain whereas the impedance analogy does not.\n\nThe impedance analogy is widely used to model the behaviour of mechanical filters. These are filters that are intended for use in an electronic circuit, but work entirely by mechanical vibrational waves. Transducers are provided at the input and output of the filter to convert between the electrical and mechanical domains.\n\nAnother very common use is in the field of audio equipment, such as loudspeakers. Loudspeakers consist of a transducer and mechanical moving parts. Acoustic waves themselves are waves of mechanical motion: of air molecules or some other fluid medium. A very early application of this type was to make significant improvements to the abysmal audio performance of phonographs. In 1929 Edward Norton designed the mechanical parts of a phonograph to behave as a maximally flat filter, thus anticipating the electronic Butterworth filter.\n\nBefore an electrical analogy can be developed for a mechanical system, it must first be described as an abstract mechanical network. The mechanical system is broken down into a number of ideal elements each of which can then be paired with an electrical analogue. The symbols used for these mechanical elements on network diagrams are shown in the following sections on each individual element.\n\nThe mechanical analogies of lumped electrical elements are also lumped elements, that is, it is assumed that the mechanical component possessing the element is small enough that the time taken by mechanical waves to propagate from one end of the component to the other can be neglected. Analogies can also be developed for distributed elements such as transmission lines but the greatest benefits are with lumped element circuits. Mechanical analogies are required for the three passive electrical elements, namely, resistance, inductance and capacitance. What these analogies are is determined by what mechanical property is chosen to represent <nowiki>\"effort\"</nowiki>, the analogy of voltage, and the property chosen to represent <nowiki>\"flow\"</nowiki>, the analogy of current. In the impedance analogy the effort variable is force and the flow variable is velocity.\n\nThe mechanical analogy of electrical resistance is the loss of energy of a moving system through such processes as friction. A mechanical component analogous to a resistor is a shock absorber and the property analogous to resistance is damping. A resistor is governed by the constitutive equation of Ohm's law,\n\nThe analogous equation in the mechanical domain is,\n\nElectrical resistance represents the real part of electrical impedance. Likewise, mechanical resistance is the real part of mechanical impedance.\n\nThe mechanical analogy of inductance in the impedance analogy is mass. A mechanical component analogous to an inductor is a large, rigid weight. An inductor is governed by the constitutive equation,\n\nThe analogous equation in the mechanical domain is Newton's second law of motion,\n\nThe impedance of an inductor is purely imaginary and is given by,\n\nThe analogous mechanical impedance is given by,\n\nThe mechanical analogy of capacitance in the impedance analogy is compliance. It is more common in mechanics to discuss stiffness, the inverse of compliance. The analogy of stiffness in the electrical domain is the less commonly used elastance, the inverse of capacitance. A mechanical component analogous to a capacitor is a spring. A capacitor is governed by the constitutive equation,\n\nThe analogous equation in the mechanical domain is a form of Hooke's law,\n\nThe impedance of a capacitor is purely imaginary and is given by,\n\nThe analogous mechanical impedance is given by,\n\nAlternatively, one can write,\n\nwhich is more directly analogous to the electrical expression when capacitance is used.\n\nA mechanical resonator consists of both a mass element and a compliance element. Mechanical resonators are analogous to electrical LC circuits consisting of inductance and capacitance. Real mechanical components unavoidably have both mass and compliance so it is a practical proposition to make resonators as a single component. In fact, it is more difficult to make a pure mass or pure compliance as a single component. A spring can be made with a certain compliance and mass minimised, or a mass can be made with compliance minimised, but neither can be eliminated altogether. Mechanical resonators are a key component of mechanical filters.\n\nAnalogues exist for the active electrical elements of the voltage source and the current source (generators). The mechanical analogue in the impedance analogy of the constant voltage generator is the constant force generator. The mechanical analogue of the constant current generator is the constant velocity generator.\n\nAn example of a constant force generator is the constant-force spring. This is analogous to a real voltage source, such as a battery, which remains near constant-voltage with load provided that the load resistance is much higher than the battery internal resistance. An example of a practical constant velocity generator is a lightly loaded powerful machine, such as a motor, driving a belt.\n\nElectromechanical systems require transducers to convert between the electrical and mechanical domains. They are analogous to two-port networks and like those can be described by a pair of simultaneous equations and four arbitrary parameters. There are numerous possible representations, but the form most applicable to the impedance analogy has the arbitrary parameters in units of impedance. In matrix form (with the electrical side taken as port 1) this representation is, \n\nThe element formula_13 is the open circuit mechanical impedance, that is, the impedance presented by the mechanical side of the transducer when no current (open circuit) is entering the electrical side. The element formula_14, conversely, is the clamped electrical impedance, that is, the impedance presented to the electrical side when the mechanical side is clamped and prevented from moving (velocity is zero). The remaining two elements, formula_15 and formula_16, describe the transducer forward and reverse transfer functions respectively. They are both analogous to transfer impedances and are hybrid ratios of an electrical and mechanical quantity.\n\nThe mechanical analogy of a transformer is a simple machine such as a pulley or a lever. The force applied to the load can be greater or less than the input force depending on whether the mechanical advantage of the machine is greater or less than unity respectively. Mechanical advantage is analogous to transformer turns ratio in the impedance analogy. A mechanical advantage greater than unity is analogous to a step-up transformer and less than unity is analogous to a step-down transformer.\n\nThe figure shows a mechanical arrangement of a platform of mass \"M\" that is suspended above the substrate by a spring of stiffness \"S\" and a damper of resistance \"R\". The impedance analogy equivalent circuit is shown to the right of this arrangement and consists of a series resonant circuit. This system has a resonant frequency, and may have a natural frequency of oscillation if not too heavily damped.\n\nThe circuit diagram shows an impedance analogy model of the human ear. The ear canal section is followed by a transformer representing the eardrum. The eardrum is the transducer between the acoustic waves in air in the ear canal and the mechanical vibrations in the bones of the middle ear. At the cochlea there is another change of medium from mechanical vibrations to the fluid filling the cochlea. This example thus demonstrates the power of electrical analogies in bringing together three domains (acoustic, mechanical and fluid flow) into a single unified whole. If the nerve impulses flowing to the brain had also been included in the model then the electrical domain would have made four domains encompassed in the model.\n\nThe cochlea portion of the circuit uses a finite element analysis of the continuous transmission line of the cochlear duct. An ideal representation of such a structure would use infinitesimal elements, and there would thus be an infinite number of them. In this model the cochlea is divided into 350 sections and each section is modelled using a small number of lumped elements.\n\nThe principal advantage of the impedance analogy over its alternative, the mobility analogy, is that it maintains the analogy between electrical and mechanical impedance. That is, a mechanical impedance is represented as an electrical impedance and a mechanical resistance is represented as an electrical resistance in the electrical equivalent circuit. It is also natural to think of force as analogous to voltage (generator voltages are often called electromotive force) and velocity as analogous to current. It is this basic analogy that leads to the analogy between electrical and mechanical impedance.\n\nThe principal disadvantage of the impedance analogy is that it does not preserve the topology of the mechanical system. Elements that are in series in the mechanical system are in parallel in the electrical equivalent circuit and vice versa.\n\nThe impedance matrix representation of a transducer transforms force in the mechanical domain into current in the electrical domain. Likewise, velocity in the mechanical domain is transformed into voltage in the electrical domain. A two-port device that transforms a voltage into an analogous quantity can be represented as a simple transformer. A device that transforms a voltage into an analogue of the dual property of voltage (that is, current, whose analogue is velocity) is represented as a gyrator. Since force is analogous to voltage, not current, this may seem like a disadvantage on the face of it. However, many practical transducers, especially at audio frequencies, work by electromagnetic induction and are governed by just such a relationship. For instance, the force on a current-carrying conductor is given by,\n\nThe impedance analogy is sometimes called the Maxwell analogy after James Clerk Maxwell (1831–1879) who used mechanical analogies to explain his ideas of electromagnetic fields. However, the term \"impedance\" was not coined until 1886 (by Oliver Heaviside), the idea of complex impedance was introduced by Arthur E. Kennelly in 1893, and the concept of impedance was not extended into the mechanical domain until 1920 by Kennelly and Arthur Gordon Webster.\n\nHenri Poincaré in 1907 was the first to describe a transducer as a pair of linear algebraic equations relating electrical variables (voltage and current) to mechanical variables (force and velocity). Wegel, in 1921, was the first to express these equations in terms of mechanical impedance as well as electrical impedance.\n\n"}
{"id": "2137031", "url": "https://en.wikipedia.org/wiki?curid=2137031", "title": "International Association of Art Critics", "text": "International Association of Art Critics\n\nThe International Association of Art Critics (\"Association Internationale des Critiques d’Art\", \"AICA\") was founded in 1950 to revitalize critical discourse, which suffered under Fascism during World War II. Affiliated with UNESCO AICA was admitted to the rank of non-governmental organization in 1951.\n\nThe main objectives of AICA are:\n\nInternational Conferences of A.I.C.L. in Greece (2004 and 2010)\n\n"}
{"id": "4599324", "url": "https://en.wikipedia.org/wiki?curid=4599324", "title": "Iraq Freedom Congress", "text": "Iraq Freedom Congress\n\nThe Iraq Freedom Congress (or Iraqi Freedom Congress, IFC) is a libertarian socialist, progressive, democratic, and secularist group. The Congress was formed in March 2005 by labor groups, women's groups, students, and neighborhood groups concerned about sectarian violence, Ba'athism, Islamism, and nationalism, as well as the post-war occupation.\n\nThe Congress has been active in protests, most recently regarding post-occupation security and checkpoint policy.\n\n"}
{"id": "144930", "url": "https://en.wikipedia.org/wiki?curid=144930", "title": "James Mark Baldwin", "text": "James Mark Baldwin\n\nJames Mark Baldwin (January 12, 1861, Columbia, South Carolina – November 8, 1934, Paris) was an American philosopher and psychologist who was educated at Princeton under the supervision of Scottish philosopher James McCosh and who was one of the founders of the Department of Psychology at the university. He made important contributions to early psychology, psychiatry, and to the theory of evolution.\n\nBaldwin was born and raised in Columbia, South Carolina. His father, who was from Connecticut, was an abolitionist and was known to purchase slaves in order to free them. During the Civil War his father moved north, but the family remained in their home until the time of Sherman's March. Upon their return after the war, Baldwin's father was part of the Reconstruction Era government. Baldwin was sent north to receive his secondary education in New Jersey. As a result, he chose to attend the College of New Jersey (now Princeton University).\n\nBaldwin started in theology under the tutelage of the college's president, James McCosh, but soon switched to philosophy. He was awarded the Green Fellowship in Mental Science (named after his future father-in-law, the head of the Princeton Theological Seminary) and used it to study in Germany with Wilhelm Wundt at Leipzig and with Friedrich Paulsen at Berlin (1884–1934).\n\nIn 1885 he became Instructor in French and German at the Princeton Theological Seminary. He translated Théodule-Armand Ribot's \"German Psychology of Today\" and wrote his first paper \"The Postulates of a Physiological Psychology\". Ribot's work traced the origins of psychology from Immanuel Kant through Johann Friedrich Herbart, Gustav Theodor Fechner, Hermann Lotze to Wundt.\n\nIn 1887, while working as a professor of philosophy at Lake Forest College he married Helen Hayes Green, the daughter of the President of the Seminary, William Henry Green. At Lake Forest he published the first part of his \"Handbook of Psychology (Senses and Intellect)\" in which he directed attention to the new experimental psychology of Ernst Heinrich Weber, Fechner and Wundt.\n\nIn 1889 he went to the University of Toronto as the Chair of Logic and Metaphysics. His creation of a laboratory of experimental psychology at Toronto (which he claimed was the first in the British Empire) coincided with the birth of his daughters Helen (1889) and Elizabeth (1891) which inspired the quantitative and experimental research on infant development that was to make such a vivid impression on Jean Piaget and Lawrence Kohlberg through Baldwin's \"Mental Development in the Child and the Race: Methods and Processes\" (1894) dedicated to the subject.\nA second part of \"Handbook of Psychology (Feeling and Will)\" appeared in 1891.\n\nDuring this creative phase Baldwin traveled to France (1892) to visit the important psychologists Charcot (at the Salpêtrière), Hippolyte Bernheim (at Nancy), and Pierre Janet.\n\nIn 1893 he was called back to his alma mater, Princeton University, where he was offered the Stuart Chair in Psychology and the opportunity to establish a new psychology laboratory. He would stay at Princeton until 1903 working out the highlights of his career reflected in \"Social and Ethical Interpretations in Mental Development: A Study in Social Psychology\" (1897) where he took his previous \"Mental Development\" to the critical stage in which it survived in the work of Lev Vygotsky, through Vygotsky in the crucial work of Alexander Luria, and in the synthesis of both by Aleksey Leontyev. He also edited the English editions of Karl Groos's \"Play of Animals\" (1898) and \"Play of Men\" (1901).\n\nIt was during this time that Baldwin wrote \"A New Factor of Evolution\" (June 1896/\"The American Naturalist\") which later became known as the \"Baldwin Effect\". But other important contributors should not be overlooked. Conwy Lloyd Morgan was perhaps closest to understanding the so-called \"Baldwin Effect\". In his \"Habit and Instinct\" (1896) he phrased a comparable version of the theory, as he did in an address to a session of the New York Academy of Sciences (February 1896) in the presence of Baldwin. (1896/\"Of modification and variation\".\" Science\" 4(99) (November 20):733-739). As did Henry Fairfield Osborn (1896/\"A mode of evolution requiring neither natural selection nor the inheritance of acquired characteristics\". \"Transactions of the New York Academy of Science\" 15:141-148). The \"Baldwin Effect\", building in part on the principle of \"organic selection\" proposed by Baldwin in \"Mental Development\" did only receive its name from George Gaylord Simpson in 1953. (in: \"Evolution\" 7:110-117) (see: David J. Depew in \"Evolution and Learning\" M.I.T. 2003)\n\nBaldwin complemented his psychological work with philosophy, in particular epistemology his contribution to which he presented in the presidential address to the American Psychological Association in 1897. By then the work on the \"Dictionary of Philosophy and Psychology\" (1902) had been announced and a period of intense philosophical correspondence ensued with the contributors to the project: William James, John Dewey, Charles Sanders Peirce, Josiah Royce, George Edward Moore, Bernard Bosanquet, James McKeen Cattell, Edward B. Titchener, Hugo Münsterberg, Christine Ladd-Franklin, Adolf Meyer, George Stout, Franklin Henry Giddings, Edward Bagnall Poulton and others.\n\nIn 1899 Baldwin went to Oxford to supervise the completion of the \"Dictionary\"... (1902). He was awarded an Honorary Doctorate in Science at Oxford University. (In the light of the foregoing, the deafening silence with which J. M. Baldwin was later treated in Oxford publications on the mind may well come to be regarded as one of the significant omissions in the history of ideas for the 20th century. Compare for example Richard Gregory: \"The Oxford Companion to the Mind\", first edition, 1987.)\n\nIn 1903, partly as a result of a dispute with Princeton president Woodrow Wilson and in part due to an offer involving more pay and less teaching, he moved to a professorship of philosophy and psychology at Johns Hopkins University. Here, he re-opened the experimental laboratory that had been founded by G. Stanley Hall in 1884 (but had closed with Hall's departure to take over the presidency of Clark University in 1888).\n\nIn Baltimore Baldwin started to work on \"Thoughts and Things: A Study of the Development and Meaning of Thought, Or Genetic Logic\" (1906), a densely integrative rendering of his ideas culminating in \"Genetic Theory of Reality: Being the Outcome of Genetic Logic as Issuing in the Aesthetic Theory of Reality called Pancalism\" (1915). This book introduced the concept that knowledge grows through childhood in a series of distinct stages that involve interaction between innate abilities and environmental feedback. He further stated that the initial physical development gives way to language and cognitive abilities such that the child emerges as a result of social and physical growth.\n\nIn Baltimore also Baldwin was arrested in a raid on a \"colored\" brothel (1908), a scandal that put an end to his American career. Forced to leave Johns Hopkins he looked for residence in Paris. He was to reside in France till his death in 1934.\n\nHis first years (1908–1912) in France were interrupted by long stays in Mexico where he advised on university matters and lectured at the School of Higher Studies at the National University in Mexico City. His \"Darwin and the Humanities\" (1909) and \"Individual and Society\" (1911) date from this period.\nIn 1912 he took permanent residence in Paris.\n\nBaldwin's residence in France resulted in his pointing out the urgency of American non-neutral support for his new hosts on the French battlefields of World War I. He published \"American Neutrality, Its Cause and Cure\" (1916) for the purpose, and when in 1916 he survived a German torpedo attack on the in the English Channel – on the return trip from a visit to William Osler at Oxford – his open telegram to the President of the United States on the affair became frontpage news (\"New York Times\"). With the entry of America in the war (1917) he helped to organize the Paris branch of the American Navy League, acting as its Chairman till 1922. In 1926 his memoirs \"Between Two Wars (1861-1921)\" were published. He died in Paris on November 8, 1934.\n\nBaldwin was prominent among early experimental psychologists and was voted by his peers as the fifth most important psychologist in America in a 1903 survey conducted by James McKeen Cattell, but it was his contributions to developmental psychology that were the most important. His stepwise theory of cognitive development was a major influence on the later, and much more widely known, developmental theory of Jean Piaget. Equally important is his contribution to the development of what is currently known as Theory of Mind (ToM). His ideas on the relationship of Ego and Alter were developed by Pierre Janet; while his stress on how \"My sense of self grows by imitation of you...an imitative creation\" contributed to the mirror stage of Jacques Lacan.\n\nHis contributions to the young discipline's early journals and institutions were highly significant as well. Baldwin was a co-founder (with James McKeen Cattell) of \"Psychological Review\" (which was founded explicitly to compete with G. Stanley Hall's \"American Journal of Psychology\"), \"Psychological Monographs\" and \"Psychological Index.\" He was also the founding editor of \"Psychological Bulletin\".\n\nIn 1892 he was vice-president of the International Congress of Psychology held in London, and in 1897–1898 president of the American Psychological Association; he received a gold medal from the Royal Academy of Arts and Sciences of Denmark (1897), and was honorary president of the International Congress of Criminal Anthropology held in Geneva in 1896.\n\nThe idea of organic selection came from the interpretation of the observable data in Baldwin's experimental study of infant reaching and its role in mental development. Every practice of the infant's movement intended to advance the integration of behavior favorable to development in the experimental framework appeared to be selected from an excess of movement in the trial of imitation.\n\nIn further stages of development – the ones most critical to an understanding of the evolution of mind – this was graphically illustrated in the child's efforts to draw and learning to write. (\"Mental Development in the Child and the Race\")\n\nIn later editions of \"Mental Development\" Baldwin changed the term \"\"organic\" selection\" into \"\"functional\" selection\".\n\nSo, from the outset the idea was well linked to the philosophy of mind Baldwin was emancipating from the models inspired by divine pre-establishment. (Spinoza) (Wozniak, 2001)\n\nIt is the communication of this insight into the practice-related nature of dynamogenic development, above all its integration as a creative factor in the fabric of society, that helped the students of Baldwin to understand what was left of Lamarck's signature. Singularly illustrated by Gregory Bateson in \"Mind and Nature\" (1979) and reintegrated in contemporary studies by Terrence Deacon (\"The Symbolic Species: The co-evolution of language and the human brain\", 1997) and other scholars of biosemiotics.\n\nIn the human species the faculty of niche building is favored by a practical intelligence able to design the circumstances that will put its vital acquirements out of harm's way in terms of (linearly predicted) natural selection. It is precisely in the fields of study relating to massive selection pressures against which other species seem to be without defenses – biological development in the face of novel pandemics (AIDS, mad cow disease) – that the arguments relative to the natural heredity of intelligent acquirements have resurfaced in a way most challenging to science.\n\nBaldwin's most important theoretical legacy is the concept of the Baldwin effect or \"Baldwinian evolution\". Baldwin proposed, against the neo-Lamarckians of his day (most notably Edward Drinker Cope), that there is a mechanism whereby epigenetic factors come to shape the congenital endowment as much as – or more than – natural selection pressures. In particular, human behavioral decisions made and sustained across generations as a set of cultural practices ought to be considered among the factors shaping the human genome.\n\nFor example, the incest taboo, if powerfully enforced, removes the natural selection pressure against the possession of incest-favoring instincts. After a few generations without this natural selection pressure, unless such genetic material were profoundly fixed, it would tend to diversify and lose its function. Humans would no longer be innately averse to incest, but would rely on their capacity to internalize such rules from cultural practices.\n\nThe opposite case can also be true: cultural practice might selectively breed humans to meet the fitness conditions of new environments, cultural and physical, which earlier hominids could not have survived. Baldwinian evolution might strengthen or weaken a genetic trait.\n\nBaldwin's contribution to this field places him at the heart of contemporary controversies in the fields of evolutionary psychology and wider sociobiology. Few people did more than Robert Wozniak, Professor of Psychology at Bryn Mawr College, for the rediscovery of the significance of James Mark Baldwin in the history of ideas.\n\nApart from articles in the \"Psychological Review\", Baldwin wrote:\n\nHe also largely contributed to the \"Dictionary of Philosophy and Psychology\" (1901–1905), of which he was editor in-chief.\n\n\nHothersall, David (2004). \"History of Psychology\" (4th ed.). New York, NY; McGraw-Hill Companies, Inc.\n\n"}
{"id": "173505", "url": "https://en.wikipedia.org/wiki?curid=173505", "title": "Just war theory", "text": "Just war theory\n\nJust war theory (Latin: \"jus bellum justum\") is a doctrine, also referred to as a tradition, of military ethics studied by military leaders, theologians, ethicists and policy makers. The purpose of the doctrine is to ensure war is morally justifiable through a series of criteria, all of which must be met for a war to be considered just. The criteria are split into two groups: \"right to go to war\" (\"jus ad bellum\") and \"right conduct in war\" (\"jus in bello\"). The first concerns the morality of going to war, and the second the moral conduct within war. Recently there have been calls for the inclusion of a third category of just war theory—\"jus post bellum\"—dealing with the morality of post-war settlement and reconstruction.\n\nJust war theory postulates that war, while terrible, is not always the worst option. Important responsibilities, undesirable outcomes, or preventable atrocities may justify war.\n\nOpponents of just war theory may be either inclined to a stricter pacifist standard (proposing that there has never been and/or can never be a justifiable basis for war) or toward a more permissive nationalist standard (proposing that a war need only serve a nation's interests to be justifiable). In a large number of cases, philosophers state that individuals need not be of guilty conscience if required to fight. A few ennoble the virtues of the soldier while declaring their apprehensions for war itself. A few, such as Rousseau, argue for insurrection against oppressive rule.\n\nThe historical aspect, or the \"just war tradition\", deals with the historical body of rules or agreements that have applied in various wars across the ages. The just war tradition also considers the writings of various philosophers and lawyers through history, and examines both their philosophical visions of war's ethical limits and whether their thoughts have contributed to the body of conventions that have evolved to guide war and warfare.\n\nThe Indian Hindu epic, the Mahabharata, offers one of the first written discussions of a \"just war\" (\"dharma-yuddha\" or \"righteous war\"). In it, one of five ruling brothers asks if the suffering caused by war can ever be justified, and then a long discussion ensues between the siblings, establishing criteria like \"proportionality\" (chariots cannot attack cavalry, only other chariots; no attacking people in distress), \"just means\" (no poisoned or barbed arrows), \"just cause\" (no attacking out of rage), and fair treatment of captives and the wounded.\nThe war in the Mahabharata is preceded by context that develops the \"just cause\" for the war including last minute efforts to reconcile differences to avoid war. At the beginning of the war, there is the discussion of \"just conduct\" appropriate to the context of war.\n\nA 2017 study found that the just war tradition can be traced as far back as to Ancient Egypt, \"demonstrating that just war thought developed beyond the boundaries of Europe and existed many centuries earlier than the advent of Christianity or even the emergence of Greco-Roman doctrine.\"\n\nIn ancient Rome, a \"just cause\" for war might include the necessity of repelling an invasion, or retaliation for pillaging or a breach of treaty. War was always potentially \"nefas\" (\"wrong, forbidden\"), and risked religious pollution and divine disfavor. A \"just war\" (\"bellum iustum\") thus required a ritualized declaration by the fetial priests. More broadly, conventions of war and treaty-making were part of the \"ius gentium\", the \"law of nations\", the customary moral obligations regarded as innate and universal to human beings. The quintessential explanation of Just War theory in the ancient world is found in Cicero's \"De Officiis\", Book 1, sections 1.11.33–1.13.41. Although, it is well known that Julius Caesar did not often follow these necessities. \n\nChristian theory of the Just War begins with Augustine of Hippo and Thomas Aquinas.\n\nAugustine of Hippo claimed that, while individuals should not resort immediately to violence, God has given the sword to government for good reason (based upon Romans 13:4). In \"Contra Faustum Manichaeum\" book 22 sections 69–76, Augustine argues that Christians, as part of a government, need not be ashamed of protecting peace and punishing wickedness when forced to do so by a government. Augustine asserted that this was a personal, philosophical stance: \"What is here required is not a bodily action, but an inward disposition. The sacred seat of virtue is the heart.\"\n\nNonetheless, he asserted, peacefulness in the face of a grave wrong that could only be stopped by violence would be a sin. Defense of one's self or others could be a necessity, especially when authorized by a legitimate authority:They who have waged war in obedience to the divine command, or in conformity with His laws, have represented in their persons the public justice or the wisdom of government, and in this capacity have put to death wicked men; such persons have by no means violated the commandment, \"Thou shalt not kill.\"While not breaking down the conditions necessary for war to be just, Augustine nonetheless originated the very phrase itself in his work \"The City of God\":But, say they, the wise man will wage Just Wars. As if he would not all the rather lament the necessity of just wars, if he remembers that he is a man; for if they were not just he would not wage them, and would therefore be delivered from all wars.J. Mark Mattox writes that, for the individual Christian under the rule of a government engaged in an immoral war, Augustine admonished that Christians, \"by divine edict, have no choice but to subject themselves to their political masters and [should] seek to ensure that they execute their war-fighting duty as justly as possible.\"\n\nNine hundred years later, Thomas Aquinas (1225–1274) laid out the conditions under which a war could be justified (combining the theological principles of faith with the philosophical principles of reason, he ranked among the most influential thinkers of medieval Scholasticism):\n\nIn the Summa Theologica, Thomas proceeded to distinguish between philosophy and theology, and between reason and revelation, though he emphasized that these did not contradict each other. Both are fountains of knowledge; both come from God.\n\nThe School of Salamanca expanded on Thomistic understanding of natural law and just war. It stated that war is one of the worst evils suffered by mankind. The School's adherents reasoned that war should be a last resort, and only then, when necessary to prevent an even greater evil. Diplomatic resolution is always preferable, even for the more powerful party, before a war is started. Examples of \"just war\" are:\n\nA war is not legitimate or illegitimate simply based on its original motivation: it must comply with a series of additional requirements:\n\nUnder this doctrine expansionist wars, wars of pillage, wars to convert infidels or pagans, and wars for glory are all inherently unjust.\n\nThe just war doctrine of the Catholic Church—sometimes mistaken as the \"just war theory\"—found in the 1992 \"Catechism of the Catholic Church\", in paragraph 2309, lists four strict conditions for \"legitimate defense by military force\":\n\nThe \"Compendium of the Social Doctrine of the Church\" elaborates on the Just War Doctrine in paragraphs 500 to 501:\n\nThe first work dedicated specifically to it was \"De bellis justis\" of Stanisław of Skarbimierz (1360–1431), who justified war by the Kingdom of Poland with Teutonic Knights. Francisco de Vitoria criticized the conquest of America by the Kingdom of Spain on the basis of just war theory. With Alberico Gentili and Hugo Grotius just war theory was replaced by international law theory, codified as a set of rules, which today still encompass the points commonly debated, with some modifications. The importance of the theory of just war faded with the revival of classical republicanism beginning with works of Thomas Hobbes.\n\nAlthough the criticism can be made that the application of just war theory is relativistic, one of the fundamental bases of the tradition is the Ethic of Reciprocity, particularly when it comes to \"in bello\" considerations of deportment during battle. If one set of combatants promise to treat their enemies with a modicum of restraint and respect, then the hope is that other sets of combatants will do similarly in reciprocation, (a concept not unrelated to the considerations of Game Theory).\n\nJust war theorists combine a moral abhorrence towards war with a readiness to accept that war may sometimes be necessary. The criteria of the just war tradition act as an aid to determining whether resorting to arms is morally permissible. Just war theories are attempts \"to distinguish between justifiable and unjustifiable uses of organized armed forces\"; they attempt \"to conceive of how the use of arms might be restrained, made more humane, and ultimately directed towards the aim of establishing lasting peace and justice\".\n\nThe just war tradition addresses the morality of the use of force in two parts: when it is right to resort to armed force (the concern of \"jus ad bellum\") and what is acceptable in using such force (the concern of \"jus in bello\"). In more recent years, a third category—\"jus post bellum\"—has been added, which governs the justice of war termination and peace agreements, as well as the prosecution of war criminals.\n\nSoviet leader Vladimir Lenin defined only three types of just war, all of which share the central trait of being revolutionary in character. In simple terms: \"To the Russian workers has fallen the honour and the good fortune of being the first to start the revolution—the great and only legitimate and just war, the war of the oppressed against the oppressors.\", with these two opposing categories being defined in terms of class, as is typical in the left. In that manner, Lenin shunned the more common interpretation of a defensive war as a just one -- often summarized as \"who fired the first shot?\" -- precisely because it didn't take in consideration the class factor. Which side initiated aggressions or had a grievance or any other commonly considered factor of \"jus ad bellum\" mattered not at all, he claimed; if one side was being oppressed by the other, the war against the oppressor would always be, by definition, a defensive war anyway. Any war lacking this duality of oppressed and oppressor was, in contradistinction, always a reactionary, unjust war, in which the oppressed effectively fight in order to protect their own oppressors:\n\n\"But picture to yourselves a slave-owner who owned 100 slaves warring against a slave-owner who owned 200 slaves for a more \"just\" distribution of slaves. Clearly, the application of the term \"defensive\" war, or war \"for the defence of the fatherland\" in such a case would be historically false, and in practice would be sheer deception of the common people, of philistines, of ignorant people, by the astute slaveowners. Precisely in this way are the present-day imperialist bourgeoisie deceiving the peoples by means of \"national ideology\" and the term \"defence of the fatherland\" in the present war between slave-owners for fortifying and strengthening slavery.\"\nAnarcho-capitalist scholar Murray Rothbard stated: \"a \"just\" war exists when a people tries to ward off the threat of coercive domination by another people, or to overthrow an already-existing domination. A war is \"unjust\", on the other hand, when a people try to impose domination on another people, or try to retain an already existing coercive rule over them.\"\n\nJonathan Riley-Smith writes,\n\nThe consensus among Christians on the use of violence has changed radically since the crusades were fought. The just war theory prevailing for most of the last two centuries—that violence is an evil that can, in certain situations, be condoned as the lesser of evils—is relatively young. Although it has inherited some elements (the criteria of legitimate authority, just cause, right intention) from the older war theory that first evolved around AD 400, it has rejected two premises that underpinned all medieval just wars, including crusades: first, that violence could be employed on behalf of Christ's intentions for mankind and could even be directly authorized by him; and second, that it was a morally neutral force that drew whatever ethical coloring it had from the intentions of the perpetrators.\n\nJust War Theory has two sets of criteria, the first establishing \"jus ad bellum\" (the right to go to war), and the second establishing \"jus in bello\" (right conduct within war).\n\n\nIn modern terms, just war is waged in terms of self-defense, or in defense of another (with sufficient evidence).\n\nOnce war has begun, just war theory (\"jus in bello\") also directs how combatants are to act or should act:\n\n\n\n\n\n\nIn April 1917, two weeks after the United States Congress declared war on Germany, Cardinal James Gibbons of Baltimore, the \"de facto\" head of the U.S. Catholic church, issued a letter that all Catholics were to support the war. The Episcopal bishop of New York, William Manning said the following: \nIn recent years, some theorists, such as Gary Bass, Louis Iasiello and Brian Orend, have proposed a third category within Just War theory. \"Jus post bellum\" concerns justice after a war, including peace treaties, reconstruction, environmental remediation, war crimes trials, and war reparations. \"Jus post bellum\" has been added to deal with the fact that some hostile actions may take place outside a traditional battlefield. \"Jus post bellum\" governs the justice of war termination and peace agreements, as well as the prosecution of war criminals, and publicly labeled terrorists. This idea has largely been added to help decide what to do if there are prisoners that have been taken during battle. It is, through government labeling and public opinion, that people use \"jus post bellum\" to justify the pursuit of labeled terrorist for the safety of the government's state in a modern context. The actual fault lies with the aggressor, so by being the aggressor they forfeit their rights for honorable treatment by their actions. This is the theory used to justify the actions taken by anyone fighting in a war to treat prisoners outside of war. Actions after a conflict can be warranted by actions observed during war, meaning that there can be justification to meet violence with violence even after war. Orend, who was one of the theorist mentioned earlier, proposes the following principles:\n\n\nThere are many theories that correlate with the Just War Theory doctrine, which include:\n\n\nThese theorists either approve of war as retaliation, or of war as a last resort.\n\n\nThese theorists do not approve of war, but provide arguments for justifying retaliation when another starts war.\n\n\n\n"}
{"id": "48883792", "url": "https://en.wikipedia.org/wiki?curid=48883792", "title": "Krogh model", "text": "Krogh model\n\nKrogh model is a scientific model in the area mass transfer explaining concentration of molecular oxygen through a cylindrical capillary tube as a function of a changing position over the capillary tube's length. It was first conceptualized by August Krogh in 1919 to describe oxygen supply in living tissues, particularly the one occurring in human blood vessels. Its applicability has been extended to various academic fields, and has been successful explaining drug diffusion, water transport, and ice formation in tissues.\n\nKrogh model is derived by applying Fick's laws of diffusion and the law of conservation of mass over a radial interval formula_1\n\nAlthough Krogh model is a good approximation, it underestimates oxygen consumption because the cylinder model does not include all the tissue surrounding the capillary.\n\n"}
{"id": "187588", "url": "https://en.wikipedia.org/wiki?curid=187588", "title": "Lamarckism", "text": "Lamarckism\n\nLamarckism (or Lamarckian inheritance) is the hypothesis that an organism can pass on characteristics that it has acquired through use or disuse during its lifetime to its offspring. It is also known as the inheritance of acquired characteristics or soft inheritance. It is inaccurately named after the French biologist Jean-Baptiste Lamarck (1744–1829), who incorporated the action of soft inheritance into his evolutionary theories as a supplement to his concept of orthogenesis, a drive towards complexity. The theory is cited in textbooks to contrast with Darwinism. This paints a false picture of the history of biology, as Lamarck did not originate the idea of soft inheritance, which was known from the classical era onwards, and it was not the primary focus of Lamarck's theory of evolution. Further, in \"On the Origin of Species\" (1859), Charles Darwin supported the idea of \"use and disuse inheritance\", though rejecting other aspects of Lamarck's theory.\n\nMany researchers from the 1860s onwards attempted to find evidence for the theory, but these have all been explained away either by other mechanisms such as genetic contamination, or as fraud. Later, Mendelian genetics supplanted the notion of inheritance of acquired traits, eventually leading to the development of the modern synthesis, and the general abandonment of the Lamarckism in biology. Despite this, interest in Lamarckism has continued.\n\nStudies in the field of epigenetics and somatic hypermutation have highlighted the possible inheritance of behavioral traits acquired by the previous generation. These remain controversial, not least because historians of science have asserted that it is inaccurate to describe transgenerational epigenetic inheritance as a form of Lamarckism. The inheritance of the hologenome, consisting of the genomes of all an organism's symbiotic microbes as well as its own genome, is also somewhat Lamarckian in effect, though entirely Darwinian in its mechanisms.\n\nThe inheritance of acquired characteristics was proposed in ancient times, and remained a current idea for many centuries. The historian of science Conway Zirkle wrote in 1935 that:\nZirkle noted that Hippocrates described pangenesis, the theory that what is inherited derives from the whole body of the parent, whereas Aristotle thought it impossible; but that all the same, Aristotle implicitly agreed to the inheritance of acquired characteristics, giving the example of the inheritance of a scar, or of blindness, though noting that children do not always resemble their parents. Zirkle recorded that Pliny the Elder thought much the same. Zirkle also pointed out that stories involving the idea of inheritance of acquired characteristics appear numerous times in ancient mythology and the Bible, and persisted through to Rudyard Kipling's \"Just So Stories\". Erasmus Darwin's \"Zoonomia\" (c. 1795) suggested that warm-blooded animals develop from \"one living filament... with the power of acquiring new parts\" in response to stimuli, with each round of \"improvements\" being inherited by successive generations.\n\nCharles Darwin's \"On the Origin of Species\" proposed natural selection as the main mechanism for development of species, but did not rule out a variant of Lamarckism as a supplementary mechanism. Darwin called this pangenesis, and explained it in the final chapter of his book \"The Variation of Animals and Plants under Domestication\" (1868), after describing numerous examples to demonstrate what he considered to be the inheritance of acquired characteristics. Pangenesis, which he emphasised was a hypothesis, was based on the idea that somatic cells would, in response to environmental stimulation (use and disuse), throw off 'gemmules' or 'pangenes' which travelled around the body, though not necessarily in the bloodstream. These pangenes were microscopic particles that supposedly contained information about the characteristics of their parent cell, and Darwin believed that they eventually accumulated in the germ cells where they could pass on to the next generation the newly acquired characteristics of the parents.\n\nDarwin's half-cousin, Francis Galton, carried out experiments on rabbits, with Darwin's cooperation, in which he transfused the blood of one variety of rabbit into another variety in the expectation that its offspring would show some characteristics of the first. They did not, and Galton declared that he had disproved Darwin's hypothesis of pangenesis, but Darwin objected, in a letter to the scientific journal \"Nature\", that he had done nothing of the sort, since he had never mentioned blood in his writings. He pointed out that he regarded pangenesis as occurring in protozoa and plants, which have no blood, as well as in animals.\n\nBetween 1800 and 1830, Lamarck proposed a systematic theoretical framework for understanding evolution. He saw evolution as comprising four laws:\n\n\nIn 1830, in an aside from his evolutionary framework, Lamarck briefly mentioned two traditional ideas in his discussion of heredity, in his day considered to be generally true. The first was the idea of use versus disuse; he theorized that individuals lose characteristics they do not require, or use, and develop characteristics that are useful. The second was to argue that the acquired traits were heritable. He gave as an imagined illustration the idea that when giraffes stretch their necks to reach leaves high in trees, they would strengthen and gradually lengthen their necks. These giraffes would then have offspring with slightly longer necks. In the same way, he argued, a blacksmith, through his work, strengthens the muscles in his arms, and thus his sons would have similar muscular development when they mature. Lamarck stated the following two laws:\n\n\nEnglish translation:\n\n\nIn essence, a change in the environment brings about change in \"needs\" (\"besoins\"), resulting in change in behavior, bringing change in organ usage and development, bringing change in form over time—and thus the gradual transmutation of the species. However, as the evolutionary biologists and historians of science Michael Ghiselin and Steven Jay Gould have pointed out, these ideas were not original to Lamarck.\n\nThe idea that germline cells contain information that passes to each generation unaffected by experience and independent of the somatic (body) cells, came to be referred to as the Weismann barrier, as it would make Lamarckian inheritance from changes to the body difficult or impossible.\n\nAugust Weismann conducted the experiment of removing the tails of 68 white mice, and those of their offspring over five generations, and reporting that no mice were born in consequence without a tail or even with a shorter tail. In 1889, he stated that \"901 young were produced by five generations of artificially mutilated parents, and yet there was not a single example of a rudimentary tail or of any other abnormality in this organ.\" The experiment, and the theory behind it, were thought at the time to be a refutation of Lamarckism.\n\nHowever, the experiment's effectiveness in refuting Lamarck's hypothesis is doubtful, as it did not address the \"use and disuse\" of characteristics in response to the environment. The biologist Peter Gauthier noted in 1990 that:\n\nCan Weismann's experiment be considered a case of disuse? Lamarck proposed that when an organ was not used, it slowly, and very gradually atrophied. In time, over the course of many generations, it would gradually disappear as it was inherited in its modified form in each successive generation. Cutting the tails off mice does not seem to meet the qualifications of disuse, but rather falls in a category of accidental misuse... Lamarck's hypothesis has never been proven experimentally and there is no known mechanism to support the idea that somatic change, however acquired, can in some way induce a change in the germplasm. On the other hand it is difficult to disprove Lamarck's idea experimentally, and it seems that Weismann's experiment fails to provide the evidence to deny the Lamarckian hypothesis, since it lacks a key factor, namely the willful exertion of the animal in overcoming environmental obstacles.\n\nThe biologist and historian of science Michael Ghiselin also considered the Weismann tail-chopping experiment to have no bearing on the Lamarckian hypothesis, writing in 1994 that:\n\nThe acquired characteristics that figured in Lamarck's thinking were changes that resulted from an individual's own drives and actions, not from the actions of external agents. Lamarck was not concerned with wounds, injuries or mutilations, and nothing that Lamarck had set forth was tested or \"disproven\" by the Weismann tail-chopping experiment.\n\nThe identification of Lamarckism with the inheritance of acquired characteristics is regarded by evolutionary biologists including Michael Ghiselin as a falsified artifact of the subsequent history of evolutionary thought, repeated in textbooks without analysis, and wrongly contrasted with a falsified picture of Darwin's thinking. Ghiselin notes that \"Darwin accepted the inheritance of acquired characteristics, just as Lamarck did, and Darwin even thought that there was some experimental evidence to support it.\" American paleontologist and historian of science Stephen Jay Gould wrote that in the late 19th century, evolutionists \"re-read Lamarck, cast aside the guts of it ... and elevated one aspect of the mechanics—inheritance of acquired characters—to a central focus it never had for Lamarck himself.\" He argued that \"the restriction of 'Lamarckism' to this relatively small and non-distinctive corner of Lamarck's thought must be labelled as more than a misnomer, and truly a discredit to the memory of a man and his much more comprehensive system.\"\n\nThe period of the history of evolutionary thought between Darwin's death in the 1880s, and the foundation of population genetics in the 1920s and the beginnings of the modern evolutionary synthesis in the 1930s, is called the eclipse of Darwinism by some historians of science. During that time many scientists and philosophers accepted the reality of evolution but doubted whether natural selection was the main evolutionary mechanism.\n\nAmong the most popular alternatives were theories involving the inheritance of characteristics acquired during an organism's lifetime. Scientists who felt that such Lamarckian mechanisms were the key to evolution were called neo-Lamarckians. They included the British botanist George Henslow (1835–1925), who studied the effects of environmental stress on the growth of plants, in the belief that such environmentally-induced variation might explain much of plant evolution, and the American entomologist Alpheus Spring Packard, Jr., who studied blind animals living in caves and wrote a book in 1901 about Lamarck and his work. Also included were paleontologists like Edward Drinker Cope and Alpheus Hyatt, who observed that the fossil record showed orderly, almost linear, patterns of development that they felt were better explained by Lamarckian mechanisms than by natural selection. Some people, including Cope and the Darwin critic Samuel Butler, felt that inheritance of acquired characteristics would let organisms shape their own evolution, since organisms that acquired new habits would change the use patterns of their organs, which would kick-start Lamarckian evolution. They considered this philosophically superior to Darwin's mechanism of random variation acted on by selective pressures. Lamarckism also appealed to those, like the philosopher Herbert Spencer and the German anatomist Ernst Haeckel, who saw evolution as an inherently progressive process. The German zoologist Theodor Eimer combined Larmarckism with ideas about orthogenesis, the idea that evolution is directed towards a goal.\n\nWith the development of the modern synthesis of the theory of evolution, and a lack of evidence for a mechanism for acquiring and passing on new characteristics, or even their heritability, Lamarckism largely fell from favour. Unlike neo-Darwinism, neo-Lamarckism is a loose grouping of largely heterodox theories and mechanisms that emerged after Lamarck's time, rather than a coherent body of theoretical work.\n\nNeo-Lamarckian versions of evolution were widespread in the late 19th century. The idea that living things could to some degree choose the characteristics that would be inherited allowed them to be in charge of their own destiny as opposed to the Darwinian view, which placed them at the mercy of the environment. Such ideas were more popular than natural selection in the late 19th century as it made it possible for biological evolution to fit into a framework of a divine or naturally willed plan, thus the neo-Lamarckian view of evolution was often advocated by proponents of orthogenesis. According to the historian of science Peter J. Bowler, writing in 2003:\n\nScientists from the 1860s onwards conducted numerous experiments that purported to show Lamarckian inheritance. Some examples are described in the table.\n\nA century after Lamarck, scientists and philosophers continued to seek mechanisms and evidence for the inheritance of acquired characteristics. Experiments were sometimes reported as successful, but from the beginning these were either criticised on scientific grounds or shown to be fakes. For instance, in 1906, the philosopher Eugenio Rignano argued for a version that he called \"centro-epigenesis\", but it was rejected by most scientists. Some of the experimental approaches are described in the table.\n\nThe British anthropologist Frederic Wood Jones and the South African paleontologist Robert Broom supported a neo-Lamarckian view of human evolution. The German anthropologist Hermann Klaatsch relied on a neo-Lamarckian model of evolution to try and explain the origin of bipedalism. Neo-Lamarckism remained influential in biology until the 1940s when the role of natural selection was reasserted in evolution as part of the modern evolutionary synthesis.\nHerbert Graham Cannon, a British zoologist, defended Lamarckism in his 1959 book \"Lamarck and Modern Genetics\". In the 1960s, \"biochemical Lamarckism\" was advocated by the embryologist Paul Wintrebert.\n\nNeo-Lamarckism was dominant in French biology for more than a century. French scientists who supported neo-Lamarckism included Edmond Perrier (1844–1921), Alfred Giard (1846–1908), Gaston Bonnier (1853–1922) and Pierre-Paul Grassé (1895–1985). They followed two traditions, one mechanistic, one vitalistic after Henri Bergson's philosophy of evolution.\n\nIn 1987, Ryuichi Matsuda coined the term \"pan-environmentalism\" for his evolutionary theory which he saw as a fusion of Darwinism with neo-Lamarckism. He held that heterochrony is a main mechanism for evolutionary change and that novelty in evolution can be generated by genetic assimilation. His views were criticized by Arthur M. Shapiro for providing no solid evidence for his theory. Shapiro noted that \"Matsuda himself accepts too much at face value and is prone to wish-fulfilling interpretation.\"\n\nA form of Lamarckism was revived in the Soviet Union of the 1930s when Trofim Lysenko promoted the ideologically-driven research programme, Lysenkoism; this suited the ideological opposition of Joseph Stalin to genetics. Lysenkoism influenced Soviet agricultural policy which in turn was later blamed for crop failures.\n\nGeorge Gaylord Simpson in his book \"Tempo and Mode in Evolution\" (1944) claimed that experiments in heredity have failed to corroborate any Lamarckian process. Simpson noted that neo-Lamarckism \"stresses a factor that Lamarck rejected: inheritance of direct effects of the environment\" and neo-Lamarckism is closer to Darwin's pangenesis than Lamarck's views. Simpson wrote, \"the inheritance of acquired characters, failed to meet the tests of observation and has been almost universally discarded by biologists.\"\n\nBotanist Conway Zirkle pointed out that Lamarck did not originate the hypothesis that acquired characters were heritable, therefore it is incorrect to refer to it as Lamarckism:\n\nWhat Lamarck really did was to accept the hypothesis that acquired characters were heritable, a notion which had been held almost universally for well over two thousand years and which his contemporaries accepted as a matter of course, and to assume that the results of such inheritance were cumulative from generation to generation, thus producing, in time, new species. His individual contribution to biological theory consisted in his application to the problem of the origin of species of the view that acquired characters were inherited and in showing that evolution could be inferred logically from the accepted biological hypotheses. He would doubtless have been greatly astonished to learn that a belief in the inheritance of acquired characters is now labeled \"Lamarckian,\" although he would almost certainly have felt flattered if evolution itself had been so designated.\n\nPeter Medawar wrote regarding Lamarckism, \"very few professional biologists believe that anything of the kind occurs—or can occur—but the notion persists for a variety of nonscientific reasons.\" Medawar stated there is no known mechanism by which an adaptation acquired in an individual's lifetime can be imprinted on the genome and Lamarckian inheritance is not valid unless it excludes the possibility of natural selection but this has not been demonstrated in any experiment.\n\nMartin Gardner wrote in his book \"Fads and Fallacies in the Name of Science\" (1957):\n\nA host of experiments have been designed to test Lamarckianism. All that have been verified have proved negative. On the other hand, tens of thousands of experiments— reported in the journals and carefully checked and rechecked by geneticists throughout the world— have established the correctness of the gene-mutation theory beyond all reasonable doubt... In spite of the rapidly increasing evidence for natural selection, Lamarck has never ceased to have loyal followers... There is indeed a strong emotional appeal in the thought that every little effort an animal puts forth is somehow transmitted to his progeny.\n\nAccording to Ernst Mayr, any Lamarckian theory involving the inheritance of acquired characters has been refuted as \"DNA does not directly participate in the making of the phenotype and that the phenotype, in turn, does not control the composition of the DNA.\" Peter J. Bowler has written that although many early scientists took Lamarckism seriously, it was discredited by genetics in the early twentieth century.\n\nEpigenetic inheritance has been argued by scientists including Eva Jablonka and Marion J. Lamb to be Lamarckian. Epigenetics is based on hereditary elements other than genes that pass into the germ cells. These include methylation patterns in DNA and chromatin marks on histone proteins, both involved in gene regulation. These marks are responsive to environmental stimuli, differentially affect gene expression, and are adaptive, with phenotypic effects that persist for some generations. The mechanism may also enable the inheritance of behavioral traits, for example in chickens rats and human populations that have experienced starvation, DNA methylation resulting in altered gene function in both the starved population and their offspring. Methylation similarly mediates epigenetic inheritance in plants such as rice. Small RNA molecules, too, may mediate inherited resistance to infection. Handel and Romagopalan commented that \"epigenetics allows the peaceful co-existence of Darwinian and Lamarckian evolution.\"\n\nJoseph Springer and Dennis Holley commented in 2013 that:\n\nLamarck and his ideas were ridiculed and discredited. In a strange twist of fate, Lamarck may have the last laugh. Epigenetics, an emerging field of genetics, has shown that Lamarck may have been at least partially correct all along. It seems that reversible and heritable changes can occur without a change in DNA sequence (genotype) and that such changes may be induced spontaneously or in response to environmental factors—Lamarck's \"acquired traits.\" Determining which observed phenotypes are genetically inherited and which are environmentally induced remains an important and ongoing part of the study of genetics, developmental biology, and medicine.\n\nThe prokaryotic CRISPR system and Piwi-interacting RNA could be classified as Lamarckian, within a Darwinian framework.\n\nHowever, the significance of epigenetics in evolution is uncertain. Critics such as Jerry Coyne point out that epigenetic inheritance lasts for only a few generations, so it is not a stable basis for evolutionary change.\n\nThe evolutionary biologist T. Ryan Gregory contends that epigenetic inheritance should not be considered Lamarckian. According to Gregory, Lamarck did not claim the environment imposed direct effects on organisms. Instead, Lamarck \"argued that the environment created needs to which organisms responded by using some features more and others less, that this resulted in those features being accentuated or attenuated, and that this difference was then inherited by offspring.\" Gregory has stated that Lamarckian evolution in the context of epigenetics is actually closer to Darwin's view rather than Lamarck's.\n\nIn 2007, David Haig wrote that research into epigenetic processes does allow a Lamarckian element in evolution but the processes do not challenge the main tenets of the modern evolutionary synthesis as modern Lamarckians have claimed. Haig argued for the primacy of DNA and evolution of epigenetic switches by natural selection. Haig has written that there is a \"visceral attraction\" to Lamarckian evolution from the public and some scientists, as it posits the world with a meaning, in which organisms can shape their own evolutionary destiny.\n\nThomas Dickens and Qazi Rahman (2012) have argued that epigenetic mechanisms such as DNA methylation and histone modification are genetically inherited under the control of natural selection and do not challenge the modern synthesis. They dispute the claims of Jablonka and Lamb on Lamarckian epigenetic processes.\n\nIn 2015, Khursheed Iqbal and colleagues discovered that although \"endocrine disruptors exert direct epigenetic effects in the exposed fetal germ cells, these are corrected by reprogramming events in the next generation.\" Also in 2015, Adam Weiss argued that bringing back Lamarck in the context of epigenetics is misleading, commenting, \"We should remember [Lamarck] for the good he contributed to science, not for things that resemble his theory only superficially. Indeed, thinking of CRISPR and other phenomena as Lamarckian only obscures the simple and elegant way evolution really works.\"\n\nIn the 1970s, the Australian immunologist Edward J. Steele developed a neo-Lamarckian theory of somatic hypermutation within the immune system and coupled it to the reverse transcription of RNA derived from body cells to the DNA of germline cells. This reverse transcription process supposedly enabled characteristics or bodily changes acquired during a lifetime to be written back into the DNA and passed on to subsequent generations.\n\nThe mechanism was meant to explain why homologous DNA sequences from the VDJ gene regions of parent mice were found in their germ cells and seemed to persist in the offspring for a few generations. The mechanism involved the somatic selection and clonal amplification of newly acquired antibody gene sequences generated via somatic hypermutation in B-cells. The messenger RNA products of these somatically novel genes were captured by retroviruses endogenous to the B-cells and were then transported through the bloodstream where they could breach the Weismann or soma-germ barrier and reverse transcribe the newly acquired genes into the cells of the germ line, in the manner of Darwin's pangenes.\n\nThe historian of biology Peter J. Bowler noted in 1989 that other scientists had been unable to reproduce his results, and described the scientific consensus at the time:\n\nBowler commented that \"[Steele's] work was bitterly criticized at the time by biologists who doubted his experimental results and rejected his hypothetical mechanism as implausible.\"\n\nThe hologenome theory of evolution, while Darwinian, has Lamarckian aspects. An individual animal or plant lives in symbiosis with many microorganisms, and together they have a \"hologenome\" consisting of all their genomes. The hologenome can vary like any other genome by mutation, sexual recombination, and chromosome rearrangement, but in addition it can vary when populations of microorganisms increase or decrease (resembling Lamarckian use and disuse), and when it gains new kinds of microorganism (resembling Lamarckian inheritance of acquired characteristics). These changes are then passed on to offspring. The mechanism is largely uncontroversial, and natural selection does sometimes occur at whole system (hologenome) level, but it is not clear that this is always the case.\n\nThe Baldwin effect, named after the psychologist James Mark Baldwin by George Gaylord Simpson in 1953, proposes that the ability to learn new behaviours can improve an animal's reproductive success, and hence the course of natural selection on its genetic makeup. Simpson stated that the mechanism was \"not inconsistent with the modern synthesis\" of evolutionary theory, though he doubted that it occurred very often, or could be proven to occur. He noted that the Baldwin effect provide a reconciliation between the neo-Darwinian and neo-Lamarckian approaches, something that the modern synthesis had seemed to render unnecessary. In particular, the effect allows animals to adapt to a new stress in the environment through behavioural changes, followed by genetic change. This somewhat resembles Lamarckism but without requiring animals to inherit characteristics acquired by their parents. The Baldwin effect is broadly accepted by Darwinists.\n\nWithin the history of technology, Lamarckism has been used in linking cultural development to human evolution by classifying artefacts as extensions of human anatomy: in other words, as the acquired cultural characteristics of human beings. Ben Cullen has shown that a strong element of Lamarckism exists in sociocultural evolution.\n\n"}
{"id": "22258202", "url": "https://en.wikipedia.org/wiki?curid=22258202", "title": "Launch status check", "text": "Launch status check\n\nA launch status check, also known as a \"go/no go poll\" and several other terms occurs at the beginning of an American spaceflight mission in which flight controllers monitoring various systems are queried for operation and readiness status before a launch can proceed. For Space Shuttle missions, in the firing room at the Launch Control Center, the NASA Test Director (NTD) performed this check via a voice communications link with other NASA personnel. The NTD was the leader of the shuttle test team responsible for directing and integrating all flight crew, orbiter, external tank/solid rocket booster and ground support testing in the shuttle launch countdown. The NTD was also responsible for the safety of all personnel inside the pad after external tank loading, including the flight crew. He reported to the Launch Director. The Launch director declares if a mission is go for launch.\n\n\nIn the Apollo program, the MCC launch status check was initiated by the Flight Director, or FLIGHT. The following \"preflight check\" order was used before the launch of Apollo 13:\n\nVaries depending on the type of mission and model of craft, here is one example:\n\n\n\n"}
{"id": "33904406", "url": "https://en.wikipedia.org/wiki?curid=33904406", "title": "Length measurement", "text": "Length measurement\n\nLength measurement is implemented in practice in many ways. The most commonly used approaches are the transit-time methods and the interferometer methods based upon the speed of light. For objects such as crystals and diffraction gratings, diffraction is used with X-rays and electron beams. Measurement techniques for three-dimensional structures very small in every dimension use specialized instruments such as ion microscopy coupled with intensive computer modeling.\n\nFor a discussion of astronomical methods for determining cosmological distances, see the article Cosmic distance ladder.\n\nThe ruler the simplest kind of length measurement tool: lengths are defined by printed marks or engravings on a stick. The meter was initially defined using a ruler before more accurate methods became available.\n\nGauge blocks are a common method for precise measurement or calibration of measurement tools.\n\nFor small or microscopic objects, microphotography where the length is calibrated using a graticule can be used. A graticule is a piece that has lines for precise lengths etched into it. Graticules may be fitted into the eyepiece or they may be used on the measurement plane.\n\nThe basic idea behind a transit-time measurement of length is to send a signal from one end of the length to be measured to the other, and back again. The time for the round trip is the transit time Δt, and the length ℓ is then 2ℓ = Δt*\"v\",with \"v\" the speed of propagation of the signal, assuming that is the same in both directions. If light is used for the signal, its speed depends upon the medium in which it propagates; in SI units the speed is a defined value \"c\" in the reference medium of classical vacuum. Thus, when light is used in a transit-time approach, length measurements are not subject to knowledge of the source frequency (apart from possible frequency dependence of the correction to relate the medium to classical vacuum), but are subject to the error in measuring transit times, in particular, errors introduced by the response times of the pulse emission and detection instrumentation. An additional uncertainty is the \"refractive index correction\" relating the medium used to the reference vacuum, taken in SI units to be the classical vacuum. A refractive index of the medium larger than one slows the light.\n\nTransit-time measurement underlies most radio navigation systems for boats and aircraft, for example, radar and the nearly obsolete Long Range Aid to Navigation LORAN-C. For example, in one radar system, pulses of electromagnetic radiation are sent out by the vehicle (interrogating pulses) and trigger a response from a \"responder beacon\". The time interval between the sending and the receiving of a pulse is monitored and used to determine a distance. In the global positioning system a code of ones and zeros is emitted at a known time from multiple satellites, and their times of arrival are noted at a receiver along with the time they were sent (encoded in the messages). Assuming the receiver clock can be related to the synchronized clocks on the satellites, the \"transit time\" can be found and used to provide the distance to each satellite. Receiver clock error is corrected by combining the data from four satellites.\n\nSuch techniques vary in accuracy according to the distances over which they are intended for use. For example, LORAN-C is accurate to about GPS about enhanced GPS, in which a correction signal is transmitted from terrestrial stations (that is, differential GPS (DGPS)) or via satellites (that is, Wide Area Augmentation System (WAAS)) can bring accuracy to a few meters or or, in specific applications, tens of centimeters. Time-of-flight systems for robotics (for example, Laser Detection and Ranging LADAR and Light Detection and Ranging LIDAR) aim at lengths of and have an accuracy of about \n\nIn many practical circumstances, and for precision work, measurement of dimension using transit-time measurements is used only as an initial indicator of length and is refined using an interferometer. Generally, transit time measurements are preferred for longer lengths, and interferometers for shorter lengths.\n\nThe figure shows schematically how length is determined using a Michelson interferometer: the two panels show a laser source emitting a light beam split by a \"beam splitter\" (BS) to travel two paths. The light is recombined by bouncing the two components off a pair of \"corner cubes\" (CC) that return the two components to the beam splitter again to be reassembled. The corner cube serves to displace the incident from the reflected beam, which avoids some complications caused by superposing the two beams. The distance between the left-hand corner cube and the beam splitter is compared to that separation on the fixed leg as the left-hand spacing is adjusted to compare the length of the object to be measured.\n\nIn the top panel the path is such that the two beams reinforce each other after reassembly, leading to a strong light pattern (sun). The bottom panel shows a path that is made a half wavelength longer by moving the left-hand mirror a quarter wavelength further away, increasing the path difference by a half wavelength. The result is the two beams are in opposition to each other at reassembly, and the recombined light intensity drops to zero (clouds). Thus, as the spacing between the mirrors is adjusted, the observed light intensity cycles between reinforcement and cancellation as the number of wavelengths of path difference changes, and the observed intensity alternately peaks (bright sun) and dims (dark clouds). This behavior is called interference and the machine is called an interferometer. By \"counting fringes\" it is found how many wavelengths long the measured path is compared to the fixed leg. In this way, measurements are made in units of wavelengths \"λ\" corresponding to a particular atomic transition. The length in wavelengths can be converted to a length in units of metres if the selected transition has a known frequency \"f\". The length as a certain number of wavelengths \"λ\" is related to the metre using \"λ\" = . With \"c\" a defined value of 299,792,458 m/s, the error in a measured length in wavelengths is increased by this conversion to metres by the error in measuring the frequency of the light source.\n\nBy using sources of several wavelengths to generate sum and difference , absolute distance measurements become possible.\n\nThis methodology for length determination requires a careful specification of the wavelength of the light used, and is one reason for employing a laser source where the wavelength can be held stable. Regardless of stability, however, the precise frequency of any source has linewidth limitations. Other significant errors are introduced by the interferometer itself; in particular: errors in light beam alignment, collimation and fractional fringe determination. Corrections also are made to account for departures of the medium (for example, air) from the reference medium of classical vacuum. Resolution using wavelengths is in the range of ΔL/L ≈ depending upon the length measured, the wavelength and the type of interferometer used.\n\nThe measurement also requires careful specification of the medium in which the light propagates. A \"refractive index correction\" is made to relate the medium used to the reference vacuum, taken in SI units to be the classical vacuum. These refractive index corrections can be found more accurately by adding frequencies, for example, frequencies at which propagation is sensitive to the presence of water vapor. This way non-ideal contributions to the refractive index can be measured and corrected for at another frequency using established theoretical models.\n\nIt may be noted again, by way of contrast, that the transit-time measurement of length is independent of any knowledge of the source frequency, except for a possible dependence of the correction relating the measurement medium to the reference medium of classical vacuum, which may indeed depend on the frequency of the source. Where a pulse train or some other wave-shaping is used, a range of frequencies may be involved.\n\nFor small objects, different methods are used that also depend upon determining size in units of wavelengths. For instance, in the case of a crystal, atomic spacings can be determined using X-ray diffraction. The present best value for the lattice parameter of silicon, denoted \"a\", is:\n\ncorresponding to a resolution of ΔL/L ≈ Similar techniques can provide the dimensions of small structures repeated in large periodic arrays like a diffraction grating.\n\nSuch measurements allow the calibration of electron microscopes, extending measurement capabilities. For non-relativistic electrons in an electron microscope, the de Broglie wavelength is:\n\nwith \"V\" the electrical voltage drop traversed by the electron, \"m\" the electron mass, \"e\" the elementary charge, and \"h\" the Planck constant. This wavelength can be measured in terms of inter-atomic spacing using a crystal diffraction pattern, and related to the metre through an optical measurement of the lattice spacing on the same crystal. This process of extending calibration is called \"metrological traceability\". The use of metrological traceability to connect different regimes of measurement is similar to the idea behind the cosmic distance ladder for different ranges of astronomical length. Both calibrate different methods for length measurement using overlapping ranges of applicability.\n\nMeasuring dimensions of localized structures (as opposed to large arrays of atoms like a crystal), as in modern integrated circuits, is done using the scanning electron microscope. This instrument bounces electrons off the object to be measured in a high vacuum enclosure, and the reflected electrons are collected as a photodetector image that is interpreted by a computer. These are not transit-time measurements, but are based upon comparison of Fourier transforms of images with theoretical results from computer modeling. Such elaborate methods are required because the image depends on the three-dimensional geometry of the measured feature, for example, the contour of an edge, and not just upon one- or two-dimensional properties. The underlying limitations are the beam width and the wavelength of the electron beam (determining diffraction), determined, as already discussed, by the electron beam energy. \nThe calibration of these scanning electron microscope measurements is tricky, as results depend upon the material measured and its geometry. A typical wavelength is and a typical resolution is about \n\nOther small dimension techniques are the atomic force microscope, the focused ion beam and the helium ion microscope. Calibration is attempted using standard samples measured by transmission electron microscope (TEM).\n\nNuclear Overhauser effect spectroscopy (NOESY) is a specialized type of nuclear magnetic resonance spectroscopy where distances between atoms can be measured. It is based on the effect where nuclear spin cross-relaxation after excitation by a radio pulse depends on the distance between the nuclei. Unlike spin-spin coupling, NOE propagates through space and does not require that the atoms are connected by bonds, so it is a true distance measurement instead of a chemical measurement. Unlike diffraction measurements, NOESY does not require a crystalline sample, but is done in solution state and can be applied to substances that are difficult to crystallize.\n\nIn some systems of units, unlike the current SI system, lengths are fundamental units (for example, \"wavelengths\" in the older SI units and \"bohrs\" in atomic units) and are not defined by times of transit. Even in such units, however, the \"comparison\" of two lengths can be made by comparing the two transit times of light along the lengths. Such time-of-flight methodology may or may not be more accurate than the determination of a length as a multiple of the fundamental length unit.\n\n"}
{"id": "3489545", "url": "https://en.wikipedia.org/wiki?curid=3489545", "title": "Lin Hsin Hsin Art Museum", "text": "Lin Hsin Hsin Art Museum\n\nThe Lin Hsin Hsin Art Museum is notable as the first virtual museum completely metaphor after a real-world museum. It even has a live cyber graffiti wall and a search engine. \n\nThis online art museum website was originally established by the IT Inventor, digital artist, poet and composer Lin Hsin Hsin from Singapore in 1994 during the initial expansion of the World Wide Web. The site presents Lin Hsin Hsin's real-world contemporary art, digital art -- created by several breakthrough technologies, and the latest -- digitally created, displayed and performed in real-time on Android smartphone through online exhibits and has won several awards. Using advanced technology, the website was the first of its kind in Asia.\n\n"}
{"id": "8419626", "url": "https://en.wikipedia.org/wiki?curid=8419626", "title": "Linear response function", "text": "Linear response function\n\nA linear response function describes the input-output relationship of a signal transducer such as a radio turning electromagnetic waves into music or a neuron turning synaptic input into a response. Because of its many applications in information theory, physics and engineering there exist alternative names for specific linear response functions such as susceptibility, impulse response or impedance, see also transfer function. The concept of a Green's function or fundamental solution of an ordinary differential equation is closely related.\n\nDenote the input of a system by formula_1 (e.g. a force), and the response of the system by formula_2 (e.g. a position). Generally, the value of formula_2 will depend not only on the present value of formula_1, but also on past values. Approximately formula_2 is a weighted sum of the previous values of formula_6, with the weights given by the linear response function formula_7:\n\nThe explicit term on the right-hand side is the leading order term of a Volterra expansion for the full nonlinear response. If the system in question is highly non-linear, higher order terms in the expansion, denoted by the dots, become important and the signal transducer cannot adequately be described just by its linear response function.\n\nThe complex-valued Fourier transform formula_9 of the linear response function is very useful as it describes the output of the system if the input is a sine wave formula_10 with frequency formula_11. The output reads\n\nwith amplitude gain formula_13 and phase shift formula_14.\n\nConsider a damped harmonic oscillator with input given by an external driving force formula_1,\n\nThe complex-valued Fourier transform of the linear response function is given by\n\nThe amplitude gain is given by the magnitude of the complex number formula_18 and the phase shift by the arctan of the imaginary part of the function, divided by the real one.\n\nFrom this representation, we see that for small formula_19 the Fourier transform formula_9 of the linear response function yields a pronounced maximum (\"Resonance\") at the frequency formula_21. The linear response function for a harmonic oscillator is mathematically identical to that of an RLC circuit. The width of the maximum formula_22 typically is much smaller than formula_23 so that the Quality factor formula_24 can be extremely large.\n\nThe exposition of linear response theory, in the context of quantum statistics, can be found in a paper by Ryogo Kubo. This defines particularly the Kubo formula, which considers the general case that the \"force\" h(t) is a perturbation of the basic operator of the system, the Hamiltonian, formula_25 where formula_26 corresponds to a measurable quantity as input, while the output x(t) is the perturbation of the thermal expectation of another measurable quantity formula_27. The Kubo formula then defines the quantum-statistical calculation of the susceptibility formula_28 by a general formula involving only the mentioned operators. \n\nAs a consequence of the principle of causality the complex-valued function formula_29 has poles only in the lower half-plane. This leads to the Kramers–Kronig relations, which relates the real and the imaginary parts of formula_29 by integration. The simplest example is once more the damped harmonic oscillator.\n\n\n"}
{"id": "10753377", "url": "https://en.wikipedia.org/wiki?curid=10753377", "title": "MOSAIC threat assessment systems", "text": "MOSAIC threat assessment systems\n\nMOSAIC threat assessment systems (MOSAIC) is a method developed by Gavin de Becker and Associates in the early 1980s to assess and screen threats and inappropriate communications.\n\nWalt Risler of Indiana University assisted in the early development of the method, and Robert Martin, founding commander of the Los Angeles Police Department Threat Management Unit played a role in later development and enhancements. (Martin now heads up the MOSAIC threat assessment Unit at Gavin de Becker & Associates.)\n\nThe first MOSAIC systems were developed before 1992. The computer-assisted MOSAIC method is now used by the Supreme Court Police to assess threats to the Justices, by the U.S. Capitol Police for threats against Members of Congress, by police agencies protecting the governors of eleven states, by many large corporations, and by more than twenty top universities.\n\nThere are different MOSAIC systems for different situations, including:\n\n\nThe MOSAIC method poses a series of questions to users, accompanied by a range of possible answers. For every area of inquiry, the system provides a button for “Premise of the Question” – providing immediate on-screen research citations about why that particular area of inquiry is part of the assessment process. MOSAIC calculates the value of the answers selected by the assessor, and expresses the results on a scale of 1 to 10. Unlike most assessment tools, many of which are paper checklists, MOSAIC automatically produces a full written report, describing the factors that were considered and the selections made by the user.\n\nMOSAIC’s on-line resources include a library of research, publications, and training videos that users can access during an assessment.\n\nMOSAIC for assessing unwanted pursuit of public figures (MAPP) is used by agencies protecting elected and appointed officials, national security agencies, and iconic public figures.\n\nLieutenant Tom Taylor, four-time president of the National Governor’s Security Association, wrote for the Institute of Police Technology and Management:\nThe consistent way in which MOSAIC methodically guides an evaluation and documents the findings is what sets it apart. In fact, since it places less emphasis on the presence (or lack) of a direct threat, as well as any denials of intent that are uttered in an interview, MOSAIC forces the investigator to look at all of the factors present in the situation.\nIn the mid-nineties, Ted Calhoun of the United States Marshals Service undertook a research project about threats and attacks on federal judges. He studied and analyzed more than 3,000 cases of threats against Federal judges and prosecutors. Calhoun felt that these threats – often made by people whose lives were directly affected by the court – were inherently different from threats to other public figures, with whom the threateners rarely had any real contact.\n\nThe marshal's service selected the MOSAIC method for applying Calhoun’s research, and Gavin de Becker & Associates was commissioned to co-develop a new system: MOSAIC for Assessment of Threats to Judges (MAJ).\n\nThe book \"Hunters and Howlers: Threats & Violence Against Federal Judicial Officials\" describes the method that evolved into this MOSAIC:\nBy drawing as complete a mosaic of the threatened and each of his inappropriate communications as possible, de Becker’s system identifies those situations requiring a defensive reaction or a proactive response. The problem is thus managed to the best and least intrusive protection of the victim. The whole approach is informed by an intelligent, comprehensive process of thinking. De Becker’s assessments are the best because he asks the most comprehensive questions, and he consistently asks them of every communication. To do fewer risks more.\nA variation of MAJ is now used by Sheriff’s Deputies, bailiffs, and others tasked to protect judges and other judicial officials.\n\nDV - MOSAIC assesses situations involving domestic violence. As of April 2010, DV - MOSAIC is available at no-cost to the public at www.mosaicmethod.com.\n\nA study funded by the U.S. Department of Justice and published by the National Criminal Justice Reference Service found that when compared to two specific instruments, the Domestic Violence Screening Instrument (DSVI) and the Kingston Screening Instrument for Domestic Violence (K-SID), DV - MOSAIC “performed best in predicting subsequent stalking or threats.” The study also reported that MOSAIC tested highest on “sensitivity,” correctly classifying most of the women that were re-assaulted; had the strongest correlation between the victims’ perception of risk of re-assault and risk of serious harm; captured relevant information equally well with victims of various ethnicities; had scores that were significantly associated with abuse; and provided uniformity of assessment (called Inter-rater Reliability) such that ten different people of different abilities and styles would come up with the same preliminary rating.\n\nA synopsis of the findings is available at https://www.mosaicmethod.com/documents/DOJ_Study.pdf.\n\nThe study has also been published by the University of Wisconsin-Madison, The Journal of Interpersonal Violence, the United Nations Entity for Gender Equity, and the Violence Against Women Network.\n\nDV-MOSAIC is used by many police departments around the nation.\n“MOSAIC is proving successful. In the suburbs of Los Angeles where it is used, officials attribute much of the 70% decline in domestic violence to the system.\"\nRobert Ressler, a criminologist who worked in the FBI's Behavioral Sciences Unit for 16 years, has referred federal agencies to de Becker. “It has a futuristic ability to predict crime and has a proven track record,\" Ressler said. \"You can predict a crime and deal with a potential situation based on a reading from a database. It will help law enforcement deal with situations successfully.\"\n\nMOSAIC – workplace violence assesses the three most likely sources of violence in the workplace: angry employees, angry former employees, and stalkers who pursue their targets at the workplace.\n\nThe development of this MOSAIC was guided by an Advisory Board of experts and practitioners from industry, government, and law enforcement.\n\nThis version of MOSAIC is used by professionals in security departments, legal departments, and human resources offices of large organizations, government agencies, and universities.\n\nThe development of MOSAIC for threats by students (MAST) included an exploration into the pre-incident indicators of explosive school violence. The process drew on more than two hundred experts and practitioners from the fields of education, counseling, psychology, parenting, threat assessment, law enforcement, the judiciary, and students.\n\nThe MAST is used by school administrators, counselors, and security/law enforcement officers.\n\nIn 1997, Chief James Perrotti of the Yale University police stated that the department used MOSAIC for assessing threats made to Yale professors. He stated that MOSAIC allowed police to prioritize cases and better allocate their resources.\n\nIn 2007, the Missouri Campus Task Force report to the Governor of Missouri on campus security and violence prevention included the recommendation that “Each institution should thoroughly evaluate the viability and appropriateness of using assessment tools (e.g. MOSAIC) designed to identify individuals with the potential for violent behavior.\"\n\nThe Superintendent of the L.A. County Office of Education said about MOSAIC: “This is not something where I call you in and give you the third degree. I give myself the third degree because parents want to know why. It just can't be, ‘I don't like your child, I don't like what he does, and I can't explain why.’ This is bringing us to a higher level.\"\n\nIn April 2010, Oprah Winfrey dedicated an hour-long show to applying the MOSAIC method to domestic violence situations. It was announced on the show that Gavin de Becker was making MOSAIC available to anyone at no cost. Tens of thousands of audience members accessed MOSAIC within the first two weeks, and this MOSAIC remains available to anyone today at no cost at www.oprah.com or www.mosaicmethod.com.\n\nIn an article on the method, psychologist Hill Walker, a professor at the University of Oregon who had studied behavioral disorders in schoolchildren for 34 years told Wired Magazine, \"There are some serious validity issues here, some reputation-ruining implications.\" The developers note that Hill Walker has never seen or used a MOSAIC system, and was commenting on the idea or concept, not the actual method. In a letter to the editor following publication of the article, de Becker wrote that MOSAIC for Assessment of Student Threats (MAST) “is the opposite of profiling in that it is always applied to an actual known individual, and it always explores actual behavior and circumstance.\n\nIn an editorial, Professor Laurence Steinberg (who had never seen MOSAIC) questioned the value of the method for predicting violence:\nIn a nation of 90,000 schools, trying to pick out the dozen or so students a year who might commit murder is like looking for a needle in a haystack the size of Kansas \n\nDe Becker responded that MOSAIC for Assessment of Student Threats (MAST) is never applied to the general population of students, and rather just to those students who self-identify by making a threat.\n\nThe NY Times published a letter by de Becker, commenting on Professor Steinberg's editorial.\n\n\n"}
{"id": "2727288", "url": "https://en.wikipedia.org/wiki?curid=2727288", "title": "Multivector", "text": "Multivector\n\nIn multilinear algebra, a multivector, sometimes called Clifford number, is an element of the exterior algebra of a vector space . This algebra is graded, associative and alternating, and consists of linear combinations of simple -vectors (also known as decomposable -vectors or -blades) of the form\nwhere formula_2 are in .\n\n\"Multivector\" may mean either \"homogeneous\" elements (all terms of the linear combination have the same grade or degree , that is are the product of the same number of vectors), which are referred to as -vectors or -vectors, or may allow sums of terms in different degrees.\n\nIn differential geometry, a -vector is the antisymmetric tensor obtained by taking linear combinations of the wedge product of tangent vectors, for some integer . It is the dual concept to a -form.\n\nFor and , these are often called respectively \"scalars\", \"vectors\", \"bivectors\" and \"trivectors\"; they are respectively dual to 0-forms, 1-forms, 2-forms and 3-forms.\n\nThe wedge product operation used to construct multivectors is linear, associative and alternating, which reflect the properties of the determinant. This means for vectors u, v and w in a vector space \"V\" and for scalars \"α\", \"β\", the wedge product has the properties,\n\n\nThe product of \"p\" vectors is called a grade \"p\" multivector, or a \"p\"-vector. The maximum grade of a multivector is the dimension of the vector space \"V\".\n\nThe linearity of the wedge product allows a multivector to be defined as the linear combination of basis multivectors. There are () basis \"p\"-vectors in an \"n\"-dimensional vector space.\n\nThe \"p\"-vector obtained from the wedge product of \"p\" separate vectors in an \"n\"-dimensional space has components that define the projected -volumes of the \"p\"-parallelotope spanned by the vectors. The square root of the sum of the squares of these components defines the volume of the \"p\"-parallelotope.\n\nThe following examples show that a bivector in two dimensions measures the area of a parallelogram, and the magnitude of a bivector in three dimensions also measures the area of a parallelogram. Similarly, a three-vector in three dimensions measures the volume of a parallelepiped.\n\nIt is easy to check that the magnitude of a three-vector in four dimensions measures the volume of the parallelepiped spanned by these vectors.\n\nProperties of multivectors can be seen by considering the two dimensional vector space . Let the basis vectors be e and e, so u and v are given by\n\nand the multivector , also called a bivector, is computed to be\n\nThe vertical bars denote the determinant of the matrix, which is the area of the parallelogram spanned by the vectors u and v. The magnitude of is the area of this parallelogram. Notice that because \"V\" has dimension two the basis bivector is the only multivector in Λ\"V\".\n\nThe relationship between the magnitude of a multivector and the area or volume spanned by the vectors is an important feature in all dimensions. Furthermore, the linear functional version of a multivector that computes this volume is known as a differential form.\n\nMore features of multivectors can be seen by considering the three dimensional vector space . In this case, let the basis vectors be e, e, and e, so u, v and w are given by\n\nand the bivector is computed to be\n\nThe components of this bivector are the same as the components of the cross product. The magnitude of this bivector is the square root of the sum of the squares of its components.\n\nThis shows that the magnitude of the bivector is the area of the parallelogram spanned by the vectors u and v as it lies in the three-dimensional space \"V\". The components of the bivector are the projected areas of the parallelogram on each of the three coordinate planes.\n\nNotice that because \"V\" has dimension three, there is one basis three-vector in Λ\"V\". Compute the three-vector\nThis shows that the magnitude of the three-vector is the volume of the parallelepiped spanned by the three vectors u, v and w.\n\nIn higher-dimensional spaces, the component three-vectors are projections of the volume of a parallelepiped onto the coordinate three-spaces, and the magnitude of the three-vector is the volume of the parallelepiped as it sits in the higher-dimensional space.\n\nIn this section, we consider multivectors on a projective space \"P\", which provide a convenient set of coordinates for lines, planes and hyperplanes that have properties similar to the homogeneous coordinates of points, called Grassmann coordinates.\n\nPoints in a real projective space \"P\" are defined to be lines through the origin of the vector space R. For example, the projective plane \"P\" is the set of lines through the origin of R. Thus, multivectors defined on R can be viewed as multivectors on \"P\".\n\nA convenient way to view a multivector on \"P\" is to examine it in an affine component of \"P\", which is the intersection of the lines through the origin of R with a selected hyperplane, such as . Lines through the origin of R intersect the plane to define an affine version of the projective plane that only lacks the points , called the points at infinity.\n\nPoints in the affine component of the projective plane have coordinates . A linear combination of two points and defines a plane in R that intersects E in the line joining p and q. The multivector defines a parallelogram in R given by\nNotice that substitution of for p multiplies this multivector by a constant. Therefore, the components of are homogeneous coordinates for the plane through the origin of R.\n\nThe set of points on the line through p and q is the intersection of the plane defined by with the plane . These points satisfy , that is,\n\nwhich simplifies to the equation of a line\n\nThis equation is satisfied by points for real values of α and β.\n\nThe three components of that define the line \"λ\" are called the Grassmann coordinates of the line. Because three homogeneous coordinates define both a point and a line, the geometry of points is said to be dual to the geometry of lines in the projective plane. This is called the principle of duality.\n\nThree dimensional projective space, \"P\" consists of all lines through the origin of R. Let the three dimensional hyperplane, , be the affine component of projective space defined by the points . The multivector defines a parallelepiped in R given by\n\nNotice that substitution of for p multiplies this multivector by a constant. Therefore, the components of are homogeneous coordinates for the 3-space through the origin of R.\n\nA plane in the affine component is the set of points in the intersection of H with the 3-space defined by . These points satisfy , that is,\n\nwhich simplifies to the equation of a plane\n\nThis equation is satisfied by points for real values of \"α\", \"β\" and \"γ\".\n\nThe four components of that define the plane \"λ\" are called the Grassmann coordinates of the plane. Because four homogeneous coordinates define both a point and a plane in projective space, the geometry of points is dual to the geometry of planes.\n\nA line as the join of two points: In projective space the line \"λ\" through two points p and q can be viewed as the intersection of the affine space with the plane in R. The multivector provides homogeneous coordinates for the line\n\nThese are known as the Plücker coordinates of the line, though they are also an example of Grassmann coordinates.\n\nA line as the intersection of two planes: A line \"μ\" in projective space can also be defined as the set of points x that form the intersection of two planes \"π\" and \"ρ\" defined by grade three multivectors, so the points x are the solutions to the linear equations\n\nIn order to obtain the Plucker coordinates of the line \"μ\", map the multivectors \"π\" and \"ρ\" to their dual point coordinates using the Hodge star operator,\n\nthen\n\nSo, the Plücker coordinates of the line \"μ\" are given by\n\nBecause the six homogeneous coordinates of a line can be obtained from the join of two points or the intersection of two planes, the line is said to be self dual in projective space.\n\nW. K. Clifford combined multivectors with the inner product defined on the vector space, in order to obtain a general construction for hypercomplex numbers that includes the usual complex numbers and Hamilton's quaternions.\n\nThe Clifford product between two vectors u and v is linear and associative like the wedge product, and has the additional property that the multivector uv is coupled to the inner product by Clifford's relation,\n\nClifford's relation preserves the alternating property for the product of vectors that are perpendicular. This can be seen for the orthogonal unit vectors in R. Clifford's relation yields\n\ntherefore the basis vectors are alternating,\n\nIn contrast to the wedge product, the Clifford product of a vector with itself is no longer zero. To see this compute the product,\n\nwhich yields\n\nThe set of multivectors constructed using Clifford's product yields an associative algebra known as a Clifford algebra. Inner products with different properties can be used to construct different Clifford algebras.\n\nMultivectors play a central role in the mathematical formulation of physics known as geometric algebra. The term \"geometric algebra\" was used by E. Artin for matrix methods in projective geometry. It was D. Hestenes who used \"geometric algebra\" to describe the application of Clifford algebras to classical mechanics, This formulation was expanded to \"geometric calculus\" by D. Hestenes and G. Sobczyk, who provided new terminology for a variety of features in this application of Clifford algebra to physics. C. Doran and A. Lasenby show that Hestene's geometric algebra provides a convenient formulation for modern physics.\n\nIn geometric algebra, a multivector is defined to be the sum of different-grade \"k\"-blades, such as the summation of a scalar, a vector, and a \"2\"-vector. A sum of only \"k\"-grade components is called a \"k\"-vector, or a \"homogeneous\" multivector.\n\nThe highest grade element in a space is called a \"pseudoscalar\".\n\nIf a given element is homogeneous of a grade \"k\", then it is a \"k\"-vector, but not necessarily a \"k\"-blade. Such an element is a \"k\"-blade when it can be expressed as the wedge product of \"k\" vectors. A geometric algebra generated by a 4-dimensional Euclidean vector space illustrates the point with an example: The sum of any two blades with one taken from the XY-plane and the other taken from the ZW-plane will form a 2-vector that is not a 2-blade. In a geometric algebra generated by a Euclidean vector space of dimension 2 or 3, all sums of 2-blades may be written as a single 2-blade.\n\n\nIn the presence of a volume form (such as given an inner product and an orientation), pseudovectors and pseudoscalars can be identified with vectors and scalars, which is routine in vector calculus, but without a volume form this cannot be done without a choice.\n\nIn the algebra of physical space (the geometric algebra of Euclidean 3-space, used as a model of (3+1)-spacetime), a sum of a scalar and a vector is called a paravector, and represents a point in spacetime (the vector the space, the scalar the time).\n\nA bivector is therefore an element of the antisymmetric tensor product of a tangent space with itself.\n\nIn geometric algebra, also, a bivector is a grade 2 element (a 2-vector) resulting from the wedge product of two vectors, and so it is geometrically an \"oriented area\", in the same way a \"vector\" is an oriented line segment. If a and b are two vectors, the bivector has\n\nBivectors are connected to pseudovectors, and are used to represent rotations in geometric algebra.\n\nAs bivectors are elements of a vector space Λ\"V\" (where \"V\" is a finite-dimensional vector space with ), it makes sense to define an inner product on this vector space as follows. First, write any element in terms of a basis as\n\nwhere the Einstein summation convention is being used.\n\nNow define a map by insisting that\n\nwhere formula_31 are a set of numbers.\n\nBivectors play many important roles in physics, for example, in the classification of electromagnetic fields.\n\n"}
{"id": "21999", "url": "https://en.wikipedia.org/wiki?curid=21999", "title": "Nomenklatura", "text": "Nomenklatura\n\nThe nomenklatura (; ) were a category of people within the Soviet Union and other Eastern Bloc countries who held various key administrative positions in the bureaucracy, running all spheres of those countries' activity: government, industry, agriculture, education, etc., whose positions were granted only with approval by the communist party of each country or region.\n\nVirtually all members of the nomenklatura were members of the Communist Party. Critics of Stalin, such as Milovan Đilas, critically defined them as a \"new class\". Trotsky used the term \"caste\" rather than \"class\", because he saw the Soviet Union as a degenerated workers' state, not a new class society. Later developments of Trotsky's theories, such as Tony Cliff's theory of state capitalism, did refer to the nomenklatura as a new \"class\". Richard Pipes, an anti-communist writer, claimed that this system mainly reflected a continuation of the old Tsarist regime, as many former Tsarist officials or \"careerists\" joined the Bolshevik government during and after the Russian Civil War.\n\nThe \"nomenklatura\" forming a \"de facto\" elite of public powers in the previous Eastern Bloc, may be compared to the western \"establishment\" holding or controlling both private and public powers (e.g., media, finance, trade, industry, state and institutions).\n\nThe Russian term is derived from the Latin \"nomenclatura\", meaning a list of names.\n\nThe term was popularized in the West by the Soviet dissident Michael Voslenski, who in 1970 wrote a book titled \"Nomenklatura: The Soviet Ruling Class\" ().\n\nThe nomenklatura referred to the Communist Party's governance to make appointments to key positions throughout the governmental system, as well as throughout the party's own hierarchy. Specifically, the nomenklatura consisted of two separate lists: one was for key positions, appointments to which were made by authorities within the party; the other was for persons who were potential candidates for appointment to those positions. The Politburo, as part of its nomenklatura authority, maintained a list of ministerial and ambassadorial positions that it had the power to fill, as well as a separate list of potential candidates to occupy those positions.\n\nCoextensive with the nomenklatura were patron-client relations. Officials who had the authority to appoint individuals to certain positions cultivated loyalties among those whom they appointed. The patron (the official making the appointment) promoted the interests of clients in return for their support. Powerful patrons, such as the members of the Politburo, had many clients. Moreover, an official could be both a client (in relation to a higher-level patron) and a patron (to other, lower-level officials).\n\nBecause a client was beholden to his patron for his position, the client was eager to please his patron by carrying out his policies. The Soviet power structure essentially consisted (according to its critics) of groups of vassals (clients) who had an overlord (the patron). The higher the patron, the more clients the patron had. Patrons protected their clients and tried to promote their careers. In return for the patron's efforts to promote their careers, the clients remained loyal to their patron. Thus, by promoting his clients' careers, the patron could advance his own power.\n\nThe nomenklatura system arose early in Soviet history. Vladimir Lenin wrote that appointments were to take the following criteria into account: reliability, political attitude, qualifications, and administrative ability. Joseph Stalin, who was the first general secretary of the party, also was known as \"Comrade File Cabinet\" (Tovarishch Kartotekov) for his assiduous attention to the details of the party's appointments. Seeking to make appointments in a more systematic fashion, Stalin built the party's patronage system and used it to distribute his clients throughout the party bureaucracy.\n\nUnder Stalin's direction in 1922, the party created departments of the Central Committee and other organs at lower levels that were responsible for the registration and appointment of party officials. Known as uchraspred, these organs supervised appointments to important party posts. According to American sovietologist Seweryn Bialer, after Leonid Brezhnev's accession to power in October 1964, the party considerably expanded its appointment authority. However, in the late 1980s some official statements indicated that the party intended to reduce its appointment authority, particularly in the area of economic management, in line with Mikhail Gorbachev's reform efforts.\n\nAt the all-union level, the Party Building and Cadre Work Department supervised party nomenklatura appointments. This department maintained records on party members throughout the country, made appointments to positions on the all-union level, and approved nomenklatura appointments on the lower levels of the hierarchy. The head of this department sometimes was a member of the Secretariat and was often a protégé of the general secretary.\n\nEvery party committee and party organizational department, from the all-union level in Moscow to the district and city levels, prepared two lists according to their needs. The basic (osnovnoi) list detailed positions in the political, administrative, economic, military, cultural, and educational bureaucracies that the committee and its department had responsibility for filling. The registered (uchetnyi) list enumerated the persons suitable for these positions.\n\nAn official in the party or government bureaucracy could not advance in the nomenklatura without the assistance of a patron. In return for this assistance in promoting his career, the client carried out the policies of the patron. Patron–client relations thus help to explain the ability of party leaders to generate widespread support for their policies. The presence of patron–client relations between party officials and officials in other bureaucracies also helped to account for the large-scale control the party exercised over the Soviet society. All of the 2 million members of the nomenklatura system understood that they held their positions only as a result of a favor bestowed on them by a superior official in the party and that they could easily be replaced if they manifested disloyalty to their patron. Self-interest dictated that members of the nomenklatura submit to the control of their patrons in the party.\n\nClients sometimes could attempt to supplant their patron. For example, Nikita Khrushchev, one of Lazar M. Kaganovich's former protégés, helped to oust the latter in 1957. Seven years later, Leonid Brezhnev, a client of Khrushchev, helped to remove his boss from power. The power of the general secretary was consolidated to the extent that he placed his clients in positions of power and influence. The ideal for the general secretary, writes Soviet émigré observer Michael Voslensky, \"is to be overlord of vassals selected by oneself.\"\n\nSeveral factors explain the entrenchment of patron–client relations. Firstly, in a centralized government system, promotion in the bureaucratic-political hierarchy was the only path to power. Secondly, the most important criterion for promotion in this hierarchy was approval from one's supervisors, who evaluated their subordinates on the basis of political criteria and their ability to contribute to the fulfillment of the economic plan. Thirdly, political rivalries were present at all levels of the party and state bureaucracies but were especially prevalent at the top. Power and influence decided the outcomes of these struggles, and the number and positions of one's clients were critical components of that power and influence. Fourthly, because fulfillment of the economic plan was decisive, systemic pressures led officials to conspire together and use their ties to achieve that goal.\n\nThe faction led by Brezhnev provides a good case study of patron–client relations in the Soviet system. Many members of the Brezhnev faction came from Dnipropetrovsk, where Brezhnev had served as first secretary of the provincial party organization. Andrei P. Kirilenko, a Politburo member and Central Committee secretary under Brezhnev, was first secretary of the regional committee of Dnipropetrovsk. Volodymyr Shcherbytsky, named as first secretary of the Ukrainian apparatus under Brezhnev, succeeded Kirilenko in that position. Nikolai Alexandrovich Tikhonov, appointed by Brezhnev as first deputy chairman of the Soviet Union's Council of Ministers, graduated from the Dnipropetrovsk Metallurgical Institute, and presided over the economic council of Dnipropetrovsk Oblast. Finally, Nikolai Shchelokov, minister of internal affairs under Brezhnev, was a former chairman of the Dnipropetrovsk soviet.\n\nPatron–client relations had implications for policy making in the party and government bureaucracies. Promotion of trusted subordinates into influential positions facilitated policy formation and policy execution. A network of clients helped to ensure that a patron's policies could be carried out. In addition, patrons relied on their clients to provide an accurate flow of information on events throughout the country. This information assisted policymakers in ensuring that their programs were being implemented.\n\nMilovan Đilas, a critic of Stalin, wrote of the nomenklatura as the \"new class\" in his book \"\", and he claimed that it was seen by ordinary citizens as a bureaucratic elite that enjoyed special privileges and had supplanted the earlier wealthy capitalist élites.\n\nSome Marxists, such as Ernest Mandel, have criticised Đilas and the theory of state capitalism: The hypothesis that the Soviet bureaucracy is a new ruling class does not correspond to a serious analysis of the real development and the real contradictions of Soviet society and economy in the last fifty years. Such a hypothesis must imply, from the point of view of historical materialism, that a new exploitative mode of production has emerged in that country. If this were so, we would be confronted, for the first time in history, with a \"ruling class\" whose general behavior and private interests (which of course dictate that behavior) run counter to the needs and inner logic of the existing socio-economic system. Indeed, one of the main characteristics of the Soviet economy is the impossibility of reconciling the needs of planning, of optimizing economic growth (not from an \"absolute\" point of view, but from within the logic of the system itself) with the material self-interest of the bureaucracy.\n\n\n"}
{"id": "38195799", "url": "https://en.wikipedia.org/wiki?curid=38195799", "title": "People's Uprising rally, 2013", "text": "People's Uprising rally, 2013\n\nThe People's Uprising rally or Himpunan Kebangkitan Rakyat (Malay) was a rally that was held in Kuala Lumpur, Malaysia on 12 January 2013. The rally was held by various Malaysian opposition friendly non-governmental organisations and opposition parties in opposition to many of the government policies and decisions that have been claimed by left wing supporters to be unfair and affecting the Malaysian populace. The main venue which the organisers have chosen for the rally is Stadium Merdeka. The event was also known as the KL 112 rally, where the numbers indicate the date of the event.\n\nAccording to social activist and icon Hishamuddin Rais, the grievances against the government are many and varied The following lists of grouses:\n\nThe election watchdog group Bersih which organised a number of rallies calling for electoral reform in Malaysia namely the rallies in the Bersih rallies in 2011 and 2012 has revealed that it is not amongst the organisers of the latest rally but has said that many of members will be attending the rally on their own capacity.\n\nThe opposition coalition Pakatan Rakyat has made a 10-point declaration:\n\nThe police have advised the organisers of the rally to hold the rally in Bukit Jalil stadium instead of having it in Stadium Merdeka.\n\nIn anticipation of the protests the government began the closure of Dataran Merdeka to prevent protesters from gathering at that location.\n\nThe government has also warned against civil servants from attending the rally.\n\nVarious public universities in Malaysia echoed the government's warning telling students and teachers not to attend the rally.\n\nThe government-controlled mainstream media gave scant coverage on the rally.\n\nThis is the chronology of the people's uprising rally 2013 or (KL112) that was held in Stadium Merdeka, Kuala Lumpur, Saturday 12 January 2013. \"(Chronology sorted from down to upward)\"\n\nThe Malaysian human rights watchdog Suhakam and the Malaysian Bar Council will be monitoring the rally. The Bar Council will be sending in a team of lawyers to anyone who needs legal assistances should they be arrested by the police. Former Bar Council president K Ragunath criticised the Bar Council for showing a tendency to support the opposition.\n\nThere were fears by a Malaysian NGO Solidariti Anak Muda Malaysia (SAMM) that there would be some provocations by the police during the demonstration. After the rally ended, the PDRM or the Malaysian Royal Police was praised as they shown tremendous amount of patience and restraint.\n\nAccording to different estimates there were about 100,000 to 150,000 people who attended the rally, which was quite peaceful compared the previous rallies. Many of the rally goers gathered at various points of the city before making their way to Stadium Merdeka where the rally organisers conducted a number of speeches in support of their ideals.\n\nThe police was commended for the way they handled the rally by both government as well as opposition leaders and from the general public.\n\n"}
{"id": "1694884", "url": "https://en.wikipedia.org/wiki?curid=1694884", "title": "Planar algebra", "text": "Planar algebra\n\nIn mathematics, planar algebras first appeared in the work of Vaughan Jones on the standard invariant of a II subfactor.\n\nThey also provide an appropriate algebraic framework for many knot invariants (in particular the Jones polynomial), and have been used in describing the properties of Khovanov homology with respect to tangle composition. Any subfactor planar algebra provides a family of unitary representations of Thompson groups.\n\nAny finite group (and quantum generalization) can be encoded as a planar algebra.\n\nThe idea of the planar algebra is to be a diagrammatic axiomatization of the standard invariant.\n\nA (shaded) planar tangle is the data of finitely many \"input\" disks, one \"output\" disk, non-intersecting strings giving an even number, say formula_1, intervals per disk and one formula_2-marked interval per disk. \n\nHere, the mark is shown as a formula_2-shape. On each input disk it is placed between two adjacent outgoing strings, and on the output disk it is placed between two adjacent incoming strings. A planar tangle is defined up to isotopy.\n\nTo \"compose\" two planar tangles, put the output disk of one into an input of the other, having as many intervals, same shading of marked intervals and such that the formula_2-marked intervals coincide. Finally we remove the coinciding circles. Note that two planar tangles can have zero, one or several possible compositions.\n\nThe planar operad is the set of all the planar tangles (up to isomorphism) with such compositions.\n\nA planar algebra is a \"representation\" of the planar operad; more precisely, it is a family of vector spaces formula_5, called formula_6-box spaces, on which \"acts\" the planar operad, i.e. for any tangle formula_7 (with one output disk and formula_8 input disks with formula_9 and formula_10 intervals respectively) there is a multilinear map\n\nwith formula_12 according to the shading of the formula_13-marked intervals, and these maps (also called partition functions) respect the composition of tangle in such a way that all the diagrams as below commute.\n\nThe family of vector spaces formula_14 generated by the planar tangles having formula_1 intervals on their \"output\" disk and a white (or black) formula_16-marked interval, admits a planar algebra structure.\n\nThe Temperley-Lieb-Jones planar algebra formula_17 is generated by the planar tangles without input disk; its formula_18-box space formula_19 is generated by\n\nMoreover, a closed string is replaced by a multiplication by formula_20.\n\nNote that the dimension of formula_21 is the Catalan number formula_22.\nThis planar algebra encodes the notion of Temperley–Lieb algebra.\n\nA semisimple and cosemisimple Hopf algebra over an algebraically closed field is encoded in a planar algebra defined by generators and relations.\n\nA subfactor planar algebra is a planar formula_2-algebra formula_5 which is: \n\nNote that by (2) and (3), any closed string (shaded or not) counts for the same constant formula_20.\n\nThe tangle action deals with the adjoint by: \n\nwith formula_31 the mirror image of formula_32 and formula_33 the adjoint of formula_34 in formula_35.\n\n\"No-ghost theorem\": The planar algebra formula_36 has no ghost (i.e. element formula_37 with formula_38) if and only if \nFor formula_20 as above, and after a quotient by the null ideal formula_41 (generated by elements formula_37 with formula_43), we get a subfactor planar algebra. Any subfactor planar algebra with constant formula_20 admits formula_45 as planar subalgebra.\n\nA planar algebra formula_46 is a subfactor planar algebra if and only if it is the standard invariant of an extremal subfactor formula_47 of index formula_48, with formula_49 and formula_50.\nA finite depth or irreducible subfactor is extremal (formula_51 on formula_52). \nThere is a subfactor planar algebra encoding any finite group (and any finite dimensional Hopf formula_53-algebra, called Kac algebra), defined by generators and relations.\n\nThe subfactor planar algebra associated to an inclusion of finite groups,\ndoes not always remember the (core-free) inclusion.\n\nA Bisch-Jones planar algebra formula_54, initially called Fuss-Catalan, can be defined as for formula_36 but by allowing two colors of string with their own constant formula_56 and formula_57. For formula_58 as above, and after the null quotient, it is a planar subalgebra of any subfactor planar algebra with an intermediate such that formula_59 and formula_60. \n\nThe first finite depth subfactor planar algebra of index formula_61 is called the Haagerup subfactor planar algebra.\n\nThe subfactor planar algebras are completely classified for index at most formula_63 \n\nand a bit beyond.\n\nThis classification was initiated by Uffe Haagerup.\n\nIt uses (among other things) a listing of possible principal graphs, together with the embedding theorem\n\nand the jellyfish algorithm.\n\nA subfactor planar algebra remembers the subfactor (i.e. its standard invariant is complete) if it is amenable.\nA finite depth hyperfinite subfactor is amenable. \n\nAbout the non-amenable case: there are unclassifiably many irreducible hyperfinite subfactors of index 6 that all have the same standard invariant.\nLet formula_64 be a finite index subfactor, and formula_65 the corresponding subfactor planar algebra. Assume that formula_65 is irreducible (i.e. formula_67). Let formula_68 be an intermediate subfactor. Let the Jones projection formula_69. Note that formula_70. Let formula_71 and formula_72.\n\nNote that formula_73 and formula_74.\n\nLet the bijective linear map formula_75 be the Fourier transform, also called formula_76-click (of the outer star) or formula_77 rotation; and let formula_78 be the coproduct of formula_37 and formula_80.\n\nNote that the word \"coproduct\" is a diminutive of \"convolution product\". It is a binary operation.\n\nThe coproduct satisfies the equality formula_81\n\nFor any positive operators formula_82, the coproduct formula_83 is also positive; this can be seen diagrammatically:\n\nLet formula_84 be the contragredient formula_85 (also called formula_86 rotation). The map formula_87 corresponds to four formula_76-clicks of the outer star, so it's the identity map, and then formula_89.\n\nIn the Kac algebra case, the contragredient is exactly the antipode, which, for a finite group, correspond to the inverse.\n\nA biprojection is a projection formula_90 with formula_91 a multiple of a projection. \nNote that formula_92 and formula_93 are biprojections; this can be seen as follows:\n\nA projection formula_94 is a biprojection iff it is the Jones projection formula_95 of an intermediate subfactor formula_96\n\n, iff formula_97.\n\nGalois correspondence:\n\nFor any irreducible subfactor planar algebra, the set of biprojections is a finite lattice, of the form formula_98, as for an interval of finite groups formula_99.\n\nUsing the biprojections, we can make the intermediate subfactor planar algebras. \n\nThe uncertainty principle extends to any irreducible subfactor planar algebra formula_100:\n\nLet formula_101 with formula_102 the range projection of formula_103 and formula_104 the unnormalized trace (i.e. formula_105 on formula_106).\n\nNoncommutative uncertainty principle: \nAssuming formula_103 and formula_110 positive, the equality holds if and only if formula_103 is a biprojection. More generally, the equality holds if and only if formula_103 is the \"bi-shift\" of a biprojection.\n"}
{"id": "59090739", "url": "https://en.wikipedia.org/wiki?curid=59090739", "title": "Punished by Rewards", "text": "Punished by Rewards\n\nPunished by Rewards: The Trouble with Gold Stars, Incentive Plans, A's, Praise, and Other Bribes is a 1993 book by Alfie Kohn that argues against the use of rewards to incentivize behavior.\n\n"}
{"id": "34872093", "url": "https://en.wikipedia.org/wiki?curid=34872093", "title": "Relationship-contingent self-esteem", "text": "Relationship-contingent self-esteem\n\nRelationship contingent self-esteem (RCSE) is a type of self-esteem that derives from the outcomes, process, and nature of one’s romantic relationship. Like other types of contingent self-esteem, it is generally linked with lower levels of self-esteem and well-being.\nIt can be unhealthy for the relationship because it paves the way for excessive bias for negative interpretations of relationship events. Past research has shown that relationship-contingent self-esteem is independent of feelings of commitment to one’s relationship, closeness to one’s partner, and satisfaction in the relationship. Also, this research showed that it was linked to “obsessive immersion or preoccupation” with the romantic relationship.\n\nPast research has measured RCSE with a psychological scale consisting of 11 items. The scale contains two related sub-scales: the general Contingent Self-Esteem Scale and the Contingencies of Self-Worth Scale. The internal consistency of the scale is high, as is the two-week test-retest reliability.\n\nLike other types of contingent self-esteem, RCSE is generally linked with lower levels of self-esteem and well-being. It is also associated with excessive reassurance seeking behavior, preoccupied attachment style, and insecure attachment style. Ironically, these styles of attachment do not allow the person to attain the relationship security that they seek. For example, displaying excessive reassurance seeking behavior from one’s partner can be a source of discord and strain on the relationship. In addition, those with insecure attachment styles are less able to seek support and care giving in effective ways from their partners.\n\nThose who are high in RCSE are often high in rejection sensitivity. High rejection sensitivity is the tendency to anxiously expect rejection from one’s significant other.\nThose who are high in rejection sensitivity act much more negatively in a discussion about relationship conflict with their significant others than do those who are low in rejection sensitivity. In turn, this may cause the highly rejection sensitive individual’s partners to feel angrier after a discussion about conflict than do partners of individuals low in rejection sensitivity.\n\nWhen self-esteem is contingent upon an external domain of life, in this case, the relationship, it will motivate a person to pursue short-term and long-term goals that enhance and promote that domain (i.e. the relationship). People with RCSE will want to prove that they are a success in their relationship because it will validate their sense of self.\n\nOne example of the unhealthy nature of relationship-contingent self-esteem is the link between RCSE and greater approval sex motives. Sexual motivation may involve intimacy motives (i.e. the drive to create further intimacy or closeness) or approval sex motives (i.e. the drive to avoid disapproval from one’s partner about frequency or quality of sex). Since theories about relationship contingent self-esteem posit that individuals who derive their self-esteem based on relationship outcomes may be more motivated than others to avoid negative outcomes and increase positive outcomes, it follows that these motives may apply in the sexual motivation arena. A study by Sanchez and colleagues investigated the relationship between relationship contingent self-worth, approval sex motives, intimacy motives, sexual autonomy, and sexual satisfaction among women in committed relationships.\n\nRCSE may lead to low levels of autonomy by causing a person in a relationship to cater to the other person’s needs or the needs of the relationship at the expense of the needs of his or her self. For example, if RCSE affects sexual motives by shifting the focus from achieving intimacy to garnering approval from one’s partner, this may lower sexual autonomy and satisfaction.\n\nIn general, when people are extremely motivated to protect or enhance self-esteem, they are more susceptible to stress or anxiety because failure to do so results in a loss of self-esteem. Thus, for a person whose self-esteem is contingent upon relationship success, they will be motivated to maintain and enhance the relationship in order to protect their own self-esteem. This may result in losing the sense of autonomy, or the sense that one is the originator of one’s own behavior and is doing things because one wants to rather than because one has to. A person who views losing the relationship as a threat to oneself may act out of fear rather than confidence and self-expression, thereby losing a sense of autonomy.\n\nRCSE is thought to be unhealthy for the relationship because it paves the way for excessive bias for negative interpretations of relationship events. A negative interpretive bias will in turn affect a person’s behavior toward his or her partner. For example, since RCSE is often associated with having high levels of rejection sensitivity, the negative effects of rejection sensitivity will often damage the relationships of those with highly relationship contingent self-esteem. A daily diary study of members of committed romantic couple’s thoughts and moods revealed that partners of highly rejection sensitive women showed notable upsurges in relationship dissatisfaction and thoughts of ending their relationships. In addition, being a person who is high in rejection sensitivity in a relationship predicted breakup within a year. When RCSE causes greater approval sex motives which in turn diminish a feeling of genuine sexual satisfaction, it may have negative consequences on the relationship by reducing sexual autonomy and satisfaction. This has implications not just for the individual suffering from it directly but probably also for his or her partner, who is likely to sense his or her partner’s levels of sexual autonomy and satisfaction.\nHaving RCSE can make a person preoccupied with his or her own perceptions of the events in the relationship and perceptions of his or her partner’s behavior at the expense of considering the partner’s perceptions and experience. Excessive rumination and preoccupation with the state of the relationship because of a sense of pressure to keep up the relationship may eat up cognitive and emotional resources, which in turn may diminish the capacity empathize with the partner’s experience. As a result, partners of individuals with RCSE may begin to distrust these individuals’ motivations and drives within the relationship.\n\nSince having self-worth that is contingent upon the success of one’s relationship will motivate one to pursue success in order to preserve self-esteem, some researchers believe that the anxiety and stress associated with such pursuits will lead to long-term physical and mental health problems. Long term anxiety and stress can activate the pituitary-adrenal-cortical system, which can in turn lead to increases in cholesterol and triglycerides in the blood stream. This in turn elevates the risk for heart disease. Another pathway to physical health problems is through unhealthy attempts at coping. For example, college students whose appearance and image contribute to their level of self-esteem may engage in unhealthy behaviors such as binge drinking and excessive partying. In addition, past research has shown that people who are more concerned with how others evaluate and perceive them tend to diet excessively, smoke, undergo cosmetic surgery, and consume more alcohol. In the realm of mental health, self-esteem that is highly unstable can contribute to higher levels of depressive symptoms.\n\nThe healthy alternative to RCSE is to develop non-contingent self-esteem. This approach would require those in relationships to abandon external outcomes as a source for their own self-worth. By doing so, it would free up their energies to relate on a mindful level to their relationship partner. By shifting one’s source of self-worth from approval from others to compassion towards others, for instance, minor setbacks in a relationship will not be so alarming because it will not threaten the sense of self.\n\nAlternatively, one can nurture self-compassion instead of self-esteem. Self-compassion researcher Professor Kristin Neff describes self-compassion as a combination of nurturing self-kindness over self-judgment, a sense of humanity over isolation, and a state of mindfulness rather than over-identification. A person experiencing distress who exercises self-compassion would be gentle towards him or herself rather than be harsh and critical and would recognize that he or she is not alone- rather, all people suffer at some point or another. This approach, Professor Neff and colleagues have found, can be beneficial for romantic relationships. In their study, self-compassion, not self-esteem, was significantly linked with greater relationship satisfaction. In addition, the presence of self-compassion predicted more positive relationship behaviors and less negative behaviors.\n\nResearch on RCSE has only been conducted in American samples. In collectivist cultures, life satisfaction is often a result of living in harmony with the community and within one’s relationships with others rather than the attainment of high self-esteem. Therefore, it is unclear whether there is a difference in prevalence of RCSE among different nations and cultures. Some researchers believe that the pursuit of a self-esteem, which is a hallmark consequence of all types of contingent self-esteem, is a fundamentally American phenomenon. The Protestant Ethic and ideas of self-reliance and meritocracy may lead Americans to believe that self-worth must be earned by performance and attainment, and that some people are worthier than others.\n\nRCSE, while a construct still in its infancy in psychology research, is an important model for psychology because it challenges and expands upon existing notions of self-esteem. First of all, RCSE distinguishes between levels of state self-esteem and trait self-esteem. Whereas trait self-esteem seems to be relatively stable over time, state self-esteem can fluctuate in relatively short periods of time depending on circumstances, successes, and failures. RCSE affects the state levels of self-esteem by boosting self-esteem if the relationship is going well, but diminishing it when it is not. In addition, the construct of RCSE challenges existing theory by suggesting that self-esteem does not exist in a vacuum, but that it is contingent upon success in one or more domains. Those domains may be internal or external. RCSE is an example of an contingency on an external domain, namely that of romantic relationships.\n\nFuture research can investigate whether RCSE differs among cultures and contexts and whether it has harmful consequences in all cultures. In addition, more research can examine whether there are gender differences in relationship contingent self-esteem, and if so, what the nature of those differences are. Research on RCSE’s developmental trajectory can illuminate the factors that contribute to shaping it. Further research on negative implications of RCSE can inform interventions and therapies designed to steer people away from RCSE and towards more healthy alternatives, such as self-compassion or non-contingent self-esteem.\n"}
{"id": "10982859", "url": "https://en.wikipedia.org/wiki?curid=10982859", "title": "Richard B. Moore", "text": "Richard B. Moore\n\nRichard Benjamin Moore (9 August 1893 – 1978) was a Barbados-born African-Caribbean civil rights activist and prominent Socialist. He was also one of the earlier advocates of the term African American as opposed to Negro, or \"black\".\n\nRichard Benjamin Moore was a Barbadian writer born on 9 August 1893 in Barbados, West Indies, to Richard Henry Moore and Josephine Thorne Moore. In Barbados, the Richard Henry and Josephine Moore family was considered middle-class in terms of socioeconomic status. Richard Henry Moore was the moneymaker of the family, working as a preacher and building contractor in Barbados. Unfortunately, tragedy struck when Richard B. Moore’s mother died when he was three years old. Moore’s father later remarried, to Elizabeth Mclean. Soon thereafter, Moore’s father died in 1902 when young Richard was aged nine. With both biological parents dead, Moore was raised by his stepmother Elizabeth Mclean.\n\nMclean wanted to carry out Richard senior’s wishes of giving young Richard the best education. It is for this reason that Mclean aided young Richard in traveling to the United States. In hopes of furthering his education, Moore migrated to The United States of America and arrived in New York City on 4 July 1909. However, Moore would not become a naturalised citizen until 11 September 1924. Although African Americans were free in the United States, they were far from being treated equal to European-Americans in America. Moore was immediately faced with ethnic discrimination when it came to employment and educational opportunities among other things. Although trained in Barbados to do clerical work, he was forced to turn to the more unfavourable jobs such as elevator operator and work in a silk manufacturing firm.\n\nDue to the struggles that Moore encountered and observed, he became a strong vocalist for the rights of African Americans. In 1919 he joined the African Blood Brotherhood (ABB), which was an organization formed to defend African Americans from race riots and lynching. Moore, along with other African-American advocates, joined the Socialist Party in the early 1920s. Moore joined the Socialist Party, in part, because at the time the Socialist party was transforming itself into a force to fight against segregation.\n\nMoore was a frequent political candidate of the Socialist Party. In 1928 he ran for U.S. Congress in New York's 21st Congressional District. In 1934, Moore ran on the Socialist ticket for Chief Judge of the New York Court of Appeals. In 1935, he became the organiser for the International Labor Defense in the New England Territory. He used his position in this organisation to speak on behalf the Scottsboro Boys, a case in which nine young African-American males were accused of raping two young European-American women. \n\nIn 1942, Moore was expelled from the Socialist Party because he was accused of being an African-American nationalist; keeping African-American issues on the front burner.\n\nHe continued his efforts for equal rights in America. He also played a leading role in Caribbean advocacy groups. Moore, like his friend Hubert Harrison, was a bibliophile, collecting over 15,000 books and pamphlets on the African-American experiences worldwide. This collection of books is currently housed in a library that Moore developed in Barbados. Moore also ran the Frederick Douglass Book Center in Harlem. \n\nMoore wrote a few books himself, including \"The Name Negro, Its Origin and Evil Use\" (1960) and \"Caribs, Cannibals and Human Relations\" (1972). He also had essays and articles published in various magazines and journals including the \"Negro Champion\", \"Daily Worker\", and \"Freedomways\".\n\nRichard Benjamin Moore died in his homeland of Barbados in 1978 at the age of 85.\n\nJoyce Moore Turner and W. Burghart Turner, \"Richard B. Moore: Caribbean Militant in Harlem.\" Bloomington: Indiana University Press, 1988.\nJoyce Moore Turner, \"Caribbean Crusaders and the Harlem Renaissance.\" Urbana: Illinois Press, 2005.\n\n"}
{"id": "1810137", "url": "https://en.wikipedia.org/wiki?curid=1810137", "title": "Sequence diagram", "text": "Sequence diagram\n\nA sequence diagram shows object interactions arranged in time sequence. It depicts the objects and classes involved in the scenario and the sequence of messages exchanged between the objects needed to carry out the functionality of the scenario. Sequence diagrams are typically associated with use case realizations in the Logical View of the system under development. Sequence diagrams are sometimes called event diagrams or event scenarios.\n\nA sequence diagram shows, as parallel vertical lines (\"lifelines\"), different processes or objects that live simultaneously, and, as horizontal arrows, the messages exchanged between them, in the order in which they occur. This allows the specification of simple runtime scenarios in a graphical manner.\n\nIf the lifeline is that of an object, it demonstrates a role. Leaving the instance name blank can represent anonymous and unnamed instances.\n\nMessages, written with horizontal arrows with the message name written above them, display interaction. Solid arrow heads represent synchronous calls, open arrow heads represent asynchronous messages, and dashed lines represent reply messages.\nIf a caller sends a synchronous message, it must wait until the message is done, such as invoking a subroutine. If a caller sends an asynchronous message, it can continue processing and doesn’t have to wait for a response. Asynchronous calls are present in multithreaded applications, event-driven applications and in message-oriented middleware. \nActivation boxes, or method-call boxes, are opaque rectangles drawn on top of lifelines to represent that processes are being performed in response to the message (ExecutionSpecifications in UML).\n\nObjects calling methods on themselves use messages and add new activation boxes on top of any others to indicate a further level of processing. If an object is destroyed (removed from memory), an X is drawn on bottom of the lifeline, and the dashed line ceases to be drawn below it. It should be the result of a message, either from the object itself, or another.\n\nA message sent from outside the diagram can be represented by a message originating from a filled-in circle (\"found message\" in UML) or from a border of the sequence diagram (\"gate\" in UML).\n\nUML has introduced significant improvements to the capabilities of sequence diagrams. Most of these improvements are based on the idea of \"interaction fragments\" which represent smaller pieces of an enclosing interaction. Multiple interaction fragments are combined to create a variety of \"combined fragments\", which are then used to model interactions that include parallelism, conditional branches, optional interactions.\n\n"}
{"id": "23868049", "url": "https://en.wikipedia.org/wiki?curid=23868049", "title": "Sequential algorithm", "text": "Sequential algorithm\n\nIn computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with \"concurrent algorithm\" or \"parallel algorithm;\" most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.\n\n"}
{"id": "40782000", "url": "https://en.wikipedia.org/wiki?curid=40782000", "title": "Simplicial commutative ring", "text": "Simplicial commutative ring\n\nIn algebra, a simplicial commutative ring is a commutative monoid in the category of simplicial abelian groups, or, equivalently, a simplicial object in the category of commutative rings. If \"A\" is a simplicial commutative ring, then it can be shown that formula_1 is a commutative ring and formula_2 are modules over that ring (in fact, formula_3 is a graded ring over formula_1.)\n\nA topology-counterpart of this notion is a commutative ring spectrum.\n\nLet \"A\" be a simplicial commutative ring. Then the ring structure of \"A\" gives formula_5 the structure of a graded-commutative graded ring as follows.\n\nBy the Dold–Kan correspondence, formula_3 is the homology of the chain complex corresponding to \"A\"; in particular, it is a graded abelian group. Next, to multiply two elements, writing formula_7 for the simplicial circle, let formula_8 be two maps. Then the composition\nthe second map the multiplication of \"A\", induces formula_10. This in turn gives an element in formula_11. We have thus defined the graded multiplication formula_12. It is associative since the smash product is. It is graded-commutative (i.e., formula_13) since the involution formula_14 introduces minus sign.\n\nIf \"M\" is a simplicial module over \"A\" (that is, \"M\" is a simplicial abelian group with an action of \"A\"), then the similar argument shows that formula_15 has the structure of a graded module over formula_3. (cf. module spectrum.)\n\nBy definition, the category of affine derived schemes is the opposite category of the category of simplicial commutative rings; an object corresponding to \"A\" will be denoted by formula_17.\n\n\n"}
{"id": "52535241", "url": "https://en.wikipedia.org/wiki?curid=52535241", "title": "Singularity (system theory)", "text": "Singularity (system theory)\n\nThe term singularity for an explanation of unstable systems was first, and in a most general meaning used in 1873 by James Clerk Maxwell. Maxwell does not differentiate between dynamical systems and social systems. Therefore, a singularity refers to a context in which a small change can cause a large effect. The existence of singularities is primarily an argument against determinism and absolute causality for Maxwell. Indeed, following the same initial conditions will always achieve the same results, but such a statement is of little value in a world in which the same initial conditions are never repeated.\n\nIn summary, singularities are determined by the following characteristics which can vary in strength:\n\nA further development of Maxwell's thoughts in relation to dynamic systems was carried out first by the French mathematician Henri Poincaré. Poincaré distinguished four different simple singularities (points singuliers) of differential equations. These are the node (les noeuds), the saddle (les cols), the focus (les foyers) and the center (les centers).\nIn recent times, the chaos theory found special attention. However, deterministic chaos is just a special case of a singularity, in which a small cause produces a large observable effect due to a nonlinear dynamic behavior. In contrast the singularities raised by Maxwell, such as a loose rock at a singular point on a slope, show a linear dynamic behavior as it was demonstrated by Poincaré.\nSingularities are the common staple of the chaos theory, catastrophe theory and bifurcation theory.\n\nIn social systems, a deterministic chaos is unlikely, because the elements of the system are some individuals that engage with awareness, will and foresight purposefully into the dynamic behavior of the system. However, this does not exclude that approaches deterministic chaos in social systems are available. Rather, there is also an increase in the social development of nonlinear dynamics and instabilities .\nChaos in the colloquial sense of complete disorder or confusion, however, is to be found. It is often the basis for singularities, where cause-and-effect relationships are not clear. There are already numerous examples of singularities in social systems with Maxwell and Poincaré. Maxwell states that a word can start a war and all the great discoveries of man based on singular states. Poincaré gives the example of a roofer who drops a brick and randomly kills a passing man.\n\nThe development of systems provides the science currently so before that by a singular Big Bang uniformly dispersed plasma spread after the creation of the universe in space, which is cooled with increasing expansion, so that formed atoms and finally for very small (singular) fluctuations in the uniform density inhomogeneities created self-reinforcing. They subsequently led to the formation of galaxies, stars and other systems in the universe, from which humans emerged at the end. Even if the singularity of the Big Bang can be avoided in the mathematical models, singularities remain an essential element of history.\nThe evolutionary history shows that not only successful mutations can be perceived as positive singularities, but the humanization and the human becoming, the singular most important event in the evolution and represents a jump from the continuum of past evolutionary development of the planet Earth.\nRecently, Ward and Kirschvink show that the history of life has been more influenced by disasters than by continuous evolution. Disasters are here first destructive singularities that create space for new developments in the sense of innovations as productive singularities.\n\nClosely related is the notion of singularity with the concept of complexity. J.C. Maxwell has already pointed out that a system has all the more singular points, the more complex it is. Complexity is also the basis of perceived chaos and singularities. \nSuppose a seemingly insignificant event that produces a great effect, even in a simple context, how difficult would it be to detect the reason in a complex situation with tremendously many elements and relationships.\nComplexity that is kind of a breeding ground for singularities, shows the downfall of ancient cultures. Causes such as intruders, internal conflicts or natural disasters are not sufficient alone to justify the destruction of a culture. Rather requirement is an increasing complexity and associated declining marginal returns.\nThe financial crisis of 2007-2008 shows how difficult decisions are in a very complex environment. Thus, the complexity of financial systems and financial products is a major challenge of the financial markets and institutions to look at. One solution is to reduce complexity and increase the potential for adaptation and robustness. In a complex world with increasing singularities, it is therefore necessary to abandon optimization potential to gain adaptability to external shocks and disasters.\n\n\n"}
{"id": "332042", "url": "https://en.wikipedia.org/wiki?curid=332042", "title": "Strappado", "text": "Strappado\n\nThe strappado, also known as corda, is a form of torture wherein the victim's hands are tied behind his or her back and suspended by a rope attached to the wrists, typically resulting in dislocated shoulders. Weights may be added to the body to intensify the effect and increase the pain. This kind of torture would generally not last more than an hour, without rest, as it would likely result in death.\n\nOther names for strappado include \"reverse hanging\", \"Palestinian hanging\" (because of its alleged use by Israel against Palestinians) and \"il tormento della corda\". Historically, it was used by the medieval Inquisition and many governments, such as the civil law court (1543–1798) of the Order of St. John at the Castellania in Valletta, Malta.\n\nThe proper strappado causes permanent visible damage. Pain and resistance are different from person to person, generally due to the weight of the person himself/herself or the weight attached. It is not, as Samuel Johnson erroneously entered in his dictionary, a \"chastisement by blows\".\n\nThere are three variants of this torture. In the first, victims have their arms tied behind their backs; a large rope is then tied to the wrists and passed over a pulley, beam, or a hook on the roof. The torturer pulls on this rope until the victim is hanging from the arms. Since the hands are tied behind the victim's back, this will cause a very intense pain and possible dislocation of the arms. The full weight of the subject's body is then supported by the extended and internally rotated shoulder sockets. While the technique shows no external injuries, it can cause long-term nerve, ligament, or tendon damage. The technique typically causes brachial plexus injury, leading to paralysis or loss of sensation in the arms.\n\nThe second variation, known as squassation, is similar to the first, but a series of drops are added, meaning that the victim is allowed to drop until his or her fall is suddenly checked by the rope. In addition to the damage caused by the suspension, the painful jerk would cause major stress to the extended and vulnerable arms, leading to broken shoulders. It is believed that Niccolò Machiavelli, during his 1513 imprisonment after allegedly conspiring against the Medici family in Florence, was subjected to this form of strappado.\n\nIn the third variant, the victim's hands are tied to the front. The victim is also hung from the hands, but the ankles are tied and a heavy weight is attached to them. This will cause pain and possible damage not only to the arms, but also to the legs and hips.\n\nAccording to William Godwin, Girolamo Savonarola was tortured by strappado multiple times before being put to death in a trial by ordeal (fire); Savonarola, however, apparently renounced his confessions after being tortured, which eventually led to his sentence of burning at the stake.\nThis device was thought to be used during the Salem Witch Trials of Salem, Massachusetts in 1692 to torture accused witches.\n\nIn the Sachsenhausen concentration camp, prisoners were punished by having their hands tied behind their back, and then being suspended on a pole with their feet just above the ground for half an hour. This was also systematically used in Dachau Concentration Camp for infringement of camp rules. Originally the punishment used tree branches, but it was moved into a shower room on special poles so that victims could not relieve the pain as they could by using the tree. Holocaust survivor Jean Améry writes of his own experiences of strappado when being tortured at the Nazi concentration camp of Fort Breendonk in occupied Belgium. Améry's analytical descriptions of the torture were included in a number of works by the writer W. G. Sebald (notably the novel \"Austerlitz\").\n\nThe \"ropes\" was one of several torture methods employed at the Hỏa Lò Prison, popularly known as the Hanoi Hilton. The site was used by the North Vietnamese Army to house, torture and interrogate captured servicemen, mostly American airmen shot down during bombing raids (including USAF officer Joseph Kittinger, and US Navy pilot [and later US Senator] John McCain). The aim of the torture was usually not acquiring military information; rather, it was to break the will of the prisoners, both individually and as a group, and to get written or recorded statements from the prisoners who criticized U.S. conduct of the war and praised how the North Vietnamese treated them.\n\nAccording to a 1997 Human Rights Watch report, this technique was \"widely employed\" by the security forces of Turkey, where it is \"usually used together with high-pressure water, electric shock, beating, or sexual molestation such as squeezing the testicles or breast or placing a nightstick against or in the vagina or anus\". In 1996, the European Court of Human Rights found Turkey guilty of torture for its use of reverse hanging. Turkey has been admonished by Amnesty International and other international human rights groups concerning the use of the technique.\n\nIn 2003, one of the Bulgarian nurses interrogated during the HIV trial in Libya, Snezhana Dimitrova, stated that she had been tortured in this way.\n\nThey tied my hands behind my back. Then they hung me from a door. It feels like they are stretching you from all sides. My torso was twisted and my shoulders were dislocated from their joints from time to time. The pain cannot be described. The translator was shouting, \"Confess or you will die here\".\n\nIn November 2003, Manadel al-Jamadi, a suspected terrorist, was tortured to death at Abu Ghraib Prison during a Central Intelligence Agency interrogation. It was revealed in February 2005 that he had died after 30 minutes of interrogation during which he was suspended by the wrist, bound behind his back. \n\nIn 2017, video footage was released of Iraqi Army members inflicting strappado torture following successes in the Battle of Mosul.\n"}
{"id": "1267220", "url": "https://en.wikipedia.org/wiki?curid=1267220", "title": "Sympatry", "text": "Sympatry\n\nIn biology, two related species or populations are considered sympatric when they exist in the same geographic area and thus frequently encounter one another. An initially interbreeding population that splits into two or more distinct species sharing a common range exemplifies sympatric speciation. Such speciation may be a product of reproductive isolation – which prevents hybrid offspring from being viable or able to reproduce, thereby reducing gene flow – that results in genetic divergence. Sympatric speciation does not imply secondary contact, which is speciation or divergence in allopatry followed by range expansions leading to an area of sympatry. Sympatric species or taxa in secondary contact may or may not interbreed.\n\nFour main types of population pairs exist in nature. Sympatric populations (or species) contrast with parapatric populations, which contact one another in adjacent but not shared ranges and do not interbreed; peripatric species, which are separated only by areas in which neither organism occurs; and allopatric species, which occur in entirely distinct ranges that are neither adjacent nor overlapping. Allopatric populations isolated from one another by geographical factors (e.g., mountain ranges or bodies of water) may experience genetic—and, ultimately, phenotypic—changes in response to their varying environments. These may drive allopatric speciation, which is arguably the dominant mode of speciation.\n\nThe lack of geographic isolation as a definitive barrier between sympatric species has yielded controversy among ecologists, biologists, and zoologists regarding the validity of the term. As such, researchers have long debated the conditions under which sympatry truly applies, especially with respect to parasitism. Because parasitic organisms often inhabit multiple hosts during a life cycle, evolutionary biologist Ernst Mayr stated that internal parasites existing within different hosts demonstrate allopatry, not sympatry. Today, however, many biologists consider parasites and their hosts to be sympatric (see examples below). Conversely, zoologist Michael J. D. White considered two populations sympatric if genetic interbreeding was viable within the habitat overlap. This may be further specified as sympatry occurring within one deme; that is, reproductive individuals must be able to locate one another in the same population in order to be sympatric.\n\nOthers question the ability of sympatry to result in complete speciation: until recently, many researchers considered it nonexistent, doubting that selection alone could create disparate, but not geographically separated, species. In 2003, biologist Karen McCoy suggested that sympatry can act as a mode of speciation only when \"the probability of mating between two individuals depend[s] [solely] on their genotypes, [and the genes are] dispersed throughout the range of the population during the period of reproduction\". In essence, sympatric speciation does require very strong forces of natural selection to be acting on heritable traits, as there is no geographic isolation to aid in the splitting process. Yet, recent research has begun to indicate that sympatric speciation is not as uncommon as was once assumed.\n\nSyntopy is a special case of sympatry. It means the joint occurrence of two species in the same habitat at the same time. Just as the broader term sympatry, \"syntopy\" is used especially for close species that might hybridise or even be sister species. \"Sympatric\" species occur together in the same region, but do not necessarily share the same localities as \"syntopic\" species do. Areas of syntopy are of interest because they allow to study how similar species may coexist without outcompeting each other.\n\nAs an example, the two bat species \"Myotis auriculus\" and \"M. evotis\" were found to be syntopic in North America. In contrast, the marbled newt and the northern crested newt have a large sympatric range in western France, but differ in their habitat preferences and only rarely occur syntopically in the same breeding ponds.\n\nThe lack of geographic constraint in isolating sympatric populations implies that the emerging species avoid interbreeding via other mechanisms. Before speciation is complete, two diverging populations may still produce viable offspring. As speciation progresses, isolating mechanisms – such as gametic incompatibility that renders fertilization of the egg impossible – are selected for in order to increase the reproductive divide between the two populations.\n\nSympatric groups frequently show a greater ability to discriminate between their own species and other closely related species than do allopatric groups. This is shown in the study of hybrid zones. It is also apparent in the differences in levels of prezygotic isolation (by factors that prevent formation of a viable zygote) in both sympatric and allopatric populations. There are two main theories regarding this process: 1) differential fusion, which suggests that only populations with a keen ability to discriminate between species will persist in sympatry; and 2) character displacement, which implies that distinguishing characteristics will be heightened in areas where the species co-occur in order to facilitate discrimination.\n\nReinforcement is the process by which natural selection reinforces reproductive isolation. In sympatry, reinforcement increases species discrimination and sexual adaptation in order to avoid maladaptive hybridization and encourage speciation. If hybrid offspring are either sterile or less-fit than non-hybrid offspring, mating between members of two different species will be selected against. Natural selection decreases the probability of such hybridization by selecting for the ability to identify mates of one's own species from those of another species.\n\nReproductive character displacement strengthens the reproductive barriers between sympatric species by encouraging the divergence of traits that are crucial to reproduction. Divergence is frequently distinguished by assortative mating between individuals of the two species. For example, divergence in the mating signals of two species will limit hybridization by reducing one's ability to identify an individual of the second species as a potential mate. Support for the reproductive character displacement hypothesis comes from observations of sympatric species in overlapping habitats in nature. Increased prezygotic isolation, which is associated with reproductive character displacement, has been observed in cicadas of genus \"Magicicada\", stickleback fish, and the flowering plants of the genus \"Phlox\".\n\nAn alternative explanation for species discrimination in sympatry is differential fusion. This hypothesis states that of the many species have historically come into contact with one another, the only ones that persist in sympatry (and thus are seen today) are species with strong mating discrimination. On the other hand, species lacking strong mating discrimination are assumed to have fused while in contact, forming one distinct species.\n\nDifferential fusion is less widely recognized than character displacement, and several of its implications are refuted by experimental evidence. For example, differential fusion implies greater postzygotic isolation among sympatric species, as this functions to prevent fusion between the species. However, Coyne and Orr found equal levels of postzygotic isolation among sympatric and allopatric species pairs in closely related \"Drosophila\". Nevertheless, differential fusion remains a possible, though not complete, contributor to species discrimination.\n\nSympatry has been increasingly evidenced in current research. Because of this, sympatric speciation – which was once highly debated among researchers – is progressively gaining credibility as a viable form of speciation.\n\nSeveral distinct types of killer whale (\"Orcinus orca\"), which are characterized by an array of morphological and behavioral differences, live in sympatry throughout the North Atlantic, North Pacific and Antarctic oceans. In the North Pacific, three whale populations – called \"transient\", \"resident\", and \"offshore\" – demonstrate partial sympatry, crossing paths with relative frequency. The results of recent genetic analyses using mtDNA indicate that this is due to secondary contact, in which the three types encountered one another following the bidirectional migration of \"offshore\" and \"resident\" whales between the North Atlantic and North Pacific. Partial sympatry in these whales is, therefore, not the result of speciation. Furthermore, killer whale populations that consist of all three types have been documented in the Atlantic, evidencing that interbreeding occurs among them. Thus, secondary contact does not always result in total reproductive isolation, as has often been predicted.\n\nThe parasitic great spotted cuckoo (\"Clamator glandarius\") and its magpie host, both native to Southern Europe, are completely sympatric species. However, the duration of their sympatry varies with location. For example, great spotted cuckoos and their magpie hosts in Hoya de Gaudix, southern Spain, have lived in sympatry since the early 1960s, while species in other locations have more recently become sympatric. Great spotted cuckoos, when in South Africa, are sympatric with at least 8 species of starling and 2 crows, pied crow and Cape crow.\n\nThe great spotted cuckoo exhibits brood parasitism by laying a mimicked version of the magpie egg in the magpie's nest. Since cuckoo eggs hatch before magpie eggs, magpie hatchlings must compete with cuckoo hatchlings for resources provided by the magpie mother. This relationship between the cuckoo and the magpie in various locations can be characterized as either recently sympatric or anciently sympatric. The results of an experiment by Soler and Moller (1990) showed that in areas of ancient sympatry (species in cohabitation for many generations), magpies were more likely to reject most of the cuckoo eggs, as these magpies had developed counter-adaptations that aid in identification of egg type. In areas of recent sympatry, magpies rejected comparatively fewer cuckoo eggs. Thus, sympatry can cause coevolution, by which both species undergo genetic changes due to the selective pressures that one species exerts on the other.\n\nLeafcutter ants protect and nourish various species of fungus as a source of food in a system known as ant-fungus mutualism. Leafcutter ants belonging to the genus \"Acromyrmex\" are known for their mutualistic relationship with Basidiomycete fungi. Ant colonies are closely associated with their fungus colonies, and may have co-evolved with a consistent vertical lineage of fungi in individual colonies. Ant populations defend against the horizontal transmission of foreign fungi to their fungal colony, as this transmission may lead to competitive stress on the local fungal garden. Invaders are identified and removed by the ant colony, inhibiting competition and fungal interbreeding. This active isolation of individual populations helps maintain the genetic purity of the fungal colony, and this mechanism may lead to sympatric speciation within a shared habitat.\n\n\n"}
{"id": "44674508", "url": "https://en.wikipedia.org/wiki?curid=44674508", "title": "Tatsuo Miyajima", "text": "Tatsuo Miyajima\n\nMiyajima was born in Edogawa City, Tokyo on January 16, 1957. He graduated from the Oil Painting course in the Fine Arts department of Tokyo National University of Fine Arts and Music in 1984, and completed his MA at the same university in 1986.\n\nAlthough Miyajima originally trained as a painter, and briefly considered himself to be a performance artist, the majority of his work now takes the form of installation and sculpture. He has admitted that, in effect, his work now \"performs\" on his behalf. His core artistic concepts are: \"Keep Changing, Connect with Everything, Continue Forever.\"\n\nIn 1970s, Miyajima practised performance art. He was initially influenced by the work of Joseph Beuys, Allan Krapow and Christo, and considered his performance work as an \"action for society\". The desire to create more enduring work - in contrast to the necessarily ephemeral nature of his performance and actions - motivated him to begin working on sculpture and installations.\n\nMiyajima made his first LED counter in 1988; this has formed the basis for much of his later work. Typically, a block will display two digits in red or green, and count from 1 to 99. The counters never register zero, because, for Miyajima, the idea of zero is a purely Western concept. He has subsequently linked together different displays so that they can respond to each other; he calls these systems 'regions'.\n\nMiyajima's first solo exhibitions include \"Human Stone\" at Gallery Parergon, Tokyo in 1983, and \"Time\" at Maki Gallery, Tokyo in 1986. More recently he has shown at Modern Art Museum of Fort Worth (1996), Fondation Cartier pour l'Art Contemporain (1996), San Francisco Museum of Modern Art (1997), Miyanomori Art Museum, Hokkaido (2010), and Ullens Center for Contemporary Art, Beijing (2011).\n\nHe has exhibited as part of numerous group exhibitions, notably the Venice Biennale in 1988 and 1999, as well Hiroshima City Museum of Contemporary Art (2008), and Museum of Contemporary Art, Sydney (2012).\n\nMiyajima is represented by Buchmann Galerie and Lisson Gallery. In 2010, one of Miyajima's works, \"T. L. Sakura\", was sold for $375,173 at Christie's Hong Kong.\n\nThe following museums and institutions have works by Miyajima in their collection:\n\n"}
{"id": "42770735", "url": "https://en.wikipedia.org/wiki?curid=42770735", "title": "The Moral Obligation to Be Intelligent", "text": "The Moral Obligation to Be Intelligent\n\nThe Moral Obligation to Be Intelligent is an influential essay, part of the essay collection \"The Moral Obligation to Be Intelligent, and Other Essays\" published in 1915 by John Erskine, English professor at Columbia University.\n\nThe essay was first read before the Phi Beta Kappa Society of Amherst College, where Erskine taught before joining Columbia. Later, it was published in quarterly magazine \"The Hibbert Journal\" in 1914. During his tenure at Columbia from 1909 and 1937, Erskine formulated General Honors Course. In the early 1920s he started teaching a \"great books\" course at Columbia, which later founded the influential Great Books movement.\n\nIn 1963, the essay was published in the \"Gateway to the Great Books\" - Volume 10: Philosophical Essays, a 10-volume book series published by Encyclopædia Britannica Inc. in 1963.\n\nSubsequently it was also included in the book of selected essays also titled \"The Moral Obligation to Be Intelligent: Selected Essays\" (New York: Farrar, Straus & Giroux) published posthumously in 2000), edited by Lionel Trilling, another Columbia faculty and literary critic, and had an introduction by critic Leon Wieseltier. Trilling was one of Erskine's students and later taught the \"Great Books\" course himself; he chose the essay as the lead and the title as it characterized the essence his selection. \n\n"}
{"id": "50902679", "url": "https://en.wikipedia.org/wiki?curid=50902679", "title": "The Visual Workplace", "text": "The Visual Workplace\n\n\"\n\nThe visual workplace is a continuous improvement paradigm that is closely related to Lean, the Toyota Production System (TPS), and operational excellence yet offers its own comprehensive methodology for significant financial and cultural improvement gains. Introduced by Gwendolyn Galsworth in her 1997 book \"Visual Systems,\"., this system integrates and codifies the many iterations of visuality in the world of continuous improvement.\n\nVisual communication rests on the natural inclination of humans to use pictures, graphics, and other images to quickly and simply convey meaning and understand information. For instance, look at the practices and applications that civil engineers have developed to handle complex human interaction on our roads and highways, as well as the entire field of wayfinding in public spaces \n\nThe same logic eventually migrated into the workplace, notably in post-war Japan, and most saliently at Toyoda Motors where visual applications (visual devices) became a commonplace element in the Toyota Production System (TPS). Other leading companies in Japan, such as Canon and Okidata, adopted many of the same practices. However, while visibility was clearly a part of Japan's success solution, it was only noticed—or cited in the literature—as a generalized principle and not a codified system or a framework of thinking. For example, Dr. Robert W. Hall, in his 1983 book, \"Zero Inventories,\" states: \"Establishing visibility of all forms of production problems is very important. ...The entire idea is instant communication.\"\n\nSpecifically, Japan's JIT (just-in-time) manufacturing approach had an easy-to-understand visual interface: andon (stacked lights) kanban (pick-up tickets for control material quantity), color-coding (to make the match between items), scheduling boards for daily production, easy-to-read labels on shelving, and lines on the floor to trace out locations.\n\nJapanese master practitioners also noted that visual devices made it easy to see the difference between normal and abnormal: \"…abnormal conditions and problems need to be obvious enough to catch people's attention. Because of the emphasis on visual methods for quick information transfer, the practice is called ’management by sight’ or ‘visual control’.\" Suzaki also compared the responsiveness of a well-tuned production system with the way the human body responds to stimuli and problems: \"…Corrective action is taken right away, just as our muscles pull our hand away when we touch a hot plate.\"\n\nMichel Greif's book, \"The Visual Factory\" conferred the name for the first time, though Dr. Greif's theme focused primarily on the ability of visual applications to increase the interest of hourly workers in their own performance and their participation in company improvement activities.\n\nThroughout this period (1983–1991), Gwendolyn Galsworth was head of training and development at Productivity Inc. in Cambridge, Massachusetts, a publishing, training, and consulting firm known for bringing the work of Japan's manufacturing leaders to the United StatesGalsworth headed study missions to Japan and observed visuality in Japan first-hand. She also had the opportunity of working one-on-one with many of Japan's seminal thinkers, including Taiichi Ohno, Ryuji Fukuda, and Shigeo Shingo. Dr. Shingo personally tasked Galsworth with developing his mistake proofing/poka-yoke methodology for western companies. It was the synthesis of all these factors and influences that lead Dr. Galsworth to develop and codify the many threads of visuality into a coherent methodology of the visual workplace.\n\nThe visual workplace, then, is an overall operational strategy and philosophy, geared to help organizations continually achieve their goals through visual devices and systems. Galsworth continues to be the main driving force behind the practice and articulation of workplace visuality, along with a network of individuals and companies loosely coupled as visual workplace practitioners around the world. The visual workplace is a large body of knowledge and know-how, with a strong guiding philosophy of continuous improvement with an emphasis on the centrality of the individual in the prosperity of the enterprise.\n\nWhile virtually all major improvement paradigms in use in the West incorporate some element of visuality, the entire codified set of visual principles and practices, from the foundation of 5S through to visual guarantees (poka-yoke), rests on this definition: \"The visual workplace is a self-ordering, self-explaining, self-regulating, and self-improving work environment—where what is supposed to happen does happen, on time, every time, day or night—because of visual devices.\"\n\nA visual workplace is defined by devices designed to visually share information about organizational operations in order to make human and machine performance safer, more exact, more repeatable, and more reliable. The more the process becomes visual, the more production velocity increases.\n\nThis is accomplished in parallel with generating new levels of employee engagement and contribution, which in turn lead to improved alignment within the enterprise and significant bottom line benefits. In an effective visual workplace, this level of information can be seen and understood without coaching, supervision or the need for an explanation—at best, without speaking a word.\n\nThe key principle is to install vital information visually as close to the point of use as possible. When a step-by-step methodology is applied, the visual workplace targets the elimination of the seventh waste, \"motion\", defined as \"moving without working.\"\n\nOriginally implemented and refined in manufacturing settings, the concept of the visual workplace is now taking hold in such wide-ranging venues as libraries and hospitals.\n"}
{"id": "21304742", "url": "https://en.wikipedia.org/wiki?curid=21304742", "title": "Validity (logic)", "text": "Validity (logic)\n\nIn logic, an argument is valid if and only if it takes a form that makes it impossible for the premises to be true and the conclusion nevertheless to be false. It is not required that a valid argument have premises that are actually true, but to have premises that, if they were true, would guarantee the truth of the argument's conclusion. A formula is valid if and only if it is true under every interpretation, and an argument form (or schema) is valid if and only if every argument of that logical form is valid.\n\nAn argument is valid if and only if the truth of its premises entails the truth of its conclusion and each step, sub-argument, or logical operation in the argument is valid. Under such conditions it would be self-contradictory to affirm the premises and deny the conclusion. The corresponding conditional of a valid argument is a logical truth and the negation of its corresponding conditional is a contradiction. The conclusion is a logical consequence of its premises.\n\nAn argument that is not valid is said to be \"invalid\".\n\nAn example of a valid argument is given by the following well-known syllogism:\n\nWhat makes this a valid argument is not that it has true premises and a true conclusion, but the logical necessity of the conclusion, given the two premises. The argument would be just as valid were the premises and conclusion false. The following argument is of the same logical form but with false premises and a false conclusion, and it is equally valid:\n\nNo matter how the universe might be constructed, it could never be the case that these arguments should turn out to have simultaneously true premises but a false conclusion. The above arguments may be contrasted with the following invalid one:\n\nIn this case, the conclusion contradicts the deductive logic of the preceding premises, rather than deriving from it. Therefore, the argument is logically 'invalid', even though the conclusion could be considered 'true' in general terms. The premise 'All men are immortal' would likewise be deemed false outside of the framework of classical logic. However, within that system 'true' and 'false' essentially function more like mathematical states such as binary 1s and 0s than the philosophical concepts normally associated with those terms.\n\nA standard view is that whether an argument is valid is a matter of the argument's logical form. Many techniques are employed by logicians to represent an argument's logical form. A simple example, applied to two of the above illustrations, is the following: Let the letters 'P', 'Q', and 'S' stand, respectively, for the set of men, the set of mortals, and Socrates. Using these symbols, the first argument may be abbreviated as:\n\nSimilarly, the third argument becomes:\n\nAn argument is termed formally valid if it has structural self-consistency, i.e. if when the operands between premises are all true, the derived conclusion is always also true. In the third example, the initial premises cannot logically result in the conclusion and is therefore categorized as an invalid argument.\n\nA formula of a formal language is a valid formula if and only if it is true under every possible interpretation of the language. In propositional logic, they are tautologies.\n\nA statement can be called valid, i.e. logical truth, if it is true in all interpretations.\n\nValidity of deduction is not affected by the truth of the premise or the truth of the conclusion. The following deduction is perfectly valid:\n\nThe problem with the argument is that it is not \"sound\". In order for a deductive argument to be sound, the deduction must be valid and all the premises true.\n\nModel theory analyzes formulae with respect to particular classes of interpretation in suitable mathematical structures. On this reading, formula is valid if all such interpretations make it true. An inference is valid if all interpretations that validate the premises validate the conclusion. This is known as \"semantic validity\".\n\nIn truth-preserving validity, the interpretation under which all variables are assigned a truth value of 'true' produces a truth value of 'true'.\n\nIn a false-preserving validity, the interpretation under which all variables are assigned a truth value of 'false' produces a truth value of 'false'.\n\n\n"}
{"id": "50066201", "url": "https://en.wikipedia.org/wiki?curid=50066201", "title": "Women's fear of crime", "text": "Women's fear of crime\n\nWomen's fear of crime refers to women's fear of being a victim of crime, independent of actual victimization. Although fear of crime is a concern for people of all genders, studies consistently find that women around the world tend to have much higher levels of fear of crime than men, despite the fact that in many places, and for most offenses, men's actual victimization rates are higher. Fear of crime is related to perceived risk of victimization, but is not the same; fear of crime may be generalized instead of referring to specific offenses, and perceived risk may also be considered a demographic factor that contributes to fear of crime. Women tend to have higher levels for both perceived risk and fear of crime.\n\nIn women's everyday lives, fear of crime can have negative effects, such as reducing their environmental mobility. Studies have shown that women tend to avoid certain behaviors, such as walking alone at night, because they are fearful of crime, and would feel more comfortable with these behaviors if they felt safer.\n\nSocial scientists have differing views on the causes of women's fear of crime. Some have argued that women's heightened fear of crime is due to women's higher levels physical vulnerability compared to men, although feminist work generally resists this generalization and often tries to relocate the cause to larger societal factors. It is nonetheless important that most women are aware of pervasive cultural view that women are more vulnerable than men, which may make them think they are more likely to be victimized and therefore contribute to their fear; in this way, it would be perceived vulnerability and not actual vulnerability that is the cause of women's fear. Some research has also suggested that women are in fact not much more fearful about crime than men, but that dominant cultural ideas about masculinity may make men reluctant to talk about their fear or report it in surveys.\n\nFeminist discourse on fear of crime tends to explain women's higher levels of fear with the unequal gender structure in most societies, which places women beneath men within the power structure and thus puts them especially at risk for victimization by men. This theory refers to the oppressive social control of women, arguing that some crimes against women (such as rape) and the socialization that women receive to feel vulnerable and fear male violence are used by the patriarchy to assert male dominance and \"keep women in their place.\"\n\nAs rape is by far the most gendered crime by victimization, some feminist scholars have suggested that fear of rape is the most important and most unique element of women's fear of crime, or even that women's fear of crime is in fact a generalized fear of rape. Proponents of this theory, often referred to as the \"shadow of sexual assault hypothesis,\" often note that women tend to fear that rape will co-occur with other crimes, such as burglary, a fear that is not found among men. Some supporters of the theory also note that sexual harassment, which most women will experience in their lifetimes, especially contributes to the fear of rape; in some cases, women's rejection of unwanted sexual advances leads to threats, and even \"benevolent\" harassment may increase women's wariness and fear of men in public spaces.\n\nThe fear of rape, unlike other fears of specific crimes, is almost exclusive to women. Among women, it is also one of the strongest crime-related fears, and is the strongest crime-related fear for young women. Levels of fear of rape vary among women by age, race/ethnicity, residential area, and other factors, but are especially high for women who have been victims of rape in the past or know victims personally (the latter group may include a significant portion of women, with one study estimating that over half of women know rape victims). The fear of rape may also be related to the fear of murder, as women tend to overestimate the proportion of rape victims who are murdered during their attacks. Stigma and blame are also factors: what many feminists refer to as the \"rape myth,\" the popular idea that women can be blamed for their rape and that women are responsible for preventing rape by the regulation of their behavior, often serves to support the fear of rape.\n\nAlthough women as a whole demographic are more fearful of crime, specific subgroups of women may have higher levels of fear or be more likely to change their behavior because of it. In many studies, the demographics found to have the highest generalized fear of crime are single, older, urban, women of color, and of lower socioeconomic class. For fear of victimization for specific crimes, the demographics with the highest levels of fear may be different.\n\nGenerally, research has demonstrated that women from urban areas have higher levels of fear of crime. Even within cities, fear levels may differ from neighborhood to neighborhood. Increased social disorganization in the neighborhood (as measured by homelessness, drug sales, vandalism, prostitution, etc.) and higher rates of neighborhood serious crime lead to higher levels of fear of crime for both men and women, but both factors have a stronger effect on women's fear of crime.\n\nUrban and rural communities tend to have very different levels of fear of crime. Rural areas are almost always perceived by residents and outsiders as safer, so it is often assumed that fear of crime levels will be lower there. Still, 2005 research in New Zealand and the United Kingdom noted that fear of crime levels in rural areas is on the rise, and found that sources of fear of crime among rural women often include perceived encroachment of urban influence (through people or attitudes) into their communities.\n\nTheorists have suggested that Black and Latina women, and women of color in general, in the United States may have higher rates of fear of crime due to increased social vulnerability; because of institutional racism and sexism against women of color, their identities may put them at greater risk of victimization, leading to higher levels of fear.\n\nIn general, proposed solutions to women's fear of crime either place the responsibility on individual women (through preventive strategies) or on official agencies (through infrastructure improvements, anti-rape education, more involved policing, etc.), and are often framed as a combination of both. Of those that expect women to protect themselves from crime, most focus on the dangers for women in public spaces; however, as women usually face their highest rates of victimization in the home or at the hands of known people, these campaigns have been suggested to be particularly ill-equipped to help solve the problem of women's high fear of crime, and to support an untrue picture of women's victimization.\n\nOne of the most common individual strategies for dealing with fear of crime and preventing victimization is simple avoidance, the attempt to stay away from areas (such as dark alleys or public transportation) where it is believed victimization is likely to occur; research has found that women employ avoidance strategies more often than men do. Avoided areas may include neighborhoods with high crime rates, but for many women also include any unfamiliar areas. Women may also employ other isolation strategies by avoiding social interaction with strangers, ignoring them or moving quickly and with purpose to discourage interaction\n\nAnother common method of allaying fear of crime among women is by \"crime-proofing\" homes or possessions. Popular examples include home security systems, locking away valuables, or participating in neighborhood watch programs. These strategies are used by people of all genders, but some are more often employed by women. For example, many women in an American study reported choosing purses with zippers or holding purses protectively to defend against theft and purse-snatching.\n\nAs interest in women's safety and women's fear of crime has increased, so has interest in precautionary strategies; for example, in the past few decades, women's self-defense classes, books, and other self-defense instruction have become increasingly popular. Some women also choose to carry weapons, such as knives or guns, or repellents like pepper spray and Mace to ward off potential attackers.\n\nFeminist commentators usually take the view that the responsibility for reducing women's fear of crime lies with the society, and that fear must be combatted at its source by addressing men's violence against women.\n\nAlthough most research on women's fear of crime has been done in English-speaking countries, with the most done in the United States, similar trends in women's fear of crime have been found around the world.\n\nA 2014 study using data from 20 countries in Sub-Saharan Africa concluded that fear of crime has a stronger negative effect on women's subjective well-being compared to men's, with subjective wellbeing defined as self-reported satisfaction with life. In the study, fear of crime had a statistically significant correlation with subjective wellbeing for females, but no significant correlation for males, suggesting that for the men in the study, fear of crime was not an important factor in determining their happiness and life satisfaction.\n\nA 2013 study of Hong Kong social work students found significant gender differences in fear of crime levels. Consistent with the shadow of sexual assault hypothesis, the study found that the women had the highest levels of fear for rape, and that fear of rape was a predictor for fear of other crimes. Hong Kong has one of the lowest rates of crime and victimization in the world, so this study may suggest that the presence and size of gender differences in fear of crime are not strongly correlated with total crime and victimization rates.\n\nA 1998 study in Glasgow, Scotland, found a gender disparity in fear of crime levels smaller than what is generally accepted. The study also found that men and women with similar fear levels tended to use similar reasoning to explain their fear of crime or lack of fear, although men's and women's fear appeared in different situations (men tended to be more often fearful about property crime, whereas women were more fearful about violent crime).\n\nA 2010 Turkish study using a large nationwide sample confirmed that women expressed significantly higher levels of fear of crime than men. The study also found that previous victimization, a consistent predictor for higher levels of fear in women, was present at close to equal rates in the male and female samples, suggesting that prior victimization has a stronger effect on women's fear of crime than on men's. Also, if the study's sample is representative of the Turkish population, women have slightly higher victimization rates than men, and so their fear does not reflect the \"gender-fear paradox\" of victimization found in many other developed countries.\n"}
{"id": "35936877", "url": "https://en.wikipedia.org/wiki?curid=35936877", "title": "Wonderful life theory", "text": "Wonderful life theory\n\nIn biology, the wonderful life theory, also known as contingency theory, postulates that after hundreds of different phyla evolved during the Cambrian period, many of them subsequently became extinct, leaving the relatively few phyla that exist today. The theory was first suggested in 1989 by Stephen Jay Gould in his book \"Wonderful Life\".\n"}
{"id": "2294938", "url": "https://en.wikipedia.org/wiki?curid=2294938", "title": "Zebra (medicine)", "text": "Zebra (medicine)\n\nZebra is the American medical slang for arriving at an exotic medical diagnosis when a more commonplace explanation is more likely. It is shorthand for the aphorism coined in the late 1940s by Dr. Theodore Woodward, professor at the University of Maryland School of Medicine, who instructed his medical interns: \"When you hear hoofbeats, think of horses not zebras\". Since horses are common in Maryland while zebras are relatively rare, logically one could confidently guess that an animal making hoofbeats is probably a horse. By 1960, the aphorism was widely known in medical circles.\n\nAs explained by Sotos, medical novices are predisposed to make rare diagnoses because of (a) the availability heuristic (\"events more easily remembered are judged more probable\") and (b) the phenomenon first enunciated in \"Rhetorica ad Herennium\" (circa 85 BC), \"the striking and the novel stay longer in the mind.\" Thus, the aphorism is an important caution against these biases when teaching medical students to weigh medical evidence.\n\nDiagnosticians have noted, however, that \"zebra\"-type diagnoses must nonetheless be held in mind until the evidence conclusively rules them out:\nThe term for an obscure and rare diagnosis in medicine is fascinoma.\n\nNecrotic skin lesions in the United States are often diagnosed as loxoscelism (recluse spider bites), even in areas where \"Loxosceles\" species are rare or not present. This is a matter of concern because such misdiagnoses can delay correct diagnosis and treatment.\n\nEhlers–Danlos syndrome and hypermobility syndrome are both medical zebras and yet are commonly under diagnosed and misdiagnosed. The EDS and hypermobility syndrome awareness movements have adopted the zebra as the mascot for their campaigning.\n\nThe television series \"Scrubs\" episode \"My Balancing Act\", focuses on this theme, and the medical veteran Dr. Cox uses the metaphor to explain to intern John \"J. D.\" Dorian why he should first consider a simple diagnosis.\n\nThe television series \"House\" had the working title \"Chasing Zebras, Circling the Drain\", a reference to the show's recurring theme of hunting for obscure diagnoses while a patient is in a critical condition. The title character, diagnostic expert Dr. Gregory House rejects the aphorism, arguing that any cases with simple solutions would have been successfully diagnosed by someone else before reaching him. \n\nThe episode of the television series \"\" was named after this term and cites a version of the aphorism in the teaser.\n\nHarvard Medical School and Harvard School of Dental Medicine's 2017 music video \"IT'S NOT A ZEBRA!\" depicts a group of first-year medical students erroneously trying to diagnose a patient with a rare and unlikely disease, when in fact she has the flu. \n\n\n"}
{"id": "20116588", "url": "https://en.wikipedia.org/wiki?curid=20116588", "title": "Zero Waste Event", "text": "Zero Waste Event\n\nA Zero Waste Event (or \"ZeeWee\" as it has been nicknamed) is one in which event organizers plan ahead to reduce solid waste from the event, reuse various elements such as banners, and set up Zero-Waste Stations for those recyclable and compostable materials such as paper cups, food scraps, and plastic water bottles that are generated by the event. ZeeWees can range from large scale sports events to weddings and parties.\n\nAs sustainability becomes ever more accepted as a concept within organizations and communities across the nation, the idea of Zero Waste Events is spreading. Additionally, the steady expansion of recycling and commercial composting infrastructure makes it possible to offer recycling and food composting at events in many locations. Increasingly Zero Waste Events are sharing best practices and developing new ways to divert event materials from landfills.\n\nZeeWee's utilize many if not all of the following strategies.\n\nFront-end waste prevention\n\nReuse of event equipment\n\nRecycling event materials\n\nWhen the bid was submitted for the London 2012 Olympic and Paralympic Games, LOCOG, The London 2012 Organising Committee of the Olympic and Paralympic Games, pledged not just to host the biggest sporting event in the world but also to stage the first truly sustainable Games. Central to this was an ambitious target to send zero waste from Games-time venues directly to landfill – something that no Games has attempted before. \nIn February 2012 an initiative was launched to share the lessons learnt from delivering a zero waste games with the wider waste and events community.\n"}
{"id": "1337152", "url": "https://en.wikipedia.org/wiki?curid=1337152", "title": "Zoo hypothesis", "text": "Zoo hypothesis\n\nThe zoo hypothesis speculates as to the assumed behavior and existence of technically advanced extraterrestrial life and the reasons they refrain from contacting Earth and is one of many theoretical explanations for the Fermi paradox. The hypothesis is that alien life intentionally avoids communication with Earth, and one of its main interpretations is that it does so to allow for natural evolution and sociocultural development, avoiding interplanetary contamination, similarly to people observing animals at a zoo. The hypothesis seeks to explain the apparent absence of extraterrestrial life despite its generally accepted plausibility and hence the reasonable expectation of its existence.\n\nAliens might, for example, choose to allow contact once the human species has passed certain technological, political, or ethical standards. They might withhold contact until humans force contact upon them, possibly by sending a spacecraft to planets they inhabit. Alternatively, a reluctance to initiate contact could reflect a sensible desire to minimize risk. An alien society with advanced remote-sensing technologies may conclude that direct contact with neighbors confers added risks to oneself without an added benefit. \n\nThe zoo hypothesis assumes first that a large number of alien cultures exist, and second that these aliens have great reverence for independent, natural evolution and development. In particular, assuming that intelligence is a physical process that acts to maximize the diversity of a system's accessible futures, a fundamental motivation for the zoo hypothesis would be that premature contact would \"unintelligently\" reduce the overall diversity of paths the universe itself could take. \n\nThese ideas are perhaps most plausible if there is a relatively universal cultural or legal policy among a plurality of extraterrestrial civilizations necessitating isolation with respect to civilizations at Earth-like stages of development. In a universe without a hegemonic power, random single civilizations with independent principles would make contact. This makes a crowded Universe with clearly defined rules seem more plausible.\n\nIf there is a plurality of alien cultures, however, this theory may break down under the uniformity of motive concept because it would take just a single extraterrestrial civilization to decide to act contrary to the imperative within our range of detection for it to be undone, and the probability of such a violation increases with the number of civilizations. This idea, however, becomes more plausible if all civilizations tend to evolve similar cultural standards and values with regard to contact much like convergent evolution on Earth has independently evolved eyes on numerous occasions, or all civilizations follow the lead of some particularly distinguished civilization, such as the first civilization among them.\n\nWith this in mind, a modified zoo hypothesis becomes a more appealing answer to the Fermi paradox. The time between the emergence of the first civilization within the Milky Way and all subsequent civilizations could be enormous. Monte Carlo simulation shows the first few inter-arrival times between emergent civilizations would be similar in length to geologic epochs on Earth. Just what could a civilization do with a ten-million, one-hundred-million, or half-billion-year head start?\n\nEven if this first grand civilization is long gone, their initial legacy could live on in the form of a passed-down tradition, or perhaps an artificial life form dedicated to such a goal without the risk of death. Beyond this, it does not even have to be the first civilization, but simply the first to spread its doctrine and control over a large volume of the galaxy. If just one civilization gained this hegemony in the distant past, it could form an unbroken chain of taboo against rapacious colonization in favour of non-interference in those civilizations that follow. The uniformity of motive concept previously mentioned would become moot in such a situation.\n\nIf the oldest civilization still present in the Milky Way has, for example, a 100-million-year time advantage over the next oldest civilization, then it is conceivable that they could be in the singular position of being able to control, monitor, influence or isolate the emergence of every civilization that follows within their sphere of influence. This is analogous to what happens on Earth within our own civilization on a daily basis, in that everyone born on this planet is born into a pre-existing system of familial associations, customs, traditions and laws that were already long established before our birth and which we have little or no control over.\n\n"}
