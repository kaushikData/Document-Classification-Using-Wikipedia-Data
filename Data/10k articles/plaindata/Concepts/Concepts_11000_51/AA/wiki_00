{"id": "52786633", "url": "https://en.wikipedia.org/wiki?curid=52786633", "title": "2017 Chicago torture incident", "text": "2017 Chicago torture incident\n\nIn January 2017, a mentally disabled youth in a suburb of Chicago, Illinois, was filmed being physically and verbally abused by four individuals. The incident was livestreamed on Facebook, making the incident a live streaming crime.\n\nThe victim met with an acquaintance from high school at a McDonald's on New Year's Eve, and on January 3 was found by a police officer to appear injured while walking with a suspect on a sidewalk. The four suspects were arrested after the incident was livestreamed by one of the women on Facebook, and charged with hate crimes and other offenses.\n\nOn December 31, 2016, the 18-year-old victim was dropped off at a McDonald's in suburban Streamwood, Illinois, by his parents. The victim knew one of the suspects before the incident had occurred as the two had attended the same school in Aurora, Illinois, and the victim had mistakenly thought that the two were friends. When the victim went to the McDonald's, he had the intention of spending time with this friend. On January 2, 2017, the victim's parents filed a report that he was missing.\n\nPolice noted that one of the male suspects stole a van and, pretending that it was his own van, went to meet the victim. The victim got into the van with the suspect, and they went to one of the suspect's friend's houses on the West Side of Chicago for a two-day visit. During that time, the victim slept in the van. On January 3, they went to the residence of two of the other suspects, who were sisters. While at the sisters' apartment, the victim and the male suspect \"play-fought\", which ended in the sisters getting angry and tying the victim up.\n\nThe younger of the two sisters then turned on a Facebook Live stream to record the following events. The victim was bound, gagged, beaten, taunted, had part of his scalp removed with a knife, and was forced to kiss the floor and drink from a toilet bowl. The attackers are heard shouting \"Fuck Trump\" and \"Fuck white people\" in the video. One of the suspects contacted the victim's mother and demanded a $300 ransom for the victim's return. Although the Facebook Live stream only lasted 28 minutes, the victim was tied up for hours. Police suspected that the attack stopped when neighbors in a downstairs apartment complained about noise levels.\n\nThe victim was an 18-year-old, mentally-disabled, white youth.\n\nFour suspects were arrested and charged with aggravated kidnapping, aggravated unlawful restraint, aggravated battery, and hate crime. Two were 18-year-old males, Jordan Hill and Tesfaye Cooper, and one was Brittany Covington, an 18-year-old female; her sister Tanishia Covington was a 24-year-old female.\n\nOn February 10, 2017, all four suspects pleaded not guilty at their arraignment. On May 16, 2017, a judge set bail for the four individuals: $900,000 for Hill; \n$800,000 for Cooper; $500,000 for Brittany Covington; and $200,000 for Tanishia Covington. None of the four defendants were able to post bail.\n\nOn December 8, 2017, Brittany Covington plead guilty to the charges of committing a hate crime, intimidation and aggravated battery. Additional charges, such as kidnapping, were dropped as part of her plea deal. Covington was sentenced to four years of probation and 200 hours of community service. Cook County Circuit Judge William Hooks said that he could have sentenced her to prison, but did not because \"I'm not sure if I did that you'd be coming out any better.\"\n\nOn April 19, 2018, Tanishia Covington plead guilty to the charges of committing a hate crime, intimidation and aggravated battery and was sentenced to three years in prison.\n\nOn July 5, 2018, Jordan Hill plead guilty to the charges of aggravated kidnapping and committing a hate crime and was sentenced to eight years in prison.\n\nOn July 12, 2018, Tesfaye Cooper (20yo) plead guilty to a hate crime and aggravated kidnapping and has been sentenced to seven years in prison.\n\nOn January 3, at approximately 5:15 p.m., Harrison District Officer Michael Donnelly saw the victim walking with the suspect that the victim had gone to high school with. The victim was observed by Officer Donnelly to be wearing summer clothing during winter conditions. Police said the victim appeared \"injured\" and \"confused\". After checking the victim's name through police databases it was discovered that he had been reported missing. Donnelly later stated, \"I observed him wearing a tank top, inside-out, backwards, jean shorts and sandals on...He was bloodied. He was battered. He was very discombobulated.\" Running the victim's name through police databases, Officer Donnelly discovered that the victim was reported as a missing person and brought him to the hospital.\n\nThe live stream was later deleted, but archives still exist. There was widespread outrage over the beating. In its aftermath, the hashtag #BLMKidnapping was trending on Twitter, implying a connection with the Black Lives Matter (BLM) movement. None of the attackers specifically mentioned Black Lives Matter in the video and the police found there to be no direct connection. Representatives for Black Lives Matter's Chicago branch denounced the beating and stated that they were uninvolved, and police stated that they found no evidence that Black Lives Matter was the motive of the incident. Some far right media outlets and conspiracy theorists (including Infowars and Glenn Beck) suggested that the rhetoric of Black Lives Matter and its supporters had encouraged the attackers. Other commentators disputed this.\n\nPresident Barack Obama released a statement condemning the incident, saying, \"What we have seen as surfacing, I think, are a lot of problems that have been there a long time... Whether it's tensions between police and communities, (or) hate crimes of the despicable sort that has just now recently surfaced on Facebook.\" Chicago Mayor Rahm Emanuel said, \"Anyone who has seen it [finds the video] both sickening and sickened by it,\" while Illinois Governor Bruce Rauner and his wife Diana said that they were \"deeply saddened and disturbed by the horrific violence\" depicted in the live stream.\n"}
{"id": "30744522", "url": "https://en.wikipedia.org/wiki?curid=30744522", "title": "Applications of evolution", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n"}
{"id": "46736055", "url": "https://en.wikipedia.org/wiki?curid=46736055", "title": "Atmospheric lidar", "text": "Atmospheric lidar\n\nAtmospheric lidar is a class of instruments that uses laser light to study atmospheric properties from the ground up to the top of the atmosphere. Such instruments have been used to study, among other, atmospheric gases, aerosols, clouds, and temperature.\n\nThe basic concepts to study the atmosphere using light were developed before World War II. In 1930, E.H. Synge proposed to study the density of the upper atmosphere using a searchlight beam \n. In the following years, searchlight beams were used to study cloud altitude using both scanning and pulsed light. Advanced techniques to study cloud properties using scattered light with different wavelengths were also proposed. With the first experiments, light scattering patterns were observed in the troposphere that were not compatible with a pure molecular atmosphere. This incompatibility was attributed to suspended haze particles.\nSimilar techniques were also developed in the U.S.S.R. The searchlight beam technique continued to improve after the end of the War, with more precise instruments and new atmospheric parameters, like temperature At the same time, pulsed light was used to construct a rangefinder to measure the distance of objects, but remained only an experimental design.\n\nIn 1960, T. Maiman demonstrated the first functional laser at Hughes Research Laboratories. The demonstration was a pivotal moment for lidar development. Soon afterwards, engineers at Hughes Aircraft Company developed a laser rangefinder using ruby laser light.\nThe new device, named colidar (coherent light detection and ranging), gained widespread publicity\n. In 1962, L. Smullin and G. Fiocco used a ruby laser to detect echoes from the Moon. During their experiments they observed light scattered in the upper atmosphere that they attributed to dust particles. Soon, several research groups constructed similar devices to observe the atmosphere. By 1969, “over 20 lasers were in use by meteorologists in the United States on at least a semi-routine basis” for various applications including aerosol measurements, sub-visible cirrus and noctilucent clouds observations, and visibility measurement\n\nA simplified representation of a lidar set-up is demonstrated in Figure 1. The transmission unit consists of a laser source, followed by a series of mirrors, and a beam expander which sends the collimated light beam vertically up to the open atmosphere. Part of the transmitted radiation is scattered by atmospheric components (i.e., gases, molecules, aerosols, clouds) backward to the lidar, where it is collected by a telescope. The backscattered light is driven to an optical analyzer where the optical signal is first spectrally separated, amplified and transformed to an electrical signal. Finally, the signal is digitized and stored in a computer unit.\n\nLidars have been proven useful for classification of cloud types (i.e., cumuli versus cirrus). Cloud boundaries can be retrieved from a ground-based lidar operating at a visible and/or near-infrared band. Cloud-base height can be identified by the time difference between the transmittance of the laser pulse to the sky and the detection of the backscattered light by the telescope. The laser beam is always attenuated when it penetrates through the clouds. However, when a powerful laser (e.g., Nd:YAG laser with high energy per pulse) is used, cloud tops can be retrieved too. Another physical parameter that can be retrieved is the cloud phase. By using a linear polarized laser beam, a linear particle depolarization ratio (δ) can be defined as the ratio of measured perpendicular backscatter intensity over parallel backscatter intensity with respect to the transmitter polarization axis:\nformula_1\n\nWhen this parameter is zero (the backscattered signal is linearly polarized), the cloud contains liquid spherical droplets. However, when the cloud contains ice crystals, backscattered light arrives at the receiver unit with a cross-polarized component, and δ has a higher value (0 < δ < 1). Liquid droplets tend to behave as symmetrical scattering elements, while ice crystals are asymmetrical.\n\nThe use of the polarization ratio generally includes an implicit assumption that the particles in the volume are randomly oriented. The polarization properties of oriented particles cannot be properly represented by the depolarization ratio. Ice crystals are known to horizontally orient when they are large enough that drag forces overcome the randomizing effects of Brownian motion. Rain is also generally oriented, where drag forces flatten the drops along the fall direction. In such cases, the measured depolarization ratio may depend on the particular polarization state used by the lidar system. Some polarization lidar systems can measure the entire backscatter phase matrix, thereby avoiding the ambiguity of the depolarization ratio when oriented particles are present.\n\nOne of the biggest uncertainties for climate change is the importance of aerosol direct and indirect effects. The uncertainties were stressed in the 4th Assessment Report by the Intergovernmental Panel for Climate Change (IPCC). The large diversity of aerosol optical properties, including their sources and the meteorological processes they are subjected to, requires vertically resolved measurements, which can only be performed with routine lidar observations. Networks of aerosol lidars such as the European Aerosol Research Lidar Network (EARLINET) were established to investigate aerosol properties, along with transport and modification phenomena, in a coherent way on a regional to continental scale. As of 2015, EARLINET consists of 27 lidar stations hosting more than 44000 profiles.\nElastic-backscatter lidars (EBL) have been used extensively to investigate clouds and aerosol layers since the 1960s. EBLs detect the total backscattered signal (particle and molecular contributions). Profiles of the extinction coefficient have to be estimated using the molecular signal and the assumption of a conditionally “constant” (roughly speaking) aerosol extinction to backscatter ratio, called the lidar ratio. The main equation involved, known as the lidar equation is:\nwhere \"P(r)\" is the power of the backscattered radiation received by the lidar telescope in distance \"r\", \"E\" is transmitted laser-pulse energy, \"L\" is the lidar constant summarizing its optical and detection characteristics, \"O(r)\" is the overlap function, and formula_2 and formula_3 are the aerosol/molecular backscatter- and extinction coefficient respectively. Molecular backscatter and extinction can be derived by meteorological data, therefore the only unknowns in the lidar equation are formula_2 and formula_3. However the lidar ratio, as an intensive aerosol property, strongly depends on the size, morphology and chemical composition of the particles and is highly variable with respect to height, which often risks the extinction profile credibility. The process for calculating backscatter- and extinction coefficient profiles from EBL returns is widely known as the Klett method and was originally formalised by Hitschfeld and Bordan in 1954. \nThe aforementioned defect on estimating extinction profiles is overcome by Raman (inelastic) backscatter lidar and high spectral resolution lidar (HSRL). Raman lidar works by additionally measuring the inelastic backscatter by nitrogen and/or oxygen molecules. HSRL uses a processing approach but obtains the additional measure of molecular only backscatter at the transmitted wavelength by blocking the spectrally narrow aerosol returns and passing the spectrally broad molecular returns. These techniques provide a direct calculation of the extinction coefficient, eliminating the need for a lidar ratio assumption since any additional terms involved (e.g. the molecular extinction coefficient) are handled by meteorological (e.g. radiosoundings) and standard-atmosphere data. After some mathematical manipulations of the lidar equation the extinction-related equation reads:\n(r,{\\lambda _{inc}}) = \\frac{\\frac{\\mathrm{d}}}\\ln \\frac{N_\\mathrm{sca}}} - {\\alpha _\\mathrm{mol}}(r,{\\lambda _\\mathrm{inc}}) - {\\alpha _\\mathrm{aer}}(r,{\\lambda _\\mathrm{sca}})} {1 + {\\left( {\\frac}}} \\right)}^{\\AA (r)}},\nwhere the subscripts “inc” and “sca” refer to the incident laser light and the shifted backscattered light respectively (in HSRL these terms are the same thus further simplifying the equation, but the distinction is needed in the case of Raman lidar), N is the nitrogen / oxygen molecule number density and formula_6 is the Ångström exponent. A drawback of this method is the presence of a derivative in the resulting extinction coefficient formula () which results in potential numerical instability, introducing an active field of research.\n\nExtracting the microphysical properties of particles is motivated by the need for a deeper understanding of the effect of aerosols on climate by investigating their spatial and temporal variability. A key parameter is the distribution of the number of particles with respect to their size. Other microphysical parameters involving the characterization of aerosols are the mean (effective) radius, the total volume and surface-area concentration, the complex refractive index and the single-scattering albedo (climate forcing). While knowing the aerosol properties (forward problem) and predicting the lidar signal is a straightforward calculation, the inverse process is mathematically ill-posed (i.e., non-unique and incomplete solution space), showing a strong sensitivity on input uncertainties.\nOptical parameters can be obtained from measurements using multi-wavelength elastic-Raman lidar systems. The parameters are used as inputs to the inversion algorithms. \nThe extinction (formula_7) and backscatter (formula_8) coefficients measured by a multi-wavelength (formula_9 ) lidar is related to the number size distribution via the Fredholm integral equation of the first kind:\n(\\lambda ) = \\int\\limits_}^} (r,\\lambda ;m)n(r)dr} ,\nwhere r is particle radius, m is the complex refractive index, and ? are the kernel functions which summarize the size, shape and composition of particles. The non-linear dependence on the refractive index is usually tackled by assuming a grid of viable options. The solution space is built and further restricted by physical and/or mathematical constraints and the particle size bounds formula_10 are also pre-determined. The model Eq. () further assumes a wavelength-independent refractive index.\nThe wavelength is restricted to several discrete values depending on current technology and availability of the lidar system. The minimum optical data setup consists of 5 values, where formula_11 nm, formula_12.\nEq. () has to be discretized as it cannot be solved analytically. The theory of inverse ill-posed problems demonstrates that potential noisy components in the lidar data will cause the solution to blow up, regardless of the error level magnitude. Regularization methods are used to counteract the inherent instability of the inversion. The goal of these methods is to filter out the noisy components of the solutions, keeping at the same time as much of the solution content as possible. The ideal compromise between noise and regularity is expressed by the so-called parameter choice rules. Commonly used regularization methods are the Truncated Singular Value Decomposition, Tikhonov regularization combined with the Discrepancy Principle, the L-curve method or the Generalized Cross Validation method as a parameter choice rule. \nWhile the model Eq. () offers a reasonable approximation for almost-spherical particles (e.g. biomass burning aerosols), it no longer provides a viable description for the non-spherical case. Particle shape is known to have substantial effects for the scattering in side- and backward direction. Recent studies show that the spheroidal particle approximation is able to reproduce the optical data much more accurately than spheres.\n\nLidar systems can be used to measure concentration profiles of atmospheric gases (i.e., water vapor, ozone), and industrial emissions (i.e., SO, NO, HCl). Such measurements are performed using two basic types of lidar; Raman lidar and Differential Absorption lidars (DIAL). In the first type, the Raman lidar detects the scattering of the laser beam due to Raman scattering. The frequency shift induced by such scattering is unique for each molecule, and acts as a “signature” to detect its specific contribution. The second type, DIAL systems, emit two beams with two distinct frequencies. One beam is tuned exactly on a molecular absorption line and the other beam is tuned in a nearby wavelength without molecular absorption. By examining the intensity difference of the scattered light at the two frequencies, DIAL systems can separate the contribution of the specific molecule in the atmosphere.\n\nLidar systems can measure atmospheric temperature from the ground up to approximately 120 km using a variety of techniques, each adapted for a specific altitude range \nMeasuring temperature in the lower part of the atmosphere is typically done by taking advantage of temperature-dependent changes in molecular scattering properties. Rotational Raman systems can take advantage of the temperature-dependent intensity of the rotational Raman band of laser light scattered from reference gases like nitrogen and oxygen\n. By precisely measuring only this scattered light, such systems can determine the temperature profile up to 40 km during night and up to 12 km during day. In rare cases, DIAL systems are also used to retrieve temperature profiles, which take advantage of the temperature-dependent shape of specific molecular absorption lines.\n\nEBLs are used to derive temperature profiles from the upper atmosphere (~ 30 km to ~ 100 km). Without the presence of clouds or aerosol, the backscattered laser light from these altitudes is only due to molecular scattering. The received signal is proportional to molecular numerical density, which is in turn connected to temperature based on the ideal gas law. Temperature profiles at higher altitudes, up to 120 km, can be derived by measuring the broadening of absorption spectra of atoms of metals such as Na, Ca, K, Fe. State-of-the-art lidar systems can combine several of these techniques in one system and deliver temperature profiles from the ground up to approximately 90 km altitude \n\nLidars are capable of retrieving the complete wind vector based on the optical Doppler effect. The so-called Doppler lidars can capture the movement of molecules and particles by detecting the frequency shift of the backscattered light. In particular, supposing that the emitting radiation is at a frequency f=c/λ, where λ is the wavelength of the laser beam, for a moving target (i.e., aerosol particle or molecule) with a relative line-of-sight velocity v, the backscattered light detected by the lidar receiver has a frequency shift equal to Δf=2v/c. The particle velocity is defined where a positive line-of-sight velocity means that a target is moving towards the lidar and leads to a positive frequency shift. In literature regarding lidar applications, the line-of-sight velocity is always referred as radial velocity. The magnitude of the shift can be detected by several methods, the major being coherent and direct-detection detection techniques \nWhen aerosols are used as tracers, the strength of the return signal depends upon the aerosol load in the atmosphere and this is known to be dependent upon geographic location, the condition of the atmosphere, and the synoptic situation. The operational wavelength can be any wavelength sensitive to the underlying particle sizes. In general, aerosol return improves at lower wavelengths in the UV band. Nevertheless, the lidar signal gets more sensitive to air molecules in the UV band, and an expected aerosol-to-molecule backscatter ratio is harder to be met. Doppler lidars are usually pointed to zenith and provide vertically-resolved profiles of the vertical wind component. Scanning techniques are applied to retrieve the horizontal wind component.\nSeveral such systems are operated from the ground for applications related to e.g. airports, wind-farms, study of the Planetary Boundary Layer turbulence etc. The ADM-Aeolus satellite mission of the European Space Agency, will be the first wind lidar to operate from space.\n\nJAXA and Mitsubishi Electric are developing the SafeAvio airborne lidar to halve accidents due to clear-air turbulence.\nThe 1.9 kW, 148-kg (325-lb.) prototype has a spatial resolution of 300 m (980 ft.) and a 1-30-km (0.5-16-nmi) remote sensing range reduced to 9 km at 40,000 ft.\nIt will alert crews to tell passengers to fasten seatbelts, before developing automatic attitude control to minimize shaking.\nThe prototype was flight-tested in Boeing’s 777F EcoDemonstrator in March 2018, goals and requirements should be determined by March 2019, and a feasibility report should be completed by March 2020 before a decision to develop the system.\n\nLidars take advantage of resonance scattering in the upper atmosphere to detect metallic atoms. In such systems, the emitted laser light has to be precisely tuned in the resonance frequency of the studied species\nThe first such measurements were the detection of atomic layers of metallic Sodium (Na) in the mesopause. The same technique is now applied to detect metallic Potassium (K), Lithium (Li), Calcium (Ca), and Calcium ion (Ca ion), and Iron (Fe). These measurements provide important information in an under-studied region of the atmosphere and have helped increase the knowledge on species concentration, origin, and the complex atmospheric dynamics at these altitudes.\n\nThe planetary boundary layer (PBL) is the part of the troposphere that is directly influenced by the presence of the earth’s surface, and responds to surface forcings with a timescale of about an hour or less\n. Convective turbulent mixing processes are dominant in the mixed layer (ML) of the PBL and have a major influence on the growth and transport of atmospheric pollutants. Meteorological variables (i.e. temperature, humidity, wind) in the PBL are critically important as inputs for reliable simulations in air quality models. One of the key parameters which determine the vertical extent of the ML is the PBL height.\n\nFrom an observational perspective, PBL height has historically been measured with radiosondes but in recent years remote sensing instruments such as lidar have been utilized. Since it is well known that PBL height varies greatly in both time and space, on the order of a few meters and several minutes, radiosoundings aren’t the optimal choice for observations of PBL height. The concept of using lidar to detect PBL height relies on the assumption that there is a strong gradient in the concentration of aerosols in the ML versus the free atmosphere. An advantage of using remote sensing instruments over radiosondes for detection of the PBL height is the possibility of nearly continuous monitoring versus typical observations of twice per day from radiosondes. Continuous monitoring of PBL height will allow for a better understanding of the depth of convective turbulent processes in the ML which are a primary driver of air pollutants.\n\nThe depth of the PBL is defined as the height of the inversion level separating the free troposphere (FT) from the boundary layer. Normally at the top of the PBL, buoyancy flux reaches a minimum and large gradients of potential temperature, water vapor, and aerosols are observed. Identifying an accurate position of the depth of the PBL is essential for reliable representation of parameters in meteorological and air quality models as the PBL is the region of maximum turbulence.\nIt is well known that convective mixing processes are predominant in the PBL which in result influences the structure and composition of aerosols. Knowing the vertical extent of the convective mixing will allow a more accurate depiction of the atmosphere in the boundary layer. \nIn recent years, remote sensing instruments such as lidar have been employed to identify and observe the PBL height. An advantage to using lidar is its high-resolution temporal and vertical spatial coverage which can possibly be operated continuously and in a nearly automated status. Thus, an instantaneous PBL height can be recorded which allows more in-depth analysis such as diurnal evolution and long-term climate studies.\n\nSeveral methods have been applied to determine the PBL height from lidar observations. They are both objective and subjective methods. Objective methods consist of various forms of derivative methods, wavelet analysis methods, the variance method, and the ideal profile fitting method. Visual inspection methods are infrequently used as a subjective approach but they are not the best approach.\n\nCeilometers are a ground based Lidar optimised for measurement of cloud on the approach path of aircraft, they can also be used for PBL studies.\n\n"}
{"id": "43523820", "url": "https://en.wikipedia.org/wiki?curid=43523820", "title": "Caliber (mathematics)", "text": "Caliber (mathematics)\n\nIn mathematics, the caliber or calibre of a topological space \"X\" is a cardinal \"κ\" such that for every set of \"κ\" nonempty open subsets of \"X\" there is some point of \"X\" contained in \"κ\" of these subsets. This concept was introduced by .\n\nThere is a similar concept for posets. A pre-caliber of a poset \"P\" is a cardinal \"κ\" such that for any collection of elements of \"P\" indexed by \"κ\", there is a subcollection of cardinality \"κ\" that is centered. Here a subset of a poset is called centered if for any finite subset there is an element of the poset less than or equal to all of them.\n\n"}
{"id": "18025004", "url": "https://en.wikipedia.org/wiki?curid=18025004", "title": "Clandestine abuse", "text": "Clandestine abuse\n\nClandestine abuse is sexual, psychological, or physical abuse \"that is kept secret for a purpose, concealed, or underhanded.\"\n\nChild sexual abuse is often kept secret:\n\nWhile it is not always possible to stop every case of clandestine abuse, it may be possible to prevent many incidents through policies in youth organizations.\n\nThe social isolation model asserts that:\nThe BSA policy states:\nOther policies of the BSA state:\nA person, especially a child, may be abused in secret because the victim has witnessed \"another\" clandestine crime, such as a working Methamphetamine laboratory. The FBI concluded that \"A coordinated multidisciplinary team is critical to ensure that the needs of meth’s youngest victims are met and that adequate information is available to prosecute child endangerment cases successfully.\"\n\n\n"}
{"id": "5620190", "url": "https://en.wikipedia.org/wiki?curid=5620190", "title": "Class (locomotive)", "text": "Class (locomotive)\n\nClass (locomotive) refers to a group of locomotives built to a common design for a single railroad. Often members of a particular class had detail variations between individual examples, and these could lead to subclasses. Sometimes technical alterations (especially rebuilding, superheating, re-engining, etc.) move a locomotive from one class to another. Different railways had different systems, and sometimes one railway (or its successors) used different systems at different times and for different purposes, or applied those classifications inconsistently. Sometimes therefore it is not clear where one class begins and another ends. The result is a classic example of the Lumper splitter problem.\n\nAs locomotives became more numerous the need arose to deal with them in groups of similar engines rather than as named or numbered individuals. These groups were named \"classes\" and at first tended to reflect capability rather than design. For example, the Baltimore and Ohio Railroad grouped its roster into four classes before the Civil War, though they had by that point dozens of different designs.\n\nLater classes were based on design. A group of locomotives built off the same blueprints constituted a class, and if some of the locomotives in the class were sufficiently modified, a new class might be established for the modified examples. When electric locomotives were introduced, the same scheme was applied to them.\n\nSince steam and early electric locomotives were usually custom built, classes were assigned by the railroad, and each railroad had its own system. Mergers of lines and sales of locomotives brought about changes of class. Early diesels were often fitted into the locomotive class system, but since they were generally not custom built the use of manufacturer model designations overtook the class system and made it irrelevant, except for historical discussion.\n\nUsually the class system for a railroad was built on a simply hierarchy which assigned each class a code.\n\nThe first level was usually for the wheel arrangement and was usually coded by a letter of the alphabet. Different railroads used different codes, so that \"J\" on the New York Central Railroad meant a 4-6-4 (Hudson), while on the Norfolk and Western Railway it mean a 4-8-4 (Northern), and on the Baltimore and Ohio Railroad it denoted a 4-4-0 with a Wootten firebox.\n\nArticulated locomotives were handled through two different methods. On many railroads each wheel arrangement was assigned its own unique letter, due to the limited number of arrangements that had to be represented. On other railroads this was insufficient, and commonly articulated locomotives were represented with a two letter code, one letter for the arrangement of each half. Therefore, the Pennsylvania Railroad GG1 (UIC 2Co+Co2) is represented as if it were two ten-wheelers (4-6-0) coupled tail-to-tail.\n\nA sequence number was often added to distinguish different designs of the same wheel arrangement. As a rule the first design for a given arrangement had no sequence number, so that numbering stated at 1 with the second design for the wheel arrangement. For example, there were two main classes of 2-10-2 locomotives on the B&O, labelled \"S\" and \"S-1\".\n\nLetter suffixes were often used to indicate variants in a basic design. Sometimes these also referred to specific characteristics; for example, for many years the Pennsylvania Railroad used a \"s\" suffix to indicate superheating, while on the B&O a \"t\" suffix indicated an engine with an oversize tender.\n\nSub-classes are groups of locomotives that many have different mechanical characteristics between the sub-class of locomotives and the original design of those locomotives.\nFor example: when NSWGR C30 class suburban tank engines where displaced because of the electrification of Sydney suburban railways, many were converted to tender engines adding the prefix of “t” to C30. Some were even converted to superheighted engines, adding another prefix of “s” to either C30 or C30T to become C30S or C30TS.\n\nMost locomotives were given simple codes, but some classes were named, formally or not.\n\n\nThe old railway companies had various systems of classification. Taking the \"Big Four\" companies which operated from 1923 to 1947:\n\nThe class number was usually taken from the first member of each class, e.g. \"5700 Class\" or \"57XX Class\" for locomotives in the number series beginning 5700.\n\nEach class was given a letter or number but these were not very meaningful. For example, \"700\" Class locomotives were 0-6-0s, but so were \"Q\" Class engines. See:\n\nEach locomotive was given a power classification, e.g. \"3F\". However, many different classes would have the same power classification so this was not helpful for identifying classes.\n\nThe LNER's classification system was the most helpful to railway enthusiasts. Each wheel arrangement was given a letter (e.g. A for 4-6-2, B for 4-6-0, etc.) and this was followed by a number denoting the class (e.g. A1, B1, etc.).\n\n"}
{"id": "1299398", "url": "https://en.wikipedia.org/wiki?curid=1299398", "title": "Clean hands", "text": "Clean hands\n\nClean hands, sometimes called the clean hands doctrine or the dirty hands doctrine, is an equitable defense in which the defendant argues that the plaintiff is not entitled to obtain an equitable remedy because the plaintiff is acting unethically or has acted in bad faith with respect to the subject of the complaint—that is, with \"unclean hands\". The defendant has the burden of proof to show the plaintiff is not acting in good faith. The doctrine is often stated as \"those seeking equity must do equity\" or \"equity must come with clean hands\". This is a matter of protocol, characterised by A. P. Herbert in \"Uncommon Law\" by his fictional Judge Mildew saying (as Herbert says, \"less elegantly\"), \"A dirty dog will not have justice by the court\".\n\nThe clean hands doctrine is used in U.S. patent law to deny equitable or legal relief to a patentee that has engaged in improper conduct, such as using the patent to extend monopoly power beyond the claims of the patent.\n\nA defendant's unclean hands can also be claimed and proven by the plaintiff to claim other equitable remedies and to prevent that defendant from asserting equitable affirmative defenses. In other words, 'unclean hands' can be used offensively by the plaintiff as well as defensively by the defendant. Historically, the doctrine of unclean hands can be traced as far back as the Fourth Lateran Council.\n\nEquitable remedies are generally remedies other than the payment of damages. This would include such remedies as obtaining an injunction, or requiring specific performance of a contract. Before the development of the courts of equity in England, such remedies were unavailable in the common law courts, which were usually under the aegis of the local ducal noble, whereas equity flowed from the circuit-riding presence of the King's Chancellor who brought along his retainers to enforce his orders (in this sense, the development of the equity courts was a political effort by the Crown to further limit the power of local nobles). Such remedies were developed in the equity courts, as the payment of damages was often not a sufficient remedy for a plaintiff in certain circumstances. For example, if a landowner polluted the land of the neighbor, the common law tort of nuisance would only allow the innocent party to recover damages. Common law had no remedy that would force the defendant to stop the pollution. Equity courts developed such a remedy, the injunction, that provided an ongoing bar to the activity that caused the damage, as well as affirmative orders called mandamus which compelled a directed party to do a certain thing (which the Chancellor could enforce while there). \n\nEquity courts realized that such extraordinary remedies were only justified in extraordinary cases, and would generally not grant such a remedy where damages were sufficient to make the plaintiff whole. For example, if a car dealership broke a contract of sale and refused to deliver a particular car, which now could only be obtained for $10,000 more than what the plaintiff was willing to pay, the courts would merely award the plaintiff $10,000 (in addition to the original amount paid, if it had already been paid). It would not force the dealer to obtain exactly the same car and sell it to the plaintiff. However, if the subject matter of the sale were unique, such as a particular work of art or real estate, the court would order specific performance and require the sale.\n\nHowever, equity courts also realized that these extraordinary remedies were subject to abuse. For example, if a doctor had signed a non-compete clause with a clinic, the non-compete clause might prevent the doctor from earning a living if he left the clinic's employment. As such, the court will generally only grant these remedies on the strictest terms. If there is any indication that the plaintiff seeking the remedy had acted in bad faith, either prior to the commencement of the litigation or afterwards, the court will generally not grant the remedy. For example, if the doctor left the clinic because it was involved in insurance fraud, a court would most likely refuse to enforce the non-compete agreement by issuing an injunction, although it might allow the clinic to recover damages if they did lose business to the doctor. Much of the various limitations, often expressed as \"maxims of equity\", arose from the sheer physical fact that the Chancellor and his enforcers would soon be moving on to another locale, and thus equity tended to be careful in allowing access to its powerful remedies and to order only that which would be quickly or effectively enforced.\n\n"}
{"id": "49899", "url": "https://en.wikipedia.org/wiki?curid=49899", "title": "Courage", "text": "Courage\n\nCourage (also called bravery or valour) is the choice and willingness to confront agony, pain, danger, uncertainty, or intimidation. Physical courage is bravery in the face of physical pain, hardship, death or threat of death, while moral courage is the ability to act rightly in the face of popular opposition, shame, scandal, discouragement, or personal loss.\n\nThe classical virtue of fortitude (\"andreia, fortitudo\") is also translated \"courage\", but includes the aspects of perseverance and patience.\n\nIn the Western tradition, notable thoughts on courage have come from philosophers, Socrates, Plato, Aristotle, Aquinas, and Kierkegaard.\n\nMuch earlier, in the Hindu tradition, mythology has given many examples of bravery, valour and courage. Ramayana and Mahabharatha have in them many examples of both physical and moral courage.\n\nIn the Eastern tradition, some thoughts on courage were offered by the \"Tao Te Ching\". More recently, courage has been explored by the discipline of psychology.\n\nDaniel Putman, a professor at the University of Wisconsin - Fox Valley, wrote an article titled \"The Emotions of Courage\". Using a text from Aristotle's \"Nicomanachean Ethics\" as the basis for his article, he discusses the relationship between fear and confidence in the emotion of courage. \n\nHe states that \"courage involves deliberate choice in the face of painful or fearful circumstances for the sake of a worthy goal\". With this realization, Putman concludes that \"there is a close connection between fear and confidence\". \n\nFear and confidence in relation to courage can determine the success of a courageous act or goal.They can be seen as the independent variables in courage, and their relationship can affect how we respond to fear. In addition, the confidence that is being discussed here is self-confidence; Confidence in knowing one's skills and abilities and being able determine when to fight a fear or when to flight it. Putman states that:\nWhen trying to understand how fear and confidence play into courage, we need to look back at Aristotle's quote. According to Putman, Aristotle is referring to an appropriate level of fear and confidence in courage. \"Fear, although it might vary from person to person, is not completely relative and is only appropriate if it \"matches the danger of the situation\".\"The same goes for confidence in that there are two aspects to self-confidence in a dangerous situation.\n\nWithout an appropriate balance between fear and confidence when facing a threat, one cannot have the courage to overcome it. Putman states \"if the two emotions are distinct, then excesses or deficiencies in either fear or confidence can distort courage.\"\n\nAs noted above, an \"excess or deficiency of either fear or confidence, can distort courage\". According to Putman, there are four possibilities:\n\nPutman explains each of these possibilities accordingly: an individual in the first possibility is perceived as a coward; in the second possibility a rash person. . For the remaining two possibilites, he uses analogies to explain them. The third possibility can occur if someone experienced a traumatic experience that brought about great anxiety for much of their life.Then the fear that they experience would often be inappropriate and excessive. Yet as a defensive mechanism, the person would show excessive levels of confidence as a way to confront their irrational fear and \"\"prove\" something to oneself or other\". So this distortions could be seen as a coping method for their fear. For the last possibility, it can be seen as hopelessness. Putman says this is similar to \"a person on a sinking ship\". \"This example is of a person who has low confidence and possibly low self-regard who suddenly loses all fear\". The distortion of low fear and low confidence can occur in a situation where an individual accepts what is going to happen to them. In regards to this example, they lose all fear because they know death is unavoidable and the reason it is unavoidable is because they do not have the ability to handle or overcome the situation.\n\nThus, Daniel Putman identifies fear and courage as being deeply intertwined and that they rely on distinct perceptions: \n\nThe early Greek philosopher Plato (c. 428–348 BCE) set the groundwork for how courage would be viewed to future philosophers. Plato's early writings found in \"Laches\" show a discussion on courage, but they fail to come to a satisfactory conclusion on what courage is.\n\nDuring the debate between three leaders, including Socrates, many definitions of courage are mentioned.\n\nWhile many definitions are given in Plato's \"Laches\", all are refuted, giving the reader a sense of Plato's argument style. \"Laches\" is an early writing of Plato's, which may be a reason he does not come to a clear conclusion. In this early writing, Plato is still developing his ideas and shows influence from his teachers like Socrates.\n\nIn one of his later writings, \"The Republic\", Plato gives more concrete ideas of what he believes courage to be. Civic courage is described as a sort of perseverance – \"preservation of the belief that has been inculcated by the law through education about what things and sorts of things are to be feared\". Ideas of courage being perseverance also are seen in \"Laches\". Plato further explains this perseverance as being able to persevere through all emotions, like suffering, pleasure, and fear.\n\nAs a desirable quality, courage is discussed broadly in Aristotle's \"Nicomachean Ethics\", where its vice of shortage is cowardice and its vice of excess is recklessness.\n\nIn the Roman Empire, courage formed part of the universal virtue of \"virtus\". Roman philosopher and statesman Cicero (106–43 BCE) lists the cardinal virtues does not name them such: \n\nIn medieval virtue ethics, championed by Averroes and Thomas Aquinas and still important to Roman Catholicism, courage is referred to as \"Fortitude\".\n\nAccording to Thomas Aquinas:\n\nPart of his justification for this hierarchy is that:\nOn fortitude's general and special nature, Aquinas says:\nAquinas holds fortitude or courage as being primarily about endurance, not attack:\n\nIn both Catholicism and Anglicanism, courage is also one of the seven gifts of the Holy Spirit. For Thomas Aquinas, Fortitude is the virtue to remove any obstacle that keeps the will from following reason. Thomas Aquinas argues that Courage is a virtue which, along with the Christian virtues in the Summa Theologica, can only be exemplified with the presence of the Christian virtues: faith, hope, and mercy. In order to understand true courage in Christianity it takes someone who displays the virtues of faith, hope, and mercy. Courage is a natural virtue which Saint Augustine did not consider a virtue for Christians. Thomas Aquinas considers courage a virtue through the Christian virtue of mercy. Only through mercy and charity can we call the natural virtue of courage a Christian virtue. Unlike Aristotle, Aquinas’ courage is about endurance, not bravery in battle.\n\nThe \"Tao Te Ching\" contends that courage is derived from love (\"慈 loving 故 causes 能 ability 勇 brave\"), explaining, \"One of courage, with audacity, will die. One of courage, but gentle, spares death. From these two kinds of courage arise harm and benefit.\"\n\nIn Hindu tradition, Courage (shauriya) / Bravery (dhairya), and Patience (taamasa) appear as the first two of ten characteristics (lakshana) of dharma in the Hindu Manusmṛti, besides forgiveness (kshama), tolerance (dama), honesty (asthaya), physical restraint (indriya nigraha), cleanliness (shouchya), perceptiveness (dhi), knowledge (vidhya), truthfulness (satya), and control of anger (akrodh).\n\nIslamic beliefs also present courage and self-control as a key factor in facing the Devil (both within and external); many believe this because of the courage the Prophets of the past displayed (through peace and patience) against people who despised them for their beliefs.\n\nThomas Hobbes lists virtues into the categories of moral virtues and virtues of men in his work \"Man and Citizen.\" Hobbes outlines moral virtues as virtues in citizens, that is virtues that without exception are beneficial to society as a whole. These moral virtues are justice (i.e. not violating the law) and charity. Courage as well as prudence and temperance are listed as the virtues of men. By this Hobbes means that these virtues are invested solely in the private good as opposed to the public good of justice and charity. Hobbes describes courage and prudence as a strength of mind as opposed to a goodness of manners. These virtues are always meant to act in the interests of individual while the positive and/or negative effects of society are merely a byproduct. This stems forth from the idea put forth in \"Leviathan\" that the state of nature is \"solitary, poor, nasty, brutish and short.\" According to Hobbes courage is a virtue of the individual in order to ensure a better chance of survival while the moral virtues address Hobbes's social contract which civilized men display (in varying degrees) in order to avoid the state of nature. Hobbes also uses the idea of fortitude as an idea of virtue. Fortitude is \"to dare\" according to Hobbes, but also to \"resist stoutly in present dangers.\" This a more in depth elaboration of Hobbes's concept of courage that is addressed earlier in \"Man and Citizen.\" This idea relates back to Hobbes's idea that self-preservation is the most fundamental aspect of behavior.\n\nDavid Hume listed virtues into two categories in his work \"A Treatise of Human Nature\" as artificial virtues and natural virtues. Hume noted in the Treatise that courage is a natural virtue. In the Treatise's section \"Of Pride and Humility, Their Objects and Causes\", Hume clearly stated courage is a cause of pride: \"Every valuable quality of the mind, whether of the imagination, judgment, memory or disposition; wit, good-sense, learning, courage, justice, integrity; all these are the cause of pride; and their opposites of humility\".\n\nHume also related courage and joy to have positive effects on the soul: \"(...) since the soul, when elevated with joy and courage, in a manner seeks opposition, and throws itself with alacrity into any scene of thought or action, where its courage meets with matter to nourish and employ it\". Along with courage nourishing and employing, Hume also wrote that courage defends humans in the Treatise: \"We easily gain from the liberality of others, but are always in danger of losing by their avarice: Courage defends us, but cowardice lays us open to every attack\".\n\nHume wrote what excessive courage does to a hero's character in the Treatise's section \"Of the Other Virtues and Vices\": \"Accordingly we may observe, that an excessive courage and magnanimity, especially when it displays itself under the frowns of fortune, contributes in a great measure, to the character of a hero, and will render a person the admiration of posterity; at the same time, that it ruins his affairs, and leads him into dangers and difficulties, with which otherwise he would never have been acquainted\".\n\nOther understandings of courage that Hume offered can be derived from Hume's views on morals, reason, sentiment, and virtue from his work \"An Enquiry Concerning the Principles of Morals.\"\n\nSøren Kierkegaard opposed courage to angst, while Paul Tillich opposed an existential \"courage to be\" with non-being, fundamentally equating it with religion:\nJ.R.R. Tolkien identified in his 1936 lecture \"\" a \"Northern 'theory of courage'\" – the heroic or \"virtuous pagan\" insistence to do the right thing even in the face of certain defeat without promise of reward or salvation:\nVirtuous pagan heroism or courage in this sense is \"trusting in your own strength,\" as observed by Jacob Grimm in his \"Teutonic Mythology\":\n\nErnest Hemingway famously defined courage as \"grace under pressure.\"\n\nWinston Churchill stated, \"Courage is rightly esteemed the first of human qualities because it is the quality that guarantees all others.\"\n\nAccording to Maya Angelou, \"Courage is the most important of the virtues, because without courage you can't practice any other virtue consistently. You can practice any virtue erratically, but nothing consistently without courage.\"\n\nIn \"Beyond Good and Evil\", Friedrich Nietzsche describes master–slave morality, in which a noble man regards himself as a \"determiner of values;\" one who does not require approval, but passes judgment. Later, in the same text, he lists man's four virtues as \"courage, insight, sympathy, and solitude,\" and goes on to emphasize the importance of courage: \"The great epochs of our life are the occasions when we gain the courage to re-baptize our evil qualities as our best qualities.\"\n\nAccording to the Swiss psychologist Andreas Dick, courage consists of the following components: \n\n1. put at risk, risk or repugnance, or sacrifice safety or convenience, which may result in death, bodily harm, social condemnation or emotional deprivation;\n\n2. a knowledge of wisdom and prudence about what is right and wrong in a given moment;\n\n3. Hope and confidence in a happy, meaningful outcome;\n\n4. a free will;\n\n5. a motive based on love.\n\nResearchers who want to study the concept and the emotion of courage have continued to come across a certain problem. While there are \"numerous definitions of courage\", they are unable to set \"an operational definition of courage on which to base sound explicit theories\". Rate et al. states that because of a lack of an operational definition, the advancement of research in courage is limited. So they conducted studies to try to find \"a common structure of courage\". Their goal from their research of implicit theories was to find \"people's form and content on the idea of courage\". Many researchers created studies on implicit theories by creating a questionnaire that would ask \"What is courage?\". In addition, in order to \"develop a measurement scale of courage, ten experts in the field of psychology came together to define courage. They defined it as:\nAlso, because courage is a \"multi-dimensional construct, it can be \"better understood as an exceptional response to specific external conditions or circumstances than as an attribute, disposition, or character trait\". Meaning that rather than being a show of character or an attribute, courage is a response to fear \n\nFrom their research, they were able to find the \"four necessary components of people's notion of courage\"..They are:\n\nWith these four components, they were able to define courage as:\nTo further the discussion of the implicit theories of courage, the researchers stated that future research could consider looking into the concept of courage and fear and how individual's might feel fear, overcome it and act, and act despite of it. \n\nIts accompanying animal is the lion. Often, fortitude is depicted as having tamed the ferocious lion. Cf. e.g. the Tarot trump called Strength. It is sometimes seen in the Catholic Church as a depiction of Christ's triumph over sin. It also is a symbol in some cultures as a savior of the people who live in a community with sin and corruption.\n\nSeveral awards claim to recognize courageous actions, including:\n\n\n\n"}
{"id": "17455275", "url": "https://en.wikipedia.org/wiki?curid=17455275", "title": "Display aspect ratio", "text": "Display aspect ratio\n\nThe aspect ratio of a display device is the proportional relationship between its width and its height. It is expressed as two numbers separated by a colon (x:y). Common aspect ratios for displays, past and present, include , , and .\n\nAs of 2016, most computer monitors use widescreen displays with an aspect ratio of 16:9, although some portable PCs use narrower aspect ratios like 3:2 and 16:10 while some high-end desktop monitors have adopted ultrawide displays.\n\nThe following table summarises the different aspect ratios that have been used in computer displays:\n\nUntil about 2003, most computer monitors had a 4:3 aspect ratio and some had 5:4. Between 2003 and 2006, monitors with aspect ratio became commonly available, first in laptops and later also in standalone computer monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen movie viewing and computer game play, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time. 16:10 became the most common sold aspect ratio for widescreen computer monitors until 2008.\n\nIn 2008, the computer industry started to move from 4:3 and 16:10 to 16:9 as the standard aspect ratio for monitors and laptops. A 2008 report by DisplaySearch cited a number of reasons for this shift, including the ability for PC and monitor manufacturers to expand their product ranges by offering products with wider screens and higher resolutions, helping consumers to more easily adopt such products and \"stimulating the growth of the notebook PC and LCD monitor market\".\n\nBy 2010, virtually all computer monitor and laptop manufacturers had also moved to the 16:9 aspect ratio, and the availability of 16:10 aspect ratio in mass market had become very limited. In 2011, non-widescreen displays with 4:3 aspect ratios still were being manufactured, but in small quantities. The reasons for this according to Bennie Budler, product manager of IT products at Samsung South Africa was that the \"demand for the old 'Square monitors' has decreased rapidly over the last couple of years\". He also predicted that \"by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand.\"\n\nIn 2012, 1920×1080 was the most commonly used resolution among Steam users. At the same time, the most common resolution globally was 1366×768, overtaking the previous leader 1024×768.\n\n3:2 displays first appeared in computers in 2013 with the Chromebook Pixel and later gained popularity in 2-in-1 PCs like Microsoft's Surface line. As of 2018, a number of manufacturers are either producing or planning to produce portable PCs with 3:2 displays.\n\nSince 2014, a number of high-end desktop monitors have been released that use ultrawide displays with aspect ratios that roughly match the various anamorphic formats used in film, but are commonly marketed as 21:9. Resolutions for such displays include 2560x1080, 3440x1440 and 3840x1600.\n\nIn 2017, Samsung released a curved gaming display with an aspect ratio of 32:9 and resolution of 3840x1080.\n\nSince 2011, several monitors complying with the Digital Cinema Initiatives 4K standard have been produced. The standard specifies a resolution of 4096×2160 and an aspect ratio of almost 1.9:1.\n\nFrom 2005 to 2013 most video games were mainly made for the 16:9 aspect ratio and 16:9 computer displays therefore offer the best compatibility. 16:9 video games are letterboxed on a 16:10 or 4:3 display or have reduced field of view. \n\nAs of 2013, many games are adopting support for 21:9 ultrawide resolutions, which can give a gameplay advantage due to increased field of view, although this is not always the case.\n\n4:3 monitors have the best compatibility with older games released prior to 2005 when that aspect ratio was the mainstream standard for computer displays.\n\nAs of 2017, the most common aspect ratio for TV broadcasts is 16:9, whereas movies are generally made in the wider 21:9 aspect ratio. Most modern TVs are 16:9, which causes letterboxing when viewing 21:9 content, and pillarboxing when viewing 4:3 content such as older films or TV broadcasts, unless the content is cropped or stretched to fill the entire display.\n\nMicrosoft recommends a 16:9 display for tablet computers running Windows 8.\n\nMicrosoft recommends a 16:9 display for Office 2013.\n\nFor viewing documents in A4 paper size (which has a 1.41:1 aspect ratio), whether in portrait mode or two side-by-side in landscape mode, 4:3 or 16:10 fits best. For photographs in the standard 135 film and print size (with a 3:2 aspect ratio), 16:10 fits best; for photographs taken with consumer-level digital cameras, 4:3 fits perfectly.\n\nThe size of a computer monitor is given as the diagonal measurement of its display area, usually in inches. Wider aspect ratios result in smaller overall area, given the same diagonal.\n\nUntil 2010, smartphones used different aspect ratios, including 3:2 and 5:3. Since then, most smartphone manufacturers have switched to using 16:9 widescreen displays, driven at least partly by the growing popularity of HD video using the same aspect ratio.\n\nSince 2017, a number of smartphones have been released using 18:9 or even wider aspect ratios (such as 18.5:9 or 19.5:9); such displays are expected to appear on increasingly more phones. Reasons for this trend include the ability for manufacturers to use a nominally larger display without increasing the width of the phone, as well as the 18:9 ratio being well-suited for VR applications and the proposed Univisium film format.\n\nMost televisions were built with an aspect ratio of 4:3 until the early 2010s, when widescreen TVs with 16:9 displays became the standard. This aspect ratio was chosen as the geometric mean between 4:3 and 2.35:1, an average of the various aspect ratios used in film. While 16:9 is well-suited for modern HDTV broadcasts, older 4:3 video has to be either padded with bars on both sides, cropped or stretched, while movies shot with wider aspect ratios are usually letterboxed, with black bars on top and bottom.\n\n"}
{"id": "39833408", "url": "https://en.wikipedia.org/wiki?curid=39833408", "title": "Dropmire", "text": "Dropmire\n\nDropmire is a surveillance program by the United States' National Security Agency (NSA) aimed at surveillance of foreign embassies and diplomatic staff, including those of NATO allies. The program's existence was revealed in June 2013 by whistleblower Edward Snowden in \"The Guardian\" newspaper. The report reveals that at least 38 foreign embassies were under surveillance, some of them as far back as 2007.\n\nEarlier in June 2013, \"The Guardian\" had reported that the NSA spied on diplomats during the 2009 G-20 London Summit, but no precise program name was revealed at the time.\n\nDiplomatic spying by the United States had been revealed as far back as 2010, when it was revealed that US agencies had spied on the Secretary-General of the United Nations, Ban Ki-moon – at the time, it was not known that this had been done as part of a systematic program.\n\n"}
{"id": "43721974", "url": "https://en.wikipedia.org/wiki?curid=43721974", "title": "E-dense semigroup", "text": "E-dense semigroup\n\nIn abstract algebra, an E\"-dense semigroup (also called an E\"-inversive semigroup) is a semigroup in which every element \"a\" has at least one weak inverse \"x\", meaning that \"xax\" = \"x\". The notion of weak inverse is (as the name suggests) weaker than the notion of inverse used in a regular semigroup (which requires that \"axa\"=\"a\").\n\nThe above definition of an \"E\"-inversive semigroup \"S\" is equivalent with any of the following:\n\nThis explains the name of the notion as the set of idempotents of a semigroup \"S\" is typically denoted by \"E\"(\"S\").\n\nThe concept of \"E\"-inversive semigroup was introduced by Gabriel Thierrin in 1955. Some authors use \"E\"-dense to refer only to \"E\"-inversive semigroups in which the idempotents commute.\n\nMore generally, a subsemigroup \"T\" of \"S\" is said dense in \"S\" if, for all \"x\" ∈ \"S\", there exists \"y\" ∈ \"S\" such that both \"xy\" ∈ \"T\" and \"yx\" ∈ \"T\".\n\nA semigroup with zero is said to be an \"E\"*-dense semigroup if every element other than the zero has at least one non-zero weak inverse. Semigroups in this class have also been called 0-inversive semigroups.\n\n\n\n"}
{"id": "37746017", "url": "https://en.wikipedia.org/wiki?curid=37746017", "title": "Emotional promiscuity", "text": "Emotional promiscuity\n\nEmotional promiscuity has been addressed in both the popular press as well as in scientific literature. \n\nIn the popular press, there was a book published in 2007 entitled, \"Avoid the Heartbreak of Emotional Promiscuity,\" written by Brienne Murk. In her book, Murk argues that (a) guarding one's heart is required for staying pure, (b) emotions cannot be trusted, and (c) that young people must set physical and emotional boundaries. Murk also attempts to discourage young people from having premarital sex and engaging in activities she deems impure. The book is written from a Christian perspective, and its assertions based on the author's religious interpretations.\n\nIn scientific literature, Jones (2011) examined the concept of \"Emotional Promiscuity\" as a personality/individual difference construct Jones defines emotional promiscuity as the predisposition to fall in love easily, fast, and often. Such individuals tend to love the feeling of falling in love, and want to be with many romantic partners.\n\nEmotional promiscuity can be assessed through self-report using the Emotional Promiscuity Scale (or EP Scale; and is distinct from both sexual promiscuity and anxious attachment \n\nIn addition, emotional promiscuity poses a health risk when combined with sexual promiscuity.\n\nIn particular individuals who fall in love frequently and easily, and who are also comfortable with casual sex, have many unprotected sexual partners. The authors argue this finding results from premature trust which eventuates into the faulty perception that protection is unnecessary.\n"}
{"id": "41461151", "url": "https://en.wikipedia.org/wiki?curid=41461151", "title": "Fixing Sex", "text": "Fixing Sex\n\nFixing Sex: Intersex, Medical Authority, and Lived Experience, a book by Stanford anthropologist and bioethicist Katrina Karkazis, was published in 2008. Described as \"thoughtful\", \"meticulous\", and an \"authoritative treatise on intersex\", the book examines the perspectives of intersex people, their families, and clinicians to offer compassionate look at the treatment of people born with atypical sex characteristics.\n\nIn a scholarly work, Karkazis draws heavily on interviews with intersex adults, parents, and physicians to explore how intersex is understood and treated. In part 1, she reviews the history of treatment for intersex traits, highlighting the work of John Money and the introduction of the, then new, terms \"gender\", \"gender role\" and \"gender identity\". She explores the events following publication of Milton Diamond's study of the David Reimer or \"John/Joan\" case, and the ways in which public opinion impacted on medical treatment. In part 2, Karkazis presents an analysis of current medical approaches to intersex, and the risks involved, in the wake of a 2006 \"consensus statement on the management of intersex disorders\". She also reviews the methods utilised to assign a sex of rearing to intersex infants, such as genitals and penis size, chromosomes, fertility, \"sexing of the brain\", and parental wishes; these impact upon determination whether or not to proceed with early genital surgery. Part 3 interviews parents of children with complete androgen insensitivity syndrome and congenital adrenal hyperplasia, and adults with intersex experiences. Part 3 also looks at activism by intersex organizations.\n\nThe book has been well received by both clinicians and intersex groups. Gary Berkovitz, writing in the New England Journal of Medicine states that Karkazis's analysis is fair, compelling, and eloquent; \"Current consensus guidelines recommend early separation of the vagina and urethra for female subjects with abnormalities in the formation of the sex organs... Karkazis presents a compelling argument for the deferment of subsequent surgery until the patient is able to decide.\" Elizabeth Reis, reviewing the book in American Journal of Bioethics, states that the book identifies risk of incontinence, fistulas, scarring and lack of physical sensation arising from surgical intervention, and the psychological harm caused by the knowledge that \"one's genitals are 'wrong,' requiring constant medical scrutiny and 'fixing'. It \"masterfully examines the concerns and fears of all those with a stake in the intersex debate: physicians, parents, intersex adults, and activists. . . . Karkazis’s honest, multi-pronged approach poses critical questions.\" Mijeon in American Journal of Human Genetics writes that the \"conclusion is quite fitting\", \"the history of thinking about the body ... can be highly politicized and controversial\". Kenneth Copeland MD, former president of the Lawson Wilkins Pediatric Endocrine Society describes the book as \"Masterfully balancing all aspects of one of the most polarizing, contentious topics in medicine... the most recent authoritative treatise on intersex.\"\n\nGayle Rubin describes the book as \"meticulous, sensitive, and brilliantly executed\". Arlene Baratz MD, (Accord Alliance) describes the book as \"a velvet-gloved punch to the gut\", \"astonishing, a tale told straight from the mouths of affected adults, parents, and physicians in tender and lyrical prose.\" Intersex community organization Organisation Intersex International Australia regards the book as \"approachable,\" \"compelling and recommended reading\".\n\nThe book was referenced by \"Involuntary or coerced sterilisation of intersex people in Australia,\" a 2013 report of a committee of the Senate of Australia in 2013.\n\nThe book was nominated for the Margaret Mead Award, 2010, and a finalist for the Lambda Literary Award, 2009.\n\n"}
{"id": "32967653", "url": "https://en.wikipedia.org/wiki?curid=32967653", "title": "Functional boxplot", "text": "Functional boxplot\n\nIn statistical graphics, the functional boxplot is an informative exploratory tool that has been proposed for visualizing functional data. Analogous to the classical boxplot, the descriptive statistics of a functional boxplot are: the envelope of the 50% central region, the median curve and the maximum non-outlying envelope.\n\nTo construct a functional boxplot, data ordering is the first step. In functional data analysis, each observation is a real function, therefore, different from the classical boxplot where data are simply ordered from the smallest sample value to the largest, in a functional boxplot, functional data, e.g. curves or images, are ordered by a notion of band depth or a modified band depth. It allows for ordering functional data from the center outwards and, thus, introduces a measure to define functional quantiles and the centrality or outlyingness of an observation. Having the ranks of functional data, the functional boxplot is a natural extension of the classical boxplot.\n\nIn the classical boxplot, the box itself represents the middle 50% of the data. Since the data ordering in the functional boxplot is from the center outwards, the 50% central region is defined by the band delimited by the 50% of deepest, or the most central observations. The\nborder of the 50% central region is defined as the envelope representing the box in a classical boxplot. Thus, this 50% central\nregion is the analog to the \"interquartile range\" (IQR) and gives a useful indication of the spread of the central 50% of the curves.\nThis is a robust range for interpretation because the 50% central region is not affected by outliers or extreme values, and gives a\nless biased visualization of the curves' spread. The observation in the box indicates the median, or the most central observation which is also a robust statistic to measure centrality.\n\nThe \"whiskers\" of the boxplot are the vertical lines of the plot extending from the box and indicating the maximum envelope of the\ndataset except the outliers.\n\nOutliers can be detected in a functional boxplot by the 1.5 times the 50% central region empirical rule, analogous to the 1.5 IQR empirical rule for classical boxplots. The fences are obtained by inflating the envelope of the 50% central region by 1.5 times the\nheight of the 50% central region. Any observations outside the fences are flagged as potential outliers. When each observation is simply a point, the functional boxplot degenerates to a classical boxplot, and it is different from the pointwise boxplots.\n\nBy introducing the concept of central regions, the functional boxplot can be generalized to an enhanced functional boxplot where the 25% and 75% central regions are provided as well.\n\nSpatio-temporal data can be viewed as a temporal curve at each spatial location, or a spatial surface at each time. In the latter case, a volume-based surface band depth can be used to order sample surfaces and leads to a three-dimensional surface boxplot with similar characteristics as the functional boxplots. Similarly, the fences are obtained by the 1.5 times the 50% central region rule. Any surface outside the fences are flagged as outlier candidates. The surface boxplot is a natural extension of the functional boxplot to R.\n\nThe command fbplot for functional boxplots is in fda R package, and MATLAB code is also available.\n\n"}
{"id": "1452962", "url": "https://en.wikipedia.org/wiki?curid=1452962", "title": "Gyaru-moji", "text": "Gyaru-moji\n\nLike the English phenomenon of SMS language, it is most often used for sending cell phone text messages, but while text is used as a form of informal shorthand, a message typed in gyaru-moji usually requires more characters and effort than the same message typed in plain Japanese. Since writing in gyaru-moji requires extra effort, and due to the perception of confidentiality, sending gyaru-moji messages to a peer is seen as a sign of informality or friendship. The origin of this style is unclear but it has been proposed that magazines targeted at teenage girls first made it popular, and the phenomenon started to gain wider attention in media around 2002.\n\nThe style has been met with increasing criticism, as its use continues to expand. Reported instances of girls using the writing in school work, OLs (Office Ladies) adopting the style in the workplace, and gyaru-moji being used in karaoke subtitling, are examples of this. Laura Miller has analyzed gyaru moji as an example of gender resistance.\n\nLike leet, gyaru-moji replaces characters with visually similar characters or combinations of characters. Hiragana consisting of connected strokes are replaced by symbols or Greek letters: for example, す (\"su\") may be rendered as the section symbol codice_1. Hiragana consisting of detached elements are replaced by sequences of kana, Western letters, or symbols. For example, ほ (\"ho\") may be typed as codice_2 (vertical bar and hiragana \"ma\") or codice_3 (open parenthesis and \"ma\"), け (\"ke\") may be typed as codice_4 (katakana \"re na\"), codice_5 (capital i, \"na\"), or codice_6 (open parenthesis, dagger), and た (ta) may be typed as codice_7 (katakana \"na\", equals sign) or codice_8 (dagger, hiragana \"ko\"). Katakana is frequently replaced by similar-looking kanji, such as 世 for セ (se) or 干 for チ (chi), in a reversal of the process that turned man'yōgana into kana. Kana and rōmaji may be mixed freely, even within a word, and Latin letters in rōmaji may be replaced with similar-looking Cyrillic letters, such as replacing N with И (Cyrillic I). Compound kanji are decomposed into left and right elements, which are written as individual kanji or kana. For example, the kanji 好 in 好き, meaning \"like, enjoy\" may be split into 女子 (the kanji for woman and child, respectively).\n\nIn addition to the basic obfuscation provided by character replacement, another technique used to disguise the content of the message is to use vocabulary and grammar that is uncharacteristic of standard usage. Combined with character substitution, this can make the meaning of the message almost unintelligible to those not “in the know”. This is analogous to the use of leet’s specialized grammar. However, the flexible nature of the Japanese language means that although gyaru-moji phrases sound peculiar to someone expecting formal or even commonly colloquial Japanese, they are often technically still grammatically correct.\n\nFor example, the sentence (“Watashi wa ima totemo yoi kibun desu.”) is “Right now I am feeling very good.” in standard normal-polite Japanese.\nBy first rewording this as or (“Chōkimochi ii!”) – which roughly translates as “(I have a) Super good feeling!” – and then converting to gyaru-moji to get or , the message could prove difficult for those not versed in the style to understand.\n\nThe original Japanese hiragana followed by romaji, and then various versions of the Japanese character in gyaru moji. The following chart is also available .\n\n\n\n\n\n\n\n\n\n\nHere are some examples of gyaru-moji created from compound kanji. The kanji characters are followed by their reading and meaning, and the gyaru-moji derived from them:\n\n\n"}
{"id": "300264", "url": "https://en.wikipedia.org/wiki?curid=300264", "title": "Heritage language", "text": "Heritage language\n\nA heritage language is a minority language learnt by its speakers at home as children, but it is never fully developed because its speakers grow up with a dominant language in which they become more competent. Polinsky & Kagan label it as a continuum that ranges from fluent speakers to barely-speaking individuals of the home language. In some countries or cultures in which they determine one's mother tongue by the ethnic group, a heritage language would be linked to the native language.\n\nThe term can also refer to the language of a person's family or community that the person does not speak or understand but culturally identify with it.\n\n\"Heritage language\" is the term used to describe a language which is predominantly spoken by \"nonsocietal\" groups and linguistic minorities. \n\nIn various fields, such as foreign language education and linguistics, the definitions of heritage language become more specific and divergent. In foreign language education, heritage language is defined in terms of a student’s upbringing and functional proficiency in the language: a student raised in a home where a non-majority language is spoken is a heritage speaker of that language if she/he possesses some proficiency in it. Under this definition, individuals that have some cultural connection with the language but do not speak it are not considered heritage students. This restricted definition became popular in the mid 1990s with the publication of \"Standards for Foreign Language Learning\" by the American Council on the Teaching of Foreign Languages.\n\nAmong linguists, heritage language is an end-state language that is defined based on the temporal order of acquisition and, often, the language dominance in the individual. A heritage speaker acquires the heritage language as their first language through natural input in the home environment and acquires the majority language as a second language, usually when she/he starts school and talks about different topics with people in school, or by exposure through media (written texts, internet, popular culture etc.). As exposure to the heritage language decreases and exposure to the majority language increases, the majority language becomes the individual’s dominant language and acquisition of the heritage language changes. The results of these changes can be seen in divergence of the heritage language from monolingual norms in the areas of phonology, lexical knowledge (knowledge of vocabulary or words), morphology, syntax, semantics and code-switching, although mastery of the heritage language may vary from purely receptive skills in only informal spoken language to native-like fluency.\n\nAs stated by Polinsky and Kagan: \"The definition of a heritage speaker in general and for specific languages continues to be debated. The debate is of particular significance in such languages as Chinese, Arabic, and languages of India and the Philippines, where speakers of multiple languages or dialects are seen as heritage speakers of a single standard language taught for geographic, cultural or other reasons (Mandarin Chinese, Classical Arabic, Hindi, or Tagalog, respectively).\"\n\nOne idea that prevails in the literature is that \"[heritage] languages include indigenous languages that are often endangered. . . as well as world languages that are commonly spoken in many other regions of the world (Spanish in the United States, Arabic in France)\". However, that view is not shared worldwide. In Canada, for example, Indigenous languages are not classified as heritage languages. \n\nThe label \"heritage\" is given to a language based principally on the social status of its speakers and not necessarily on any linguistic property. Thus, while Spanish typically comes in second in terms of native speakers worldwide and has official status in a number of countries, it is considered a heritage language in the English-dominant United States. Outside the United States, heritage language definitions and use vary.\n\nSpeakers of the same heritage language raised in the same community may differ significantly in terms of their language abilities, yet be considered heritage speakers under this definition. Some heritage speakers may be highly proficient in the language, possessing several registers, while other heritage speakers may be able to understand the language but not produce it. Other individuals that simply have a cultural connection with a minority language but do not speak it may consider it to be their heritage language. It is held by some that ownership does not necessarily depend on usership: “Some Aboriginal people distinguish between usership and ownership. There are even those who claim that they own a language although they only know one single word of it: its name.”\n\nHeritage learners have a fluent command of the dominant language and are comfortable using it in formal settings, due to their exposure to the language through formal education. Their command of the heritage language, however, varies widely. Some heritage learners may lose some fluency in the first language after beginning formal education in the dominant language. Others may use the heritage language consistently at home and with family, but receive minimal to no formal training in the heritage language and thus may struggle with literacy skills or using it in broader settings outside of the home. An additional factor that affects the acquisition of the learner, is whether he or she shows willingness or reluctance towards learning the heritage language.\n\nOne factor that has been shown to influence the loss of fluency in the heritage language is age. Studies have shown that younger bilingual children are more susceptible to fluency loss than older bilingual children. The older the child is when the dominant language is introduced, the less likely he/she is going to lose ability in using his/her first language (the heritage language). This is because the older the child is, the more exposure and knowledge of use the child will have had with the heritage language, and thus the heritage language will remain as their primary language. Researchers found that this phenomenon primarily deals with the memory network of an individual. Once a memory network is organized, it is difficult for the brain to reorganize information contrary to the initial information, because the previous information was processed first. This phenomenon becomes a struggle for adults who are trying to learn a different language. Once an individual has learned a language fluently, they will be heavily influenced by the grammatical rules and pronunciations of their first language they learned, while learning a new language.\n\nAn emerging effective way of measuring the proficiency of a heritage speaker is by speech rate. A study of gender restructuring in heritage Russian showed that heritage speakers fell into two groups: those who maintained the three-gender system and those who radically reanalyzed the system as a two-gender system. The heritage speakers who reanalyzed the three-gender system as a two-gender system had a strong correlation with a slower speech rate. The correlation is straightforward—lower proficiency speakers have more difficulty accessing lexical items; thus, their speech is slowed down.\n\nAlthough speech rate has been shown to be an effective way of measuring proficiency of heritage speakers, some heritage speakers are reluctant to produce any heritage language whatsoever. Lexical proficiency is an alternative method that is also effective in measuring proficiency. In a study with heritage Russian speakers, there was a strong correlation between the speaker's knowledge of lexical items (measured using a basic word list of about 200) and the speaker's control over grammatical knowledge such as agreement, temporal marking, and embedding.\n\nSome heritage speakers explicitly study the language to gain additional proficiency. The learning trajectories of heritage speakers are markedly different from the trajectories of second language learners with little or no previous exposure to a target language. For instance, heritage learners typically show a phonological advantage over second language learners in both perception and production of the heritage language, even when their exposure to the heritage language was interrupted very early in life. Heritage speakers also tend to distinguish, rather than conflate, easily confusable sounds in the heritage language and the dominant language more reliably than second language learners. In morphosyntax as well, heritage speakers have been found to be more native-like than second language learners, although they are typically significantly different from native speakers.\nMany linguists frame this change in heritage language acquisition as “incomplete acquisition” or \"attrition.\" \"Incomplete acquisition,\" loosely defined by Montrul, is \"the outcome of language acquisition that is not complete in childhood.\" In this incomplete acquisition, there are particular properties of the language that were not able to reach age-appropriate levels of proficiency after the dominant language has been introduced. Attrition, as defined by Montrul, is the loss of a certain property of a language after one has already mastered it with native-speaker level accuracy. These two cases of language loss have been used by Montrul and many other linguists to describe the change in heritage language acquisition. However, this is not the only viewpoint of linguists to describe heritage language acquisition. One argument against incomplete acquisition is that the input that heritage speakers receive is different from monolinguals (the input may be affected by cross-generational attrition, among other factors), thus the comparison of heritage speakers against monolinguals is weak. This argument by Pascual and Rothman claims that the acquisition of the heritage language is therefore not incomplete, but complete and simply different from monolingual acquisition of a language. Another argument argues for a shift in focus on the result of incomplete acquisition of a heritage language to the process of heritage language acquisition. In this argument, the crucial factor in changes to heritage language acquisition is the extent to which the heritage speaker activates and processes the heritage language. This new model thus moves away from language acquisition that is dependent on the exposure to input of the language and moves towards dependence on the frequency of processing for production and comprehension of the heritage language.\n\nSome colleges and universities offer courses prepared for speakers of heritage languages. For example, students who grow up learning some Spanish in the home may enroll in a course that will build on their Spanish abilities.\n\n\n\n\n"}
{"id": "13622335", "url": "https://en.wikipedia.org/wiki?curid=13622335", "title": "Hosic report", "text": "Hosic report\n\nIn 1917 the Hosic Report on the Reorganization of English in (United States) Secondary Schools placed courteous letter writing as a number one priority. This might provide rationale for why Pennell and Cusack (1924), writers of a manual for teachers of reading activities, included letter writing in suggested instructional activities. Other important activities suggested in the Hosic report were expository writing, analysis of writing pieces, reports, literary composition, and debate.\n\nIn 1917, the Hosic Report on the Reorganization of English in the Secondary Schools the first report to cite the relationship between speaking and writing, reported that the purpose of teaching composition was to enable the student to speak and write correctly to convince and interest the reader (Burrows, 1977). The first step was to cultivate sincerity through language; next to develop accuracy; and last to arouse artistic and individual expression. The goals focused on developing effective content and correct form including handwriting, spelling, grammar, capitals, and punctuation. Beyond these goals, importance was placed on enlarging vocabulary through reading. Reading authors with \"concise and vigorous style\" was encouraged so that the students could imitate \"fine writing\" (Burrows, 1977, p. 28).\n\nFrom the 1917 Hosic Report, the teacher was urged to focus on developing the individual writer rather than certain forms and rules. The teacher was to observe and respond to the maturing of each student’s growth in thought and expression. This sounds very much like teachers were being instructed to teach in response to the individual learner rather than just deliver a scripted program. Teachers were told to offer many opportunities for students to write and speak on topics that were familiar and of interest to them so that they could give attention to correctness and present thoughts to convince and interest others. The belief was that composition should grow out of the experiences of the child.\n\nThe experiences included recent field trips and outside interests such as home activities and sports. Purpose and audience were stressed. In teaching composition, interesting content came first followed in importance by organization and then mechanics (punctuation, spelling, sentence structure, and word choice).\n\nThe Hosic report stated that fact writing and imaginative writing came from two types of minds. It went on to say that short story writing should not be a passing standard for students but that they should be given opportunities to see if they had a talent for it. For those that did show talent and interest, special training should be provided. The same was advocated for debating. Teachers were encouraged to use pictures to stimulate writing development using newspaper editorial cartoons as examples. Authentic purposes for letter writing with students’ interest as content were promoted. Letters by Stevenson, Dickens, Carroll, and Lincoln served as models for informal letter writing. Whose letters today would be excellent exemplars? The report mentioned that students who were clever at writing conversations which was a sign of character should have special training in dramatization and material for dramatizations could be used from local history. Smith (2002) reported that in 1920 (soon after this report) that suggestions for dramatizations followed longer stories in readers to support comprehension. Repeatedly, the Hosic Report stressed that the work should spring from the students' interests. It was after this report in the 1920s that Language Experience Charts based on children’s experiences were initiated. Reading material was developed by using students’ own compositions about their firsthand experiences (Smith, 2002). It was also at this time that ability grouping, flexible promotions, differentiated assignments, and promoting individual progression were reported (Smith, 2002, p. 183). The emphasis in writing and reading instruction at this time seemed to be giving birth to differentiated instruction and responding to student interest. The 1917 Hosic Report proposed that composition activities should come from the life of the student and \"develop in him the power to express his individual experiences\" (Burrows, 1977, p. 32). Developing the skill of oral composition was also a strong emphasis in order to develop the power to think in front of an audience and find language for personal expression.\n\nJames Hosic, author of this report, was an Emeritus professor at Teachers College, Columbia University as well as being the first Secretary (now Executive Director) of the National Council of Teachers of English from 1911 through 1920.\n\n"}
{"id": "10318680", "url": "https://en.wikipedia.org/wiki?curid=10318680", "title": "I Am a Strange Loop", "text": "I Am a Strange Loop\n\nI Am a Strange Loop is a 2007 book by Douglas Hofstadter, examining in depth the concept of a \"strange loop\" to explain the sense of \"I\". The concept of a \"strange loop\" was originally developed in his 1979 book \"Gödel, Escher, Bach\".\n\nHofstadter had previously expressed disappointment with how \"Gödel, Escher, Bach,\" which won the Pulitzer Prize in 1980 for general nonfiction, was received. In the preface to its 20th-anniversary edition, Hofstadter laments that the book was perceived as a hodgepodge of neat things with no central theme. He states: \"GEB is a very personal attempt to say how it is that animate beings can come out of inanimate matter. What is a self, and how can a self come out of stuff that is as selfless as a stone or a puddle?\"\n\nHofstadter seeks to remedy this problem in \"I Am a Strange Loop\" by focusing and expounding on the central message of \"Gödel, Escher, Bach\". He demonstrates how the properties of self-referential systems, demonstrated most famously in Gödel's incompleteness theorems, can be used to describe the unique properties of minds.\n\nAs an exploration of the sense of \"I\", Hofstadter explores his own life, and those to whom he has been close.\n\n"}
{"id": "100241", "url": "https://en.wikipedia.org/wiki?curid=100241", "title": "Kalki", "text": "Kalki\n\nKalki, also called Kalkin or Karki, is the tenth avatar of Hindu god Vishnu to end the Kali Yuga, one of the four periods in endless cycle of existence (\"krita\") in Vaishnavism cosmology. He is described in the Puranas as the avatar who rejuvenates existence by ending the darkest and destructive period to remove adharma and ushering in the Satya Yuga, while riding a white horse with a fiery sword. The description and details of Kalki are inconsistent among the Puranic texts. He is, for example, only an invisible force destroying evil and chaos in some texts, while an actual person who kills those who persecute others, and portrayed as someone leading an army of Brahmin warriors in some. His mythology has been compared to the concepts of Messiah, Apocalypse, Frashokereti and Maitreya in other religions.\n\nKalki is also found in Buddhist texts. In Tibetan Buddhism, the \"Kalachakra-Tantra\" describes 25 rulers, each named Kalki who rule from the heavenly Shambhala. The last Kalki of Shambhala destroys a barbarian Muslim army, after which Buddhism flourishes. This text is dated to about 10th-century CE.\nHowever, some Muslims also claim that the prophet Muhammed was the Kalki avatar, because of his association with a flying white horse, and his father's name is similar in meaning to that of Vishnuyasha, the Father of Kalki. Some prophecies of Kalki say that he will kill the sinful people and atheists by the millions. The Kalki Purana is the main cannon text about Lord Kalki. In the Kalki Purana Kalki is described as the Supreme Being, source and end of all other beings, and ruler of the universe and gods.\n\nThe name Kalki means \"white horse\", \"destroyer of filth\", and \"eternity\". Some scholars such as Otto Schrader to suggest that the original term may have been \"karki\" (white horse knight, from the horse) which morphed into Kalki. This proposal is supported by two versions of \"Mahabharata\" manuscripts (e.g. the G3.6 manuscript) that have been found, where the Sanskrit verses name the avatar to be \"karki\", rather than \"kalki\".\n\nKalki is an avatara of Vishnu. Avatara means \"descent\" and refers to a descent of the divine into the material realm of human existence. The Garuda Purana lists ten avatars, with Kalki being the tenth. He is described as the avatar who appears at the end of the Kali Yuga. He ends the darkest, degenerating and chaotic stage of the Kali \"Yuga\" (period) to remove adharma and ushers in the Satya Yuga, while riding a white horse with a fiery sword. He restarts a new cycle of time. He is described as a Brahmin warrior in the Puranas.\n\nIn the Buddhist text \"Kalachakra Tantra\", the righteous kings are called Kalki (Kalkin, lit. chieftain) living in Sambhala. There are many Kalki in this text, each fighting barbarism, persecution and chaos. The last Kalki is called \"Cakrin\" and is predicted to end the chaos and degeneration by assembling a large army to eradicate the \"forces of Islam\". A great war and Armageddon will destroy the barbaric Muslim forces, states the text. According to Donald Lopez – a professor of Buddhist Studies, Kalki is predicted to start the new cycle of perfect era where \"Buddhism will flourish, people will live long, happy lives and righteousness will reign supreme\". The text is significant in establishing the chronology of the Kalki idea to be from post-7th century, probably the 9th or 10th century. Lopez states that the Buddhist text likely borrowed it from Hindu mythology. Other scholars, such as Yijiu Jin, state that the text originated in Central Asia in the 10th-century, and Tibetan literature picked up a version of it in India around 1027 CE.\n\nKalki is mentioned in several hindu texts including the Agni Purana and Ramayana and Mahabartha. The epithet \"Kalmallkinam\", meaning \"brilliant remover of darkness\", is found in the Vedic literature for Rudra (later Shiva), which has been interpreted to be \"forerunner of Kalki\".\n\nKalki appears for the first time in the great war epic \"Mahabharata\". The mention of Kalki in the \"Mahabharata\" occurs only once, over the verses 3.188.85–3.189.6. The Kalki avatar is found in the Maha-Puranas such as \"Vishnu Purana\", \"Matsya Purana\", and \"Bhagavata Purana\". However, the details relating the Kalki mythologies are divergent between the Epic and the Puranas, as well as within the Puranas.\n\nIn the \"Mahabharata\", according to Hiltebeitel, Kalki is an extension of the Parasurama avatar legend where a Brahmin warrior destroys Kshatriyas who were abusing their power to spread chaos, evil and persecution of the powerless. The Epic character of Kalki restores dharma, restores justice in the world, but does not end the cycle of existence. The Kalkin section in the \"Mahabharata\" occurs in the Markandeya section. There, states Luis Reimann, can \"hardly be any doubt that the Markandeya section is a late addition to the Epic. Making Yudhisthira ask a question about conditions at the end of Kali and the beginning of Krta — something far removed from his own situation — is merely a device for justifying the inclusion of this subject matter in the Epic.\"\n\nThis myth may have developed in the Hindu texts both as a reaction to the invasions of the Indian subcontinent by various armies over the centuries from its northwest, and the mythologies these invaders brought with them.\n\nAccording to John Mitchiner, the Kalki concept was likely borrowed \"in some measure from similar Jewish, Zoroastrian and other religions\". Mitchiner states that some Puranas such as the Yuga Purana do not mention Kalki and offer a different cosmology than the other Puranas. The Yuga Purana mythologizes in greater details the post-Maurya era Indo-Greek and Saka era, while the Manvantara theme containing the Kalki idea is mythologized greater in other Puranas. Luis Gonzales-Reimann concurs with Mitchiner, stating that the Yuga Purana does not mention Kalki. In other texts such as the sections 2.36 and 2.37 of the Vayu Purana, states Reimann, it is not Kalkin who ends the Kali Yuga, but a different character named Pramiti. Most historians, states Arvind Sharma, link the development of Kalki mythology in Hinduism to the suffering caused by foreign invasions.\n\nA text named Kalki Purana is a relatively recent text, likely composed in Bengal. Its dating floruit is the 18th-century. Wendy Doniger dates the Kalki mythology containing \"Kalki Purana\" to between 1500 and 1700 CE.\n\nIn the \"Kalki Purana\", Kalki marries princess Padmavati, the daughter of Brhadratha of Simhala. He fights an evil army and many wars, ends evil but does not end existence. Kalki returns to Sambhala, inaugurates a new \"yuga\" for the good and then goes to Vaikuntha after ruling the world for 1000 years.\n\nThe Kalki Purana states that Kalki will kill all the atheists and will have many powerful divine weapons obtained from Lord Shiva. It is said that Kalki will obtain these after training under Lord Parasurama. \n\nThe Kalki Purana describes Lord Kalki as the supreme deity even above Brahma and Shiva, and that he will kill all the sinful kings who have corrupted the world, and restore the dharma.\n\nThe Kalki avatar appears in the historic Sikh texts, most notably in Dasam Granth as Nihakalanki, a text that is traditionally attributed to Guru Gobind Singh. The \"Chaubis Avatar\" (24 avatars) section mentions sage Matsyanra describing the appearance of Vishnu avatars to fight evil, greed, violence and ignorance. It includes Kalki as the twenty-fourth incarnation to lead the war between the forces of righteousness and unrighteousness, states Dhavan.\n\n[Lord Shiva said to Lord Kalki:] \"This horse was manifested from Garuda, and it can go anywhere at will and assume many different forms. Here also is a parrot [ Shuka ] that knows everything - past, present, and future. I would like to offer You both the horse and the parrot and so please accept them. By the influence of this horse and parrot, the people of the world will know You as a learned scholar of all scriptures who is a master of the art of releasing arrows, and thus the conqueror of all. I would also like to present You this sharp, strong sword and so please accept it. The handle of this sword is bedecked with jewels, and it is extremely powerful. As such, the sword will help You to reduce the heavy burden of the earth.\"\n\nThereafter, Lord Kalki picked up His brightly shining trident and bow and arrows and sets out from His palace, riding upon His victorious horse and wearing His amulet.\n\n[Shuka said to Padmavati:] [Lord Kalki] received a sword, horse, parrot, and shield from Mahadeva, as a benediction.\n\nIn the cyclic concept of time (\"Puranic Kalpa\"), Kaliyuga is variously estimated to last between 400,000 and 432,000 years. In some Vaishnava texts, Kalki is forecasted to appear on a white horse, at the end of Kaliyuga, to end the age of degeneration and to restore virtue and world order.\n\nThe Kalki Purana states that Kalki will be born to the family of Sumati and Vishnuyasha \nor alternatively . Awejsirdenee and Bishenjun, He appears at the end of Kali Yuga to restore the order of the world. Vishnuyasha is stated to be a prominent Brahmin headman of the village called Shambhala. He will become the king, a \"Turner of the Wheel\", and one who triumphs. He will eliminate all barbarians and robbers, end \"adharma\", restart \"dharma\", and save the good people. After that, humanity will be transformed and will prevail on earth, and the golden age will begin.\n\nIn the Kanchipuram temple, two relief Puranic panels depict Kalki, one relating to lunar (daughter-based) dynasty and another to solar (son-based) dynasty. In these panels, states D Dennis Hudson, the story depicted is in terms of Kalki fighting and defeating asura Kali. He rides a white horse called Devadatta, ends evil, purifies everyone's minds and consciousness, and heralds the start of Krita Yuga.\n\nList of people who have claimed to be the Kalki avatar:\n\n\n \n"}
{"id": "1458404", "url": "https://en.wikipedia.org/wiki?curid=1458404", "title": "Kuznets curve", "text": "Kuznets curve\n\nIn economics, a Kuznets curve graphs the hypothesis that as an economy develops, market forces first increase and then decrease economic inequality. The hypothesis was first advanced by economist Simon Kuznets in the 1950s and '60s.\n\nOne explanation of such a progression suggests that early in development, investment opportunities for those who have money multiply, while an influx of cheap rural labor to the cities holds down wages. Whereas in mature economies, human capital accrual (an estimate of cost that has been incurred but not yet paid) takes the place of physical capital accrual as the main source of growth; and inequality slows growth by lowering education levels because poorer, disadvantaged people lack finance for their education in imperfect credit-markets.\n\nThe Kuznets curve implies that as a nation undergoes industrialization – and especially the mechanization of agriculture – the center of the nation’s economy will shift to the cities. As internal migration by farmers looking for better-paying jobs in urban hubs causes a significant rural-urban inequality gap (the owners of firms would be profiting, while laborers from those industries would see their incomes rise at a much slower rate and agricultural workers would possibly see their incomes decrease), rural populations decrease as urban populations increase. Inequality is then expected to decrease when a certain level of average income is reached and the processes of industrialization – democratization and the rise of the welfare state – allow for the benefits from rapid growth, and increase the per-capita income. Kuznets believed that inequality would follow an inverted “U” shape as it rises and then falls again with the increase of income per-capita.\n\nKuznets curve diagrams show an inverted U curve, although variables along the axes are often mixed and matched, with inequality or the Gini coefficient on the Y axis and economic development, time or per-capita incomes on the X axis. \n\nSince 1991 the environmental Kuznets curve (EKC) has become a standard feature in the technical literature of environmental policy, though its application there has been strongly contested.\n\nThe Kuznets ratio is a measurement of the ratio of income going to the highest-earning households (usually defined by the upper 20%) and the income going to the lowest-earning households, which is commonly measured by either the lowest 20% or lowest 40% of income. Comparing 20% to 20%, perfect equality is expressed as 1; 20% to 40% changes this value to 0.5.\n\nKuznets had two similar explanations for this historical phenomenon:\nIn both explanations, inequality will decrease after 50% of the shift force switches over to the higher paying sector.\n\nCritics of the Kuznets curve theory argue that its U-shape comes not from progression in the development of individual countries, but rather from historical differences between countries. For instance, many of the middle income countries used in Kuznets' data set were in Latin America, a region with historically high levels of inequality. When controlling for this variable, the U-shape of the curve tends to disappear (e.g. Deininger and Squire, 1998). Regarding the empirical evidence, based on large panels of countries or time series approaches, Fields (2001) considers the Kuznets hypothesis refuted.\n\nThe East Asian miracle has been used to criticize the validity of the Kuznets curve theory. The rapid economic growth of eight East Asian countries—Japan, South Korea, Hong Kong, Taiwan, Singapore (Four Asian Tigers), Indonesia, Thailand, and Malaysia—between 1965 and 1990, was called the East Asian miracle (EAM). Manufacturing and export grew quickly and powerfully. Yet simultaneously, life expectancy was found to increase and population levels living in absolute poverty decreased. This development process was contrary to the Kuznets curve theory. Many studies have been done to identify how the EAM was able to ensure that the benefits of rapid economic growth were distributed broadly among the population, because Kuznets’ theory stated that rapid capital accumulation would lead to an initial increase in inequality. Joseph Stiglitz argues the East Asian experience of an intensive and successful economic development process along with an immediate decrease in population inequality can be explained by the immediate re-investment of initial benefits into land reform (increasing rural productivity, income, and savings), universal education (providing greater equality and what Stiglitz calls an “intellectual infrastructure” for productivity ), and industrial policies that distributed income more equally through high and increasing wages and limited the price increases of commodities. These factors increased the average citizen’s ability to consume and invest within the economy, further contributing to economic growth. Stiglitz highlights that the high rates of growth provided the resources to promote equality, which acted as a positive-feedback loop to support the high rates of growth. The EAM defies the Kuznets curve, which insists growth produces inequality, and that inequality is a necessity for overall growth.\n\nCambridge University Lecturer Gabriel Palma recently found no evidence for a ‘Kuznets curve’ in inequality:\n\n\"[T]he statistical evidence for the ‘upwards’ side of the 'Inverted-U' between inequality and income per capita seems to have vanished, as many low and low-middle income countries now have a distribution of income similar to that of most middle-income countries (other than those of Latin America and Southern Africa). That is, half of Sub-Saharan Africa and many countries in Asian, including India, China and Vietnam, now have an income distribution similar to that found in North Africa, the Caribbean and the second-tier NICs. And this level is also similar to that of half of the first-tier NICs, the Mediterranean EU and the Anglophone OECD (excluding the US). As a result, about 80% of the world population now live in countries with a Gini around 40.\"\n\nPalma goes on to note that, among middle-income countries, only those in Latin America and Southern Africa live in an inequality league of their own. Instead of a Kuznets curve, he breaks income inequality into deciles which contain 10% of the population relating to income inequality. Palma then shows that there are two distributional trends taking place in inequality within a country:\n\n\"One is ‘centrifugal’, and takes place at the two tails of the distribution—leading to an increased diversity across country in the shares appropriated by the top 10 percent and bottom forty percent. The other is ‘centripetal’, and takes place in the middle—leading to a remarkable uniformity across countries in the share of income going to the half of the population located between deciles 5 to 9.\"\n\nTherefore, it is the share of the richest 10% of the population that affects the share of the poorest 40% of the population with the middle to upper-middle staying the same across all countries.\n\nIn a biography about Simon Kuznets' scientific methods, economist Robert Fogel noted Kuznets' own reservations about the \"fragility of the data\" which underpinned the hypothesis. Fogel notes that most of Kuznets' paper was devoted to explicating the conflicting factors at play. Fogel emphasized Kuznets' opinion that \"even if the data turned out to be valid, they pertained to an extremely limited period of time and to exceptional historical experiences.\" Fogel noted that despite these \"repeated warnings\", Kuznets' caveats were overlooked, and the Kuznets curve was \"raised to the level of law\" by other economists.\n\nDobson and Ramlogan’s research looked to identify the relationship between inequality and trade liberalization. There have been mixed findings with this idea – some developing countries have experienced greater inequality, less inequality, or no difference at all, due to trade liberalization. Because of this, Dobson and Ramlogan suggest that perhaps trade openness can be related to inequality through a Kuznets curve framework.\nA trade liberalization-versus-inequality graph measures trade openness along the x-axis and inequality along the y-axis. Dobson and Ramlogan determine trade openness by the ratio of exports and imports (the total trade) and the average tariff rate; inequality is determined by gross primary school enrolment rates, the share of agriculture in total output, the rate of inflation, and cumulative privatization. By studying data from several Latin American countries that have implemented trade liberalization policies in the past 30 years, the Kuznets curve seems to apply to the relationship between trade liberalization and inequality (measured by the GINI coefficient). However, many of these nations saw a shift from low-skill labour production to natural resource intensive activities. This shift would not benefit low-skill workers as much. So although their evidence seems to support the Kuznets theory in relation to trade liberalization, Dobson and Ramlogan assert that policies for redistribution must be simultaneously implemented in order to mitigate the initial increase in inequality.\n\nThe environmental Kuznets curve is a hypothesized relationship between environmental quality and economic development: various indicators of environmental degradation tend to get worse as modern economic growth occurs until average income reaches a certain point over the course of development. The EKC suggests, in sum, that \"the solution to pollution is economic growth.\" \n\nAlthough the subject of continuing debate, there is considerable evidence to support the application of environmental Kuznets curve for various environmental health indicators, such as water and air pollution, which show the inverted U-shaped curve as per capita income and/or GDP rise . It has been argued that this trend occurs in the level of many of the environmental pollutants, such as sulfur dioxide, nitrogen oxide, lead, DDT, chlorofluorocarbons, sewage, and other chemicals previously released directly into the air or water. For example, between 1970 and 2006, the United States' inflation-adjusted GDP grew by 195%, the number of cars and trucks in the country more than doubled, and the total number of miles driven increased by 178%. However, during that same period certain regulatory changes and technological innovations led to decreases in annual emissions of carbon monoxide from 197 million tons to 89 million, nitrogen oxides emissions from 27 million tons to 19 million, sulfur dioxide emissions from 31 million tons to 15 million, particulate emissions by 80%, and lead emissions by more than 98%.\n\nDeforestation may follow a Kuznets curve (cf. forest transition curve). Among countries with a per capita GDP of at least $4,600, net deforestation has ceased to exist. Yet it has been argued that wealthier countries are able to maintain forests along with high consumption by ‘exporting’ deforestation.\n\nHowever, the applicability of the EKC is debatable when it comes to other pollutants, some natural resource use, and biodiversity conservation. For example, energy, land and resource use (sometimes called the \"ecological footprint\") may not fall with rising income. While the ratio of energy \"per real GDP\" has fallen, \"total\" energy use is still rising in most developed countries as are \"total\" emission of many greenhouse gases. Additionally, the status of many key \"ecosystem services\" provided by ecosystems, such as freshwater provision (Perman, \"et al\"., 2003), soil fertility, and fisheries, have continued to decline in developed countries. Proponents of the EKC argue that this varied relationship does not necessarily invalidate the hypothesis, but instead that the applicability of the Kuznets curves to various environmental indicators may differ when considering different ecosystems, economics, regulatory schemes, and technologies.\n\nAt least one critic argues that the US is still struggling to attain the income level necessary to prioritize certain environmental pollutants such as carbon emissions, which have yet to follow the EKC. Yandle \"et al.\" argue that the EKC has not been found to apply to carbon because most pollutants create localized problems like lead and sulfur, so there is a greater urgency and response to cleaning up such pollutants. As a country develops, the marginal value of cleaning up such pollutants makes a large direct improvement to the quality of citizens' lives. Conversely, reducing carbon dioxide emissions does not have a dramatic impact at a local level, so the impetus to clean them up is only for the altruistic reason of improving the global environment. This becomes a tragedy of the commons where it is most efficient for everyone to pollute and for no one to clean up, and everyone is worse as a result (Hardin, 1968). Thus, even in a country like the US with a high level of income, carbon emissions are not decreasing in accordance with the EKC. However, there seems to be little consensus about whether EKC is formed with regard to CO2 emissions, as CO is a global pollutant that has yet to prove its validity within Kuznet's Curve. That said, Yandle \"et al.\" also concluded that \"policies that stimulate growth (trade liberalization, economic restructuring, and price reform) should be good for the environment\".\n\nOther critics points out that researchers also disagree about the shape of the curve when longer-term time scales are evaluated. For example, Millimet and Stengos regard the traditional \"inverse U\" shape as actually being an \"N\" shape, indicating that pollution increases as a country develops, decreases once the threshold GDP is reached, and then begins increasing as national income continues to increase. While such findings are still being debated, it could prove to be important because it poses the concerning question of whether pollution actually begins to decline for good when an economic threshold is reached or whether the decrease is only in local pollutants and pollution is simply exported to poorer developing countries. Levinson concludes that the environmental Kuznets curve is insufficient to support a pollution policy regardless whether it is laissez-faire or interventionist, although the literature has been used this way by the press.\n\nArrow \"et al.\" argue pollution-income progression of agrarian communities (clean) to industrial economies (pollution intensive) to service economies (cleaner) would appear to be false if pollution increases again at the end due to higher levels of income and consumption of the population at large. A difficulty with this model is that it lacks predictive power because it is highly uncertain how the next phase of economic development will be characterized. \n\nSuri and Chapman argue that the EKC is not applicable on the global scale, as a net pollution reduction may not actually be occurring globally. Wealthy nations have a trend of exporting the activities that create the most pollution, like manufacturing of clothing and furniture, to poorer nations that are still in the process of industrial development (Suri and Chapman, 1998). This could mean that as the world's poor nations develop, they will have nowhere to export their pollution. Thus, this progression of environmental clean-up occurring in conjunction with economic growth cannot be replicated indefinitely because there may be nowhere to export waste and pollution-intensive processes. However, Gene Grossman and Alan B. Krueger, the authors who initially made the correlation between economic growth, environmental clean-up, and the Kuznets curve, conclude that there is \"no evidence that environmental quality deteriorates steadily with economic growth.\" \n\nStern warns \"it is very easy to do bad econometrics\", and says \"the history of the EKC exemplifies what can go wrong\". He finds that \"little or no attention has been paid to the statistical properties of the data used such as serial dependence or stochastic trends in time series and few tests of model adequacy have been carried out or presented. However, one of the main purposes of doing econometrics is to test which apparent relationships ... are valid and which are spurious correlations\". He states his unequivocal finding: \"When we do take such statistics into account and use appropriate techniques we find that the EKC does not exist (Perman and Stern 2003). Instead, we get a more realistic view of the effect of economic growth and technological changes on environmental quality. It seems that most indicators of environmental degradation are monotonically rising in income though the 'income elasticity' is less than one and is not a simple function of income alone. Time related effects reduce environmental impacts in countries at all levels of income. However, in rapidly growing middle income countries the scale effect, which increases pollution and other degradation, overwhelms the time effect. In wealthy countries, growth is slower, and pollution reduction efforts can overcome the scale effect. This is the origin of the apparent EKC effect\".\n\n\n\n"}
{"id": "12172720", "url": "https://en.wikipedia.org/wiki?curid=12172720", "title": "Market-based environmental policy instruments", "text": "Market-based environmental policy instruments\n\nIn environmental law and policy, market-based instruments (MBIs) are policy instruments that use markets, price, and other economic variables to provide incentives for polluters to reduce or eliminate negative environmental externalities. MBIs seek to address the market failure of externalities (such as pollution) by incorporating the external cost of production or consumption activities through taxes or charges on processes or products, or by creating property rights and facilitating the establishment of a proxy market for the use of environmental services. Market-based instruments are also referred to as economic instruments, price-based instruments, new environmental policy instruments (NEPIs) or 'new instruments of environmental policy. \n\nExamples include environmentally related taxes, charges and subsidies, emissions trading and other tradeable permit systems, deposit-refund systems, environmental labeling laws, licenses, and economic property rights. For instance, the European Union Emission Trading Scheme is an example of a market-based instrument to reduce greenhouse gas emissions.\n\nMarket-based instruments differ from other policy instruments such as voluntary agreements (actors voluntarily agree to take action) and regulatory instruments (sometimes called \"command-and-control\"; public authorities mandate the performance to be achieved or the technologies to be used). However, implementing an MBI also commonly requires some form of regulation. Market based instruments can be implemented in a systematic manner, across an economy or region, across economic sectors, or by environmental medium (e.g. water). Individual MBIs are instances of environmental pricing reform.\n\nAccording to Kete (2002), \"policymaking appears to be in transition towards more market-oriented instruments, but it remains an open-ended experiment whether we shall successfully execute a long-term s\"ocial\" transition that involves the private sector and the state in new relationships implied by the pollution prevention and economic instruments rhetoric.\"\n\nFor example, although the use of new environmental policy instruments only grew significantly in Britain in the 1990s, David Lloyd George may have introduced the first market-based instrument of environmental policy in the UK when a Fuel tax was levied in 1909 during his ministry.\n\nA market-based transferable permit sets a maximum level of pollution (a 'cap'), but is likely to achieve that level at a lower cost than other means, and, importantly, may reduce below that level due to technological innovation.\n\nWhen using a transferable-permit system, it is very important to accurately measure the initial problem and also how it changes over time. This is because it can be expensive to make adjustments (either in terms of compensation or through undermining the property rights of the permits). Permits' effectiveness can also be affected by things like market liquidity, the quality of the property right, and existing market power. Another important aspect of transferable permits is whether they are auctioned or allocated via grandfathering.\n\nAn argument against permits is that formalising emission rights is effectively giving people a license to pollute, which is believed to be socially unacceptable. However, although valuing adverse environmental impacts may be controversial, the acceptable cost of preventing these impacts is implicit in all regulatory decisions.\n\nA market-based tax approach determines a maximum cost for control measures. This gives polluters an incentive to reduce pollution at a lower cost than the tax rate. There is no cap; the quantity of pollution reduced depends on the chosen tax rate.\n\nA tax approach is more flexible than permits, as the tax rate can be adjusted until it creates the most effective incentive. Taxes also have lower compliance costs than permits. However, taxes are less effective at achieving reductions in target quantities than permits. Using a tax potentially enables a double dividend, by using the revenue generated by the tax to reduce other distortionary taxes through revenue recycling. There can also be conflict between objectives with a tax: less pollution means less revenue.\n\nAn alternate approach to environmental regulation is a command and control approach. This is much more prescriptive than market-based instruments. Command and control regulatory instruments include emissions standards, process/equipment specifications, limits on input/output/discharges, requirements to disclose information, and audits. Command and control approaches have been criticised for restricting technology, as there is no incentive for firms to innovate.\n\nMarket-based instruments do not prescribe that firms use specific technologies, or that all firms reduce their emissions by the same amount, which allows firms greater flexibility in their approaches to pollution management. However, command and control approaches may be beneficial as a starting point, when regulators are faced with a significant problem yet have too little information to support a market-based instrument. Command and control approaches can also be preferred when regulators are faced with a thin market, where the limited potential trading pools mean the gains of a market-based instrument would not exceed the costs (a key requirement for a successful market-based approach).\n\nMarket-based instruments may also be inappropriate in dealing with emissions with local impacts, as trading would be restricted to within that region. They may also be inappropriate for emissions with global impacts, as international cooperation may be difficult to attain.\n\nFor a variety of reasons, environmental advocates initially opposed the use of market-based instruments except under very constrained conditions. However, after the successful use of freely traded credits in the lead phasedown in the U.S. environmental advocates recognized that trading markets has benefits for the environment as well. Thereafter, beginning with the proposal of the acid rain allowance market, environmental advocates have supported the use of trading in a variety of contexts.\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "276410", "url": "https://en.wikipedia.org/wiki?curid=276410", "title": "Module (mathematics)", "text": "Module (mathematics)\n\nIn mathematics, a module is one of the fundamental algebraic structures used in abstract algebra. A module over a ring is a generalization of the notion of vector space over a field, wherein the corresponding scalars are the elements of an arbitrary given ring (with identity) and a multiplication (on the left and/or on the right) is defined between elements of the ring and elements of the module.\n\nThus, a module, like a vector space, is an additive abelian group; a product is defined between elements of the ring and elements of the module that is distributive over the addition operation of each parameter and is compatible with the ring multiplication.\n\nModules are very closely related to the representation theory of groups. They are also one of the central notions of commutative algebra and homological algebra, and are used widely in algebraic geometry and algebraic topology.\n\nIn a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law. In a module, the scalars need only be a ring, so the module concept represents a significant generalization. In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules. In non-commutative algebra the distinction between left ideals, ideals, and modules becomes more pronounced, though some ring-theoretic conditions can be expressed either about left ideals or left modules.\n\nMuch of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a \"well-behaved\" ring, such as a principal ideal domain. However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis, and even those that do, free modules, need not have a unique rank if the underlying ring does not satisfy the invariant basis number condition, unlike vector spaces, which always have a (possibly infinite) basis whose cardinality is then unique. (These last two assertions require the axiom of choice in general, but not in the case of finite-dimensional spaces, or certain well-behaved infinite-dimensional spaces such as L spaces.)\n\nSuppose that \"R\" is a ring and 1 is its multiplicative identity.\nA left \"R\"-module \"M\" consists of an abelian group and an operation such that for all \"r\", \"s\" in \"R\" and \"x\", \"y\" in \"M\", we have:\n\nThe operation of the ring on \"M\" is called \"scalar multiplication\", and is usually written by juxtaposition, i.e. as \"rx\" for \"r\" in \"R\" and \"x\" in \"M\", though here it is denoted as to distinguish it from the ring multiplication operation, denoted here by juxtaposition. The notation \"M\" indicates a left \"R\"-module \"M\". A right \"R\"-module \"M\" or \"M\" is defined similarly, except that the ring acts on the right; i.e., scalar multiplication takes the form , and the above axioms are written with scalars \"r\" and \"s\" on the right of \"x\" and \"y\".\n\nAuthors who do not require rings to be unital omit condition 4 above in the definition of an \"R\"-module, and so would call the structures defined above \"unital left \"R\"-modules\". In this article, consistent with the glossary of ring theory, all rings and modules are assumed to be unital.\n\nIf one writes the scalar action as \"f\" so that , and \"f\" for the map that takes each \"r\" to its corresponding map \"f\" , then the first axiom states that every \"f\" is a group endomorphism of \"M\", and the other three axioms assert that the map given by is a ring homomorphism from \"R\" to the endomorphism ring End(\"M\"). Thus a module is a ring action on an abelian group (cf. group action. Also consider monoid action of multiplicative structure of \"R\"). In this sense, module theory generalizes representation theory, which deals with group actions on vector spaces, or equivalently group ring actions.\n\nA bimodule is a module that is a left module and a right module such that the two multiplications are compatible.\n\nIf \"R\" is commutative, then left \"R\"-modules are the same as right \"R\"-modules and are simply called \"R\"-modules.\n\n\nSuppose \"M\" is a left \"R\"-module and \"N\" is a subgroup of \"M\". Then \"N\" is a submodule (or \"R\"-submodule, to be more explicit) if, for any \"n\" in \"N\" and any \"r\" in \"R\", the product is in \"N\" (or for a right module).\n\nThe set of submodules of a given module \"M\", together with the two binary operations + and ∩, forms a lattice which satisfies the modular law:\nGiven submodules \"U\", \"N\", \"N\" of \"M\" such that , then the following two submodules are equal: .\n\nIf \"M\" and \"N\" are left \"R\"-modules, then a map is a homomorphism of \"R\"-modules if, for any \"m\", \"n\" in \"M\" and \"r\", \"s\" in \"R\",\nThis, like any homomorphism of mathematical objects, is just a mapping which preserves the structure of the objects. Another name for a homomorphism of modules over \"R\" is an \"R\"-linear map.\n\nA bijective module homomorphism is an isomorphism of modules, and the two modules are called \"isomorphic\". Two isomorphic modules are identical for all practical purposes, differing solely in the notation for their elements.\n\nThe kernel of a module homomorphism is the submodule of \"M\" consisting of all elements that are sent to zero by \"f\". The isomorphism theorems familiar from groups and vector spaces are also valid for \"R\"-modules.\n\nThe left \"R\"-modules, together with their module homomorphisms, form a category, written as \"R\"-Mod (see category of modules for more.) This is an abelian category.\n\nFinitely generated. An \"R\"-module \"M\" is finitely generated if there exist finitely many elements \"x\", ..., \"x\" in \"M\" such that every element of \"M\" is a linear combination of those elements with coefficients from the ring \"R\".\n\nCyclic. A module is called a cyclic module if it is generated by one element.\n\nFree. A free \"R\"-module is a module that has a basis, or equivalently, one that is isomorphic to a direct sum of copies of the ring \"R\". These are the modules that behave very much like vector spaces.\n\nProjective. Projective modules are direct summands of free modules and share many of their desirable properties.\n\nInjective. Injective modules are defined dually to projective modules.\n\nFlat. A module is called flat if taking the tensor product of it with any exact sequence of \"R\"-modules preserves exactness.\n\nTorsionless module. A module is called torsionless if it embeds into its algebraic dual.\n\nSimple. A simple module \"S\" is a module that is not {0} and whose only submodules are {0} and \"S\". Simple modules are sometimes called \"irreducible\".\n\nSemisimple. A semisimple module is a direct sum (finite or not) of simple modules. Historically these modules are also called \"completely reducible\".\n\nIndecomposable. An indecomposable module is a non-zero module that cannot be written as a direct sum of two non-zero submodules. Every simple module is indecomposable, but there are indecomposable modules which are not simple (e.g. uniform modules).\n\nFaithful. A faithful module \"M\" is one where the action of each in \"R\" on \"M\" is nontrivial (i.e. for some \"x\" in \"M\"). Equivalently, the annihilator of \"M\" is the zero ideal.\n\nTorsion-free. A torsion-free module is a module over a ring such that 0 is the only element annihilated by a regular element (non zero-divisor) of the ring.\n\nNoetherian. A Noetherian module is a module which satisfies the ascending chain condition on submodules, that is, every increasing chain of submodules becomes stationary after finitely many steps. Equivalently, every submodule is finitely generated.\n\nArtinian. An Artinian module is a module which satisfies the descending chain condition on submodules, that is, every decreasing chain of submodules becomes stationary after finitely many steps.\n\nGraded. A graded module is a module with a decomposition as a direct sum over a graded ring such that for all \"x\" and \"y\".\n\nUniform. A uniform module is a module in which all pairs of nonzero submodules have nonzero intersection.\n\nIf \"M\" is a left \"R\"-module, then the \"action\" of an element \"r\" in \"R\" is defined to be the map that sends each \"x\" to \"rx\" (or \"xr\" in the case of a right module), and is necessarily a group endomorphism of the abelian group . The set of all group endomorphisms of \"M\" is denoted End(\"M\") and forms a ring under addition and composition, and sending a ring element \"r\" of \"R\" to its action actually defines a ring homomorphism from \"R\" to End(\"M\").\n\nSuch a ring homomorphism is called a \"representation\" of \"R\" over the abelian group \"M\"; an alternative and equivalent way of defining left \"R\"-modules is to say that a left \"R\"-module is an abelian group \"M\" together with a representation of \"R\" over it.\n\nA representation is called \"faithful\" if and only if the map is injective. In terms of modules, this means that if \"r\" is an element of \"R\" such that for all \"x\" in \"M\", then . Every abelian group is a faithful module over the integers or over some modular arithmetic Z/\"n\"Z.\n\nAny ring \"R\" can be viewed as a preadditive category with a single object. With this understanding, a left \"R\"-module is nothing but a (covariant) additive functor from \"R\" to the category Ab of abelian groups. Right \"R\"-modules are contravariant additive functors. This suggests that, if \"C\" is any preadditive category, a covariant additive functor from \"C\" to Ab should be considered a generalized left module over \"C\"; these functors form a functor category \"C\"-Mod which is the natural generalization of the module category \"R\"-Mod.\n\nModules over \"commutative\" rings can be generalized in a different direction: take a ringed space (\"X\", O) and consider the sheaves of O-modules; see sheaf of modules for more. These form a category O-Mod, and play an important role in modern algebraic geometry. If \"X\" has only a single point, then this is a module category in the old sense over the commutative ring O(\"X\").\n\nOne can also consider modules over a semiring. Modules over rings are abelian groups, but modules over semirings are only commutative monoids. Most applications of modules are still possible. In particular, for any semiring \"S\" the matrices over \"S\" form a semiring over which the tuples of elements from \"S\" are a module (in this generalized sense only). This allows a further generalization of the concept of vector space incorporating the semirings from theoretical computer science.\n\nOver near-rings, one can consider near-ring modules, a nonabelian generalization of modules.\n\n\n\n"}
{"id": "38909404", "url": "https://en.wikipedia.org/wiki?curid=38909404", "title": "Odhikar", "text": "Odhikar\n\nOdhikar is a Bangladesh-based human rights organization that was founded in October 1994. It is a member of the International Federation for Human Rights (FIDH). It has been publishing an annual Activity Report since 2003. Odhikar's work has been cited by Human Rights Watch in their 2011 World Report on Bangladesh.\n\nOn 10 June 2013, Odhikar published a fact finding report on the 2013 Operation at Motijheel Shapla Chattar claiming 61 deaths, but refused to provide any names of the victims report, citing security concerns for the families of the victims.\n"}
{"id": "9251555", "url": "https://en.wikipedia.org/wiki?curid=9251555", "title": "Okinawan scripts", "text": "Okinawan scripts\n\nOkinawan language, spoken in Okinawa Island, was once the official language of the Ryukyu Kingdom. At the time, documents were written in kanji and hiragana, derived from Japan. \n\nNowadays, most Japanese, as well as most Okinawans, tend to think of Okinawan as merely a dialect of Standard Japanese, even though the language is not mutually intelligible to Japanese speakers. As a \"dialect\", modern Okinawan language is not written frequently. When it is, the Japanese writing system is generally used with an \"ad hoc\" manner. There is no standard orthography for the modern language. Nonetheless, there are a few systems announced by scholars and laypeople alike. None of them are widespread among the native speakers, but those systems can write the language with less ambiguity than the \"ad hoc\" conventions. The Roman alphabet in some form or another is used in some publications, especially those of an academic nature.\n\nThe modern conventional \"ad hoc\" spellings found in Okinawa.\n\nThe system devised by the Council for the Dissemination of Okinawan Dialect (沖縄方言普及協議会). \n\nThe system devised by Okinawa Center of Language Study, a section of University of the Ryukyus. Unlike others, this method is intended purely as a phonetic guidance, basically uses katakana only. For the sake of an easier comparison, corresponding hiragana are used in this article.\n\n新沖縄文字 (\"Shin Okinawa-moji\"), devised by , in his textbook \"Utsukushii Okinawa no Hōgen\" (美しい沖縄の方言; \"The beautiful Okinawan Dialect\"; ). The rule applies to hiragana only. Katakana is used as in Japanese; just like in the conventional usage of Okinawan.\n"}
{"id": "6241951", "url": "https://en.wikipedia.org/wiki?curid=6241951", "title": "Peacebuilding", "text": "Peacebuilding\n\nPeacebuilding is an intervention technique or method that is designed to prevent the start or resumption of violent conflict by creating a sustainable peace. Peacebuilding activities address the root causes or potential causes of violence, create a societal expectation for peaceful conflict resolution, and stabilize society politically and socioeconomically.\n\nThe activities included in peacebuilding vary depending on the situation and the agent of peacebuilding. Successful peacebuilding activities create an environment supportive of self-sustaining, durable peace; reconcile opponents; prevent conflict from restarting; integrate civil society; create rule of law mechanisms; and address underlying structural and societal issues. Researchers and practitioners also increasingly find that peacebuilding is most effective and durable when it relies upon local conceptions of peace and the underlying dynamics which foster or enable conflict.\n\nThe exact definition varies depending on the actor, with some definitions specifying what activities fall within the scope of peacebuilding or restricting peacebuilding to post-conflict interventions.\n\nEven if peacebuilding has remained a largely amorphous concept without clear guidelines or goals, common to all definitions is the agreement that improving human security is the central task of peacebuilding. In this sense, peacebuilding includes a wide range of efforts by diverse actors in government and civil society at the community, national, and international levels to address the root causes of violence and ensure civilians have freedom from fear (negative peace), freedom from want (positive peace) and freedom from humiliation before, during, and after violent conflict.  \n\nAlthough many of peacebuilding's aims overlap with those of peacemaking, peacekeeping and conflict resolution, it is a distinct idea. Peacemaking involves stopping an ongoing conflict, whereas peacebuilding happens before a conflict starts or once it ends. Peacekeeping prevents the resumption of fighting following a conflict; it does not address the underlying causes of violence or work to create societal change, as peacebuilding does. Peacekeeping also differs from peacebuilding in that it only occurs after conflict ends, not before it begins. Conflict resolution does not include some components of peacebuilding, such as state building and socioeconomic development.\n\nIn 2007, the UN Secretary-General's Policy Committee defined peacebuilding as follows: \"Peacebuilding involves a range of measures targeted to reduce the risk of lapsing or relapsing into conflict by strengthening national capacities at all levels for conflict management, and to lay the foundations for sustainable peace and sustainable development. Peacebuilding strategies must be coherent and tailored to specific needs of the country concerned, based on national ownership, and should comprise a carefully prioritized, sequenced, and therefore relatively narrow set of activities aimed at achieving the above objectives.\"\n\nThere are two broad approaches to peacebuilding.\n\nFirst, peacebuilding can refer to \"direct work\" that intentionally focuses on addressing the factors driving or mitigating conflict. When applying the term \"peacebuilding\" to this work, there is an explicit attempt by those designing and planning a peacebuilding effort to reduce structural or direct violence.\n\nSecond, the term peacebuilding can also refer to efforts to coordinate a multi-level, multisectoral strategy, including ensuring that there is funding and proper communication and coordination mechanisms between humanitarian assistance, development, governance, security, justice and other sectors that may not use the term \"peacebuilding\" to describe themselves. The concept is not one to impose on specific sectors. Rather some scholars use the term peacebuilding as an overarching concept useful for describing a range of interrelated efforts.\n\nWhile some use the term to refer to only post-conflict or post-war contexts, most use the term more broadly to refer to any stage of conflict. Before conflict becomes violent, preventive peacebuilding efforts, such as diplomatic, economic development, social, educational, health, legal and security sector reform programs, address potential sources of instability and violence. This is also termed conflict prevention. Peacebuilding efforts aim to manage, mitigate, resolve and transform central aspects of the conflict through official diplomacy; as well as through civil society peace processes and informal dialogue, negotiation, and mediation. Peacebuilding addresses economic, social and political root causes of violence and fosters reconciliation to prevent the return of structural and direct violence. Peacebuilding efforts aim to change beliefs, attitudes and behaviors to transform the short and long term dynamics between individuals and groups toward a more stable, peaceful coexistence. Peacebuilding is an approach to an entire set of interrelated efforts that support peace.\n\nIn the 1970s, Norwegian sociologist Johan Galtung first created the term peacebuilding through his promotion of systems that would create sustainable peace. Such systems needed to address the root causes of conflict and support local capacity for peace management and conflict resolution. Galtung's work emphasized a bottom-up approach that decentralized social and economic structures, amounting to a call for a societal shift from structures of coercion and violence to a culture of peace. American sociologist John Paul Lederach proposed a different concept of peacebuilding as engaging grassroots, local, NGO, international and other actors to create a sustainable peace process. He does not advocate the same degree of structural change as Galtung.\n\nPeacebuilding has since expanded to include many different dimensions, such as disarmament, demobilization and reintegration and rebuilding governmental, economic and civil society institutions. The concept was popularized in the international community through UN Secretary-General Boutros Boutros-Ghali's 1992 report An Agenda for Peace. The report defined post-conflict peacebuilding as an “action to identify and support structures which will tend to strengthen and solidify peace in order to avoid a relapse into conflict\" At the 2005 World Summit, the United Nations began creating a peacebuilding architecture based on Kofi Annan's proposals. The proposal called for three organizations: the UN Peacebuilding Commission, which was founded in 2005; the UN Peacebuilding Fund, founded in 2006; and the UN Peacebuilding Support Office, which was created in 2005. These three organizations enable the Secretary-General to coordinate the UN's peacebuilding efforts. National governments' interest in the topic has also increased due to fears that failed states serve as breeding grounds for conflict and extremism and thus threaten international security. Some states have begun to view peacebuilding as a way to demonstrate their relevance. However, peacebuilding activities continue to account for small percentages of states' budgets.\n\nThe Marshall Plan was a long-term postconflict peacebuilding intervention in Europe with which the United States aimed to rebuild the continent following the destruction of World War II. The Plan successfully promoted economic development in the areas it funded. More recently, peacebuilding has been implemented in postconflict situations in countries including Bosnia and Herzegovina, Kosovo, Northern Ireland, Cyprus, and South Africa.\n\nThe activities included in peacebuilding vary depending on the situation and the agent of peacebuilding. Successful peacebuilding activities create an environment supportive of self-sustaining, durable peace; reconcile opponents; prevent conflict from restarting; integrate civil society; create rule of law mechanisms; and address underlying structural and societal issues. To accomplish these goals, peacebuilding must address functional structures, emotional conditions and social psychology, social stability, rule of law and ethics, and cultural sensitivities.\n\nPreconflict peacebuilding interventions aim to prevent the start of violent conflict. These strategies involve a variety of actors and sectors in order to transform the conflict. Even though the definition of peacebuilding includes preconflict interventions, in practice most peacebuilding interventions are postconflict. However, many peacebuilding scholars advocate an increased focus on preconflict peacebuilding in the future.\n\nThere are many different approaches to categorization of forms of peacebuilding among the peacebuilding field's many scholars.\n\nBarnett \"et al.\" divides postconflict peacebuilding into three dimensions: stabilizing the post-conflict zone, restoring state institutions, and dealing with social and economic issues. Activities within the first dimension reinforce state stability post-conflict and discourage former combatants from returning to war (disarmament, demobilization and reintegration, or DDR). Second dimension activities build state capacity to provide basic public goods and increase state legitimacy. Programs in the third dimension build a post-conflict society's ability to manage conflicts peacefully and promote socioeconomic development.\nA mixture of locally and internationally focused components is key to building a long-term sustainable peace. Mac Ginty says that while different \"indigenous\" communities utilize different conflict resolution techniques, most of them share the common characteristics described in the table below. Since indigenous peacebuilding practices arise from local communities, they are tailored to local context and culture in a way that generalized international peacebuilding approaches are not.\n\nThe United Nations participates in many aspects of peacebuilding, both through the peacebuilding architecture established in 2005-6 and through other agencies.\n\nThe World Bank and International Monetary Fund focus on the economic and financial aspects of peacebuilding. The World Bank assists in post-conflict reconstruction and recovery by helping rebuild society's socioeconomic framework. The International Monetary Fund deals with post-conflict recovery and peacebuilding by acting to restore assets and production levels.\n\nThe EU's European Commission describes its peacebuilding activities as conflict prevention and management, and rehabilitation and reconstruction. Conflict prevention and management entails stopping the imminent outbreak of violence and encouraging a broad peace process. Rehabilitation and reconstruction deals with rebuilding the local economy and institutional capacity. The European Commission Conflict Prevention and Peace building 2001-2010 was subjected to a major external evaluation conducted by Aide a la Decisions Economique (ADE) with the European Centre for Development Policy Management which was presented in 2011. The European External Action Service created in 2010 also has a specific Division of Conflict Prevention, Peacebuilding and Mediation.\n\nFrance\n\nGermany\n\nSwitzerland\n\nUnited Kingdom\n\nUnited States\n\n\n\nWomen have traditionally played a limited role in peacebuilding processes even though they often bear the responsibility for providing for their families' basic needs in the aftermath of violent conflict. They are especially likely to be unrepresented or underrepresented in negotiations, political decision-making, upper-level policymaking and senior judicial positions. Many societies' patriarchal cultures prevent them from recognizing the role women can play in peacebuilding. However, many peacebuilding academics and the United Nations have recognized that women play a vital role in securing the three pillars of sustainable peace: economic recovery and reconciliation, social cohesion and development and political legitimacy, security and governance.\n\nAt the request of the Security Council, the Secretary-General issued a report on women's participation in peacebuilding in 2010. The report outlines the challenges women continue to face in participating in recovery and peacebuilding process and the negative impact this exclustion has on them and societies more broadly. To respond to these challenges, it advocates a comprehensive 7-point action plan covering the seven commitment areas: mediation; post-conflict planning; financing; civilian capacity; post-conflict governance; rule of law; and economic recovery. The action plan aims to facilitate progress on the women, peace and security agenda. The monitoring and implementation of this action plan is now being led jointly by the Peacebuilding Support Office and UN Women. In April 2011, the two organizations convened a workshop to ensure that women are included in future post-disaster and post-conflict planning documents. In the same year, the PBF selected seven gender-sensitive peacebuilding projects to receive $5 million in funding.\n\nPorter discusses the growing role of female leadership in countries prone to war and its impact on peacebuilding. When the book was written, seven countries prone to violent conflict had female heads of state. Ellen Johnson-Sirleaf of Liberia and Michelle Bachelet of Chile were the first female heads of state from their respective countries and President Johnson-Sirleaf was the first female head of state in Africa. Both women utilized their gender to harness \"the power of maternal symbolism - the hope that a woman could best close wounds left on their societies by war and dictatorship.\"\n\nThe UN Peacebuilding Commission works in Burundi, Central African Republic, Guinea, Guinea-Bissau, Liberia and Sierra Leone and the UN Peacebuilding Fund funds projects in Burundi, Central African Republic, Chad, Comoros, Côte d'Ivoire, the Democratic Republic of the Congo, Guinea, Guinea Bissau, Guatemala, Haiti, Kenya, Kyrgyzstan, Lebanon, Liberia, Nepal, Niger, Sierra Leone, Somalia, Sri Lanka, Sudan, South Sudan, Timor-Leste and Uganda. Other UN organizations are working in Haiti (MINUSTAH), Lebanon, Afghanistan, Kosovo and Iraq.\n\nThe World Bank's International Development Association maintains the Trust Fund for East Timor in Timor-Leste. The TFET has assisted reconstruction, community empowerment and local governance in the country.\n\nAs part of the War in Afghanistan and the War in Iraq, the United States has invested $104 billion in reconstruction and relief efforts for the two countries. The Iraq Relief and Reconstruction Fund alone received $21 billion during FY2003 and FY2004. The money came from the United States Department of State, United States Agency for International Development and the United States Department of Defense and included funding for security, health, education, social welfare, governance, economic growth and humanitarian issues.\n\nCivil society organisations sometimes even are working on Peacebuilding themselves. This for example is the case in Kenya, according to the magazine D+C Development and Cooperation. After the election riots in Kenya in 2008, civil society organisations started programmes to avoid similar disasters in the future, for instance the Truth, Justice and Reconciliation Commission (TJRC) and peace meetings organised by the church and they supported the National Cohesion and Integration Commission.\n\nIn 2010, the UNPBC conducted a review of its work with the first four countries on its agenda. An independent review by the Pulitzer Center on Crisis Reporting also highlighted some of the PBC's early successes and challenges.\n\nOne comprehensive study finds that UN peacebuilding missions significantly increase the likelihood of democratization.\n\nJennifer Hazen contends there are two major debates relating to peacebuilding; the first centres on the role of the liberal democratic model in designing peacebuilding activities and measuring outcomes and the other one questions the role of third-party actors in peacebuliding.\n\nRegarding the debate about the role of the liberal democratic model in peacebuilding, one side contends that liberal democracy is a viable end goal for peacebuilding activities in itself but that the activities implemented to achieve it need to be revised; a rushed transition to democratic elections and market economy can undermine stability and elections held or economic legislation enacted are an inappropriate yardstick for success. Institutional change is necessary and transitions need to be incremental.\nAnother side contends that liberal democracy might be an insufficient or even inappropriate goal for peacebuilding efforts and that the focus must be on a social transformation to develop non-violent mechanisms of conflict resolution regardless of their form.\n\nWith regards to the role of third-party actors, David Chandler contends that external support creates dependency and undermines local and domestic politics, thus undermining autonomy and the capacity for self-governance and leaving governments weak and dependent on foreign assistance once the third-party actors depart. Since the logic of peacebuilding relies on building and strengthening institutions to alter societal beliefs and behaviour, success relies on the populations' endorsement of these institutions. Any third party attempt at institution building without genuine domestic support will result in hollow institutions - this can lead to a situation in which democratic institutions are established before domestic politics have developed in a liberal, democratic fashion, and an unstable polity.\n\nSéverine Autesserre offers a different approach, which focuses on the role of everyday practices in peacebuilding. She argue that the foreign peace builders' everyday practices, habits, and narratives strongly influence peacebuilding effectiveness. Autesserre stresses that international peacebuilders do not fully understand the conflicts they are trying to resolve because they rarely include local leaders in decision making, do not speak the local languages, and do not stay posted long enough to oversee effective change. This leaves decision makers out of touch with the key players in the peacebuilding process.\n\nJeremy Weinstein challenges the assumption that weak and failing states cannot rebuild themselves. He contends that through the process of autonomous recovery, international peacekeeping missions can be unnecessary for recovery because they assume that conflicts cannot be resolved by the country internally. He describes autonomous recovery as a \"process through which countries achieve a lasting peace, a systematic reduction in violence, and postwar political and economic development in the absence of international intervention\". Through peace and institutions generated by allowing war to run its natural course, autonomous recovery can be viewed as a success. He claims that war leads to peace by allowing the naturally stronger belligerent gain power, rather than a brokered peace deal that leaves two sides still capable of fighting. Secondly he claims that war provides a competition among providers of public goods until one can control a monopoly. He says that war can create an incentive to create institutions at all levels in order to consolidate power and extract resources from the citizens while also giving some power to the citizens depending upon how much the institutions rely on them for tax revenues.\n\nVirginia Fortna of Columbia University, however, holds that peacekeeping interventions actually do substantively matter following the end of a civil war. She claims that selection bias, where opponents point only to failed peacekeeping interventions and do not compare these missions to those situations where interventions do not occur, is partly to blame for criticisms. Fortna says that peacekeeping missions rarely go into easily resolvable situations while they are sent into tougher, more risky post war situations where missions are more likely to fail, and peace agreements are unlikely to be committed to. When all factors of a certain peacekeeping case study are properly considered, Fortna shows that peacekeeping missions do in fact help increase the chances of sustained peace after a civil war.\n\nBarnett \"et al.\" criticizes peacebuilding organizations for undertaking supply-driven rather than demand-driven peacebuilding; they provide the peacebuilding services in which their organization specializes, not necessarily those that the recipient most needs. In addition, he argues that many of their actions are based on organizations precedent rather than empirical analysis of which interventions are and are not effective. More recently, Ben Hillman has criticized international donor efforts to strengthen local governments in the wake of conflict. He argues that international donors typically do not have the knowledge, skills or resources to bring meaningful change to the way post-conflict societies are governed.\n\nMany academics argue that peacebuilding is a manifestation of liberal internationalism and therefore imposes Western values and practices onto other cultures. Mac Ginty states that although peacebuilding does not project all aspects of Western culture on to the recipient states, it does transmit some of them, including concepts like neoliberalism that the West requires recipients of aid to follow more closely than most Western countries do. Barnett also comments that the promotion of liberalization and democratization may undermine the peacebuilding process if security and stable institutions are not pursued concurrently. Richmond has shown how 'liberal peacebuilding' represents a political encounter that may produce a post-liberal form of peace. Local and international actors, norms, institutions and interests engage with each other in various different contexts, according to their respective power relations and their different conceptions of legitimate authority structures.\n\n\n\n"}
{"id": "153390", "url": "https://en.wikipedia.org/wiki?curid=153390", "title": "Phillips curve", "text": "Phillips curve\n\nThe Phillips curve is a single-equation econometric model, named after William\nPhillips, describing a historical inverse relationship between rates of unemployment and corresponding rates of rises in wages that result within an economy. Stated simply, decreased unemployment, (i.e., increased levels of employment) in an economy will correlate with higher rates of wage rises. Phillips did not himself state there was any relationship between employment and inflation, this notion was a trivial deduction from his statistical findings. Samuelson and Solow made the connection explicit and subsequently Milton Friedman from 1967 put the theoretical structure in place. In so doing, Friedman was to successfully predict the imminent collapse of Phillip's a-theoretic correlation.\n\nWhile there is a short run tradeoff between unemployment and inflation, it has not been observed in the long run. In 1968, Milton Friedman asserted that the Phillips curve was only applicable in the short-run and that in the long-run, inflationary policies would not decrease unemployment. Friedman then correctly predicted that in the 1973–75 recession, both inflation and unemployment would increase. The long-run Phillips curve is now seen as a vertical line at the natural rate of unemployment, where the rate of inflation has no effect on unemployment. In recent years the slope of the Phillips curve appears to have declined and there has been significant questioning of the usefulness of the Phillips curve in predicting inflation. Nonetheless, the Phillips curve remains the primary framework for understanding and forecasting inflation used in central banks.\n\nWilliam Phillips, a New Zealand born economist, wrote a paper in 1958 titled \"The Relation between Unemployment and the Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957\", which was published in the quarterly journal \"Economica\". In the paper Phillips describes how he observed an inverse relationship between money wage changes and unemployment in the British economy over the period examined. Similar patterns were found in other countries and in 1960 Paul Samuelson and Robert Solow took Phillips' work and made explicit the link between inflation and unemployment: when inflation was high, unemployment was low, and vice versa.\n\nIn the 1920s, an American economist Irving Fisher noted this kind of Phillips curve relationship. However, Phillips' original curve described the behavior of money wages.\n\nIn the years following Phillips' 1958 paper, many economists in the advanced industrial countries believed that his results showed that there was a permanently stable relationship between inflation and unemployment. One implication of this for government policy was that governments could control unemployment and inflation with a Keynesian policy. They could tolerate a reasonably high rate of inflation as this would lead to lower unemployment – there would be a trade-off between inflation and unemployment. For example, monetary policy and/or fiscal policy could be used to stimulate the economy, raising gross domestic product and lowering the unemployment rate. Moving along the Phillips curve, this would lead to a higher inflation rate, the cost of enjoying lower unemployment rates. Economist James Forder argues that this view is historically false and that neither economists nor governments took that view and that the 'Phillips curve myth' was an invention of the 1970s.\n\nSince 1974, seven Nobel Prizes have been given to economists for, among other things, work critical of some variations of the Phillips curve. Some of this criticism is based on the United States' experience during the 1970s, which had periods of high unemployment and high inflation at the same time. The authors receiving those prizes include Thomas Sargent, Christopher Sims, Edmund Phelps, Edward Prescott, Robert A. Mundell, Robert E. Lucas, Milton Friedman, and F.A. Hayek.\n\nIn the 1970s, many countries experienced high levels of both inflation and unemployment also known as stagflation. Theories based on the Phillips curve suggested that this could not happen, and the curve came under a concerted attack from a group of economists headed by Milton Friedman. Friedman argued that the Phillips curve relationship was only a short-run phenomenon. In this he followed eight years after Samuelson and Solow [1960] who wrote \"All of our discussion has been phrased in short-run terms, dealing with what might happen in the next few years. It would be wrong, though, to think that our Figure 2 menu that related obtainable price and unemployment behavior will maintain its same shape in the longer run. What we do in a policy way during the next few years might cause it to shift in a definite way.\" As Samuelson and Solow had argued 8 years earlier, he argued that in the long run, workers and employers will take inflation into account, resulting in employment contracts that increase pay at rates near anticipated inflation. Unemployment would then begin to rise back to its previous level, but now with higher inflation rates. This result implies that over the longer-run there is no trade-off between inflation and unemployment. This implication is significant for practical reasons because it implies that central banks should not set unemployment targets below the natural rate.\n\nMore recent research suggests that there is a moderate trade-off between low-levels of inflation and unemployment. Work by George Akerlof, William Dickens, and George Perry, implies that if inflation is reduced from two to zero percent, unemployment will be permanently increased by 1.5 percent. This is because workers generally have a higher tolerance for real wage cuts than nominal ones. For example, a worker will more likely accept a wage increase of two percent when inflation is three percent, than a wage cut of one percent when the inflation rate is zero.\n\nMost economists no longer use the Phillips curve in its original form because it was shown to be too simplistic. This can be seen in a cursory analysis of US inflation and unemployment data from 1953–92. There is no single curve that will fit the data, but there are three rough aggregations—1955–71, 1974–84, and 1985–92—each of which shows a general, downwards slope, but at three very different levels with the shifts occurring abruptly. The data for 1953–54 and 1972–73 do not group easily, and a more formal analysis posits up to five groups/curves over the period.\n\nBut still today, modified forms of the Phillips curve that take inflationary expectations into account remain influential. The theory goes under several names, with some variation in its details, but all modern versions distinguish between short-run and long-run effects on unemployment. Modern Phillips curve models include both a short-run Phillips Curve and a long-run Phillips Curve. This is because in the short run, there is generally an inverse relationship between inflation and the unemployment rate; as illustrated in the downward sloping short-run Phillips curve. In the long run, that relationship breaks down and the economy eventually returns to the natural rate of unemployment regardless of the inflation rate.\n\nThe \"short-run Phillips curve\" is also called the \"expectations-augmented Phillips curve\", since it shifts up when inflationary expectations rise, Edmund Phelps and Milton Friedman argued. In the long run, this implies that monetary policy cannot affect unemployment, which adjusts back to its \"natural rate\", also called the \"NAIRU\" or \"long-run Phillips curve\". However, this long-run \"neutrality\" of monetary policy does allow for short run fluctuations and the ability of the monetary authority to temporarily decrease unemployment by increasing permanent inflation, and vice versa. The popular textbook of Blanchard gives a textbook presentation of the expectations-augmented Phillips curve.\n\nAn equation like the expectations-augmented Phillips curve also appears in many recent New Keynesian dynamic stochastic general equilibrium models. In these macroeconomic models with sticky prices, there is a positive relation between the rate of inflation and the level of demand, and therefore a negative relation between the rate of inflation and the rate of unemployment. This relationship is often called the \"New Keynesian Phillips curve\". Like the expectations-augmented Phillips curve, the New Keynesian Phillips curve implies that increased inflation can lower unemployment temporarily, but cannot lower it permanently. Two influential papers that incorporate a New Keynesian Phillips curve are Clarida, Galí, and Gertler (1999), and Blanchard and Galí (2007).\n\nThere are at least two different mathematical derivations of the Phillips curve. First, there is the traditional or Keynesian version. Then, there is the new Classical version associated with Robert E. Lucas, Jr.\n\nThe original Phillips curve literature was not based on the unaided application of economic theory. Instead, it was based on empirical generalizations. After that, economists tried to develop theories that fit the data.\n\nThe traditional Phillips curve story starts with a wage Phillips Curve, of the sort described by Phillips himself. This describes the rate of growth of money wages (gW). Here and below, the operator g is the equivalent of \"the percentage rate of growth of\" the variable that follows.\nThe \"money wage rate\" (W) is shorthand for total money wage costs per production employee, including benefits and payroll taxes. The focus is on only production workers' money wages, because (as discussed below) these costs are crucial to pricing decisions by the firms.\n\nThis equation tells us that the growth of money wages rises with the trend rate of growth of money wages (indicated by the superscript \"T\") and falls with the unemployment rate (U). The function f() is assumed to be monotonically increasing with U so that the dampening of money-wage increases by unemployment is shown by the negative sign in the equation above.\n\nThere are several possible stories behind this equation. A major one is that money wages are set by \"bilateral negotiations\" under partial bilateral monopoly: as the unemployment rate rises, \"all else constant\" worker bargaining power falls, so that workers are less able to increase their wages in the face of employer resistance.\n\nDuring the 1970s, this story had to be modified, because (as the late Abba Lerner had suggested in the 1940s) workers try to keep up with inflation. Since the 1970s, the equation has been changed to introduce the role of inflationary expectations (or the expected inflation rate, gP). This produces the expectations-augmented wage Phillips curve:\n\nThe introduction of inflationary expectations into the equation implies that actual inflation can \"feed back\" into inflationary expectations and thus cause further inflation. The late economist James Tobin dubbed the last term \"inflationary inertia,\" because in the current period, inflation exists which represents an inflationary impulse left over from the past.\n\nIt also involved much more than expectations, including the price-wage spiral. In this spiral, employers try to protect profits by raising their prices and employees try to keep up with inflation to protect their real wages. This process can feed on itself, becoming a self-fulfilling prophecy.\n\nThe parameter λ (which is presumed constant during any time period) represents the degree to which employees can gain money wage increases to keep up with expected inflation, preventing a fall in expected real wages. It is usually assumed that this parameter equals unity in the long run.\n\nIn addition, the function f() was modified to introduce the idea of the non-accelerating inflation rate of unemployment (NAIRU) or what's sometimes called the \"natural\" rate of unemployment or the\ninflation-threshold unemployment rate:\n\nHere, U* is the NAIRU. As discussed below, if U < U*, inflation tends to accelerate. Similarly, if U > U*, inflation tends to slow. It is assumed that f(0) = 0, so that when U = U*, the f term drops out of the equation.\n\nIn equation [1], the roles of gW and gP seem to be redundant, playing much the same role. However, assuming that λ is equal to unity, it can be seen that they are not. If the trend rate of growth of money wages equals zero, then the case where U equals U* implies that gW equals expected inflation. That is, expected real wages are constant.\n\nIn any reasonable economy, however, having constant expected real wages could only be consistent with actual real wages that are constant over the long haul. This does not fit with economic experience in the U.S. or any other major industrial country. Even though real wages have not risen much in recent years, there have been important increases over the decades.\n\nAn alternative is to assume that the trend rate of growth of money wages equals the trend rate of growth of average labor productivity (Z). That is:\n\nUnder assumption [2], when U equals U* and λ equals unity, expected real wages would increase with labor productivity. This would be consistent with an economy in which actual real wages increase with labor productivity. Deviations of real-wage trends from those of labor productivity might be explained by reference to other variables in the model.\n\nNext, there is price behavior. The standard assumption is that markets are \"imperfectly competitive\", where most businesses have some power to set prices. So the model assumes that the average business sets a unit price (P) as a mark-up (M) over the unit labor cost in production measured at a standard rate of capacity utilization (say, at 90 percent use of plant and equipment) and then adds in the unit materials cost.\n\nThe standardization involves later ignoring deviations from the trend in labor productivity. For example, assume that the growth of labor productivity is the same as that in the trend and that current productivity equals its trend value:\n\nThe markup reflects both the firm's degree of market power and the extent to which overhead costs have to be paid. Put another way, all else equal, M rises with the firm's power to set prices or with a rise of overhead costs relative to total costs.\n\nSo pricing follows this equation:\n\nUMC is unit raw materials cost (total raw materials costs divided by total output). So the equation can be restated as:\n\nThis equation can again be stated as:\n\nNow, assume that both the average price/cost mark-up (M) and UMC are constant. On the other hand, labor productivity grows, as before. Thus, an equation determining the price inflation rate (gP) is:\n\nThen, combined with the wage Phillips curve [equation 1] and the assumption made above about the trend behavior of money wages [equation 2], this price-inflation equation gives us a simple expectations-augmented price Phillips curve:\n\nSome assume that we can simply add in gUMC, the rate of growth of UMC, in order to represent the role of supply shocks (of the sort that plagued the U.S. during the 1970s). This produces a standard short-term Phillips curve:\n\nEconomist Robert J. Gordon has called this the \"Triangle Model\" because it explains short-run inflationary behavior by three factors: demand inflation (due to low unemployment), supply-shock inflation (gUMC), and inflationary expectations or inertial inflation.\n\nIn the \"long run\", it is assumed, inflationary expectations catch up with and equal actual inflation so that gP = gP. This represents the long-term equilibrium of expectations adjustment. Part of this adjustment may involve the adaptation of expectations to the experience with actual inflation. Another might involve guesses made by people in the economy based on other evidence. (The latter idea gave us the notion of so-called rational expectations.)\n\nExpectational equilibrium gives us the long-term Phillips curve. First, with λ less than unity:\n\nThis is nothing but a steeper version of the short-run Phillips curve above. Inflation rises as unemployment falls, while this connection is stronger. That is, a low unemployment rate (less than U*) will be associated with a higher inflation rate in the long run than in the short run. This occurs because the actual higher-inflation situation seen in the short run feeds back to raise inflationary expectations, which in turn raises the inflation rate further. Similarly, at high unemployment rates (greater than U*) lead to low inflation\nrates. These in turn encourage lower inflationary expectations, so that inflation itself drops again.\n\nThis logic goes further if λ is equal to unity, i.e., if workers are able to protect their wages \"completely\" from expected inflation, even in the short run. Now, the Triangle Model equation becomes:\n\nIf we further assume (as seems reasonable) that there are no long-term supply shocks, this can be simplified to become:\n\nAll of the assumptions imply that in the long run, there is only one possible unemployment rate, U* at any one time. This uniqueness explains why some call this unemployment rate \"natural.\"\n\nTo truly understand and criticize the uniqueness of U*, a more sophisticated and realistic model is needed. For example, we might introduce the idea that workers in different sectors push for money wage increases that are similar to those in other sectors. Or we might make the model even more realistic. One important place to look is at the determination of the mark-up, M.\n\nThe Phillips curve equation can be derived from the (short-run) Lucas aggregate supply function. The Lucas approach is very different from that the traditional view. Instead of starting with empirical data, he started with a classical economic model following very simple economic principles.\n\nStart with the aggregate supply function:\n\nwhere Y is log value of the actual output, Y is log value of the \"natural\" level of output, a is a positive constant, P is log value of the actual price level, and P is log value of the expected price level. Lucas assumes that Y has a unique value.\n\nNote that this equation indicates that when expectations of future inflation (or, more correctly, the future price level) are \"totally accurate\", the last term drops out, so that actual output equals the so-called \"natural\" level of real GDP. This means that in the Lucas aggregate supply curve, the \"only\" reason why actual real GDP should deviate from potential—and the actual unemployment rate should deviate from the \"natural\" rate—is because of \"incorrect expectations\" of what is going to happen with prices in the future. (The idea has been expressed first by Keynes, \"General Theory\", Chapter 20 section III paragraph 4).\n\nThis differs from other views of the Phillips curve, in which the failure to attain the \"natural\" level of output can be due to the imperfection or incompleteness of markets, the stickiness of prices, and the like. In the non-Lucas view, incorrect expectations can contribute to aggregate demand failure, but they are not the only cause. To the \"new Classical\" followers of Lucas, markets are presumed to be perfect and always attain equilibrium (given inflationary expectations).\n\nWe re-arrange the equation into:\n\nNext we add unexpected exogenous shocks to the world supply v:\n\nSubtracting last year's price levels P will give us inflation rates, because\n\nand\n\nwhere π and π are the inflation and expected inflation respectively.\n\nThere is also a negative relationship between output and unemployment (as expressed by Okun's law). Therefore, using\nwhere b is a positive constant, U is unemployment, and U is the natural rate of unemployment or NAIRU, we arrive at the final form of the short-run Phillips curve:\n\nThis equation, plotting inflation rate π against unemployment U gives the downward-sloping curve in the diagram that characterises the Phillips curve.\n\nThe New Keynesian Phillips curve was originally derived by Roberts in 1995, and since been used in most state-of-the-art New Keynesian DSGE models like the one of Clarida, Galí, and Gertler (2000).\n\nwhere formula_11. The current expectations of next period's inflation are incorporated as formula_12\n\nIn the 1970s, new theories, such as rational expectations and the NAIRU (non-accelerating inflation rate of unemployment) arose to explain how stagflation could occur. The latter theory, also known as the \"natural rate of unemployment\", distinguished between the \"short-term\" Phillips curve and the \"long-term\" one. The short-term Phillips Curve looked like a normal Phillips Curve, but shifted in the long run as expectations changed. In the long run, only a single rate of unemployment (the NAIRU or \"natural\" rate) was consistent with a stable inflation rate. The long-run Phillips Curve was thus vertical, so there was no trade-off between inflation and unemployment. Edmund Phelps won the Nobel Prize in Economics in 2006 in part for this work. However, the expectations argument was in fact very widely understood (albeit not formally) before Phelps' work on it.\n\nIn the diagram, the long-run Phillips curve is the vertical red line. The NAIRU theory says that when unemployment is at the rate defined by this line, inflation will be stable. However, in the short-run policymakers will face an inflation-unemployment rate tradeoff marked by the \"Initial Short-Run Phillips Curve\" in the graph. Policymakers can therefore reduce the unemployment rate temporarily, moving from point A to point B through expansionary policy. However, according to the NAIRU, exploiting this short-run tradeoff will raise inflation expectations, shifting the short-run curve rightward to the \"new short-run Phillips curve\" and moving the point of equilibrium from B to C. Thus the reduction in unemployment below the \"Natural Rate\" will be temporary, and lead only to higher inflation in the long run.\n\nSince the short-run curve shifts outward due to the attempt to reduce unemployment, the expansionary policy ultimately worsens the exploitable tradeoff between unemployment and inflation. That is, it results in more inflation at each short-run unemployment rate. The name \"NAIRU\" arises because with actual unemployment below it, inflation accelerates, while with unemployment above it, inflation decelerates. With the actual rate equal to it, inflation is stable, neither accelerating nor decelerating. One practical use of this model was to provide an explanation for stagflation, which confounded the traditional Phillips curve.\n\nThe rational expectations theory said that expectations of inflation were equal to what actually happened, with some minor and temporary errors. This in turn suggested that the short-run period was so short that it was non-existent: any effort to reduce unemployment below the NAIRU, for example, would \"immediately\" cause inflationary expectations to rise and thus imply that the policy would fail. Unemployment would never deviate from the NAIRU except due to random and transitory mistakes in developing expectations about future inflation rates. In this perspective, any deviation of the actual unemployment rate from the NAIRU was an illusion.\n\nHowever, in the 1990s in the U.S., it became increasingly clear that the NAIRU did not have a unique equilibrium and could change in unpredictable ways. In the late 1990s, the actual unemployment rate fell below 4% of the labor force, much lower than almost all estimates of the NAIRU. But inflation stayed very moderate rather than accelerating. So, just as the Phillips curve had become a subject of debate, so did the NAIRU.\n\nFurthermore, the concept of rational expectations had become subject to much doubt when it became clear that the main assumption of models based on it was that there exists a single (unique) equilibrium in the economy that is set ahead of time, determined independently of demand conditions. The experience of the 1990s suggests that this assumption cannot be sustained.\n\nThe Phillips curve started as an empirical observation in search of a theoretical explanation. Specifically, the Phillips curve tried to determine whether the inflation-unemployment link was causal or simply correlational. There are several major explanations of the short-term Phillips curve regularity.\n\nTo Milton Friedman there is a short-term correlation between inflation shocks and employment. When an inflationary surprise occurs, workers are fooled into accepting lower pay because they do not see the fall in real wages right away. Firms hire them because they see the inflation as allowing higher profits for given nominal wages. This is a movement along the Phillips curve as with change A. Eventually, workers discover that real wages have fallen, so they push for higher money wages. This causes the Phillips curve to shift upward and to the right, as with B. Some research underlines that some implicit and serious assumptions are actually in the background of the Friedmanian Phillips curve. This information asymmetry and a special pattern of flexibility of prices and wages are both necessary if one wants to maintain the mechanism told by Friedman. However, as it is argued, these presumptions remain completely unrevealed and theoretically ungrounded by Friedman.\n\nEconomists such as Edmund Phelps reject this theory because it implies that workers suffer from money illusion. According to them, rational workers would only react to real wages, that is, inflation adjusted wages. However, one of the characteristics of a modern industrial economy is that workers do not encounter their employers in an atomized and perfect market. They operate in a complex combination of imperfect markets, monopolies, monopsonies, labor unions, and other institutions. In many cases, they may lack the bargaining power to \"act on\" their expectations, no matter how rational they are, or their perceptions, no matter how free of money illusion they are. It is not that high inflation \"causes\" low unemployment (as in Milton Friedman's theory) as much as \"vice versa\": Low unemployment raises worker bargaining power, allowing them to successfully push for higher nominal wages. To protect profits, employers raise prices.\n\nSimilarly, built-in inflation is not simply a matter of subjective \"inflationary expectations\" but also reflects the fact that high inflation can gather momentum and continue beyond the time when it was started, due to the objective price/wage spiral.\n\nHowever, other economists, like Jeffrey Herbener, argue that price is market-determined and competitive firms cannot simply raise prices. They reject the Phillips curve entirely, concluding that unemployment's influence is only a small portion of a much larger inflation picture that includes prices of raw materials, intermediate goods, cost of raising capital, worker productivity, land, and other factors.\n\nRobert J. Gordon of Northwestern University has analyzed the Phillips curve to produce what he calls the triangle model, in which the actual inflation rate is determined by the sum of\n\n\nThe last reflects inflationary expectations and the price/wage spiral. Supply shocks and changes in built-in inflation are the main factors shifting the short-run Phillips Curve and changing the trade-off. In this theory, it is not only inflationary expectations that can cause stagflation. For example, the steep climb of oil prices during the 1970s could have this result.\n\nChanges in built-in inflation follow the partial-adjustment logic behind most theories of the NAIRU:\n\n\nIn between these two lies the NAIRU, where the Phillips curve does not have any inherent tendency to shift, so that the inflation rate is stable. However, there seems to be a range in the middle between \"high\" and \"low\" where built-in inflation stays stable. The ends of this \"non-accelerating inflation range of unemployment rates\" change over time.\n\n\n\n"}
{"id": "23774097", "url": "https://en.wikipedia.org/wiki?curid=23774097", "title": "Placebo button", "text": "Placebo button\n\nA placebo button is a push-button or other control which has apparent functionality but has no physical effect when pressed. Such buttons can appear to work, by lighting up or otherwise reacting, which rewards the user by giving them an illusion of control. They are commonly placed in situations where it would have once been useful to have such a button but the system now operates automatically, such as a manual thermostat in a temperature-regulated office. Were the control removed entirely, some users would feel frustrated at the awareness they were not in control.\n\nIn many cases a button may appear to do nothing but in fact causes behavior that is not immediately apparent; this can give the appearance of it being a placebo button.\n\nMany walk buttons at pedestrian crossings were once functional in New York City, but now serve as placebo buttons.\n\nIn the United Kingdom and Hong Kong, pedestrian push-buttons on crossings using the Split Cycle Offset Optimisation Technique may or may not have any real effect on crossing timings, depending on their location and the time of day, and some junctions may be completely automated, with push-buttons which do not have any effect at all. In other areas the buttons have an effect only during the night. Some do not affect the actual lights timing but requires the button having been pressed to activate pedestrian green lights.\n\nLondon Underground 1992 stock, 1995 stock and 1996 stock include door control buttons. The doors are normally driver operated, but a switch in the driving cab can hand control to passengers once the driver activates the buttons, much like mainline railway stock. In addition, London Underground D stock used on the District line were built with door open buttons which worked much like those of the 1992, 1995 and 1996 stock. These buttons were subsequently removed when the stock was refurbished.\n\nIt has been reported that the temperature set point adjustment on thermostats in many office buildings in the United States is non-functional, installed to give tenants' employees a similar illusion of control. In some cases they act as input devices to a central control computer but in others they serve no purpose other than to keep employees contented.\n\nA common implementation in buildings with an HVAC central control computer is to allow the thermostats to provide a graded level of control. Temperatures in such a system are governed by the central controller's settings, which are typically set by the building maintenance staff or HVAC engineers. The individual thermostats in various offices provide the controller with a temperature reading of the zone (provided the thermocouples are not installed as inline duct sensors), but also serve as modifiers for the central controller's set point. While the thermostat may include settings from, for example, , the actual effect of the thermostat is to apply \"pressure\" to the central controller's set point. Thus, if the controller's setting is , setting the thermostat to its maximum warm or cool settings will deflect the output temperature, generally by only a few degrees Fahrenheit (about two degrees Celsius) at most. So, although the thermostat can be set to its lowest marking of , in reality, it may change the HVAC system's output temperature only to . In this case, the thermostat has a \"swing\" of 2 °C (4 °F) — it can alter the produced temperature from the main controller's set point by a maximum of 1 °C (2 °F) in either direction. Consequently, while not purely a placebo, the thermostat in this setup does not provide the level of control that is expected, but the combination of the lower setting number and the feeling of a slight change in temperature can induce the office occupants to believe that the temperature was significantly decreased.\n\nPlacebo thermostats work on two psychological principles, which are classical conditioning and the placebo effect. First, placebo thermostats work in accordance with classical conditioning. Classical conditioning was first discovered by Ivan Pavlov and is a type of learning which pairs a stimulus with a physiological response. Applied to placebo thermostats, this is when the employee adjusts the thermostat and hears the noise of hissing or a fan running and consequently physically feels more content. This is due to the countless trials involving the thermostat in their own home, which actually works. The employee has paired the sound of hissing or a fan running to being more physically content due to the actual temperature change and therefore when they experience the noise at work they feel the same way even though there is no change in temperature. As long as individuals get the result they are looking for (noise associated with temperature change) they will continue with the practice (changing the placebo thermostat). Additionally, placebo thermostats work due to the placebo effect. The placebo effect works on the basis that individuals will experience what they believe they will experience. This is attributed to Expectancy theory, which states that the placebo effect is mediated by overt expectancies. The most common example is in medical testing: inactive sugar pills are given to patients who are told they are actually medicine. Some patients will experience relief from symptoms regardless. According to expectancy theory, if people believe they are going to experience a temperature change after changing a placebo thermostat they may psychologically experience one without an actual change happening. Both psychological concepts of classical conditioning and the placebo effect may play a role in the effectiveness of placebo thermostats.\n\n"}
{"id": "3050350", "url": "https://en.wikipedia.org/wiki?curid=3050350", "title": "Plural society", "text": "Plural society\n\nA plural society is defined by Fredrik Barth as a society combining ethnic contrasts: the economic interdependence of those groups, and their ecological specialization (i.e., use of different environmental resources by each ethnic group). The ecological interdependence, or the lack of competition, between ethnic groups may be based on the different activities in the same region or on long–term occupation of different regions in the\nDefined by J S Furnivall as a medley of peoples - European, Chinese, Indian and native, who do mix but do not combine. Each group holds by its own religion, its own culture and language, its own ideas and ways. As individuals they meet, but only in the marketplace in buying and selling. There is a plural society, with different sections of the community living side by side, within the same political unit.\n\nDuring research about plural societies, Asim Ejaz, Student of M.phil Political Science in Islamia university bahawalpur, Pakistan, presented his analytical summary about the book of Arend Lijphart, \"democracy in plural societies\" that it is so much difficult to achieve and stable democratic government in plural society. As Aristotle says about stable governing system that, “a state aims at being, as far as it can be, a society composed of equal & peers”. For the stability of democratic regimes, there must be social homogeneity and political consensus among the deep social divisions, and, there must be termination of political differences. There are these criteria, because, the factors that help in producing instability and breakdown of democracies.\nArend Lijphart, therefore, used a particular form of democracy, “Consociational Democracy”, which is, according to him, may be difficult but it is not at all impossible to achieve and maintain stable democratic government in plural societies.\n\nConsociational democracy can be characterized by the cooperative attitude and behavior of the leaders of the different segments of the population. In other meanings, there will be elite cooperation. This model of Consociational democracy is both, normative and an empirical. In Austria, Belgium, Netherland and Switzerland, there are sharp political divisions, but due to Consoiciational democracy, there is existence of political stability. In Austria, political stability can be observed in the forms of Catholic-Socialist elite cooperation and grand coalition.\n\nIn non-Western countries, as Arend Lijphart highlights twin problems, and there are, sharp cleavages of various kind and political stability. For the successful democratic regimes in the third world, due to plural societies, Consociational democracy is based, also on normative model. A Plural society is a society, divided by segmental cleavages, and, political stability is characterized by system maintenance, legitimacy, civil order and effectiveness. Without these four elements, which are also interdependent, political stability cannot exist. According to Gabriel Almond, there are four types of political systems;\n\n1) Anglo-American Political System\n2) Continental European Political System\n3) Pre-Industrial Political System\n4) Totalitarian Political System.\n\nHe says that Anglo-American and Continental European Political systems show democratic regimes. The Anglo-American political system is a homogenous and secular political system, while the Continental European political system is characterized by a fragmentation of political culture due to plural societies within European countries.\n\nAccording to Gabriel Almond, Separation of power doctrine is also concerned with political stability. He extends the idea of “separation of power” from three formal branches of government, executives and legislature, to informal political subcultures like parties, interest groups and the media of communication. He much more emphasizes on input structures than the output structures.\n\nDuverger and Neumann argue that there is a close relationship between the number of parties and democratic stability, but a two party system not only seems to correspond to the nature of things because it can moderate better than multiparty systems. In other words, a two party system is the best aggregation. In Switzerland, there is a multiparty system, while in Austria, there is a two party system.\n\nArend Lijphart says that there are deep divisions between different segments of the population and absence of a unifying consensus in most of the Asian, African and South American countries like Guyana, Surinam and Trinidad. According to Cliffard Geertz, Communal attachment is called “primordial loyalties”, which may be based on language, religion, custom, region, race or assumed blood ties. Each communal group hold its assumed ties, therefore there is political instability and breakdown of democracy up until now.\n\nHe argues that due to political development, western countries have created homogeneity among their plural societies, as like idealize British society. But Gabriel Almond says that, in Continental European political system, there is no secularism and political homogeneity, but there is cultural homogeneity. He argues that, non-western countries would become more comprehensive and less remote they use this continental type, which is based on a multi-racial (multi-national) society and lacking in strong consensus.\n\nFurnivall says that democracy is achieved by the European countries with the help of Consociationalism, and, there is fulfillment of the requirements and demands of the divided societies through appropriate processes. On the other hand, in non-western countries, there is lack of strength in social will and social unity due to divided society, and, it is dangerous for both, the democracy and a considerable degree of political unity.\n\nAs Arend Lijhpart argues that there is constitutionalization for the segments of plural societies, and its better solution is consociational or semi-consociational democratic system. This system provides the facility of Mutual Veto regarding the decision making process on the specific issues within the country, to all the segments of society with equality. but Arend Lijhpart highlights Malaysia and Lebanon for its perfect example. In Lebanon, there are Shia's and Sunni's in the Muslim Segment while Christians are in the minority. Similarly in Malaysia, there are Chinese with local Communities.\n\n\n"}
{"id": "229719", "url": "https://en.wikipedia.org/wiki?curid=229719", "title": "Popularity", "text": "Popularity\n\nIn sociology, the popularity of a person, idea, item or other concept can be defined in terms of liking, attraction, dominance and superiority. With respect to interpersonal popularity, there are two primary divisions: perceived and sociometric.\n\nAccording to psychologist Tessa Lansu at the Radboud University Nijmegen, \"Popularity [has] to do with being the middle point of a group and having influence on it.\"\n\nThe term \"Popularity\" is borrowed from the Latin term \"popularis\", which originally meant \"common.\" The current definition of the word popular, the \"fact or condition of being well liked by the people\", was first seen in 1601.\n\nWhile popularity is a trait often ascribed to an individual, it is an inherently social phenomenon and thus can only be understood in the context of groups of people. Popularity is a collective perception, and individuals report the consensus of a group's feelings towards an individual or object when rating popularity. It takes a group of people to like something, so the more that people advocate for something or claim that someone is best liked, the more attention it will get, and the more popular it will be deemed.\n\nNotwithstanding the above, popularity as a concept can be applied, assigned, or directed towards objects such as songs, movies, websites, activities, soaps, foods etc. Together, these objects collectively make up popular culture, or the consensus of mainstream preferences in society. In essence, anything, human or non-human, can be deemed popular.\n\nFor many years, popularity research focused on a definition of popularity that was based on being \"well liked.\" Eventually, it was discovered that those who are \"perceived\" as popular are not necessarily the most well liked as originally assumed. When students are given the opportunity to freely elect those they like most and those they perceive as popular, a discrepancy often emerges. This is evidence that there are two main forms of personal popularity that social psychology recognizes, sociometric popularity and perceived popularity.\n\nSociometric popularity can be defined by how liked an individual is. This \"liking\" is correlated with prosocial behaviours. Those who act in prosocial ways are likely to be deemed sociometrically popular. Often they are known for their interpersonal abilities, their empathy for others, and their willingness to cooperate non-aggressively. This is a more private judgement, characterized by likeability, that will not generally be shared in a group setting. Often, it is impossible to know whom individuals find popular on this scale unless confidentiality is ensured.\n\nPerceived popularity is used to describe those individuals who are known among their peers as being popular. Unlike sociometric popularity, perceived popularity is often associated with aggression and dominance and is not dependent on prosocial behaviors. This form of popularity is often explored by the popular media. Notable works dealing with perceived popularity include \"Mean Girls\", \"Odd Girl Out\", and \"Ferris Bueller's Day Off\". Individuals who have perceived popularity are often highly socially visible and frequently emulated but rarely liked. Since perceived popularity is a measure of visible reputation and emulation, this form of popularity is most openly discussed, agreed upon within a group, and what most people refer to when they call someone popular.\n\nTo date, only one comprehensive theory of interpersonal popularity has been proposed: that of A. L. Freedman in the book \"Popularity Explained.\" The 3 Factor Model proposed attempts to reconcile the two concepts of sociometric and perceived popularity by combining them orthogonally and providing distinct definitions for each. In doing so, it reconciles the counter intuitive fact that liking does not guarantee perceived popularity nor does perceived popularity guarantee being well liked.\n\n''Popularity Explained'' was first published as a blog before being converted to a book and various versions have been available on Amazon since 2013.\n\nThere are four primary concepts that \"Popularity Explained\" relies on.\n\n\nAccording to Freedman, an individual's place in the social landscape is determined by a combination of three factors: \"what\" they are; \"who\" they are; and the situation.\n\nOne of the most widely agreed upon theories about what leads to an increased level of popularity for an individual is the perceived value which that individual brings to the group. This seems to be true for members of all groups, but is especially demonstrable in groups that exist for a specific purpose. For example, sports teams exist with the goal of being successful in competitions against other sports teams. Study groups exist so that the members of the group can mutually benefit from one another's academic knowledge. In these situations, leaders often emerge because other members of the group perceive them as adding a lot of value to the group as a whole. On a sports team, this means that the best players are usually elected captain and in study groups people might be more inclined to like an individual who has a lot of knowledge to share. It has been argued that this may be a result of our evolutionary tendencies to favor individuals who are most likely to aid in our own survival.\n\nIt is also of note that the actual value which an individual brings to a group is not of consequence in determining his or her popularity; the only thing that is important is his or her value as perceived by the other members of the group. While perceived value and actual value may often overlap, this is not a requisite and it has been shown that there are instances in which an individual's actual value is relatively low, but they are perceived as highly valuable nevertheless.\n\nAttractiveness, specifically physical attractiveness, has been shown to have very profound effects on popularity. People who are physically attractive are more likely to be thought of as possessing positive traits. People who are attractive are expected to perform better on tasks and are more likely to be trusted. Additionally, they are judged to possess many other positive traits such as mental health, intelligence, social awareness, and dominance.\n\nAdditionally, people who are of above average attractiveness are assumed to also be of above average value to the group. Research shows that attractive people are often perceived to have many positive traits based on nothing other than their looks, regardless of how accurate these perceptions are. This phenomenon is known as the Halo effect This means that, in addition to being more well-liked, attractive people are more likely to be seen as bringing actual value to the group, even when they may be of little or no value at all. In essence, physically attractive people are given the benefit of the doubt while less attractive individuals must prove that they are bringing value to the group. It has been shown empirically that being physically attractive is correlated with both sociometric and perceived popularity. Some possible explanations for this include increased social visibility and an increased level of tolerance for aggressive, social interactions that may increase perceived popularity.\n\nThe degree to which an individual is perceived as popular is often highly correlated with the level of aggression with which that individual interacts with his or her peers. There are two main categories of aggression, relational and overt, both of which have varying consequences for popularity depending on several factors, such as the gender and attractiveness of the aggressor.\n\nRelational aggression is nonviolent aggression that is emotionally damaging to another individual. Examples of relationally aggressive activities include ignoring or excluding an individual from a group, delivering personal insults to another person, and the spreading of rumors. Relational aggression is more frequently used by females than males.\n\nIt has been found that relational aggression almost always has a strongly negative relationship with sociometric popularity but can have a positive relationship with perceived popularity depending on the perceived level of attractiveness of the aggressor. For an aggressor who is perceived as unattractive, relational aggression, by both males and females, leads to less perceived popularity. For an attractive aggressor however, relational aggression has been found to actually have a positive relationship with perceived popularity.\n\nThe relationship between attractiveness and aggression is further intertwined by the finding that increased levels of physical attractiveness actually further decreased the sociometric popularity of relationally aggressive individuals.\n\nIn short, the more physically attractive an individual is, the more likely they are to experience decreased levels of sociometric popularity but increased levels of perceived popularity for engaging in relationally aggressive activities.\n\nOvert aggression is aggression that involves individuals physically interacting with each other in acts such as pushing, hitting, kicking or otherwise causing physical harm or submission in the other person. This includes threats of violence and physical intimidation as well.\n\nIt has been shown that overt aggression directly leads to perceived popularity when the aggressor is attractive. Experiments that are controlled for levels of physical attractiveness show that individuals who are attractive and overtly aggressive have a higher degree of perceived popularity than attractive non-overtly aggressive individuals. This was found to be true to a small degree for females and a large degree for males.\n\nAttractive individuals who are overtly aggressive barely suffer any consequences in terms of sociometric popularity. This is a key difference between overt and relational aggression because relational aggression has a strongly negative relationship on sociometric popularity, especially for attractive individuals. For unattractive individuals, there is again a strongly negative relationship between overt aggression and sociometric popularity. This means that attractive individuals stand to gain a lot of perceived popularity at the cost of very little sociometric popularity by being overtly aggressive while unattractive individuals stand to gain very little perceived popularity from acts of overt aggression but will be heavily penalized with regards to sociometric popularity.\n\nAccording to Talcott Parsons, as rewritten by Fons Trompenaars,\nthere are four main types of culture : marked by love/hate (Middle East, Mediterranean, Latin America) approval/criticism (United Kingdom, Canada, Scandinavia, Germanic countries) esteem/contempt (Japan, Eastern Asia). The last one, responsiveness/rejection, is typical for the United States. \n\nThere is no effort for popularity in Northern or Southern Europe, Latin America or Asia. This emotional bonding is specific for the high schools of the United States. \nIn the love/hate cultures, the family and close friends are more important than popularity. In the approval/criticism cultures, actions are more important than persons, no strong links develop during school.\n\nPopularity is gauged primarily through social status. Because of the importance of social status, peers play the primary role in social decision making so that individuals can increase the chances that others like them. However, as children, individuals tend to do this through friendship, academics, and interpersonal conduct. By adulthood, work and romantic relationships become much more important. This peer functioning and gaining popularity is a key player in increasing interest in social networks and groups in the workplace. To succeed in such a work environment, adults then place popularity as a higher priority than any other goal, even romance.\n\nThese two types of popularity, perceived popularity and sociometric popularity, are more correlated for girls than they are for boys. However, it is said that men can possess these qualities to a larger extent, making them more likely to be a leader, more powerful, and more central in a group, but also more likely than women to be socially excluded. Boys tend to become popular based on athletic ability, coolness, toughness, and interpersonal skills; however, the more popular a boy gets, the worse he tends to do on his academic work. On the other hand, this negative view of academics is not seen at all in popular girls, who gain popularity based on family background (primarily socioeconomic status), physical appearance, and social ability. Boys are also known to be more competitive and rule focused, whereas girls have more emotional intimacy.\n\nIn some instances, it has been found that in predominantly white high schools, attractive non-white students are on average significantly more sociometrically popular than equally attractive white students. One theory that has been put forth to explain this phenomenon is a high degree of group cohesiveness among minority students compared with the relative lack of cohesion amongst members of the majority. Since there is more cohesion, there is more availability for one person to be liked by many since they are all in contact. This acts like Zipf's Law, where the cohesion is a confounding factor that forces the greater links in the smaller minority, causing them to be more noticed and thus more popular. When considering race as a predictor for perceived popularity by asking a class how popular and important each other person is, African American students were rated most popular by their peers. Popularity in race was found to be correlated with athleticism, and because African Americans have a stereotype of being better at sports than individuals of other races, they are viewed as more popular. Additionally, White and Hispanic children were rated as more popular the better they succeeded in school and came from a higher socioeconomic background. No single factor can explain popularity, but instead the interaction between many factors such as race and athleticism vs. academics.\n\nMore tasks in the workplace are being done in teams, leading to a greater need of people to seek and feel social approval. In academic settings, a high social standing among peers is associated with positive academic outcomes. Popularity also leads to students in academic environments to receive more help, have more positive relationships and stereotypes, and be more approached by peers. While this is the research found in schools, it is likely to be generalized to a workplace.\n\nPopularity is positively linked to job satisfaction, individual job performance, and group performance. The popular worker, besides just feeling more satisfied with his job, feels more secure, believes he has better working conditions, trusts his supervisor, and possesses more positive opportunities for communication with both management and co-workers, causing a greater feeling of responsibility and belongingness at work. Others prefer to work with popular individuals, most notably in manual labor jobs because, although they might not be the most knowledgeable for the job, they are approachable, willing to help, cooperative in group work, and are more likely to treat their coworkers as an equal. If an employee feels good-natured, genial, but not overly independent, more people will say that they most prefer to work with that employee.\n\nAccording to the mere-exposure effect, employees in more central positions that must relate to many others throughout the day, such as a manager, are more likely to be considered popular.\nThere are many characteristics that contribute to popularity:\n\nWith a greater focus on groups in the workplace, it is essential that leaders effectively deal with and mediate groups to avoid clashing. Sometimes a leader does not need to be popular to be effective, but there are a few characteristics that can help a leader be more accepted and better liked by his group. Without group or team cohesiveness, there is no correlation between leadership and popularity; however, when a group is cohesive, the higher up someone is in the leadership hierarchy, the more popular they are for two reasons. First, a cohesive group feels more personal responsibility for their work, thus placing more value on better performance. Cohesive members see leaders as taking a bulk of the work and investing a lot of personal time, so when they see a job's value they can ascribe its success to the leader. This greatest contribution principle is perceived as a great asset to the team, and members view the leader more favorably and he gains popularity. Secondly, cohesive groups have well established group values. Leaders can become more popular in these groups by realizing and acting on dominant group values. Supporting group morals and standards leads to high positive valuation from the group, leading to popularity.\n\nPoplularity is a term widely applicable to the modern era thanks primarily to social networking technology. Being \"liked\" has been taken to a completely different level on ubiquitous sites such as Facebook.\n\nPopularity is a social phenomenon but it can also be ascribed to objects that people interact with. Collective attention is the only way to make something popular, and information cascades play a large role in rapid rises in something's popularity. Rankings for things in popular culture, like movies and music, often do not reflect the public's taste, but rather the taste of the first few buyers because social influence plays a large role in determining what is popular and what is not through an information cascade.\n\nInformation cascades have strong influence causing individuals to imitate the actions of others, whether or not they are in agreement. For example, when downloading music, people don't decide 100% independently which songs to buy. Often they are influenced by charts depicting which songs are already trending. Since people rely on what those before them do, one can manipulate what becomes popular among the public by manipulating a website's download rankings. Experts paid to predict sales often fail but not because they are bad at their jobs; instead, it is because they cannot control the information cascade that ensues after first exposure by consumers. Music is again, an excellent example. Good songs rarely perform poorly on the charts and poor songs rarely perform very well, but there is tremendous variance that still makes predicting the popularity of any one song very difficult.\n\nExperts can determine if a product will sell in the top 50% of related products or not, but it is difficult to be more specific than that. Due to the strong impact that influence plays, this evidence emphasizes the need for marketers. They have a significant opportunity to show their products in the best light, with the most famous people, or being in the media most often. Such constant exposure is a way of gaining more product followers. Marketers can often make the difference between an average product and a popular product. However, since popularity is primarily constructed as a general consensus of a group's attitude towards something, word-of-mouth is a more effective way to attract new attention. Websites and blogs start by recommendations from one friend to another, as they move through social networking services. Eventually, when the fad is large enough, the media catches on to the craze. This spreading by word-of-mouth is the social information cascade that allows something to grow in usage and attention throughout a social group until everyone is telling everyone else about it, at which point it is deemed popular.\n\nIndividuals also rely on what others say when they know that the information they are given could be completely incorrect. This is known as groupthink. Relying on others to influence one's own decisions is a very powerful social influence, but can have negative impacts.\n\nThe popularity of many different things can be described by Zipf's powerlaw which posits that there is a low frequency of very large quantities and a high frequency of low quantities. This illustrates popularity of many different objects.\n\nFor example, there are few very popular websites, but many websites have small followings. This is the result of interest; as many people use e-mail, it is common for sites like Yahoo! to be accessed by large numbers of people; however, a small subset of people would be interested in a blog on a particular video game. In this situation, only Yahoo! would be deemed a popular site by the public. This can additionally be seen in social networking services, such as Facebook. The majority of people have about 130 friends, while very few people have larger social networks. However, some individuals do have more than 5,000 friends. This reflects that very few people can be extremely well-connected, but many people are somewhat connected. The number of friends a person has, has been a way to determine how popular an individual is, so the small number of people who have an extremely high number of friends is a way of using social networking services, like Facebook, to illustrate how only a few people are deemed popular.\nPopular people may not be those who are best liked interpersonally by their peers, but they do receive most of the positive behavior from coworkers when compared to nonpopular workers. This is a result of the differences between sociometric and perceived popularity. When asked who is most popular, employees typically respond based on perceived popularity; however, they really prefer the social interactions with those who are more sociometrically popular. For each individual to ensure that they are consistent with the group's popularity consensus, those who are high in perceived popularity are treated with the same positive behaviors as those who are more interpersonally, but privately, liked by specific individuals. Well-liked workers are most likely to get salary increases and promotions, while disliked (unpopular) workers are the first to get their salary cut back or laid off during recessions.\n\nDuring interactions with others in the work environment, more popular individuals receive more organizational citizenship behavior (helping and courteousness from others) and less counter productive work behavior (rude reactions and withheld information) than those who are considered less popular in the workplace. Coworkers agree with each other on who is and who is not popular and, as a group, treat popular coworkers more favorably. While popularity has proven to be a big determiner of getting more positive feedback and interactions from coworkers, such a quality matters less in organizations where workloads and interdependence is high, such as the medical field.\n\nIn many instances, physical appearance has been used as one indicator of popularity. Attractiveness plays a large role in the workplace and physical appearance influences hiring, whether or not the job might benefit from it. For example, some jobs, such as salesperson, benefit from attractiveness when it comes down to the bottom line, but there have been many studies which have shown that, in general, attractiveness is not at all a valid predictor of on-the-job performance. Many individuals have previously thought this was only a phenomenon in the more individualistic cultures of the Western world, but research has shown that attractiveness also plays a role in hiring in collectivist cultures as well. Because of the prevalence of this problem during the hiring process in all cultures, researchers have recommended training a group to ignore such influencers, just like legislation has worked to control for differences in sex, race, and disabilities.\n\n\n"}
{"id": "2379996", "url": "https://en.wikipedia.org/wiki?curid=2379996", "title": "Population health", "text": "Population health\n\nPopulation health has been defined as \"the health outcomes of a group of individuals, including the distribution of such outcomes within the group\". It is an approach to health that aims to improve the health of an entire human population. This concept does not refer to animal or plant populations. It has been described as consisting of three components. These are \"health outcomes, patterns of health determinants, and policies and interventions\". A priority considered important in achieving the aim of Population Health is to reduce health inequities or disparities among different population groups due to, among other factors, the social determinants of health, SDOH. The SDOH include all the factors (social, environmental, cultural and physical) that the different populations are born into, grow up and function with throughout their lifetimes which potentially have a measurable impact on the health of human populations. The Population Health concept represents a change in the focus from the individual-level, characteristic of most mainstream medicine. It also seeks to complement the classic efforts of public health agencies by addressing a broader range of factors shown to impact the health of different populations. The World Health Organization's Commission on Social Determinants of Health, reported in 2008, that the SDOH factors were responsible for the bulk of diseases and injuries and these were the major causes of health inequities in all countries. In the US, SDOH were estimated to account for 70% of avoidable mortality.\n\nFrom a population health perspective, health has been defined not simply as a state free from disease but as \"the capacity of people to adapt to, respond to, or control life's challenges and changes\". The World Health Organization (WHO) defined health in its broader sense in 1946 as \"a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity.\"\n\nHealthy People 2020 is a web site sponsored by the US Department of Health and Human Services, representing the cumulative effort of 34 years of interest by the Surgeon General's office and others. It identifies 42 topics considered social determinants of health and approximately 1200 specific goals considered to improve population health. It provides links to the current research available for selected topics and identifies and supports the need for community involvement considered essential to address these problems realistically.\n\nRecently, human role has been encouraged by the influence of population growth there has been increasing interest from epidemiologists on the subject of economic inequality and its relation to the health of populations. There is a very robust correlation between socioeconomic status and health. This correlation suggests that it is not only the poor who tend to be sick when everyone else is healthy, heart disease, ulcers, type 2 diabetes, rheumatoid arthritis, certain types of cancer, and premature aging. Despite the reality of the SES Gradient, there is debate as to its cause. A number of researchers (A. Leigh, C. Jencks, A. Clarkwest—see also Russell Sage working papers) see a definite link between economic status and mortality due to the greater economic resources of the better-off, but they find little correlation due to social status differences.\n\nOther researchers such as Richard G. Wilkinson, J. Lynch, and G.A. Kaplan have found that socioeconomic status strongly affects health even when controlling for economic resources and access to health care. Most famous for linking social status with health are the Whitehall studies—a series of studies conducted on civil servants in London. The studies found that, despite the fact that all civil servants in England have the same access to health care, there was a strong correlation between social status and health. The studies found that this relationship stayed strong even when controlling for health-affecting habits such as exercise, smoking and drinking. Furthermore, it has been noted that no amount of medical attention will help decrease the likelihood of someone getting type 1 diabetes or rheumatoid arthritis—yet both are more common among populations with lower socioeconomic status. Lastly, it has been found that amongst the wealthiest quarter of countries on earth (a set stretching from Luxembourg to Slovakia) there is no relation between a country's wealth and general population health—suggesting that past a certain level, absolute levels of wealth have little impact on population health, but relative levels within a country do.\nThe concept of psychosocial stress attempts to explain how psychosocial phenomenon such as status and social stratification can lead to the many diseases associated with the SES gradient. Higher levels of economic inequality tend to intensify social hierarchies and generally degrades the quality of social relations—leading to greater levels of stress and stress related diseases. Richard Wilkinson found this to be true not only for the poorest members of society, but also for the wealthiest. Economic inequality is bad for everyone's health. \nInequality does not only affect the health of human populations. David H. Abbott at the Wisconsin National Primate Research Center found that among many primate species, less egalitarian social structures correlated with higher levels of stress hormones among socially subordinate individuals. Research by Robert Sapolsky of Stanford University provides similar findings.\n\nThere is well-documented variation in health outcomes and health care utilization & costs by geographic variation in the U.S., down to the level of Hospital Referral Regions (defined as a regional health care market, which may cross state boundaries, of which there are 306 in the U.S.). There is ongoing debate as to the relative contributions of race, gender, poverty, education level and place to these variations. The Office of Epidemiology of the Maternal and Child Health Bureau recommends using an analytic approach (Fixed Effects or hybrid Fixed Effects) to research on health disparities to reduce the confounding effects of neighborhood (geographic) variables on the outcomes.\n\nFamily planning programs (including contraceptives, sexuality education, and promotion of safe sex) play a major role in population health. Family planning is one of the most highly cost-effective interventions in medicine. Family planning saves lives and money by reducing unintended pregnancy and the transmission of sexually transmitted infections.\n\nFor example, the United States Agency for International Development lists as benefits of its international family planning program:\n\n\nOne method to improve population health is population health management (PHM), which has been defined as \"the technical field of endeavor which utilizes a variety of individual, organizational and cultural interventions to help improve the morbidity patterns (i.e., the illness and injury burden) and the health care use behavior of defined populations\". PHM is distinguished from disease management by including more chronic conditions and diseases, by use of \"a single point of contact and coordination\", and by \"predictive modeling across multiple clinical conditions\". PHM is considered broader than disease management in that it also includes \"intensive care management for individuals at the highest level of risk\" and \"personal health management... for those at lower levels of predicted health risk\". Many PHM-related articles are published in \"Population Health Management\", the official journal of .\n\nThe following road map has been suggested for helping healthcare organizations navigate the path toward implementing effective population health management:\n\nHealthcare reform is driving change to traditional hospital reimbursement models. Prior to the introduction of the Patient Protection and Affordable Care Act (PPACA), hospitals were reimbursed based on the volume of procedures through fee-for-service models. Under the PPACA, reimbursement models are shifting from volume to value. New reimbursement models are built around pay for performance, a value-based reimbursement approach, which places financial incentives around patient outcomes and has drastically changed the way US hospitals must conduct business to remain financially viable. In addition to focusing on improving patient experience of care and reducing costs, hospitals must also focus on improving the health of populations (IHI Triple Aim).\n\nAs participation in value-based reimbursement models such as accountable care organizations (ACOs) increases, these initiatives will help drive population health. Within the ACO model, hospitals have to meet specific quality benchmarks, focus on prevention, and carefully manage patients with chronic diseases. Providers get paid more for keeping their patients healthy and out of the hospital. Studies have shown that inpatient admission rates have dropped over the past ten years in communities that were early adopters of the ACO model and implemented population health measures to treat \"less sick\" patients in the outpatient setting. A study conducted in the Chicago area showed a decline in inpatient utilization rates across all age groups, which was an average of a 5% overall drop in inpatient admissions.\n\nHospitals are finding it financially advantageous to focus on population health management and keeping people in the community well. The goal of population health management is to improve patient outcomes and increase health capital. Other goals include preventing disease, closing care gaps, and cost savings for providers. In the last few years, more effort has been directed towards developing telehealth services, community-based clinics in areas with high proportion of residents using the emergency department as primary care, and patient care coordinator roles to coordinate healthcare services across the care continuum.\n\nHealth can be considered a capital good; health capital is part of human capital as defined by the Grossman model. Health can be considered both an investment good and consumption good. Factors such as obesity and smoking have negative effects on health capital, while education, wage rate, and age may also impact health capital. When people are healthier through preventative care, they have the potential to live a longer and healthier life, work more and participate in the economy, and produce more based on the work done. These factors all have the potential to increase earnings. Some states, like New York, have implemented statewide initiatives to address population health. In New York state there are 11 such programs. One example is the Mohawk Valley Population Health Improvement Program (http://www.mvphip.org/). These programs work to address the needs of the people in their region, as well as assist their local community based organizations and social services to gather data, address health disparities, and explore evidence-based interventions that will ultimately lead to better health for everyone. Following a similar approach, Cullati et al. developed a theoretical framework for the development and onset of vulnerability in later life based on the concept of \"reserves\". The advantages to use the concept of reserves in interdisciplinary studies, as compared with related concepts such as resources and capital, is to strengthen the importance of constitution and sustainability of reserves (the “use it or lose it” paradigm) and the presence of thresholds, below which functioning becomes challenging. \n\n\n"}
{"id": "51902843", "url": "https://en.wikipedia.org/wiki?curid=51902843", "title": "Population health policies and interventions", "text": "Population health policies and interventions\n\nPopulation health, a field which focuses on the improvement of the health outcomes for a group of individuals, has been described as consisting of three components: \"health outcomes, patterns of health determinants, and policies and interventions\". Policies and Interventions define the methods in which health outcomes and patterns of health determinants are implemented. Policies which are helpful \"improve the conditions under which people live\". Interventions encourage healthy behaviors for individuals or populations through \"program elements or strategies designed to produce behavior changes or improve health status\".\n\nPolicies and interventions are needed due to the inequalities amongst populations and the inconsistent way care is administered. Policies can include \"necessary community and personal social and health services\" as well as taxes on alcohol and soft drinks and implement smoking cessation policies. Interventions can include therapeutic or preventative health care and may also include actions taken by the individual or by someone on behalf of the individual. The application of population health is determined by the policies and interventions which can be implemented within an organization, city, state or country.\n\nCountries, states, provinces and providers across the globe are shifting towards better systems to respond to inconsistent health outcomes, mitigate decreasing margins and replace outdated methods such as fee-for-service health delivery. Payment model reforms, including the Accountable Care Organization (ACO), provide roadmaps for healthcare reform and drive many of its constituents towards more effective and innovative means for improving health outcomes. Population health management is a common approach for resolving these challenges but it involves new methods, tools, systems and implementations to correct inefficiencies and improve health outcomes.\n\nPopulation health tools and computer systems include data exchange, large datasets, and advanced software which are used to supply data scientists and research teams with appropriate information which can then be used by policy makers and change agents. This method helps to set policies around population health as well as intervention strategies which are then used to respond to the needs of a population.\n\nPolicy for population health \"sets priorities\" and are a \"guide to action to change what would otherwise occur\". Policies are based on \"social sciences of sociology, economics, demography, public health, anthropology, and epidemiology\" and determine how outcomes can be accomplished are implemented at various levels. Such guides determine laws, policies, and ordinances and are defined by policymakers. Examples of policies include \"smoking bans, excise taxes on cigarettes and alcohol, seat belt laws, water fluoridation, and restaurant menu labeling\". They may be applied to a commercial establishment such as a restaurant, business workplace or within a city or state level. Policies should be evidence based and require academic studies or research to support the approach. This will assure that the appropriate measures needed for each demographic are promoted to encourage the necessary intervention practices which can be applied to each population or to the nation as a whole.\n\nPolicymakers can be classified as both private and public and are defined as someone who is in a position of authority to implement health policies. A public policy maker could be a government official and a private policymaker could be a business owner or administrator. Policymakers are influenced by, and can also be, change agents. Change agents include \"legislators in Washington, an attorney general, regulators at the FDA, an advocacy group or other organizations that clearly have influence\".\n\nThe goal for any political strategies surrounding population health is to \"improve chances of success for policy adoption and implementation\". Such strategies include the creation of funds to support initiatives and the construction of strategies which limit conflicts of interest in the implementation of public policy.\n\nA political strategy implemented to limit the sale and exposure to tobacco products and restrict the tobacco company's ability to benefit politically from charitable donations is the creation of the World Health Framework Convention on Tobacco Control (FCTC) treaty. The legally binding document is supported by numerous countries, government/nongovernment agencies and provides the necessary power to prevent negative influences on population health policies.\n\nInterventions in population health \"shift the distribution of health risk by addressing the underlying social, economic and environmental conditions\" and are implemented through \"programs or policies designed and developed in the health sector, but they are more likely to be in sectors elsewhere, such as education, housing or employment\". They are aimed at reducing such things as childhood obesity, cardiovascular disease, smoking and mental health issues throughout society. The means in which interventions are devised is through extensive research and contributions from medical scientists, researchers, and medical professionals. They are implemented by but are not limited to educators, practicing physicians, business administrators and mental health professionals.\n\nA typical approach includes assessing the conditions and possible improvements which can be made within the social determinants that have been identified. Each approach is handled at a state or health plan level.\n\nOne example was a workplace in China which implemented policies and interventions for their staff to fight depression. By recognizing the importance of mental health, they were able to reduce depression and improve job satisfaction across the company. The company published its research and findings to promote \"enterprises taking more responsibility for the provision of mental health services to their employees\".\n\nAnother example was the implementation of a smoking cessation program to the province of Ontario. Studies were performed on weekly visit rates to psychiatric emergency departments before and after the implementation. The result was a \"15.5% reduction in patient visits for patients with a primary diagnosis of psychotic disorder\".\n\nAs is the common understanding of population health, health inequalities, defined as a \"generic term used to designate differences, variations, and disparities in the health achievements of individuals and groups\", must be considered to correctly implement the most effective policies and interventions. Based on a population and its socioeconomic, geographic, ethnicity and other factors, policies and interventions may vary. Policies implemented for one population may be less effective and more costly than it would be for another similar population. For example, US policies tend to be more costly than European and have less impact. Research has shown that in some instances, \"Americans had worse outcomes than their international peers\" and also had \"the lowest life expectancy at birth of the countries studied\".\n\n\n\n"}
{"id": "2752447", "url": "https://en.wikipedia.org/wiki?curid=2752447", "title": "Portable Draughts Notation", "text": "Portable Draughts Notation\n\nPortable Draughts Notation (.PDN) is the standard computer-processable format for recording draughts games. This format is derived from Portable Game Notation, which is the standard chess format.\n\nPDN files are text files which must contain Tag Pairs and Movetext for each game.\n\nTag pairs begin with \"[\", the name of the tag, the tag value enclosed in double-quotes, and a closing \"]\". There must be a newline after each tag. Tag names are case-sensitive.\n\nPDN data for archival storage is required to provide 7 tags.\n\n\n\nA position can be stored by the codice_1 tag:\n\n\nExamples:\n\nMovetext contains the actual moves for the game. Moves begin with the source square number, then a \"-\" or \"x\", finally destination square number. Jumps must be specified by each square that would be jumped (\"11x18x25\"), or two squares only (\"11x25\").\n\nThe end of the game must contain the 4 standard result codes: \"1-0\", \"1/2-1/2\" \"0-1\", and \"*\". The codes must be the same as the codice_6 tag pair.\n\nAn annotator who wishes to suggest alternative moves to those actually played in the game may insert variations enclosed in parentheses.\n\nComments may be added by either a \";\" (a comment that continues to the end of the line) or a \"{\" (which continues until a matching \"}\"). Comments do not nest.\n\nPDN can be used in a wide variety of draughts variants synthesized with different board sizes. For example, PDN can represent 8×8 and 10×10 boards. It can represent unmatched board sizes by specifying unequivalent values in the codice_7 and the codice_8 headers. It can include the optional codice_9 tag to differentiate between different variants.\n\nType-number: this is one of the following type-numbers:\n\n\n\n [Event \"itsyourturn.com USA vs. World 8/04\"]\n\n"}
{"id": "931158", "url": "https://en.wikipedia.org/wiki?curid=931158", "title": "Power projection", "text": "Power projection\n\nPower projection (or force projection) is a term used in military and political science to refer to the capacity of a state \"to apply all or some of its elements of national power — political, economic, informational, or military — to rapidly and effectively deploy and sustain forces in and from multiple dispersed locations to respond to crises, to contribute to deterrence, and to enhance regional stability.\"\n\nThis ability is a crucial element of a state's power in international relations. Any state able to direct its military forces outside the limited bounds of its territory might be said to have \"some\" level of power projection capability, but the term itself is used most frequently in reference to militaries with a worldwide reach (or at least significantly broader than a state's immediate area). Even states with sizable hard power assets (such as a large standing army) may only be able to exert limited regional influence so long as they lack the means of effectively projecting their power on a global scale. Generally, only a select few states are able to overcome the logistical difficulties inherent in the deployment and direction of a modern, mechanized military force.\n\nWhile traditional measures of power projection typically focus on hard power assets (tanks, soldiers, aircraft, naval vessels, etc.), the developing theory of soft power notes that power projection does not necessarily have to involve the active use of military forces in combat. Assets for power projection can often serve dual uses, as the deployment of various countries' militaries during the humanitarian response to the 2004 Indian Ocean earthquake illustrates. The ability of a state to project its forces into an area may serve as an effective diplomatic lever, influencing the decision-making process and acting as a potential deterrent on other states' behavior.\n\nEarly examples of power projection includes Roman dominance of Europe: the ability to project power is tied to the ability to innovate and field such innovations. Roman engineering innovations such as machines (pile driver), concrete, aqueducts and modern roads provided the footing for an economic engine that powered a military that was unmatched in its day. Examples of Roman power projection include Julius Caesar building the Rhine bridge in 10 days to demonstrate the ability to march its 40000 troops as it sees fit: the local inhabitants enjoyed the natural protection of the river and fled when this natural protection was overcome. Although Rome is far from the center of modern-power, its influence can be seen in the architecture of modern capitols around the world (domes, arches, columns). The demonstration of an extraordinary innovative military capability will signal power and when properly applied terminate conflicts summarily.\n\nDuring the Ming treasure voyages in the 15th century, the Chinese treasure fleet was heavily militarized to exercise power projection around the Indian Ocean and thereby promote its interests. \n\nThe modern ability to project power and exert influence on a global scale can be tied to innovations stemming from the Industrial Revolution and the associated modernizations in technology, communications, finance and bureaucracy; this finally allowed the state to create unprecedented amounts of wealth and to effectively marshal these resources to exert power over long distances.\n\nThe first such industrial-technological power was the British Empire in the 19th century. As a maritime power, its strength and ability to project power to further its interests lay in the Royal Navy. A worldwide system of naval bases and coaling stations, a large logistical bureaucracy to oversee shipbuilding, the supply of coal, food, water, and sailors, and an industrial base for the manufacture and technological enhancement of the fleet were among the essential ingredients for this capability. During the First Opium War (1839–42), it was this capacity that enabled a British expeditionary force of 15 barracks ships, 4 steam-powered gunboats and 25 smaller boats with 4,000 marines to successfully defend its interests 6,000 miles from the fleet's home port. The era of gunboat diplomacy that this inaugurated was an exercise in Western power projection, and was normally used in the defence of commercial and political interests.\nThe Anglo-French expeditionary force sent to shore up the Ottoman Empire against Russian aggression during the Crimean War (1853–56) was one of the first examples of a planned expeditionary power-projection campaign. It was the first campaign to use modern technology, including steam-powered warships and telegraph communications.\n\nAnother illustrative example of industrial power projection, was the punitive British Expedition to Abyssinia in 1868 as a retaliation against Emperor Tewodros II of Ethiopia's imprisonment of several missionaries and British government representatives. The expeditionary force sent was a tremendous logistical and technological challenge at the time. Commanded by Lieutenant-General Sir Robert Napier of the Bombay Army, military intelligence was used to estimate the required size of the army and the difficulties of traversing the inhospitable terrain.\n\nA force of over 30,000 was shipped from British India to Zula on the Red Sea on a fleet of more than 280 steam ships, while an advance detachment of engineers built a large port with two piers, warehouses and a lighthouse, and constructed a 20-mile-long railway towards the interior. A road was also built for the artillery to be moved along with the help of elephants. After three months of trekking, the British force repelled an Ethiopian attack and bombarded the fortress of Magdala into submission; Tewodros committed suicide.\n\nIn the Russo-Japanese War of 1904–05, the Japanese destruction of the Imperial Russian Navy's Pacific Fleet demonstrated Imperial Russia's inability to project force in the East. This immediately diminished Russia's diplomatic sway in that region. At the same time, Russia's western armies became less credible, as mobilization exposed organizational flaws and threw the western armies into chaos. This led analysts in Europe, such as German chief of staff Count Alfred von Schlieffen, to conclude that Russia would prove inept at projecting force in Europe, thus demoting Russia in European diplomatic relations.\n\nMany other actions can be considered projections of force. The 19th century is full of incidents such as the 1864 Bombardment of Kagoshima and the Boxer Rebellion. More recently, the Falklands War provided an example of the United Kingdom's ability to project force far from home. Other recent examples of power projection include the U.S.-led Invasion of Iraq and the NATO bombing of Yugoslavia. The ability of the U.S. Navy, the British Royal Navy, and the French Navy to deploy large numbers of ships for long periods of time away from home are notable projection abilities.\n\nThe U.S. Department of Defense defines power projection as the \"ability of a nation to apply all or some of its elements of national power—political, economic, informational, or military—to rapidly and effectively deploy and sustain forces in and from multiple dispersed locations to respond to crises, to contribute to deterrence, and to enhance regional stability\".\n\nAs distance between a fighting force and its headquarters increases, command and control inevitably becomes more difficult. Modern-day power projection often employs high-tech communications and information technology to overcome these difficulties, a process sometimes described as the \"Revolution in Military Affairs\".\n\nWhile a few long-range weapons such as the intercontinental ballistic missiles (ICBMs) are capable of projecting deadly force in their own right, it is military logistics that is at the heart of power projection. The ability to integrate naval and air forces with land armies as part of joint warfare is a key aspect of effective power projection; airlift and sealift capabilities facilitate the deployment of soldiers and weapons to a distant theater of war.\n\nThe aircraft carrier strike group, strategic bomber, ballistic missile submarine, and strategic airlifter are all examples of power projection platforms. Military units designed to be light and mobile, such as airborne forces (paratroopers and air assault forces) and amphibious assault forces, are utilized in power projection. Forward basing is another method of power projection, which, by pre-positioning military units or stockpiles of arms at strategically located military bases outside a country's territory, reduces the time and distance needed to mobilize them.\n\nScholars have disaggregated military power projection into nine different categories based on political goals and level of force. Four of these employ \"soft\" military power (securing sea lanes of communication, non-combatant evacuation, humanitarian response, and peacekeeping) and the rest are primarily concerned with \"hard\" military power (show the flag, compellence/deterrence, punishment, armed intervention, and conquest).\n\nExamples of soft power projection include:\n\n\nExamples of hard power projection include:\n\n\n\n"}
{"id": "44672530", "url": "https://en.wikipedia.org/wiki?curid=44672530", "title": "Quadratic integrate and fire", "text": "Quadratic integrate and fire\n\nThe quadratic integrate and fire (QIF) model is a biological neuron model and a type of integrate-and-fire neuron which describes action potentials in neurons. In contrast to physiologically accurate but computationally expensive neuron models like the Hodgkin–Huxley model, the QIF model seeks only to produce action potential-like patterns and ignores subtleties like gating variables, which play an important role in generating action potentials in a real neuron. However, the QIF model is incredibly easy to implement and compute, and relatively straightforward to study and understand, thus has found ubiquitous use in computational neuroscience .\n\nA quadratic integrate and fire neuron is defined by the autonomous differential equation,\n\nwhere formula_2 is a real positive constant. Note that a solution to this differential equation is the tangent function, which blows up in finite time. Thus a \"spike\" is said to have occurred when the solution reaches positive infinity, and the solution is reset to negative infinity.\n\nWhen implementing this model in computers, a threshold crossing value (formula_3) and a reset value (formula_4) is assigned, so that when the solution rises above the threshold, formula_5, the solution is immediately reset to formula_4\n"}
{"id": "10300787", "url": "https://en.wikipedia.org/wiki?curid=10300787", "title": "Quantum healing", "text": "Quantum healing\n\nQuantum healing is a pseudo-scientific mixture of ideas purported to draw on quantum mechanics, psychology, philosophy, and neurophysiology that asserts that quantum phenomena are responsible for health and wellbeing. There are a number of different versions, which allude to various quantum ideas including wave particle duality and virtual particles, and more generally \"energy\" and vibrations. Quantum healing is a form of alternative medicine.\n\nThe term was coined by Deepak Chopra. His discussions of quantum healing have been characterised as technobabble - \"incoherent babbling strewn with scientific terms\" which \"drives crazy people who actually understand physics\" and \"redefining Wrong\".\n\nQuantum healing has a number of vocal followers, but is widely regarded as being nonsensical by the scientific community. The main criticism revolves around its systematic misinterpretations of modern physics, especially of the fact that macroscopic objects (such as the human body or individual cells) are much too large to exhibit inherently quantum properties like interference and wave function collapse. Most literature on quantum healing is almost entirely philosophical, omitting the rigorous mathematics that makes quantum electrodynamics possible.\n\nPhysicist Brian Cox argues that misuse of the word \"quantum\", such as its use in \"quantum healing\", has a negative effect on society as it undermines genuine science and discourages people from engaging with conventional medicine. He states that \"for some scientists, the unfortunate distortion and misappropriation of scientific ideas that often accompanies their integration into popular culture is an unacceptable price to pay.\"\n\n"}
{"id": "27096001", "url": "https://en.wikipedia.org/wiki?curid=27096001", "title": "Recognition failure of recallable words", "text": "Recognition failure of recallable words\n\nThe recognition failure of recallable words is an experimental phenomenon in cognitive psychology originally discovered by the memory researcher Endel Tulving and colleagues. Although recognition of previously-studied words through a recognition memory test, in which the words are re-presented for a memory judgment, generally yields a greater response probability than the recall of previously studied words through a recall test, in which the words must be mentally retrieved from memory, Tulving found that this typical result could be reversed by manipulating the retrieval cues provided at test.\n\nTulving's procedure first tested the recognition memory of participants for a list of paired associates using an item recognition procedure, to which participants responded \"yes\" if the pair of items was valid and \"no\" if the pair was invalid. Next, the participants were given a recall test, in which they were shown one item in each pair and attempted to recall from memory the matching item. Tulving discovered that some items which participants had failed to recognize during the item recognition trials were recalled successfully during the subsequent recall test.\n\nAs an example of this effect, many people will fail to recognize the name \"James Fenimore Cooper\" if \"Cooper\" is presented as the cue, yet they will easily recall the author's surname if given the appropriate retrieval cue, \"What is the author with first name James Fenimore?\" Although this example of recognition failure of recallable words occurs in semantic memory, Tulving has also demonstrated this phenomenon in episodic memory.\n\nTulving and Wiseman also examined the association between recognition and cued recall for individual list items. The resulting Tulving-Wiseman function describes the correlation between the probability of recalling an item and the probability of recognizing the item conditional on recall having been successful. The function predicts a moderate correlation between recognition and cued recall, suggesting a need to distinguish between retrieval based on the familiarity of an item's specific properties and retrieval based on the recollection of relational information between two items. There is also some contradictory evidence that bases recognition failure on retrieval failure.\n"}
{"id": "8994982", "url": "https://en.wikipedia.org/wiki?curid=8994982", "title": "Relational space", "text": "Relational space\n\nThe relational theory of space is a metaphysical theory according to which space is composed of relations between objects, with the implication that it cannot exist in the absence of matter. Its opposite is the container theory. A relativistic physical theory implies a relational metaphysics, but not the other way round: even if space is composed of nothing but relations between observers and events, it would be conceptually possible for all observers to agree on their measurements, whereas relativity implies they will disagree. Newtonian physics can be cast in relational terms, but Newton insisted, for philosophical reasons, on absolute (container) space. The subject was famously debated by Gottfried Wilhelm Leibniz and a supporter of Newton's in the Leibniz–Clarke correspondence.\n\nAn absolute approach can also be applied to time, with, for instance, the implication that there might have been vast epochs of time before the first event.\n\n"}
{"id": "2616121", "url": "https://en.wikipedia.org/wiki?curid=2616121", "title": "Right-wing authoritarianism", "text": "Right-wing authoritarianism\n\nRight-wing authoritarianism (RWA) is a personality and ideological variable studied in political, social and personality psychology. Right-wing authoritarians are people who have a high degree of willingness to submit to authorities they perceive as established and legitimate, who adhere to societal conventions and norms and who are hostile and punitive in their attitudes towards people who do not adhere to them. They value uniformity and are in favour of using group authority, including coercion, to achieve it.\n\nThe concept of right-wing authoritarianism was introduced in 1981 by Canadian-American psychologist Bob Altemeyer as a refinement of the authoritarian personality theory originally pioneered by University of California, Berkeley researchers Theodor W. Adorno, Else Frenkel-Brunswik, Daniel Levinson and Nevitt Sanford. After extensive questionnaire research and statistical analysis, Altemeyer found that only three of the original nine hypothesized components of the model correlated together: authoritarian submission, authoritarian aggression and conventionalism. Researchers have traditionally assumed that there was just one kind of authoritarian personality, who could be either a follower or a leader. The discovery that followers and leaders are usually different types of authoritarians is based on research done by Sam McFarland.\n\nRight-wing authoritarianism is measured by the RWA scale, which uses a Likert scale response. The first scored item on the scale states: \"Our country desperately needs a mighty leader who will do what has to be done to destroy the radical new ways and sinfulness that are ruining us\". People who strongly agree with this are showing a tendency toward authoritarian submission (\"Our country desperately needs a mighty leader\"), authoritarian aggression (\"who will do what has to be done to destroy\") and conventionalism (\"the radical new ways and sinfulness that are ruining us\").\nPsychometrically, the RWA scale was a significant improvement over the F-scale, which was the original measure of the authoritarian personality. The F-scale was worded so that agreement always indicated an authoritarian response, thus leaving it susceptible to the acquiescence response bias. The RWA scale is balanced to have an equal number of pro and anti authoritarian statements. The RWA scale also has excellent internal reliability, with coefficient alpha typically measuring between 0.85 and 0.94.\n\nThe RWA scale has been modified over the years, as many of the items lost their social significance as society changed and the current version is 22 items long.\n\nAlthough Altemeyer has continually updated the scale, researchers in different domains have tended to lock-in on particular versions. For example, in the social psychology of religion the 1992 version of the scale is still commonly used. In addition, the length of the earlier versions (30 items) led many researchers to develop shorter versions of the scale. Some of those are published, but many researchers simply select a subset of items to use in their research, a practice that Altemeyer strongly criticizes.\n\nThe uni-dimensionality of the scale has also been challenged recently. For example, Funke showed that it is possible to extract the three underlying dimensions of RWA if the double- and triple-barreled nature of the items is removed. Given the possibility of underlying dimensions emerging from the scale, it is then the case that the scale is no longer balanced, since all the items primarily capturing authoritarian aggression are pro-trait worded (higher scores mean more authoritarianism) and all the items primarily measuring conventionalism are con-trait worded (higher scores mean less authoritarianism). Work by Mavor, Louis and Sibley recently demonstrated that the existence of 2 or 3 factors in the RWA scale reflects real differences in these dimensions rather than acquiescence response bias.\n\nRight-wing authoritarians want society and social interactions structured in ways that increase uniformity and minimize diversity. In order to achieve that, they tend to be in favour of social control, coercion and the use of group authority to place constraints on the behaviours of people such as political dissidents and ethnic minorities. These constraints might include restrictions on immigration, limits on free speech and association and laws regulating moral behaviour. It is the willingness to support or take action that leads to increased social uniformity that makes right-wing authoritarianism more than just a personal distaste for difference. Right-wing authoritarianism is characterized by obedience to authority, moral absolutism, racial and ethnic prejudice and intolerance and punitiveness towards dissidents and deviants. In parenting, right-wing authoritarians value children's obedience, neatness and good manners.\n\nDuckitt & Bizumic (2013) write that RWA measures the support \"for the subordination of individual freedom and autonomy to the collective and its authority\". Their studies show that it can be split into three distinct factors:\n\nThe names of these three components of RWA resemble their standard use but actually correspond, instead, to the three statistical factors.\n\nRight-wing authoritarianism was previously split differently into three attitudinal and behavioral clusters which correlate together:\n\nThe terminology of \"authoritarianism\", \"right-wing authoritarianism\" and \"authoritarian personality\" tend to be used interchangeably by psychologists, though inclusion of the term \"personality\" may indicate a psychodynamic interpretation consistent with the original formulation of the theory.\n\nThe phrase \"right-wing\" in right-wing authoritarianism does not necessarily refer to someone's politics, but to psychological preferences and personality. It means that the person tends to follow the established conventions and authorities in society. In theory, the authorities could have either right-wing or left-wing political views.\n\nMilton Rokeach's dogmatism scale was an early attempt to measure pure authoritarianism, whether left or right. The scale was carefully designed to measure closed-mindedness without regard to ideology. Nevertheless, researchers found that it correlated with British political conservativism. In a similar line of research, Philip Tetlock found that right wing beliefs are associated with less integrative complexity than left wing beliefs. People with moderate liberal attitudes had the highest integrative complexity in their cognitions.\n\nThere have been a number of other attempts to identify \"left-wing authoritarians\" in the United States and Canada. These would be people who submit to leftist authorities, are highly conventional to liberal viewpoints and are aggressive to people who oppose left-wing ideology. These attempts have failed because measures of authoritarianism always correlate at least slightly with the right. However, left-wing authoritarians were found in Eastern Europe. There are certainly extremists across the political spectrum, but most psychologists now believe that authoritarianism is a predominantly right-wing phenomenon.\n\nAlthough authoritarians in North America generally support conservative political parties, this finding must be considered in a historical and cultural context. For example, during the Cold War authoritarians in the United States were usually anti-communist, whereas in the Soviet Union authoritarians generally supported the Communist Party and were opposed to capitalism. Authoritarians thus generally favor the established ways and oppose social and political change, hence even politics usually labeled as right or left-wing is not descriptive. While communism in the Soviet Union is seen as leftist, it still inspired the same responses. Furthermore, recent research indicates that political progressives can exhibit the qualities of authoritarianism when they are asked about conservative Christians. This leaves questions over what makes various ideologies left or right open to interpretation. A 2017 study found evidence that was suggestive of the existence of left-wing authoritarians.\n\nAccording to Karen Stenner, an Australian professor who specializes in authoritarianism, says it is different from conservatism because authoritarianism reflects aversion to difference across space (i.e. diversity of people and beliefs at any given moment), while conservatism reflects aversion to difference over time (i.e. change). Stenner argues that conservatives will embrace racial diversity, civil liberties and moral freedom to the extent they are already institutionalized authoritatively-supported traditions and are therefore supportive of social stability. Conservatives tend to be drawn to authoritarianism when public opinion is fractious and there is a loss of confidence in public institutions, but in general they value stability and certainty over increased uniformity. However, Stenner says that authoritarians also want difference restricted even when so doing would require significant social change and instability.\n\nAccording to research by Altemeyer, right-wing authoritarians tend to exhibit cognitive errors and symptoms of faulty reasoning. Specifically, they are more likely to make incorrect inferences from evidence and to hold contradictory ideas that result from compartmentalized thinking. They are also more likely to uncritically accept insufficient evidence that supports their beliefs and they are less likely to acknowledge their own limitations. Whether right-wing authoritarians are less intelligent than average is disputed, with Stenner arguing that variables such as high verbal ability (indicative of high cognitive capacity) have a very substantial ameliorative effect in diminishing authoritarian tendencies. Measured against other factors of personality, authoritarians generally score lower on openness to experience and slightly higher on conscientiousness.\n\nAltemeyer suggested that authoritarian politicians are more likely to be in the Conservative or Reform party in Canada, or the Republican Party in the United States. They generally have a conservative economic philosophy, are highly nationalistic, oppose abortion, support capital punishment, oppose gun control legislation and do not value social equality. The RWA scale reliably correlates with political party affiliation, reactions to Watergate, pro-capitalist attitudes, religious orthodoxy and acceptance of covert governmental activities such as illegal wiretaps. Although authoritarianism is correlated with conservative political ideology, not all authoritarians are conservative and not all conservatives are authoritarian. It is also worth noting that many authoritarians have no interest in politics.\n\nAuthoritarians are generally more favorable to punishment and control than personal freedom and diversity. For example, they are more willing to suspend constitutional guarantees of liberty such as the Bill of Rights. They are more likely to advocate strict, punitive sentences for criminals and report that punishing such people is satisfying for them. They tend to be ethnocentric and prejudiced against racial and ethnic minorities and homosexuals. However, Stenner argues that authoritarians will support programs intended to increase opportunities for minority groups, such as affirmative action, if they believe such programs will lead to greater societal uniformity.\n\nIn roleplaying situations, authoritarians tend to seek dominance over others by being competitive and destructive instead of cooperative. In a study by Altemeyer, 68 authoritarians played a three-hour simulation of the Earth's future entitled the Global Change Game. Unlike a comparison game played by individuals with low RWA scores, which resulted in world peace and widespread international cooperation, the simulation by authoritarians became highly militarized and eventually entered the stage of nuclear war. By the end of the high RWA game, the entire population of the earth was declared dead.\n\nThe vast majority of research on right-wing authoritarianism has been done in the United States and Canada. However, a 2003 cross-cultural study examined the relation between authoritarianism and individualism-collectivism in samples (1,080) from Bulgaria, Canada, Germany, Japan, New Zealand, Poland and the United States. Both at the individual level and the societal level, authoritarianism was correlated with vertical individualism (or dominance seeking) and vertical or hierarchical collectivism, which is the tendency to submit to the demands of one's ingroup. A study done on both Israeli and Palestinian students in Israel found that RWA scores of right-wing party supporters were significantly higher than those of left-wing party supporters and scores of secular subjects were lowest.\n\nRight-wing authoritarianism has been found to correlate only slightly with Social Dominance Orientation (SDO). The two measures can be thought of as two sides of the same coin: RWA provides submissive followers and SDO provides power-seeking leaders.\n\nResearch comparing RWA with the Big Five personality traits has found that RWA is positively correlated with conscientiousness (r = 0.15) and negatively correlated with openness to experience (r = −0.36). SDO has a somewhat different pattern of correlations with the Big Five, as it is also associated with low openness to experience (r = −0.16), but is not significantly correlated with conscientiousness (r = −0.05) and instead has a negative correlation with agreeableness (r = −0.29). Low openness to experience and high conscientiousness have been found to be predictive of social conformity. People low in openness to experience tend to prefer clear, unambiguous moral rules and are more likely to support the existing social order insofar as it provides clear guidance about social norms for behavior and how the world should be. People low in openness to experience are also more sensitive to threats (both real and symbolic) to the social order and hence tend to view outgroups who deviate from traditional social norms and values as a threat to ingroup norms and values. Conscientiousness is associated with a preference for order, structure and security, hence this might explain the connection with RWA.\n\nA recent refinement to this body of research was presented in Karen Stenner's 2005 book, \"The Authoritarian Dynamic\". Stenner argues that RWA is best understood as expressing a dynamic response to external threat, not a static disposition based only on the traits of submission, aggression and conventionalism. Stenner is critical of Altemeyer's social learning interpretation and argues that it cannot account for how levels of authoritarianism fluctuate with social conditions. She argues that the RWA Scale can be viewed as a measure of expressed authoritarianism, but that other measures are needed to assess authoritarian predispositions which interact with threatening circumstances to produce the authoritarian response.\n\nRecent criticism has also come as a result of treating RWA as uni-dimensional even in contexts where it makes no sense to do so. For example, RWA has been used in regression analyses with fundamentalism as another predictor and attitudes to homosexuality and racism as the outcomes. For example, this research seemed to show that fundamentalism would be associated with reduced racism once the authoritarian component was removed and this was summarized in a recent review of the field. However, since the RWA Scale has items that also measure fundamentalist religiosity and attitudes to homosexuality, this undermines the interpretation of such analyses. Even worse is the possibility that the unrecognised dimensionality in RWA can cause a statistical artifact to arise in such regressions, which can reduce or even reverse some of the relationships. Mavor and colleagues have argued that this artifact eliminates or even reverses any apparent tendency for fundamentalism to reduce racism once RWA is controlled. The implication is that in some domains such as the social psychology of religion it is not only preferable to think of RWA as consisting of at least two components, but essential in order to avoid statistical errors and incorrect conclusions. Several options currently exist for scales that acknowledge at least the two main underlying components in the scale (aggression/submission and conventionalism).\n\nAltemeyer's research on authoritarianism has been challenged by psychologist John J. Ray, who questions the sampling methods used and the ability of the RWA Scale to predict authoritarian behavior and provides evidence that the RWA Scale measures conservatism rather than \"directiveness\", a construct that John J. Ray invented and that he relates to authoritarianism. However, Ray's approach is a minority position among researchers and other psychologists have found that both the RWA Scale and the original F-Scale are good predictors of both attitudes and behavior.\n\nIn 2012, the American \"Journal of Political Science\" published an article discussing the correlation between conservatism and psychoticism, which they associated with authoritarianism, among other traits. In 2015, they released an erratum showing that psychoticism is actually more associated with liberalism, whereas neuroticism is more associated with conservatism.\n\nIn 2017, the new regality theory suggested a reinterpretation of RWA in the light of evolutionary psychology. Regality theory agrees that authoritarianism is a dynamic response to external threat, and more specifically to collective danger. But rather than seeing authoritarianism as a psychological aberration, regality theory sees it as an evolved response to perceived collective danger. The tendency to support a strong leader in times of collective danger has contributed to Darwinian fitness in human prehistory because it helped solve the collective action problem in war and suppress free riders. It is argued that regality theory adds a deeper level of analysis to our understanding of authoritarianism and avoids the political bias that the research in the authoritarian personality and RWA is often criticized for.\n\n\n\n"}
{"id": "948956", "url": "https://en.wikipedia.org/wiki?curid=948956", "title": "Self-love", "text": "Self-love\n\nSelf-love has often been seen as a moral flaw, akin to vanity and selfishness.\nThe Merriam-Webster dictionary later describes self-love as to \"love of self\" or \"regard for one's own happiness or advantage\". Synonyms of this concept are: amour propre, conceit, conceitedness, egotism, and many more. However, throughout the centuries this definition has adopted a more positive connotation through self-love protests, the Hippie era, the new age feminist movement as well as the increase in mental health awareness that promotes self-love.\n\nCicero considered those who were \"sui amantes sine rivali\" (lovers of themselves without rivals) were doomed to end in failure – a theme adopted by Francis Bacon in his condemnation of extreme self-lovers, who would burn down their own home, only to roast themselves an egg.\n\nSelf-love was first recognized in 1563 but was only later studied by philosophers William James and Erich Fromm, who studied emotional human behaviour, such as self-esteem and self-worth.\nHowever, it was later defined in 1956 by psychologist and social philosopher Erich Fromm, who proposed that loving oneself is different from being arrogant, conceited or egocentric, meaning that instead caring about oneself and taking responsibility for oneself.\n\nHowever, Augustine – with his theology of evil as a mere distortion of the good – considered that the sin of pride was only a perversion of a normal, more modest degree of self-love.\n\nEric Fromm proposed a re-evaluation of self-love in more positive sense, arguing that in order to be able to truly love another person, a person first needs to love oneself in the way of respecting oneself and knowing oneself (e.g. being realistic and honest about one's strengths and weaknesses).\n\nErik H. Erikson similarly wrote of a post-narcissistic appreciation of the value of the ego, while Carl Rogers saw one result of successful therapy as the regaining of a quiet sense of pleasure in being one's own self.\n\nSelf-love or self-worth was later defined by A.P. Gregg and C. Sedikides in 2003 as \"referring to a person's subjective appraisal of himself or herself as intrinsically positive or negative\". Robert H. Wozniak described William James's theory of self-esteem and claimed that self-love was measured in \"... three different but interrelated aspects of self: the material self (all those aspects of material existence in which we feel a strong sense of ownership, our bodies, our families, our possessions), the social self (our felt social relations), and the spiritual self (our feelings of our own subjectivity)\".\n\nMental health was first described by William Sweetser (1797–1875) as the maintenance of \"mental hygiene\". His analysis was demonstrated in his essay \"Temperance Society\" published August 26, 1830, which claimed that regular maintenance of mental hygiene created a positive impact on the well-being of individuals and the community as well.\n\nAccording to the American Association of Suicidology, there have been 44,193 suicides in 2015 alone, 5,491 of them being youth aged between 15–24 years old. The number of teenager and young adult suicides are escalating at an alarming rate every year, further demonstrating that the social environment has an impact on mental health conditions. The association conducted a study in 2008 which researched the impact of low self-esteem and lack of self-love and its relation to suicidal tendencies and attempts. They defined self-love as being \"beliefs about oneself (self-based self-esteem) and beliefs about how other people regard oneself (other-based self-esteem)\". It concluded that \"depression, hopelessness, and low self-esteem are implications of vulnerability factors for suicide ideation\" and that \"these findings suggest that even in the context of depression and hopelessness, low self-esteem may add to the risk for suicide ideation\".\n\nSelf-love was first promoted in the early years of the Hippie era (also known as the Beat Generation of the 1960s). After witnessing the devastating consequences of World War II and having troops still fighting in the Vietnam War, western (especially North American) societies began promoting \"peace and love\" to help generate positive energy and to promote the preservation of dissipating environmental factors, such as the emergence of oil pipelines and the recognition of pollution caused by the greenhouse effect.\n\nThese deteriorating living conditions caused worldwide protests that primarily focused on ending the war, but secondarily promoted a positive environment aided by the fundamental concept of crowd psychology. This post-war community was left very vulnerable to persuasion but began encouraging freedom, harmony and the possibility of a brighter, non-violent future. These protests took place on almost all continents and included countries such as the United States (primarily New York City and California), England and Australia. Their dedication, perseverance and empathy towards human life defined this generation as being peace advocates and carefree souls.\n\nThe emergence of the feminist movement began as early as the 19th century, but only began having major influence during the second wave movement, which included women's rights protests that inevitably led to women gaining the right to vote. These protests not only promoted equality but also suggested that women should recognize their self-worth through the knowledge and acceptance of self-love. Elizabeth Cady Stanton used the Declaration of Independence as a guideline to demonstrate that women have been harshly treated throughout the centuries in her feminist essay titled \"Declaration of Sentiments\". In the essay she claims that \"all men and women are created equal; ... that among these [rights] are life, liberty, and the pursuit of happiness\"; and that without these rights, the capacity to feel self-worth and self-love is scarce. This historical essay suggests that a lack of self-esteem and fear of self-love affects modern women due to lingering post-industrial gender conditions.\n\nSelf-love has also been used as a radical tool in communities of Color in the United States. In the 1970s Black-Power movement, the slogan \"Black is beautiful!\" became a way for African-Americans to throw off the mantle of predominately White beauty norms. The dominant cultural aesthetic pre-1970s was to straighten Black hair with a perm or hot comb. During the Black Power movement, the \"afro\" or \"fro\" became the popular hairstyle. It involved letting Black Hair grow naturally, without chemical treatment, so as to embrace and flaunt the extremely curly hair texture of Black people. Hair was teased out the hair using a pick. The goal was to cause the hair to form a halo around the head, flaunting the Blackness of its wearer. This form of self-love and empowerment during the 70s was a way for African-Americans to combat the stigma against their natural hair texture, which was, and still is, largely seen as unprofessional in the modern workplace.\n\nThe emergence of new technology has given society an easier way to communicate with one another and research faster. Social media has created a platform for self-love promotion and mental health awareness in order to end the stigma surrounding mental health and to address self-love positively rather than negatively.\n\nA few modern examples of self-love promotion platforms include:\n\nBeck, Bhar, Brown & Ghahramanlou‐Holloway (2008). \"Self-Esteem and Suicide Ideation in Psychiatric Outpatients\". Suicide and Life Threatening Behavior 38.\n\nMalvolio is described as \"sick of self-love...a distempered appetite\" in \"Twelfth Night\" (I.v.85-6), lacking self-perspective.\n\nSelf-love or self-worth was later defined by A.P. Gregg and C. Sedikides in 2003.\n\n"}
{"id": "21926324", "url": "https://en.wikipedia.org/wiki?curid=21926324", "title": "Self-similarity matrix", "text": "Self-similarity matrix\n\nIn data analysis, the self-similarity matrix is a graphical representation of similar sequences in a data series.\n\nSimilarity can be explained by different measures, like spatial distance (distance matrix), correlation, or comparison of local histograms or spectral properties (e.g. IXEGRAM). This technique is also applied for the search of a given pattern in a long data series as in gene matching. A similarity plot can be the starting point for dot plots or recurrence plots.\n\nTo construct a self-similarity matrix, one first transforms a data series into an ordered sequence of feature vectors formula_1, where each vector formula_2 describes the relevant features of a data series in a given local interval. Then the self-similarity matrix is formed by computing the similarity of pairs of feature vectors\n\nwhere formula_4 is a function measuring the similarity of the two vectors, for instance, the inner product formula_5. Then similar segments of feature vectors will show up as path of high similarity along diagonals of the matrix.\nSimilarity plots are used for action recognition that is invariant to point of view \nand for audio segmentation using spectral clustering of the self-similarity matrix.\n\n\n\n"}
{"id": "996630", "url": "https://en.wikipedia.org/wiki?curid=996630", "title": "Senpai and kōhai", "text": "Senpai and kōhai\n\nSenpai (, \"earlier colleague\") and kōhai (, \"later colleague\") are terms from the Japanese language describing an informal hierarchical interpersonal relationship found in organizations, associations, clubs, businesses, and schools in Japan. The concept is based in Japanese philosophy and has permeated Japanese society.\n\nThe relationship is an interdependent one, as a \"senpai\" requires a \"kōhai\" and vice versa, and establishes a bond determined by the date of entry to an organization. The \"kōhai\" defers to the \"senpai\"s seniority and experience, and speaks to the \"senpai\" using honorific language.\n\nThe relationship is an interdependent one, as a \"senpai\" requires a \"kōhai\" and vice versa, and establishes a bond determined by the date of entry to an organization. \"Senpai\" refers to the member of higher experience, hierarchy, level, or age in the organization who offers assistance, friendship, and counsel to a new or inexperienced member, known as the \"kōhai\", who must demonstrate gratitude, respect, and occasionally personal loyalty. The \"Senpai\" acts at the same time as a friend. This relation is similar to the interpersonal relation between tutor and tutored in Eastern culture, but differs in that the \"senpai\" and \"kōhai\" must work in the same organization.\n\nThe relation originates in Confucian teaching, as well as the morals and ethics that have arrived in Japan from ancient China and have spread throughout various aspects of Japanese philosophy. The \"senpai–kōhai\" relation is a vertical hierarchy (like a father–son relation) that emphasizes respect for authority, for the chain of command, and for one's elders, eliminating all forms of internal competition and reinforcing the unity of the organization.\n\nOver time this mechanism has allowed the transfer of experience and knowledge, as well as the expansion of acquaintances, to maintaining the art of teaching alive. It also allows the development of beneficial experiences between both, as the \"kōhai\" benefits from the \"senpai\"s knowledge and the \"senpai\" learns new experiences from the \"kōhai\" by way of developing a sense of responsibility. This comradeship does not imply friendship; a \"senpai\" and \"kōhai\" may become friends, but such is not an expectation.\n\nThe Korean terms \"seonbae\" and \"hubae\" are written with the same Chinese characters and indicate a similar senior–junior relationship. The terms may have derived from Japanese, and may have had Chinese origins, though the \"senpai–kōhai\" relationship does not exist in Chinese culture. However, it is close to the system of sifu–student system of Chinese martial arts.\n\nThe \"senpai–kōhai\" system has existed since the beginning of Japanese history. Three elements have had a significant impact on its development: Confucianism, the traditional Japanese family system, and the Civil Code of 1898.\n\nConfucianism arrived from China between the 6th and 9th centuries, but the derived line of thought that brought about deep social changes in Japan was Neo-Confucianism, which became the official doctrine of the Tokugawa shogunate (1603–1867). The precepts of loyalty and filial piety as tribute ( \"\") dominated the Japanese at the time, as respect for elders and ancestor worship that Chinese Confucianism taught were well accepted by the Japanese, and these influences have spread throughout daily life. Like other Chinese influences, the Japanese adopted these ideas selectively and in their own manner, so that the \"loyalty\" in Confucianism was taken as loyalty to a feudal lord or the Emperor.\n\nThe Japanese family system ( \"\") was also regulated by Confucian codes of conduct and had an influence on the establishment of the \"senpai–kōhai\" relation. In this family system the father, as male head, had absolute power over the family and the eldest son inherited the family property. The father had power because he was the one to receive an education and was seen to have superior ethical knowledge. Since reverence for superiors was considered a virtue in Japanese society, the wife and children had to obey it. In addition to the hereditary system, only the eldest son could receive his father's possessions, and neither the eldest daughter nor the younger children received anything from him.\n\nThe last factor influencing the \"senpai–kōhai\" system was the Civil Code of 1898, which strengthened the rules of privilege of seniority and reinforced the traditional family system, giving clear definitions of hierarchical values within the family. This was called \"koshusei\" (, \"family-head system\"), in which the head of the household had the right to command his family and the eldest son inherited that position. These statutes were abolished in 1947, after the surrender of Japan at the end of World War II. These ideals nevertheless remained during the following years as a psychological influence in Japanese society.\n\nThe seniority rules are reflected in various grammatical rules in the Japanese language. A person who speaks respectfully to a superior uses honorific language ( \"), which is divided into three categories:\n\n\n\"Sonkeigo\" and \"kenjōgo\" have expressions (verbs, nouns, and special prefixes) particular to the type of language; for example, the ordinary Japanese verb for \"to do\" is \"suru\", but in \"sonkeigo\" is \"nasaru\" and in \"kenjōgo\" is \"itasu\".\n\nAnother rule in the hierarchical relation is the use of honorific suffixes of address. A \"senpai\" addresses a \"kōhai\" with the suffix \"-kun\" after the \"kōhai\"s given name or surname, regardless if the \"kōhai\" is male or female. A \"kōhai\" similarly addresses a \"senpai\" with the suffix \"-senpai\" or \"-san\"; it is extremely unusual for a \"kōhai\" to refer to a \"senpai\" with the suffix \"-sama\", which indicates the highest level of respect to the person spoken to.\n\nOne place the \"senpai–kōhai\" relation applies to its greatest extent in Japan is in schools. For example, in junior and senior high schools (especially in school clubs) third-year students (who are the oldest) demonstrate great power as \"senpais\". It is common in school sports clubs for new \"kōhais\" to have to perform basic tasks such as retrieving balls, cleaning playing fields, taking care of equipment, and even wash elder students' clothes. They must also bow to or salute their \"senpais\" when congratulated, and \"senpais\" may punish \"kōhais\" or treat them severely.\n\nThe main reason for these humble actions is that it is believed that team members can become good players only if they are submissive, obedient, and follow the orders of the trainer or captain, and thus become a humble, responsible, and cooperative citizen in the future. Relations in Japanese schools also place a stronger emphasis on the age than on the abilities of students. The rules of superiority between a \"senpai\" and a \"kōhai\" are analogous to the teacher–student relation, in which the age and experience of the teacher must be respected and never questioned.\n\nThe \"senpai–kōhai\" relation is weaker in universities, as students of a variety of ages attend the same classes; students show respect to older members primarily through polite language (\"teineigo\"). Vertical seniority rules nevertheless prevail between teachers based on academic rank and experience.\n\nThe \"senpai–kōhai\" system also prevails in Japanese businesses. The social environment in Japanese businesses is regulated by two standards: the system of superiority and the system of permanent employment. The status, salary, and position of employees depend heavily of seniority, and veteran employees generally take the highest positions and receive higher salaries than their subordinates. Until the turn of the 20th and 21st centuries, employment was guaranteed for life and thus such employees did not have to worry about losing their positions.\n\nThe \"senpai–kōhai\" relation is a cornerstone in interpersonal relations within the Japanese business world; for example, at meetings the lower-level employee should sit in the seat closest to the door, called \"shimoza\" (, \"lower seat\"), while the senior employee (sometimes the boss) sits next to some important guest in a position called \"kamiza\" (, \"upper seat\"). During meetings, most employees do not give their opinions, but simply listen and concur with their superiors, although they can express opinions with the prior consent of the employees of greater rank and influence in the company.\n\nOutside Japan, the \"senpai–kōhai\" relation is often found in the teaching of Japanese martial arts, though misunderstandings arise due to lack of historical knowledge, and as the vertical social hierarchy of Japan does not exist in Western cultures.\n\nDespite the \"senpai–kōhai\" relation's deep roots in Japanese society, there have been changes since the end of the 20th century in academic and business organizations. \"Kōhais\" no longer show as much respect to the experience of their \"senpais\", the relation has become more superficial, and the age factor has begun to lose importance. The student body has diversified with Japanese students who have spent a large part of their lives overseas and have returned to Japan, as well as foreign students without a mentality rooted in the Japanese hierarchical system.\n\nThe collapse of the economic bubble in the early 1990s caused a high level of unemployment, including the laying off of high-ranked employees. Companies since then first began to consider employees' skills rather than age or length of service with the company, due to which many long-serving employees lost their positions over being incapable of fulfilling expectations. Gradually many companies have had to restructure their salary and promotion systems, and seniority has thus lost some influence in Japanese society.\n\nAttitudes towards the \"senpai–kōhai\" system vary from appreciation for traditions and the benefits of a good \"senpai–kōhai\" relationship; to reluctant acquiescence; to antipathy. Those who criticize the system find it arbitrary and unfair, that \"senpais\" were often pushy, and that the system results in students who are shy or afraid of standing out from the group. For example, some \"kōhais\" fear that if they outperform their \"senpais\" in an activity, their \"senpai\" will lose face, for which \"kōhais\" must apologize. In some cases, the relation is open to violence and bullying. Most Japanese people—even those who criticize it—accept the \"senpai–kōhai\" system as common-sense aspect of society, straying from which will have inevitably negative social consequences.\n\n\n"}
{"id": "238682", "url": "https://en.wikipedia.org/wiki?curid=238682", "title": "Simplicity", "text": "Simplicity\n\nSimplicity is the state or quality of being simple. Something easy to understand or explain seems simple, in contrast to something complicated. Alternatively, as Herbert A. Simon suggests, something is simple or complex depending on the way we choose to describe it. In some uses, the label \"simplicity\" can imply beauty, purity, or clarity. In other cases, the term may occur with negative connotations to suggest, a deficit or insufficiency of nuance or of complexity of a thing, relative to what one supposes as required.\n\nThe concept of simplicity has been related to in the field of epistemology and philosophy of science (e.g., in Occam's razor). Religions also reflect on simplicity with concepts such as divine simplicity. In the context of human lifestyle, simplicity can denote freedom from hardship, effort or confusion; specifically, it can refer to a simple living style.\n\n\"Simplicity\" is the state or quality of being simple. An easy-to-understand tutorial or white paper with a narrow focus can be seen as simple, particularly in contrast more complicated thorough documentation covering a topic's breadth. Similarly, \"simplicity\" can be used in reference to tasks that need little skill to perform well.\n\nIn some uses, the label \"simplicity\" can imply beauty, purity, or clarity. In other cases, the term may occur with negative connotations to suggest, a deficit or insufficiency of nuance or of complexity of a thing, relative to what one supposes as required, as when referring to people as simpletons.\n\nThe concept of simplicity has been related to in the field of epistemology and philosophy of science.\n\nAccording to Occam's razor, all other things being equal, the \"simplest\" theory is most likely true. In other words, simplicity is a meta-scientific criterion by which scientists evaluate competing theories.\n\nA distinction is often made between two senses of simplicity: syntactic simplicity (the number and complexity of hypotheses), and ontological simplicity (the number and complexity of things postulated). These two aspects of simplicity are often referred to as elegance and parsimony respectively.\n\nJohn von Neumann defines simplicity as important esthetic criteria of scientific models: \n\nSimplicity is a theme in the Christian religion. According to St. Thomas Aquinas, God is infinitely simple. The Roman Catholic and Anglican religious orders of Franciscans also strive for personal simplicity. Members of the Religious Society of Friends (Quakers) practice the Testimony of Simplicity, which involves simplifying one's life to focus on what is important and disregard or avoid what is least important. Simplicity is tenet of Anabaptistism, and some Anabaptist groups like the Bruderhof, make an effort to live simply.\n\nIn the context of human lifestyle, simplicity can denote freedom from hardship, effort or confusion.\n\n\n\n"}
{"id": "49473927", "url": "https://en.wikipedia.org/wiki?curid=49473927", "title": "Sinopieris dubernardi bromkampi", "text": "Sinopieris dubernardi bromkampi\n\nSnopieris dubernardi bromkampi is a subspecies of \"Sinopieris dubernardi\". It is a butterfly in the genus \"Sinopieris\". It was described by Otto Bang-Haas in 1938. It can be found in southeast Tibet.\n"}
{"id": "48728816", "url": "https://en.wikipedia.org/wiki?curid=48728816", "title": "Stan Salett", "text": "Stan Salett\n\nStan Salett (born April 6, 1936) is a civil rights organizer, national education policy advisor and creator of the Upward Bound Program and helped to initiate Head Start. In the early 1960s Salett was an organizer for the Congress of Racial Equality (CORE) and helped organize the March on Washington for Jobs and Freedom. He was the first director of education of the Office of Economic Opportunity, where the Head Start program was created. He co-founded the National Committee for Citizens in Education, dedicated to promoting parent and citizen involvement in schools. During President Lyndon Johnson administration he initiated the National Upward Bound program. While working in Washington, D.C. he served on the staff of all three Kennedy brothers: President Kennedy's Committee on Youth Employment, Attorney General Robert Kennedy's President's Committee on Juvenile Delinquency and Senator Edward Kennedy's Presidential campaign in 1980. He was an active school board member in Maryland in the 80s. During President Bill Clinton's transition he vetted candidates for Attorney General and Secretary of the Interior.\n\nIn 2011 he published his memoir, \"The Edge of Politics: Stories from the Civil Rights Movement, the War on Poverty & the Challenges of School Reform.\" He received the New England Education Opportunity Association's Claiborne Pell Award in 2013. Presently he is President of the Foundation for the Future of Youth, a division of the Eigen Arnett Educational and Cultural Foundation. He has developed special search engines to meet a variety of human needs such as the elimination of human trafficking, the improvement of school performance and the scarcity of the global water supply. In 2016 Salett has been involved in The Independent Media Institute study which evaluated the movement to privatize public education. It was revealed that, \"… in the past two decades, a small group of billionaires – including News Corporation's Rupert Murdoch, who once called public schools an \"untapped 500-billion-dollar sector\" – have worked to assert private control over public education ...\" Salett was presented with the 2017 Distinguished Graduate Award from Boston Latin School for his career to public service and public policy work. The Award is given each year to alumni exemplifying the Boston Latin School motto, \"sumus primi\".\n\nAs of 2016, he resides in Washington, DC, and Chestertown, MD with his wife Elizabeth. He has 2 sons, singer-songwriters Peter Salett and Steve Salett, owner of Saltlands Studio in Brooklyn, NY and Reservoir in Manhattan.\n\n"}
{"id": "58569059", "url": "https://en.wikipedia.org/wiki?curid=58569059", "title": "Stop Our Ship (SOS)", "text": "Stop Our Ship (SOS)\n\nThe Stop Our Ship (SOS) movement, a component of the overall civilian and GI movements against the Vietnam War, was directed towards and developed onboard U.S. Navy ships, particularly aircraft carriers heading to Southeast Asia. It was concentrated on and around major U.S. Naval stations and ships on the West Coast from mid-1970 to the end of the Vietnam War in 1975, and at its height involved tens of thousands of antiwar civilians, military personnel and veterans. It was sparked by the tactical shift of U.S. combat operations in Southeast Asia from the ground to the air. As the ground war stalemated and Army grunts increasingly refused to fight or resisted the war in various other ways, the U.S. “turned increasingly to air bombardment” . By 1972 there were twice as many Seventh Fleet aircraft carriers in the Gulf of Tonkin as previously and the antiwar movement, which was at its height in the U.S. and worldwide, became a significant factor in the Navy. While no ships were actually prevented from returning to war, the campaigns, combined with the broad antiwar and rebellious sentiment of the times, stirred up substantial difficulties for the Navy, including active duty sailors refusing to sail with their ships, circulating petitions and antiwar propaganda onboard, disobeying orders, and committing sabotage, as well as persistent civilian antiwar activity in support of dissident sailors. Several ship combat missions were postponed or altered and one ship was delayed by a combination of a civilian blockade and crewmen jumping overboard. \nThe major targets of the SOS movement were four different aircraft carriers, the USS \"Constellation\", the USS \"Coral Sea\", the USS \"Kitty Hawk\" and the USS \"Enterprise\", with lesser but significant activity on and around a number of other ships.\n\nThe first effort began in 1970 in San Diego, the principal homeport of the U.S. Navy’s Pacific Fleet. An antiwar group, San Diego Nonviolent Action, came up with idea of trying to stop an aircraft carrier from returning to Vietnam through education and non-violent action like a harbor blockade or through preventing sailors from getting to the naval base. They joined with other antiwar forces including a newly formed organization of antiwar military officers called the Concerned Officers Movement and decided to focus their efforts on the USS \"Constellation\". Very quickly this effort, which came to be called The Harbor Project (the SOS name developed later), expanded into a multi-faceted campaign and set the spark for the larger SOS movement to come.\n\nThe plan to stop an aircraft carrier attracted antiwar activists from all over California, including Joan Baez and David Harris. Harris suggested organizing a citywide referendum on whether the Constellation should set sail, and soon hundreds of activists, including sailors and veterans, were canvassing the city organizing the Constellation Vote. This led to a citywide straw vote in late September 1971 with 54,721 votes counted. Over 82% of voters elected to keep the ship home, including 73% of the military personnel who voted. While not a \"real\" vote, the impact on public opinion was appreciable. The commander-in-chief of the Pacific Fleet was quoted as saying \"never was there such a concerted effort to entice American servicemen from their posts.\"\n\nThe involvement of large numbers of antiwar officers and enlisted men created significant debate in the traditionally pro-military town. It also permitted creative methods not normally available to other antiwar groups, such as the CONSTELLATION STAY HOME FOR PEACE banner frequently seen being towed over the city by recently retired navy flight instructor LT John Huyler, and the Constellation Vote stickers found everywhere on board the USS Constellation, including in the captain's personal bathroom. Bathroom stickers weren't the only complication the Captain had to deal with. Over 1,300 of the ship's sailors signed a petition requesting actress Jane Fonda's antiwar FTA Show, known to most GI's as the \"Fuck The Army\" show, be allowed onboard. The captain refused this request but then got himself in hot water by intercepting and destroying 2,500 pieces of U.S. mail sent by antiwar activists to crewmembers. Faced with a possible court of inquiry and health problems, the captain was removed from command before the ship sailed.\n\nLaying the basis for the broader SOS movement to come, a considerable amount of research was conducted into the role of aircraft carriers in modern warfare by Professor William Watson of MIT, who was then a visiting Professor of History at UC San Diego. He argued in a widely distributed pamphlet that aircraft carriers had become weapons \"used to crush popular uprisings and to bully the weaker and poorer countries of the world.\"\n\nWhen the Constellation set sail for Vietnam in on Oct 1, 1971 nine of its crew publicly refused to go and took sanctuary in a local Catholic church, Christ the King. The \"Connie 9\", as they were quickly dubbed, were soon arrested in an early morning raid by US Marshals and flown back to the ship, but within weeks were honorably discharged from the Navy.\n\nAs news of the efforts to stop the sailing of the Constellation spread, crewmen aboard the USS \"Coral Sea\", an aircraft carrier stationed at the Alameda Naval Air Station in the San Francisco area, decided to circulate a petition and the SOS movement officially acquired its name. Their “Stop Our Ship” petition stated in all capital letters: “THE SHIP CAN BE PREVENTED FROM TAKING AN ACTIVE PART IN THE CONFLICT IF WE THE MAJORITY VOICE OUR OPINION THAT WE DO NOT BELIEVE IN THE VIETNAM WAR.” It then asked fellow sailors to sign if they felt the ship should not go to Vietnam. The petition was started on September 12, 1971 and within three days it had gathered 300 signatures, at which point the ship Executive Officer ordered it confiscated.\n\nUndeterred, the antiwar sailors created dozens of new copies and begin recirculating it with instructions about how to avoid confiscation. The military brass, however, declared the petition illegal and arrested three crewmen for passing out antiwar literature. With the ship now at sea for pre-deployment trials, a tit-for tat battle erupted. On September 28, 14 sailors passed out petitions and an antiwar GI underground newspaper in the mess hall in front of the Captain. The Captain responded by having several men arrested, some of whom claimed they were beaten in the brig. Word of these activities reached the civilian antiwar movement in the San Francisco area and when the Coral Sea passed under the Golden Gate Bridge as it returned to port on October 7, a large SOS banner was hung over the side of the bridge. Seventy crewmen responded in kind forming the letters SOS on the flight deck as the ship passed under the bridge. The Navy then began to discharge those it viewed as leaders of the petition drive.\n\nBy the time the dust had settled, over a 1,000 members of the crew had signed the petition - about 25% of the over 4,000 men assigned to the ship. On November 6, over 300 men from Coral Sea marched with over 10,000 others in an anti-war demonstration in San Francisco. In an unprecedented move the Berkeley City Council declared the city a sanctuary for the men on the Coral Sea who were refusing to go to war. The Council encouraged city residents to \"provide bedding, food, medical and legal help\" to the sailors, and \"passed a motion to establish a protected space where soldiers could access counseling and other support.” Ten local churches also offered sanctuary. In response a U.S. Attorney in San Francisco said “he would not hesitate to prosecute anyone who willfully harbors a military deserter”. When the ship sailed on November 12, around 1,200 protestors demonstrated outside of Naval Air Station Alameda to encourage sailors not to sail and at least 35 crewmen failed to report for duty.\n\nWhen the Coral Sea arrived in Hawaii in late November, it was greeted by a special performance in Honolulu of the FTA antiwar show, featuring Jane Fonda, Donald Sutherland, and Country Joe McDonald. More than 4,000 people attended, including approximately 2,500 members of the military and several hundred sailors from the Coral Sea. After the show, approximately 50 crewmen met with the show’s cast. When the ship pulled out of port to continue to Southeast Asia, another 53 sailors were missing.In January 1972 with the ship off Vietnam, Secretary of the Navy John Chafee visited the Coral Sea. SOS activists onboard held a demonstration and presented the Navy Secretary with a petition which 36 of them had signed.\n\nMeanwhile, back in San Diego, another aircraft carrier, the USS \"Kitty Hawk\", was preparing for deployment to Southeast Asia. Sailors on board, with help from the Concerned Military (the San Diego Concerned Officers Movement had broadened its membership to include enlisted personnel and changed its name) and civilian antiwar forces, began publishing their own newspaper called \"Kitty Litter\" and organizing a personalized version of SOS called “Stop the Hawk”. They circulated an antiwar petition which gathered several hundred signatures, but most copies were confiscated by higher ups. On November 15, 1972 around 150 Kitty Hawk crewmen attended a rally to hear Joan Baez sing and listen to four of their crewmates speak out against the war. Two days later, when the ship set sail for Indochina, 7 members of the crew publicly refused to sail and took refuge in local churches.\n\nWith the ship at sea, copies of \"Kitty Litter\" continued to be published by civilian supporters and mailed to antiwar crewmen who circulated them onboard and submitted articles. One article published in the August 1972 issue pointed to a serious problem onboard: \"In two consecutive masts, Captain Townsend has put several black men into the brig for fighting with white men, while dismissing \"with a warninn\" a white man who spoke in a demeaning manner to a black man and then proceeded to punch him in the stomach! Needless to say, this angered the black men on board, as well as many white men\". Clearly more trouble was brewing on the \"Hawk\" that was soon starkly revealed (see below).\n\nThe world first nuclear powered aircraft carrier, the USS \"Enterprise\", the Navy’s pride and joy, was not immune from dissention. In mid-1972 a number of dissident sailors on board began publishing a mock version of the ship’s official paper the \"Enterprise Ledger\". Called the \"SOS Enterprise Ledger\", it was virtually identical to the original except that it contained an antiwar and rebellious message. The ship’s commanding officer responded by issuing regulations prohibiting the distribution of literature he hadn't approved. The SOS sailors decided they needed to learn more about military law and attended a class called “Turning the Regs Around” conducted by the \"Bay Area Military Law Panel\". They then wrote and distributed a pamphlet outlining the legal rights of GIs.\n\nThey then knew they could legally petition Congress so they created a petition opposing both the war in Vietnam and the military's denial of basic G.I. rights. The petition began circulating on board and was quickly gathering signatures when it was confiscated by the brass. The Navy was quite alarmed by this activity onboard their showpiece ship and Naval Intelligence agents were called in. They began interrogating known SOS activists about alleged sabotage incidents and threatening revocation of security clearances. Since most of the agents’ questions were about SOS, SOS plans and past SOS activity, it seemed to the sailors that they were trying to find out more about SOS through scare tactics – as one sailor put it, “making us think we were going to be charged with sabotage to scare us into talking to clear ourselves.\" When the ship set sail from Alameda Naval Air Station on September 12, 1972, 5 SOS organizers were escorted off the ship under armed guard, while antiwar GI's, vets and civilians protested on shore and members of the \"Peoples Blockade\" sailed small boats in the San Francisco Bay symbolically blocking the Enterprise.\n\nWhen the USS \"Midway\" left Alameda NAS in April 1972, sixteen enlisted men assigned to Fighter Squadron 151 onboard the carrier signed an antiwar letter to President Nixon.\n\nOnboard the USS \"Ticonderoga\" in San Diego, sailors organized a movement they called “Stop It Now” (SIN), and 3 of the crew refused to board the ship when it set sail in May 1972. Once at sea, protests continued and there were reports of antiwar meetings of as many as 75 crew members.\n\nIn Norfolk, Virginia, on the East Coast, civilian antiwar groups organized against the sailing of the USS \"America\". When it began to leave port on June 5, 1972, thirty-one activists in thirteen canoes and kayaks positioned themselves in front of the massive ship. “When the Coast Guard moved in to clear the demonstrators, hundreds of sailors on the deck of the America jeered and pelted the cutters with garbage; in a clear show of support for the protesters.”\n\nAlso in June, but back up at Alameda, civilians were protesting the pending departure of the USS \"Oriskany\". “When the ship left, on June 6, as estimated twenty-five crew members refused to sail, including a group of ten men who turned themselves in to naval authorities on June 13 and issued a bitter public\" statement. In part they said: \"the only way to end the genocide being perpetrated now in South East Asia is for us, the actual pawns in the political game, to quit playing.\"\n\nGiven the strict discipline within the military as well as the not uncommon heavy handed response to internal dissent from military commanders, civilian support was an essential part of these military protests. From the beginning civilians and military veterans (many newly discharged from the very same ships) played a supporting role in the key Navy ports.\nIn San Diego, the local antiwar groups, including some of those who had waged the initial campaign against the USS \"Constellation\", took their support to a more organized level by banding together in 1972 to form the \"Center for Servicemen’s Rights\". They rented a large storefront space in the middle of the main GI strip which contained a bookstore, mimeo, meeting rooms and a stage; staffed by a \"collective of volunteers that included active duty sailors, a few Marines, veterans, and civilian activists.\" They provided individual and group counselling on GI rights, conscience objection and other means of opposing the war, as well as support for sailors who wanted to publish their own newspapers, leaflets or articles.\n\nThe San Francisco underground GI newspaper \"Up Against the Bulkhead\" and its staff played an important role in supporting the SOS movement, particularly the dissident sailors of the \"Coral Sea\". Founded in 1970, by late 1971 it could be found everywhere GIs congregated in the San Francisco Bay Area and sailors knew where to go when they wanted counterculture help and friendship. As two \"Bulkhead\" staffers reported years later, \"In 1971 a handful of sailors from the USS Coral Sea\" showed up at the \"Bulkhead\" office \"ready for action\". The sailors \"were a colorful lot, veritable hippies in uniform...more than eager not only to investigate ways to oppose the war, but also ways to sample the offerings of countercultural hedonism.\" With \"Bulkhead\" support the sailors went on to start the first SOS petition and officially launch the \"Stop Our Ship\" name.\n\nIn January 1972, attorneys from the National Lawyers Guild, worked with several active duty sailors to create a support center in Olongapo, Philippines, just outside Subic Bay, the largest U.S. Navy installation in the Pacific at that time. Sparked by the appearance of the FTA Show in December, the center was soon publishing an underground newspaper called \"Seasick\". The center supported dissenting GIs and worked to improve the conditions for sailors on shore leave, including by helping sailors gather over 500 signatures on a petition against police brutality by the Shore Patrol. The center was swiftly put out of business in September 1972 when martial law was declared in the Philippines (see below).\n\nThere were SOS Offices in Oakland, serving the San Francisco Bay Area, and Los Angeles. In Los Angeles, the office opened in January 1972 with SOS standing for \"Support Our Soldiers\", carrying on efforts initiated several years earlier by supporters of the GI Coffeehouse movement. They began putting out a newsletter called \"SOS News\" and announced in their first issue that they were “a support office for the GI Movement.” They pledged to take responsibility for “raising money, providing literature and films, recruiting staff, maintaining communications and providing publicity for GI coffeehouses and projects.” In Oakland, the Support Our Soldiers office offered similar supprt to the GI movement. Their newsletter stated that they would \"try to fill the needs for money, staff, educational materials, and communication that individual organizing projects cannot fulfill themselves.\"\n\nIn March 1972 the last U.S. ground combat division was withdrawn from Vietnam just as the North Vietnamese Army launched the Nguyễn Huệ or Easter Offensive. The U.S. responded by massively increasing its air war. “One main component was to be a flotilla of Seventh Fleet aircraft carriers (twice as many as in 1971) massed in the Gulf of Tonkin”. As the air and naval activity in the Pacific intensified, drastic changes occurred within the Navy. “During the remainder of the year, as many as four carriers…were on combat station in the Tonkin Gulf at one time…. Normal fleet routine was completely disrupted…. For the crew members involved, the escalation created severe hardships”. And as the concerns of sailors for their safety at sea \"became entangled with the enlarged Indochina war effort\" it \"led to protests against conditions that under other circumstances might have been bearable but, for the purpose of bombing Vietnam, were seen as intolerable”. Many of the sailors were already ambivalent or even antagonistic to the war, and now they were confronted with extremely intense and difficult, even unsafe, working conditions. The U.S.’s new strategy “was torpedoed by a massive antiwar movement among the sailors, who combined escalating protests and rebellions with a wide-spread campaign of sabotage.”\n\nThe Navy’s sudden need for additional personnel greatly intensified the pressure for new recruits and training. The Great Lakes Naval Station, located north of Chicago, IL, is the largest U.S. Navy training base and the only boot camp for naval enlistees. On May 20, 1972, the Movement for a Democratic Military (MDM) and the \"Chicago Area Military Project\" organized an Armed Forces Day demonstration with 400 GIs joining a crowd of over 2,000. Soon a petition was being circulated on the base opposing the increased speed-up in training that gathered more than 600 signatures.\n\nCrew members aboard the USS Nitro, a munitions ship loaded with armament at the Naval Weapons Station in Earle, New Jersey, staged one of the most dramatic protests. Worried about the war and the unsafe conditions on board they contacted civilian antiwar organizations who helped them draw up a list of specific shipboard hazards, which they circulated, gathering signatures from 48 members of the crew. The petition resulted in little change, but on April 24, 1972 as the ship was pulling out of port it was greeted by an antiwar blockade of seventeen canoes and small boats. As the Coast Guard attempted to disperse the demonstrators, they were first confronted by dissent from within their ranks, and then, one of the Nitro’s crew on the “ship’s deck suddenly stood up on the rail, raised the clenched-fist salute, and literally jumped overboard!” He was quickly followed by six more crew members, including one non-swimmer who had donned a life jacket.\nWilliam Monks, one of the seven who jumped explained his actions later: \n\nCol. Robert D. Heinl, Jr. stated in a 1971 study in the \"Armed Forces Journal\" called “The Collapse of the Armed Forces”, that “Internally speaking, racial conflicts and drugs…are tearing the services apart today.” Historians of the war have documented that non-white GIs were often given the dirtiest jobs and frequently sent to the front lines in combat situations. And a task force reporting to the Secretary of Defense at the time, Melvin R. Laird, found that racial discrimination in the military was not confined to the military but was “also a problem of a racist society”. Historian Gerald Gill contended that by 1970 most black soldiers thought the war was a mistake, “hypocritical in intent and racist and imperialist by design.” One black soldier was quoted as saying that black soldiers were sent on risky assignments by white officers so that ‘there would be one less nigger to worry about back home.’”\n\nIn 1972 the problems that Heinl and others were describing in the Army were about to explode in the Navy. As an article in \"The New York Times\" put it, “The Navy, with its tradition of Filipino waiters and Southern WASP leadership, never really was an alternative for America's young city blacks during the early years of the Vietnam war.” But, Navy commanders under pressure to increase recruitment to meet the needs of the expanding air war, “decided to change the Navy's image and its appeal to blacks.” By late 1972 approximately 12 percent “of new Navy recruits were black”, and they “were typically assigned to the ship's most miserable jobs”, and often \"thrust into the dreariest, most menial, and most unpopular jobs on board\" Several unprecedented developments on board U.S. Navy ships dramatically exposed the intersection of antiwar sentiment, civil rights issues, discontent over working and safety conditions and racism. One of the first of these incidents highlighted many of the common threads between them.\n\nWhen martial law was declared in the Philippines in September 1972, the U.S. Navy “used the occasion to crack down on so-called troublemakers and…loaded more than two hundred enlisted men from eleven different ships onto planes for immediate transfer back to San Diego.” Once in San Diego many of the men wanted to let the public know what had happened and on October 24 “a racially mixed group of thirty-one enlisted men held a press conference to denounce the deplorable conditions, harassment, and racial prejudice they encountered while at sea.” Their statement, partially excerpted here, captured many of the issues swirling within the Navy at that time:\n\nIn October 1972 the Hawk pulled into Subic Bay after 8 months at sea, expecting a rest before heading home. Instead, the crew was notified they were returning to combat operations in Indochina. Tensions, already high from accelerated combat operations, flared even more. Blacks made up 7 percent of the enlisted crew on board the Hawk, and the majority of them “were assigned to the toughest and dirtiest Navy jobs, in the deck force and on flight decks, while whites populated the more coveted and higher tech jobs in the crew.” The black crew members felt that had been “treated like dogs.” With a strong sense of black pride and influenced by the civil rights and Black Power movements in the U.S., they were very unhappy with their conditions. During the nights prior to deployment, several racial incidents occurred in Subic Bay enlisted men’s club. On October 8, a black sailor went on stage and shouted \"Black power! This war is the white man's war!\" and then continued to preach to the drunken crowd. A glass was thrown that hit him in the head, and a fight between blacks and whites broke out. On October 11, the night before the ship was to depart, a larger black-white fight erupted in the EM club which was broken up by a marine riot squad with clubs. Five black and four white sailors were arrested but returned to the ship before it departed. Back at sea, unhappy about being redeployed and overworked, many of the crew were now also deeply angry over racial tensions.\n\nOnce underway on October 12, an investigating officer began an inquiry into the fight by summoning several black sailors. “This [singling out of blacks only] was hard to understand in terms of the tensions” the NY Times quoted an officer saying “who had access to all of the reports of the incident.” One of those summoned brought “nine companions with him and grew belligerent”. The nine then stormed out to join other black sailors on the mess deck with the crowd soon growing to over one hundred. The chief master-at-arms became alarmed and summoned the marines, a move which was described later by “officialdom” as the first of “a series of mistakes” and an explosive situation quickly developed.\n\nThe ship’s executive officer ordered the blacks and the Marines to separate ends of the ship, but the Captain issued conflicting orders. In the ensuing confusion, the blacks and the Marines ran into each other on the hanger deck and a fight broke out. “The fighting spread rapidly, with bands of blacks and whites marauding throughout the ship’s decks and attaching each other with fists, chains, wrenches, and pipes.” More conflicting orders were issued resulting in more confusion and fights raged on for much of the night. The fights left 40 white and 6 black sailors injured, including three who had to be evacuated to onshore medical facilities. \n\nBy morning, a tense calm was achieved and normal flight operations were resumed. However, once arrests were made for the fighting, all of the 25 arrested were black. “The arrest pattern caused as grave concern in some areas of the Pentagon as the outbreak itself. ‘Anytime you have a so-called race riot and you lock up 28 blacks,’ one black Navy official noted caustically during a recent interview, ‘that has to raise some questions.’”\n\nAs word spread in the fleet about the incidents onboard the Kitty Hawk, many of the black sailors on the Constellation were “swearing an affinity with their beleaguered brothers on the Kitty Hawk.” By late October 1972, with the ship undergoing training exercises of the coast of Southern California, black crewmembers formed an organization called the “Black Fraction,” “with the aim of protecting minority interests in promotion policies and in the administration of military justice.” They elected a “chairman and three spokesmen to deal with the officers and petty officers of the ship.”\n\nAs commanders became aware of the Black Faction, they agreed to a meeting between them and the executive officer with the objective of easing tensions. Prior to the meeting, however, the command “singled out fifteen members of Black Faction as agitators and ordered that six of them be given immediate less-than-honorable discharges.” At the same time, notice was given ship wide that 250 men would be administratively discharged. Feeling they were being singled out for retaliation for their activism and fearing that most of the additional discharges would be directed at them, over one hundred sailors, including several whites, staged a sit in and refused to work on the morning of November 3.\n\nDuring the day, members of the ship’s Human Relations Council attempted to meet with the mutineers in a private dining room with little success as they demanded to see the captain. Around midnight, with the sit-in continuing the captain announced to the ship that any complaints had to go through the chain of command. “’O.K., that’s it,’ shouted a black sailor in the dining room. ‘They want another Kitty Hawk.’ There were shouts of ‘Let’s get the captain,’ and ‘Let’s take the ship.” The Captain then called general quarters and ordered senior officers and petty officers to pick out white sailors and move to the mess deck to surround the mutineers where they had now moved. A tense standoff ensued that slowly dissipated overnight.\n\nIn the morning the captain “washe[d] his hands of the mutineers. He [did] not want to admit mutiny, so he cannot charge the blacks with that. Yet he want[ed] them off his ship.” The captain, in consultation with others all the way up to the Chief of Naval Operations in Washington, decided to cut sea operations short and set the dissidents ashore. Once docked, 144 crew members left the ship, including 8 whites. The Constellation returned to sea but returned a few days later to pick up the mutinous sailors. Most of the men, however, refused to board the ship and on November 9 “staged a defiant dockside strike - perhaps the largest act of mass defiance in naval history.” Despite these unprecedented actions, none of the sailors were arrested, most were simply reassigned to other duty stations, while a few received what were described as “trifling punishments”.\n\nAs early as 1970 ships had been forced out of action by sabotage by crew members. For example, in June 1970 the destroyer Richard B. Anderson “was kept from sailing to Vietnam for eight weeks when crew members deliberately wrecked an engine.” But, in tandem with the SOS Movement, naval sabotage became an even more serious issue in 1972 as the air war dramatically expanded. “In July fires were started on the Forrestal and Ranger, the eighteenth instance of sabotage aboard the latter vessel, a prime target back home for peace activists’ ‘Stop Our Ships’ agitation.” The fire on the Forrestal resulted in over $7 million in damage and was the largest single act of sabotage in naval history. In March 1972 when the aircraft carrier USS Midway received orders for Vietnam, “dissident crewmen deliberately spilled three thousand gallons of oil into the bay. The captain of the Constellation “told a press conference in November of 1972 that ‘saboteurs were at work’ during the period of unrest aboard his ship.” The Commander in Chief of the Atlantic Fleet stated in October 1972 that the increase in sabotage was a “grave liability” to the ability of the Navy to operate.\n“All told, by the close of that year the United States Navy would log seventy-four instances of sabotage, more than half on aircraft carriers, none of them attributable to ‘enemy’ action.” An investigation by the House Armed Services Committee revealed how much the sabotage had undermined naval combat operations and contributed to other forms of unrest during the 1972 bombing campaign. They stated that the October 1972 redeployment of the Kitty Hawk to combat (discussed above) which resulted in riots and near mutiny was apparently “due to the incidents of sabotage aboard her sister ships U.S.S. Ranger and U.S.S. Forrestal.”\n\nWhile the SOS movement had no cohesive central organization or leadership, it was united by the overarching, if ultimately unattained, goal of stopping warships from entering combat in Indochina. Despite this failure on the stated objective, it was a significant part of the latter stages of the overall anti-Vietnam War movement which did, in fact, help to end the war.\n\n"}
{"id": "3240770", "url": "https://en.wikipedia.org/wiki?curid=3240770", "title": "Tacit assumption", "text": "Tacit assumption\n\nA tacit assumption or implicit assumption is an assumption that includes the underlying agreements or statements made in the development of a logical argument, course of action, decision, or judgment that are not explicitly voiced nor necessarily understood by the decision maker or judge. Often, these assumptions are made based on personal life experiences, and are not consciously apparent in the decision making environment. These assumptions can be the source of apparent paradoxes, misunderstandings and resistance to change in human organizational behavior.\n\n"}
{"id": "21465818", "url": "https://en.wikipedia.org/wiki?curid=21465818", "title": "The Emily Post Institute", "text": "The Emily Post Institute\n\nThe Emily Post Institute was created by etiquette author Emily Post in 1946. Post was an American writer and socialite who became the most famous authority on how to behave graciously in society and business. She published \"Etiquette in Society, in Business, in Politics, and at Home\" in 1922, which was wildly popular; later updated editions were published as \"Etiquette: The Blue Book of Social Usage\". Acting as a social civility barometer, EPI elucidates new manners for today's world based on the core tenets of honesty, respect, and consideration.\nFounded in 1946 by Emily Post and her son Ned, The Emily Post Institute, Inc. (or EPI for short), continues to promote good manners and civility in America and around the world. Spanning five generations, this family business maintains and evolves the standards of etiquette that Emily Post established with her seminal 1922 book. Manners change over time and vary depending on location and culture. The Emily Post Institute studies this evolution. \n\nThe organization, located in Burlington, Vermont, provides etiquette experts and advice to news outlets and other corporations.\n\n\n"}
{"id": "52150772", "url": "https://en.wikipedia.org/wiki?curid=52150772", "title": "Transmission Arts", "text": "Transmission Arts\n\nTransmission Arts, also sometimes known as Radio art, are defined \"as a multiplicity of practices and media working with the idea of transmission or the physical properties of the Electromagnetic spectrum (radio). Transmission works often manifest themselves in participatory live art or time-based art, and include, but are not limited to, sound, video, light, installation, and performance.\" \n\nBy their very nature, transmission artworks engage the public, and respond to radio and broadcast histories and materials. Organizations such as Wave Farm facilitate artist exploration of transmission-based creativity.\n\n"}
{"id": "435084", "url": "https://en.wikipedia.org/wiki?curid=435084", "title": "Urn problem", "text": "Urn problem\n\nIn probability and statistics, an urn problem is an idealized mental exercise in which some objects of real interest (such as atoms, people, cars, etc.) are represented as colored balls in an urn or other container. One pretends to remove one or more balls from the urn; the goal is to determine the probability of drawing one color or another, \nor some other properties. A number of important variations are described below.\n\nAn urn model is either a set of probabilities that describe events within an urn problem, or it is a probability distribution, or a family of such distributions, of random variables associated with urn problems.\n\nIn this basic urn model in probability theory, the urn contains \"x\" white and \"y\" black balls, well-mixed together. One ball is drawn randomly from the urn and its color observed; it is then placed back in the urn (or not), and the selection process is repeated.\n\nPossible questions that can be answered in this model are:\n\n\nIn \"Ars Conjectandi\" (1713), Jacob Bernoulli considered the problem of determining, given a number of pebbles drawn from an urn, the proportions of different colored pebbles within the urn. This problem was known as the \"inverse probability\" problem, and was a topic of research in the eighteenth century, attracting the attention of Abraham de Moivre and Thomas Bayes.\n\nBernoulli used the Latin word \"urna\", which primarily means a clay vessel, but is also the term used in ancient Rome for a vessel of any kind for collecting ballots or lots; the present-day Italian word for ballot box is still \"urna\". Bernoulli's inspiration may have been lotteries, elections, or games of chance which involved drawing balls from a container, and it has been asserted that \nElections in medieval and renaissance Venice, including that of the doge, often included the choice of electors by lot, using balls of different colors drawn from an urn.\n\n\n"}
