{"id": "33456414", "url": "https://en.wikipedia.org/wiki?curid=33456414", "title": "A Causal Theory of Knowing", "text": "A Causal Theory of Knowing\n\n\"A Causal Theory of Knowing\" is a philosophical essay written by Alvin Goldman in 1967, published in \"The Journal of Philosophy\". It is based on existing theories of knowledge in the realm of epistemology, the study of philosophy through the scope of knowledge. The essay attempts to define knowledge by connecting facts, beliefs and knowledge through underlying and connective series called causal chains. It provides a causal theory of knowledge.\n\nA causal chain is repeatedly described as a sequence of events for which one event in a chain causes the next. According to Goldman, these chains can only exist with the presence of an accepted fact, a belief in the fact, and a cause for the subject to believe the fact. The essay also explores the ideas of perception and memory through the use of the causal chains and the concept of knowledge.\n\nThe essay is regarded as an improvement and rebuttal of Edmund Gettier's \"Is Justified True Belief Knowledge?\", which is one of many attempts to explain the necessary conditions for knowledge to develop. Goldman implements the causal connection to reiterate his own theory of knowledge. Knowledge exists, says Goldman, if and only if the belief is justified by a reaction to the accepted fact.\n\nGoldman's theory later counters that of Michael Clark, stating that his own theory including figures and diagrams is more appropriate than Clark's. \"A Causal Theory of Knowing\" uses figures which make explicit references to causal beliefs. Clark's model does not utilize these arrows, and Goldman states that the lack of these arrows deems Clark's model deficient.\n\nAlvin Goldman, currently a professor of philosophy at Rutgers University, wrote \"A Causal Theory of Knowing\" when he was in his late twenties. Goldman received his Ph.D. from Princeton University, and has taught at numerous universities.\n\nGoldman's research deals mainly with epistemology and other cognitive sciences. \"A Causal Theory of Knowing\" was Goldman's first published paper explaining his own views of epistemology. Currently, Goldman has written more than ten essays focusing on knowledge and cognitive science.\n\nThe essay starts with a definition of Gettier's theory, followed by multiple reiterations of the idea of causal connections, figures to explain knowledge through a visual perspective, and references to perception and memory through causal chains.\n\nThe essay tends to focus on examples in which knowledge or other sensations do not exist, rather than proving a certain fact to be known. Goldman also states on multiple occasions that he does not wish to explain the causal process in detail, instead pointing out counterexamples. At numerous times in the essay, he also points out that he does not intend to give definitive answers to each of the propositions mentioned.\n\nGoldman also refocuses the idea of perception, or knowledge through sensation (specifically sight) using his own theory of knowing. The concept of causal perception indicates that one observes something only if the object itself causes the sensation of sight to be accepted as known. Thus, the object's existence must be factual and one must believe its existence. While all knowledge comes from facts, inferred knowledge emerges when physical object facts cause sense data which can be perceived as senses. The sense data can also be used to make conclusions, known as inferred knowledge, about certain physical object facts.\n\nFrom \"A Causal Theory of Knowing\", Goldman constructs the idea that memory is also a causal process. Memory is explained as being an extension of knowledge into the future, and remembering is the act of recalling a fact that has already been known. Further, the theory states that if knowledge is forgotten at one time, it cannot be considered a memory in the future. According to Goldman, if a fact is known at Time 1 but forgotten at Time 2, and then at Time 3 that the fact is perceived again but not known, at Time 3 the original fact is not a memory because there is no causal connection between the fact and the memory.\n\n"}
{"id": "55257518", "url": "https://en.wikipedia.org/wiki?curid=55257518", "title": "Amity-enmity complex", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy... has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n"}
{"id": "1134", "url": "https://en.wikipedia.org/wiki?curid=1134", "title": "Analysis", "text": "Analysis\n\nAnalysis is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 B.C.), though \"analysis\" as a formal concept is a relatively recent development.\n\nThe word comes from the Ancient Greek ἀνάλυσις (\"analysis\", \"a breaking up\", from \"ana-\" \"up, throughout\" and \"lysis\" \"a loosening\").\n\nAs a formal concept, the method has variously been ascribed to Alhazen, René Descartes (\"Discourse on the Method\"), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).\n\nThe field of chemistry uses analysis in at least three ways: to identify the components of a particular chemical compound (qualitative analysis), to identify the proportions of components in a mixture (quantitative analysis), and to break down chemical processes and examine chemical reactions between elements of matter. For an example of its use, analysis of the concentration of elements is important in managing a nuclear reactor, so nuclear scientists will analyse neutron activation to develop discrete measurements within vast samples. A matrix can have a considerable effect on the way a chemical analysis is conducted and the quality of its results. Analysis can be done manually or with a device. Chemical analysis is an important element of national security among the major world powers with materials\n\nChemists can use isotope analysis to assist analysts with issues in anthropology, archeology, food chemistry, forensics, geology, and a host of other questions of physical science. Analysts can discern the origins of natural and man-made isotopes in the study of environmental radioactivity.\n\n\n\n\nAnalysts in the field of engineering look at requirements, structures, mechanisms, systems and dimensions. Electrical engineers analyse systems in electronics. Life cycles and system failures are broken down and studied by engineers. It is also looking at different factors incorporated within the design.\n\nThe field of intelligence employs analysts to break down and understand a wide array of questions. Intelligence agencies may use heuristics, inductive and deductive reasoning, social network analysis, dynamic network analysis, link analysis, and brainstorming to sort through problems they face. Military intelligence may explore issues through the use of game theory, Red Teaming, and wargaming. Signals intelligence applies cryptanalysis and frequency analysis to break codes and ciphers. Business intelligence applies theories of competitive intelligence analysis and competitor analysis to resolve questions in the marketplace. Law enforcement intelligence applies a number of theories in crime analysis.\n\nLinguistics look at individual languages and language in general. It breaks language down and analyses its component parts: theory, sounds and their meaning, utterance usage, word origins, the history of words, the meaning of words and word combinations, sentence construction, basic construction beyond the sentence level, stylistics, and conversation. It examines the above using statistics and modeling, and semantics. It analyses language in context of anthropology, biology, evolution, geography, history, neurology, psychology, and sociology. It also takes the applied approach, looking at individual language development and clinical issues.\n\nLiterary criticism is the analysis of literature. The focus can be as diverse as the analysis of Homer or Freud. While not all literary-critical methods are primarily analytical in nature, the main approach to the teaching of literature in the west since the mid-twentieth century, literary formal analysis or close reading, is. This method, rooted in the academic movement labelled The New Criticism, approaches texts – chiefly short poems such as sonnets, which by virtue of their small size and significant complexity lend themselves well to this type of analysis – as units of discourse that can be understood in themselves, without reference to biographical or historical frameworks. This method of analysis breaks up the text linguistically in a study of prosody (the formal analysis of meter) and phonic effects such as alliteration and rhyme, and cognitively in examination of the interplay of syntactic structures, figurative language, and other elements of the poem that work to produce its larger effects.\n\nModern mathematical analysis is the study of infinite processes. It is the branch of mathematics that includes calculus. It can be applied in the study of classical concepts of mathematics, such as real numbers, complex variables, trigonometric functions, and algorithms, or of non-classical concepts like constructivism, harmonics, infinity, and vectors.\n\nFlorian Cajori explains in (1893) the difference between modern and ancient mathematical analysis, as distinct from logical analysis, as follows:\n\nThe terms \"synthesis\" and \"analysis\" are used in mathematics in a more special sense than in logic. In ancient mathematics they had a different meaning from what they now have. The oldest definition of mathematical analysis as opposed to synthesis is that given in [appended to] Euclid, XIII. 5, which in all probability was framed by Eudoxus: \"Analysis is the obtaining of the thing sought by assuming it and so reasoning up to an admitted truth; synthesis is the obtaining of the thing sought by reasoning up to the inference and proof of it.\" \n\nThe analytic method is not conclusive, unless all operations involved in it are known to be reversible. To remove all doubt, the Greeks, as a rule, added to the analytic process a synthetic one, consisting of a reversion of all operations occurring in the analysis. Thus the aim of analysis was to aid in the discovery of synthetic proofs or solutions.\nJames Gow uses a similar argument as Cajori, with the following clarification, in his \"A Short History of Greek Mathematics\" (1884):\nThe synthetic proof proceeds by shewing that the proposed new truth involves certain admitted truths. An analytic proof begins by an assumption, upon which a synthetic reasoning is founded. The Greeks distinguished \"theoretic\" from \"problematic\" analysis. A theoretic analysis is of the following kind. To \"prove\" that A is B, \"assume\" first that A is B. If so, then, since B is C and C is D and D is E, therefore A is E. If this be known a falsity, A is not B. But if this be a known truth and all the intermediate propositions be convertible, then the reverse process, A is E, E is D, D is C, C is B, therefore A is B, constitutes a synthetic proof of the original theorem. Problematic analysis is applied in all cases where it is proposed to construct a figure which is assumed to satisfy a given condition. The problem is then converted into some theorem which is involved in the condition and which is proved synthetically, and the steps of this synthetic proof taken backwards are a synthetic solution of the problem.\n\n\n\n\n\nIn statistics, the term \"analysis\" may refer to any method used\nfor data analysis. Among the many such methods, some are:\n\n\n\n"}
{"id": "1799525", "url": "https://en.wikipedia.org/wiki?curid=1799525", "title": "Antipathy", "text": "Antipathy\n\nAntipathy is a voluntary or involuntary dislike for something or somebody, the opposite of sympathy. While antipathy may be induced by experience, it sometimes exists without a rational cause-and-effect explanation being present to the individuals involved.\n\nThus, the origin of antipathy has been subject to various philosophical and psychological explanations, which some people find convincing and others regard as highly speculative. The exploration of a philosophical aspect for antipathy has been found in an essay by John Locke, an early modern 17th century philosopher.\n\nInterpersonal antipathy is often irrationally ascribed to mannerisms or certain physical characteristics, which are perceived as signs for character traits (e.g., close, deep set eyes as a sign for dullness or cruelty). Further, the negative feeling sometimes takes place rapidly and without reasoning, functioning below the level of attention, thus resembling an automatic process. \n\nChester Alexander’s empirical findings suggest that an important characteristic of antipathies is that they are \"marginal to reflective consciousness\". Alexander based this conclusion on the fact that many of the subjects of the study reported to have never thought much about their antipathies, have not tried to analyze them or discuss them with others. \n\nSympathy and antipathy modify social behavior. Although it is generally assumed that antipathy causes avoidance, some empirical studies gathered evidence that an antipathetic reaction to objects was not followed by any effort to avoid future encounters.\n\nIn personality psychology, antipathy may be related to low agreeableness.\n\nSophie Bryant observed the occurrence of pseudo-antipathy which consists in \"the careless and arbitrary interpretation of another person's acts and expressions in accordance with the worst side of one's self\". In other words, people tend to project their own faults onto others and dislike or hate them. Pseudo-antipathy is based on the (implicit) knowledge about the negative sides of a person's own character. Bryant compares the resulting feeling with \"a certain wrong-headed sense of cleansing\".\n"}
{"id": "845412", "url": "https://en.wikipedia.org/wiki?curid=845412", "title": "BAC Mustard", "text": "BAC Mustard\n\nThe Multi-Unit Space Transport And Recovery Device or MUSTARD, usually written as Mustard, was a concept explored by the British Aircraft Corporation (BAC) during the mid-1960s for launching payloads weighing as much as into orbit. Operating as a multi-stage rocket for launch, the individual stages were near-identical modules, each flying back to land as a spaceplane.\n\nBritish interest in re-usable space vehicles started life under English Electric at Warton, Lancashire, UK, as part of a government-sponsored series of wider studies into high-speed vehicles and sub-orbital spaceplanes. Their aerospace activities were later merged with other firms to form BAC. American influence had already been seen, but now BAC studied the various transatlantic projects with greater interest. The Douglas Astro in particular impressed them and around the beginning of 1964 they took it as the starting point for their own clustered design, which they called the Multi-Unit Space Transport And Recovery Device or MUSTARD, but it was usually written simply as Mustard.\n\nThe last major design study was drawn up early in 1967 and the project continued at a lower level until it was finally terminated in 1970 by the British government's decision to participate in the new American post-Apollo project. Key Mustard project staff spent the first two years of the 1970s at North American Rockwell contributing to the initial study which would eventually lead to the Space Shuttle. By then the prospect of collaboration had faded and, with UK government interest also gone, the Mustard project was terminated.\n\nBAC was itself later merged into British Aerospace (B.Ae) and when the HOTOL project arose in 1984 it was moved to Warton to take advantage of the expertise built up during the Mustard project.\n\nMustard was a modular re-usable space launch system, comprising multiple copies of a single vehicle design, each of which was configured for a different role as a booster stage or an orbital spaceplane. The core vehicle design followed the Douglas Astro delta-winged reusable vehicle, as would the later US Space Shuttle, in being a vertically-launched rocket with integral wings so that it landed horizontally as an aeroplane. The design team was led by Tom Smith, Chief of the Aerospace Department at BAC.\n\nThe design evolved through a total of fifteen proposed variants or schemes, each typically comprising a deep-keeled lifting-body airframe with delta wings in a smooth blended wing body layout, with twin tail fins rising from the wing tips and canted outwards. Some early variants had a compound-delta wing with inboard tail fins. Power was provided by anything from one to four rocket engines in the rear fuselage.\n\nThere were two primary vehicle configurations, respectively for the orbiter and booster stages. The orbiter could be manned and had ducting to receive fuel from the boosters. The boosters were unmanned and incorporated systems to pump fuel across to the orbiter or to each other. In this way the orbiter could remain fully topped-up for its long orbital injection flight, while all the vehicles could have a standard design of fuel tank.\n\nVarious clustering and stacking arrangements were explored. Where the Astro had launched as a two-stage step-rocket with the booster much larger than the orbiter, Mustard comprised from three to five similar-sized modules.\n\nEarly studies focused around a vehicle with a shallow 120° \"vee\" underside to both body and wings, so that three could be clustered in a triangle. Some included a fourth, orbital vehicle mounted on top of three boosters. The most efficient regime was to empty one booster at a time, keeping the others topped up for as long as possible, so that the first-stage booster could be dropped as soon as possible. The three boosters would be emptied in turn. But this led to an asymmetric mass loading which BAC believed to be a significant problem, so later designs used a sideways stacking system in which flatter modules were stacked more like sheets of paper.\n\nAt , at around 30 nautical miles, the last of the booster units would separate and land like aircraft.\n\nThe spacecraft would place its payload into orbit at around 1000 nautical miles, after 10 minutes from launch, and then return in a like manner.\n\n\n\n"}
{"id": "843918", "url": "https://en.wikipedia.org/wiki?curid=843918", "title": "Barthélemy Boganda", "text": "Barthélemy Boganda\n\nBarthélemy Boganda (4 April 1910 – 29 March 1959) was the leading nationalist politician of what is now the Central African Republic. Boganda was active prior to his country's independence, during the period when the area, part of French Equatorial Africa, was administered by France under the name of Oubangui-Chari. He served as the first Prime Minister of the Central African Republic autonomous territory.\n\nBoganda was born into a family of subsistence farmers, and was adopted and educated by Roman Catholic Church missionaries. In 1938, he was ordained as the first Roman Catholic priest from Oubangui-Chari. During World War II, Boganda served in a number of missions and after was persuaded by the Bishop of Bangui to enter politics. In 1946, he became the first Oubanguian elected to the French National Assembly, where he maintained a political platform against racism and the colonial regime. He then returned to Oubangui-Chari to form a grassroots movement in opposition of French colonialism. The movement led to the 1949 foundation of the Movement for the Social Evolution of Black Africa (MESAN), which became popular among villagers and the working class. Boganda's reputation was slightly damaged when he was laicized from the priesthood after marrying Michelle Jourdain, a parliamentary secretary. Nonetheless, he continued to advocate for equal treatment and civil rights for blacks in the territory well into the 1950s.\n\nIn 1958, after the French Fourth Republic began to consider granting independence to most of its African colonies, Boganda met with Prime Minister Charles de Gaulle to discuss terms for the independence of Oubangui-Chari. De Gaulle accepted Boganda's terms, and on 1 December, Boganda declared the establishment of the Central African Republic. He became the autonomous territory's first Prime Minister and intended to serve as the first President of the independent CAR. He was killed in a mysterious plane crash on 29 March 1959, while en route to Bangui. Experts found a trace of explosives in the plane's wreckage, but revelation of this detail was withheld. Although those responsible for the crash were never identified, people have suspected the French secret service, and even Boganda's wife, of being involved. Slightly more than one year later, Boganda's dream was realized, when the Central African Republic attained formal independence from France.\n\nBoganda was born to a family of subsistence farmers in Bobangui, a large M'Baka village in the Lobaye basin located at the edge of the equatorial forest some southwest of Bangui. French commercial exploitation of Central Africa had reached an apogee around the time of Boganda's birth, and although interrupted by World War I, activity resumed in the 1920s. The French consortia used what was essentially a form of slavery—the \"corvée\"—and one of the most notorious was the Compagnie forestière de la Sangha-Oubangui, involved in rubber gathering in the Lobaye district.\n\nIn the late 1920s, Boganda's mother was beaten to death by the company's officials while collecting rubber in the forest. His uncle, whose son Jean-Bédel Bokassa would later crown himself as the Emperor of the Central African Empire, was beaten to death at the colonial police station as a result of his alleged resistance to work. Boganda's father was a witch doctor who had engaged in cannibalistic rituals.\n\nDuring his early years, Boganda was adopted by Catholic missionaries. As a boy he attended the school opened at Mbaiki (the administrative centre for the Lobaye prefecture) by the post's founder, Lieutenant Mayer. From December 1921 to December 1922, he spent two hours a day with Monsignor Jean-Réné Calloch learning how to read, while spending the rest of his time performing manual labour. On 24 December, he was received into the church under the name Barthélemy, in honour of one of the Twelve Apostles of Jesus Christ who was believed to have worked as Christian missionary in Africa. Father Gabriel Herrau sent Boganda to the Catholic School of Betou and then to the school of the Saint Paul Mission at Bangui, where he completed his primary studies under Mgr Calloch, whom he would consider his spiritual father. The missionaries there, encouraged by his intellectual promise and pious demeanour, helped him continue secondary studies at small seminaries in Brazzaville and Kisantu (under Belgian Jesuits) before he moved on to the great seminary at Yaoundé. On 17 March 1938, fulfilling an ambition he had had since age twelve, he was ordained and became the first Roman Catholic priest native to Oubangui-Chari, as the colony was then called. He ministered at Bangui, Grimari and Bangassou, and in 1939, his bishop denied his request to join the French Army. He was needed at home, as many Frenchmen involved with the church had been recalled to the metropole to fight in World War II, during which he served in a number of missions.\n\nAfter World War II, Boganda was urged by the Bishop of Bangui, Mgr Grandin, to complement his humanitarian and social works through political action. Boganda decided to run for election to the National Assembly of France. On 10 November 1946, he became the first Oubanguian elected to the assembly after winning almost half of the total votes cast and defeating three other candidates, including the outgoing incumbent, François Joseph Reste, who had formerly served as the Governor-General of French Equatorial Africa. Boganda arrived in Paris attired in his clerical garb and introduced himself to his fellow legislators as the son of a polygamous cannibal. From 1947 on, Boganda conducted a lively campaign against racism and the colonial regime. Soon realizing the limits of his influence in France (he served in parliament until 1958 but gradually detached himself from its activities), he returned to Oubangui-Chari to organise a grassroots movement of teachers, truck drivers and small producers to oppose French colonialism, although his previous attempt to set up a marketing cooperative among African planters of his own ethnicity had failed. On 28 September 1949, at Bangui, he founded the Movement for the Social Evolution of Black Africa (MESAN), a quasi-religious political movement and party that sought to affirm black humanity and quickly came to dominate local politics. His political creed was summed up in the Sango phrase \"zo kwe zo\", which translated to \"every human being is a person\". Effectively, he was looking for equal treatment and civil rights for blacks within the French Union rather than independence, at least for the time being. He demarginalised large masses of people—women, youth, workers, poor cultivators—with the intent of unleashing the creativity of the Oubanguian people by placing them centre stage in the making of their country's history.\nThe movement was more popular among villagers than among \"évolué\" townsmen, whom Boganda considered servile and to whom he applied the derogatory term \"Mboundjou-Voko\" (\"Black-Whites\"). Additionally, he created the Intergroupe Liberal Oubanguien (ILO) in 1953, which aimed to elect an equal number of black and white politicians to the assembly, so that a united electoral college could be established. MESAN's activities angered the French administration and the companies trading in cotton, coffee, diamonds and other commodities. The Bangui chamber of commerce was controlled by these companies, and the men who gathered at this club strongly resented the demise of forced labour and the resultant rise of black nationalism. They hated Boganda in particular, viewing him as a dangerous revolutionary demagogue and a threat to their \"free enterprise\", and they resolved to get rid of him. They also set up local RPF branches to counter MESAN, and the presence of African Democratic Rally (RDA) in the other three territories of French Equatorial Africa posed some menace for MESAN, but by 1958, although other parties were allowed, they had been reduced to tiny groups. On many occasions, General Charles de Gaulle expressed his sympathy for Oubangui-Chari, which had supported de Gaulle's Free French Forces as early as August 1940, and refused to support the violent intrigues of the RPF against Boganda and his men. He received Boganda, by then head of the Grand Council of French Equatorial Africa and pushing for independence, in Paris in July 1958 and was in turn received at Brazzaville in August. The discussions there led to the General accepting Boganda's demands for independence and the endorsement of the French Community in September throughout French Equatorial Africa.\n\nBoganda's attachment to his chosen calling weakened when he met and fell in love with a young Frenchwoman, Michelle Jourdain, who was employed as a parliamentary secretary. They were married on 13 June 1950, for which Boganda was expelled from the priesthood and cut off from the Catholic hierarchy's support. Boganda and Jourdain would later have two daughters and a son. The affair caused a minor scandal in Paris, but it did little to dent his popularity with his people. In the National Assembly he continued to battle, often in vain, against repressive features of the French administration in Oubangui-Chari. Arbitrary arrest, low wages, compulsory cotton cultivation, and the exclusion of blacks from restaurants and cinemas were all targets of his rhetoric.\n\nOn 29 March 1951, Boganda was sentenced to two months in prison following his arrest on 10 January for \"endangering the peace\" after intervening in a local market dispute (the \"Bokanga incident\" in Lobaye). His wife was sentenced to 15 days in prison, but neither served their terms. On 17 June, he was re-elected to the National Assembly with 48% of the vote despite the obstacles placed in his way by the administration and strong opposition by the authorities, colonists, and the missions, with two prominent French candidates seeking to oust him. At this time, he emerged as an extraordinarily popular messianic folk hero and his country's leading nationalist; MESAN became the majority party in the Territorial Assembly elections in March 1952. In this period he divided his time between his coffee plantation, his emancipation work and new political positions. In April 1954, an incident that would showcase Boganda's talent and appeal with crowds erupted at Berbérati. A white public works agent, who had recently been reprimanded for his brutality toward Africans, announced that his cook and the cook's wife had died. A riot broke out and the governor sent in parachutists while armoured vehicles patrolled the streets. Boganda hesitated to appear in a village that was not one of his strongholds, but did so anyway and declared before the rioters that justice would be the same for blacks and whites. Upon hearing Boganda's words, the crowd became calm and dispersed.\nHe played a crucial role at the beginning of internal autonomy (1956–1958), although the relatively conservative Boganda remained sympathetic to French interests and still did not advocate immediate independence. For Boganda, the 1956 election, in which he took 89% of the vote against another Oubanguian, was an uncontested speaker's platform with which the colonial administration had come to terms; the French had realised that opposing him would be dangerous and sought to accommodate him. That year he agreed to European representation on election lists in exchange for the financial support of French business leaders, and on 18 November, was elected as the first mayor of Bangui. On 31 March 1957, MESAN won all seats in the Territorial Assembly election; on 18 June, Boganda was elected president of the Grand Council of French Equatorial Africa (a forum he used to broadcast his views on African unity) and in May was appointed vice-president of the Oubangui-Chari Government Council (the French governor was still its president).\n\nA pragmatist, Boganda spoke before the local assembly on 30 December 1957 in praise of the new Comité de Salut Economique, which envisioned joint administration of the economy between French colonials and MESAN territorial councilors (he called it \"the union of capital and Oubanguian labour\"), but lack of French investment and opposition by Oubanguians soon led him to turn away from the idea. With the numerous declarations of independence being made in much of Francophone Africa, Boganda advised that an independent Oubangui-Chari would face major economic problems from the onset. Instead, he advocated the independence of all of French Equatorial Africa and its integration into a United States of Latin Africa comprising the former French, Belgian, and Portuguese colonies of Central Africa; he intended for Oubangui-Chari to become a federal unit within that structure. However, such a federation proved unrealistic, foundering on the rocks of regional jealousy and personal ambition, and Boganda came to accept a constitution covering only Oubangui-Chari as the Central African Republic. Thus, after 1 December 1958, when Boganda declared the establishment of the Central African Republic as an autonomous member of the French Community, the name was applied only to the former Oubangui-Chari. On 8 December, the CAR's first government came into being with Boganda as prime minister; a French governor remained in the country but was now called high commissioner. The new government began by adopting a law banning nudity and vagabondage, Boganda's missionary education still showing through. Its main task, however, was to draw up a constitution, which was democratic and modelled to some extent on that of France; this was approved by the assembly on 16 February 1959. Formal independence came later, on 13 August 1960.\n\nBoganda was poised to become the first president of the independent CAR when he boarded a plane at Berbérati for a flight to Bangui on 29 March 1959, just prior to legislative elections. The aircraft exploded in midair over Boukpayanga in the sub-prefecture of Boda (about west of Bangui), killing all passengers and crew. No clear cause has ever been ascertained for the mysterious crash and no commission of inquiry was ever formed; sabotage was widely suspected. The nation was shocked at the death of its revered leader, whose funeral on 2 April at the cathedral of Notre-Dame de Bangui saw a great outpouring of grief from thousands of Oubanguians. The 7 May edition of the Paris weekly \"L'Express\" revealed that experts had found traces of explosive in the wreckage, but the French high commissioner banned the sale of that magazine edition when it appeared in the CAR. Many suspected that expatriate businessmen from the Bangui chamber of commerce, possibly aided by the French secret service, played a role. Michelle Jourdain was also suspected of being involved: by 1959, relations between Boganda and his wife had deteriorated, and he thought of leaving her and returning to the priesthood. She had a large insurance policy on his life, taken just days before the accident. According to Brian Titley, author of \"Dark Age: The Political Odyssey of Emperor Bokassa\", there are good reasons for suspecting her involvement in the plane crash.\n\nAbel Goumba, the vice-premier and finance minister whom Titley describes as \"intelligent, honest, and strongly nationalistic\", emerged as Boganda's logical successor. However, his close confidant and cousin, interior minister David Dacko, more likely to lead a regime deferential to foreign interests, was backed by the high commissioner, Colonel Roger Barberot, with the support of the chamber of commerce and Michelle Jourdain. He thus brushed aside Goumba and by 1962 had shut down the opposition, with MESAN becoming the country's single party. The events after Boganda's death are strongly evocative of other French efforts to maintain economic domination by ensuring that compliant leaders came to power in its former colonies. It also robbed the country of a charismatic leader in the Houphouët-Boigny or Senghor mould, whose prestige alone might have sufficed to retain civilian rule, which ended when Bokassa deposed the unpopular Dacko in 1966.\n\nBoganda is not only considered the hero and father of his nation but also as one of the great leaders of decolonization in Africa; the historian Georges Chaffard described him after his death as \"the most prestigious and the most capable of Equatorial political men,\" while political historian Gérard Prunier called him \"probably the most gifted and most inventive of French Africa's decolonization generation of politicians.\" Among the places named after him are an avenue in Bangui, one of the city's largest high schools, a Château Boganda and Barthélemy Boganda Stadium. 29 March, the anniversary of his death, is Boganda Day, a public holiday. Boganda was also the designer of the flag of the Central African Republic, originally intended for the United States of Latin Africa.\n\nBoganda is one in a long line of African political leaders who, in an attempt to develop specifically national political cultures, were presented (or presented themselves) as the great national leader, glorified and sometimes nearly deified. They were hailed as the fathers of their nations and considered wise in the ways of understanding the best interests of their peoples. Others who became particular objects of hero-worship include Léopold Sédar Senghor, Félix Houphouët-Boigny, Moktar Ould Daddah, Ahmed Sékou Touré, Modibo Keïta, Léon M'ba and Daniel Ouezzin Coulibaly. Boganda did little to discourage wide circulation of tales about his supernatural powers, putative invulnerability and even immortality. Shortly before his death, a large crowd waited on the shore of the Ubangui River to see him cross by walking upon the waters. He did not show up, but apparently a good many people still believed that he could have made the miraculous crossing. More than just a charismatic political leader, he was seen as the \"black Christ\", a great religious figure endowed with extraordinary powers. Along with Congo-Brazzaville's Fulbert Youlou, who remained a priest while president, Boganda was not particularly concerned with his religious mission once he entered politics, but he unabashedly used the enormous popular respect for the Church and the cloth to political advantage. He successfully manipulated religious symbols (clerical garb, crosses, baptism, disciples, acolytes, etc.) for political purposes.\n\nOnce he died, his mystique grew: he was a national martyr, and miracles were regularly attributed to him. The Boganda myth continues to exercise a strong hold on many people in the CAR, and it has frequently been used by his successors in their appeals for national unity. Those who were related to him even tenuously, such as Bokassa (who was from the same village and minority ethnic group, was the son of his mother's uncle, justified his coup using Boganda's name and created a cult of Boganda as founder of the party and state), or Dacko (who posed as the ideological successor of Boganda by championing for \"national reconciliation\" during the 1981 election) were able to capture some of his aura and use it to their advantage.\n\n\n"}
{"id": "33547930", "url": "https://en.wikipedia.org/wiki?curid=33547930", "title": "Collective Induction", "text": "Collective Induction\n\nCollective induction is a task developed by Steiner and used in research on group problem solving. Broadly, the method entails \"the cooperative search for descriptive, predictive, and explanatory generalizations, rules, and principles\" among members in a group working on the same task. James Larson further defined collective induction tasks as \"[tasks] in which problem solvers work cooperatively to induce a general rule or principle that can account parsimoniously for a given set of facts or observations\" This particular process has been used to determine if groups are better problem solvers than individuals.\n\nRule Induction Tasks\n\nThe most frequently used collective induction task is a task that requires participants to discern the pattern that a particular series of playing cards are laid in. The task experimenter first generates a rule about how cards should be laid. The rule may be determined based on any facet of the cards. For example, the pattern could be two red cards followed by a spade. The experimenter begins by laying a card that fits the rule. Then group members make suggestions for what card should be laid next. If the card fits the pattern, the experimenter places the card to the right of the previous correctly laid card. If the card does not fit the pattern, then the experimenter places it below the previous card. Participants work to discern the pattern. After ten rounds, participants are asked to propose their hypothesis about what the rule could be. Some modifications to this procedure have been used. For example, in some variations, groups are allowed to put forth multiple hypotheses and use multiple card displays \n\nAnother, less widely used collective induction task also involves cards. In this task, four cards are used with the letters A or D on one side of the card and the numbers 4 or 7 on the other side. The cards are placed on the table to show one face of each letter and number. Participants are asked to determine the minimum number of cards they will need to turn over in order to determine if cards with vowels always have even numbers on their opposite side.\n\nDo groups perform better than individuals?\n\nThis question has been tested by comparing group performance to the performance of individuals alone or in a nominal group, a group of individuals whose performance is grouped together even though they did not work together. For example, if groups of four worked on a task together, then researchers could see if they performed better than individuals by randomly combining the efforts of four individuals who worked on the same task independently. This allows researchers to assess the potential gains or losses caused by group interaction while recognizing that more people working on a task may be more likely to arrive at the solution by random chance. Group performance is compared against both the best performing individual in each nominal group (thus answering the question, \"do groups perform better than all individuals?\") and the average of all individual performances (\"do groups perform better than average?\"). Several experiments have sought to address this question and results have consistently suggested that groups perform better than the average individual though not as well as the best individual in nominal groups.\n\nThe first researchers to test if group performance is better than individual performance were Laughlin and Shippy in 1983. They found that groups performed better than individuals, proposing more plausible hypotheses and finding the correct rule with fewer rounds than individuals. However, this does not address whether individuals working cooperatively can perform better than the best member of a similarly sized nominal group. This was tested in three separate experiments by Laughlin and colleagues. In these studies, results indicated that groups working cooperatively performed about as well as the best individual in each nominal group and significantly better than the second, third, and worst members of the nominal group. No study to date has shown that groups perform better than the best member of a nominal group.\n\nTwelve Postulates of Collective Induction Tasks\n\nAs noted by Patrick Laughlin, there are twelve postulates of collective induction tasks. These rules are the result of years of research on collective induction tasks with groups and were originally published in the journal Organizational Behavior and Human Decision Processes. These twelve postulates compose a theory of collective induction.\n\nSee Also\n\nSteiner, I. D. (1972). \"Group processes and productivity\". New York: Academic Press.\n\nReferences\n"}
{"id": "35979526", "url": "https://en.wikipedia.org/wiki?curid=35979526", "title": "Conservation management system", "text": "Conservation management system\n\nA conservation management system (CMS) is a procedure for maintaining a species or habitat in a particular state. It is a means whereby humankind secures wildlife in a favourable condition for contemplation, education or research, in perpetuity. It is an important topic in cultural ecology, where conservation management counterbalances the unchecked exploitative management of natural resources. Conservation management systems are vital for turning sustainable development strategies into successful operations.\n\nIn New Zealand the Department of Conservation develops conservation management strategies in conjunction with the community as a means of prioritising conservation issues.\n\nConservation management has historically adopted ideals deriving from 3 discursive approaches: the classic approach, populist approach, and neoliberal approach. All three approaches have differing ideas about the nexus of conservation and development and their potential interactions. \n\nNational Parks are heavily managed conservations areas. The approach adopted by a conservation authority will influence the management of a Park and dictate how the park authorities view the role of the park and the relationship visitors may have with it. \nAn example of a park adopting a populist approach is the Rouge National Urban Park located in Canada’s largest city Toronto. Though controlled by the Government of Canada through Parks Canada, the Rouge National Urban Park encourages the community to access the park to learn, play and live. The complexity of the Park being in a large metropolitan city has meant that Parks Canada has incorporated the surrounding communities into the planning, implementation, and management of the park. \n\n\n"}
{"id": "19948728", "url": "https://en.wikipedia.org/wiki?curid=19948728", "title": "Covering number", "text": "Covering number\n\nIn mathematics, a covering number is the number of spherical balls of a given size needed to completely cover a given space, with possible overlaps. Two related concepts are the \"packing number\", the number of disjoint balls that fit in a space, and the \"metric entropy\", the number of points that fit in a space when constrained to lie at some fixed minimum distance apart.\n\nLet (\"M\", \"d\") be a metric space, let \"K\" be a subset of \"M\", and let \"r\" be a positive real number. Let \"B\"(\"x\") denote the ball of radius \"r\" centered at \"x\". A subset \"C\" of \"M\" is an \"r-external covering\" of \"K\" if:\nIn other words, for every formula_2 there exists formula_3 such that formula_4.\n\nIf furthermore \"C\" is a subset of \"K\", then it is an \"r-internal covering\".\n\nThe external covering number of \"K\", denoted formula_5, is the minimum cardinality of any external covering of \"K\". The internal covering number, denoted formula_6, is the minimum cardinality of any internal covering.\n\nA subset \"P\" of \"K\" is a \"packing\" if formula_7 and the set formula_8 is pairwise disjoint. The packing number of \"K\", denoted formula_9, is the maximum cardinality of any packing of \"K\".\n\nA subset \"S\" of \"K\" is \"r\"-\"separated\" if each pair of points \"x\" and \"y\" in \"S\" satisfies \"d\"(\"x\", \"y\") ≥ \"r\". The metric entropy of \"K\", denoted formula_10, is the maximum cardinality of any \"r\"-separated subset of \"K\".\n\n1. The metric space is the real line formula_11. formula_12 is a set of real numbers whose absolute value is at most formula_13. Then, there is an external covering of formula_14 intervals of length formula_15, covering the interval formula_16. Hence:\n\n2. The metric space is the Euclidean space formula_18 with the Euclidean metric.\nformula_19 is a set of vectors whose length (norm) is at most formula_13.\nIf formula_21 lies in a \"d\"-dimensional subspace of formula_18, then:\n\n3. The metric space is the space of real-valued functions, with the l-infinity metric.\nThe covering number formula_6\nis the smallest number formula_13 \nsuch that, there exist formula_26\nsuch that, for all formula_27 there exists formula_28\nsuch that the supremum distance between formula_29 and formula_30\nis at most formula_15.\nThe above bound is not relevant since the space is formula_32-dimensional.\nHowever, when formula_21 is a compact set, every covering of it has a finite sub-covering, \nso formula_6 is finite.\n\n1. The internal and external covering numbers, the packing number, and the metric entropy are all closely related. The following chain of inequalities holds for any subset \"K\" of a metric space and any positive real number \"r\".\n\n2. Each function except the internal covering number is non-increasing in \"r\" and non-decreasing in \"K\". The internal covering number is monotone in \"r\" but not necessarily in \"K\".\n\nThe following properties relate to covering numbers in the standard Euclidean space formula_18:\n\n3. If all vectors in formula_21 are translated by a constant vector formula_38, then the covering number does not change.\n\n4. If all vectors in formula_21 are multiplied by a scalar formula_40, then:\n\n5. If all vectors in formula_21 are operated by a Lipschitz function formula_44 with Lipschitz constant formula_13, then:\n\nLet formula_21 be a space of real-valued functions, with the l-infinity metric (see example 3 above).\nSuppose all functions in formula_21 are bounded by a real constant formula_50.\nThen, the covering number can be used to bound the generalization error\nof learning functions from formula_21,\nrelative to the squared loss:\n\n"}
{"id": "15342640", "url": "https://en.wikipedia.org/wiki?curid=15342640", "title": "DPSIR", "text": "DPSIR\n\nDPSIR (drivers, pressures, state, impact and response model of intervention) is a causal framework for describing the interactions between society and the environment: Human impact on the environment and vice versa because of the interdependence of the components.\n\nThis framework has been adopted by the European Environment Agency. The components of this model are:\n\nThis framework is an extension of the pressure-state-response model developed by OECD.\nAs a first step, data and information on all the different elements in the DPSIR chain is collected. Then possible connections between these different aspects are postulated. Through the use of the DPSIR modelling framework, it is possible to gauge the effectiveness of responses put into place.\n\n"}
{"id": "4389540", "url": "https://en.wikipedia.org/wiki?curid=4389540", "title": "Dalit Bahujan Shramik Union", "text": "Dalit Bahujan Shramik Union\n\nThe Dalit Bahujan Shramik Union (DBSU) is a Dalit-Bahujan civil rights organization locally coordinated from Gandhinagar, Hyderabad, Andhra Pradesh, South India.\n\nThe DBSU was previously known as the Andhra Pradesh Dalit Bahujan Vyavasaya Vruthidharula Union (APDBVVU). It was a conglomeration of various Dalit-Bahujan owned and managed community-based organisations across Andhra Pradesh. It is dedicated to Damodaram Sanjeevaiah (former Chief Minister of Andhra Pradesh and President of AICC) and Shyam Sundar. Its vision is \"equal opportunities for all to identity, security, livelihoods and future.\"\nTo defend and expand \"dalit bahujan\" (DB) right to land, livelihoods and self-respect through sustained public action ensuring equal participation of women.\nDBSU works in 16 districts of Andhra Pradesh with 550,000 DB families, establishing an efficient structure with equal participation of women. They possess legal ownership of 25,000 acres (100 km²) of land, while utilizing government programmes to further public safety and secure civil rights. In panchayati raj DBSU has become a viable political force, working so others can live with dignity.\n\n\nThe DBSU is headed by a State Committee, operating through a General Secretary and an Organising Secretary elected for a term of two years.\n\n\n\n\n\n"}
{"id": "739425", "url": "https://en.wikipedia.org/wiki?curid=739425", "title": "Daniel Spoerri", "text": "Daniel Spoerri\n\nDaniel Spoerri (born 27 March 1930 in Galați) is a Swiss artist and writer born in Romania. Spoerri is best known for his \"snare-pictures,\" a type of assemblage or object art, in which he captures a group of objects, such as the remains of meals eaten by individuals, including the plates, silverware and glasses, all of which are fixed to the table or board, which is then displayed on a wall. He also is widely acclaimed for his book, \"Topographie Anécdotée* du Hasard\" (\"An Anecdoted Topography of Chance\"), a literary analog to his snare-pictures, in which he mapped all the objects located on his table at a particular moment, describing each with his personal recollections evoked by the object. \n\nSpoerri was born Daniel Isaac Feinstein, on 27 March 1930, in Galați, Romania. Although his father, Isaac Feinstein, had converted to Christianity, after Romania entered the War on the side of Nazi Germany he was arrested and killed in 1941. His mother, born Lydia Spoerri, was Swiss and was therefore able to emigrate with her family of 6 children to Switzerland in 1942. There, he was adopted by his maternal uncle Professor Theophil Spoerri and registered as Daniel Spoerri, a name he has retained.\n\nIn the 1950s he was active in dance, studying classical dance with Olga Preobrajenska and in 1954 becoming the lead dancer at the State Opera of Bern, Switzerland. He later staged several avant-garde plays including Ionesco's \"The Bald Soprano\" and Picasso's surrealist \"Desire Trapped by the Tail.\" During that period he met a number of Surrealist artists, including Jean Tinguely, Marcel Duchamp and Man Ray, and also a number of artists subsequently associated with the Fluxus movement, including Robert Filliou, Dieter Roth and Emmett Williams. In the late 1950s, Spoerri married Vera Mertz.\n\nIn 1959 Spoerri founded Editions MAT (\"Multiplication d'art Transformable\"), a venture which produced and sold copies of three-dimensional constructed artworks by artists such as Marcel Duchamp, Dieter Roth, Jean Tinguely and Victor Vasarely. One of the best known works produced by Editions MAT was Man Ray's Indestructible Object. Spoerri is credited with coining the term \"multiples\" for such works.\n\nIn 1960, Spoerri made his first \"snare-picture\". Spoerri later explained snare-pictures as follows: \"objects found in chance positions, in order or disorder (on tables, in boxes, drawers, etc.) are fixed (‘snared’) as they are. Only the plane is changed: since the result is called a picture, what was horizontal becomes vertical. Example: remains of a meal are fixed to the table at which the meal was consumed and the table hung on the wall.\" His first \"snare-picture\", \"Kichka's Breakfast\" was created from his girlfriend's leftover breakfast. The piece is now in the collection in the Museum of Modern Art in New York. One snare-picture, made in 1964, consists of the remains of a meal eaten by Marcel Duchamp. This work holds the auction price record for Spoerri, selling for €136,312 ($200,580) in January 2008, distantly followed by another snare-picture from 1972, which sold for €44,181 ($69,860) in April 2008.\n\n In connection with a one-man show of his snare-pictures at the Galerie Lawrence in Paris in 1962, Spoerri wrote his \"Topographie Anécdotée* du Hasard\" (Anecdoted Topography of Chance). Spoerri was then living at the Hotel Carcassone in Paris, in room number 13 on the fifth floor. To the right of the entrance door was a table which his wife Vera had painted blue. Spoerri drew on a ‘map\" the overlapping outlines of all the 80 objects that were lying on the table on 17 October 1961 at exactly 3:47 p.m. Each object was assigned a number and Spoerri wrote a brief description of each object and the memories or associations it evoked. The descriptions cross referenced other objects on the table which were related. The \"Topographie Anécdotée* du Hasard\" was printed as a small pamphlet of 53 pages plus a fold out map and index and was distributed as an advertisement for the exhibit. The \"Topographie Anécdotée* du Hasard\" is more than just a catalog of random objects, however; read in its entirety, it provides a coherent and compelling picture of Spoerri's travels, friends and artistic endeavors.\n\n\"The Anecdoted Topography of Chance\" has been called a \"quasi-autobiographical \"tour de force\".\" In 1966, the Something Else Press in New York City published an English translation of the \"Topographie Anécdotée* du Hasard\" by Emmett Williams, entitled \"An Anecdoted Topography of Chance (Re-Anecdoted Version)\". Roland Topor added sketches of each object and additional annotations were added \"at random\" by Williams and others and by Spoerri himself. A number of appendices were added to the work and a greatly expanded index. The \"Topographie Anécdotée* du Hasard\" became a cult classic and was published in German translation by Dieter Roth in 1968 as \"Anekdoten zu einer Topographie des Zufalls\". Roth increased in fact the volume of the book by almost a third by adding his own poetic annotations. In 1990, the original French version was reprinted by the Centre Pompidou in Paris, and in 1995, an expanded English version was published by the Atlas Press in London, with additional material and annotations, and all of Dieter Roth's texts.\n\nSpoerri was one of the original signers of the manifesto creating the Nouveau réalisme (New Realism) art movement, an avant garde endeavor begun in 1960. His use of everyday life as the main subject-matter of his art reflects his involvement in the New Realism movement.\n\nSpoerri is also closely associated with the Fluxus art movement, a movement formed in the early 1960s, \"characterized by a strongly Dadaist attitude, [whose] participants were a divergent group of individualists whose most common theme was their delight in spontaneity and humor.\" It has been said that his \"Anecdoted Topography of Chance\" \"seems perfectly to embody aspects of its spirit.\" \n\nA major theme of Spoerri's artwork is food, and he has called this aspect of his work \"Eat Art.\" This is seen not only in his snare-pictures of eaten meals, but in a variety of other contexts. For example, in 1961 he sold in an art-gallery in Copenhagen store-bought canned food which he had signed and rubber-stamped \"Attention: Work of Art.\" In 1963, he enacted a sort of performance art called \"Restaurant de la Galerie J\" in Paris, for which he cooked on several evenings. Art-critics took over the role of waiters, playing on the idea of the critic bringing the art to the consumers and giving them an understanding of the work. On June 18, 1968, Spoerri opened the Restaurant Spoerri in Düsseldorf, and on September 18, 1970, he opened the Eat-Art-Gallery upstairs. He also published in 1970 a diary of his life on the Greek island of Symi, in which he included numerous recipes of the dishes he ate there. Originally titled \"A Gastronomic Itinerary\", it was later republished under the title \"Mythology & Meatballs\".\n\nSpoerri continued to make snare-pictures, including snares of eaten meals, into the 1990s. He also has created assemblage works, mounting objects on reproductions of 19th century medical illustrations as backgrounds. Spoerri has also produced serigraph and bronze versions of his works.\n\nSpoerri has led a nomadic life, living variously in Bern, Paris, the Greek island of Symi, Düsseldorf, Basel, Munich and Vienna. In 1997 he moved to the Tuscan town of Seggiano where he opened \"Il Giardino di Daniel Spoerri\" (the Garden of Daniel Spoerri), a sculpture garden, where works by a number of artists are displayed.\n\n\n\n"}
{"id": "753145", "url": "https://en.wikipedia.org/wiki?curid=753145", "title": "Del in cylindrical and spherical coordinates", "text": "Del in cylindrical and spherical coordinates\n\nThis is a list of some vector calculus formulae for working with common curvilinear coordinate systems.\n\n\nformula_12\nformula_13\n\nThe expressions for formula_14 and formula_15 are found in the same way.\n\nformula_16\n\nformula_17\n\nformula_18\n\nformula_19\n\nformula_20\n\nformula_21\n\nformula_22\n\nformula_23\n\nformula_24\n\nformula_25\n\nThe unit vector of a coordinate parameter u is defined in such a way that a small positive change in u causes the position vector formula_26 to change in formula_27 direction.\n\nTherefore, formula_28 where s is the arc length parameter.\n\nFor two sets of coordinate systems formula_29 and formula_30, according to chain rule, formula_31\n\nNow, let all of formula_32 but one and then divide both sides by the corresponding differential of that coordinate parameter, we find:\n\nformula_33\n\n\n"}
{"id": "20287991", "url": "https://en.wikipedia.org/wiki?curid=20287991", "title": "Divergent question", "text": "Divergent question\n\nA divergent question is a question with no specific answer, but rather exercises one's ability to think broadly about a certain topic.\n\nPopular in inquiry education, divergent questions allow students to explore different avenues and create many different variations and alternative answers or scenarios. Correctness may be based on logical projections, may be contextual, or arrived at through basic knowledge, conjecture, inference, projection, creation, intuition, or imagination. These types of questions often require students to analyze, synthesize, or evaluate a knowledge base and then project or predict different outcomes.\n\nA simple example of a divergent question is:\n\n\"Write down as many different uses as you can think of for the following objects: (1) a brick, (2) a blanket.\"\n"}
{"id": "268344", "url": "https://en.wikipedia.org/wiki?curid=268344", "title": "Efficiency", "text": "Efficiency\n\nEfficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result. In a more general sense, it is the ability to do things well, successfully, and without waste. In more mathematical or scientific terms, it is a measure of the extent to which input is well used for an intended task or function (output). It often specifically comprises the capability of a specific application of effort to produce a specific outcome with a minimum amount or quantity of waste, expense, or unnecessary effort. Efficiency refers to very different inputs and outputs in different fields and industries.\n\nEfficiency is very often confused with effectiveness. In general, efficiency is a measurable concept, quantitatively determined by the ratio of useful output to total input. Effectiveness is the simpler concept of being able to achieve a desired result, which can be expressed quantitatively but doesn't usually require more complicated mathematics than addition. Efficiency can often be expressed as a percentage of the result that could ideally be expected, for example if no energy were lost due to friction or other causes, in which case 100% of fuel or other input would be used to produce the desired result. This does not always apply, not even in all cases in which efficiency can be assigned a numerical value, e.g. not for specific impulse.\n\nA common but confusing way of distinguishing between efficiency and effectiveness is the saying \"Efficiency is doing things right, while effectiveness is doing the right things.\" This saying indirectly emphasizes that the selection of objectives of a production process is just as important as the quality of that process. This saying popular in business however obscures the more common sense of \"effectiveness\", which would/should produce the following mnemonic: \"Efficiency is doing things right; effectiveness is getting things done.\" This makes it clear that effectiveness, for example large production numbers, can also be achieved through inefficient processes if, for example, workers are willing or used to working longer hours or with greater physical effort than in other companies or countries or if they can be forced to do so. Similarly, a company can achieve effectiveness, for example large production numbers, through inefficient processes if it can afford to use more energy per product, for example if energy prices or labor costs or both are lower than for its competitors.\n\nFor example, one may measure how directly two objects are communicating: downloading music directly from a computer to a mobile device is more efficient than using a mobile device's microphone to record music sounds that come from a computer's speakers.\n\nEfficiency is often measured as the ratio of useful output to total input, which can be expressed with the mathematical formula \"r\"=\"P\"/\"C\", where \"P\" is the amount of useful output (\"product\") produced per the amount \"C\" (\"cost\") of resources consumed. This may correspond to a percentage if products and consumables are quantified in compatible units, and if consumables are transformed into products via a conservative process. For example, in the analysis of the energy conversion efficiency of heat engines in thermodynamics, the product \"P\" may be the amount of useful work output, while the consumable \"C\" is the amount of high-temperature heat input. Due to the conservation of energy, \"P\" can never be greater than \"C\", and so the efficiency \"r\" is never greater than 100% (and in fact must be even less at finite temperatures).\n\n\n\n\n\n"}
{"id": "39104546", "url": "https://en.wikipedia.org/wiki?curid=39104546", "title": "Evolutionary psychology of language", "text": "Evolutionary psychology of language\n\nEvolutionary psychology of language is the study of the evolutionary history of language as a psychological faculty within the discipline of evolutionary psychology. It makes the assumption that language is the result of a Darwinian adaptation.\n\nThere are many competing theories of how language might have evolved, if indeed it is an evolutionary adaptation. They stem from the belief that language development could result from an adaptation, an exaptation, or a by-product. Genetics also influence the study of the evolution of language. It has been speculated that the FOXP2 gene may be what gives humans the ability to develop grammar and syntax.\n\nIn the debate surrounding the evolutionary psychology of language, three sides emerge: those who believe in language as an adaptation, those who believe it is a by-product of another adaptation, and those who believe it is an exaptation.\n\nScientist and psychologists Steven Pinker and Paul Bloom argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop. The complexity of the mechanisms, the faculty of language and the ability to learn language provides a comparative resource between the psychological evolved traits and the physical evolved traits.\n\nPinker, though he mostly agrees with Noam Chomsky, a linguist and cognitive scientist, in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction, but Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.\n\nNoam Chomsky spearheaded the debate on the faculty of language as a cognitive by-product, or spandrel. As a linguist, rather than an evolutionary biologist, his theoretical emphasis was on the infinite capacity of speech and speaking: there are a fixed number of words, but there is an infinite combination of the words. His analysis from this considers that the ability of our cognition to perceive infinite possibilities, or create infinite possibilities, helped give way to the extreme complexity found in our language. Both Chomsky and Gould argue that the complexity of the brain is in itself an adaptation, and language arises from such complexities.\nOn the issue of whether language is best seen as having evolved as an adaptation or as a by product, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptationist view of evolution, and dismisses Pinker and Bloom's question of whether \"Language has evolved as an adaptation\" as being misleading.\nHe argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in \"The Symbolic Species\" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.\n\nExaptations, like adaptations, are fitness-enhancing characteristics, but, according to Stephen Jay Gould, their purposes were appropriated as the species evolved. This can be for one of two reasons: either the trait’s original function was no longer necessary so the trait took on a new purpose or a trait that does not arise for a certain purpose, but later becomes important. Typically exaptations have a specific shape and design which becomes the space for a new function. The foundation of this argument comes from the low-lying position of the larynx in humans. Other mammals have this same positioning of the larynx, but no other species has acquired language. This leads exaptationists to see an evolved modification away from its original purpose.\n\nResearch has shown that “genetic constraints” on language evolution could have caused a “specialized” and “species-specific language module. It is through this module that there are many specified “domain-specific linguistic properties,” such as syntax and agreement. Adaptationists believe that language genes “coevolved with human language itself for the purpose of communication.” This view suggests that the genes that are involved with language would only have coevolved in a very stable linguist environment. This shows that language could not have evolved in a rapidly changing environment because that type of environment would not have been stable enough for natural selection. Without natural selection, the genes would not have coevolved with the ability for language, and instead, would have come from “cultural conventions.” The adaptationist belief that genes coevolved with language also suggests that there are no “arbitrary properties of language.” This is because they would have coevolved with language through natural selection.\nThe Baldwin effect provides a possible explanation for how language characteristics that are learned over time could become encoded in genes. He suggested, like Darwin did, that organisms that can adapt a trait faster have a “selective advantage.” As generations pass, less environmental stimuli is needed for organisms of the species to develop that trait. Eventually no environmental stimuli are needed and it is at this point that the trait has become “genetically encoded.”\n\nThe genetic and cognitive components of language have long been under speculation, only recently have linguists been able to point out a gene that may possibly explain how language works. Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gopnik identified a dominant gene that causes language impairment in the KE family of Britain. The KE family has a mutation in the FOXP2, that makes them suffer from a speech and language disorder. It has been argued that the FOXP2 gene is the grammar gene, which is what allows humans the ability to form proper syntax and make our communication of higher quality. Children that grow up in a stable environment are able to develop highly proficient language without any instruction. Individuals with a mutation to their FOXP2 gene have trouble mastering complex sentences, and shows signs of developmental verbal dyspraxia.\n\nThis gene most likely evolved in the hominin line after the hominin and the chimpanzee lines split; this accounts for the fact that humans are the only ones able to learn and understand grammar. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. This suggests that speech evolved late in overall spectrum of human evolution.\n\nIn the world there are nearly 7000 languages, there is great amount of variation and this variation is thought to have come about through cultural differentiation. There are four factors that are thought to be the reason as to why there is language variation between cultures: founder effects, drift, hybridization and adaptation. With the vast amounts of lands available different tribes began to form and to claim their territory, in order to differentiate themselves many of these groups made changes to their language and this how the evolution of languages began. There also tended to be drifts in the population a certain group would get lost and be isolated from the rest of the group, this group would lose touch with the other groups and before they knew there had been mutations in their language and a whole new language had been formed.\n\nHybridization also played a big role in the language evolution, one group would come in contact with another tribe and they would pick up words and sounds from each other eventually leading to the formation of a new language. Adaptation would also play a role in the evolution of language differentiation, the environment and the circumstances were constantly changing therefore the groups had to adapt to the environment and their language had to adapt to it as well, it is all about maximizing fitness.\n\nAtkinson theorized that language may have originated in Africa since African languages have a greater variation of speech sounds than other languages. Those sounds are seen as the root for the other languages that exist across the world.\n\nResearch indicates that nonhuman animals (e.g., apes, dolphins, and songbirds) show evidence of language. Comparative studies of the sensory-motor system reveal that speech is not special to humans: nonhuman primates can discriminate between two different spoken languages. Anatomical aspects of humans, particularly the descended larynx, has been believed to be unique to humans' capacity to speak. However, further research revealed that several other mammals have a descended larynx beside humans, which indicates that a descended larynx must not be the only anatomical feature needed for speech production.\nVocal imitation is not uniquely human as well. Songbirds seem to acquire species-specific songs by imitating. Because nonhuman primates do not have a descended larynx, they lack vocal imitative capacity, which is why studies involving these primates have taught them nonverbal means of communication, e.g., sign language.\n\nKoko and Nim Chimpsky are two apes that have successfully learned to use sign language, but not to the extent that a human being can. Nim is a chimpanzee that was taken in by a family in the 1970s and was raised as if he were a human child. Nim was able to master 150 signs, which were limited but useful. Koko was a gorilla that was taken in by a Berkley student. She was able to master 600 signs for generative communication. Koko and Nim were not able to develop speech due to the fact that they lack the larynx which is what distinguishes humans from other animals and allows them to speak.\n\n"}
{"id": "44401868", "url": "https://en.wikipedia.org/wiki?curid=44401868", "title": "Ewa Partum", "text": "Ewa Partum\n\nEwa Partum (born 1945, Grodzisk Mazowiecki near Warsaw, Poland) is a poetry artist, performance artist, filmmaker, mail artist, and conceptual artist. Her involvement with feminist movements and themes includes the WACK! Art and the Feminist Revolution in Los Angeles in 2007. Her honors include an exhibition honoring female artists’ work at the Regional Contemporary Art Fund of Lorraine (Frac Lorraine) in Metz, France.\n\nBeginning in 1963, Partum studied at the State Higher School of Fine Arts in Łódź. She then went to the Academy of Fine Arts in Warsaw in 1965. She studied in the Department of Painting, where Partum was particularly interested in poetry as art, and she received her diploma in 1970. \n\nPartum engages in linguistic and performative play in an attempt to discover new artistic language. This search for a \"new language\" is rooted in her belief that painting has exhausted its potential to generate new or transformative ideas.\n\nPartum's work explores issues of female identity, including the gender bias of the art world. In interviews, she speaks about the difficulties and discrimination that she has faced as a female performance artist. This bias inspired her decision to perform naked in many pieces. At one point, Partum declared she would perform naked until women in the art world obtained equal rights. In interviews, she has spoken of the naked performances, saying \"In their interpretations, many critics concentrate on the use of my naked body in my actions without actually understanding that for me it was a matter of creating a sign: a sign pointing into one direction. It was a tautological and not, as so many are inclined to believe, an egocentric strategy... My naked body is merely a tool. I'm very focused, not making any spontaneous movements. When the performance is finished I bow towards my audience as a virtuoso after a concert...\"\n\nPartum's public work began in 1969 and continued through the seventies, integrating poetry and performance. In a performance called \"Change,\" in 1974, Partum had a makeup artist work on half of her naked body in front of an audience. She announced that she was a work of art, making her body an element of the feminist discourse. This can be used as an example of essentialism, in that art is about the woman’s body, and using the body as a canvas in feminist performance.\n\nAnother work, \"Self-Identification \"(1980)\",\" included photomontages of her naked figure superimposed against scenes of Warsaw, among pedestrians, next to a policewoman, and in front of the presidential place, showing the controversial nature of the naked female body. The piece's title speaks to the complicated and fraught search for a female identity in a world in which the female body is often observed as a source of shock or shame.\n\nPartum's work often focuses on the harm of traditional gender biases and expectations. For example, in \"Stupid Woman \"(1981)\" \"she performed exaggerated compliance to norms of female submissiveness and stupidity by \"kiss[ing] hands of members of the audience, pour[ing] alcohol over [her] head, writ[ing] with whipped cream on the floor 'sweet art' - in other words act[ing] like a drunk and stupid woman.\" In another work, \"Women, Marriage is Against You! \"(1980)\", \"she dressed in a wedding dress and wrapped herself in transparent foil, topped with a sign that said \"For Men.\" Then, she cut through the dress and the foil, emerging naked.\n\nSince 1971, she has been creating \"Poems By Ewa,\" conceptual poetry in the shape of poetic objects in which she often imprints her lips. Early conceptual-poetic pieces include Active Poetry, in which Partum used the wind to scatter cut-out letters in a variety of landscapes, using their random distribution to create poetry. Her later poems compose of a strong feminist and social tone, although it is never stated in a clear or obvious way to the viewer. In one of Partum’s performances during the period of martial law in 1982, she is naked and wearing red lipstick, and formed the word “Solidarnosc,” meaning solidarity, one by one with her lips on a piece of white paper. This expresses one of the themes of her work that includes parodying the manners in which women try to conform to men’s idealized expectations of them. In an exhibition on the subject of the Berlin Wall, she photographs herself naked in high-heels, and holding up a big letter “O” in her right hand and a “W” in her left, signifying Ost-West Schatten, or East-West shadows.\n\nPartum's films act both as documentation of her poetic and performative work and as explorations of film as a medium, in line with the structural cinema of the 1960s. For example, in \"Tautological Cinema (\"a series of short films made between 1973 and 1974), Partum examines the automated nature and materiality of film. The content of one piece, \"Ten Meters of Film, \"draws a simple correlation between film length and running time, denoting on the screen the number of meters of film that have run through the projector (up to ten). Another short film display the artist's face in a variety of scenes with at least one feature obscured. The film calls into question the ability of the artist to fully communicate an idea, arguing that once the work is observed by another, the authority of the artist is lost. Of the films and of film as a medium, Partum as said, \"It’s not dealing with aesthetics. It’s rather a new sort of philosophical practice that operates in the area in which the relation between the film image and the camerawork covers the whole interest in the film itself.”\n\nPartum also worked as the organizer and curator of a very influential mail art gallery from 1971 through 1977. The gallery, Galeria Address, displayed the work of Fluxus artists, and others. As part of the gallery, Partum published catalogs and books.\n\nThroughout the 1960s and 1970s, Partum's work was often affected by Polish censors, who refused to allows its distribution in publications.\nIn July 2013, the Frac Lorraine in Metz, Francep held an exhibition entitled, “Bad Girls: A Collection in Action,” honoring female artists who dedicated their work to deconstructing established order and creating room for freedom, experience, and life, changing the audience’s vision of the future and encouraging social change.\n\nSeveral retrospectives of Partum's work have occurred, including \"Ewa Partum: The Legality of Space \"at the Wyspa Institute of Art (2006). In interviews, Partum has celebrated the work of retrospective exhibitions, in that they allow the audience and critics to \"trace the genealogy of the artist's language.\"\n\nExhibitions of her work include:\n\nIn her early years of being an artists Ewa Partum became upset with the discrimination of her art for being and female artists so she began to use her naked body in her artworks. Even though Patrum knew she would receive even more discrimination for this she continued to do it because of the later message she knew it would send. \n"}
{"id": "14520306", "url": "https://en.wikipedia.org/wiki?curid=14520306", "title": "Gender inequality", "text": "Gender inequality\n\nGender inequality is the idea and situation that men and women are not equal. Gender inequality refers to unequal treatment or perceptions of individuals wholly or partly due to their gender. It arises from differences in gender roles. Gender systems are often dichotomous and hierarchical. Gender inequality stems from distinctions, whether empirically grounded or socially constructed. Women lag behind men in many domains, including education, labor market opportunities and political representation and in pay.\n\nNatural differences exist between the sexes base on biological and anatomic factors, most notably differing reproductive roles. Biological differences include chromosomes and hormonal differences. There is a natural difference also in the relative physical strengths (on average) of the sexes, both in the lower body and more pronouncedly in the upper-body, though this does not mean that any given man is stronger than any given woman. Men, on average, are taller, which provides both advantages and disadvantages. Women live significantly longer than men, though it is not clear to what extent this is a biological difference - see Life expectancy. Men have larger lung volumes and more circulating blood cells and clotting factors, while women have more circulating white blood cells and produce antibodies faster. Differences such as these are hypothesized to be an adaption allowing for sexual specialization.\n\nPrenatal hormone exposure influences to what extent one exhibits traditional masculine or feminine behavior. No differences between males and females exist in general intelligence. Men are significantly more likely to take risks than women. Men are also more likely than women to be aggressive, a trait influenced by prenatal and possibly current androgen exposure. It has been theorized that these differences combined with physical differences are an adaption representing sexual division of labor. A second theory proposes sex differences in intergroup aggression represent adaptions in male aggression to allow for territory, resource and mate acquisition. Females are (on average) more empathetic than males, though this does not mean that any given woman is more empathetic than any given man. Men and women have better visuospatial and verbal memory, respectively. These changes are influenced by the male sex hormone testosterone, which increases visuospatial memory in both genders when administered.\n\nFrom birth males and females are raised differently and experience different environments throughout their lives. In the eyes of society, gender has a huge role to play in many major milestones or characteristics in life; like personality. Males and females are lead on different paths before they are able to choose their own. The colour blue is most commonly associated with boys and they get toys like monster trucks or more sport related things to play with from the time that they are babies. Girls are more commonly introduced to the colour pink, dolls, dresses, and playing house where they are taking care of the dolls as if they were children. The norm of blue is for boys and pink is for girls is cultural and has not always historically been around. These paths set by parents or other adult figures in the child's life set them on certain paths. This leads to a difference in personality, career paths, or relationships. Throughout life males and females are seen as two very different species who have very different personalities and should stay on separate paths.\n\nThe gender pay gap is the average difference between men's and women's aggregate wages or salaries. The gap is due to a variety of factors, including differences in education choices, differences in preferred job and industry, differences in the types of positions held by men and women, differences in the type of jobs men typically go into as opposed to women (especially highly paid high risk jobs), differences in amount of work experiences, difference in length of the work week, and breaks in employment. These factors resolve 60% to 75% of the pay gap, depending on the source. Various explanations for the remaining 25% to 40% have been suggested, including women's lower willingness and ability to negotiate salary and sexual discrimination. According to the European Commission direct discrimination only explains a small part of gender wage differences.\n\nIn the United States, the \"average\" female's \"unadjusted\" annual salary has been cited as 78% of that of the \"average\" male. However, multiple studies from OECD, AAUW, and the US Department of Labor have found that pay rates between males and females varied by 5–6.6% or, females earning 94 cents to every dollar earned by their male counterparts, when wages were adjusted to different individual choices made by male and female workers in college major, occupation, working hours, and maternal/parental leave. The remaining 6% of the gap has been speculated to originate from deficiency in salary negotiating skills and sexual discrimination.\n\nHuman capital theories refer to the education, knowledge, training, experience, or skill of a person which makes them potentially valuable to an employer. This has historically been understood as a cause of the gendered wage gap but is no longer a predominant cause as women and men in certain occupations tend to have similar education levels or other credentials. Even when such characteristics of jobs and workers are controlled for, the presence of women within a certain occupation leads to lower wages. This earnings discrimination is considered to be a part of pollution theory. This theory suggests that jobs which are predominated by women offer lower wages than do jobs simply because of the presence of women within the occupation. As women enter an occupation, this reduces the amount of prestige associated with the job and men subsequently leave these occupations. The entering of women into specific occupations suggests that less competent workers have begun to be hired or that the occupation is becoming deskilled. Men are reluctant to enter female-dominated occupations because of this and similarly resist the entrance of women into male-dominated occupations. \n\nThe gendered income disparity can also be attributed in part to occupational segregation, where groups of people are distributed across occupations according to ascribed characteristics; in this case, gender. Occupational gender segregation can be understood to contain two components or dimensions; horizontal segregation and vertical segregation. With horizontal segregation, occupational sex segregation occurs as men and women are thought to possess different physical, emotional, and mental capabilities. These different capabilities make the genders vary in the types of jobs they are suited for. This can be specifically viewed with the gendered division between manual and non-manual labor. With vertical segregation, occupational sex segregation occurs as occupations are stratified according to the power, authority, income, and prestige associated with the occupation and women are excluded from holding such jobs.\n\nAs women entered the workforce in larger numbers since the 1960s, occupations have become segregated based on the amount femininity or masculinity presupposed to be associated with each occupation. Census data suggests that while some occupations have become more gender integrated (mail carriers, bartenders, bus drivers, and real estate agents), occupations including teachers, nurses, secretaries, and librarians have become female-dominated while occupations including architects, electrical engineers, and airplane pilots remain predominately male in composition. Based on the census data, women occupy the service sector jobs at higher rates than men. Women’s overrepresentation in service sector jobs, as opposed to jobs that require managerial work acts as a reinforcement of women and men into traditional gender roles that causes gender inequality.\n“The gender wage gap is an indicator of women’s earnings compared with men’s. It is figured by dividing the average annual earnings for women by the average annual earnings for men.” (Higgins et al., 2014) \nScholars disagree about how much of the male-female wage gap depends on factors such as experience, education, occupation, and other job-relevant characteristics. Sociologist Douglas Massey found that 41% remains unexplained, while CONSAD analysts found that these factors explain between 65.1 and 76.4 percent of the raw wage gap. CONSAD also noted that other factors such as benefits and overtime explain \"additional portions of the raw gender wage gap\".\n\nThe glass ceiling effect is also considered a possible contributor to the gender wage gap or income disparity. This effect suggests that gender provides significant disadvantages towards the top of job hierarchies which become worse as a person’s career goes on. The term glass ceiling implies that invisible or artificial barriers exist which prevent women from advancing within their jobs or receiving promotions. These barriers exist in spite of the achievements or qualifications of the women and still exist when other characteristics that are job-relevant such as experience, education, and abilities are controlled for. The inequality effects of the glass ceiling are more prevalent within higher-powered or higher income occupations, with fewer women holding these types of occupations. The glass ceiling effect also indicates the limited chances of women for income raises and promotion or advancement to more prestigious positions or jobs. As women are prevented by these artificial barriers, from either receiving job promotions or income raises, the effects of the inequality of the glass ceiling increase over the course of a woman’s career.\n\nStatistical discrimination is also cited as a cause for income disparities and gendered inequality in the workplace. Statistical discrimination indicates the likelihood of employers to deny women access to certain occupational tracks because women are more likely than men to leave their job or the labor force when they become married or pregnant. Women are instead given positions that dead-end or jobs that have very little mobility.\n\nIn Third World countries such as the Dominican Republic, female entrepreneurs are statistically more prone to failure in business. In the event of a business failure women often return to their domestic lifestyle despite the absence of income. On the other hand, men tend to search for other employment as the household is not a priority.\n\nThe gender earnings ratio suggests that there has been an increase in women’s earnings comparative to men. Men’s plateau in earnings began after the 1970s, allowing for the increase in women’s wages to close the ratio between incomes. Despite the smaller ratio between men and women’s wages, disparity still exists. Census data suggests that women’s earnings are 71 percent of men's earnings in 1999.\n\nThe gendered wage gap varies in its width among different races. Whites comparatively have the greatest wage gap between the genders. With whites, women earn 78% of the wages that white men do. With African Americans, women earn 90% of the wages that African American men do.\n\nThere are some exceptions where women earn more than men: According to a survey on gender pay inequality by the International Trade Union Confederation, female workers in the Gulf state of Bahrain earn 40 percent more than male workers.\n\nIn 2014, a report by the International Labor Organization (ILO) reveals the wage gap between Cambodian women factory workers and other male counterparts. There was a $25 USD monthly pay difference conveying that women have a much lower power and being devalued not only at home but also in the workplace.\n\nThe gender gap also appeared to narrow considerably beginning in the mid-1960s. Where some 5% of first-year students in professional programs were female in 1965, by 1985 this number had jumped to 40% in law and medicine, and over 30% in dentistry and business school. Before the highly effective birth control pill was available, women planning professional careers, which required a long-term, expensive commitment, had to \"pay the penalty of abstinence or cope with considerable uncertainty regarding pregnancy.\" This control over their reproductive decisions allowed women to more easily make long-term decisions about their education and professional opportunities. Women are highly underrepresented on boards of directors and in senior positions in the private sector.\n\nAdditionally, with reliable birth control, young men and women had more reason to delay marriage. This meant that the marriage market available to any women who \"delay[ed] marriage to pursue a career... would not be as depleted. Thus the Pill \"could\" have influenced women's careers, college majors, professional degrees, and the age at marriage.\"\n\nStudies on sexism in science and technology fields have produced conflicting results. Corinne et al. found that science faculty of both sexes rated a male applicant as significantly more competent and hireable than an identical female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. Williams and Ceci, however, found that science and technology faculty of both sexes \"preferred female applicants 2:1 over identically qualified males with matching lifestyles\" for tenure-track positions. Studies show parents are more likely to expect their sons, rather than their daughters, to work in a science, technology, engineering or mathematics field – even when their 15-year-old boys and girls perform at the same level in mathematics.\n\nA survey by the U.K. Office for National Statistics in 2016 showed that in the health sector 56% of roles are held by women, while in teaching it is 68%. However equality is less evident in other area; only 30% of M.P.'s are women and only 32% of finance and investment analysts. In the natural and social sciences 43% of employees are women, and in the environmental sector 42%.\n\nA 2010 study conducted by David R. Hekman and colleagues found that customers who viewed videos featuring a black male, a white female, or a white male actor playing the role of an employee helping a customer were 19 percent more satisfied with the white male employee's performance.\n\nThis discrepancy with race can be found as early as 1947, when Kenneth Clark conducted a study in which black children were asked to choose between white and black dolls. White male dolls were the ones children preferred to play with.\n\nAlthough the disparities between men and women are decreasing in the medical field, gender inequalities still exist as social problems. From 1999 to 2008, recently qualified female doctors in the US made almost $170,000,000 less than their male counterparts. The pay discrepancy could not be explained by specialty choice, practice setting, work hours, or other characteristics. A case study carried out on Swedish medical doctors showed that the gender wage gap among physicians was greater in 2007 than in 1975.\n\nGender roles are heavily influenced by biology, with male-female play styles correlating with sex hormones, sexual orientation, aggressive traits, and pain. Furthermore, females with congenital adrenal hyperplasia demonstrate increased masculinity and it has been shown that rhesus macaque children exhibit preferences for stereotypically male and female toys.\n\nGender equality in relationships has been growing over the years but for the majority of relationships, the power lies with the male. Even now men and women present themselves as divided along gender lines. A study done by Szymanowicz and Furnham, looked at the cultural stereotypes of intelligence in men and women, showing the gender inequality in self-presentation. This study showed that females thought if they revealed their intelligence to a potential partner, then it would diminish their chance with him. Men however would much more readily discuss their own intelligence with a potential partner. Also, women are aware of people’s negative reactions to IQ, so they limit its disclosure to only trusted friends. Females would disclose IQ more often than men with the expectation that a real true friend would respond in a positive way. Intelligence continues to be viewed as a more masculine trait, than feminine trait. The article suggested that men might think women with a high IQ would lack traits that were desirable in a mate such as warmth, nurturance, sensitivity, or kindness. Another discovery was that females thought that friends should be told about one’s IQ more so than males. However, males expressed doubts about the test’s reliability and the importance of IQ in real life more so than women. The inequality is highlighted when a couple starts to decide who is in charge of family issues and who is primarily responsible for earning income. For example, in Londa Schiebinger’s book, \"Has Feminism Changed Science?\", she claims that \"Married men with families on average earn more money, live longer and happier, and progress faster in their careers,\" while \"for a working woman, a family is a liability, extra baggage threatening to drag down her career.\" Furthermore, statistics had shown that \"only 17 percent of the women who are full professors of engineering have children, while 82 percent of the men do.\"\n\nDespite the increase in women in the labour force since the mid-1900s, traditional gender roles are still prevalent in American society. Women may be expected to put their educational and career goals on hold in order to raise children, while their husbands work. However, women who choose to work as well as fulfill a perceived gender role of cleaning the house and taking care of the children. Despite the fact that different households may divide chores more evenly, there is evidence that supports that women have retained the primary caregiver role within familial life despite contributions economically. This evidence suggest that women who work outside the home often put an extra 18 hours a week doing household or childcare related chores as opposed to men who average 12 minutes a day in childcare activities. One study by van Hooff showed that modern couples, do not necessarily purposefully divide things like household chores along gender lines, but instead may rationalize it and make excuses. One excuse used is that women are more competent at household chores and have more motivation to do them. Another is that some say the demands of the males’ jobs is higher.\n\nThere was a study conducted at an \"urban comprehensive school\". They were asked questions regarding their views in sexual inequality. Many parents were for the equal pay for men and women. They also were in favor for men to help with the housework. In this study, the majority of the people who were interviewed wanted gender equality and more people wants a change in gender roles. Where men stay home, cleans, and cooks while the women can work and help support the family.\n\nGender roles have changed drastically over the past few decades. In the article, it says that in 1920-1966, there was data recorded that women spent the most time care-tending with the home and family. There was a study made with the gender roles with the males and females, The results showed that as women spend less time in the house, men have taken over the role as the mother. The article also said that women who work spend less time within the house and with their children if they have any. Furthermore, men are taking the roles of women in the homes and its changing as time goes on. Robin A. Douthitt, the author of the article, \"The Division of Labor Within the Home: Have Gender Roles Changed?\" concluded by saying, \"(1) men do not spend significnatly more time with chil- dren when their wives are employed and (2) employed women spend signifi- cantly less time in child care than their full-time homemaker counterparts, over a 10-year period both mothers and fathers are spending more total time with children.\" (703).\n\nOne survey showed that men rate their technological skills in activities such as basic computer functions and online participatory communication higher than women. However, it should be noted that this study was a self-reporting study, where men evaluate themselves on their own perceived capabilities. It thus is not data based on actual ability, but merely perceived ability, as participants' ability was not assessed. Additionally, this study is inevitably subject to the significant bias associated with self-reported data.\n\nIn contrary to such findings, a carefully controlled study that analyzed data sets from 25 developing countries led to the consistent finding that the reason why fewer women access and use digital technology is a direct result of their unfavorable conditions and ongoing discrimination with respect to employment, education and income. When controlling for these variables, women turn out to be more active users of digital tools than men. This turns the alleged digital gender divide into an opportunity: given women's affinity for ICT, and given that digital technologies are tools that can improve living conditions, ICT represents a concrete and tangible opportunity to tackle longstanding challenges of gender inequalities in developing countries, including access to employment, income, education and health services.\n\nMany countries have laws that give less inheritance of ancestral property for women compared to men.\n\nGender inequalities often stem from social structures that have institutionalized conceptions of gender differences.\n\nMarginalization occurs on an individual level when someone feels as if they are on the fringes or margins of their respective society. This is a social process and displays how current policies in place can affect people. For example, media advertisements display young girls with easy bake ovens (promoting being a housewife) as well as with dolls that they can feed and change the diaper of (promoting being a mother).\n\nCultural stereotypes, which can dictate specific roles, are engrained in both men and women and these stereotypes are a possible explanation for gender inequality and the resulting gendered wage disparity. Women have traditionally been viewed as being caring and nurturing and are designated to occupations which require such skills. While these skills are culturally valued, they were typically associated with domesticity, so occupations requiring these same skills are not economically valued. Men have traditionally been viewed as the main worker in the home, so jobs held by men have been historically economically valued and occupations predominated by men continue to be economically valued and earn higher wages.\n\nGender Stereotypes influenced greatly by gender expectations, different expectations on gender influence how people determine their roles, appearance, behaviors, etc. When expectations of gender roles deeply rooted in people's mind, people' values and ideas started to be influenced and leading to situation of stereotypes, which actualize their ideas into actions and perform different standards labelling the behaviors of people. Gender stereotypes limit opportunities of different gender when their performance or abilities were standardizing according to their gender-at-birth, that women and men may encounter limitations and difficulties when challenging the society through performing behaviors that their gender \"not supposed\" to perform. For example, men may receive judgments when they trying to stay at home and finish housework and allow their wives to go out and work instead, as men are expected to be work outside for earning money for the family. The traditional concepts of gender stereotypes are being challenged nowadays in different societies and improvement could be observed that men could also be responsible for housework, women could also be construction worker in some societies. It is still a long process when traditional concepts and values have deep-rooted in people's mind, that higher acceptance towards gender roles and characteristics is homely to be gradually developed.\n\nBonnie Spanier coined the term hereditary inequality. Her opinion is that some scientific publications depict human fertilization such that sperms seem to actively compete for the \"passive\" egg, even though in reality it is complicated (e.g. the egg has specific active membrane proteins that select sperm etc.)\n\nGender inequality can further be understood through the mechanisms of sexism. Discrimination takes place in this manner as men and women are subject to prejudicial treatment on the basis of gender alone. Sexism occurs when men and women are framed within two dimensions of social cognition.\n\nDiscrimination also plays out with networking and in preferential treatment within the economic market. Men typically occupy positions of power within the job economy. Due to taste or preference for other men because they share similar characteristics, men in these positions of power are more likely to hire or promote other men, thus discriminating against women.\n\nSonja B. Starr conducted a study in the US that found that the prison sentences that men serve are on average 63% longer than those that women serve when controlling for arrest offense and criminal history. However, the study does not purport to explain why this is the case. Starr does not believe that men are disadvantaged generally. Men's rights advocates have argued that men being over-represented in both those who commit murder and the victims of murder is evidence that men are being harmed by outmoded cultural attitudes.\n\nThe New York Film Academy took a closer look at the women in Hollywood and gathered statistics from the top 500 films from 2007 to 2012, for their history and achievements, or lack of.\n\nThere was a 5:1 ratio of men to women working in films. 30.8% of women having speaking characters, who may or may not have been a part of the 28.8% of women who were written to wear revealing clothing compared to the 7% of men who did, or the 26.2% of women who wore little to no clothing opposed to the 9.4% of men who did the same. A study analyzing five years of text from over 2,000 news sources found a similar 5:1 ratio of male to female names overall, and 3:1 for names in entertainment.\n\nHollywood actresses are paid less than actors. Topping \"Forbes\" highest paid actors list of 2013 was Robert Downey Jr. with $75 million, yet Angelina Jolie topped the highest paid actresses list with only $33 million, which tied with Denzel Washington ($33 million) and Liam Neeson ($32 million), who were the last two on the top ten highest paid actors list.\n\nIn the 2013 Academy Awards, 140 men were nominated for an award, but only 35 women were nominated. No woman was nominated for directing, cinematography, film editing, writing (original screenplay), or original score that year. Since the Academy Awards began in 1929, only seven women producers have won the Best Picture category (all of whom were co-producers with men), and only eight women have been nominated for Best Original Screenplay. Lina Wertmuller (1976), Jane Campion (1994), Sofia Coppola (2004), and Kathryn Bigelow (2012) were the only four women to be nominated for Best Director, with Bigelow being the first woman to win for her film \"The Hurt Locker\". 77% of the Academy Awards' voters are male.\n\nA group of Hollywood actors have launched their own social movement titled #AskMoreOfHim. This movement is built on the basis of men speaking out against sexual misconduct against females.   A number of male activists, specifically in the film industry, have signed an open letter explaining their responsibility in the ownership of their actions, as well as calling out the actions of others. The letter has been signed and supported by Friends actor David Schwimmer, shown above, among many others. The Hollywood Reporter published their support saying, “We applaud the courage and pledge our support to the courageous women — and men, and gender non-conforming individuals — who have come forward to recount their experiences of harassment, abuse and violence at the hands of men in our country. As men, we have a special responsibility to prevent abuse from happening in the first place... After all, the vast majority of sexual harassment, abuse and violence is perpetrated by men, whether in Hollywood or not.” This accountability is set to change the way women are seen and treated in the film and television industry, hopefully ending in the closing of the gap women are experiencing in pay, promotion, and overall respect. This initiative was created in response to the #MeToo movement. The #MeToo movement, started by a single tweet, asked women to share their stories of sexual assault against men in a professional setting. Within one day, 30,000 women had used the hashtag sharing their stories. Many women feel as if they have more power in their voices than they ever had and are choosing to make personal claims that may have been brushed under the rug prior to the internet culture we’re now living in. According to Time Magazine, 95% of women in the film and entertainment industry admit to being sexually harassed by men in their industry. In addition to the #MeToo movement, women in industry are using #TimesUp, with the goal of aiming to help prevent sexual harassment in the workplace for victims who cannot afford their own resources. \n\nGender inequality and discrimination are argued to cause and perpetuate poverty and vulnerability in society as a whole. Household and intra-household knowledge and resources are key influences in individuals' abilities to take advantage of external livelihood opportunities or respond appropriately to threats. High education levels and social integration significantly improve the productivity of all members of the household and improve equity throughout society. Gender Equity Indices seek to provide the tools to demonstrate this feature of poverty.\n\nPoverty has many different factors, one of which is the gender wage gap. Women are more likely to be living in poverty and the wage gap is one of the causes.\n\nThere are many difficulties in creating a comprehensive response. It is argued that the Millennium Development Goals (MDGs) fail to acknowledge gender inequality as a cross-cutting issue. Gender is mentioned in MDG3 and MDG5: MDG3 measures gender parity in education, the share of women in wage employment and the proportion women in national legislatures. MDG5 focuses on maternal mortality and on universal access to reproductive health. These targets are significantly off-track.\n\nAddressing gender inequality through social protection programmes designed to increase equity would be an effective way of reducing gender inequality, according to the Overseas Development Institute (ODI). Researchers at the ODI argue for the need to develop the following in social protection in order to reduce gender inequality and increase growth:\nThe ODI maintains that society limits governments' ability to act on economic incentives.\n\nNGOs tend to protect women against gender inequality and structural violence.\n\nDuring war, combatants primarily target men. Both sexes die however, due to disease, malnutrition and incidental crime and violence, as well as the battlefield injuries which predominately affect men. A 2009 review of papers and data covering war related deaths disaggregated by gender concluded \"It appears to be difficult to say whether more men or women die from conflict conditions overall.\" The ratio also depends on the type of war, for example in the Falklands War 904 of the 907 dead were men. Conversely figures for war deaths in 1990, almost all relating to civil war, gave ratios in the order of 1.3 males per female.\n\nAnother opportunity to tackle gender inequality is presented by modern information and communication technologies. In a carefully controlled study, it has been shown that women embrace digital technology more than men. Given that digital information and communication technologies have the potential to provide access to employment, education, income, health services, participation, protection, and safety, among others (ICT4D), the natural affinity of women with these new communication tools provide women with a tangible bootstrapping opportunity to tackle social discrimination.\n\nGender inequality is a result of the persistent discrimination of one group of people based upon gender and it manifests itself differently according to race, culture, politics, country, and economic situation. It is furthermore considered a causal factor of violence against women. While gender discrimination happens to both men and women in individual situations, discrimination against women is an entrenched, global pandemic. In the Democratic Republic of the Congo, rape and violence against women and girls is used as a tool of war. In Afghanistan, girls have had acid thrown in their faces for attending school. Considerable focus has been given to the issue of gender inequality at the international level by organizations such as the United Nations (UN), the Organisation for Economic Co-operation and Development (OECD), and the World Bank, particularly in developing countries. The causes and effects of gender inequality vary geographically, as do methods for combating it.\n\nOne example of the continued existence of gender inequality in Asia is the \"missing girls\" phenomenon. \"Many families desire male children in order to ensure an extra source of income. In China, females are perceived as less valuable for labor and unable to provide sustenance.\" Moreover, gender inequality also reflected in educational aspect in rural China . Gender inequality existed because of gender stereotypes in rural China, families may consider that is useless for girls to acquire knowledge at school because they will marry someone one day and their major responsibility is to take care of housework. When people have expectations on the gender roles, that considering marriage is the major goals of a girl's life in rural China, gender inequality easily existed to limit the rights and opportunities of women.\n\nA Cambodian said, \"Men are gold, women are white cloth\", emphasizing that women had a lower value and importance compared to men. In Cambodia, approximately 15% (485,000 hectares) of land was owned by women. In Asian culture, there is a stereotype that women usually have lower status than men because males carry on the family name and hold the responsibilities to take care of the family. Females have a less important role, mainly to carry out domestic chores, and taking care of husbands and children. Women are also the main victims of poverty as they have little or no access to education, low pay and low chances owning assets such as lands, homes or even basic items.\n\nIn Cambodia, the Ministry of Women's Affairs (MoWA) was formed in 1998 with the role of improving women's overall power and status in the country.\n\nThe Global Gender Gap Report put out by the World Economic Forum (WEF) in 2013 ranks nations on a scale of 0 to 1, with a score of 1.0 indicating full gender equality. A nation with 35 women and 65 men in political office would get a score of 0.538 as the WEF is measuring the gap between the two figures and not the actual percentage of women in a given category. While Europe holds the top four spots for gender equality, with Iceland, Finland, Norway and Sweden ranking 1st through 4th respectively, it also contains two nations ranked in the bottom 30 countries, Albania at 108 and Turkey at 120. The Nordic Countries, for several years, have been at the forefront of bridging the gap in gender inequality. Every Nordic country, aside from Denmark who is at 0.778, has reached above a 0.800 score. In contrast to the Nordic nations, the countries of Albania and Turkey continue to struggle with gender inequality. Albania and Turkey failed to break the top 100 nations in 2 of 4 and 3 of 4 factors, respectively. However, despite the disparity, European nations continue to make advances in the many factors that are used to determine a nation's gender gap score.\n\nWestern Europe, a region most often described as comprising the non-communist members of post-WWII Europe, has, for the most part been doing well in eliminating the gender gap. Western Europe holds 12 of the top 20 spots on the Global Gender Gap Report for overall score. While remaining mostly in the top 50 nations, four Western European nations fall below that benchmark. Portugal is just outside of the top 50 at number 51 with score of 0.706 while Italy (71), Greece (81) and Malta (84) received scores of 0.689, 0.678 and 0.676, respectively.\n\nA large portion of Eastern Europe, a region most often described as the former communist members of post-WWII Europe, resides between 40th and 100th place in the Global Gender Gap Report. A few outlier countries include Lithuania, which jumped nine places (37th to 28th) from 2011 to 2013, Latvia, which has held the 12th spot for two consecutive years, Albania and Turkey.\n\nIndia ranking remains low in gender equality measures by the World Economic Forum, although the rank has been improving in recent years. When broken down into components that contribute the rank, India performs well on political empowerment, but is scored near the bottom with China on sex selective abortion. India also scores poorly on overall female to male literacy and health rankings. India with a 2013 ranking of 101 out of 136 countries had an overall score of 0.6551, while Iceland, the nation that topped the list, had an overall score of 0.8731 (no gender gap would yield a score of 1.0). Gender inequalities impact India's sex ratio, women's health over their lifetimes, their educational attainment, and economic conditions. It is a multifaceted issue that concerns men and women alike.\n\nThe labor force participation rate of women was 80.7% in 2013. Nancy Lockwood of the Society for Human Resource Management, the world's largest human resources association with members in 140 countries, in a 2009 report wrote that female labor participation is lower than men, but has been rapidly increasing since the 1990s. Out of India's 397 million workers in 2001, 124 million were women, states Lockwood.\n\nIndia is on target to meet its Millennium Development Goal of gender parity in education before 2016. UNICEF's measures of attendance rate and Gender Equality in Education Index (GEEI) attempt to capture the quality of education. Despite some gains, India needs to triple its rate of improvement to reach GEEI score of 95% by 2015 under the Millennium Development Goals. A 1998 report stated that rural India girls continue to be less educated than the boys.\n\nThe World Economic Forum measures gender equity through a series of economic, educational, and political benchmarks. It has ranked the United States as 19th (up from 31st in 2009) in terms of achieving gender equity. The US Department of Labor has indicated that in 2009, \"the median weekly earnings of women who were full-time wage and salary workers was... 80 percent of men's\". The Department of Justice found that in 2009, \"the percentage of female victims (26%) of intimate partner violence was about 5 times that of male victims (5%)\". \"The United States ranks 41st in a ranking of 184 countries on maternal deaths during pregnancy and childbirth, below all other industrialized nations and a number of developing countries\" and women only represent 20% of members of Congress.\n\nExisting research on the topic of gender/sex and politics has found differences in political affiliation, beliefs, and voting behavior between men and women, although these differences vary across cultures. Gender is omnipresent in every culture, and while there are many factors to consider when labeling people \"Democrat\" or \"Republican\"—such as race and religion—gender is especially prominent in politics. Studying gender and political behavior poses challenges, as it can be difficult to determine if men and women actually differ in substantial ways in their political views and voting behavior, or if biases and stereotypes about gender cause people to make assumptions. However, trends in voting behavior among men and women have been proven through research.\n\nResearch shows that women in postindustrial countries like the United States, Canada, and Germany primarily identified as conservative before the 1960s; however, as time has progressed and new waves of feminism have occurred, women have become more left-wing due to shared beliefs and values between women and parties more on the left. Women in these countries typically oppose war and the death penalty, favor gun control, support environment protection, and are more supportive of programs that help people of lower socioeconomic statuses. Voting behaviors of men have not experienced as drastic of a shift over the last fifty years as women in their voting behavior and political affiliations. These behaviors tend to consistently be more conservative than women overall. These trends change with every generation, and factors such as culture, race, and religion also must be considered when discussing political affiliation. These factors make the connection between gender and political affiliation complex due to intersectionality.\n\nCandidate gender also plays a role in voting behavior. Women candidates are far more likely than male candidates to be scrutinized and have their competence questioned by both men and women when they are seeking information on candidates in the beginning stages of election campaigns. Democrat male voters tend to seek more information about female Democrat candidates over male Democrat candidates. Female Republican voters tend to seek more information about female Republican candidates. For this reason, female candidates in either party typically need to work harder to prove themselves competent more than their male counterparts.\n\nOverall, politics in the United States are dominated by men, which can pose many challenges to women who decide to enter the political sphere. As the number of women participants in politics continue to increase around the world, the gender of female candidates serves as both a benefit and a hindrance within their campaign themes and advertising practices. The overarching challenge seems to be that—no matter their actions—women are unable to win in the political sphere as different standards are used to judge them when compared to their male counterparts.\n\nOne area in particular that exemplifies varying perceptions between male and female candidates is the way female candidates decide to dress and how their choice is evaluated. When women decide to dress more masculine, they are perceived as being \"conspicuous.\" When they decide to dress more feminine, they are perceived as \"deficient.\" At the same time, however, women in politics are generally expected to adhere to the masculine standard, thereby validating the idea that gender is binary and that power is associated with masculinity. As illustrated by the points above, these simultaneous, mixed messages create a \"double-bind\" for women. Some scholars go on to claim that this masculine standard represents symbolic violence against women in politics.\n\nPolitical knowledge is a second area where male and female candidates are evaluated differently and where political science research has consistently shown women with a lower level of knowledge than their male counterparts. One reason for this finding is the argument that there are different areas of political knowledge that different groups consider. Due to this line of thought, scholars are advocating the replacement of traditional political knowledge with gender-relevant political knowledge because women are not as politically disadvantaged as it may appear.\n\nA third area that affects women's engagement in politics is their low level of political interest and perception of politics as a \"men's game.\" Despite female candidates' political contributions being equal to that of male candidates, research has shown that women perceive more barriers to office in the form of rigorous campaigns, less overall recruitment, inability to balance office and family commitments, hesitancy to enter competitive environments, and a general lack of belief in their own merit and competence. Male candidates are evaluated most heavily on their achievements, while female candidates are evaluated on their appearance, voice, verbal dexterity, and facial features in addition to their achievements.\n\nSeveral forms of action have been taken to combat institutionalized sexism. People are beginning to speak up or \"talk back\" in a constructive way to expose gender inequality in politics, as well as gender inequality and under-representation in other institutions. Researchers who have delved into the topic of institutionalized sexism in politics have introduced the term \"undoing gender.\" This term focuses on education and an overarching understanding of gender by encouraging \"social interactions that reduce gender difference.\" Some feminists argue that \"undoing gender\" is problematic because it is context-dependent and may actually reinforce gender. For this reason, researchers suggest \"doing gender differently\" by dismantling gender norms and expectations in politics, but this can also depend on culture and level of government (e.g. local versus federal).\n\nAnother key to combating institutionalized sexism in politics is to diffuse gender norms through \"gender-balanced decision-making,\" particularly at the international level, which \"establishes expectations about appropriate levels of women in decision-making positions.\" In conjunction with this solution, scholars have started placing emphasis on \"the value of the individual and the importance of capturing individual experience.\" This is done throughout a candidate's political career—whether that candidate is male or female—instead of the collective male or female candidate experience. Five recommended areas of further study for examining the role of gender in U.S. political participation are (1) realizing the \"intersection between gender and perceptions\"; (2) investigating the influence of \"local electoral politics\"; (3) examining \"gender socialization\"; (4) discerning the connection \"between gender and political conservatism\"; and (5) recognizing the influence of female political role models in recent years. Due to the fact that gender is intricately entwined in every societal institution, gender in politics can only change once gender norms in other institutions change, as well.\n\n\nHiggins, M. and Reagan, M. (n.d). The gender wage gap, 9th ed. North Mankato: Abdo Publishing, pp. 9–11\n"}
{"id": "1725625", "url": "https://en.wikipedia.org/wiki?curid=1725625", "title": "Handschu agreement", "text": "Handschu agreement\n\nThe Handschu agreement is a set of guidelines that regulate police behavior in New York City with regard to political activity.\n\nIn 1971, 21 members of the Black Panther Party were tried for conspiracy to blow up police stations and department stores. They were acquitted of all charges after only 90 minutes of jury deliberation. The trial revealed the extent to which the NYPD had infiltrated and kept dossiers on not only the Black Panthers and other radical groups, but also on anti-war groups, gay rights activists, educational reform advocates, religious groups, and civic organizations.\n\nThe Handschu agreement, or decree, was the result of a class-action lawsuit filed against the City of New York, its Police Commissioner and the Intelligence Division of the New York City Police Department (NYPD) on behalf of Barbara Handschu and fifteen other plaintiffs affiliated with various political or ideological associations and organizations, known as \"Handschu v. Special Services Division\", 605 F.Supp. 1384, affirmed 787 F.2d 828. The plaintiffs claimed that \"informers and infiltrators provoked, solicited and induced members of lawful political and social groups to engage in unlawful activities\"; that files were maintained with respect to \"persons, places, and activities entirely unrelated to legitimate law enforcement purposes, such as those attending meetings of lawful organizations\"; and that information from these files was made available to academic institutions, prospective employers, licensing agencies and others. In addition, plaintiffs protested seven types of police misconduct: (1) the use of informers; (2) infiltration; (3) interrogation; (4) overt surveillance; (5) summary punishment; (6) intelligence gathering; and (7) electronic surveillance, and alleged that these police practices which punished and repressed lawful dissent had had a \"chilling effect\" upon the exercise of freedom of speech, assembly and association, that they violated constitutional prohibitions against unreasonable searches and seizures, and that they abridged rights of privacy and due process.\n\nIn 1985, the court found that police surveillance of political activity violated constitutional protections of free speech. This ruling resulted in a consent decree which prohibited the NYPD from engaging \"in any investigation of political activity except through the … Intelligence Division [of the Police Department]\" and required that any \"such investigations shall be conducted\" only in accordance with the Guidelines incorporated into the Decree. The Guidelines further prohibited the Intelligence Division from \"commencing an investigation\" into the political, ideological or religious activities of an individual or group unless \"specific information has been received by the Police Department that a person or group engaged in political activity is engaged in, about to engage in or has threatened to engage in conduct which constitutes a crime….\"\n\nAccording to the terms of the agreement, purely political activity can only be investigated by the Public Security Section (PSS) of the NYPD's Intelligence Division, and then only when the Section suspects criminal activity. When the PSS does suspect criminal activity on the part of political groups, it must obtain a warrant from the three-person Handschu Authority, a commission made up of two deputy commissioners and a mayor-appointed civilian.\n\nThe agreement also prohibits indiscriminate police videorecording and photographing of public gatherings when there is no indication that unlawful activity is occurring.\n\nThe department is also prohibited from sharing information pertaining to political activity with other law enforcement agencies unless those agencies agree to abide by the terms of the Handschu agreement.\n\nThe court order mandates the compiling of annual, publicly available reports listing the surveillance requests made by the NYPD and the number of such requests granted.\n\nIn 2002, the NYPD asked federal judge Charles S. Haight, Jr., the judge who presided over the original case, to abrogate numerous provisions of the agreement, claiming that they inhibited the department's ability to prevent future terrorist attacks. The requests, if granted, would allow any branch of the department to investigate any political activity, even without suspicion of criminal activity, but only by accessing publicly available meetings and information \"as members of the public.\" It would further allow the department to perform any type of surveillance of public gatherings deemed Constitutional, which is likely to include videorecording. The department also requested that the function of the Handschu authority be changed to investigating complaints of constitutional violations, which would take away the authority's power to oversee political investigations.\n\nIn regards to the requested changes, Police Commissioner Raymond W. Kelly said, \"Today we live in a more dangerous, constantly changing world, one with challenges and threats that were never envisioned when the Handschu guidelines were written.\"\n\nChris Dunn of the New York Civil Liberties Union, called the proposal \"troubling,\" stating that the NYPD \"has no legitimate reason to spy on lawful political activity.\"\n\nFormer New York City mayor Ed Koch, previously a critic of police surveillance, agreed with the proposal to rescind large portions of the Handschu agreement. \"That's the necessary cost of protecting the public in these times where we're dealing with terrorism,\" he said.\n\nOn February 11, 2003, Haight ruled that the NYPD should be permitted to modify the 1985 Decree, but not to the extent originally requested by the City. The judge instructed the city to adapt the U.S. Attorney General's Guidelines on General Crimes, Racketeering Enterprise and Terrorism Enterprise Investigations, issued by Attorney General John Ashcroft in May, 2002. This weakened the original decree in that a preliminary inquiry could be initiated when there is \"information … which indicates the possibility of criminal activity.\"\n\nIn February 2007, Judge Haight ruled that the NYPD had violated the terms of the agreement by videotaping two demonstrations by advocates of the homeless, one in Harlem in March 2005, the other in front of New York City Mayor Michael Bloomberg's house in December 2005. On June 13, 2007, Haight reversed the ruling, saying that attorneys for the city had provided new evidence indicating that the protestors may have been \"disorderly.\" Lawyers for the demonstrators vowed to continue fighting to prove that the action was legal and peaceful.\n\n"}
{"id": "2828828", "url": "https://en.wikipedia.org/wiki?curid=2828828", "title": "Hedonic treadmill", "text": "Hedonic treadmill\n\nThe hedonic treadmill, also known as hedonic adaptation, is the observed tendency of humans to quickly return to a relatively stable level of happiness despite major positive or negative events or life changes.\nAccording to this theory, as a person makes more money, expectations and desires rise in tandem, which results in no permanent gain in happiness. Brickman and Campbell coined the term in their essay \"Hedonic Relativism and Planning the Good Society\" (1971). During the late 1990s, the concept was modified by Michael Eysenck, a British psychologist, to become the current \"hedonic treadmill theory\" which compares the pursuit of happiness to a person on a treadmill, who has to keep walking just to stay in the same place. The concept dates back centuries, to such writers as St. Augustine, cited in Robert Burton's 1621 \"Anatomy of Melancholy\": \"A true saying it is, \"Desire hath no rest\", is infinite in itself, endless, and as one calls it, a perpetual rack, or horse-mill.\"\n\nThe hedonic (or happiness) set point has gained interest throughout the field of positive psychology where it has been developed and revised further. Given that hedonic adaptation generally demonstrates that a person's long-term happiness is not significantly affected by otherwise impacting events, positive psychology has concerned itself with the discovery of things that can lead to lasting changes in happiness levels.\nHedonic adaptation is a process or mechanism that reduces the affective impact of emotional events. Generally, hedonic adaptation involves a happiness \"set point\", whereby humans generally maintain a constant level of happiness throughout their lives, despite events that occur in their environment. The process of hedonic adaptation is often conceptualized as a treadmill, since one must continually work to maintain a certain level of happiness. Others conceptualize hedonic adaptation as functioning similarly to a thermostat (a negative feedback system) that works to maintain an individual's happiness set point. One of the main concerns of positive psychology is determining how to maintain or raise one's happiness set point, and further, what kind of practices lead to lasting happiness.\n\nHedonic adaptation can occur in a variety of ways. Generally, the process involves cognitive changes, such as shifting values, goals, attention and interpretation of a situation. Further, neurochemical processes desensitize overstimulated hedonic pathways in the brain, which possibly prevents persistently high levels of intense positive or negative feelings. The process of adaptation can also occur through the tendency of humans to construct elaborate rationales for considering themselves deprived through a process social theorist Gregg Easterbrook calls \"abundance denial\".\n\nThe \"Hedonic Treadmill\" is a term coined by Brickman and Campbell in their article \"Hedonic Relativism and Planning the Good Society\" (1971), describing the tendency of people to keep a fairly stable baseline level of happiness despite external events and fluctuations in demographic circumstances. The idea of relative happiness had been around for decades when in 1978 Brickman et al. began to approach hedonic pleasure within the framework of Helson's adaptation level theory, which holds that perception of stimulation is dependent upon comparison of former stimulations. The hedonic treadmill functions similarly to most adaptations that serve to protect and enhance perception. In the case of hedonics, the sensitization or desensitization to circumstances or environment can redirect motivation. This reorientation functions to protect against complacency, but also to accept unchangeable circumstances, and redirect efforts towards more effective goals. Frederick and Lowenstein classify three types of processes in hedonic adaptation: shifting adaptation levels, desensitization, and sensitization. Shifting adaptation levels occurs when a person experiences a shift in what is perceived as a \"neutral\" stimulus, but maintains sensitivity to stimulus differences. For example, if Sam gets a raise he will initially be happier, and then habituate to the larger salary and return to his happiness set point. But he will still be pleased when he gets a holiday bonus. Desensitization decreases sensitivity in general, which reduces sensitivity to change. Those who have lived in war zones for extended periods of time may become desensitized to the destruction that happens on a daily basis, and be less affected by the occurrence of serious injuries or losses that may once have been shocking and upsetting. Sensitization is an increase of hedonic response from continuous exposure, such as the increased pleasure and selectivity of connoisseurs for wine, or food.\n\nBrickman, Coates, and Janoff-Bulman were among the first to investigate the hedonic treadmill in their 1978 study, \"Lottery Winners and Accident Victims: Is Happiness Relative?\". Lottery winners and paraplegics were compared to a control group and as predicted, comparison (with past experiences and current communities) and habituation (to new circumstances) affected levels of happiness such that after the initial impact of the extremely positive or negative events, happiness levels typically went back to the average levels. This interview-based study, while not longitudinal, was the beginning of a now large body of work exploring the relativity of happiness.\n\nBrickman and Campbell originally implied that everyone returns to the same neutral set point after a significantly emotional life event. In the literature review, \"Beyond the Hedonic Treadmill, Revising the Adaptation Theory of Well-Being\" (2006), Diener, Lucas, and Scollon concluded that people are not hedonically neutral, and that individuals have different set points which are at least partially heritable. They also concluded that individuals may have more than one happiness set point, such as a life satisfaction set point and a subjective well being set point, and that because of this, one's level of happiness is not just one given set point but can vary within a given range. Diener and colleagues point to longitudinal and cross-sectional research to argue that happiness set point can change, and lastly that individuals vary in the rate and extent of adaptation they exhibit to change in circumstance.\n\nIn a longitudinal study conducted by Mancini, Bonnano, and Clark, people showed individual differences in how they responded to significant life events, such as marriage, divorce and widowhood. They recognized that some individuals do experience substantial changes to their hedonic set point over time, though most others do not, and argue that happiness set point can be relatively stable throughout the course of an individual's life, but the life satisfaction and subjective well being set points are more variable.\n\nSimilarly, the longitudinal study conducted by Fujita and Diener (2005) described the life satisfaction set point as a \"soft baseline\". This means that for most people, this baseline is similar to their happiness baseline. Typically, life satisfaction will hover around a set point for the majority of their lives and not change dramatically. However, for about a quarter of the population this set point is not stable, and does indeed move in response to a major life event.\nOther longitudinal data has shown that subjective well being set points do change over time, and that adaptation is not necessarily inevitable. In his archival data analysis, Lucas found evidence that it is possible for someone's subjective well-being set point to change drastically, such as in the case of individuals who acquire a severe, long term disability. However, as Diener, Lucas, and Scollon point out, the amount of fluctuation a person experiences around their set point is largely dependent on the individual's ability to adapt.\n\nAfter following over a thousand sets of twins for 10 years, Lykken and Tellegen (1996) concluded that almost 50% of our happiness levels are determined by genetics. Headey and Wearing (1989) suggested that our position on the spectrum of the stable personality traits (neuroticism, extraversion, and openness to experience) accounts for how we experience and perceive life events, and indirectly contributes to our happiness levels. Research on happiness has spanned decades and crossed cultures in order to test the true limits of our hedonic set point.\n\nIn general there is conflicting evidence on the validity of the hedonic treadmill, if people always return to a baseline level of happiness or if some events have the ability to change this baseline for good. While some researchers believe life events change people's baseline for good over the course of one's life, others believe people will always return to their baseline.\n\nIn recent large panel studies divorce, death of a spouse, unemployment, disability and similar events have been shown to change the long-term subjective well-being, even though some adaptation does occur and inborn factors affect this.\n\nIn the aforementioned Brickman study (1978), researchers interviewed 22 lottery winners and 29 paraplegics to determine their change in happiness levels due to their given event (winning lottery or becoming paralyzed). The event in the case of lottery winners had taken place between one month and one and a half years before the study, and in the case of paraplegics between a month and a year. The group of lottery winners reported being similarly happy before and after the event, and expected to have a similar level of happiness in a couple of years. These findings show that having a large monetary gain had no effect on their baseline level of happiness, for both present and expected happiness in the future. They found that the paraplegics reported having a higher level of happiness in the past than the rest (due to a nostalgia effect), a lower level of happiness at the time of the study than the rest (although still above the middle point of the scale, that is, they reported being more happy than unhappy) and, surprisingly, they also expected to have similar levels of happiness than the rest in a couple of years. One must note that the paraplegics did have an initial decrease in life happiness, but the key to their findings is that they expected to eventually return to their baseline in time.\n\nIn a newer study (2007), winning a medium-sized lottery prize had a lasting mental wellbeing effect of 1.4 GHQ points on Britons even two years after the event.\nSome research suggests that hedonic setpoints can potentially be raised with new compounds like NSI-189. This could potentially have critical implications for the treatment of dysthymia and depression. Other research suggests that resilience to suffering is partly due to a decreased fear response in the amygdala and increased levels of BDNF in the brain. New genetic research have found that changing a gene could increase intelligence and resilience to depressing and traumatizing events. This could have crucial benefits for those suffering from anxiety and PTSD. \nRecent research reveals certain types of brain training can increase brain size. The hippocampus volume can affect mood, hedonic setpoints, some forms of memory. A smaller hippocampus has been linked to depression and dysthymia. Certain activities and environmental factors can reset the hedonic setpoint and also grow the hippocampus to an extent. London taxi drivers' hippocampi grow on the job, and the drivers have a better memory than those who didn't become taxi drivers. In particular, the posterior hippocampus seemed to be the most important for enhanced mood and memory.\n\nLucas, Clark, Georgellis, and Diener (2003) researched changes in baseline level of well-being due to marital status, birth of first child, and layoff. While they found that a negative life event can have a greater impact on a person's psychological state and happiness set point than a positive event, they concluded that people completely adapt, finally returning to their baseline level of well-being, after divorce, losing a spouse, the birth of a child, and for women losing their job. They did not find a return to baseline for marriage or for layoffs in men. This study also illustrated that the amount of adaptation depends on the individual.\n\nWildeman, Turney, and Schnittker (2014) studied the effects of imprisonment on one's baseline level of well-being. They researched how being in jail affects one's level of happiness both short term (while in prison) and long term (after being released). They found that being in prison has negative effects on one's baseline well-being; in other words one's baseline of happiness is lower in prison than when not in prison. Once people were released from prison, they were able to bounce back to their previous level of happiness.\n\nSilver (1982) researched the effects of a traumatic accident on one's baseline level of happiness. Silver found that accident victims were able to return to a happiness set point after a period of time. For eight weeks, Silver followed accident victims who had sustained severe spinal cord injuries. About a week after their accident, Silver observed that the victims were experiencing much stronger negative emotions than positive ones. By the eighth and final week, the victims' positive emotions outweighed their negative ones. The results of this study suggest that regardless of whether the life event is significantly negative or positive, people will almost always return to their happiness baseline.\n\nFujita and Diener (2005) studied the stability of one's level of subjective well-being over time and found that for most people, there is a relatively small range in which their level of satisfaction varies. They asked a panel of 3,608 German residents to rate their current and overall satisfaction with life on a scale of 0-10, once a year for seventeen years. Only 25% of participants exhibited shifts in their level of life satisfaction over the course of the study, with just 9% of participants having experienced significant changes. They also found that those with a higher mean level of life satisfaction had more stable levels of life satisfaction than those with lower levels of satisfaction.\n\nThe concept of the happiness set point (proposed by Sonja Lyubomirsky) can be applied in clinical psychology to help patients return to their hedonic set point when negative events happen. Determining when someone is mentally distant from their happiness set point and what events trigger those changes can be extremely helpful in treating conditions such as depression. When a change occurs, clinical psychologists work with patients to recover from the depressive spell and return to their hedonic set point more quickly. Because acts of kindness often promote long-term well-being, one treatment method is to provide patients with different altruistic activities that can help a person raise his or her hedonic set point. This can in turn be helpful in reducing reckless habits in the pursuit of well-being. Further, helping patients understand that long-term happiness is relatively stable throughout one's life can help to ease anxiety surrounding impactful events.\n\nHedonic adaptation is also relevant to resilience research. Resilience is a \"class of phenomena characterized by patterns of positive adaptation in the context of significant adversity or risk,\" meaning that resilience is largely the ability for one to remain at their hedonic setpoint while going through negative experiences. Psychologists have identified various factors that contribute to a person being resilient, such as positive attachment relationships (see Attachment Theory), positive self-perceptions, self-regulatory skills (see Emotional self-regulation), ties to prosocial organizations (see prosocial behavior), and a positive outlook on life. These factors can contribute to maintaining a happiness set point even in the face of adversity or negative events.\n\nOne critical point made regarding our individual set point is to understand it may simply be a genetic tendency and not a completely determined criterion for happiness, and it can still be influenced. In a study on moderate to excessive drug intake on rats, Ahmed and Koob (1998), sought to demonstrate that the use of mind-altering drugs such as cocaine could change an individual's hedonic set point. Their findings suggest that drug usage and addiction lead to neurochemical adaptations whereby a person needs more of that substance to feel the same levels of pleasure. Thus, drug abuse can have lasting impacts on one's hedonic set point, both in terms of overall happiness and with regard to pleasure felt from drug usage.\n\nGenetic roots of the hedonic set point are also disputed. Sosis (2014) has argued the \"hedonic treadmill\" interpretation of twin studies depends on dubious assumptions. Pairs of identical twins raised apart aren't necessarily raised in substantially different environments. The similarities between twins (such as intelligence or beauty) may invoke similar reactions from the environment. Thus, we might see a notable similarity in happiness levels between twins even though there aren't happiness genes governing affect levels.\n\nFurther, hedonic adaptation may be a more common phenomenon when dealing with positive events as opposed to negative ones. Negativity bias, where people tend to focus more on negative emotions than positive emotions, can be an obstacle in raising one's happiness set point. Negative emotions often require more attention and are generally remembered better, overshadowing any positive experiences that may even outnumber negative experiences. Given that negative events hold more psychological power than positive ones, it may be difficult to create lasting positive change.\n\nHeadey (2008) concluded that an internal locus of control and having \"positive\" personality traits (notably low neuroticism) are the two most significant factors affecting one's subjective well-being. Headey also found that adopting \"non-zero sum\" goals, those which enrich one's relationships with others and with society as a whole (i.e. family-oriented and altruistic goals), increase the level of subjective well-being. Conversely, attaching importance to zero-sum life goals (career success, wealth, and social status) will have a small but nevertheless statistically significant negative impact on people's overall subjective well-being (even though the size of a household's disposable income does have a small, positive impact on subjective well-being). Duration of one's education seems to have no direct bearing on life satisfaction. And, contradicting set point theory, Headey found no return to homeostasis after sustaining a disability or developing a chronic illness. These disabling events are permanent, and thus according to cognitive model of depression, may contribute to depressive thoughts and increase neuroticism (another factor found by Headey to diminish subjective well-being). Disability appears to be the single most important factor affecting human subjective well-being. The impact of disability on subjective well-being is almost twice as large as that of the second strongest factor affecting life satisfaction—the personality trait of neuroticism.\n\n\n"}
{"id": "37974207", "url": "https://en.wikipedia.org/wiki?curid=37974207", "title": "Il Galateo", "text": "Il Galateo\n\nGalateo: The Rules of Polite Behavior (\"Il Galateo, overo de' costumi\") by Florentine Giovanni Della Casa (1503–56) was published in Venice in 1558. A guide to what one should do and avoid in ordinary social life, this courtesy book of the Renaissance explores subjects such as dress, table manners, and conversation. It became so popular that the title, which refers to the name of one of the author’s distinguished friends, entered into the Italian language as a general term for social etiquette. \n\nDella Casa did not live to see his manuscript’s widespread and lasting success, which arrived shortly after its publication. It was translated into French (1562), English (1576), Latin (1580), Spanish (1585), and German (1587), and has been read and studied in every generation. Della Casa's work set the foundation for modern etiquette writers and authorities on manners, such as “Miss Manners” Judith Martin, Amy Vanderbilt, and Emily Post.\n\nIn the twentieth century, scholars usually situated \"Galateo\" among the courtesy books and conduct manuals that were very popular during the Renaissance. In addition to Castiglione’s celebrated \"Courtier\", other important Italian treatises and dialogues include Alessandro Piccolomini’s \"Moral institutione\" (1560), Luigi Cornaro’s \"Treatise on the Sober Life\" (1558-1565), and Stefano Guazzo’s \"Art of Civil Conversation\" (1579). \n\nIn recent years, attention has turned to the humor and dramatic flair of Della Casa’s book. It has been argued that the style sheds light on Shakespeare’s comedies. When it first appeared in English translation by Robert Peterson in 1575, it would have been available in book stalls in Shakespeare's London. Stephen Greenblatt, author of \"Will in the World\", writes, \"To understand the culture out of which Shakespeare is writing, it helps to read Renaissance courtesy manuals like Baldassare Castiglione’s famous Book of the Courtier (1528) or, still better, Giovanni della Casa’s \"Galateo or, The Rules of Polite Behavior\" (1558, available in a delightful new translation by M.F. Rusnak). It is fine for gentlemen and ladies to make jokes, della Casa writes, for we all like people who are funny, and a genuine witticism produces “joy, laughter, and a kind of astonishment.” But mockery has its risks. It is perilously easy to cross a social and moral line of no return.\" \n\nDistinguished historians argue that \"Galateo\" should be read in the context of international European politics, and some contend that the work expresses an attempt to distinguish Italian excellence. “During the half-century when Italy fell prey to foreign invasion (1494-1559) and was overrun by French, Spanish and German armies, the Italian ruling classes were battered by - as they often envisaged them - \"barbarians\". In their humiliation and laboured responses, Italian writers took to reflecting on ideals, such as the ideal literary language, the ideal cardinal, ideal building types, and the ideal general or field commander. But in delineating the rules of conduct, dress and conversation for the perfect gentleman, they were saying, in effect, \"We are the ones who know how to cut the best figure in Europe\". \n\nA skilled writer in Latin, Della Casa followed Erasmus in presenting a harmonious and simple morality based on Aristotle’s \"Nicomachean Ethics\" and notion of the mean, as well as other classical sources. His treatise also reveals an obsession with graceful conduct and self-fashioning during the time of Michelangelo and Titian: “A man must not be content with doing good things, but he must also study to do them gracefully. Grace is nothing other than that luster which shines from the appropriateness of things that are suitably ordered and well arranged one with the other and together.” The work has been edited in this light by such distinguished Italian scholars as Stefano Prandi, Emanuela Scarpa, and Giorgio Manganelli.\n\nThe work may be read in the context of what Norbert Elias called the “civilizing process.” It is generally agreed that, given the popularity and impact of \"Galateo\", the cultural elite of the Italian Renaissance taught Europe how to behave. Giulio Ferroni argues that Della Casa “proposes a closed and oppressive conformity, made of caution and hypocrisy, hostile to every manifestation of liberty and originality.” Others contend, on the contrary, that the work represents ambivalence, self-control, and a modern understanding of the individual in a society based on civility, intercultural competence and social networking.\n\nDella Casa addresses gentlemanly citizens who wish to convey a winning and attractive image. With a casual style and dry humor, he writes about everyday concerns, from posture to telling jokes to table manners. “Our manners are attractive when we regard others’ pleasure and not our own delight,” Della Casa writes.\n\nUnlike Baldassare Castiglione’s \"The Book of the Courtier\", the rules of polite behavior in \"Galateo\" are not directed to ideal men in a Renaissance court. Instead, Della Casa observes the ordinary habits of people who do not realize that clipping one’s nails in public is bad. “One should not annoy others with such stuff as dreams, especially since most dreams are by and large idiotic,” we are advised.\n\nValentina D’Urso, Professor of Psychology and author of \"Le Buone Maniere\", writes, “The founding father of this literary genre, [Galateo] is an extraordinary read, lively and passionate. One doesn’t know whether to admire more its rich style or the wisdom of the practical words of advice.” \n\nThe work was preceded by a short treatise on the same subject in Latin, \"De officiis inter tenuiores et potentiores amicos\" (1546). Latin at the time was the language of learned society, and Della Casa was a first-rate classicist and public speaker. The treatise opens with a Latinate \"conciossiacosaché\", which gained Galateo a reputation for being pedantic and labored. However, Giuseppe Baretti and poets such as Giacomo Leopardi ranked Della Casa alongside Machiavelli as a master of Italian prose style. \"Una delle prose più eleganti e più attiche del secolo decimosesto,\" (one of the most elegant and Attic prose works of the sixteenth century) Leopardi said. Della Casa’s \"Galateo\" is, in the words of scholar E. H. Wilkins, “still valuable…for the pleasant ease with which most of it is written, and for its common sense, its plentiful humor, and its general amenity.”\n\nDella Casa frequently alludes to Dante and more often to Boccaccio, whose Decameron he evidently knew very well and whose style he imitates. Several comments on language in Galateo reflect the Tuscan language model proposed about the same time by Della Casa’s friend Pietro Bembo.\n\nIn the first chapter it is said that a gentleman should be at all times courteous, pleasant, and in manners beautiful. Although good manners may not appear as important as liberality, constancy, or magnanimity, they are nonetheless a virtue for achieving the esteem of others. \n\nOne must not mention, do, or think anything that invokes images in the mind that are dirty or disreputable. One should not reveal by one's gestures that said person has just returned from the bathroom, do not blow one's nose and look into the handkerchief, avoid spitting and yawning. \n\nDella Casa tells his reader that outward appearance is very important, so clothes must be tailored and conform to prevailing custom, reflecting one’s social status. \n\nIn Chapter 7, Della Casa deals with a pivotal subject - conversation: you have to talk about topics of interest to all present and show respect to everyone, avoiding anything that is base or petty. \n\nIn Chapter 14 of the way we talk about the places where you are with other people and the first thing that the author discusses types of ceremonies, false flatteries, and fawning behavior. Another matter is whether the ceremonies are made to us: never refuse because it could be taken as a sign of arrogance.\n\nDella Casa returns to illustrate the customs of conversation and public speaking. Language should, as much as possible, be \"orderly and well-expressed\" so that the listener is able to understand what the speaker intends. In addition to the clarity of the words used, it is also important that they sound pleasant. Before talking about any topic, it is good to have it formed in your mind. It is not polite to interrupt someone while talking, nor to help him find his words.\n\nIn the last three chapters, the author writes about behaviour in general: what you do should be appropriate and done with grace. A gentleman should never run, or walk too slowly. Della Casa brings us to behavior at the table, such as not scratching, not eating like a pig, not using a toothpick or sharing food. In Della Casa’s vision, slight slips of decorum become taboo.\n\nIt was probably first drafted during his stay at the Abbey of Saint Eustace at Nervesa, near Treviso, between 1551 and 1555. \"Galateo\" was first published in Venice, and was edited by Erasmus Gemini in 1558. The first separate publication appeared in Milan a year later. \nThe Vatican manuscript (formerly Parraciani Ricci), in Latin with autograph corrections, was edited and published by Gennaro Barbarisi in 1990. The manuscript contains neither the title nor the division into chapters. Many variants in the first edition are attributed to Erasmus Gemini. \n\nThe Spanish Galateo of Lucas Gracián Dantisco was very influential in the seventeenth century. In the Enlightenment, the letters of Lord Chesterfield show the influence of Galateo, as does a self-help manuscript of George Washington. The first American edition was published in Baltimore in 1811, with a short appendix on how to slice and serve meats.\n\n\n"}
{"id": "549136", "url": "https://en.wikipedia.org/wiki?curid=549136", "title": "Illusion of control", "text": "Illusion of control\n\nThe illusion of control is the tendency for people to overestimate their ability to control events; for example, it occurs when someone feels a sense of control over outcomes that they demonstrably do not influence. The effect was named by psychologist Ellen Langer and has been replicated in many different contexts. It is thought to influence gambling behavior and belief in the paranormal. Along with illusory superiority and optimism bias, the illusion of control is one of the positive illusions.\n\nPsychological theorists have consistently emphasized the importance of perceptions of control over life events. One of the earliest instances of this is when Adler argued that people strive for proficiency in their lives. Heider later proposed that humans have a strong motive to control their environment and Wyatt Mann hypothesized a basic competence motive that people satisfy by exerting control. Wiener, an attribution theorist, modified his original theory of achievement motivation to include a controllability dimension. Kelley then argued that people's failure to detect noncontingencies may result in their attributing uncontrollable outcomes to personal causes. Nearer to the present, Taylor and Brown argued that positive illusions, including the illusion of control, foster mental health.\n\nThe illusion is more common in familiar situations, and in situations where the person knows the desired outcome. Feedback that emphasizes success rather than failure can increase the effect, while feedback that emphasizes failure can decrease or reverse the effect. The illusion is weaker for depressed individuals and is stronger when individuals have an emotional need to control the outcome. The illusion is strengthened by stressful and competitive situations, including financial trading. Although people are likely to overestimate their control when the situations are heavily chance-determined, they also tend to underestimate their control when they actually have it, which runs contrary to some theories of the illusion and its adaptiveness. People also showed a higher illusion of control when they were allowed to become familiar with a task through practice trials, make their choice before the event happens like with throwing dice, and when they can make their choice rather than have it made for them with the same odds. People are more likely to show control when they have more answers right at the beginning than at the end, even when the people had the same number of correct answers.\n\nThe illusion might arise because people lack direct introspective insight into whether they are in control of events. This has been called the introspection illusion. Instead they may judge their degree of control by a process that is often unreliable. As a result, they see themselves as responsible for events when there is little or no causal link. In one study, college students were in a virtual reality setting to treat a fear of heights using an elevator. Those who were told that they had control, yet had none felt as though they had as much control as those who actually did have control over the elevator. Those who were led to believe they did not have control said they felt as though they had little control.\n\nAt times, people attempt to gain control by transferring responsibility to more capable or “luckier” others to act for them. By forfeiting direct control, it is perceived to be a valid way of maximizing outcomes. This illusion of control by proxy is a significant theoretical extension of the traditional illusion of control model. People will of course give up control if another person is thought to have more knowledge or skill in areas such as medicine where actual skill and knowledge are involved. In cases like these it is entirely rational to give up responsibility to people such as doctors. However, when it comes to events of pure chance, allowing another to make decisions (or gamble) on one's behalf, because they are seen as luckier is not rational and would go against people's well-documented desire for control in uncontrollable situations. However, it does seem plausible since people generally believe that they can possess luck and employ it to advantage in games of chance, and it is not a far leap that others may also be seen as lucky and able to control uncontrollable events.\n\nIn one instance, a lottery pool at a company decides who picks the numbers and buys the tickets based on the wins and losses of each member. The member with the best record becomes the representative until they accumulate a certain number of losses and then a new representative is picked based on wins and losses. Even though no member is truly better than the other and it is all by chance, they still would rather have someone with seemingly more luck to have control over them.\n\nIn another real-world example, in the 2002 Olympics men's and women's hockey finals, Team Canada beat Team USA but it was later believed that the win was the result of the luck of a Canadian coin that was secretly placed under the ice before the game. The members of Team Canada were the only people who knew the coin had been placed there. The coin was later put in the Hockey Hall of Fame where there was an opening so people could touch it. People believed they could transfer luck from the coin to themselves by touching it, and thereby change their own luck.\n\nThe illusion of control is demonstrated by three converging lines of evidence: 1) laboratory experiments, 2) observed behavior in familiar games of chance such as lotteries, and 3) self-reports of real-world behavior.\n\nOne kind of laboratory demonstration involves two lights marked \"Score\" and \"No Score\". Subjects have to try to control which one lights up. In one version of this experiment, subjects could press either of two buttons. Another version had one button, which subjects decided on each trial to press or not. Subjects had a variable degree of control over the lights, or none at all, depending on how the buttons were connected. The experimenters made clear that there might be no relation between the subjects' actions and the lights. Subjects estimated how much control they had over the lights. These estimates bore no relation to how much control they actually had, but was related to how often the \"Score\" light lit up. Even when their choices made no difference at all, subjects confidently reported exerting some control over the lights.\n\nEllen Langer's research demonstrated that people were more likely to behave as if they could exercise control in a chance situation where \"skill cues\" were present. By skill cues, Langer meant properties of the situation more normally associated with the exercise of skill, in particular the exercise of choice, competition, familiarity with the stimulus and involvement in decisions. One simple form of this effect is found in casinos: when rolling dice in a craps game people tend to throw harder when they need high numbers and softer for low numbers.\n\nIn another experiment, subjects had to predict the outcome of thirty coin tosses. The feedback was rigged so that each subject was right exactly half the time, but the groups differed in where their \"hits\" occurred. Some were told that their early guesses were accurate. Others were told that their successes were distributed evenly through the thirty trials. Afterwards, they were surveyed about their performance. Subjects with early \"hits\" overestimated their total successes and had higher expectations of how they would perform on future guessing games. This result resembles the \"irrational primacy effect\" in which people give greater weight to information that occurs earlier in a series. Forty percent of the subjects believed their performance on this chance task would improve with practice, and twenty-five percent said that distraction would impair their performance.\n\nAnother of Langer's experiments—replicated by other researchers—involves a lottery. Subjects are either given tickets at random or allowed to choose their own. They can then trade their tickets for others with a higher chance of paying out. Subjects who had chosen their own ticket were more reluctant to part with it. Tickets bearing familiar symbols were less likely to be exchanged than others with unfamiliar symbols. Although these lotteries were random, subjects behaved as though their choice of ticket affected the outcome. Participants who chose their own numbers were less likely to trade their ticket even for one in a game with better odds.\n\nAnother way to investigate perceptions of control is to ask people about hypothetical situations, for example their likelihood of being involved in a motor vehicle accident. On average, drivers regard accidents as much less likely in \"high-control\" situations, such as when they are driving, than in \"low-control\" situations, such as when they are in the passenger seat. They also rate a high-control accident, such as driving into the car in front, as much less likely than a low-control accident such as being hit from behind by another driver.\n\nEllen Langer, who first demonstrated the illusion of control, explained her findings in terms of a confusion between skill and chance situations. She proposed that people base their judgments of control on \"skill cues\". These are features of a situation that are usually associated with games of skill, such as competitiveness, familiarity and individual choice. When more of these skill cues are present, the illusion is stronger.\n\nSuzanne Thompson and colleagues argued that Langer's explanation was inadequate to explain all the variations in the effect. As an alternative, they proposed that judgments about control are based on a procedure that they called the \"control heuristic\". This theory proposes that judgments of control to depend on two conditions; an intention to create the outcome, and a relationship between the action and outcome. In games of chance, these two conditions frequently go together. As well as an intention to win, there is an action, such as throwing a die or pulling a lever on a slot machine, which is immediately followed by an outcome. Even though the outcome is selected randomly, the control heuristic would result in the player feeling a degree of control over the outcome.\n\nSelf-regulation theory offers another explanation. To the extent that people are driven by internal goals concerned with the exercise of control over their environment, they will seek to reassert control in conditions of chaos, uncertainty or stress. One way of coping with a lack of real control is to falsely attribute oneself control of the situation.\n\nThe core self-evaluations (CSE) trait is a stable personality trait composed of locus of control, neuroticism, self-efficacy, and self-esteem. While those with high core self-evaluations are likely to believe that they control their own environment (i.e., internal locus of control), very high levels of CSE may lead to the illusion of control.\n\nTaylor and Brown have argued that positive illusions, including the illusion of control, are adaptive as they motivate people to persist at tasks when they might otherwise give up. This position is supported by Albert Bandura's claim that \"optimistic self-appraisals of capability, that are not unduly disparate from what is possible, can be advantageous, whereas veridical judgements can be self-limiting\". His argument is essentially concerned with the adaptive effect of optimistic beliefs about control and performance in circumstances where control is possible, rather than perceived control in circumstances where outcomes do not depend on an individual's behavior.\n\nBandura has also suggested that: \n\"In activities where the margins of error are narrow and missteps can produce costly or injurious consequences, personal well-being is best served by highly accurate efficacy appraisal.\"\n\nTaylor and Brown argue that positive illusions are adaptive, since there is evidence that they are more common in normally mentally healthy individuals than in depressed individuals. However, Pacini, Muir and Epstein have shown that this may be because depressed people overcompensate for a tendency toward maladaptive intuitive processing by exercising excessive rational control in trivial situations, and note that the difference with non-depressed people disappears in more consequential circumstances.\n\nThere is also empirical evidence that high self-efficacy can be maladaptive in some circumstances. In a scenario-based study, Whyte et al. showed that participants in whom they had induced high self-efficacy were significantly more likely to escalate commitment to a failing course of action. Knee and Zuckerman have challenged the definition of mental health used by Taylor and Brown and argue that lack of illusions is associated with a non-defensive personality oriented towards growth and learning and with low ego involvement in outcomes. They present evidence that self-determined individuals are less prone to these illusions. In the late 1970s, Abramson and Alloy demonstrated that depressed individuals held a more accurate view than their non-depressed counterparts in a test which measured illusion of control. This finding held true even when the depression was manipulated experimentally. However, when replicating the findings Msetfi et al. (2005, 2007) found that the overestimation of control in nondepressed people only showed up when the interval was long enough, implying that this is because they take more aspects of a situation into account than their depressed counterparts. Also, Dykman et al. (1989) showed that depressed people believe they have no control in situations where they actually do, so their perception is not more accurate overall. Allan et al. (2007) has proposed that the pessimistic bias of depressives resulted in \"depressive realism\" when asked about estimation of control, because depressed individuals are more likely to say no even if they have control.\n\nA number of studies have found a link between a sense of control and health, especially in older people.\n\nFenton-O'Creevy et al. argue, as do Gollwittzer and Kinney, that while illusory beliefs about control may promote goal striving, they are not conducive to sound decision-making. Illusions of control may cause insensitivity to feedback, impede learning and predispose toward greater objective risk taking (since subjective risk will be reduced by illusion of control).\n\nPsychologist Daniel Wegner argues that an illusion of control over external events underlies belief in psychokinesis, a supposed paranormal ability to move objects directly using the mind. As evidence, Wegner cites a series of experiments on magical thinking in which subjects were induced to think they had influenced external events. In one experiment, subjects watched a basketball player taking a series of free throws. When they were instructed to visualise him making his shots, they felt that they had contributed to his success.\n\nOne study examined traders working in the City of London's investment banks. They each watched a graph being plotted on a computer screen, similar to a real-time graph of a stock price or index. Using three computer keys, they had to raise the value as high as possible. They were warned that the value showed random variations, but that the keys might have some effect. In fact, the fluctuations were not affected by the keys. The traders' ratings of their success measured their susceptibility to the illusion of control. This score was then compared with each trader's performance. Those who were more prone to the illusion scored significantly lower on analysis, risk management and contribution to profits. They also earned significantly less.\n\n"}
{"id": "167740", "url": "https://en.wikipedia.org/wiki?curid=167740", "title": "Insult", "text": "Insult\n\nAn insult is an expression or statement (or sometimes behavior) which is disrespectful or scornful. Insults may be intentional or accidental. An insult may be factual, but at the same time pejorative, such as the word \"inbred\".\n\nLacan considered insults a primary form of social interaction, central to the imaginary order – \"a situation that is symbolised in the 'Yah-boo, so are you' of the transitivist quarrel, the original form of aggressive communication\".\n\nErving Goffman points out that every \"crack or remark set up the possibility of a counter-riposte, topper, or squelch, that is, a comeback\". He cites the example of possible interchanges at a dance in a school gym:\n\nBackhanding is referred to as slapping someone using the back of the hand instead of the palm—that is generally a symbolic, less forceful blow. Correspondingly, a backhanded (or left-handed) compliment, or asteism, is an insult that is disguised as, or accompanied by, a compliment, especially in situations where the belittling or condescension is intentional.\n\nExamples of backhanded compliments include, but not limited to:\n\n\nVerbal insults often take a phallic form; this includes offensive profanity. and may also include insults to ones sexuality.\n\nThe flyting was a formalized sequence of literary insults: invective or flyting, the literary equivalent of the spell-binding curse, uses similar incantatory devices for opposite reasons, as in Dunbar's \"Flyting with Kennedy\".\n\n\"A little-known survival of the ancient 'flytings', or contests-in-insults of the Anglo-Scottish bards, is the type of xenophobic humor once known as 'water wit' in which passengers in small boats crossing the Thames ... would insult each other grossly, in all the untouchable safety of being able to get away fast.\"\n\nSamuel Johnson once triumphed in such an exchange: \"a fellow having attacked him with some coarse raillery, Johnson answered him thus, 'Sir, your wife, \"under pretence of keeping a bawdy-house\", is a receiver of stolen goods.\n\nVarious typologies of insults have been proposed over the years. Ethologist Desmond Morris, noting that \"almost any action can operate as an Insult Signal if it is performed out of its appropriate context – at the wrong time or in the wrong place\", classes such signals in ten 'basic categories\":\n\nElizabethans took great interest in such analyses, distinguishing out, for example, the \"fleering frump ... when we give a mock with a scornful countenance as in some smiling sort looking aside or by drawing the lip awry, or shrinking up the nose\". Shakespeare humerously set up an insult-hierarchy of seven-fold \"degrees. The first, the Retort Courteous; the second, the Quip Modest; the third, the Reply Churlish; the fourth, the Reproof Valiant; the fifth, the Countercheck Quarrelsome; the sixth, the Lie with Circumstance; the seventh, the Lie Direct\".\n\nWhat qualifies as an insult is also determined both by the individual social situation and by changing social mores. Thus on one hand the insulting \"obscene invitations of a man to a strange girl can be the spicy endearments of a husband to his wife\".\n\n"}
{"id": "12811840", "url": "https://en.wikipedia.org/wiki?curid=12811840", "title": "Intermodal mapping", "text": "Intermodal mapping\n\nIntermodal mapping is the ability, inborn according to research, of humans to recognize stimuli using a sense different from the one it was originally presented to. This implies that stimuli are represented universally in the brain and available to all senses and need not be learned by pairing (classical conditioning).\n"}
{"id": "33234902", "url": "https://en.wikipedia.org/wiki?curid=33234902", "title": "Interpersonal complementarity hypothesis", "text": "Interpersonal complementarity hypothesis\n\nInterpersonal complementarity hypothesis asserts that individuals often behave in ways that evoke complementary or reciprocal behavior from others. More specifically, this hypothesis predicts that positive behaviors evoke positive behaviors, negative behaviors evoke negative behaviors, dominant behaviors evoke submissive behaviors, and vice versa.\n\nEssentially, each action carried out by a member of a group has the ability to elicit predictable actions from other group members. For example, individuals who display evidence of positive behavior (e.g., smiling, behaving cooperatively) tend to trigger positively valenced behaviors from others. In much the same way, group members who behave in a docile or submissive fashion tend to elicit complementary, dominant behaviors from other members of the group. This behavioral congruency, as it applies to obedience and authority, has been illustrated in several studies assessing power hierarchies present in groups. These studies highlight the increased comfort experienced by individuals when the power or status behavior of others complement that of their own (e.g., a \"leader\" preferring a \"follower\").\n\n"}
{"id": "2908224", "url": "https://en.wikipedia.org/wiki?curid=2908224", "title": "Klein geometry", "text": "Klein geometry\n\nIn mathematics, a Klein geometry is a type of geometry motivated by Felix Klein in his influential Erlangen program. More specifically, it is a homogeneous space \"X\" together with a transitive action on \"X\" by a Lie group \"G\", which acts as the symmetry group of the geometry.\n\nFor background and motivation see the article on the Erlangen program.\n\nA Klein geometry is a pair where \"G\" is a Lie group and \"H\" is a closed Lie subgroup of \"G\" such that the (left) coset space \"G\"/\"H\" is connected. The group \"G\" is called the principal group of the geometry and \"G\"/\"H\" is called the space of the geometry (or, by an abuse of terminology, simply the \"Klein geometry\"). The space of a Klein geometry is a smooth manifold of dimension\n\nThere is a natural smooth left action of \"G\" on \"X\" given by\nClearly, this action is transitive (take ), so that one may then regard \"X\" as a homogeneous space for the action of \"G\". The stabilizer of the identity coset is precisely the group \"H\".\n\nGiven any connected smooth manifold \"X\" and a smooth transitive action by a Lie group \"G\" on \"X\", we can construct an associated Klein geometry by fixing a basepoint \"x\" in \"X\" and letting \"H\" be the stabilizer subgroup of \"x\" in \"G\". The group \"H\" is necessarily a closed subgroup of \"G\" and \"X\" is naturally diffeomorphic to \"G\"/\"H\".\n\nTwo Klein geometries and are geometrically isomorphic if there is a Lie group isomorphism so that . In particular, if \"φ\" is conjugation by an element , we see that and are isomorphic. The Klein geometry associated to a homogeneous space \"X\" is then unique up to isomorphism (i.e. it is independent of the chosen basepoint \"x\").\n\nGiven a Lie group \"G\" and closed subgroup \"H\", there is natural right action of \"H\" on \"G\" given by right multiplication. This action is both free and proper. The orbits are simply the left cosets of \"H\" in \"G\". One concludes that \"G\" has the structure of a smooth principal \"H\"-bundle over the left coset space \"G\"/\"H\":\n\nThe action of \"G\" on need not be effective. The kernel of a Klein geometry is defined to be the kernel of the action of \"G\" on \"X\". It is given by\nThe kernel \"K\" may also be described as the core of \"H\" in \"G\" (i.e. the largest subgroup of \"H\" that is normal in \"G\"). It is the group generated by all the normal subgroups of \"G\" that lie in \"H\".\n\nA Klein geometry is said to be effective if and locally effective if \"K\" is discrete. If is a Klein geometry with kernel \"K\", then is an effective Klein geometry canonically associated to .\n\nA Klein geometry is geometrically oriented if \"G\" is connected. (This does \"not\" imply that \"G\"/\"H\" is an oriented manifold). If \"H\" is connected it follows that \"G\" is also connected (this is because \"G\"/\"H\" is assumed to be connected, and is a fibration).\n\nGiven any Klein geometry , there is a geometrically oriented geometry canonically associated to with the same base space \"G\"/\"H\". This is the geometry where \"G\" is the identity component of \"G\". Note that .\n\nA Klein geometry is said to be reductive and \"G\"/\"H\" a reductive homogeneous space if the Lie algebra formula_4 of \"H\" has an \"H\"-invariant complement in formula_5.\n\nIn the following table, there is a description of the classical geometries, modeled as Klein geometries.\n"}
{"id": "15616713", "url": "https://en.wikipedia.org/wiki?curid=15616713", "title": "Kline sphere characterization", "text": "Kline sphere characterization\n\nIn mathematics, a Kline sphere characterization, named after John Robert Kline, is a topological characterization of a two-dimensional sphere in terms of what sort of subset separates it. Its proof was one of the first notable accomplishments of R. H. Bing; Bing gave an alternate proof using brick partitioning in his paper \"Complementary domains of continuous curves\" \n\nA simple closed curve in a two-dimensional sphere (for instance, its equator) separates the sphere into two pieces upon removal. If one removes a pair of points from a sphere, however, the remainder is connected. Kline's sphere characterization states that the converse is true: If a nondegenerate locally connected metric continuum is separated by any simple closed curve but by no pair of points, then it is a two-dimensional sphere.\n\n"}
{"id": "1884726", "url": "https://en.wikipedia.org/wiki?curid=1884726", "title": "Lau v. Nichols", "text": "Lau v. Nichols\n\nLau v. Nichols, 414 U.S. 563 (1974), was a United States Supreme Court case in which the Court unanimously decided that the lack of supplemental language instruction in public school for students with limited English proficiency violated the Civil Rights Act of 1964. The court held that since non-English speakers were denied a meaningful education, the disparate impact caused by the school policy violated the Civil Rights Act of 1964 and the school district was demanded to provide students with \"appropriate relief\".\n\nIn 1971, the San Francisco school system desegregated based on the result of Supreme Court case \"Lee v. Johnson.\" 2856 Chinese students, who were not fluent in English, were integrated back into the San Francisco Unified School District (SFUSD). Only about 1000 of those students were provided supplemental English instruction. Of the other 1800-plus Chinese students who weren't fluent in English, many were placed in special education classes while some were forced to be in the same grade for years.\n\nEven though the Bilingual Education Act was passed by Congress in 1968 to address the needs of Limited English Speaking Abilities students, the funding was limited. School participation in those programs was also voluntary, and by 1972, \"only 100,391 students nationally, out of approximately 5,000,000 in need were enrolled in a Title VII-funded program.\"\n\nEdward H. Steinman, a public-interest lawyer, reached out to the parents of Kinney Kinmon Lau and other Chinese students with limited English proficiency. He encouraged them to challenge the school district, and they filed a class action suit against Alan H. Nichols, the president of the SFUSD at the time, and other officials in the school district. The students claimed that they were not receiving special help in school due to their inability to speak English, and they argued they were entitled to special help under the Fourteenth Amendment and the Civil Rights Act of 1964 because of equal protection and the ban on educational discrimination.\n\nThe District Court for the Northern District of California denied the relief and the Court of Appeals for the Ninth Circuit affirmed the decision. The District Court argued that since a uniform policy was used for all students in SFUSD and that the district didn't intentionally discriminate against students with limited English proficiency, equal protection was provided and the Fourteenth Amendment was not violated. The Court of Appeals claimed that since the school district provided the same treatment for all students, even though some students were disadvantaged due to their limited fluency in English, the school district was not required to make up for the different starting points of students. The students appealed the Court of Appeal's decision to the Supreme Court.\n\nThe Supreme Court issued its decision on January 21, 1974, with the Court unanimously ruling in favor of Lau. Instead of examining the equal protection clause from the 14th Amendment, the Court relied on Section 601 of the Civil Rights Act of 1964. Since the school system received federal funding, it was required to provide equal opportunities and access to all students. The Court claimed that even though the school districts provided equal treatment for all students, it still imposed disparate impact on the non-English speaking students since they were not able to understand the class material as effectively as other students and therefore were deprived of having \"meaningful\" education. The Court also referenced the guideline established by the Office for Civil Rights (OCR) of the Department of Health, Education and Welfare in 1970, which stated that language could be used as a proxy of discrimination on national origin and that \"the district must take affirmative steps to rectify the language deficiency in order to open its instructional program to these students.\" The Supreme Court demanded the school district to make necessary changes to provide equal education to the non-English speakers, but it didn't state any specific remedies for the district to follow.\n\nJustice Potter Stewart, joined by Chief Justice Burger and Justice Blackmun, concurred with this decision as he stated that affirmative remedial efforts, suggested by the OCR, were constitutional and appropriate in this case as long as the efforts were \"reasonably related to the purposes of the enabling legislation.\" In his concurrence joined by Chief Justice Burger, Justice Blackmun also suggested that \"numbers are at the heart of this case\" and that if the case only involved a few students, rather than nearly 2,000, the decision would not be the same.\n\nLau remains an important decision in bilingual education history. In this case, the Supreme Court found the violation of Civil Rights Act of 1964 based on the discriminatory effect of the school policy regardless of the intent of the officials. It prohibited the \"sink and swim\" policy and set a precedent of finding disparate impact in violation of the Civil Rights Act. The decision was subsequently followed by the passing of Equal Educational Opportunities Act of 1974 in Congress, which specifically prohibited discrimination against faculty and student in public schools and required the school districts to take \"appropriate action\" to overcome the barriers to equal participation of all students. It increased funding to the Bilingual Education Act and made additional English instruction mandatory, which effectively extended the Lau ruling to all public schools. The Officer for Civil Rights then developed a remedial guideline in 1975, otherwise known as the Lau Remedies, that specified methods and approaches for the school districts to follow in order to provide a meaningful education to students with limited English proficiency. This led to the development of bilingual programs and additional English instructions in most public schools.\n\nHowever, there have been challenges to the Lau decision in recent decades. In the Supreme Court case \"Alexander v. Sandoval\", 532 U.S. 275 (2001), the Court claimed that private plaintiffs did not have the right of action to sue against disparate impact violation under Title VI and they must provide proofs of intentional discrimination. It implied that students can no longer sue schools for policies that cause disparate impact, which significantly weakened the foundation of the Lau decision.\n\n\n"}
{"id": "44617040", "url": "https://en.wikipedia.org/wiki?curid=44617040", "title": "Law on Use of Languages and Scripts of National Minorities", "text": "Law on Use of Languages and Scripts of National Minorities\n\nLaw on Use of Languages and Scripts of National Minorities () is law which defines use of minority languages in Croatia. Additionally \"Croatian Constitutional law on national minorities rights\" and \"The Law on Education in language and script of national minorities\" explicitly define rights on usage of minority languages in Croatia.\n\nLocal governments to which this law applies \"(Municipalities of Croatia with at least one third of members of ethnic minoritiy or municipality where right is defined by international agreement)\" are required to explicitly prescribe equal official use of minority language or script throughout its territory, regulate in detail realization of those rights and expressly prescribe all particular rights guaranteed by Law on Use of Languages and Scripts of National Minorities. They are required to define these rights in their local statutes.\n\nIn April 2015 United Nations Human Rights Committee has urged Croatia to ensure the right of minorities to use their language and alphabet. Committee report stated that particularly concerns the use of Serbian Cyrillic in the town of Vukovar and municipalities concerned.\n\n"}
{"id": "6533945", "url": "https://en.wikipedia.org/wiki?curid=6533945", "title": "Leaky abstraction", "text": "Leaky abstraction\n\nIn software development, a leaky abstraction is an abstraction that leaks details that it is supposed to abstract away.\n\nThe term \"leaky abstraction\" was popularized in 2002 by Joel Spolsky. An earlier paper by Kiczales describes some of the issues with imperfect abstractions and presents a potential solution to the problem by allowing for the customization of the abstraction itself.\n\nAs coined by Spolsky, the Law of Leaky Abstractions states:\n\nThis statement highlights a particularly problematic cause of software defects: the reliance of the software developer on an abstraction's infallibility.\n\nSpolsky's article gives examples of an abstraction that works most of the time, but where a detail of the underlying complexity cannot be ignored, thus leaking complexity out of the abstraction back into the software that uses the abstraction.\n\nAs systems become more complex, software developers must rely upon more abstractions. Each abstraction tries to hide complexity, letting a developer write software that \"handles\" the many variations of modern computing.\n\nHowever, this law claims that developers of \"reliable\" software must learn the abstraction's underlying details anyway.\n\nSpolsky's article cites many examples of leaky abstractions that create problems for software development:\n\n"}
{"id": "19007131", "url": "https://en.wikipedia.org/wiki?curid=19007131", "title": "Legal certainty", "text": "Legal certainty\n\nLegal certainty is a principle in national and international law which holds that the law must provide those subject to it with the ability to regulate their conduct. Legal certainty is internationally recognised as a central requirement for the rule of law.\n\nThe legal system needs to permit those subject to the law to regulate their conduct with certainty and to protect those subject to the law from arbitrary use of state power. Legal certainty represents a requirement that decisions be made according to legal rules, i.e. be lawful. The concept of legal certainty may be strongly linked to that of individual autonomy in national jurisprudence. The degree to which the concept of legal certainty is incorporated into law varies depending on national jurisprudence. However, legal certainty frequently serves as the central principle for the development of legal methods by which law is made, interpreted and applied.\n\nLegal certainty is an established legal concept both in the civil law legal systems and common law legal systems. In the civil law tradition, legal certainty is defined in terms of maximum predictability of officials' behaviour. In the common law tradition, legal certainty is often explained in terms of citizens' ability to organise their affairs in such a way that does not break the law. In both legal traditions, legal certainty is regarded as grounding value for the legality of legislative and administrative measures taken by public authorities.\n\nThe legal philosopher Gustav Radbruch regarded legal certainty, justice and purposiveness as the three fundamental pillars of law. Today legal certainty is internationally recognised as a central requirement for the rule of law. According to the Organisation for Economic Co-operation and Development (OECD) the concept of the rule of law \"first and foremost seeks to emphasize the necessity of establishing a rule-based society in the interest of legal certainty and predictability.\" At the G8 Foreign Ministers' Meeting in Potsdam in 2007, the G8 committed to the rule of law as a core principle entailing adherence to the principle of legal certainty.\n\nMost European nations regard legal certainty as a fundamental quality of the legal system and a guiding requirement for the rule of law, although they have differing meanings of the term. \n\nThe concept can be traced through English common law in that system's recognition that legal certainty requires that laws be made such that people can comply with them.\n\nIt also recognised in all European civil legal systems. The concept is recognised in Germany as \"\", in France as \"\", in Spain as \"\", in Italy as \"\", in the Benelux countries as \"rechtszekerheid\", in Sweden as \"\", in Poland as \"\", and in Finland as \"\". Legal certainty is now recognised as one of the general principles of European community law and \"requires that all law be sufficiently precise to allow the person – if need be, with appropriate advice – to foresee, to a degree that is reasonable in the circumstances, the consequences which a given action may entail\". The principle of legal certainty, and as such the rule of law, requires that:\n\nThe concept of legal certainty has been recognised as one of the general principles of European Union law by the European Court of Justice since the 1960s. It is an important general principle of international law and public law, which predates European Union law. As a general principle in European Union law, it means that the law must be certain, in that it is clear and precise, and its legal implications foreseeable, especially when applied to financial obligations. The adoption of laws which will have legal effect in the European Union must have a proper legal basis. Legislation in member states which implements European Union law must be worded so that it is clearly understandable by those who are subject to the law.\n\nIn European Union law the general principle of legal certainty prohibits \"Ex post facto\" laws, i.e. laws should not take effect before they are published. The general principle also requires that sufficient information must be made public to enable parties to know what the law is and comply with it. For example in \"Opel Austria v Council\" the European Court of Justice held that European Council Regulation did not come into effect until it had been published. Opel had brought the action on the basis that the Regulation in question violated the principle of legal certainty, because it legally came into effect before it had been notified and the regulation published. The doctrine of legitimate expectation, which has its roots in the principles of legal certainty and good faith, is also a central element of the general principle of legal certainty in European Union law.\n\nThe legitimate expectation doctrine holds that \"those who act in good faith on the basis of law as it is, or seems to be, should not be frustrated in their expectations\". This means that a European Union institution, once it has induced a party to take a particular course of action, must not renege on its earlier position if doing so would cause that party to suffer loss. The European Court of Justice has considered the legitimate expectation doctrine in cases where violation of the general principle of legal certainty was alleged in numerous cases involving agricultural policy and European Council regulations, with the leading case being \"Mulder v Minister van Landbouw en Visserij\".\n\nThe misuse of power test is another significant element of the general principle of legal certainty in European Union law. It holds that a lawful power must not be exercised for any other purpose than that for which it was conferred. According to the misuse of power test a decision by a European Union institution is only a misuse of power if \"it appears, on the basis of objective, relevant and consistent evidence, to have been adopted with the exclusive or main purpose of achieving any end other than those stated.\" A rare instance where the European Court of Justice has held that a European Union institution has misused its powers, and therefore violated the general principle of legal uncertainty, is \"Giuffrida v Commission\". The general principle of legal certainty is particularly stringently applied when European Union law imposes financial burdens on private parties.\n\nThe concept of legal certainty is recognised by the European Court of Human Rights.\n\nIn United States law the principle of legal certainty is phrased as fair warning and the void for vagueness principle.\n\nHowever, in the \"Calder v. Bull\" case it was established that in the U.S. the prohibition of ex post facto laws applies only to criminal cases, not to civil law.\n\n"}
{"id": "39291986", "url": "https://en.wikipedia.org/wiki?curid=39291986", "title": "Linear seismic inversion", "text": "Linear seismic inversion\n\nInverse modeling is a mathematical technique where the objective is to determine the physical properties of the subsurface of an earth region that has produced a given seismogram. Cooke and Schneider (1983) defined it as calculation of the earth’s structure and physical parameters from some set of observed seismic data. The underlying assumption in this method is that the collected seismic data are from an earth structure that matches the cross-section computed from the inversion algorithm. Some common earth properties that are inverted for include acoustic velocity, formation and fluid densities, acoustic impedance, Poisson's ratio, formation compressibility, shear rigidity, porosity, and fluid saturation.\n\nThe method has long been useful for geophysicists and can be categorized into two broad types: Deterministic and stochastic inversion. Deterministic inversion methods are based on comparison of the output from an earth model with the observed field data and continuously updating the earth model parameters to minimize a function, which is usually some form of difference between model output and field observation. As such, this method of inversion to which linear inversion falls under is posed as an minimization problem and the accepted earth model is the set of model parameters that minimizes the objective function in producing a numerical seismogram which best compares with collected field seismic data.\n\nOn the other hand, stochastic inversion methods are used to generate constrained models as used in reservoir flow simulation, using geostatistical tools like kriging. As opposed to deterministic inversion methods which produce a single set of model parameters, stochastic methods generate a suite of alternate earth model parameters which all obey the model constraint. However, the two methods are related as the results of deterministic models is the average of all the possible non-unique solutions of stochastic methods. Since seismic linear inversion is a deterministic inversion method, the stochastic method will not be discussed beyond this point.\n\nThe deterministic nature of linear inversion requires a functional relationship which models, in terms of the earth model parameters, the seismic variable to be inverted. This functional relationship is some mathematical model derived from the fundamental laws of physics and is more often called a forward model. The aim of the technique is to minimize a function which is dependent on the difference between the convolution of the forward model with a source wavelet and the field collected seismic trace. As in the field of optimization, this function to be minimized is called the objective function and in convectional inverse modeling, is simply the difference between the convolved forward model and the seismic trace. As earlier mentioned, different types of variables can be inverted for but for clarity, these variables will be referred to as the impedance series of the earth model. In the following subsections we will describe in more detail, in the context of linear inversion as a minimization problem, the different components that are necessary to invert seismic data.\n\nThe centerpiece of seismic linear inversion is the forward model which models the generation of the experimental data collected. According to Wiggins (1972), it provides a functional (computational) relationship between the model parameters and calculated values for the observed traces. Depending on the seismic data collected, this model may vary from the classical wave equations for predicting particle displacement or fluid pressure for sound wave propagation through rock or fluids, to some variants of these classical equations. For example, the forward model in Tarantola (1984) is the wave equation for pressure variation in a liquid media during seismic wave propagation while by assuming constant velocity layers with plane interfaces, Kanasewich and Chiu (1985) used the brachistotrone model of John Bernoulli for travel time of a ray along a path. In Cooke and Schneider (1983), the model is a synthetic trace generation algorithm expressed as in Eqn. 3, where R(t) is generated in the Z-domain by recursive formula. In whatever form the forward model appears, it is important that it not only predicts the collected field data, but also models how the data is generated. Thus, the forward model by Cooke and Schneider (1983) can only be used to invert CMP data since the model invariably assumes no spreading loss by mimicking the response of a laterally homogeneous earth to a plane-wave source\n\nwhere \"s\"(\"t\") = synthetic trace, \"w\"(\"t\") = source wavelet, and \"R\"(\"t\") = reflectivity function.\n\nAn important numerical process in inverse modeling is to minimize the objective function, which is a function defined in terms of the difference between the collected field seismic data and the numerically computed seismic data. Classical objective functions include the sum of squared deviations between experimental and numerical data, as in the least squares methods, the sum of the magnitude of the difference between field and numerical data, or some variant of these definitions. Irrespective of the definition used, numerical solution of the inverse problem is obtained as earth model that minimize the objective function.\n\nIn addition to the objective function, other constraints like known model parameters and known layer interfaces in some regions of the earth are also incorporated in the inverse modeling procedure. These constraints, according to Francis 2006, help to reduce non-uniqueness of the inversion solution by providing a priori information that is not contained in the inverted data while Cooke and Schneider (1983) reports their useful in controlling noise and when working in a geophysically well-known area.\n\nThe objective of mathematical analysis of inverse modeling is to cast the generalized linear inverse problem into a simple matrix algebra by considering all the components described in previous sections. viz; forward model, objective function etc. Generally, the numerically generated seismic data are non-linear functions of the earth model parameters. To remove the non-linearity and create a platform for application of linear algebra concepts, the forward model is linearized by expansion using a Taylor series as carried out below. For more details see Wiggins (1972), Cooke and Schneider (1983).\n\nConsider a set of formula_1 seismic field observations formula_2, for formula_3 and a set of formula_4 earth model parameters formula_5 to be inverted for, for formula_6. The field observations can be represented in either formula_7 or formula_8, where formula_9 and formula_7 are vectorial representations of model parameters and the field observations as a function of earth parameters. Similarly, for formula_11 representing guesses of model parameters, formula_12 is the vector of numerical computed seismic data using the forward model of Sec. 1.3. Taylor's series expansion of formula_7 about formula_14 is given below.\n\nformula_15 is called the difference vector in Cooke and Schneider (1983). It has a size of formula_16 and its components are the difference between the observed trace and the numerically computed seismic data. formula_17 is the corrector vector of size formula_18, while formula_19 is called the sensitivity matrix. It has a size of formula_20 and its comments are such that each column is the partial derivative of a component of the forward function with respect to one of the unknown earth model parameters. Similarly, each row is the partial derivative of a component of the numerically computed seismic trace with respect to all unknown model parameters.\n\nformula_12 is computed from the forward model, while formula_7 is the experimental data. Thus,formula_15 is a known quality. On the other hand, formula_17 is unknown and is obtained by solution of Eqn. 10. This equation is theoretically solvable only when formula_19 is invertible, that is, if it is a square matrix so that the number of observations formula_1 is equal to the number formula_4 of unknown earth parameters. If this is the case, the unknown corrector vector formula_17, is solved for as shown below, using any of the classical direct or iterative solvers for solution of a set of linear equations.\n\nIn most seismic inversion applications, there are more observations than the number of earth parameters to be inverted for, i.e. formula_29, leading to a system of equations that is mathematically over-determined. As a result, Eqn. 10 is not theoretically solvable and an exact solution is not obtainable. An estimate of the corrector vector is obtained using the least squares procedure to find the corrector vector formula_30 that minimizes formula_31, which is the sum of the squares of the error, formula_32.\n\nThe errorformula_32 is given by\n\nIn the least squares procedure, the corrector vector that minimizes formula_34 is obtained as below.\n\nThus,\n\nFrom the above discussions, the objective function is defined as either the formula_35 or formula_36 norm of formula_30 given by\nformula_38 or formula_39 or of formula_15 given by formula_41 or formula_42.\n\nThe generalized procedure for inverting any experimental seismic data for formula_43 or formula_44, using the mathematical theory for inverse modeling, as described above, is shown in Fig. 1 and described as follows.\n\nAn initial guess of the model impedance is provided to initiate the inversion process. The forward model uses this initial guess to compute a synthetic seismic data which is subtracted from the observed seismic data to calculate the difference vector.\n\n\nIrrespective of the variable to be inverted for, the earth’s impedance is a continuous function of depth (or time in seismic data) and for numerical linear inversion technique to be applicable for this continuous physical model, the continuous properties have to be discretized and/or sampled at discrete intervals along the depth of the earth model. Thus, the total depth over which model properties are to be determined is a necessary starting point for the discretization. Commonly, as shown in Fig. 3, this properties are sampled at close discrete intervals over this depth to ensure high resolution of impedance variation along the earth’s depth. The impedance values inverted from the algorithm represents the average value in the discrete interval.\n\nConsidering that inverse modeling problem is only theoretically solvable when the number of discrete intervals for sampling the properties is equal to the number of observation in the trace to be inverted, a high-resolution sampling will lead to a large matrix which will be very expensive to invert. Furthermore, the matrix may be singular for dependent equations, the inversion can be unstable in the presence of noise and the system may be under-constrained if parameters other than the primary variables inverted for, are desired. In relation to parameters desired, other than impedance, Cooke and Schneider (1983) gives them to include source wavelet and scale factor.\n\nFinally, by treating constraints as known impedance values in some layers or discrete intervals, the number of unknown impedance values to be solved for are reduced, leading to greater accuracy in the results of the inversion algorithm.\n\nWe start with an example to invert for earth parameter values from temperature depth distribution in a given earth region. Although this example does not directly relate to seismic inversion since no traveling acoustic waves are involved, it nonetheless introduces practical application of the inversion technique in a manner easy to comprehend, before moving on to seismic applications. In this example, the temperature of the earth is measured at discrete locations in a well bore by placing temperature sensors in the target depths. By assuming a forward model of linear distribution of temperature with depth, two parameters are inverted for from the temperature depth measurements.\n\nThe forward model is given by\nwhere formula_54. Thus, the dimension of formula_14 is 2 i.e. the number of parameters inverted for is 2.\n\nThe objective of this inversion algorithm is to find formula_9, which is the value of formula_57 that minimizes the difference between the observed temperature distribution and those obtained using the forward model of Eqn. 15. Considering the dimension of the forward model or the number of temperature observations to be formula_4, the components of the forward model is written as\n\nWe present results from Marescot (2010) for the case of formula_59 for which the observed temperature values at depths were formula_60 at formula_61 and formula_62 at formula_63. These experimental data were inverted to obtain earth parameter values of formula_64 and formula_65. For a more general case with large number of temperature observations, Fig. 4 shows the final linear forward model obtained from using the inverted values of formula_66 and formula_67. The figure shows a good match between experimental and numerical data.\n\nThis examples inverts for earth layer velocity from recorded seismic wave travel times. Fig. 5 shows the initial velocity guesses and the travel times recorded from the field, while Fig. 6a shows the inverted heterogeneous velocity model, which is the solution of the inversion algorithm obtained after 30 iterations. As seen in Fig. 6b, there is good comparison between the final travel times obtained from the forward model using the inverted velocity and the field record travel times. Using these solutions, the ray path was reconstructed and is shown to be highly tortuous through the earth model as shown in Fig. 7.\n\nThis example, taken from Cooke and Schneider (1983), shows inversion of a CMP seismic trace for earth model impedance (product of density and velocity) profile. The seismic trace inverted is shown in Fig. 8 while Fig. 9a shows the inverted impedance profile with the input initial impedance used for the inversion algorithm. Also recorded alongside the seismic trace is an impedance log of the earth region as shown in Fig. 9b. The figures show good comparison between the recorded impedance log and the numerical inverted impedance from the seismic trace.\n\n"}
{"id": "9736296", "url": "https://en.wikipedia.org/wiki?curid=9736296", "title": "Linguistic performance", "text": "Linguistic performance\n\nThe term linguistic performance was used by Noam Chomsky in 1960 to describe \"the actual use of language in concrete situations\". It is used to describe both the production, sometimes called \" parole\", as well as the comprehension of language. Performance is defined in opposition to \"competence\"; the latter describes the mental knowledge that a speaker or listener has of language.\n\nPart of the motivation for the distinction between performance and competence comes from speech errors: despite having a perfect understanding of the correct forms, a speaker of a language may unintentionally produce incorrect forms. This is because performance occurs in real situations, and so is subject to many non-linguistic influences. For example, distractions or memory limitations can affect lexical retrieval (Chomsky 1965:3), and give rise to errors in both production and perception or distractions. Such non-linguistic factors are completely independent of the actual knowledge of language, and establish that speakers' knowledge of language (their competence) is distinct from their actual use of language (their performance).\n\nPublished in 1916, Ferdinand de Saussure's \"Course in General Linguistics\" describes language as \"\"a system of signs that express ideas\".\" de Saussure describes two components of language: \"langue\" and \"parole\". \"Langue\" consists of the structural relations that define a language, which includes grammar, syntax and phonology. \"Parole\" is the physical manifestation of signs; in particular the concrete manifestation of \"langue\" as speech or writing. While \"langue\" can be viewed strictly as a system of rules, it is not an absolute system such that \"parole\" must utterly conform to \"langue\". Drawing an analogy to chess, de Saussure compares \"langue\" to the rules of chess that define how the game should be played, and \"parole\" to the individual choices of a player given the possible moves allowed within the system of rules.\n\nProposed in the 1950s by Noam Chomsky, generative grammar is an analysis approach to language as a structural framework of the human mind. Through formal analysis of components such as syntax, morphology, semantics and phonology, a generative grammar seeks to model the implicit linguistic knowledge with which speakers determine grammaticality.\n\nIn transformational generative grammar theory, Chomsky distinguishes between two components of language production: competence and performance. Competence describes the mental knowledge of a language, the speaker's intrinsic understanding of sound-meaning relations as established by linguistic rules. Performance – that is the actual observed use of language – involves more factors than phonetic-semantic understanding. Performance requires extra-linguistic knowledge such as an awareness of the speaker, audience and the context, which crucially determines how speech is constructed and analyzed. It is also governed by principles of cognitive structures not considered aspects of language, such as memory, distractions, attention, and speech errors.\n\nIn 1986, Chomsky proposed a distinction similar to the competence/performance distinction, entertaining the notion of an I-Language (internal language) which is the intrinsic linguistic knowledge within a native speaker and E-Language (external language) which is the observable linguistic output of a speaker. It was I-Language that Chomsky argued should be the focus of inquiry, and not E-Language.\n\nE-language has been used to describe the application of artificial systems, such as in calculus, set theory and with natural language viewed as sets, while performance has been used purely to describes applications of natural language. Between I-Language and competence, I-Language refers to our intrinsic faculty for language, competence is used by Chomsky as an informal, general term, or as term with reference to a specific competency such as \"grammatical competence\" or \"pragmatic competence\".\n\nJohn A. Hawkins's Performance-Grammar Correspondence Hypothesis (PGCH) states that the syntactic structures of grammars are conventionalized based on whether and how much the structures are preferred in performance. Performance preference is related to structure complexity and processing, or comprehension, efficiency. Specifically, a complex structure refers to a structure containing more linguistic elements or words at the end of the structure than at the beginning. It is this structural complexity that results in decreased processing efficiency since more structure requires additional processing. This model seeks to explain word order across languages based on avoidance of unnecessary complexity in favour of increased processing efficiency. Speakers make an automatic calculation of the Immediate Constituent(IC)-to-word order ratio and produce the structure with the highest ratio. Structures with a high IC-to-word order are structures that contain the fewest number of words required for the listener to parse the structure into constituents which results in more efficient processing.\n\nIn head-initial structures, which includes example SVO and VSO word order, the speaker's goal is to order the sentence constituents from least to most complex.\n\nSVO word order can be exemplified with English; consider the example sentences in (1). In (1a) three immediate constituents (ICs) are present in the verb phrase, namely VP, PP1 and PP2, and there are four words (\"went, to, London, in\") required to parse the VP into its constituents. Therefore, the IC-to-word ratio is 3/4=75%. In contrast, in (1b) the VP is still composed of three ICs but there are now six words that are required to determine the constituent structure of the VP (\"went, in, the, late, afternoon, to\"). Thus, the ratio for (1b) is 3/6 = 50%. Hawkins proposes that speakers prefer to produce (1a) since it has a higher IC-to-word ratio and this leads to faster and more efficient processing.\n\nHawkins supports the above analysis by providing performance data to demonstrate the preference speakers have for ordering short phrases before long phrases when producing head-initial structures. The table based on English data, below, illustrates that the short prepositional phrase (PP1) is preferentially ordered before the long PP (PP2) and that this preference increases as the size differential between the two PPs increases. For example, 60% of the sentences are ordered short (PP1) to long (PP2) when PP2 was longer than PP1 by 1 word. In contrast, 99% of the sentences are ordered short to long when PP2 is longer than PP1 by 7+ words.\n\nEnglish prepositional phrase orderings by relative weight \n\nHawkins argues that the preference for short followed by long phrases applies to all languages that have head-initial structuring. This includes languages with VSO word order such as from Hungarian. By calculating the IC-to-word ratio for the Hungarian sentences in the same way as was done for the English sentences, 2a. emerges as having a higher ratio than 2b.\n\nThe VP and its constituents in 4. are constructed from their heads on the right. This means that the number of words used to calculate the ratio is counted from the head of the first phrase (PP in 3a. and NP in 3b.) to the verb (as indicated in bold above). The IC-to-word ratio for the VP in 3a. is 3/5=60% while the ratio for the VP in 3b. is 3/4=75%. Therefore, 3b. should be preferred by Japanese speakers since it has a higher IC-to-word ratio which leads to faster parsing of sentences by the listener.\n\nThe performance preference for long to short phrase ordering in SVO languages is supported by performance data. The table below shows that production of long to short phrases is preferred and that this preference increases as the size of the differential between the two phrases increases. For example, ordering of the longer 2ICm (where ICm is either a direct object NP with an accusative case particle or a PP constructed from the right periphery) before the shorter 1ICm is more frequent, and the frequency increases to 91% if the 2ICm is longer than the 1ICm by 9+ words.\n\nJapanese NPo and PPm orderings by relative weight \n\n[[Tom Wasow]] proposes that word order arises as a result of utterance planning benefiting the speaker. He introduces the concepts of early versus late commitment, where commitment is the point in the utterance where it becomes possible to predict subsequent structure. Specifically, early commitment refers to the commitment point present earlier in the utterance and late commitment refers to the commitment point present later in the utterance. He explains that early commitment will favour the listener since early prediction of subsequent structure enables faster processing. Comparatively, late commitment will favour the speaker by postponing decision making, giving the speaker more time to plan the utterance. Wasow illustrates how utterance planning influences syntactic word order by testing early versus late commitment in [[Heavy NP shift|heavy-NP shifted]] (HNPS) sentences. The idea is to examine the patterns of HNPS to determine if the performance data show sentences that are structured to favour the speaker or the listener.\n\nThe following examples illustrate what is meant by early versus late commitment and how heavy-NP shift applies to these sentences. Wasow looked at two types of verbs:\n\nVt ([[transitive verbs]]): require NP objects.\n\nIn 4a. no heavy-NP shift has been applied. The NP is available early but does not provide any additional information about the sentence structure – the \"to\" appearing late in the sentence is an example of late commitment. In contrast, in 4b., where heavy-NP shift has shifted the NP to the right, as soon as \"to\" is uttered the listener knows that the VP must contain the NP and a PP. In other words, when \"to\" is uttered it allows the listener to predict the remaining structure of the sentence early on. Thus for transitive verbs HNPS results in early commitment and favors the listener.\n\nVp ([[prepositional verbs]]): can take an NP object or an immediately following PP with no NP object\n\nNo HNPS has been applied to 5a. In 5b. the listener needs to hear the word \"something\" in order to know that the utterance contains a PP and an NP since the object NP is optional but \"something\" has been shifted to later in the sentence. Thus for prepositional verbs HNPS results in late commitment and favours the speaker.\n\nBased on the above information Wasow predicted that if sentences are constructed from the speaker's perspective then heavy-NP shift would rarely apply to sentences containing a transitive verb but would apply frequently to sentences containing a prepositional verb. The opposite prediction was made if sentences are constructed from the listener's perspective.\n\nTo test his predictions Wasow analyzed performance data (from corpora data) for the rates of occurrence of HNPS for Vt and Vp and found HNPS occurred twice as frequently in Vp than in Vt, therefore supporting the predictions made from the speaker's perspective. In contrast, he did not find evidence in support of the predictions made based on the listener's perspective. In other words, given the data above, when HNPS is applied to sentences containing a transitive verb the result favors the listener. Wasow found that HNPS applied to transitive verb sentences is rare in performance data thus supporting the speaker's perspective. Additionally, when HNPS is applied to prepositional verb structures the result favors the speaker. In his study of the performance data, Wasow found evidence of HNPS frequently applied to prepositional verb structures further supporting the speaker's perspective. Based on these findings Wasow concludes that HNPS is correlated with the speaker's preference for late commitment thereby demonstrating how speaker performance preference can influence word order.\n\nWhile the dominant views of grammar are largely oriented towards competence, many, including Chomsky himself, have argued that a complete model of grammar should be able to account for performance data. But while Chomsky argues that competence should be studied first, thereby allowing further study of performance, some systems, such as constraint grammars are built with performance as a starting point (comprehension, in the case of constraint grammars While traditional models of generative grammar have had a great deal of success in describing the structure of languages, they have been less successful in describing how language is interpreted in real situations. For example, traditional grammar describes a sentence as having an \"underlying structure\" which is different from the \"surface structure\" which speakers actually produce. In a real conversation, however, a listener interprets the meaning of a sentence in real time, as the surface structure goes by. This kind of on-line processing, which accounts for phenomena such as finishing another person's sentence, and starting a sentence without knowing how it is going to finish, is not directly accounted for in traditional generative models of grammar. Several alternative grammar models exist which may be better able to capture this surface-based aspect of linguistic performance, including\nConstraint Grammar, Lexical Functional Grammar, and Head-driven phrase structure grammar.\n\nErrors in linguistic performance not only occur in children newly acquiring their native language, second language learners, those with a disability or an acquired brain injury but among competent speakers as well. Types of performance errors that will be of focus here are those that involve errors in syntax, other types of errors can occur in the phonological, semantic features of words, for further information see speech errors. Phonological and semantic errors can be due to the repetition of words, mispronunciations, limitations in verbal working memory, and length of the utterance. Slips of the tongue are most common in spoken languages and occur when the speaker either: says something they did not mean to; produces the incorrect order of sounds or words; or uses the incorrect word. Other instances of errors in linguistic performance are slips of the hand in signed languages, slips of the ear which are errors in comprehension of utterances and slips of the pen which occur while writing. Errors of linguistic performance are perceived by both the speaker and the listener and can therefore have many interpretations depending on the persons judgement and the context in which the sentence was spoken.\n\nIt is proposed that there is a close relation between the linguistic units of grammar and the psychological units of speech which implies that there is a relation between linguistic rules and the psychological processes that create utterances. Errors in performance can occur at any level of these psychological processes. Lise Menn proposes that there are five levels of processing in speech production, each with its own possible error that could occur. According to the proposed speech processing structure by Menn an error in the syntactic properties of an utterance occurs at the positional level.\n\nAnother proposal for the levels of speech processing is made by Willem J. M. Levelt to be structured as so: \nLevelt (1993) states that we as speakers are unaware of most of these levels of performance such as articulation, which includes the movement and placement of the articulators, the formulation of the utterance which includes the words selected and their pronunciation and the rules which must be followed for the utterance to be grammatical. The levels speakers are consciously aware is the intent of the message which occurs at the level of conceptualization and then again at self-monitoring which is when the speaker would become aware of any errors that may have occurred and correct themselves.\n\nOne type of slip of the tongue which cause an error in the syntax of the utterance are called transformational errors. Transformational errors are a mental operation proposed by Chomsky in his Transformational Hypothesis and it has three parts which errors in performance can occur. These transformations are applied at the level of the underlying structures and predict the ways in which an error can occur. \n\nStructural Analysis\nerrors can occur due to the application of (a) the rule misanalyzing the tense marker causing the rule to apply incorrectly, (b) the rule not being applied when it should or (c) a rule being applied when it should not.\n\nThis example from Fromkin (1980) demonstrates a rule misanalyzing the tense marker and for subject-auxiliary inversion to be incorrectly applied. The subject-auxiliary inversion is misanalyzed as to which structure it applies, applying without the verb \"be\" in the tense as it moves to the C position. This causes \"do-support\" to occur and the verb to lack tense causing the syntactic error.\n\nThe following example from Fromkin (1980) demonstrates how a rule is being applied when it should not. The subject-auxiliary inversion rule is omitted in the error utterance, causing affix-hopping to occur and putting the tense onto the verb \"say\" creating the syntactic error. In the target the subject-auxiliary rule and then do-support applies creating the grammatically correct structure.\n\nThis example from Fromkin (1980) shows how a rule is being applied when it should not. The subject-auxiliary inversion and do-support has applied to an idiomatic expression causing the insertion of \"do\" when it should not be applied in the ungrammatical utterance.\n\nStructural Change\nErrors can occur in the carrying out of rules, even though the analysis of the phrase marker is done correctly. This can occur when the analysis requires multiple rules to occur.\n\nThe following example from Fromkin (1980) shows the relative clause rule copies the determiner phrase \"a boy\" within the clause and this causes front attaching to the Wh-marker. Deletion is then skipped, leaving the determiner phrase in the clause in the error utterance causing it to be ungrammatical. \n\nConditions errors restrict when the rule can and cannot be applied.\n\nThis last example from Fromkin (1980) shows that a rule was applied under a certain condition in which it is restricted. The subject-auxiliary inversion rule cannot apply to embedded clauses. In the case of this example it has causing for the syntactic error. \n\nA study of deaf Italians found that the second person singular of indicatives would extend to corresponding forms in imperatives and negative imperatives. \n\nThe following is an example taken from Dutch data in which there is verb omission in the embedded clause of the utterance (which is not allowed in Dutch), resulting in a performance error.\n\nA study done with Zulu speaking children with a language delay displayed errors in linguistic performance of lacking proper passive verb morphology.\n\nThe linguistic components of American Sign Language (ASL) can be broken down into four parts; the hand configuration, place of articulation, movement and other minor parameters. Hand configuration is determined by the shape of the hand, fingers and thumbs and is specific to the sign that is being used. It allows the signer to articulate what they are wanting to communicate by extending, flexing, bending or spreading the digits; the position of the thumb to the fingers; or the curvature of the hand. However, there are not an infinite amount of possible hand configurations, there are 19 classes of hand configuration primes as listed by the \"Dictionary of American Sign Language\". Place of articulation is the particular location that the sign is being performed known as the \"signing place\". The \"signing place\" can be the whole face or a particular part of it, the eyes, nose, cheek, ear, neck, trunk, any part of the arm, or the neutral area in front of the signers head and body. Movement is the most complex as it can be difficult to analyze. Movement is restricted to directional, rotations of the wrist, local movements of the hand and interactions of the hands. These movements can occur singularly, in sequence, or simultaneously. Minor parameters in ASL include contacting region, orientation and hand arrangement. They are subclasses of hand configuration.\nPerformance errors resulting in ungrammatical signs can result due to processes that change the hand configuration, place, movement or other parameter of the sign. These processes can be anticipation, preservation, or metathesis. Anticipation is caused when some characteristic of the next sign is incorporated into the sign that is presently being performed. Preservation is the opposite of anticipation where some characteristic of the preceding sign is carried over into the performance of the next sign. Metathesis occurs when two characteristics of adjacent signs are combined into one in the performance of both signs. Each of these errors will result in an incorrect sign being performed. This could result in either a different sign being performed instead of the intended one, or nonexistent signs which forms are possible and those which forms are not possible due to the structural rules. These are the main types of performance errors in sign language however on the rare occasion there is also the possibility of errors in the order of the signs performed resulting in a different meaning than what the signer intended.\n\nUnacceptable Sentences\nare ones which, although are grammatical, are not considered proper utterances. They are considered unacceptable due to the lack of our cognitive systems to process them. Speakers and listeners can be aided in the performance and processing of these sentences by eliminating time and memory constraints, increasing motivation to process these utterances and using pen and paper. In English there are three types of sentences that are grammatical but are considered unacceptable by speakers and listeners.\nWhen a speaker makes an utterance they must translate their ideas into words, then syntactically proper phrases with proper pronunciation. The speaker must have prior world knowledge and an understanding of the grammatical rules that their language enforces. When learning a second language or with children acquiring their first language, speakers usually have this knowledge before they are able to produce them. Their speech is usually slow and deliberate, using phrases they have already mastered, and with practice their skills increase. Errors of linguistic performance are judged by the listener giving many interpretations if an utterance is well-formed or ungrammatical depending on the individual. As well the context in which an utterance is used can determine if the error would be considered or not. When comparing \"Who must telephone her?\" and \"Who need telephone her?\" the former would be considered the ungrammatical phrase. However, when comparing it to \"Who want telephone her?\" it would be considered the grammatical phrase. The listener may also be the speaker. When repeating sentences with errors if the error is not comprehended then it is performed. As well if the speaker does notice the error in the sentence they are supposed to repeat they are unaware of the difference between their well-formed sentence and the ungrammatical sentence.\nAn unacceptable utterance can also be performed due to a brain injury. Three types of brain injuries that could cause errors in performance were studied by Fromkin are dysarthria, apraxia and literal paraphasia. Dysarthria is a defect in the neuromuscular connection that involves speech movement. The speech organs involved can be paralyzed or weakened, making it difficult or impossible for the speaker to produce a target utterance. Apraxia is when there is damage to the ability to initiate speech sounds with no paralysis or weakening of the articulators. Literal paraphasia causes disorganization of linguistic properties, resulting in errors of word order of phonemes. Having a brain injury and being unable to perform proper linguistic utterances, some individuals are still able to process complex sentences and formulate syntactically well formed sentences in their mind.\nChild productions when they are acquiring language are full of errors of linguistic performance. Children must go from imitating adult speech to create new phrases of their own. They will need to use their cognitive operations of the knowledge of their language they are learning to determine the rules and properties of that language. The following are examples of errors in English speaking children's productions. \n\nIn an elicited production experiment a child, Adam, was prompted to ask questions to an Old Lady\n\nThe most commonly used measure of syntax complexity is the mean length of utterance, also known as MLU. This measure is independent from how often children talk and focuses on the complexity and development of their grammatical systems, including morphological and syntactic development. The number representing a person's MLU corresponds to the complexity of the syntax being used. In general, as the MLU increases, the syntactic complexity also increases. Typically, the average MLU corresponds to a child's age due to their increase in working memory, which allows for sentences to be of greater syntactic complexity. For example, the average MLU of a 7 year old child is 7 words. However, children show more individual variability of syntactic performance with more complex syntax. Complex syntax have a higher number of phrases and clause levels, therefore adding more words to the overall syntactic structure. Seeing as there are more individual differences in MLU and syntactic development as children get older, MLU is particularly used to measure grammatical complexity among school-aged children. Other types of segmentation strategies for discourse are the T-unit and C-unit (communicative unit). If these two measurements are used to account for discourse, the average length of the sentence will be lower than if MLU is used alone. Both the T-units and C-units count each clause as a new unit, hence a lower number of units.\n\nTypical MLU per age group can be found in the following table, according to Roger Brown's five stages of syntactic and morphological development:\nHere are the steps for calculating MLU:\n\n\nHere's an example of how to calculate MLU:\n\nIn total there are 17 morphemes in this data set. In order to find the MLU, we divide the total number of morphemes (17) by the total number of utterances (4). In this particular data set, the mean length of utterance is 17/4 = 4.25.\n\nClause density refers to the degree to which utterances contain dependent clauses. This density is calculated as a ratio of the total number of clauses across sentences, divide by the number of sentences in a discourse sample. For example, if the clause density is 2.0, the ratio would indicate that the sentence being analyzed has 2 clauses on average: one main clause and one subordinate clause.\n\nHere is an example of how clause density is measured, using T-units, adapted from Silliman & Wilkinson 2007:\n\nIndices track structures to show a more comprehensive picture of a person's syntactic complexity. Some examples of indices are Development Sentence Scoring, the Index of Productive Syntax and the Syntactic Complexity Measure.\n\nDevelopmental Sentence Scoring is another method to measure syntactic performance as a clinical tool. In this indice, each consecutive utterance, or sentence, elicited from a child is scored. This is a commonly applied measurement of syntax for first and second language learners, with samples gathered from both elicited and spontaneous oral discourse. Methods for eliciting speech for these samples come in many forms, such having the participant answering questions or re-telling a story. These elicited conversations are commonly tape-recorded for playback during analysis to see how well the person can incorporate syntax among other linguistic cues. For every utterance elicited, the utterance will receive one point if it is a correct form used in adult speech. A score of 1 indicates the least complex syntactic form in the category, whereas a higher score reflects higher level grammaticality. Points are specifically awarded to an utterance based on whether or not it contains any of the eight categories outlined below.\n\nSyntactic categories measured by developmental sentence scoring with examples:\n\nIn particular, those categories that appear the earliest in speech receive a lower score, whereas later-appearing categories receive a higher score. If an entire sentence is correct according to adult-like forms, then the utterance would receive an extra point. The eight categories above are the most commonly used structures in syntactic formation, thus structures such as possessives, articles, plurals, prepositional phrases, adverbs and descriptive adjectives were omitted and not scored. Additionally, the scoring system is arbitrary when applied to certain structures. For example, there is no indication as to why \"if\" would receive four points rather than five. The scores of all the utterances are totalled in the end of the analysis and then averaged to get a final score. This means that the individual's final score reflects their entire syntactic complexity level, rather than syntactic level in a specific category. The main advantage of development sentence scoring is that the final score represents the individual's general syntactic development and allows for easier tracking of changes in language development, making this tool effective for longitudinal studies.\n\nSimilar to Development Sentence Scoring, the Index of Productive Syntax evaluates the grammatical complexity of spontaneous language samples. After age 3, Index of Productive Syntax becomes more widely used than MLU to measure syntactic complexity in children. This is because at around age 3, MLU does not distinguish between children of similar language competency as well as Index of Productive Syntax does. For this reason, MLU is initially used in early childhood development to track syntactic ability, then Index of Productive Syntax is used to maintain validity. Individual utterances in a discourse sample are scored based on the presence of 60 different syntactic forms, placed more generally under four subscales: noun phrase, verb phrase, question/negation and sentence structure forms. After a sample is recorded, a corpus is then formed based on 100 utterance transcriptions with 60 different language structures being measured in each utterance. Not included in the corpus are imitations, self-repetitions and routines, which constitute language that does not represent productive language usage. In each of the four sub-scales previously mentioned, the first two unique occurrences of a form are scored. After this, occurrences of a sub-scale are not scored. However, if a child has mastered a complex syntax structure earlier than expected, they will receive extra points.\n\nThe six main tasks in standardized testing for syntax:\n\nSome of the common standardized tests for measuring syntactic performance are the TOLD-2 Intermediate (Test of Language Development), the TOAL-2 (Test of Adolescent Language) and the CELF-R (Clinical Evaluation of Language Fundamentals, Revised Screening Test).\n\n"}
{"id": "23685094", "url": "https://en.wikipedia.org/wiki?curid=23685094", "title": "List of unusual units of measurement", "text": "List of unusual units of measurement\n\nAn unusual unit of measurement is a unit of measurement that does not form part of a coherent system of measurement; especially in that its exact quantity may not be well known or that it may be an inconvenient multiple or fraction of base units in such systems. This definition is deliberately not exact since it might seem to encompass units such as the week or the light-year which are quite \"usual\" in the sense they are often used; if they are used out of context, they may be \"unusual\", as demonstrated by the Furlong/Firkin/Fortnight (FFF) system of units. Many of the unusual units of measurements listed here are colloquial measurements, units devised to compare a measurement to common and familiar objects.\n\nOne rack unit (U) is and is used to measure rack-mountable audiovisual and computing equipment (hence, 24000U = 1 Verst.) Rack units are typically denoted without a space between the number of units and the 'U'. Thus a 4U server enclosure (case) is high.\n\nThe hand is a non-SI unit of length equal to exactly . It is normally used to measure the height of horses in some English-speaking countries, including Australia, Canada, the United Kingdom, Ireland and the United States.\n\nThe smoot is a nonstandard, humorous unit of length created as part of an MIT fraternity prank. It is named after Oliver R. Smoot, a fraternity pledge to Lambda Chi Alpha, who in October 1958 lay down repeatedly on the Harvard Bridge (between Boston and Cambridge, Massachusetts) so that his fraternity brothers could use his height to measure the length of the bridge (it was 364.4 Smoots, plus or minus an ear)\n\nThe light-nanosecond was popularized as a unit of distance by Grace Hopper as the distance which a photon could travel in one billionth of a second (roughly 30 cm or one foot): \"The speed of light is one foot per nanosecond.\" In her speaking engagements, she was well known for passing out light-nanoseconds of wire to the audience, and contrasting it with light-microseconds (a coil of wire 1,000 times as long) and light-picoseconds (the size of ground black pepper). Over the course of her life, she found many uses for this visual aid, including demonstrating the waste of sub-optimal programming, illustrating advances in computer speed, and simply giving young scientists and policy makers the ability to conceptualize the magnitude of very large and small numbers.\n\nA metric foot (defined as 300 mm, or about 11.8 inches) has been used occasionally in the UK but has never been an official unit.\n\nHorses are used to measure distances in horse racing – a \"horse length\" (shortened to merely a \"length\" when the context makes it obvious) equals roughly . Shorter distances are measured in fractions of a \"horse length\"; also common are measurements of a full or fraction of a \"head\", a \"neck\", or a \"nose\".\n\nIn rowing races such as the Oxford and Cambridge Boat Race, the margin of victory and of defeat is expressed in fractions and multiples of \"lengths\". The length of a rowing eight is about . A shorter distance is the \"canvas\", which is the length of the covered part of the boat between the bow and the bow oarsman.\n\nAn American football field is usually understood to be long, though it is technically when including the two long end zones. The field is wide. \n\nA Canadian football field is wide and long, including two long end zones.\n\nMedia in the UK also use the football pitch as a unit of length, although the area of the association football pitch is not fixed, but may vary within limits of in length and in width. The usual size of a football pitch is , the dimensions used for matches in the UEFA Champions League.\n\nA city block (in most US cities) is between . In Manhattan, the measurement \"block\" usually refers to a north-south block, which is . Sometimes people living in cities with a regularly spaced street grid will speak of \"long blocks\" and \"short blocks\". Within a typical large North American city, it is often only possible to travel along east-west and north-south streets, so travel distance between two points is often given in the number of blocks east-west plus the number north-south (known to mathematicians as the \"Manhattan Metric\").\n\nThroughout the world, well-known tall structures such as the Empire State Building () are often used as comparative measurements of height.\n\nIn the UK, well-known structures such as the Blackpool Tower (), are commonly used by British newspapers or reference books to give the comparative heights of buildings or, occasionally, mountains.\n\nIn Canada, the Toronto CN Tower () is used as a unit of length.\n\nIn France, the Eiffel Tower and Mont Saint-Michel are commonly used as units of height or volume. The Montparnasse Tower (Paris) is also – but less – used as a measurement for modern buildings, mostly in Parisian Area.\n\nThe circumference of a great circle of the Earth (about ) is often compared to large distances. For example, one might say that a large number of objects laid end-to-end at the equator \"would circle the Earth four and a half times\". According to WGS-84, the circumference of a circle through the poles (twice the length of a meridian) is and the length of the equator is . Despite the fact that the difference (0.17%) between the two is insignificant at the low precision that these quantities are typically given to, it is nevertheless often specified as being at the equator.\n\nThe definitions of both the nautical mile and the kilometre were originally derived from the Earth's circumference as measured through the poles. The nautical mile was defined as a minute of arc of latitude measured along any meridian. A circle has 360 degrees, and each degree is 60 minutes, so the nautical mile was defined as of the Earth's circumference, or about 1,852.22 metres. However, by international agreement, it is now defined to be exactly .\n\nThe metre was originally defined as of the distance from a pole to the equator, or as of the Earth's circumference as measured through the poles. This standard made the historical metre 0.0197% longer than the modern standard metre, which is calculated based on the distance covered by light in a vacuum in a set amount of time.\n\nThe distance between the Earth's and the Moon's surfaces is, on average, approximately . This distance is sometimes used in the same manner as the circumference of the Earth; that is, one might say that a large number of objects laid end-to-end \"would reach all the way to the Moon and back two-and-a-half times\".\n\nThe abbreviation for the Earth-to-Moon distance is \"LD\" which stands for \"Lunar Distance,\" used in astronomy to express close approaches of Earth by minor planets.\n\nThe siriometer is a rarely used astronomical measure equal to one million astronomical units, i.e., one million times the average distance between the Sun and Earth. This distance is equal to about 15.8 light-years, 149.6 Pm or 4.8 parsecs, and is about twice the distance from Earth to the star Sirius.\n\nOne barn is 10 square metres, about the cross-sectional area of a uranium nucleus. The name probably derives from early neutron-deflection experiments, when the uranium nucleus was described, and the phrases \"big as a barn\" and \"hit a barn door\" were used. Additional units include the microbarn (or \"outhouse\") and the yoctobarn (or \"shed\").\n\nOne brass is area (used in measurement of work done or to be done, such as plastering, painting, etc.). It is also equal, however, to of estimated or supplied loose material, such as sand, gravel, rubble, etc. This unit is prevalent in construction industry in India.\n\nThe \"square\" is an Imperial unit of area that is used in the construction industry in North America, and was historically used in Australia by real estate agents. One square is equal to . A roof's area may be calculated in square feet, then converted to squares.\n\nIn Ireland, before the 19th Century, a \"cow's grass\" was a measurement used by farmers to indicate the size of their fields. A cow's grass was equal to the amount of land that could produce enough grass to support a cow.\n\nA football pitch, or field, can be used as a man-in-the-street unit of area. The standard FIFA football pitch is long by wide (); FIFA allows for a variance of up to in length and in width in either direction (and even larger discretions if the pitch is not used for international competition), which generally results in the association football pitch generally only being used for order of magnitude comparisons.\n\nAn American football field, including both end zones, is , or (). A Canadian football field is wide and long with end zones adding a combined to the length, making it or .\n\nAn Australian rules football field may be approximately (or more) long goal to goal and (or more) wide, although the field's elliptical nature reduces its area to a certain extent. A football field has an area of approximately , twice the area of a Canadian football field and three times that of an American football field.\n\nA morgen (\"morning\" in Dutch and German) was approximately the amount of land tillable by one man behind an ox in the morning hours of a day. This was an official unit of measurement in South Africa until the 1970s, and was defined in November 2007 by the South African Law Society as having a conversion factor of 1 Morgen = . This unit of measure was also used in the Dutch colonial province of New Netherland (later New York and parts of New England).\n\nThe area of a familiar country, state or city is often used as a unit of measure, especially in journalism.\n\nEqual to 20,779 km (8,023 sq mi), the country of Wales is used in phrases such as \"an area the size of Wales\" or \"twice the area of Wales\". England is 6.275 times the size of Wales, and Scotland is roughly four times the size of Wales. Ireland is four times larger than Wales, and France is about twenty-five times larger.\n\nIn older British and Commonwealth atlases, it was common to show a known area at the same scale, and the usual area to show was either Wales for smaller scales, or Great Britain for larger areas.\n\nThe British comedy show \"The Eleven O'Clock Show\" parodied the use of this measurement, by introducing a news article about an earthquake in Wales, stating that an area the size of Wales was affected. The Radio 4 programme \"More or Less\" introduced the idea of \"kilowales\" – an area 1,000 times the size of Wales. \"The Register\" introduced the nanowales (20.78 m).\n\nThe measurement has been adopted by rainforest conservation charity Size of Wales, aiming to conserve an area of rainforest equating to the area of Wales. On 1 March 2013, the charity announced that they had succeeded in conserving an area of rainforest the size of Wales and will continue to operate to sustain and increase the protected area.\n\nIn the United States the area of the smallest state, Rhode Island (), the largest of the contiguous 48 states, Texas (), and, less commonly, Alaska () are used in a similar fashion. Antarctica's Larsen B ice shelf was approximately the size of Rhode Island until it broke up in 2002. In the 1979 movie \"The China Syndrome\", radiation is expected to contaminate \"an area the size of Pennsylvania\". Any state may be used in this fashion to describe the area of another country.\n\nThe US Central Intelligence Agency uses Washington, D.C. () as a comparison for city-sized objects.\n\nIn the Netherlands, its smallest province, Utrecht (), is often used as a comparison for regions in general.\n\nThe country of Belgium () has also often been used when comparing areas, to the point where it has been regarded as a meme and where there is a website dedicated to notable areas which have been compared to that of Belgium.\n\nThe Isle of Wight (), an island off the south coast of mainland England, is commonly used to define smaller areas.\n\nIn Denmark, the island of Bornholm (588 square kilometers) is often used to describe the size of an area.\n\nIn Germany, the Saarland () is often used to define areas.\n\nIn Brazil, it is common to compare relatively small areas to the state of Sergipe (), the smallest in the country. Smaller areas are sometimes compared to the cities of São Paulo () or Rio de Janeiro ().\n\nA metric ounce is an approximation of the imperial ounce, US dry ounce, or US fluid ounce. These three customary units vary. However, the metric ounce is usually taken as 25 or 30 ml when volume is being measured, or grams when mass is being measured.\n\nThe US Food and Drug Administration (FDA) defines the \"food labeling ounce\" as 30 ml, slightly larger than the 29.6 ml fluid ounce.\n\nThe shot is a liquid volume measure that varies from country to country and state to state depending on legislation. It is routinely used for measuring strong liquor or spirits when the amount served and consumed is smaller than the more common measures of alcoholic \"drink\" and \"pint\". There is a legally defined maximum size of a serving in some jurisdictions. The size of a \"single\" shot is . The smaller \"pony\" shot is . According to Encyclopædia Britannica Almanac 2009, a pony is 0.75 fluid ounces of liquor. According to Wolfram Alpha, one pony is 1 U.S. fluid ounce. \"Double\" shots (surprisingly not always the size of two single shots, even in the same place) are . In the UK, spirits are sold in shots of either 25 ml (approximating the old fluid ounce) or 35 ml.\n\nA board foot is a United States and Canadian unit of volume, used for lumber. It is equivalent to (). It is also found in the unit of density \"pounds per board foot\". In Australia and New Zealand the terms \"super foot\" or \"superficial foot\" were formerly used for this unit.\n\nA system of measure for timber in the round (standing or felled), now largely superseded by the metric system except in measuring hardwoods in certain countries. Its purpose is to estimate the value of sawn timber in a log, by measuring the unsawn log and allowing for wastage in the mill. Following the so-called \"quarter-girth formula\" (the square of one quarter of the circumference in inches multiplied by of the length in feet), the notional log is four feet in circumference, one inch of which yields the hoppus board foot, 1 foot yields the hoppus foot, and 50 feet yields a hoppus ton. This translates to a hoppus foot being equal to . The hoppus board foot, when milled, yields about one board foot. The volume yielded by the quarter-girth formula is 78.54% of cubic measure (i.e. 1 ft = 0.7854 h ft; 1 h ft = 1.273 ft).\n\nA cubic ton is an antiquated measure of volume, varying based on the commodity from about . It is now only used for lumber, for which one cubic ton is equivalent to .\n\nThe cord is a unit of measure of dry volume used in Canada and the United States to measure firewood and pulpwood. A cord is the amount of wood that, when \"ranked and well stowed\" (arranged so pieces are aligned, parallel, touching and compact), occupies a volume of . This corresponds to a well-stacked woodpile, 4 feet deep by 4 feet high by 8 feet wide , or any other arrangement of linear measurements that yields the same volume. A more unusual measurement for firewood is the \"rick\" or face cord. It is stacked deep with the other measurements kept the same as a cord, making it of a cord; however, regional variations mean that its precise definition is nonstandardized.\n\nThe twenty-foot equivalent unit is the volume of the smallest standard shipping container. It is equivalent to . Larger intermodal containers are commonly described in multiples of TEU, as are container ship capacities.\n\nThe approximate volume of a double-decker bus, abbreviated to DDB, has been used informally to describe the size of hole created by a major sewer collapse. For example, a report might refer to \"a 4 DDB hole\".\n\nAn acre-foot is a unit of volume commonly used in the United States in reference to large-scale water resources, such as reservoirs, aqueducts, canals, sewer flow capacity, irrigation water and river flows. It is defined by the volume of one acre of surface area to a depth of one foot (43,560 ft) which is about .\n\nFor larger volumes of liquid, one measure commonly used in the media in many countries is the Olympic-size swimming pool. A Olympic swimming pool, built to the FR3 minimum depth of would hold . The US National Institute of Standards and Technology (NIST) defines the Olympic swimming pool as 1 million litres, which is the approximate volume of the smaller FR2 pool.\n\nThe Royal Albert Hall, a large concert hall, is sometimes used as a unit of volume in the UK, for example when referring to volumes of rubbish placed in landfill. It is famously used in the line \"Now they know how many holes it takes to fill the Albert Hall.\" in The Beatles song \"A Day in the Life\". The volume of the auditorium is between 3 and 3.5 million cubic feet (between 85,000 and 99,000 cubic metres).\n\nA common measure of volume in Australia, and in the state of Victoria in particular, is the Melbourne Cricket Ground, the largest stadium in Australia and 13th largest in the world. The volume of the Melbourne Cricket Ground is 1,574,000 cubic metres, or about 630 Olympic swimming pools. The seating capacity of the Melbourne Cricket Ground (approximately 95,000 ) is also used as a unit measure of the .\n\nA unit of volume used in Australia for water. One Sydney Harbour, also called a \"Sydharb (or sydarb)\", is the amount of water in Sydney Harbour: approximately 562 gigalitres (562,000,000 cubic metres, or 0.562 of a cubic kilometre); or in terms of the more unusual measures above, about 357 Melbourne Cricket Grounds, 238,000 Olympic Swimming pools, or 476,000 acre-feet.\n\nWith a volume measure approximately 4 orders of magnitude greater than a Sydharb, the volume of the Grand Canyon may be used to visualize even larger things, like the magma chamber underneath Yellowstone and other things. According to the National Park Service, the volume of the Grand Canyon is which is ().\n\nThe volume of water which flows in one unit of time through an orifice one inch square or in diameter. The size of the unit varies from one place to another.\n\nIn 1793, the French term \"grave\" (from \"gravity\") was suggested as the base unit of mass for the metric system. In 1795, however, the name \"kilogramme\" was adopted instead.\n\nThe mass of an old bag of cement was one hundredweight ~ 112 lb, approximately 50 kg. The amount of material that, say, an aircraft could carry into the air is often visualised as the number of bags of cement that it could lift. In the concrete and petroleum industry, however, a bag of cement is defined as 94 pounds (~ 42.6 kg), because it has an apparent volume close to 1 cubic foot. When ready-mix concrete is specified, a \"bag mix\" unit is used as if the batching company mixes 5 literal bags of cement per cubic yard (or cubic metre) when a \"5 bag mix\" is ordered.\n\nWhen reporting on the masses of extrasolar planets, astronomers often discuss them in terms of multiples of Jupiter's mass ( = 1.9  kg).\nFor example, \"Astronomers recently discovered a planet outside our Solar System with a mass of approximately 3 Jupiters.\" Furthermore, the mass of Jupiter is nearly equal to one thousandth of the mass of the Sun.\n\nSolar mass ( = ) is also often used in astronomy when talking about masses of stars or galaxies; for example, Alpha Centauri A has the mass of 1.1 suns, and the Milky Way has a mass of approximately .\n\nSolar mass also has a special use when estimating orbital periods and distances of 2 bodies using Kepler's laws: \"a = MT\", where \"a\" is length of semi-major axis in AU, \"T\" is orbital period in years and \"M\" is the combined mass of objects in . In case of planet orbiting a star, \"M\" can be approximated to mean the mass of the central object. More specifically in the case of Sun and Earth the numbers reduce to \"M\" ~ 1, \"a\" ~ 1 and \"T\" ~ 1.\n\nGeorge Gamow discussed measurements of time such as the \"light-mile\" and \"light-foot\", the time taken for light to travel the specified unit distance, defined by \"reversing the procedure\" used in defining a light-year. One light-nanosecond is roughly 30 centimeters (29.9792458 cm exactly), or about a foot (30.48 cm).\n\nIn nuclear engineering and astrophysics contexts, the shake is sometimes used as a conveniently short period of time. 1 shake is defined as 10 nanoseconds.\n\nIn computing, the jiffy is the duration of one tick of the system timer interrupt. Typically, this time is 0.01 seconds, though in some earlier systems (such as the Commodore 8-bit machines) the jiffy was defined as of a second, roughly equal to the vertical refresh period (i.e. the field rate) on NTSC video hardware (and the period of AC electric power in North America).\n\nThe United States-based NASA, when conducting missions to the planet Mars, has typically used a time of day system calibrated to the mean solar day on that planet (known as a \"sol\"), training those involved on those missions to acclimate to that length of day, which is 88775 SI seconds, 2375 longer than the mean solar day on Earth. NASA's Martian timekeeping system slows down clocks so that the 24-hour day is stretched to the length of that on Mars, so that Martian hours, minutes and seconds are 2.75% longer than their SI counterparts..\n\nOne unit derived from the FFF system of units is the microfortnight, one millionth of the fundamental time unit of FFF, which equals 1.2096 seconds. This is a fairly representative example of \"hacker humor\", and is occasionally used in operating systems; for example, the OpenVMS TIMEPROMPTWAIT parameter is measured in microfortnights.\n\nThe sidereal day is based on the Earth's rotation rate relative to fixed stars, rather than the Sun. A sidereal day is approximately 23 hours, 56 minutes, 4.0905 SI seconds.\n\nThe measurement of time is unique in SI in that while the second is the base unit, and measurements of time smaller than a second use prefixed units smaller than a second (e.g. microsecond, nanosecond, etc.), measurements larger than a second instead use traditional divisions, including the sexagesimal-based \"minute\" and \"hour\" as well as the less regular \"day\" and \"year\" units. SI allows for the use of larger prefixed units based on the second, a system known as metric time, but this is unusual.\n\nThere have been numerous proposals and usage of decimal time, most of which were based on the day as the base unit. For instance, in dynastic China, the kè was a unit that represented of a day (it has since been refined to of a day, or 15 minutes). In France, a decimal time system in place from 1793 to 1805 divided the day into 10 hours, each divided into 100 minutes, in turn each divided into 100 seconds; the French Republican Calendar further extended this by assembling days into ten-day \"weeks\". Ordinal dates and Julian days, the latter of which has seen use in astronomy as it is not subject to leap year complications) allow for the expression of a decimal portion of the day. In the mid-1960s, to defeat the advantage of the recently introduced computers for the then popular rally racing in the Midwest, competition lag times in a few events were given in centids ( day, 864 seconds, 14.4 minutes), millids ( day, 86.4 seconds) and centims ( minute, 0.6 seconds) the latter two looking and sounding a bit like the related units of minutes and seconds. Decimal time proposals are frequently used in fiction, often in futuristic works.\n\nIn addition to decimal time, there also exist binary clocks and hexadecimal time.\n\nThere are two diametrically opposed definitions of the \"dog year\", primarily used to approximate the equivalent age of dogs and other animals with similar life spans. Both are based upon a popular myth regarding the aging of dogs that states that a dog ages seven years in the time it takes a human to age one year.\n\nWhen these units are used, measurements in both \"dog years\" and \"human years\" are often included together, to more clearly indicate which name is used for each unit.\n\nIn fact, the aging of a dog varies by breed (larger breeds tend to have shorter lifespans than small and medium-sized breeds); dogs also develop faster and have longer adulthoods relative to their total life span than humans. Most dogs are sexually mature by 1 year old, which corresponds to perhaps 13 years old in humans. Giant dog breeds and bulldogs tend to have the strongest linear correspondence to human aging, with longer adolescences and shorter overall lifespans; such breeds typically age about nine times as fast as humans throughout their lives.\n\nThe most common large-scale time scale is millions of years (megaannum or \"Ma\"). However, for long-term measurements, this still requires rather large numbers. Using as a measure the time it takes for the solar system to revolve once around the galactic core (GY - not to be confused with Gyr for gigayear), approximately 250 Ma, yields some easily memorizable numbers. In this scale, oceans appeared on Earth after 4 GY, life is detectable at 5 GY, and multicellular organisms first appeared at 15 GY. Dinosaurs became extinct about  GY ago, and the true age of mammals began about 0.2 GY ago. The age of the Earth is estimated at about 20 GY.\n\nThe Furman is a unit of angular measure equal to of a circle, or just under 20 arcseconds. It is named for Alan T. Furman, the American mathematician who adapted the CORDIC algorithm for 16-bit fixed-point arithmetic sometime around 1980. 16 bits give a resolution of 2 = 65,536 distinct angles.\n\nA related unit of angular measure equal to of a circle, represented by 8 bits, has found some use in machinery control where fine precision is not required, most notably crankshaft and camshaft position in internal combustion engine controllers, and in video game programming. There is no consensus as to its name, but it has been called the 8-Bit Furman, the Small Furman, the Furboy and more recently, the miFurman, (milli-binary-Furman). These units are convenient because they form cycles: for the 8-bit unit, the value overflows from 255 to 0 when a full circle has been traversed, so binary addition and subtraction work as expected. Measures are often made using a Gray code, which is trivially converted into more conventional notation.\n\nCoordinates were measured in grades on official French terrestrial ordnance charts from the French revolution well into the 20th century. 1 grade \"(or in modern symbology 1 gon)\" = 0.9° or 0.01 right angle. One advantage of this measure is that the distance between latitude lines 0.01 gon apart at the equator is almost exactly 1 kilometer \"(and would be exactly 1 km if the original definition of 1 meter = quarter-meridian had been adhered to)\". One disadvantage is that common angles like 30° and 60° are expressed by fractional values (33 and 66 respectively) so this \"decimal\" unit failed to displace the \"sexagesimal\" units equilateral-vertex — degree — minute — second invented by Babylonian astronomers.\n\nThe angular mil is used by many military organisations to measure plane angle and so to triangulate distances, given an object's apparent and actual size. It is approximately the angle which has a tangent of ; in NATO standard, this is rounded to of a circle, although other definitions are in use. Its name derives from (\"thousandth\") and so the fact it is used mostly by the military is coincidental to its name.\n\nIt is common in particle physics, where mass and energy are often interchanged, to use eV/c, where eV (electronvolt) is the kinetic energy of an electron accelerated over one volt ( joules), c is the speed of light in a vacuum (from E = mc). This definition is intuitive for a linear particle accelerator when accelerating electrons.\n\nEven more common is to use a system of natural units with c set to 1, and simply use eV as a unit of mass.\n\nIn 2011 the United States Environmental Protection Agency introduced the \"gallon gasoline equivalent\" as a unit of energy because their research showed most U.S. citizens do not understand the standard units. The gallon gasoline equivalent is defined as 33.7 kWh, or about 1.213 joules.\n\nEfficiency or fuel economy can be given as miles per gallon gasoline equivalent.\n\nThe energy of various amounts of the explosive TNT (kiloton, megaton, gigaton) is often used as a unit of explosion energy, and sometimes of asteroid impacts and violent explosive volcanic eruptions. One ton of TNT produces 4.184 joules, or (by arbitrary definition) exactly thermochemical calories (approximately 3.964 BTU). This definition is only loosely based on the actual physical properties of TNT.\n\nThe energy released by the Hiroshima bomb explosion (about 15 kt TNT equivalent, or 6 J) is often used by geologists as a unit when describing the energy of earthquakes, volcanic eruptions, and asteroid impacts.\n\nPrior to the detonation of the Hiroshima bomb, the size of the Halifax Explosion (about 3 kt TNT equivalent, or 1.26 J), was the standard for this type of relative measurement. Each explosion had been the largest known man-made detonation to date.\n\nA foe is a unit of energy equal to  joules (≈9.478 BTU) that was coined by physicist Gerry Brown of Stony Brook University. To measure the staggeringly immense amount of energy produced by a supernova, specialists occasionally use the \"foe\", an acronym derived from the phrase [ten to the power of] fifty-one ergs, or  ergs. This unit of measure is convenient because a supernova typically releases about one foe of observable energy in a very short period of time (which can be measured in seconds).\n\nThe rate at which heat is removed by melting of ice over twenty-four hours is referred to as a ton of refrigeration, or sometimes a ton of cooling. This unit of refrigeration capacity came from the days when large blocks of ice were used for cooling, and is still used to describe the heat-removal capabilities of refrigerators and chillers today. One ton of refrigeration is equal to 12,000 BTU/h exactly, or 3.517 kW.\n\nThe volume of discharge of the Amazon River sometimes used to describe large volumes of water flow such as ocean currents. The unit is equivalent to 216,000 m/s (cumecs).\n\nOne Sverdrup (Sv) is equal to 1,000,000 cubic metres per second (264,000,000 USgal/s). It is used almost exclusively in oceanography to measure the volumetric rate of transport of ocean currents.\n\nThe langley (symbol Ly) is used to measure solar radiation or insolation. It is equal to one thermochemical calorie per square centimetre (4.184 J/m or ≈3.684 BTU/sq ft) and was named after Samuel Pierpont Langley. Its symbol should not be confused with that for the light-year, ly.\n\nOne of the few CGS units to see wider use, one stokes (symbol S or St) is a unit of kinematic viscosity, defined as 1 cm/s, i.e., 10 m/s (≈1.08×10 sq ft/s).\n\nMERU (Milli Earth Rate Unit), an angular velocity equal to 1/1000 of Earth's rotation rate: 1 MERU = 0.015 degrees/hour ≈ 0.072921 microradian/second. Sometimes used to measure the angular drift rate of an inertial navigation system.\n\nIn radio astronomy, the unit of electromagnetic flux is the jansky (symbol Jy), equivalent to 10 watts per square metre per hertz (= 10 kg/s in base units, about 8.8×10 BTU/ft). It is named after the pioneering radio astronomer Karl Jansky. The brightest natural radio sources have flux densities of the order of one to one hundred jansky.\n\nA material-dependent unit used in nuclear and particle physics and engineering to measure the thickness of shielding, for example around a nuclear reactor, particle accelerator, or radiation or particle detector. 1 mwe of a material is the thickness of that material that provides the equivalent shielding of one metre (≈39.4 in) of water.\n\nThis unit is commonly used in underground science to express the extent to which the overburden (usually rock) shields an underground space or laboratory from cosmic rays. The actual thickness of overburden through which cosmic rays must traverse to reach the underground space varies as a function of direction due to the shape of the overburden, which may be a mountain, or a flat plain, or something more complex like a cliff side. To express the depth of an underground space in mwe (or kmwe for deep sites) as a single number, the convention is to use the depth beneath a flat overburden at sea level that gives the same overall cosmic ray muon flux in the underground location.\n\nThe strontium unit, formerly known as the Sunshine Unit (symbol S.U.), is a unit of biological contamination by radioactive substances (specifically strontium-90). It is equal to one picocurie of Sr-90 per gram of body calcium. Since about 2% of the human body mass is calcium, and Sr-90 has a half-life of 28.78 years, releasing 6.697+2.282 MeV per disintegration, this works out to about 1.065 grays per second. The permissible body burden was established at 1,000 S.U.\n\nBananas, like most organic material, naturally contain a certain amount of radioactive isotopes—even in the absence of any artificial pollution or contamination. The banana equivalent dose, defined as the additional dose a person will absorb from eating one banana, expresses the severity of exposure to radiation, such as resulting from nuclear weapons or medical procedures, in terms that would make sense to most people. This is approximately 78 nanosieverts - in informal publications one often sees this estimate rounded up to 0.1 μSv.\n\nIn the pulp and paper industry, molar mass is traditionally measured with a method where the intrinsic viscosity (dL/g) of the pulp sample is measured in cupriethylenediamine (Cuen). The intrinsic viscosity [η] is related to the weight-average molar mass (in daltons) by the Mark-Houwink equation: [η] = 0.070 M. However, it is typical to cite [η] values directly in dL/g, as the \"viscosity\" of the cellulose, confusingly as it is not a viscosity.\n\nIn measuring unsaturation in fatty acids, the traditional method is the iodine number. Iodine adds stoichiometrically to double bonds, so their amount is reported in grams of iodine spent per 100 grams of oil. The standard unit is a dimensionless stoichiometry ratio of moles double bonds to moles fatty acid. A similar quantity, bromine number, is used in gasoline analysis.\n\nIn pulp and paper industry, a similar kappa number is used to measure how much bleaching a pulp requires. Potassium permanganate is added to react with the unsaturated compounds (lignin and uronic acids) in the pulp and back-titrated. Originally with chlorine bleaching the required quantity of chlorine could be then calculated, although modern methods use multiple stages. Since the oxidizable compounds are not exclusively lignin and the partially pulped lignin does not have a single stoichiometry, the relation between the kappa number and the precise amount of lignin is inexact.\n\nDemography and quantitative epidemiology are statistical fields that deal with counts or proportions of people, or rates of change in these. Counts and proportions are technically dimensionless, and so have no units of measurement, although identifiers such as \"people\", \"births\", \"infections\" and the like are used for clarity. Rates of change are counts per unit of time and strictly have inverse time dimensions (per unit of time). In demography and epidemiology expressions such as \"deaths per year\" are used to clarify what is being measured.\n\nPrevalence, a common measure in epidemiology is strictly a type of denominator data, a dimensionless ratio or proportion. Prevalence may be expressed as a fraction, a percentage or as the number of cases per 1,000, 10,000 or 100,000 in the population of interest.\n\nA micromort is a unit of risk measuring a one-in-a-million probability of death (from micro- and ). Micromorts can be used to measure riskiness of various day-to-day activities. A microprobability is a one-in-a million chance of some event; thus a micromort is the microprobability of death. For example, smoking 1.4 cigarettes increases one's death risk by one micromort, as does traveling by car.\n\nThe large numbers of people involved in demography are often difficult to comprehend. A useful visualisation tool is the audience capacity of large sports stadiums (often about 100,000). Often the capacity of the largest stadium in a region serves as a unit for a large number of people. For example, Uruguay's Estadio Centenario is often used in Uruguay, while in parts of the United States, Michigan Stadium is used in this manner. In Australia, the capacity of the Melbourne Cricket Ground (about 100,000) is often cited in this manner. Hence the Melbourne Cricket Ground serves as both a measure of people and a unit of volume.\n\n\"Struck by Lightning\" is often used to put highly-infrequent events into perspective. Among the ~300 million people in the United States, there are ~300 people struck by lightning annually and ~30 killed, making a lightning strike a one in a million event and a death a one in ten million event; given a mean life expectancy of slightly over 75 years, the chances of an American ever being struck in their lifetime is about 1 in 13,000.\nFor example: “A person is about 15 times more likely to be struck by lightning in a given year than to be killed by a stranger with a diagnosis of schizophrenia or chronic psychosis.”\n\nThe growth of computing has necessitated the creation of many new units, several of which are based on unusual foundations.\n\nVolume or capacity of data is often compared to various famous works of literature or to large collections of writing. Popular units include bibles, encyclopaediae, the complete works of Shakespeare, and the Library of Congress.\n\nWhen the Compact Disc began to be used as a data storage device, the CD-ROM, journalists had to compare the disc capacity (650 megabytes) to something everyone could imagine. Since many Western households had a Christian Bible, and the Bible is a comparatively long book, it was often chosen for this purpose. The King James Version of the Bible in uncompressed plain 8-bit text contains about 4.5 million characters, so a CD-ROM can store about 150 Bibles.\n\nThe print version of the \"Encyclopædia Britannica\" is another common data volume metric. It contains approximately 300 million characters, so two copies would fit onto a CD-ROM and still have 50 megabytes (or about 11 bibles) left over.\n\nThe term \"Library of Congress\" is often used as an unusual unit of measurement to represent an impressively large quantity of data when discussing digital storage or networking technologies. It refers to the US Library of Congress. Information researchers have estimated that the entire print collections of the Library of Congress represent roughly 10 terabytes of uncompressed textual data.\n\nA measure of quantity of data or information, the \"nibble\" (sometimes spelled \"nybble\" or \"nybl\") is normally equal to 4 bits, or one half of the common 8-bit byte. The nibble is used to describe the amount of memory used to store a digit of a number stored in binary-coded decimal format, or to represent a single hexadecimal digit. Less commonly, 'nibble' may be used for any contiguous portion of a byte of specified length, e.g. \"6-bit nibble\"; this usage is most likely to be encountered in connection with a hardware architecture in which the word length is not a multiple of 8, such as older 36-bit minicomputers.\n\nIn computing, FLOPS (\"FL\"oating point \"O\"perations \"P\"er \"S\"econd) is a measure of a computer's computing power. It is also common to see measurements of kilo, mega, giga, and teraFLOPS.\n\nIt is also used to compare the performance of algorithms in practice.\n\nA measure to determine the CPU speed. It was invented by Linus Torvalds and is nowadays present on every Linux operating system. However, it is not a meaningful measure to assess the actual CPU performance.\n\nA computer programming expression, the K-LOC or KLOC, pronounced \"kay-lok\", standing for \"kilo-lines of code\", i.e., thousand lines of code. The unit was used, especially by IBM managers, to express the amount of work required to develop a piece of software. Given that estimates of 20 lines of functional code per day per programmer were often used, it is apparent that 1 K-LOC could take one programmer as long as 50 working days, or 10 working weeks. This measure is no longer in widespread use because different computer languages require different numbers of lines to achieve the same result (occasionally the measure \"assembly equivalent lines of code\" is used, with appropriate conversion factors from the language actually used to assembly language).\n\nError rates in programming are also measured in \"Errors per K-LOC\", which is called the \"defect density\". NASA's SATC is one of the few organisations to claim zero defects in a large (>500K-LOC) project, for the space shuttle software.\n\nAn alternative measurement was defined by Pegasus Mail author David Harris: the \"WaP\" is equivalent to 71,500 lines of program code, because that number of lines is the length of one edition of Leo Tolstoy's \"War and Peace\".\n\nThe \"tick\" is the amount of time between timer interrupts generated by the timer circuit of a CPU. The amount of time is processor-dependent.\n\n\"The Economist\"<nowiki>'</nowiki>s Big Mac Index compares the purchasing power parity of countries in terms of the cost of a Big Mac hamburger. This was felt to be a good measure of the prices of a basket of commodities in the local economy including labour, rent, meat, bread, cardboard, advertising, lettuce, etc.\n\nA similar system used in the UK is the 'Mars bar'. Tables of prices in Mars Bars have intermittently appeared in newspapers over the last 20 years, usually to illustrate changes in wages or prices over time without the confusion caused by inflation.\n\nThe cost of a cup of coffee (or sometimes latte) from a coffeehouse or cafe is often used as a measurement of two vectors: the relatively diminuitive expense of something frivolous, versus the power of collective contributions towards something important. Campaigns implore something to the effect of, \"for the cost of a cup of coffee, you can help stamp out diabetes.\"\n\nChess software frequently uses centipawns internally or externally as a unit measuring how strong each player's situation position is, and hence also by how much one player is beating the other, and how strong a possible move is. 100 centipawns = the value of 1 pawn - more specifically, something like the average value of the pawns at the start of the game, as the actual value of pawns depends on their position. Loss of a pawn will therefore typically lose that player 100 centipawns. The centipawn is often used for comparing possible moves, as in a given position, chess software will often rate the better of two moves within a few centipawns of each other.\n\nThe garn is NASA's unit of measure for symptoms resulting from space adaptation syndrome, the response of the human body to weightlessness in space, named after US Senator Jake Garn, who became exceptionally spacesick during an orbital flight in 1985. If an astronaut is completely incapacitated by space adaptation syndrome, he or she is under the effect of one garn of symptoms.\n\nFormerly used in real estate transactions in the American Southwest, it was the number of pregnant cows an acre of a given plot of land could support. It acted as a proxy for the agricultural quality, natural resource availability, and arability of a parcel of land.\n\nNumbers very close to, but below one are often expressed in \"nines\" (N — not to be confused with the unit newton), that is in the number of nines following the decimal separator in writing the number in question. For example, \"three nines\" or \"3N\" indicates 0.999 or 99.9%, \"four nines five\" or \"4N5\" is the expression for the number 0.99995 or 99.995%.\n\nTypical areas of usage are:\n\nThe dol (from the Latin word for pain, \"dolor\") is a unit of measurement for pain. James D. Hardy, Herbert G. Wolff, and Helen Goodell of Cornell University proposed the unit based on their studies of pain during the 1940s and 1950s. They defined one dol to equal to \"just noticeable differences\" (jnd's) in pain. The unit never came into widespread use and other methods are now used to assess the level of pain experienced by patients.\n\nThe Schmidt sting pain index and Starr sting pain index are pain scales rating the relative pain caused by different hymenopteran stings. Schmidt has refined his Schmidt Sting Pain Index (scaled from 1 to 4) with extensive anecdotal experience, culminating in a paper published in 1990 which classifies the stings of 78 species and 41 genera of Hymenoptera. The Starr sting pain scale uses the same 1-to-4 scaling.\n\nThe ASTA (American Spice Trade Association) pungency unit is based on a scientific method of measuring chili pepper \"heat\". The technique utilizes high-performance liquid chromatography to identify and measure the concentrations of the various compounds that produce a heat sensation. Scoville units are roughly the size of pungency units while measuring capsaicin, so a rough conversion is to multiply pungency by 15 to obtain Scoville heat units.\n\nThe Scoville scale is a measure of the hotness of a chili pepper. It is the degree of dilution in sugar water of a specific chili pepper extract when a panel of 5 tasters can no longer detect its \"heat\". Pure capsaicin (the chemical responsible for the \"heat\") has 16 million Scoville heat units.\n\nUp to the 20th century, alcoholic spirits were assessed in the UK by mixing with gunpowder and testing the mixture to see if it would still burn; spirit that just passed the test was said to be at 100° proof. The UK now uses percentage alcohol by volume at 20 °C (68 °F), where spirit at 100° proof is approximately 57.15% ABV; the US uses a \"proof number\" of twice the ABV at 60 °F (15.5 °C).\n\nThe Savart is an 18th-century unit for measuring the frequency ratio of two sounds, it is equal to of a decade. Still used in some programs, but considered too rough for most purposes. Cent is preferred.\n\nThe erlang, named after A. K. Erlang, as a dimensionless unit is used in telephony as a statistical measure of the offered intensity of telecommunications traffic on a group of resources. Traffic of one erlang refers to a single resource being in continuous use, or two channels being at fifty percent use, and so on, pro rata. Much telecommunications management and forecasting software uses this.\n\nWaffle House Index is used by the Federal Emergency Management Agency (FEMA) to determine the impact of a storm and the likely scale of assistance required for disaster recovery. The measure is based on the reputation of the Waffle House restaurant chain for staying open during extreme weather. This term was coined by FEMA Administrator Craig Fugate.\n\nThe crab is defined as the intensity of X-rays emitted from the Crab Nebula at a given photon energy up to 30 kiloelectronvolts. The Crab Nebula is often used for calibration of X-ray telescopes. For measuring the X-ray intensity of a less energetic source, the milliCrab (mCrab) may be used.\n\nOne crab is approximately 24 pW/m.\n\n"}
{"id": "54434451", "url": "https://en.wikipedia.org/wiki?curid=54434451", "title": "Motor planning", "text": "Motor planning\n\nIn psychology and neuroscience, motor planning is a set of processes related to the preparation of a movement that occurs during the reaction time (the time between the presentation of a stimulus to a person and that person's initiation of a motor response). Colloquially, the term applies to any process involved in the preparation of a movement during the reaction time, including perception-related and action-related processes. For example, the identification of a task-relevant stimulus is captured by the usual meaning of the term, “motor planning”, but this identification process is not strictly motor-related. Wong and colleagues (2015) have proposed a narrower definition to include only movement-related processes: \"Specification of the movement trajectory for the desired action, a description of how the will produce such an action, and finally a description of the full set of the joint trajectories or muscle activations required to execute the movement.\"\n"}
{"id": "16053771", "url": "https://en.wikipedia.org/wiki?curid=16053771", "title": "Muhlenberg legend", "text": "Muhlenberg legend\n\nThe Muhlenberg legend is an urban legend in the United States and Germany. According to the legend, the single vote of Frederick Muhlenberg, the first ever Speaker of the U.S. House of Representatives, prevented German from becoming an official language of the United States.\n\nThe kernel of truth behind this legend is a vote in the United States House of Representatives in 1794, after a group of German immigrants asked for the translation of some laws into German. This petition was debated by the House of Representatives without a decision made and a vote to adjourn and consider the recommendation at a later date was defeated by one vote, 42 to 41. There was no vote on an actual bill, merely a vote on whether or not to adjourn. Because the motion to adjourn did not pass, the matter was dropped. It was from this roll call on adjournment that the “German missed becoming the official language of the USA by one vote” legend sprang. \n\nMuhlenberg (of German descent himself, who had not voted in the roll call) was later quoted as having said \"the faster the Germans become Americans, the better it will be.\"\nThe United States has no statutory official language; English has been used on a \"de facto\" basis, owing to its status as the country's predominant language. At times various states have passed their own official language laws.\n\nThe legend has a long history, and led to a number of analyses and articles published from the late 1920s into the early 1950s explaining why the story was not true. The story was dubbed the \"Muhlenberg legend\" by at least the late 1940s. Nevertheless, the legend persists.\n\nFor example, in 1987, a letter from a former election official in Missouri emphasized the importance of voting in an Ann Landers column. He included a list of events allegedly decided by one vote from his local election manual, including that “in 1776, one vote gave America the English language instead of German.\" (In fact, versions of this error-filled list long predate the 1987 Ann Landers mention.) This led to another round of news stories again pointing out that this was a myth. Oblivious to corrections of this sort, Ann Landers ran the same list again in November 1996. A chorus of dismayed responses caused Landers to clear up the matter in a subsequent column. According to one letter writer, who begged Landers to \"stomp out that piece of fiction wherever you encounter it,\" the myth gained traction in the 1930s due to the work of Nazi propagandists. \n\nAnother version of the myth, which puts the vote in 1774 by the Continental Congress, appeared in \"Ripley's Believe It or Not!\" as early as 1930. Ripley's included the myth in a 1982 book as well. Ripley's version credits the story to an alleged letter by Heinrich Melchior Muhlenberg published in Halle in 1887.\n\nMost accounts credit Franz von Löher as the source of the legend. Löher was a German visitor to the United States who published the book \"Geschichte und Zustände der Deutschen in Amerika\" (\"History and Achievements of the Germans in America\") in 1847. Löher seemingly placed the crucial vote only in Pennsylvania, to make German the official language of that state, not the United States as a whole. (Philadelphia was where the U.S. Congress sat at the time, but it was also the capital of Pennsylvania. To further confuse matters, Muhlenburg did serve as Speaker of the Pennsylvania House before serving in that title for the U.S. Congress.) According to Löher, the vote was tied, and Muhlenberg cast the tie-breaking vote for English.\n\n\n"}
{"id": "312648", "url": "https://en.wikipedia.org/wiki?curid=312648", "title": "Mutual exclusivity", "text": "Mutual exclusivity\n\nIn logic and probability theory, two events (or propositions) are mutually exclusive or disjoint if they cannot both occur at the same time(be true). A clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both.\n\nIn the coin-tossing example, both outcomes are, in theory, collectively exhaustive, which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities. However, not all mutually exclusive events are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).\n\nIn logic, two mutually exclusive propositions are propositions that logically cannot be true in the same sense at the same time. To say that more than two propositions are mutually exclusive, depending on context, means that one cannot be true if the other one is true, or at least one of them cannot be true. The term \"pairwise mutually exclusive\" always means that two of them cannot be true simultaneously.\n\nIn probability theory, events \"E\", \"E\", ..., \"E\" are said to be mutually exclusive if the occurrence of any one of them implies the non-occurrence of the remaining \"n\" − 1 events. Therefore, two mutually exclusive events cannot both occur. Formally said, the intersection of each two of them is empty (the null event): \"A\" ∩ \"B\" = ∅. In consequence, mutually exclusive events have the property: P(\"A\" ∩ \"B\") = 0.\n\nFor example, it is impossible to draw a card that is both red and a club because clubs are always black. If just one card is drawn from the deck, either a red card (heart or diamond) or a black card (club or spade) will be drawn. When \"A\" and \"B\" are mutually exclusive, P(\"A\" ∪ \"B\") = P(\"A\") + P(\"B\"). To find the probability of drawing a red card or a club, for example, add together the probability of drawing a red card and the probability of drawing a club. In a standard 52-card deck, there are twenty-six red cards and thirteen clubs: 26/52 + 13/52 = 39/52 or 3/4.\n\nOne would have to draw at least two cards in order to draw both a red card and a club. The probability of doing so in two draws depends on whether the first card drawn were replaced before the second drawing, since without replacement there is one fewer card after the first card was drawn. The probabilities of the individual events (red, and club) are multiplied rather than added. The probability of drawing a red and a club in two drawings without replacement is then 26/52 × 13/51 × 2 = 676/2652, or 13/51. With replacement, the probability would be 26/52 × 13/52 × 2 = 676/2704, or 13/52.\n\nIn probability theory, the word \"or\" allows for the possibility of both events happening. The probability of one or both events occurring is denoted P(\"A\" ∪ \"B\") and in general it equals P(\"A\") + P(\"B\") – P(\"A\" ∩ \"B\"). Therefore, in the case of drawing a red card or a king, drawing any of a red king, a red non-king, or a black king is considered a success. In a standard 52-card deck, there are twenty-six red cards and four kings, two of which are red, so the probability of drawing a red or a king is 26/52 + 4/52 – 2/52 = 28/52.\n\nEvents are collectively exhaustive if all the possibilities for outcomes are exhausted by those possible events, so at least one of those outcomes must occur. The probability that at least one of the events will occur is equal to one. For example, there are theoretically only two possibilities for flipping a coin. Flipping a head and flipping a tail are collectively exhaustive events, and there is a probability of one of flipping either a head or a tail. Events can be both mutually exclusive and collectively exhaustive. In the case of flipping a coin, flipping a head and flipping a tail are also mutually exclusive events. Both outcomes cannot occur for a single trial (i.e., when a coin is flipped only once). The probability of flipping a head and the probability of flipping a tail can be added to yield a probability of 1: 1/2 + 1/2 =1.\n\nIn statistics and regression analysis, an independent variable that can take on only two possible values is called a dummy variable. For example, it may take on the value 0 if an observation is of a male subject or 1 if the observation is of a female subject. The two possible categories associated with the two possible values are mutually exclusive, so that no observation falls into more than one category, and the categories are exhaustive, so that every observation falls into some category. Sometimes there are three or more possible categories, which are pairwise mutually exclusive and are collectively exhaustive — for example, under 18 years of age, 18 to 64 years of age, and age 65 or above. In this case a set of dummy variables is constructed, each dummy variable having two mutually exclusive and jointly exhaustive categories — in this example, one dummy variable (called D) would equal 1 if age is less than 18, and would equal 0 \"otherwise\"; a second dummy variable (called D) would equal 1 if age is in the range 18-64, and 0 otherwise. In this set-up, the dummy variable pairs (D, D) can have the values (1,0) (under 18), (0,1) (between 18 and 64), or (0,0) (65 or older) (but not (1,1), which would nonsensically imply that an observed subject is both under 18 and between 18 and 64). Then the dummy variables can be included as independent (explanatory) variables in a regression. Note that the number of dummy variables is always one less than the number of categories: with the two categories male and female there is a single dummy variable to distinguish them, while with the three age categories two dummy variables are needed to distinguish them.\n\nSuch qualitative data can also be used for dependent variables. For example, a researcher might want to predict whether someone goes to college or not, using family income, a gender dummy variable, and so forth as explanatory variables. Here the variable to be explained is a dummy variable that equals 0 if the observed subject does not go to college and equals 1 if the subject does go to college. In such a situation, ordinary least squares (the basic regression technique) is widely seen as inadequate; instead probit regression or logistic regression is used. Further, sometimes there are three or more categories for the dependent variable — for example, no college, community college, and four-year college. In this case, the multinomial probit or multinomial logit technique is used.\n\n\n"}
{"id": "23110101", "url": "https://en.wikipedia.org/wiki?curid=23110101", "title": "Near sets", "text": "Near sets\n\nIn mathematics, near sets are either spatially close or descriptively close. Spatially close sets have nonempty intersection. In other words, spatially close sets are not disjoint sets, since they always have at least one element in common. Descriptively close sets contain elements that have matching descriptions. Such sets can be either disjoint or non-disjoint sets. Spatially near sets are also descriptively near sets.\n\nThe underlying assumption with descriptively close sets is that such sets contain elements that have location and measurable features such as colour and frequency of occurrence. The description of the element of a set is defined by a feature vector. Comparison of feature vectors provides a basis for measuring the closeness of descriptively near sets. Near set theory provides a formal basis for the observation, comparison, and classification of elements in sets based on their closeness, either spatially or descriptively. Near sets offer a framework for solving problems based on human perception that arise in areas such as image processing, computer vision as well as engineering and science problems.\n\nNear sets have a variety of applications in areas such as topology, pattern detection and classification, abstract algebra, mathematics in computer science, and solving a variety of problems based on human perception that arise in areas such as image analysis, image processing, face recognition, ethology, as well as engineering and science problems. From the beginning, descriptively near sets have proved to be useful in applications of topology, and visual pattern recognition , spanning a broad spectrum of applications that include camouflage detection, micropaleontology, handwriting forgery detection, biomedical image analysis, content-based image retrieval, population dynamics, quotient topology, textile design, visual merchandising, and topological psychology.\n\nAs an illustration of the degree of descriptive nearness between two sets, consider an example of the Henry colour model for varying degrees of nearness\nbetween sets of picture elements in pictures (see, \"e.g.\", §4.3). The two pairs of ovals in Fig. 1 and Fig. 2 contain coloured segments. Each segment in the figures corresponds to an equivalence class where all pixels in the class have similar descriptions, \"i.e.\", picture elements with similar colours. The ovals in Fig.1 are closer to each other descriptively than the ovals in Fig. 2.\n\nIt has been observed that the simple concept of \"nearness\" unifies various concepts of topological structures inasmuch as the category Near of all nearness spaces and nearness preserving maps contains categories sTop (symmetric topological spaces and continuous maps), Prox (proximity spaces and formula_1-maps), Unif (uniform spaces and uniformly continuous maps) and Cont (contiguity spaces and contiguity maps) as embedded full subcategories. The categories formula_2 and formula_3 are shown to be full supercategories of various well-known categories, including the category formula_4 of symmetric topological spaces and continuous maps, and the category formula_5 of extended metric spaces and nonexpansive maps. The notation formula_6 reads \"category\" formula_7 \"is embedded in category\" formula_8. The categories formula_9 and formula_10 are supercategories for a variety of familiar categories shown in Fig. 3. Let formula_2 denote the category of all formula_12-approach nearness spaces and contractions, and let formula_9 denote the category of all formula_12-approach merotopic spaces and contractions.\n\nAmong these familiar categories is formula_4, the symmetric form of formula_16 (see category of topological spaces), the category with objects that are topological spaces and morphisms that are continuous maps between them. formula_17 with objects that are extended metric spaces is a subcategory of formula_18 (having objects formula_12-approach spaces and contractions) (see also). Let formula_20 be extended pseudometrics on nonempty sets formula_21, respectively. The map formula_22 is a contraction if and only if formula_23 is a contraction. For nonempty subsets formula_24 , the distance function formula_25 is defined by\n\nThus formula_27AP is embedded as a full subcategory in formula_2 by the functor formula_29 defined by formula_30 and formula_31. Then formula_22 is a contraction if and only if formula_23 is a contraction. Thus formula_34 is embedded as a full subcategory in formula_2 by the functor formula_29 defined by formula_30 and formula_38 Since the category formula_5 of extended metric spaces and nonexpansive maps is a full subcategory of formula_34, therefore, formula_2 is also a full supercategory of formula_5. The category formula_2 is a topological construct.\n\nThe notions of near and far in mathematics can be traced back to works by Johann Benedict Listing and Felix Hausdorff. The related notions of resemblance and similarity can be traced back to J.H. Poincaré, who introduced sets of similar sensations (nascent tolerance classes) to represent the results of G.T. Fechner's sensation sensitivity experiments and a framework for the study of resemblance in representative spaces as models of what he termed physical continua. The elements of a physical continuum (pc) are sets of sensations. The notion of a pc and various representative spaces (tactile, visual, motor spaces) were introduced by Poincaré in an 1894 article on the mathematical continuum, an 1895 article on space and geometry and a compendious 1902 book on science and hypothesis followed by a number of elaborations, \"e.g.\". The 1893 and 1895 articles on continua (Pt. 1, ch. II) as well as representative spaces and geometry (Pt. 2, ch IV) are included as chapters in. Later, F. Riesz introduced the concept of proximity or nearness of pairs of sets at the International Congress of Mathematicians (ICM) in 1908.\n\nDuring the 1960s, E.C. Zeeman introduced tolerance spaces in modelling visual perception. A.B. Sossinsky observed in 1986 that the main idea underlying tolerance space theory comes from Poincaré, especially. In 2002, Z. Pawlak and J. Peters considered an informal approach to the perception of the nearness of physical objects such as snowflakes that was not limited to spatial nearness. In 2006, a formal approach to the descriptive nearness of objects was considered by J. Peters, A. Skowron and J. Stepaniuk in the context of proximity spaces. In 2007, descriptively near sets were introduced by J. Peters followed by the introduction of tolerance near sets. Recently, the study of descriptively near sets has led to algebraic, topological and proximity space foundations of such sets.\n\nThe adjective \"near\" in the context of near sets is used to denote the fact that observed feature value differences of distinct objects are small enough to be\nconsidered indistinguishable, \"i.e.\", within some tolerance.\n\nThe exact idea of closeness or 'resemblance' or of 'being within tolerance' is universal enough to appear, quite naturally, in almost any mathematical setting\n(see, \"e.g.\",). It is especially natural in mathematical applications: practical problems, more often than not, deal with approximate input data and only require viable results with a tolerable level of error.\n\nThe words \"near\" and \"far\" are used in daily life and it was an incisive suggestion of F. Riesz that these intuitive concepts be made rigorous. He introduced the concept of nearness of pairs of sets at the ICM in Rome in 1908. This concept is useful in simplifying teaching calculus and advanced calculus. For example, the passage from an intuitive definition of continuity of a function at a point to its rigorous epsilon-delta definition is sometime difficult for teachers to explain and for students to understand. Intuitively, continuity can be explained using nearness language, \"i.e.\", a function formula_44 is continuous at a point formula_45, provided points formula_46 near formula_45 go into points formula_48 near formula_49. Using Riesz's idea, this definition can be made more precise and its contrapositive is the familiar definition.\n\nFrom a spatial point of view, nearness (a.k.a. proximity) is considered a generalization of set intersection. For disjoint sets, a form of nearness set intersection is defined in terms of a set of objects (extracted from disjoint sets) that have similar features within some\ntolerance (see, \"e.g.\", §3 in). For example, the ovals in Fig. 1 are considered near each other, since these ovals contain pairs of classes that display similar (visually indistinguishable) colours.\n\nLet formula_50 denote a metric topological space that is endowed with one or more proximity relations and let formula_51 denote the collection of all subsets of formula_50. The collection formula_51 is called the power set of formula_50.\n\nThere are many ways to define Efremovič proximities on topological spaces (discrete proximity, standard proximity, metric proximity, Čech proximity, Alexandroff proximity, and Freudenthal proximity), For details, see § 2, pp. 93–94 in.\nThe focus here is on \"standard proximity\" on a topological space. For formula_55, formula_56 is near formula_57 (denoted by formula_58), provided their closures share a common point.\n\nThe \"closure\" of a subset formula_59 (denoted by formula_60) is the usual Kuratowski closure of a set, introduced in § 4, p. 20, is defined by\n\n\"i.e.\" formula_60 is the set of all points formula_63 in formula_50 that are close to formula_56 (formula_66 is the Hausdorff distance (see § 22, p. 128, in) between formula_63 and the set formula_56 and formula_69 (standard distance)). A \"standard\" proximity relation is defined by\n\nWhenever sets formula_56 and formula_57 have no points in common, the sets are \"far\"from each other (denoted formula_73).\n\nThe following EF-proximity space axioms are given by Jurij Michailov Smirnov based on what Vadim Arsenyevič Efremovič introduced during the first half of the 1930s. Let formula_74.\n\n\nThe pair formula_93 is called an EF-proximity space. In this context, a space is a set with some added structure. With a proximity space formula_50, the structure of formula_50 is induced by the EF-proximity relation formula_1. In a proximity space formula_50, the closure of formula_56 in formula_50 coincides with the intersection of all closed sets that contain formula_56.\n\n\nLet the set formula_50 be represented by the points inside the rectangular region in Fig. 5. Also, let formula_106 be any two non-intersection subsets (\"i.e.\" subsets spatially far from each other) in formula_50, as shown in Fig. 5. Let formula_108 (complement of the set formula_90). Then from the EF-axiom, observe the following:\n\nDescriptively near sets were introduced as a means of solving classification and pattern recognition problems arising from disjoint sets that resemble each other. Recently, the connections between near sets in EF-spaces and near sets in descriptive EF-proximity spaces have been explored in.\n\nAgain, let formula_50 be a metric topological space and let formula_112 a set of probe functions that represent features of each formula_103. The assumption made here is formula_50 contains non-abstract points that have measurable features such as gradient orientation. A non-abstract point has a location and features that can be measured (see § 3 in ).\n\nA \"probe function\" formula_115 represents a feature of a sample point in formula_50. The mapping formula_117 is defined by formula_118, where formula_119 is an n-dimensional real Euclidean vector space. formula_120 is a feature vector for formula_63, which provides a description of formula_103. For example, this leads to a proximal view of sets of picture points in digital images.\n\nTo obtain a descriptive proximity relation (denoted by formula_123), one first chooses a set of probe functions. Let formula_124 be a mapping on a subset of formula_51 into a subset of formula_126. For example, let formula_24 and formula_128 denote sets of descriptions of points in formula_106, respectively. That is,\n\nThe expression formula_131 reads formula_56 \"is descriptively near\" formula_57. Similarly, formula_134 reads formula_56 \"is descriptively far from\" formula_57. The descriptive proximity of formula_56 and formula_57 is defined by\n\nThe \"descriptive intersection\" formula_140 of formula_56 and formula_57 is defined by\n\nThat is, formula_144 is in formula_145, provided formula_146 for some formula_147. Observe that formula_56 and formula_57 can be disjoint and yet formula_145 can be nonempty.\nThe descriptive proximity relation formula_123 is defined by\n\nWhenever sets formula_56 and formula_57 have no points with matching descriptions, the sets are \"descriptively far\" from each other \n(denoted by formula_155).\n\nThe binary relation formula_123 is a \"descriptive EF-proximity\", provided the following axioms are satisfied for formula_157.\n\n\nThe pair formula_179 is called a descriptive proximity space.\n\nA \"relator\" is a nonvoid family of relations formula_180 on a nonempty set formula_50. The pair formula_182 (also denoted formula_183) is called a relator space. Relator spaces are natural generalizations of ordered sets and uniform spaces}. With the introduction of a family of proximity relations formula_184 on formula_50, we obtain a proximal relator space formula_186. For simplicity, we consider only two proximity relations, namely, the Efremovič proximity formula_1 and the descriptive proximity formula_123 in defining the \"descriptive relator\" formula_189. The pair formula_190 is called a \"proximal relator space \". In this work, formula_50 denotes a metric topological space that is endowed with the relations in a proximal relator. With the introduction of formula_190, the traditional closure of a subset (\"e.g.\", ) can be compared with the more recent descriptive closure of a subset.\n\nIn a proximal relator space formula_50, the \"descriptive closure of a set\" formula_56 (denoted by formula_195) is defined by\n\nThat is, formula_103 is in the descriptive closure of formula_56, provided the closure of formula_120 and the closure of formula_200 have at least one element in common.\n\n\n\n\n\nIn a proximal relator space, EF-proximity formula_1 leads to the following results for descriptive proximity formula_123.\n\n\n\nformula_230\n\nformula_236\n\nIn a pseudometric proximal relator space formula_50, the neighbourhood of a point formula_103 (denoted by formula_240), for formula_241, is defined by\n\nThe interior of a set formula_56 (denoted by formula_244) and boundary of formula_56 (denoted by formula_246) in a proximal relator space formula_50 are defined by\n\nA set formula_56 has a \"natural strong inclusion\" in a set formula_57 associated with formula_1} (denoted by formula_253), provided formula_254, \"i.e.\", formula_255 (formula_56 is far from the complement of formula_257). Correspondingly, a set formula_56 has a \"descriptive strong inclusion\" in a set formula_57 associated with formula_123 (denoted by formula_261), provided formula_262, \"i.e.\", formula_263 (formula_264 is far from the complement of formula_257).\n\nLet formula_266 be a descriptive formula_1-neighbourhood relation defined by\n\nThat is, formula_261, provided the description of each formula_270 is contained in the set of descriptions of the points formula_271. Now observe that any formula_106 in the proximal relator space formula_50 such that formula_155 have disjoint formula_123-neighbourhoods, \"i.e.\",\n\n\nA consideration of strong containment of a nonempty set in another set leads to the study of hit-and-miss topologies and the Wijsman topology.\n\nLet formula_12 be a real number greater than zero. In the study of sets that are proximally near within some tolerance, the set of proximity relations formula_189 is augmented with a pseudometric tolerance proximity relation (denoted by formula_281) defined by\n\nLet formula_283. In other words, a nonempty set equipped with the proximal relator formula_284 has underlying structure provided by the proximal relator formula_189 and provides a basis for the study of tolerance near sets in formula_50 that are near within some tolerance. Sets formula_106 in a descriptive pseudometric proximal relator space formula_288 are tolerance near sets (\"i.e.\", formula_289), provided\n\nRelations with the same formal properties as similarity relations of sensations considered by Poincaré are nowadays, after Zeeman, called \"tolerance relations\". A \"tolerance formula_291 on a set formula_292\" is a relation formula_293 that is reflexive and symmetric. In algebra, the term \"tolerance relation\" is also used in a narrow sense to denote reflexive and symmetric relations defined on universes of algebras that are also compatible with operations of a given algebra, \"i.e.\", they are generalizations of congruence relations (see \"e.g.\",). In referring to such relations, the term \"algebraic tolerance\" or the term \"algebraic tolerance relation\" is used.\nTransitive tolerance relations are equivalence relations. A set formula_292 together with a tolerance formula_291 is called a \"tolerance space\" (denoted formula_296). A set formula_297 is a \"formula_291-preclass\" (or briefly \"preclass\" when formula_291 is understood) if and only if for any formula_300, formula_301.\n\nThe family of all preclasses of a tolerance space is naturally ordered by set inclusion and preclasses that are maximal with respect to set inclusion are called \"formula_291-classes\" or just \"classes\", when formula_291 is understood. The family of all classes of the space formula_296 is particularly interesting and is denoted by formula_305. The family formula_305 is a covering of formula_292.\n\nThe work on similarity by Poincaré and Zeeman presage the introduction of near sets and research on similarity relations, \"e.g.\". In science and engineering, tolerance near sets are a practical application of the study of sets that are near within some tolerance. A tolerance formula_308 is directly related to the idea of closeness or resemblance (\"i.e.\", being within some tolerance) in comparing objects.\nBy way of application of Poincaré's approach in defining visual spaces and Zeeman's approach to tolerance relations, the basic idea is to compare objects such as image patches in the interior of digital images.\n\nSimple Example\n\nThe following simple example demonstrates the construction of tolerance classes from real data. Consider the 20 objects in the table below with formula_309.\n\nLet a tolerance relation be defined as\n\nThen, setting formula_311 gives the following tolerance classes:\n\nObserve that each object in a tolerance class satisfies the condition formula_313, and that almost all of the objects appear in more than one class. Moreover, there would be twenty classes if the indiscernibility relation was used since there are no two objects with matching descriptions.\n\nImage Processing Example\n\nThe following example provides an example based on digital images. Let a subimage be defined as a small subset of pixels belonging to a digital image such that the pixels contained in the subimage form a square. Then, let the sets formula_314 and formula_315 respectively represent the subimages obtained from two different images, and let formula_316. Finally, let the description of an object be given by the Green component in the RGB color model. The next step is to find all the tolerance classes using the tolerance relation defined in the previous example. Using this information, tolerance classes can be formed containing objects that have similar (within some small formula_12) values for the Green component in the RGB colour model. Furthermore, images that are near (similar) to each other should have tolerance classes divided among both images (instead of a tolerance classes contained solely in one of the images). For example, the figure accompanying this example shows a subset of the tolerance classes obtained from two leaf images. In this figure, each tolerance class is assigned a separate colour. As can be seen, the two leaves share similar tolerance classes. This example highlights a need to measure the degree of nearness of two sets.\n\nLet formula_318 denote a particular descriptive pseudometric EF-proximal relator space equipped with the proximity relation formula_281 and with nonempty subsets formula_320 and with the tolerance relation formula_321 defined in terms of a set of probes formula_322 and with formula_323, where\n\nFurther, assume formula_325 and let formula_326 denote the family of all classes in the space formula_327.\n\nLet formula_328. The distance formula_329 is defined by\n\nwhere\n\nThe details concerning formula_332 are given in. The idea behind formula_332 is that sets that are similar should have a similar number of objects in each tolerance class. Thus, for each tolerance class obtained from the covering of formula_334, formula_332 counts the number of objects that belong to formula_50 and formula_315 and takes the ratio (as a proper fraction) of their cardinalities. Furthermore, each ratio is weighted by the total size of the tolerance class (thus giving importance to the larger classes) and the final result is normalized by dividing by the sum of all the cardinalities. The range of formula_332 is in the interval [0,1], where a value of 1 is obtained if the sets are equivalent (based on object descriptions) and a value of 0 is obtained if they have no descriptions in common.\n\nAs an example of the degree of nearness between two sets, consider figure below in which each image consists of two sets of objects, formula_50 and formula_315. Each colour in the figures corresponds to a set where all the objects in the class share the same description. The idea behind formula_332 is that the nearness of sets in a perceptual system is based on the cardinality of tolerance classes that they share. Thus, the sets in left side of the figure are closer (more near) to each other in terms of their descriptions than the sets in right side of the figure.\n\nThe Near set Evaluation and Recognition (NEAR) system, is a system developed to demonstrate practical applications of near set theory to the problems of image segmentation evaluation and image correspondence. It was motivated by a need for a freely available software tool that can provide results for research and to generate interest in near set theory. The system implements a Multiple Document Interface (MDI) where each separate processing task is performed in its own child frame. The objects (in the near set sense) in this system are subimages of the images being processed and the probe functions (features) are image processing functions defined on the subimages. The system was written in C++ and was designed to facilitate the addition of new processing tasks and probe functions. Currently, the system performs six major tasks, namely, displaying equivalence and tolerance classes for an image, performing segmentation evaluation, measuring the nearness of two images, performing Content Based Image Retrieval (CBIR), and displaying the output of processing an image using a specific probe function.\n\nThe Proximity System is an application developed to demonstrate descriptive-based topological approaches to nearness and proximity within the context of digital image analysis. The Proximity System grew out of the work of S. Naimpally and J. Peters on Topological Spaces. The Proximity System was written in Java and is intended to run in two different operating environments, namely on Android smartphones and tablets, as well as desktop platforms running the Java Virtual Machine. With respect to the desktop environment, the Proximity System is a cross-platform Java application for Windows, OSX, and Linux systems, which has been tested on Windows 7 and Debian Linux using the Sun Java 6 Runtime. In terms of the implementation of the theoretical approaches, both the Android and the desktop based applications use the same back-end libraries to perform the description-based calculations, where the only differences are the user interface and the Android version has less available features due to restrictions on system resources.\n\n"}
{"id": "2288927", "url": "https://en.wikipedia.org/wiki?curid=2288927", "title": "Negative thermal expansion", "text": "Negative thermal expansion\n\nNegative thermal expansion (NTE) is an unusual physicochemical process in which some materials contract upon heating, rather than expand as most other materials do. Materials which undergo NTE have a range of potential engineering, photonic, electronic, and structural applications. For example, if one were to mix a negative thermal expansion material with a \"normal\" material which expands on heating, it could be possible to make a zero expansion composite material.\n\nThere are a number of physical processes which may cause contraction with increasing temperature, including transverse vibrational modes, Rigid Unit Modes and phase transitions.\n\nRecently, Liu et al. showed that the NTE phenomenon originates from the existence of high pressure, small volume configurations with higher entropy, with their configurations present in the stable phase matrix through thermal fluctuations. They were able to predict both the colossal positive thermal expansion (In cerium) and zero and infinite negative thermal expansion (in ) \n\nNegative thermal expansion is usually observed in non-close-packed systems with directional interactions (e.g. ice, graphene, etc.) and complex compounds (e.g. , , beta-quartz, some zeolites, etc.). However, in a paper, it was shown that negative thermal expansion (NTE) is also realized in single-component close-packed lattices with pair central force interactions. The following sufficient condition for potential giving rise to NTE behavior is proposed:\n\nformula_1\n\nwhere formula_2 is pair interatomic potential, formula_3 is the equilibrium distance. This condition is (i) necessary and sufficient in 1D and (ii) sufficient, but not necessary in 2D and 3D. \"An approximate\" necessary and sufficient condition is derived in a paper\n\nformula_4\n\nwhere formula_5 is the space dimensionality. Thus in 2D and 3D negative thermal expansion in close-packed systems with pair interactions is realized even when the third derivative of the potential is zero or even negative. Note that one-dimensional and multidimensional cases are qualitatively different. In 1D thermal expansion is cased by anharmonicity of interatomic potential only. Therefore, the sign of thermal expansion coefficient is determined by the sign of the third derivative of the potential. In multidimensional case the geometrical nonlinearity is also present, i.e. lattice vibrations are nonlinear even in the case of harmonic interatomic potential. This nonlinearity contributes to thermal expansion. Therefore, in multidimensional case both formula_6 and formula_7 are present in the condition for negative thermal expansion.\n\nThere are many potential applications for materials with controlled thermal expansion properties, as thermal expansion causes many problems in engineering, and indeed in everyday life. One simple example of a thermal expansion problem is the tendency of dental fillings to expand by an amount different from the teeth, for example when drinking a hot drink, causing toothache. If dental fillings were made of a composite material containing a mixture of materials with positive and negative thermal expansion then the overall expansion could be precisely tailored to that of tooth enamel.\n\nPerhaps one of the most studied materials to exhibit \"negative thermal expansion\" is zirconium tungstate (). This compound contracts continuously over a temperature range of 0.3 to 1050 K (at higher temperatures the material decomposes). Other materials that exhibit this behaviour include: other members of the family of materials (where A = or , M = or ) and . also is an example of controllable \"negative thermal expansion\".\n\nOrdinary ice shows NTE in its hexagonal and cubic phases at very low temperatures (below –200 °C). In its liquid form, pure water also displays negative thermal expansivity below 3.984 °C.\n\nRubber elasticity shows NTE at normal temperatures, but the reason for the effect is rather different from that in most other materials. Put simply, as the long polymer chains absorb energy, they adopt a more contorted configuration, reducing the volume of the material.\n\nQuartz () and a number of zeolites also show NTE over certain temperature ranges. Fairly pure silicon (Si) has a negative coefficient of thermal expansion for temperatures between about 18 K and 120 K.\nCubic Scandium trifluoride has this property which is explained by the quartic oscillation of the fluoride ions. The energy stored in the bending strain of the fluoride ion is proportional to the fourth power of the displacement angle, unlike most other materials where it is proportional to the square of the displacement. A fluorine atom is bound to two scandium atoms, and as temperature increases the fluorine oscillates more perpendicularly to its bonds. This draws the scandium atoms together throughout the material and it contracts. exhibits this property from 10 to 1100 K above which it shows the normal positive thermal expansion.. Shape memory alloys such as NiTi are a nascent class of materials that exhibit zero and negative thermal expansion [13].\n\n 13. ^ Ahadi, A.; Matsushita, Y.; Sawaguchi, T.; Sun, QP.; Tsuchiya, K. (2017). \"Origin of zero and negative thermal expansion in severely-deformed superelastic NiTi alloy\". Acta Materialia. 124, 79–92. doi:10.1107/S0108768194004933.\nhttps://doi.org/10.1016/j.actamat.2016.10.054\n\n\n"}
{"id": "29214912", "url": "https://en.wikipedia.org/wiki?curid=29214912", "title": "On Virtue", "text": "On Virtue\n\nOn Virtue (; ) is a Socratic dialogue attributed to Plato, but which is considered spurious. In the short dialogue, Socrates discusses with a friend questions about whether virtue can be taught. To answer this question, the author of the dialogue does little more than copy out a few passages from the \"Meno\" almost word for word.\n\n"}
{"id": "54119209", "url": "https://en.wikipedia.org/wiki?curid=54119209", "title": "Relative hour (Jewish law)", "text": "Relative hour (Jewish law)\n\nRelative hour (Hebrew singular: / ; plural: / ), sometimes called halachic hour, seasonal hour and variable hour, is a term used in rabbinic Jewish law that assigns 12 hours to each day and 12 hours to each night, all throughout the year. A relative hour has no fixed radical, but changes with the length of each day - depending on summer (when the days are long and the nights are short), and on winter (when the days are short and the nights are long). Even so, in all seasons a day is always divided into 12 hours, and a night is always divided into 12 hours, which inevitably makes for a longer hour or a shorter hour. All of the hours mentioned by the Sages in either the Mishnah or Talmud, or in other rabbinic writings, refer strictly to relative hours.\n\nAnother feature of this ancient practice is that, unlike the standard modern 12-hour clock that assigns 12 o'clock pm for noon time, in the ancient Jewish tradition noon time was always the \"sixth hour\" of the day, whereas the \"first hour\" began with the break of dawn, by most exponents of Jewish law, and with sunrise by the Vilna Gaon and Rabbi Hai Gaon. 12:o'clock am (midnight) was also the \"sixth hour\" of the night, whereas the \"first hour\" of the night began when the first three stars appeared in the night sky.\nIn old times, the hour was detected by observation of the position of the sun, or when the first three stars appeared in the night sky. During the first six hours of the day, the sun is seen in the eastern sky. At the \"sixth hour\", the sun is always at its zenith in the sky, meaning, it is either directly overhead, or parallel (depending on the hemisphere). Those persons living in the Northern Hemisphere, the sun at noon time will appear overhead slightly towards the south, whereas for those living in the Southern Hemisphere, the sun at noon time will appear overhead slightly towards the north. From the 6th and a half hour to the 12th hour, the sun inclines towards the west, until it sets. The conclusion of a day at the end of twilight may slightly vary in minutes from place to place, depending on the elevation and the terrain. Typically, nightfall ushers in more quickly in the low-lying valleys, than it does on a high mountaintop.\nThe conventional Jewish way of calibrating the time of day is to reckon the \"first hour\" of the day with the rise of dawn (), that is to say, approximately 72 minutes before sunrise, and the end of the day commencing shortly after sunset when the first three medium-size stars have appeared in the night sky. From the moment of sunset when the sun is no longer visible until the appearance of the first three medium-size stars is a unit of time called evening twilight (). In the Talmud, twilight is estimated at being the time that it takes a person to walk three quarters of a biblical mile (i.e. 1,500 cubits, insofar that a biblical mile is equal to 2,000 cubits). According to Maran's \"Shulhan Arukh\", a man traverses a biblical mile in 18 minutes, meaning, one is able to walk three quarters of a mile in 13½ minutes. According to Maimonides, a man walks a biblical mile in 24 minutes, meaning, three quarters of a mile is done in 18 minutes. In Jewish law, the short period of dusk or twilight (from the moment the sun has disappeared over the horizon until the appearance of the first three stars) is a space of time whose designation is doubtful, partly considered day and partly considered night. When the first medium-size star appears in the night sky, it is still considered day; when the second star appears, it is an ambiguous case. When the third star appears, it is the beginning of the \"first hour\" of the night. Between the break of dawn and the first three medium-size stars that appear in the night sky there are always 12 hours.\n\nIn the Modern Age of astral science and of precise astronomical calculations, it is now possible to determine the length of the ever-changing hour by simple mathematics. To determine the length of each relative hour, one needs but simply know two variables: (a) the precise time of sunrise, and (b) the precise time of sunset. Since the actual day begins approximately 72 minutes before sunrise, and ends 13½ minutes after the sun has already set and can no longer be seen over the horizon (according to Maran), or 18 minutes (according to Maimonides), by collecting the total number of minutes in any given day and dividing the total number of minutes by 12, the dividend that one is left with is the number of minutes to each hour. In summer months, when the days are long, the length of each hour during daytime can be as much as 77 minutes or more, whereas the length of each hour during nighttime can be less than 42 minutes.\n\nIn Jewish Halacha, the practical bearing of this teaching is reflected in many \"halachic\" practices. For example, whenever observant Jews refer to the appointed time for reciting the verses of \"Kriyat Shema\", ideally, this recital must be made from the time of sunrise until the end of the \"third hour\" of the day, a time that actually fluctuates on the standard 12-hour clock, depending on summer and winter. Its application is also used in determining the time of the Morning Prayer, traditionally said, as a first resort, from sunrise until the end of the \"fourth hour\", but as a last resort can be said until noon time, and which times will vary if one were to rely solely on the dials of the standard 12-hour clock, depending on the Summer months and Winter months.\n\nOn the eve of Passover, Jews are only permitted to eat leavened bread-stuffs until the \"fourth-hour\" of the day.\n\nIn Jewish tradition, prayers were usually offered at the time of the daily whole-burnt offerings. The historian, Josephus, writing about the daily whole-burnt offering, says that it was offered twice each day, in the morning and about the \"ninth hour\". The Mishnah, a compendium of Jewish oral laws compiled in the late 2nd-century CE, says of the morning daily offering that it was offered in the \"fourth hour\", but says of the late afternoon offering: \"The daily whole-burnt offering was slaughtered at a half after the \"eighth hour\", and offered up at a half after the \"ninth hour\".\" Elsewhere, when describing the slaughter of the Passover offerings on the eve of Passover (the 14th day of the lunar month Nisan), Josephus writes: \"...their feast which is called the Passover, when they slay their sacrifices, from the \"ninth hour\" to the \"eleventh\", etc.\" (roughly corresponding to 3 o'clock pm to 5 o'clock pm). Conversely, the Mishnah states that on the eve of Passover the daily whole-burnt offering was slaughtered at a half past the \"seventh hour\", and offered up at a half past the \"eighth hour\".\n\n"}
{"id": "4033927", "url": "https://en.wikipedia.org/wiki?curid=4033927", "title": "Risk-seeking", "text": "Risk-seeking\n\nIn economics and finance, a risk-seeker or risk-lover is a person who has a preference \"for\" risk. While most investors are considered risk \"averse\", one could view casino-goers as risk-seeking. If offered either $50 or a 50% each chance of either $100 or nothing, a risk-seeking person would prefer the gamble even though the gamble and the sure thing have the same expected value. \n\nRisk-seeking behavior can be observed in the negative domain formula_1 for prospect theory value functions, where the functions are convex for formula_1 but concave for formula_3.\n\nIn a study done by Friedman et al. (1995), they found significant evidence to support that low childhood conscientiousness contributed heavily to adulthood mortality. Those who were high in conscientiousness as a child were 30% less likely to die in their adulthood. Ultimately, their findings solidified that low levels of childhood conscientiousness predict risk seeking, and risk-seeking increases the chance of accidental death. Though risk-seeking deteriorates with age, risky exposure to abusive substances in adolescence can lead to lifetime risk factors due to addiction. Conscientious individuals are subject to greater internal impulse control which lets them think out risky decisions more carefully, while those low on conscientiousness are more likely to endanger themselves and others by risky, or sometimes even criminal behaviour.\n\nThe psychometric paradigm explores what stable personality traits and risk behaviours have in common with an individualistic approach. Zuckerman's (1994) sensation seeking theory is important in assessing the causative factors of certain risk-seeking behaviours. Many risk-seeking behaviours justify humans need for sensation seeking. Behaviours like adventurous sports, drug use, promiscuous sex, entrepreneurship, gambling, and dangerous driving to name a few both represent sensation seeking, as well as risk seeking. Impulsivity has been linked to risk-seeking and can be described as the desire to indulge in situations with a potential reward, and little to no planning of the potential punishments of loss or reward. Impulsivity has also been linked to sensation seeking and in recent theories have been combined to form a higher order trait called impulsive sensation seeking.\n\nThe neuropsychological paradigm looks at why people make the decisions they do, as well as the neuropsychological processes that contribute to the decisions people make. This view looks less at impulsivity, puts more emphasis on cognitive dynamics and assumes people take risks because they have assessed the future outcomes.\n\nDemographic differences also play a role in risk-seeking between individuals. Through an analysis done by scientists, they demonstrated that men typically seek risks more than women. There are biological differences in men and women that may lead to the drive to seek risks. For example, testosterone plays a large role in risk-seeking in people and women have significantly lower levels of this hormone. This hormone has behavioural effects on aggression, mood and sexual function, all of which can lead to risk-seeking decision making. In their study, they also found that testosterone in excess leads to increased sexual enjoyment, and therefore more of an incentive to engage in risky unprotected sex.\n\nChoice under uncertainty is often characterized as the maximization of expected utility. Utility is often assumed to be a function of profit or final portfolio wealth, with a positive first derivative. The utility function whose expected value is maximized is convex for a risk-seeker, concave for a risk-averse agent, and linear for a risk-neutral agent. Its convexity in the risk-seeking case has the effect of causing a mean-preserving spread of any probability distribution of wealth outcomes to be preferred over the unspread distribution.\n"}
{"id": "36752193", "url": "https://en.wikipedia.org/wiki?curid=36752193", "title": "Sin Ik-hui", "text": "Sin Ik-hui\n\nSin Ik-hui (Chosŏn'gŭl: 신익히, hanja: 申翼熙) (9 June 1892 - 5 May 1956) was a Korean independence activist and politician during the period of Japanese rule. He was Speaker of the National Assembly during President Syngman Rhee's first term (4 August 1948 and 30 May 1950) and second term (19 June 1950 and 30 May 1954). His nickname was Haegong (해공, 海公) or Haehu (해후; 海候); his courtesy name was Yeogu (여구; 如耉).\n\nSin Ik-hui was a descendant of Sin Rip and Sin Kyung-hee, Sin Saimdang. He was born in Samaru country in Gwangju, Gyeonggi Province. He became an orphan and his second elder half-brother Sin Kyu-hee nurtured him. In his early years, he studied abroad in Japan.\n\nIn 1918, he was exiled to Shanghai in China, in April 1919.\n\nHe was involved in the creation of the Provisional National Assembly of Koreas. He was elected as a Congressman of the Provisional National Assembly of Korea. On April 23, he was appointed to Vice minister of Foreign Affairs of Provisional Government of Korea.\n\nIn August 1919, Sin became vice Minister of Justice and in September, he was appointed as Justice Minister and in September 1920, Minister of Foreign Affairs. In 1930s he became an English professor at a Chinese University.\n\nIn May 1940 he was appointed to Provisional Government of Korea, and in 1944 he was reappointed to Interior Minister to the Provisional Government.\n\nIn May 1948 he was elected Congressman of National Assembly of Korea. On August 4, 1948 he was 2nd term head of First Republic and 19 June 1950, he again was Speaker until 30 May 1954.\n\nIn 1955 he was involved with the founding of the Democratic Party and elected as its fourth leader. In 1956 he ran for president, but died of heart failure and overwork at age 64. He had boarded a train to Seoul with John Chang to commence campaigning soon after registration of candidates had closed. Minutes after taking their seats however, Sin became violently ill. He rushed to the toilet, but died. When the November election was held three months later, his name was still on the ballot, and he received close to half of the people's votes.\n\n\n"}
{"id": "195193", "url": "https://en.wikipedia.org/wiki?curid=195193", "title": "Sky", "text": "Sky\n\nThe sky (or celestial dome) is everything that lies above the surface of the Earth, including the atmosphere and outer space.\n\nIn the field of astronomy, the sky is also called the celestial sphere. This is viewed from Earth's surface as an abstract dome on which the Sun, stars, planets, and Moon appear to be traveling. The celestial sphere is conventionally divided into designated areas called constellations. Usually, the term \"sky\" is used informally as the point of view from the Earth's surface; however, the meaning and usage can vary. In some cases, such as in discussing the weather, the sky refers to only the lower, more dense portions of the atmosphere.\n\nDuring daylight, the sky appears to be blue because air scatters more blue sunlight than red. At night, the sky appears to be a mostly dark surface or region spangled with stars. During the day, the Sun can be seen in the sky unless obscured by clouds. In the night sky (and to some extent during the day) the Moon, planets and stars are visible in the sky. Some of the natural phenomena seen in the sky are clouds, rainbows, and aurorae. Lightning and precipitation can also be seen in the sky during storms. Birds, insects, aircraft, and kites are often considered to fly in the sky. Due to human activities, smog during the day and light pollution during the night are often seen above large cities.\n\nExcept for light that comes directly from the sun, most of the light in the day sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh Scattering. The scattering due to molecule sized particles (as in air) is greater in the forward and backward directions than it is in the lateral direction. Scattering is significant for light at all visible wavelengths but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source, the sun. The remaining sunlight, having lost some of its short wavelength components, appears slightly less blue.\n\nScattering also occurs even more strongly in clouds. Individual water droplets exposed to white light will create a set of colored rings. If a cloud is thick enough, scattering from multiple water droplets will wash out the set of colored rings and create a washed-out white color.\n\nThe sky can turn a multitude of colors such as red, orange, purple and yellow (especially near sunset or sunrise) when the light must pass through a much longer path (or optical depth) through the atmosphere. Scattering effects also partially polarize light from the sky and are most pronounced at an angle 90° from the sun. Scattered light from the horizon travels through as much as 38 times the atmosphere as does light from the zenith, causing a blue gradient: vivid at the zenith, and pale near the horizon. Because red light also scatters if there is enough air between the source and the observer causing parts of the sky to change color during a sunset. As the amount of atmosphere nears infinity, the scattered light appears whiter and whiter.\n\nThe sun is not the only object that may appear less blue in the atmosphere. Far away clouds or snowy mountaintops may appear yellowish. The effect is not very obvious on clear days but is very pronounced when clouds cover the line of sight, reducing the blue hue from scattered sunlight. At higher altitudes, the sky tends toward darker colors since scattering is reduced due to lower air density; an extreme example is the moon, where there is no atmosphere and no scattering, making the sky on the moon black even when the sun is visible.\n\nSky luminance distribution models have been recommended by the International Commission on Illumination (CIE) for the design of daylighting schemes. Recent developments relate to “all sky models” for modelling sky luminance under weather conditions ranging from clear to overcast.\n\nThe intensity of the sky varies greatly over the day, and the primary cause of that intensity differs as well. When the sun is well above the horizon, direct scattering of sunlight (Rayleigh scattering) is the overwhelmingly dominant source of light. However, in twilight, the period of time between sunset and night and between night and sunrise, the situation is more complicated. Green flashes and green rays are optical phenomena that occur shortly after sunset or before sunrise, when a green spot is visible above the sun, usually for no more than a second or two, or it may resemble a green ray shooting up from the sunset point. Green flashes are a group of phenomena that stem from different causes, most of which occur when there is a temperature inversion (when the temperature increases with altitude rather than the normal decrease in temperature with altitude). Green flashes may be observed from any altitude (even from an aircraft). They are usually seen at an unobstructed horizon, such as over the ocean, but are also seen over cloud tops and mountain tops. Green flashes may also be observed at the horizon in association with the Moon and bright planets, including Venus and Jupiter.\n\nThe Earth's shadow is the shadow that the Earth casts on its atmosphere. This atmospheric phenomenon is sometimes seen twice a day, around the times of sunset and sunrise. When the weather conditions and the observer's viewing point permit a clear sight of the horizon, the shadow can be seen as a dark blue or greyish-blue band. Assuming the sky is clear, the Earth's shadow is visible in the half of the sky opposite to the sunset or sunrise, and is seen as a dark blue band right above the horizon. A related phenomenon is the \"Belt of Venus\" or \"anti-twilight arch\", a pink band that is visible above the dark blue band of the Earth's shadow in the same part of the sky. There is no clear dividing line between the Earth's shadow and the Belt of Venus: one colored band shades into the other in the sky.\n\nTwilight is divided into three segments according to how far the sun is below the horizon, measured in segments of 6°. After sunset the civil twilight sets in; it ends when the sun drops more than 6° below the horizon. This is followed by the nautical twilight, when the sun is 6° and 12° below the horizon (heights of between −6° and −12°), after which comes the astronomical twilight, defined as the period from −12° to −18°. When the sun drops more than 18° below the horizon, the sky generally attains its minimum brightness.\n\nSeveral sources can be identified as the source of the intrinsic brightness of the sky, namely airglow, indirect scattering of sunlight, scattering of starlight, and artificial light pollution.\n\nThe term night sky refers to the sky as seen at night. The term is usually associated with skygazing and astronomy, with reference to views of celestial bodies such as stars, the Moon, and planets that become visible on a clear night after the Sun has set. Natural light sources in a night sky include moonlight, starlight, and airglow, depending on location and timing. The fact that the sky is not completely dark at night can be easily observed. Were the sky (in the absence of moon and city lights) absolutely dark, one would not be able to see the silhouette of an object against the sky.\n\nThe night sky and studies of it have a historical place in both ancient and modern cultures. In the past, for instance, farmers have used the state of the night sky as a calendar to determine when to plant crops. The ancient belief in astrology is generally based on the belief that relationships between heavenly bodies influence or convey information about events on Earth. The \"scientific\" study of the night sky and bodies observed within it, meanwhile, takes place in the science of astronomy.\n\nWithin visible-light astronomy, the visibility of celestial objects in the night sky is affected by light pollution. The presence of the Moon in the night sky has historically hindered astronomical observation by increasing the amount of ambient lighting. With the advent of artificial light sources, however, light pollution has been a growing problem for viewing the night sky. Special filters and modifications to light fixtures can help to alleviate this problem, but for the best views, both professional and amateur optical astronomers seek viewing sites located far from major urban areas.\n\nAlong with pressure tendency, the condition of the sky is one of the more important parameters used to forecast weather in mountainous areas. Thickening of cloud cover or the invasion of a higher cloud deck is indicative of rain in the near future. At night, high thin cirrostratus clouds can lead to halos around the moon, which indicate the approach of a warm front and its associated rain. Morning fog portends fair conditions and can be associated with a marine layer, an indication of a stable atmosphere. Rainy conditions are preceded by wind or clouds which prevent fog formation. The approach of a line of thunderstorms could indicate the approach of a cold front. Cloud-free skies are indicative of fair weather for the near future. The use of sky cover in weather prediction has led to various weather lore over the centuries.\n\nWithin 36 hours of the passage of a tropical cyclone's center, the pressure begins to fall and a veil of white cirrus clouds approaches from the cyclone's direction. Within 24 hours of the closest approach to the center, low clouds begin to move in, also known as the bar of a tropical cyclone, as the barometric pressure begins to fall more rapidly and the winds begin to increase. Within 18 hours of the center's approach, squally weather is common, with sudden increases in wind accompanied by rain showers or thunderstorms. Within six hours of the center's arrival, rain becomes continuous. Within an hour of the center, the rain becomes very heavy and the highest winds within the tropical cyclone are experienced. When the center arrives with a strong tropical cyclone, weather conditions improve and the sun becomes visible as the eye moves overhead. Once the system departs, winds reverse and, along with the rain, suddenly increase. One day after the center's passage, the low overcast is replaced with a higher overcast, and the rain becomes intermittent. By 36 hours after the center's passage, the high overcast breaks and the pressure begins to level off.\n\nFlight is the process by which an object moves, through or beyond the sky (as in the case of spaceflight), by generating aerodynamic lift, propulsive thrust, aerostatically using buoyancy, or by ballistic movement, without any direct mechanical support from the ground. The engineering aspects of flight are studied in aerospace engineering which is subdivided into aeronautics, which is the study of vehicles that travel through the air, and astronautics, the study of vehicles that travel through space, and in ballistics, the study of the flight of projectiles. While human beings have been capable of flight via hot air balloons since 1783, other species have used flight for significantly longer. Animals, such as birds, bats, and insects are capable of flight. Spores and seeds from plants use flight, via use of the wind, as a method of propagating their species.\n\nMany mythologies have deities especially associated with the sky. In Egyptian religion, the sky was deified as the goddess Nut and as the god Horus. Dyeus is reconstructed as the god of the sky, or the sky personified, in Proto-Indo-European religion, whence Zeus, the god of the sky and thunder in Greek mythology and the Roman god of sky and thunder Jupiter.\n\nIn Australian Aboriginal mythology, Altjira (or Arrernte) is the main sky god and also the creator god. In Iroquois mythology, Atahensic was a sky goddess who fell down to the ground during the creation of the Earth. Many cultures have drawn constellations between stars in the sky, using them in association with legends and mythology about their deities.\n\n\n"}
{"id": "51615806", "url": "https://en.wikipedia.org/wiki?curid=51615806", "title": "Social sorting", "text": "Social sorting\n\nSocial sorting is understood as the breakdown and categorization of group- or person-related raw data into various categories and segments by data manipulators and data brokers. Social sorting involves the key task of separating one group from the other. These groups can be based on income, education, race, ethnicity, profiling, gender, occupation, social status, derived power (social and political) and geographic residence. Depending on the goals of the manipulator raw data is collected and then further evolves into meaningful data in order to be exploited for a specific purpose. For example, the formulation of profiling and predictive policing are all derivations of social sorting.\n\nThe concept is accredited to David Lyon, a sociologist who is best known for his work in surveillance studies. Contemporary times have allowed for the influx and constant growth of data collection especially in the countries of the global north. A prime example of social terming is surveillance. This practice was not always as sophisticated as today. Historically, simple tools such as labor-intensive watchers, book keeping and record keeping acted as the enablers of this form. Surveillance is now done by governments and various organizations. This technological tools that are equipped with surveillance are cameras, records of transactions done at banking machines and point of sale terminals, machine readable passports before boarding, cellular phone calls along with many other examples. This collection of raw data then enables trends to be found and the fostering of predictions.\n\nIn other contextual fashion, social sorting bears much of its resemblance to social stratification. In primitive societies there are evidence of the roots of social sorting where the sexual division of labour was concerned. Women would do most of the gathering where men would concentrate on hunting. It was argued that although this specific task of the woman may point to domestic oppression according to some observers, hunter-gatherer women would not understand this interpretation. The primitive society did show groupings and deployed categorization which perhaps without their own understanding of the understanding of the social construct was still insinuated.\n\nFurther confirmation of social sorting was evident through slavery. The racial divide between whites and blacks manifested for generations. Based on the visual appearance of their skin people of a darker complexion were grouped and made to endure hardships that included beatings, laborious field work, confinement and executions. A prime example of social sorting at work during the African slave era were mullatos were grouped together to perform household and other domestic work.\n\nCriticisms are often directed at the laws, implemented rules, educational system, job employment opportunities and at the government. Questions are asked of the integrity of many socially constructed programs led by private and government institutions. Fairness and equity are thought to be at the forefront of the list of frustrations for many people that are aware of social sorting.\n\nThere are paranoias and indifferences in regards to social sorting. The September 11 attacks and the subsequent war on terror have fueled the desire for categorizing and profiling people. The beneficiaries that are associated with it are evident as it allows for a more transparent viewership. Some researchers such as David Lyon are concerned with the rise of big data as there are many implication on the daily lives of many.\n\nAccording to David Lyon, Canadians are still unaware of the fact that surveillance which goes collaboratively with social sorting is now very much integrated into their daily lives. David Lyon discusses that the systematic routines and attention to personal detail which is encompassed into surveillance. The key criticism involves indifferent treatment to individuals based on their profile. Depending on the details of a person it can lead to the determination of whether the person may end up on a No Fly List.\n\nEmployers have now begun to engage in these screening methods to determine a person’s suitability for a particular job. Law enforcement bodies and insurance companies have now all began to utilize social sorting to their use in order to determine whether their services should be offered or rescinded. The lives of many are directly affected as they are placed into socio-economic niches.\n\nDavid Lyon insinuates that social sorting through surveillance is a modern threat to freedom. Byproducts of social sorting are isolation, segregation and marginalization. Social sorting has called into account issues that primarily involve equity and fairness. Different societies sort in different ways.\n\nOther societies across the world engage in their own forms of social sorting. Some eastern countries such as China and India place much emphasis on an individual’s level of education. Blue collar jobs which at times may be dirty and laborious are often scorned and met with resentment. Low hourly wages, limited to zero prestige and little respect are directed at individuals who are involved in these occupational roles. Furthermore, the perception and low economic advantages hinder the progression of many people. At times, it acts as a domino effect where when one falls other problems come with it. For example, low education may not contribute to a good paying job; this may in turn lead to low income which may in turn lead to residing in a low class neighbourhood.\n\nWilson & McBrier (2005) conducted a longitudinal study based on the theory of minority vulnerability of employees. These constitute to a group of African Americans who work for good financial income in the upper tier for relatively privileged jobs. \"The minority vulnerability thesis, accordingly, maintains that African Americans are more likely to experience layoffs from upper-tier occupations than Whites even when the two groups have similar background socioeconomic statuses, have accumulated similar human-capital credentials, such as educational attainment and commitment to work, and have similar job/labor market characteristics, including union status as well as economic sector of employment. Findings indicate that, after controlling for seniority, African Americans are susceptible to layoffs on a relatively broad and generalized basis that is unstructured by traditional, stratification-based causal factors, namely, background socioeconomic status, human-capital credentials, and job/labor-market characteristics.\"\n\nSchools are accredited differing levels of prestige in comparison to other institutions. Social sorting occurs amongst schools where claims are made about learning experiences and which institutions may be the best for learning all of which are left up to subjection. For example, Princeton, and Harvard are all highly rated prestigious universities for various reasons. These perceptions cause employers and students alike to question the credibility and graduate documentation based on the institution that one attends.\n\nIn 2015, The Data Broker Accountability and Transparency Act was resurrected by four U.S senators that would allow consumers to see and correct personal information held by data brokers and tell those businesses to stop sharing or selling it for marketing purposes.\n\n"}
{"id": "1089321", "url": "https://en.wikipedia.org/wiki?curid=1089321", "title": "Steve Hofmeyr", "text": "Steve Hofmeyr\n\nSteve Hofmeyr (born 29 August 1964) is a South African singer, songwriter, political activist, actor and TV presenter.\n\nHofmeyr married actress Natasha Sutherland, whom he had met on the set of \"\" in 1998. They had two sons. Hofmeyr also has three other children by other women The couple was divorced after reports of numerous affairs dominated Hofmeyr's time in the spotlight in 2008.\n\nIn December 2008, Hofmeyr allegedly assaulted Esmaré Weideman, editor of \"Huisgenoot\", a popular Afrikaans magazine, by pouring a cup of cold tea over her at the Miss South Africa finals. He was said to have blamed her and two other journalists for his divorce from Sutherland. Miss Weideman subsequently dropped her accusations.\n\nOn 19 December 2013, Hofmeyr was arrested in Bronkhorstspruit for driving at 169 km/h in an 80 km/h zone and was released on bail of R500. He was subsequently fined R10,000 in the Bronkhorstspruit Magistrate's Court on 23 January 2014.\n\nHofmeyr married Janine van der Vyver on 26 January 2014. In 2008, van der Vyver, a fitness instructor, revealed they had been seeing each other for 10 years.\n\nHofmeyr's grandfather, Steve Hofmeyr Sr., was a leader in the Ossewabrandwag, a South African political and guerrilla organisation which supported Germany during World War II.\n\nIn January 2007, there were reports that one branch of the News Cafe restaurant chain would not play Hofmeyr's song \"Pampoen\". The managing director of the company that owns the franchise denies that this is company policy and points out that many Afrikaans acts, such as Karen Zoid and Arno Carstens have performed at News Cafe.\n\nOn 12 May 2011, Hofmeyr released the lyrics to his new song called \"Ons sal dit oorleef\", which means \"We will survive this\". The song is controversial, because Hofmeyr threatened to include the ethnic slur \"kaffir\" in the lyrics of the song. \nHofmeyr removed the offensive word in his song, citing that the word would offend his black friends and colleagues.\n\nIn 2011, he made public that he supports the Afrikaner advocacy group \"Expedition for Afrikaner Self-Determination\" (Onafhanklike Afrikaner Selfbeskikkingsekspedisie, OASE).\n\nHofmeyr was heavily criticised after performing the former South African national anthem, \"Die Stem\", at a cultural festival known as \"Innibos\" in Nelspruit in July 2014. He went on to perform the anthem on international tours, and encouraged white South Africans to continue singing it, stating that it did not contain any form of hate speech.\n\nIn October 2014, Hofmeyr wrote and published a tweet stating that he believed that black South Africans were the \"architects of apartheid\" on his public Twitter account. This prompted a significant public backlash. One of Hofmeyr's critics was puppeteer Conrad Koch through his puppet Chester Missing, who launched a campaign calling on companies to stop sponsoring Hofmeyr. On 27 November 2014, Hofmeyr failed to acquire a final protection order against Koch and his puppet in the Randburg Magistrate's Court.\n\nHofmeyr has given statements indicative of apartheid denialism, leading various journalists and political analysts to label him a \"disgrace to South Africa\".\n\nHofmeyr has made numerous claims relating to murders of white South Africans. Hofmeyr has claimed that whites, and in particular Afrikaners, are being \"killed like flies\", posting on Facebook that \"my tribe is dying\". Hofmeyr also posted a picture of a \"World Cup soccer stadium\" which he claimed could be filled by the number of whites murdered by blacks. However, Africa Check, a fact-checking organisation has found Hofmeyr's claims to be \"incorrect and grossly exaggerated\" - pointing out that whites are in fact \"less likely to be murdered than any other race group\". Lizette Lancaster from the Institute for Security Studies told Africa Check that \"Whites are far less likely to be murdered than their black or coloured counterparts.\" While white South Africans account for nearly 9% of the population they account for just 1.8% of murder victims.\nDuring May 2018, The minister of Police, Mr Bheki Cele released farm attack statistics for the first time in years. <\n\n\n\n\n\n"}
{"id": "3721610", "url": "https://en.wikipedia.org/wiki?curid=3721610", "title": "Synergistic gardening", "text": "Synergistic gardening\n\nSynergistic gardening is a system of organic gardening, developed by Emilia Hazelip. The system is strongly influenced by permaculture, as well as the work of Masanobu Fukuoka and Marc Bonfils. After establishing the garden, there is no further digging, ploughing or tilling, and no use of external inputs such as manures and other fertilizers, or pesticides. Soil health is maintained by the selection of plants, mulching, and recycling of plant residues.\n\n"}
{"id": "52938002", "url": "https://en.wikipedia.org/wiki?curid=52938002", "title": "Unistar Radio Top 20", "text": "Unistar Radio Top 20\n\nUnistar Radio Top 20 is a website listing the most popular songs in Belarus. It debuted in 2004. The site contains multiple links to other Belarussian radio station sites.\n"}
{"id": "52149572", "url": "https://en.wikipedia.org/wiki?curid=52149572", "title": "V&amp;A Digital Futures", "text": "V&amp;A Digital Futures\n\nV&A \"Digital Futures\" is a series of events organized by the Victoria and Albert Museum (V&A) in the area of digital art.\n\n\"Digital Futures\" events are organized by Irini Papadimitriou of the V&A, who started the events in 2012, some at the V&A museum itself and some elsewhere around London especially but also elsewhere in the United Kingdom. Some \"Digital Futures\" events are held in conjunction with EVA London. There are some associated publications.\n\nThe V&A museum has a significant collection of computer art.\n\n\n"}
