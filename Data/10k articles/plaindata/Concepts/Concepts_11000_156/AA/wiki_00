{"id": "47619364", "url": "https://en.wikipedia.org/wiki?curid=47619364", "title": "Abortion Support Network", "text": "Abortion Support Network\n\nThe Abortion Support Network is a UK based charity which provides financial assistance, accommodation and consultation to women from the Republic of Ireland and Northern Ireland who are seeking an abortion in the UK.\n\nThe charity was founded in 2009 by Mara Clarke. and continues the work of the (now defunct) London based 1980's Irish Women's Abortion Support Group.\n\nIn 2015 they received widespread attention in the UK due to the popular parenting site Mumsnet using their annual charity fundraising appeal to raise money for the ASN.\n\nIn 2017 as part of a coalition, ASN made a submission to the Citizens' Assembly. That same year, ASN fund-raised and provided over £73,000 (€84,000) worth of grants for all associated expenses of obtaining an abortion, including travel. The team of volunteers fielded 1,009 phone calls (685 from Ireland) providing free advice.\n\n"}
{"id": "5102790", "url": "https://en.wikipedia.org/wiki?curid=5102790", "title": "Alethiology", "text": "Alethiology\n\nAlethiology (or alethology, \"the study of aletheia\") literally means the \"study of truth\", but can more accurately be translated as \"the study of the nature of truth\".|\n\nIt could be argued that \"alethiology\" synonymous with \"epistemology\", the study of knowledge, and that dividing the two is mere semantics, but sometimes a distinction is made between the two. Epistemology is the study of knowledge and its acquisition. Alethiology is specifically concerned with the \"nature\" of truth, which is only one of the areas studied by epistemologists.\n\nThe term \"alethiology\" is rare. The ten-volume \"Routledge Encyclopedia of Philosophy\" mentions it only once, in the article \"Lambert, Johann Heinrich (1728–77)\":\nThe Encyclopædia Britannica Eleventh Edition describes the discipline as \"…an uncommon expression for the doctrine of truth, used by Sir William Hamilton in his philosophic writings when treating of the rules for the discrimination of truth and error.\"\n\nThe term appears in \"The Banalization of Nihilism\" (pp. 17–18) in contrast to several other types of nihilism, especially epistemological nihilism. The views of several philosophers are then distinguished by reference to 'alethiological nihilism', 'epistemological nihilism' and the like.\n\n"}
{"id": "7380835", "url": "https://en.wikipedia.org/wiki?curid=7380835", "title": "Autonomation", "text": "Autonomation\n\nAutonomation describes a feature of machine design to effect the principle of used in the Toyota Production System (TPS) and Lean manufacturing. It may be described as \"intelligent automation\" or \"automation with a human touch\". This type of automation implements some supervisory functions rather than production functions. At Toyota this usually means that if an abnormal situation arises the machine stops and the worker will stop the production line. It is a quality control process that applies the following four principles:\nAutonomation aims to prevent the production of defective products, eliminate overproduction and focus attention on understanding the problems and ensuring that they do not reoccur.\n\nAutonomation is called by Shigeo Shingo pre-automation. It separates workers from machines through mechanisms that detect production abnormalities (many machines in Toyota have these). He says there are twenty-three stages between purely manual and fully automated work. To be fully automated machines must be able to detect \"and\" correct their own operating problems which is currently not cost-effective. However, ninety percent of the benefits of full automation can be gained by autonomation.\n\nThe purpose of autonomation is that it makes possible the rapid or immediate address, identification and correction of mistakes that occur in a process. Autonomation relieves the worker of the need to continuously judge whether the operation of the machine is normal; their efforts are now only engaged when there is a problem alerted by the machine. As well as making the work more interesting this is a necessary step if the worker is to be asked later to supervise several machines. The first example of this at Toyota was the auto-activated loom of Sakichi Toyoda that automatically and immediately stopped the loom if the vertical or lateral threads broke or ran out.\n\nFor instance rather than waiting until the end of a production line to inspect a finished product, autonomation may be employed at early steps in the process to reduce the amount of work that is added to a defective product. A worker who is self-inspecting their own work, or source-inspecting the work produced immediately before their work station is encouraged to stop the line when a defect is found. This detection is the first step in Jidoka. A machine performing the same defect detection process is engaged in autonomation.\n\nOnce the line is stopped a supervisor or person designated to help correct problems gives immediate attention to the problem the worker or machine has discovered. To complete Jidoka, not only is the defect corrected in the product where discovered, but the process is evaluated and changed to remove the possibility of making the same mistake again. One solution to the problems can be to insert a \"mistake-proofing\" device somewhere in the production line. Such a device is known as poka-yoke.\n\nTaiichi Ohno and Sakichi Toyoda, originators of the TPS and practices in the manufacturing of textiles, machinery and automobiles considered JIT & Autonomation the pillars upon which TPS is built. Jeffrey Liker and David Meier indicate that Jidoka or \"the decision to stop and fix problems as they occur rather than pushing them down the line to be resolved later\" is a large part of the difference between the effectiveness of Toyota and other companies who have tried to adopt Lean Manufacturing. Autonomation, therefore can be said to be a key element in successful Lean Manufacturing implementations.\n\nFor just-in-time (JIT) systems, it is absolutely vital to produce with zero defects, or else these defects can disrupt the production process - or the orderly flow of work.\n\nJIT and Lean Manufacturing are always searching for targets for continuous improvement in its quest for quality improvements, finding and eliminating the causes of problems so they do not continually crop up.\n\nJidoka involves the automatic detection of errors or defects during production. When a defect is detected the halting of the production forces immediate attention to the problem.\n\nThe halting causes slowed production but it is believed that this helps to detect a problem earlier and avoids the spread of bad practices.\n\nThe word \"autonomation\" 自働化, a loan word from the Sino-Japanese vocabulary, is a portmanteau of \"autonomous\" and \"automation\" 自動化, which is written using three kanji characters: 自 \"self\", \"動\" \"movement\", and 化 \"-ization\". In the Toyota Production System, the second character is replaced with 働 \"work\", which is a character derived by adding a radical representing \"human\" to the original 動.\n\nRelated concepts from the Toyota Production System:\n"}
{"id": "56018123", "url": "https://en.wikipedia.org/wiki?curid=56018123", "title": "Cascade red fox", "text": "Cascade red fox\n\nThe Cascade red fox (\"Vulpes vulpes cascadensis\") is a subspecies of red fox native to Washington in the United States.\n\nThe range of the Cascade red fox is estimated to be 4500 km, but may be as large as 40000 km. It lives in the subalpine meadows and parklands of the Cascade Mountains, as well as the open forests on the eastern slope. It does not however inhabit the densely forested western slope. It may inhabit the very southern parts of British Columbia.\n\nThe ancestors of the Cascade red fox colonized North America after crossing the Bering landbridge during the Illinoian glacation over 300,000 years ago. During the Wisconsin glaciation, they were pushed south to escape to ice free forests. From that point, they adapted to the colder climate and became distinct. After the glaciation, they moved up into the mountains where conditions were similar.\n\nThe Cascade red fox is a Natural Heritage Critically Imperiled Species, as well as a Washington Candidate Species for protection. Surveys have suggested that there has been widespread population loss. The number of populations seems to be less than 5. Population trends may be because of habitat loss, timber removal, and increased winter recreation that provides pathways for other less adapted predators to enter alpine areas. Lack of information has made conservation efforts much harder.\n\n"}
{"id": "7346", "url": "https://en.wikipedia.org/wiki?curid=7346", "title": "Centimetre–gram–second system of units", "text": "Centimetre–gram–second system of units\n\nThe centimetre–gram–second system of units (abbreviated CGS or cgs) is a variant of the metric system based on the centimetre as the unit of length, the gram as the unit of mass, and the second as the unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways of extending the CGS system to cover electromagnetism.\n\nThe CGS system has been largely supplanted by the MKS system based on the metre, kilogram, and second, which was in turn extended and replaced by the International System of Units (SI). In many fields of science and engineering, SI is the only system of units in use but there remain certain subfields where CGS is prevalent.\n\nIn measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 as and . For example, the CGS unit of force is the dyne which is defined as , so the SI unit of force, the newton (), is equal to 100,000 dynes.\n\nOn the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is more subtle. Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on which system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian units, \"ESU\", \"EMU\", and Lorentz–Heaviside units. Among these choices, Gaussian units are the most common today, and \"CGS units\" often used specifically refers to CGS-Gaussian units.\n\nThe CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1873, a committee of the British Association for the Advancement of Science, including British physicists James Clerk Maxwell and William Thomson recommended the general adoption of centimetre, gram and second as fundamental units, and to express all derived electromagnetic units in these fundamental units, using the prefix \"C.G.S. unit of ...\".\n\nThe sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide general use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre–kilogram–second) system, which in turn developed into the modern SI standard.\n\nSince the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide, in the United States more slowly than elsewhere. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as \"The Astrophysical Journal\". CGS units are still occasionally encountered in technical literature, especially in the United States in the fields of material science, electrodynamics and astronomy. The continued usage of CGS units is most prevalent in magnetism and related fields because the B and H fields have the same units in free space and there is a lot of potential for confusion when converting published measurements from cgs to MKS.\n\nThe units gram and centimetre remain useful \"as prefixed units\" within the SI system, especially for instructional physics and chemistry experiments, where they match the small scale of table-top setups. However, where derived units are needed, the SI ones are generally used and taught instead of the CGS ones today. For example, a physics lab course might ask students to record lengths in centimetres, and masses in grams, but force (a derived unit) in newtons, a usage consistent with the SI system.\n\nIn mechanics, the CGS and SI systems of units are built in an identical way. The two systems differ only in the scale of two out of the three base units (centimetre versus metre and gram versus kilogram, respectively), while the third unit (second as the unit of time) is the same in both systems.\n\nThere is a one-to-one correspondence between the base units of mechanics in CGS and SI, and the laws of mechanics are not affected by the choice of units. The definitions of all derived units in terms of the three base units are therefore the same in both systems, and there is an unambiguous one-to-one correspondence of derived units:\n\nThus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:\n\nExpressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:\n\nThe conversion factors relating electromagnetic units in the CGS and SI systems are made more complex by the differences in the formulae expressing physical laws of electromagnetism as assumed by each system of units, specifically in the nature of the constants that appear in these formulae. This illustrates the fundamental difference in the ways the two systems are built: \n\nElectromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (seemingly independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows:\n\n\nMaxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where \"c\" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18 then Ampère's force law will contain a prefactor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Ampère's force law by setting formula_20 or formula_21, will lead to a constant prefactor in the Coulomb's law.\n\nIndeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge:\nThese two laws can be used to derive Ampère's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Ampère's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field.\n\nFurthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants ε and μ, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The units of P and M are usually so chosen that the factors λ and λ′ are equal to the \"rationalization constants\" formula_30 and formula_31, respectively. If the rationalization constants are equal, then formula_32. If they are equal to one, then the system is said to be \"rationalized\": the laws for systems of spherical geometry contain factors of 4π (for example, point charges), those of cylindrical geometry – factors of 2π (for example, wires), and those of planar geometry contain no factors of π (for example, parallel-plate capacitors). However, the original CGS system used λ = λ′ = 4π, or, equivalently, formula_33. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized.\n\nThe table below shows the values of the above constants used in some common CGS subsystems:\nThe constant \"b\" in SI system is a unit-based scaling factor defined as: formula_34.\n\nAlso, note the following correspondence of the above constants to those in Jackson and Leung:\n\nIn system-independent form, Maxwell's equations can be written as:\n\nformula_39\n\nNote that of all these variants, only in Gaussian and Heaviside–Lorentz systems formula_40 equals formula_41 rather than 1. As a result, vectors formula_42 and formula_43 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS.\n\nIn one variant of the CGS system, Electrostatic units (ESU), charge is defined via the force it exerts on other charges, and current is then defined as charge per time. It is done by setting the Coulomb force constant formula_44, so that Coulomb's law does not contain an explicit prefactor.\n\nThe ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: Therefore, in electrostatic CGS units, a franklin is equal to a centimetre times square root of dyne:\nThe unit of current is defined as:\n\nDimensionally in the ESU CGS system, charge \"q\" is therefore equivalent to mLt. Hence, neither charge nor current is an independent physical quantity in ESU CGS. This reduction of units is the consequence of the Buckingham π theorem.\n\nAll electromagnetic units in ESU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix \"stat\" or with a separate abbreviation \"esu\".\n\nIn another variant of the CGS system, electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_47, so that Ampère's force law simply contains 2 as an explicit prefactor (this prefactor 2 is itself a result of integrating a more general formulation of Ampère's law over the length of the infinite wire).\n\nThe EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:\nThe unit of charge in CGS EMU is:\n\nDimensionally in the EMU CGS system, charge \"q\" is therefore equivalent to mL. Hence, neither charge nor current is an independent physical quantity in EMU CGS.\n\nAll electromagnetic units in EMU CGS system that do not have proper names are denoted by a corresponding SI name with an attached prefix \"ab\" or with a separate abbreviation \"emu\".\n\nThe ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where \"c\" = 29,979,245,800 ≈ 3⋅10 is the speed of light in vacuum in centimetres per second. Therefore, the ratio of the corresponding \"primary\" electrical and magnetic units (e.g. current, charge, voltage, etc. – quantities proportional to those that enter directly into Coulomb's law or Ampère's force law) is equal either to \"c\" or \"c\":\nand\nUnits derived from these may have ratios equal to higher powers of \"c\", for example:\n\nThe practical cgs system is a hybrid system that uses the volt and the ampere as the unit of voltage and current respectively. Doing this avoids the inconveniently large and small quantities that arise for electromagnetic units in the esu and emu systems. This system was at one time widely used by electrical engineers because the volt and amp had been adopted as international standard units by the International Electrical Congress of 1881. As well as the volt and amp, the farad (capacitance), ohm (resistance), coulomb (electric charge), and henry are consequently also used in the practical system and are the same as the SI units. However, intensive properties (that is, anything that is per unit length, area, or volume) will not be the same as SI since the cgs unit of distance is the centimetre. For instance electric field strength is in units of volts per centimetre, magnetic field strength is in oersteds and resistivity is in ohm-cm.\n\nSome physicists and electrical engineers in North America still use these hybrid units.\n\nThere were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These also include the Gaussian units and the Heaviside–Lorentz units.\n\nIn this table, \"c\" = 29,979,245,800 is the numeric value of the speed of light in vacuum when expressed in units of centimetres per second. The symbol \"↔\" is used instead of \"=\" as a reminder that the SI and CGS units are \"corresponding\" but not \"equal\" because they have incompatible dimensions. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1 F in SI, then it has a capacitance of (10 \"c\") cm in ESU; \"but\" it is usually incorrect to replace \"1 F\" with \"(10 \"c\") cm\" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is \"always\" correct to replace \"1 m\" with \"100 cm\" within an equation or formula.)\n\nOne can think of the SI value of the Coulomb constant \"k\" as:\nThis explains why SI to ESU conversions involving factors of \"c\" lead to significant simplifications of the ESU units, such as 1 statF = 1 cm and 1 statΩ = 1 s/cm: this is the consequence of the fact that in ESU system \"k\" = 1. For example, a centimetre of capacitance is the capacitance of a sphere of radius 1 cm in vacuum. The capacitance \"C\" between two concentric spheres of radii \"R\" and \"r\" in ESU CGS system is:\nBy taking the limit as \"R\" goes to infinity we see \"C\" equals \"r\".\n\nWhile the absence of explicit prefactors in some CGS subsystems simplifies some theoretical calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus “15 emu” may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra multiplicative factors in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1.0 ampere is a fixed value of a specified quantity, and so are 1.0 henry, 1.0 ohm, and 1.0 volt.\n\nA key virtue of the Gaussian CGS system is that electric and magnetic fields have the same units, 4\"πε\" is replaced by 1, and the only dimensional constant appearing in the Maxwell equations is \"c\", the speed of light. The Heaviside–Lorentz system has these desirable properties as well (with \"ε\" equaling 1), but it is a “rationalized” system (as is SI) in which the charges and fields are defined in such a way that there are many fewer factors of 4\"π\" appearing in the formulas, and it is in Heaviside–Lorentz units that the Maxwell equations take their simplest form.\n\nIn SI, and other rationalized systems (for example, Heaviside–Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4π, those concerning coils of current and straight wires contain 2π and those dealing with charged surfaces lack π entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have eliminated this \"advantage\". In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally.\n\nSpecialized unit systems are used to simplify formulas even further than \"either\" SI \"or\" CGS, by eliminating constants through some system of natural units. For example, in particle physics a system is in use where every quantity is expressed by only one unit of energy, the electronvolt, with lengths, times, and so on all converted into electronvolts by inserting factors of speed of light \"c\" and the Planck constant \"ħ\". This unit system is very convenient for calculations in particle physics, but it would be considered impractical in other contexts.\n\n\n"}
{"id": "2180274", "url": "https://en.wikipedia.org/wiki?curid=2180274", "title": "Chaining", "text": "Chaining\n\nChaining is an instructional procedure used in behavioral psychology, experimental analysis of behavior and applied behavior analysis. It involves reinforcing individual responses occurring in a sequence to form a complex behavior. It is frequently used for training behavioral sequences (or \"chains\") that are beyond the current repertoire of the learner. The term is often credited to the work of B.F. Skinner, an American psychologist working at Harvard University in the 1930s. \n\nThe chain of responses is broken down into small steps using task analysis. Parts of a chain are referred to as links. The learner's skill level is assessed by an appropriate professional and is then either taught one step at a time while being assisted through the other steps forward or backwards or if the learner already can complete a certain percentage of the steps independently, the remaining steps are all worked on during each trial total task. A verbal stimulus or prompt is used at the beginning of the teaching trial. The stimulus change that occurs between each response becomes the reinforcer for that response as well as the prompt/stimulus for the next response without requiring assistance from the teacher. For example, in purchasing a soda you pull the money out of your pocket and see the money in your hand and then put the money in the machine. Seeing the money in your hand both was the reinforcer for the first response (getting money out of pocket) and was what prompted you to do the next response (putting money in machine).\n\nAs small chains become mastered, i.e. are performed consistently following the initial discriminative stimulus prompt, they may be used as links in larger chains. (Ex. teach hand washing, tooth brushing, and showering until mastered and then teach morning hygiene routine which includes the mastered skills). Chaining requires that the teachers present the training skill in the same order each time and is most effective when teachers are delivering the same prompts to the learner. The most common forms of chaining are backward chaining, forward chaining, and total task presentation.\n\nThere are two different types of chains: homogeneous and heterogeneous. The prior homogeneous chains occur when the topography or form of response are similar in each component. In contrast, a heterogeneous chain requires different types of responses for each link.\n\n\n"}
{"id": "15841082", "url": "https://en.wikipedia.org/wiki?curid=15841082", "title": "Choice modelling", "text": "Choice modelling\n\nChoice modelling attempts to model the decision process of an individual or segment via revealed preferences or stated preferences made in a particular context or contexts. Typically, it attempts to use discrete choices (A over B; B over A, B & C) in order to infer positions of the items (A, B and C) on some relevant latent scale (typically \"utility\" in economics and various related fields). Indeed many alternative models exist in econometrics, marketing, sociometrics and other fields, including utility maximization, optimization applied to consumer theory, and a plethora of other identification strategies which may be more or less accurate depending on the data, sample, hypothesis and the particular decision being modelled. In addition, choice modelling is regarded as the most suitable method for estimating consumers' willingness to pay for quality improvements in multiple dimensions.\nThere are a number of terms which are considered to be synonyms with the term choice modelling. Some are accurate (although typically discipline or continent specific) and some are used in industry applications, although considered inaccurate in academia (such as conjoint analysis).\n\nThese include the following:\n\nAlthough disagreements in terminology persist, it is notable that the academic journal intended to provide a cross-disciplinary source of new and empirical research into the field is called the Journal of Choice Modelling.\n\nThe theory behind choice modelling was developed independently by economists and mathematical psychologists. The origins of choice modelling can be traced to Thurstone's research into food preferences in the 1920s and to random utility theory. In economics, random utility theory was then developed by Daniel McFadden and in mathematical psychology primarily by Duncan Luce and Anthony Marley. In essence, choice modelling assumes that the utility (benefit, or value) that an individual derives from item A over item B is a function of the frequency that (s)he chooses item A over item B in repeated choices. Due to his use of the normal distribution Thurstone was unable to generalise this binary choice into a multinomial choice framework (which required the multinomial logistic regression rather than probit link function), hence why the method languished for over 30 years. However, in the 1960s through 1980s the method was axiomatised and applied in a variety of types of study.\n\nChoice modelling is used in both revealed preference (RP) and stated preference (SP) studies. RP studies use the choices made already by individuals to estimate the value they ascribe to items - they \"reveal their preferences - and hence values (utilities) – by their choices\". SP studies use the choices made by individuals made under experimental conditions to estimate these values – they \"state their preferences via their choices\". McFadden successfully used revealed preferences (made in previous transport studies) to predict the demand for the Bay Area Rapid Transit (BART) before it was built. Luce and Marley had previously axiomatised random utility theory but had not used it in a real world application; furthermore they spent many years testing the method in SP studies involving psychology students.\n\nMcFadden's work earned him the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 2000. However, much of the work in choice modelling had for almost 20 years been proceeding in the field of stated preferences. Such work arose in various disciplines, originally transport and marketing, due to the need to predict demand for new products that were potentially expensive to produce. This work drew heavily on the fields of conjoint analysis and design of experiments, in order to:\nSpecifically, the aim was to present the minimum number of pairs/triples etc of (for example) mobile/cell phones in order that the analyst might estimate the value the consumer derived (in monetary units) from every possible feature of a phone. In contrast to much of the work in conjoint analysis, discrete choices (A versus B; B versus A, B & C) were to be made, rather than ratings on category rating scales (Likert scales). David Hensher and Jordan Louviere are widely credited with the first stated preference choice models. They remained pivotal figures, together with others including Joffre Swait and Moshe Ben-Akiva, and over the next three decades in the fields of transport and marketing helped develop and disseminate the methods. However, many other figures, predominantly working in transport economics and marketing, contributed to theory and practice and helped disseminate the work widely.\n\nChoice modelling from the outset suffered from a lack of standardisation of terminology and all the terms given above have been used to describe it. However, the largest disagreement has proved to be geographical: in the Americas, following industry practice there, the term \"choice-based conjoint analysis\" has come to dominate. This reflected a desire that choice modelling (1) reflect the attribute and level structure inherited from conjoint analysis, but (2) show that discrete choices, rather than numerical ratings, be used as the outcome measure elicited from consumers. Elsewhere in the world, the term discrete choice experiment has come to dominate in virtually all disciplines. Louviere (marketing and transport) and colleagues in environmental and health economics came to disavow the American terminology, claiming that it was misleading and disguised a fundamental difference discrete choice experiments have from traditional conjoint methods: discrete choice experiments have a testable theory of human decision-making underpinning them (random utility theory), whilst conjoint methods are simply a way of decomposing the value of a good using \"statistical\" designs from numerical ratings that have no \"psychological\" theory to explain what the rating scale numbers mean.\n\nDesigning a choice model or discrete choice experiment (DCE) generally follows the following steps:\n\nThis is often the easiest task, typically defined by:\n\nA good or service, for instance mobile (cell) phone, is typically described by a number of attributes (features). Phones are often described by shape, size, memory, brand, etc. The attributes to be varied in the DCE must be all those that are of interest to respondents. Omitting key attributes typically causes respondents to make inferences (guesses) about those missing from the DCE, leading to omitted variable problems. The levels must typically include all those currently available, and often are expanded to include those that are possible in future – this is particularly useful in guiding product development.\n\nA strength of DCEs and conjoint analyses is that they typically present a subset of the full factorial. For example, a phone with two brands, three shapes, three sizes and four amounts of memory has 2x3x3x4=72 possible configurations. This is the full factorial and in most cases is too large to administer to respondents. Subsets of the full factorial can be produced in a variety of ways but in general they have the following aim: to enable estimation of a certain limited number of parameters describing the good: main effects (for example the value associated with brand, holding all else equal), two-way interactions (for example the value associated with this brand and the smallest size, that brand and the smallest size), etc. This is typically achieved by deliberately confounding higher order interactions with lower order interactions. For example, two-way and three-way interactions may be confounded with main effects. This has the following consequences:\nThus, researchers have repeatedly been warned that design involves critical decisions to be made concerning whether two-way and higher order interactions are likely to be non-zero; making a mistake at the design stage effectively invalidates the results since the hypothesis of higher order interactions being non-zero is untestable.\n\nDesigns are available from catalogues and statistical programs. Traditionally they had the property of Orthogonality where all attribute levels can be estimated independently of each other. This ensures zero collinearity and can be explained using the following example.\n\nImagine a car dealership that sells both luxury cars and used low-end vehicles. Using the utility maximisation principle and assuming an MNL model, we hypothesise that the decision to buy a car from this dealership is the sum of the individual contribution of each of the following to the total utility.\n\nUsing multinomial regression on the sales data however will not tell us what we want to know. The reason is that much of the data is collinear since cars at this dealership are either:\n\n\nThere is not enough information, nor will there ever be enough, to tell us whether people are buying cars because they are European, because they are a BMW or because they are high performance. This is a fundamental reason why RP data are often unsuitable and why SP data are required. In RP data these three attributes always co-occur and in this case are perfectly correlated. That is: all BMWs are made in Germany and are of high performance. These three attributes: origin, marque and performance are said to be collinear or non-orthogonal. Only in experimental conditions, via SP data, can performance and price be varied independently – have their effects decomposed.\n\nAn experimental design (below) in a Choice Experiment is a strict scheme for controlling and presenting hypothetical scenarios, or choice sets to respondents. For the same experiment, different designs could be used, each with different properties. The best design depends on the objectives of the exercise.\n\nIt is the experimental design that drives the experiment and the ultimate capabilities of the model. Many very efficient designs exist in the public domain that allow near optimal experiments to be performed.\n\nFor example the Latin square 16 design allows the estimation of all main effects of a product that could have up to 16 (approximately 295 followed by eighteen zeros) configurations. Furthermore this could be achieved within a sample frame of only around 256 respondents.\n\nBelow is an example of a much smaller design. This is 3 main effects design.\n\nThis design would allow the estimation of main effects utilities from 81 (3) possible product configurations \"assuming all higher order interactions are zero\". A sample of around 20 respondents could model the main effects of all 81 possible product configurations with statistically significant results.\n\nSome examples of other experimental designs commonly used:\n\n\nMore recently, efficient designs have been produced. These typically minimise functions of the variance of the (unknown but estimated) parameters. A common function is the D-efficiency of the parameters. The aim of these designs is to reduce the sample size required to achieve statistical significance of the estimated utility parameters. Such designs have often incorporated Bayesian priors for the parameters, to further improve statistical precision. Highly efficient designs have become extremely popular, given the costs of recruiting larger numbers of respondents. However, key figures in the development of these designs have warned of possible limitations, most notably the following. Design efficiency is typically maximised when good A and good B are as different as possible: for instance every attribute (feature) defining the phone differs across A and B. This forces the respondent to trade across price, brand, size, memory, etc; no attribute has the same level in both A and B. This may impose cognitive burden on the respondent, leading him/her to use simplifying heuristics (\"always choose the cheapest phone\") that do not reflect his/her true utility function (decision rule). Recent empirical work has confirmed that respondents do indeed have different decision rules when answering a less efficient design compared to a highly efficient design.\n\nMore information on experimental designs may be found here. It is worth reiterating, however, that small designs that estimate main effects typically do so by deliberately confounding higher order interactions with the main effects. This means that unless those interactions are zero in practice, the analyst will obtain biased estimates of the main effects. Furthermore (s)he has (1) no way of testing this, and (2) no way of correcting it in analysis. This emphasises the crucial role of design in DCEs.\n\nConstructing the survey typically involves:\n\nTraditionally, DCEs were administered via paper and pen methods. Increasingly, with the power of the web, internet surveys have become the norm. These have advantages in terms of cost, randomising respondents to different versions of the survey, and using screening. An example of the latter would be to achieve balance in gender: if too many males answered, they can be screened out in order that the number of females matches that of males.\n\nAnalysing the data from a DCE requires the analyst to assume a particular type of decision rule - or functional form of the utility equation in economists' terms. This is usually dictated by the design: if a main effects design has been used then two-way and higher order interaction terms cannot be included in the model. Regression models are then typically estimated. These often begin with the conditional logit model - traditionally, although slightly misleadingly, referred to as the multinomial logistic (MNL) regression model by choice modellers. The MNL model converts the observed choice frequencies (being estimated probabilities, on a ratio scale) into utility estimates (on an interval scale) via the logistic function. The utility (value) associated with every attribute level can be estimated, thus allowing the analyst to construct the total utility of any possible configuration (in this case, of car or phone). However, a DCE may alternatively be used to estimate non-market environmental benefits and costs.\n\n\n\nYatchew and Griliches first proved that means and variances were confounded in limited dependent variable models (where the dependent variable takes any of a discrete set of values rather than a continuous one as in conventional linear regression). This limitation becomes acute in choice modelling for the following reason: a large estimated beta from the MNL regression model or any other choice model can mean:\nThis has significant implications for the interpretation of the output of a regression model. All statistical programs \"solve\" the mean-variance confound by setting the variance equal to a constant; all estimated beta coefficients are, in fact, an estimated beta multiplied by an estimated lamda (an inverse function of the variance). This tempts the analyst to ignore the problem. However (s)he must consider whether a set of large beta coefficients reflect strong preferences (a large true beta) or consistency in choices (a large true lamda), or some combination of the two. Dividing all estimates by one other – typically that of the price variable – cancels the confounded lamda term from numerator and denominator. This solves the problem, with the added benefit that it provides economists with the respondent's willingness to pay for each attribute level. However, the finding that results estimated in \"utility space\" do not match those estimated in \"willingness to pay space\", suggests that the confound problem is not solved by this \"trick\": variances may be attribute specific or some other function of the variables (which would explain the discrepancy). This is a subject of current research in the field.\n\nMajor problems with ratings questions that do not occur with choice models are:\n\nRankings do tend to force the individual to indicate relative preferences for the items of interest. Thus the trade-offs between these can, like in a DCE, typically be estimated. However, ranking models must test whether the same utility function is being estimated at every ranking depth: e.g. the same estimates (up to variance scale) must result from the bottom rank data as from the top rank data.\n\nBest–worst scaling (BWS) is a well-regarded alternative to ratings and ranking. It asks people to choose their most and least preferred options from a range of alternatives. By subtracting or integrating across the choice probabilities, utility scores for each alternative can be estimated on an interval or ratio scale, for individuals and/or groups. Various psychological models may be utilised by individuals to produce best-worst data, including the MaxDiff model.\n\nChoice modelling is particularly useful for:\n\n\n\n"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "48748780", "url": "https://en.wikipedia.org/wiki?curid=48748780", "title": "Deictic field and narration", "text": "Deictic field and narration\n\nIn linguistics, psychology, and literary theory, the concepts of deictic field and deictic shift are sometimes deployed in the study of narrative media. These terms provide a theoretical framework for helping literary analysts to conceptualize the ways in which readers redirect their attention away from their immediate surroundings as they become immersed in the reality generated by the text.\n\nThe term \"deixis\" refers to the ways in which language encodes contextual information into its grammatical system. More broadly, deixis refers to the inherent ambiguity of certain linguistic expressions and the interpretive processes that communicants must perform in order to disambiguate these words and phrases. Such ambiguity can only be resolved by analyzing the context in which the utterance occurs. To understand deixis, one must first understand that language grammaticalizes context-dependent features such as person, space, and time. When language is oriented toward its context, certain expressions in speech emerge that differentiate the \"here\" and \"now\" (proximal deixis) from the \"then\" and \"there\" (distal deixis). According to Karl Buhler, an Austrian psychologist who was one of the earliest to present a theory of deixis, \"When philosophers, linguists, and narrative theorists attempt to understand the role of subjectivity in language and conversely, the role of language in subjectivity, they invariably notice a certain aspect of language which seems to depend on extralinguistic, subjective, occasion-specific considerations.\" Within the context of narrative, deixis reflects those aspects of storytelling by which the audience is pragmatically directed to understand the perspective of the narrator or the perspective of the story's characters in relation to their own story-external vantage point. Essentially, deictic expressions help form the layers of narrative that direct the audience to either the narratorial discourse or to the story world. \"Deixis (adjectival form, deictic) is a psycholinguistic term for those aspects of meaning associated with self-world orientation\". Deixis is an integral component of the lens by which the audience perceives the narrative.\n\nWhen examining perspectives on narration in natural-language environments, one must not ignore William Labov, who argues that stories of personal experience can be divided into distinct sections, each of which serves a unique function within the narrative progression. Labov schematizes the organization of natural narrative using the following conceptual units: abstract, orientation, complicating action, resolution, evaluation, and coda. Generally, anecdotal narratives tend to arrange these units in the order outlined; however, this is not an inflexible, structural progression that defines how every narrative must develop. For instance, sentences and phrasal items that serve an evaluative function can be interspersed throughout a narrative. Some stretches of narrative discourse also feature overlap among these Labovian categories. Each of Labov's narrative divisions serves a characteristic purpose typified by a particular section's grammatical construction and functional role within the unfolding narrative, but the boundaries of such divisions are not always clear-cut.\n\nAs a feature of natural language, deixis is an important element of oral narrative and can be realized in different ways in each of Labov's categories. According to Galbraith, \"All language use depends on some felt relevance to situation, on the attention of participants, and their ability to lift out the topic...Like zero in mathematics and the dark space in the theater, deixis orients us within a situation without calling attention to itself\". Two of Labov's categories that often feature deixis prominently are the \"orientation\" and the \"coda\". The orientation typically occurs near the beginning of a narrative and serves to introduce the characters, settings, and events. Given its presentative quality, the orientation shifts the deictic center away from the speaker's here-and-now into the spatiotemporal coordinates of the story, which logically must occur at a time prior to the story's enunciation. The coda occurs toward the end of a narrative and functions as a means of terminating the flow of story events. By doing so, the coda reorients the speakers and listeners out of the story world and back into the communicative present.\n\nThe deictic center—sometimes called the \"origo\" or zero-point—represents the originating source in relation to which deictic expressions gain their context-dependent meaning. Often the deictic center is the speaker: thus, any tokens of \"I\" in the speaker's discourse must deictically refer back to the speaker as center; likewise, the word \"you\" must project outward from this center toward the addressee. Any participants not part of this communicative channel will be referred to in the third person. The theory of deixis is therefore egocentric in that the indexical anchorage of deictic expressions is a function of this zero-point of subjectivity. The \"I\"-center serves as the perceptual vantage point that surveys relations among salient contextual entities and events. Such a center, therefore, determines which deictic expressions are pragmatically licensed by a context that has been naturally delimited through this perceptual and evaluative locus. Thus, the appropriateness of a proximal \"this\" over a distal \"that\" is determined by the nearness of an object or a location in relation to the deictic zero-point.\n\nA deictic field contains the range of bounded participants and objects, spatial locations and landmarks, and temporal frames that point back to some deictic center as the source for their pragmatic demarcation. The deictic field radiates out from the deictic center, and the boundaries of such a field enclose the scope of objects, spaces, and events that constitute a set from which deictic expressions might seek out a potential referent. These fields function as cognitive frames that participants in a discourse can use to conceptualize their contextual surroundings in relation to each interlocutor's (alternating) function as deictic center across communicative turns. Within the context of literature, the presence of multiple deictic fields in a text can be fruitfully analyzed using the cognitive principle of deictic shift (discussed below).\n\nDeictic shift theory (DST) refers to a range of immersion processes by which readers imaginatively project hypothetical deictic centers that are anchored to communicative and experiential loci within a narrative. Such cognitive framing, theorists of DST argue, form a necessary part of the reader's involvement in narrative, whereby through a process of frame shifting the reader constructs a story world by interpreting the (deictic) cues instantiated in the text. Deictic shifting can be accomplished in several ways. The most basic shift involves the reader's initial immersion into the world of the story. Here the deictic center moves out of the here-and-now of the reader's physical environment and becomes anchored to some text-internal perceptual or presentative instance, in most cases the deictic center of a character or a narrator. Deictic shifts at the level of narration include those cues that implicate a covert or overt narrator—specifically, story commentary or instances when the narrator refers to himself or herself as an \"I.\" Such instances of commentary and evaluation often reflect the perceptual field, as well as the interpretive and ideological stance, of the narrator as he or she presents the story's events.\n\nWithin the world of the story, deictic shifts occur in a number of ways. A fundamental shift occurs when the deictic center moves from one character to another—for instance, in cases of omniscient thought report. Here the reader must adjust the deictic center accordingly and interprets the lens of the current focal window through the experiential subjectivity of the character-locus. Other forms of story-internal deictic shifts involve the cognitive framing associated with embedded narratives and other discourse-types: stories-within-stories, letters in epistolary fiction, diary entries, hypertext ruptures, etc.\n\"Within literary scholarship, it is often noted that first and second person pronouns (and less so, and differently, third) facilitate readerly identification with the textually inscribed position (the position of the character or narrator designated by that pronoun), and evoke a sense of readerly conceptual immersion in the fictional world of the story, contributing to the ways in which the scene is imaginatively 'realized' in the mind of the reader, particularly the perspective from which the scene is conceptually visualized. Cognitive poetics and cognitive narratology have employed deictic shift theory, largely based on the work of Duchan, Bruder and Hewitt, to attempt to offer a cognitive account of how these interpretative effects are created. DST proposes that readers conceptually project to the contextual locus of the speaker of deictic cues in order to comprehend them, offering a model of how the deictic referents determining such contextual coordinates are processed by readers, and how this contributes to readers' conceptualization of the world of the story\" (Deictic shifting in literature: Experimental analysis).\n\nBuhler applied the theory of deixis to narratives. He proposed the concept of Zeigfeld, or deictic field, which operates in three modes: the first, \"ad oculos\", \"operates in the here-and-now of the speaker's sensible environment;\" the second, anaphora, \"operates in the context of the discourse itself considered as a structured environment;\" and the third, what Buhler calls \"deixis at phantasma\", operates in the context \"of imagination and long-term memory.\" Buhler's model attempts \"to describe the psychological and physical process whereby the live deictic field of our own bodily orientation and experience\" is \"transposed into an imaginative construction.\" According to Buhler, \"the body-feeling representation, or Korspertastbild (what psychologists would probably now call the body schema), becomes loosened from its involvement with the HERE//NOW/I deictic coordinates of waking life in our immediate environment, and becomes available to translation into an environment we construct both conceptually and orientationally\"; this deictic coordinate system is used \"in the constructive environment to orient ourselves within 'the somewhere-realm of pure imagination and the there-and-there in memory'\".\n\nKatie Hamburger, a German narrative theorist, studied and theorized how deictic words are used in literature. In her work \"The Logic of Literature\", she argued that there are two realms of language act: reality statement and fiction (Galbraith, 24-25). Reality statements are by someone and about something. \"Acts of fictional narration, on the other hand, transfer their referentiality from the actuality of the historical world to the entertained reality of the fictive world, and transfer the subjectivity of the speaker to the subjectivity of the story world characters\". Hamburger argued that this transfer occurs due to the use of deictic adverbs, and psychological verbs.\n"}
{"id": "56261374", "url": "https://en.wikipedia.org/wiki?curid=56261374", "title": "Devonshire House Ball of 1897", "text": "Devonshire House Ball of 1897\n\nThe Devonshire House Ball or the Devonshire House Fancy Dress Ball was an elaborate fancy dress ball, hosted by the Duke and Duchess of Devonshire, held on 2 July 1897 at Devonshire House in Piccadilly to celebrate Queen Victoria’s diamond jubilee. Due to the many prominent royals, aristocrats, and society figures who attended as well as the overall lavishness of the ball, it was considered the event of the 1897 London Season.\n\nIn 1897, The Duke and Duchess of Devonshire hosted the Devonshire House Ball at Devonshire House, the London residence (in Piccadilly) of the Dukes of Devonshire in the 18th and 19th centuries. The Duke had served as a Member of Parliament and a cabinet minister as a member of the Liberal Party and the Duchess, known as the Double Duchess, was the widow of the William Montagu, 7th Duke of Manchester.\n\nFollowing the death of Prince Albert in 1861, Queen Victoria had withdrawn from social life and \"the mantle of royal entertaining\" was passed to the Prince of Wales and his wife, Alexandra. During the 1870s, they hosted a costume ball at Marlborough House, their London residence, which was considered a success and carried on the popularity of such events. The Devonshires, who were close friends of the Prince and Princess of Wales, therefore, decided to throw a costume ball thrown to celebrate Queen Victoria's diamond jubilee. The Queen's Diamond Jubilee procession had taken place on 22 June 1897 and followed a route six miles long through London. More than 700 invitations were sent out a month before the event, although some reports of the event stated up to 3,000 invites. By accident, Alfred, Duke of Saxe-Coburg and Gotha and Maria, the Duchess of Saxe-Coburg and Gotha did not receive invitations. When the Duchess of Devonshire saw her at a different jubilee fête and asked if she was coming, \"the Duchess of Saxe-Coburg and Gotha freezingly replied, 'Certainly not'\".\n\nWhile the Queen did not attend, almost all of the British royal family attended the ball and nearly every other European royal family was represented. \"The New York Times\" claimed in September 1897 that the ball was, \"to a certain degree a duplicate of the Bradley Martin ball\" which was held at The Waldorf Hotel in New York City on 10 February 1897.\n\nThe Duke of Devonshire invited the London photographic firm of James Lafayette, who had been awarded a Royal Warrant ten years previously, to set up a tent (in the garden behind the house) to photograph the guests in costume during the Ball. In 1899, the studio of Walker & Boutal published 286 of the Lafayette photographs.\n\nFollowing the ball, The Duchess received a letter from Francis Knollys, Private Secretary to the Sovereign, indicating that the Prince, later King Edward VII, who arrived after 11 o'clock, thought the party a success.\n\nAt the ball, the attendees included:\n\n\nThe Duchess of Devonshire's costume was described in detail by \"The Times\":\n\n\"The Duchess of Devonshire, as Zenobia, Queen of Palmyra, wore a magnificent costume. The skirt of gold tissue was embroidered all over in a star-like design in emeralds, sapphires, diamonds, and other jewels outlined with gold, the corners where it opened in front being elaborately wrought in the same jewels and gold to represent peacocks outspread tails. This opened to show an underdress of cream crepe de chine, delicately embroidered in silver, gold, and pearls and sprinkled all over with diamonds. The train, which was attached to the shoulders by two slender points and was fastened at the waist with a large diamond ornament, was a green velvet of a lovely shade, and was superbly embroidered in Oriental designs introducing the lotus flower in rubies, sapphires, amethysts, emeralds, and diamonds, with four borderings on contrasting grounds, separated with gold cord. The train was lined with turquoise satin. The bodice was composed of gold tissue to match the skirt, and the front was of crepe de chine hidden with a stomacher of real diamonds, rubies and emeralds and jewelled belt. A gold crown incrusted (sic) with emeralds, diamonds, and rubies, with a diamond drop at each curved end and two upstanding white ostrich feathers in the middle, and round the front festoons of pearls with a large pear shaped pearl in the centre falling on the forehead.\"\n\nOne of the most expensive costumes was worn by Charles Spencer-Churchill, 9th Duke of Marlborough who went as the French Ambassador to the Court of Catherine the Great. The velvet costume was made by the House of Worth and was embroidered in silver, pearls and diamonds with a waistcoat made out of gold and white damask. The price of the costume, which cost 5,000 francs, reportedly even shocked the Duke, who had famously married American heiress Consuelo Vanderbilt in 1895.\n\nThe ball was reproduced on the London stage in Drury Lane in September 1897 \"to the scandal of nobility and the amusement of the commoners.\" The ball was utilized as the setting for the last act of a new play entitled \"The White Heather\" by Cecil Raleigh and Henry Hamilton. \"The New York Times\" stated \"the very possessions of royalter were 'desecrated' by exhibition on the stage, for the managers, with enterprise almost America, had purchased from the costumers some of the most gorgeous habiliments worn at that revel.\" The play inspired the 1919 film, \"The White Heather\".\n\n\n"}
{"id": "39314328", "url": "https://en.wikipedia.org/wiki?curid=39314328", "title": "Eco-industrial development", "text": "Eco-industrial development\n\nEco-industrial development (EID) is a framework for industry to develop while reducing its impact on the environment. It uses a closed loop production cycle to tackle a broad set of environmental challenges such as soil and water pollution, desertification, species preservation, energy management, by-product synergy, resource efficiency, air quality, etc.\n\nMutually beneficial connections among industry, natural systems, energy, material and local communities become central factors in designing industrial production processes.\n\nThe approach itself is largely voluntary and market-driven but often pressed ahead by favorable government treatment or efforts of development co-operation.\n\nSince the early 1990s the idea of EID evolved from biological symbiosis. This concept was adapted by industrial ecologists in the search for innovative approaches to solve problems of waste, energy shortage and degradation of the environment. A continuous approach towards improving both environmental and economic outcomes is used.\n\nIn 1992, the international community officially connected development co-operation to sustainable environmental protection for the first time. At the United Nations Conference on Environment and Development (UNCED) in Rio de Janeiro, Brazil nearly 180 states signed the conference’s Rio Declaration. Although non-binding, the Rio Declaration on Environment and Development laid out 27 principles that shall guide the increasing inter-connectedness of development cooperation and sustainability. Moreover, the documents drafting was accompanied by a presentation describing the idea of eco-industrial development for the first time.\n\nIn the following years, EID became popular throughout the United States. The recently elected Clinton administration installed a summit of business, labor, government and environmental protection representatives to further develop the approach. This summit established the idea of eco-industrial parks but soon realized that at first a more efficient management of raw materials, energy and waste has to be achieved.\n\nSince then, the broad goals and application principles of EID have hardly changed and only became adapted to the rapid technological progress.\n\nIn 2012 the IGEP Foundation, for the promotion of trade, published a report called \"Pathway to Eco Industrial Development in India – Concepts and Cases\".\n\nThe field is researched by the Nation Centre for Eco-Industrial Development, a joint project by the University of Southern California and Cornell University.\n\nThe primary goal of eco-industrial development is a significant and continuous improvement in both business and environmental performance. Herein, the notion of industry not only relates to private-sector manufacturing but also covers state-owned enterprise, the service sector as well as transportation. EID’s twin guideline is reflected specifically in the “eco” of eco-industrial as it resembles ecology (decrease in pollution and waste) and economy (increase in commercial success) at the same time. In order to build a framework of defining an enterprise’s sustainable performance at the micro level, resource use optimization, minimization of waste, cleaner technologies and pollution limits are used in achieving a broad range of goals in EID: \nEco-industrial development hence explores the possibility of improvement at the local level. In unique case-to-case analyses, particular geography, human potential or business climate are investigated. In contrast to the widespread race for top-down governmental support such as tax cuts, EID emphasizes locally achievable success and rooms for improvement. As a result, purposeful enforcements of action plans can make a large difference by optimizing the interaction of business, community and ecological systems.\n\nEco-industrial development includes and employs four major conceptual instruments. Each of the approaches intends to combine the seemingly antithetic processes of industrial development and bolstering sustainability.\n\n\n\n"}
{"id": "345419", "url": "https://en.wikipedia.org/wiki?curid=345419", "title": "Economic freedom", "text": "Economic freedom\n\nEconomic freedom or economic liberty is the ability of people of a society to take economic actions. This is a term used in economic and policy debates as well as in the philosophy of economics. One approach to economic freedom comes from classical liberal and libertarian traditions emphasizing free markets, free trade, and private property under free enterprise. Another approach to economic freedom extends the welfare economics study of individual choice, with greater economic freedom coming from a \"larger\" (in some technical sense) set of possible choices. Other conceptions of economic freedom include freedom from want and the freedom to engage in collective bargaining.\n\nThe free market viewpoint defines economic liberty as the freedom to produce, trade and consume any goods and services acquired without the use of force, fraud or theft. This is embodied in the rule of law, property rights and freedom of contract, and characterized by external and internal openness of the markets, the protection of property rights and freedom of economic initiative. There are several indices of economic freedom that attempt to measure free market economic freedom. Based on these rankings correlative studies have found higher economic growth to be correlated with higher scores on the country rankings. With regards to other measures, such as equality, corruption, political and social violence and their correlation to economic freedom it has been argued that the economic freedom indices conflate unrelated policies and policy outcomes to conceal negative correlations between economic growth and economic freedom in some subcomponents.\n\nAccording to the free market view, a secure system of private property rights is an essential part of economic freedom. Such systems include two main rights: the right to control and benefit from property and the right to transfer property by voluntary means. These rights offer people the possibility of autonomy and self-determination according to their personal values and goals. Economist Milton Friedman sees property rights as \"the most basic of human rights and an essential foundation for other human rights.\" With property rights protected, people are free to choose the use of their property, earn on it, and transfer it to anyone else, as long as they do it on a voluntary basis and do not resort to force, fraud or theft. In such conditions most people can achieve much greater personal freedom and development than under a regime of government coercion. A secure system of property rights also reduces uncertainty and encourages investments, creating favorable conditions for an economy to be successful. Empirical evidence suggests that countries with strong property rights systems have economic growth rates almost twice as high as those of countries with weak property rights systems, and that a market system with significant private property rights is an essential condition for democracy. According to Hernando de Soto, much of the poverty in the Third World countries is caused by the lack of Western systems of laws and well-defined and universally recognized property rights. De Soto argues that because of the legal barriers poor people in those countries can not utilize their assets to produce more wealth. One thinker to question private property was Pierre-Joseph Proudhon, a socialist and anarchist, who argued that property is both theft and freedom.\n\nFreedom of contract is the right to choose one's contracting parties and to trade with them on any terms and conditions one sees fit. Contracts permit individuals to create their own enforceable legal rules, adapted to their unique situations. However, not all contracts need to be enforced by the state. For example, in the United States there is a large number of third-party arbitration tribunals which resolve disputes under private commercial law. Negatively understood, freedom of contract is freedom from government interference and from imposed value judgments of fairness. The notion of \"freedom of contract\" was given one of its most famous legal expressions in 1875 by Sir George Jessel MR: \n\nThe doctrine of freedom of contract received one of its strongest expressions in the US Supreme Court case of Lochner v New York which struck down legal restrictions on the working hours of bakers. \n\nCritics of the classical view of freedom of contract argue that this freedom is illusory when the bargaining power of the parties is highly unequal, most notably in the case of contracts between employers and workers. As in the case of restrictions on working hours, workers as a group may benefit from legal protections that prevent individuals agreeing to contracts that require long working hours. In its West Coast Hotel Co. v. Parrish decision in 1937, overturning Lochner, the Supreme Court cited an earlier decisions \nFrom this point on, the Lochner view of freedom of contract has been rejected by US courts.\n\nSome free market advocates argue that political and civil liberties have simultaneously expanded with market-based economies, and present empirical evidence to support the claim that economic and political freedoms are linked.\n\nIn \"Capitalism and Freedom\" (1962), Friedman further developed Friedrich Hayek's argument that economic freedom, while itself an extremely important component of total freedom, is also a necessary condition for political freedom. He commented that centralized control of economic activities was always accompanied with political repression. In his view, voluntary character of all transactions in a free market economy and wide diversity that it permits are fundamental threats to repressive political leaders and greatly diminish power to coerce. Through elimination of centralized control of economic activities, economic power is separated from political power, and the one can serve as counterbalance to the other. Friedman feels that competitive capitalism is especially important to minority groups, since impersonal market forces protect people from discrimination in their economic activities for reasons unrelated to their productivity.\n\nAustrian School economist Ludwig von Mises argued that economic and political freedom were mutually dependent: \"The idea that political freedom can be preserved in the absence of economic freedom, and vice versa, is an illusion. Political freedom is the corollary of economic freedom. It is no accident that the age of capitalism became also the age of government by the people.\"\n\nIn \"The Road to Serfdom\", Hayek argued that \"Economic control is not merely control of a sector of human life which can be separated from the rest; it is the control of the means for all our ends.\" Hayek criticized socialist policies as the slippery slope that can lead to totalitarianism.\n\nGordon Tullock has argued that \"the Hayek-Friedman argument\" predicted totalitarian governments in much of Western Europe in the late 20th century – which did not occur. He uses the example of Sweden, in which the government at that time controlled 63 percent of GNP, as an example to support his argument that the basic problem with \"The Road to Serfdom\" is \"that it offered predictions which turned out to be false. The steady advance of government in places such as Sweden has not led to any loss of non-economic freedoms.\" While criticizing Hayek, Tullock still praises the classical liberal notion of economic freedom, saying, \"Arguments for political freedom are strong, as are the arguments for economic freedom. We needn’t make one set of arguments depend on the other.\"\n\nThe annual surveys \"Economic Freedom of the World\" (EFW) and \"Index of Economic Freedom\" (IEF) are two indices which attempt to measure the degree of economic freedom in the world's nations. The EFW index, originally developed by Gwartney, Lawson and Block at the Fraser Institute was likely the most used in empirical studies as of 2000. The other major index, which was developed by The Heritage Foundation and The Wall Street Journal appears superior for data work, although as it only goes back to 1995, it is less useful for historical comparisons.\n\nAccording to the creators of the indices, these rankings correlate strongly with higher average income per person, higher income of the poorest 10%, higher life expectancy, higher literacy, lower infant mortality, higher access to water sources and less corruption. The people living in the top one-fifth of countries enjoy an average income of $23,450 and a growth rate in the 1990s of 2.56 percent per year; in contrast, the bottom one-fifth in the rankings had an average income of just $2,556 and a -0.85 percent growth rate in the 1990s. The poorest 10 percent of the population have an average income of just $728 in the lowest ranked countries compared with over $7,000 in the highest ranked countries. The life expectancy of people living in the highest ranked nations is 20 years longer than for people in the lowest ranked countries.\n\nHigher economic freedom, as measured by both the Heritage and the Fraser indices, correlates strongly with higher self-reported happiness.\n\nErik Gartzke of the Fraser Institute estimates that countries with a high EFW are significantly less likely to be involved in wars, while his measure of democracy had little or no impact.\n\nThe \"Economic Freedom of the World\" score for the entire world has grown considerably in recent decades. The average score has increased from 5.17 in 1985 to 6.4 in 2005. Of the nations in 1985, 95 nations increased their score, seven saw a decline, and six were unchanged. Using the 2008 Index of Economic Freedom methodology world economic freedom has increased 2.6 points since 1995.\n\nMembers of the World Bank Group also use \"Index of Economic Freedom\" as the indicator of investment climate, because it covers more aspects relevant to the private sector in wide number of countries.\n\nThe nature of economic freedom is often in dispute. Robert Lawson, the co-author of \"EFW\", even acknowledges the potential shortcomings of freedom indices: \"The purpose of the EFW index is to measure, no doubt imprecisely, the degree of economic freedom that exists.\" He likens the recent attempts of economists to measure economic freedom to the initial attempts of economists to measure GDP: \"They [macroeconomists] were\nscientists who sat down to design, as best they could with the tools at hand, a measure of the current economic activity of the nation. Economic activity exists and their job was to measure it. Likewise economic freedom exists. It is a thing. We can define and measure it.\"\nThus, it follows that some economists, socialists and anarchists contend that the existing indicators of economic freedom are too narrowly defined and should take into account a broader conception of economic freedoms.\n\nCritics of the indices (e.g. Thom Hartmann) also oppose the inclusion of business-related measures like corporate charters and intellectual property protection. John Miller in Dollars & Sense has stated that the indices are \"a poor barometer of either freedom more broadly construed or of prosperity.\" He argues that the high correlation between living standards and economic freedom as measured by IEF is the result of choices made in the construction of the index that guarantee this result. For example, the treatment of a large informal sector (common in poor countries) as an indicator of restrictive government policy, and the use of the change in the ratio of government spending to national income, rather than the level of this ratio. Hartmann argues that these choices cause the social democratic European countries to rank higher than countries where the government share of the economy is small but growing.\n\nEconomists Dani Rodrik and Jeffrey Sachs have separately noted that there appears to be little correlation between measured economic freedom and economic growth when the least free countries are disregarded, as indicated by the strong growth of the Chinese economy in recent years. Morris Altman found that there is a relatively large correlation between economic freedom and both per capita income and per capita growth. He argues that this is especially true when it comes to sub-indices relating to property rights and sound money, while he calls into question the importance of sub-indices relating to labor regulation and government size once certain threshold values are passed. John Miller further observes that Hong Kong and Singapore, both only \"partially free\" according to Freedom House, are leading countries on both economic freedom indices and casts doubt on the claim that measured economic freedom is associated with political freedom. However, according to the Freedom House, \"there is a high and statistically significant correlation between the level of political freedom as measured by Freedom House and economic freedom as measured by the Wall Street Journal/Heritage Foundation survey.\"\n\nAmartya Sen and other economists consider economic freedom to be measured in terms of the set of economic choices available to individuals. Economic freedom is greater when individuals have more economic choices available – when, in some technical sense, the choice set of individuals expands.\n\nThe differences between alternative views of economic freedom have been expressed in terms of Isaiah Berlin's distinction between positive freedom and negative freedom. Classical liberals favour a focus on negative freedom as did Berlin himself. By contrast Amartya Sen argues for an understanding of freedom in terms of capabilities to pursue a range of goals. One measure which attempts to assess freedom in the positive sense is Goodin, Rice, Parpo, and Eriksson's measure of discretionary time, which is an estimate of how much time people have at their disposal during which they are free to choose the activities in which they participate, after taking into account the time they need to spend acquiring the necessities of life. In his book, \"Capitalism and Freedom\", Milton Friedman explains the preservation of freedom is the reason for limited and decentralized governments. It creates positive freedom within the society allowing for freedom of choice for an individual in a free society.\n\nFranklin D. Roosevelt included freedom from want in his Four freedoms speech. Roosevelt stated that freedom from want \"translated into world terms, means economic understandings which will secure to every nation a healthy peacetime life for its inhabitants-everywhere in the world\". In terms of US policy, Roosevelt's New Deal included economic freedoms such as freedom of trade union organisation, as well as a wide range of policies of government intervention and redistributive taxation aimed at promoting freedom from want. Internationally, Roosevelt favored the policies associated with the Bretton Woods Agreement which fixed exchange rates and established international economic institutions such as the World Bank and International Monetary Fund.\n\nHerbert Hoover saw economic freedom as a fifth freedom, which secures survival of Roosevelt's Four freedoms. He described economic freedom as freedom \"for men to choose their own calling, to accumulate property in protection of their children and old age, [and] freedom of enterprise that does not injure others.\"\n\nThe Philadelphia Declaration (enshrined in the constitution of the International Labour Organization) states that \"all human beings, irrespective of race, creed or sex, have the right to pursue both their material well-being and their spiritual development in conditions of freedom and dignity, of economic security and equal opportunity.\" The ILO further states that \"The right of workers and employers to form and join organizations of their own choosing is an integral part of a free and open society.\"\n\nThe socialist view of economic freedom conceives of freedom as a concrete situation as opposed to an abstract or moral concept. This view of freedom is closely related to the socialist view of human creativity and the importance ascribed to creative freedom. Socialists view creativity as an essential aspect of human nature, thus defining freedom as a situation or state of being where individuals are able to express their creativity unhindered by constraints of both material scarcity and coercive social institutions. Marxists stress the importance of freeing the individual from what they view as coercive, exploitative and alienating social relationships of production they are compelled to partake in, as well as the importance of economic development as providing the material basis for the existence of a state of society where there are enough resources to allow for each individual to pursue his or her genuine creative interests.\n\nOne of the ways to measure economic competitiveness is by comparing an extent of economic freedom that countries have, which as surveys show can also largely explain differences in economic well-being across the world. Generally, countries with higher economic freedom have higher gross domestic product per capita and its growth rates, as well as better health care, education quality, environment protection, income equality, and happiness results. These trends of increasing prosperity are confirmed even when we compare these indicators within territories of countries. \nNevertheless, despite these benefits societies have to be aware that with increasing economic freedom they will have to face going through a phase of increasing inequality, which basically is a result of decreased redistribution, as well as other negative effects from economic liberalization, i.e., running of local enterprises out of business, takeover of competitive firms, enforcing of interests of foreign companies, dependence on foreign capital, deteriorating work rights, harmful manufacturing for the environment, introducing of commercial practices that are not favorable for consumers, as well as endangerment for survival of national cultures.\nHowever, on the bright side, these negative effects from economic freedom tend to be felt in a shorter term, and if countries use the opportunities of economic freedom in our increasingly globalized economy in a right way, as research shows their socioeconomic conditions will be significantly better than in a case of less economic freedom.\n\n\n"}
{"id": "27345497", "url": "https://en.wikipedia.org/wiki?curid=27345497", "title": "Ernesto Pujol", "text": "Ernesto Pujol\n\nErnesto Pujol is a site-specific performance artist, social choreographer, and educator with an interdisciplinary practice. Pujol was born in 1957 in Havana, Cuba and spent time in San Juan, Puerto Rico, and in Madrid and Barcelona, Spain, before moving to the United States in 1979. He has lived and worked in New York since 1984. Pujol engaged in interdisciplinary pursuits, such as psychology and literature, while doing undergraduate work in humanities and visual arts at the University of Puerto Rico, in Spanish art history at the Universidad Complutense in Spain and in philosophy at St. John Vianney College Seminary in Florida. He pursued graduate work in education at the Universidad Interamericana in San Juan, Puerto Rico, in art therapy at Pratt Institute in Brooklyn, and in communications and media theory at Hunter College in New York City. Pujol received his MFA in interdisciplinary art practice from The School of The Art Institute of Chicago.\n\nPujol first became known during the 1990s for a series of site-specific installation projects that dealt with whiteness, masculinity, collective and individual memory, loss and mourning. His more recent performance work deals with subjects such as war, the environment, and consciousness, influenced by Zen Buddhism. Since 2000, Pujol has been exploring interiority in public and private spaces He strives to reclaim public space from distractions through durational group performances in order to create space for silent reflection as critical to democracy. The artist seeks to awaken consciousness among viewers through the creation of such spaces. The writings of Carol Becker have served as a source of inspiration for Pujol with notions of citizenship, such as the artist as citizen and the citizenship of art within American democracy. He has also worked extensively with curator Mary Jane Jacob in Charleston and with curator Saralyn Reece Hardy at the Spencer Museum of Art in Lawrence and the Salina Arts Center in Kansas. \nIn 2008, Pujol did a series of performances titled \"Inheriting Salt\" and began making sculptures out the footprints he produced during the performances. \"Desert Walk\", in the collection of the Honolulu Museum of Art, is such a sculpture. Two of his largest scale projects to date took place in Salt Lake City, Utah (Awaiting, 2009) and in Honolulu, Hawaii (Speaking in Silence, 2010). The artist has worked extensively with a variety of media, including installation, photography, and performance.\n\nErnesto Pujol represented the United States in 1997 at the Second Johannesburg Biennial in South Africa, the Sixth Havana Biennial in Cuba and the Second Saaremaa Biennial in Estonia. He has also received a number of prestigious fellowships from institutions such as the Pollock-Krasner Foundation, the Joan Mitchell Foundation, the Mid-Atlantic Arts Foundation, Art Matters and the New York Foundation for the Arts. Pujol has also been an active participant with a number of arts institutions serving with the Academy for Educational Development, the National Endowment for the Arts and the New York State Council on the Arts.\n\n"}
{"id": "20776867", "url": "https://en.wikipedia.org/wiki?curid=20776867", "title": "Essence-Function", "text": "Essence-Function\n\nEssence-Function (體用, Chinese pinyin: \"tǐ yòng\", Korean: \"che-yong\"), also called Substance and Function, is a key concept in Chinese philosophy and other Far-Eastern philosophies. \"Essence\" is Absolute Reality, the fundamental \"cause\" or origin, while \"Function\" is relative or concrete reality, the concrete manifestation of \"Essence\". Ti and yong do not represent two separate things, such as Absolute Reality and Concrete Reality. They are always two, flexibly-viewed aspects of a single thing.\n\n\nTogether they form the phrase 體用 ti-yong, 체용 che-yong, \"Essence-Function\".\n\n\"Essence\" is Absolute Reality, the fundamental \"cause\" or origin, while \"Function\" is manifest or relative reality, the discernible effects or manifestations of \"Essence\". \"Essence-Function\" describes the interplay between the two: although Absolute Reality is the ultimate reality, the relative reality nevertheless also exists, as is evident from concrete reality. The relationship between these two realms is espressed in such schemata as the Five Ranks and the Oxherding Pictures. Various terms are used for \"absolute\" and \"relative\".\n\nThe tree forms a metaphor for \"Essence-Function\", with the roots being \"Essence\" and the branches being \"Function\". According to Muller \"the most important application of t'i-yung thought [...] is to the human being, where the human mind is seen as \"essence,\" and one's words, thoughts and actions are seen as \"function.\"\n\nAccording to Sung-bae Park the concept of \"essence-function\" is used by East Asian Buddhists \"to show a non-dualistic and non-discriminate nature in their enlightenment experience,\" but does not exclude notions of subjectivity and objectivity. According to Sung-bae Park, the terms \"essence\" and \"function\" can also be rendered as \"body\" and \"the body's functions,\" which is a more personal and less abstract expression of nonduality.\n\nThe t'i-yung developed in the Wei (220–265) – Chin (265–420) period of Chinese history, when \"Unification of the Three Teachings\" ideology was domimant, striving for a theoretical reconciliation of Confucianism, Daoism, and Buddhism. The t'i-yung concept was first known as \"pen-mo\" (\"primary-last\" or \"primary-subordinate\"), and developed into t'i-yung. In the initial development of the theory, \"thinkers considered one of the three philosophies as 'the primary' or 't'i' and the others as 'the last' or 'yung,' insisting that their own philosophy was superior to the others.\" However, although the theory was used to arrange the three teachings hierarchically, it also confirmed their inner unity.\n\nThe concept developed with the introduction of Buddhism in China, adapting Buddhist philosophy to a Chinese frame of reference. One of the core Madhyamaka Buddhist doctrines is the Two Truths doctrine, which says that there is a relative truth and an ultimate truth. In Madhyamaka the two truths are two \"epistemological truths\": two different ways to look at reality. Phenomenal reality is relative real or true: one can make factual statements about concrete or manifest reality, but those statements have a relative trueness, since everything that exists changes, and is bound to dissolve again. Ultimately everything is empty, sunyata, of an underlying unchanging essence. Sunyata itself is also \"empty,\" 'the emptiness of emptiness', which means that \"sunyata\" itself does not constitute a higher or ultimate \"essence\" or \"reality. The Prajnaparamita-sutras and Madhyamaka emphasized the non-duality of form and emptiness: form is emptiness, emptiness is form, as the heart sutra says. \n\nWhen Buddhism was introduced to China, the two truths doctrine was a point of confusion. Chinese thinking took this to refer to two \"ontological truths\": reality exists of two levels, a relative level and an absolute level. The doctrines of Buddha-nature and Sunyata were understood as akin to Dao and the Taoist non-being. It took the Chinese world several centuries to realize that sunyata has another meaning.\n\nBased on their understanding of the Mahayana Mahaparinirvana Sutra the Chinese supposed that the teaching of the Buddha-nature was, as stated by that sutra, the final Buddhist teaching, and that there is an essential truth above sunyata and the two truths. The idea that ultimate reality is present within the daily world of relative reality melded well with Chinese culture, which emphasized the mundane world and society. But this does not tell how the absolute is present in the relative world:\nThe notions appear already in the \"Zhongyong\" (\"Doctrine of the Mean\") attributed to Zi Si (481–402 BCE), the grandson of Confucius. The first philosopher to systematically use the ti-yong schema was Wang Bi (226–249) in his commentary to Daodejing, chapter 22, when he discussed the metaphysical relation between non-being (\"wu\") and being (\"you\"). Subsequently, the notion has been borrowed from the Neo-Daoist philosophy to other schools of Chinese philosophy, including Hua-yen and other schools of Buddhism, and Neo-Confucianism of Cheng Yi and Zhu Xi, and served as a basic tool of interpretation. With these schools it has travelled to Korea, Japan and Vietnam, and has been developed there.\n\nThe \"Awakening of Mahayana Faith\", a key text in Chinese Buddhism, also employs \"Essence-Function\". Although attributed to Aśvaghoṣa (?80-?150 CE), and traditionally thought to have been translated Paramartha (499–569), in 553, many modern scholars now opine that it was actually composed by Paramartha or one of his students.\n\nThe concept was employed by Confucian reformers of the Self-Strengthening Movement at the end of the Qing dynasty's (1644 to 1912) rule in China, in the phrase \"Chinese learning for essence, Western learning for application\". The belief was that China should maintain its own Confucian style of learning to keep the \"essence\" of society, while at the same time using Western learning for \"practical application\" in developing its infrastructure and economy.\n\n\"Essence-Function\" is an essential element in the philosophy of Wonhyo (617–686 CE). Wonhyo developed t'i-yung theory into its most influential form in his commentary on the \"Ta ch'eng ch'i hsin lun\" (\"Treatise on the Awakening of Mahayana Faith\"). This scripture proclaims the non-duality of the phenomenal or mundane world and the \"tathagata-garbha\". Wonhyo saw the \"Treatise\"'s treatment of \"t'i-yung\" as a way of harmonizing the thought of Madhyamika and Yogacara. For Wonhyo, \"t'i\" corresponds to Madhyamika's ultimate truth and \"yung\" to its conventional truth, and these, in turn, are the two gates of Yogacara's one-mind.\n\nChinul (1158–1210) and Kihwa (1376–1433) also employ and develop this idea of Essence-Function in their writings in particular ways. Wonch'uk (613–696) employed the conceptual and analytical tool, Essence-Function, as an exegetical, hermeneutical and syncretic device.\n\n\n"}
{"id": "922567", "url": "https://en.wikipedia.org/wiki?curid=922567", "title": "Fermi's golden rule", "text": "Fermi's golden rule\n\nIn quantum physics, Fermi's golden rule is a formula that describes the transition rate (probability of transition per unit time) from one energy eigenstate of a quantum system into other energy eigenstates in a continuum, effected by a weak perturbation. This rate is effectively constant.\n\nAlthough named after Enrico Fermi, most of the work leading to the Golden Rule is due to Paul Dirac who formulated 20 years earlier a virtually identical equation, including the three components of a constant, the matrix element of the perturbation and an energy difference. It was given this name because, on account of its importance, Fermi dubbed it \"Golden Rule No. 2.\" \n\nConsider the system to begin in an eigenstate, formula_1, of a given Hamiltonian, . Consider the effect of a (possibly time-dependent) perturbing Hamiltonian, . If is time-independent, the system goes only into those states in the continuum that have the same energy as the initial state. If is oscillating as a function of time with an angular frequency , the transition is into states with energies that differ by from the energy of the initial state.\n\nIn both cases, the one-to-many \"transition probability per unit of time\" from the state formula_2 to a set of final states formula_3 is essentially constant. It is given, to first order in the perturbation, by\nwhere is the density of final states (number of continuum states per unit of energy) and formula_5 is the matrix element (in bra–ket notation) of the perturbation between the final and initial states.\n\nThis transition probability is also called \"decay probability\" and is related to the inverse of the mean lifetime.\nFermi's golden rule is valid when the initial state has not been significantly depleted by scattering into the final states.\n\nThe standard way to derive the equation is to start with time-dependent perturbation theory and to take the limit for absorption under the assumption that the time of the measurement is much larger than the time needed for the transition.\n\nOnly the magnitude of the matrix element formula_6 enters the Fermi's Golden Rule. The phase of this matrix element, however, contains separate information about the transition process.\nIt appears in expressions that complement the Golden Rule in the semiclassical Boltzmann equation approach to electron transport.\n\nFermi's golden rule is displayed on a computer screen in the movie Prince of Darkness (film) where it is explained that it was translated from Latin found in an ancient text.\n\n\n"}
{"id": "222313", "url": "https://en.wikipedia.org/wiki?curid=222313", "title": "Formation evaluation", "text": "Formation evaluation\n\nIn petroleum exploration and development, formation evaluation is used to determine the ability of a borehole to produce petroleum. Essentially, it is the process of \"recognizing a commercial well when you drill one\".\n\nModern rotary drilling usually uses a heavy mud as a lubricant and as a means of producing a confining pressure against the formation face in the borehole, preventing blowouts. Only in rare and catastrophic cases, do oil and gas wells \"come in\" with a fountain of gushing oil. In real life, that is a \"blowout\"—and usually also a financial and environmental disaster. But controlling blowouts has drawbacks—mud filtrate soaks into the formation around the borehole and a mud cake plasters the sides of the hole. These factors obscure the possible presence of oil or gas in even very porous formations. Further complicating the problem is the widespread occurrence of small amounts of petroleum in the rocks of many sedimentary provinces. In fact, if a sedimentary province is absolutely barren of traces of petroleum, it is not feasible to continue drilling there.\n\nThe formation evaluation problem is a matter of answering two questions:\n\nIt is complicated by the impossibility of directly examining the formation. It is, in short, the problem of looking at the formation \"indirectly\".\n\nTools to detect oil and gas have been evolving for over a century. The simplest and most direct tool is well cuttings examination. Some older oilmen ground the cuttings between their teeth and tasted to see if crude oil was present. Today, a wellsite geologist or mudlogger uses a low powered stereoscopic microscope to determine the lithology of the formation being drilled and to estimate porosity and possible oil staining. A portable ultraviolet light chamber or \"Spook Box\" is used to examine the cuttings for fluorescence. Fluorescence can be an indication of crude oil staining, or of the presence of fluorescent minerals. They can be differentiated by placing the cuttings in a solvent filled watchglass or dimple dish. The solvent is usually carbon tetrachlorethane. Crude oil dissolves and then redeposits as a fluorescent ring when the solvent evaporates. The written strip chart recording of these examinations is called a sample log or mudlog.\n\nWell cuttings examination is a learned skill. During drilling, chips of rock, usually less than about 1/8 inch (6 mm) across, are cut from the bottom of the hole by the bit. Mud, jetting out of holes in the bit under high pressure, washes the cuttings away and up the hole. During their trip to the surface they may circulate around the turning drillpipe, mix with cuttings falling back down the hole, mix with fragments caving from the hole walls and mix with cuttings travelling faster and slower in the same upward direction. They then are screened out of the mudstream by the shale shaker and fall on a pile at its base. Determining the type of rock being drilled at any one time is a matter of knowing the 'lag time' between a chip being cut by the bit and the time it reaches the surface where it is then examined by the wellsite geologist (or mudlogger as they are sometimes called). A sample of the cuttings taken at the proper time will contain the current cuttings in a mixture of previously drilled material. Recognizing them can be very difficult at times, for example after a \"bit trip\" when a couple of miles of drill pipe has been extracted and returned to the hole in order to replace a dull bit. At such a time there is a flood of foreign material knocked from the borehole walls (cavings), making the mudloggers task all the more difficult.\n\nOne way to get more detailed samples of a formation is by coring. Two techniques commonly used at present. The first is the \"whole core\", a cylinder of rock, usually about 3\" to 4\" in diameter and up to to long. It is cut with a \"core barrel\", a hollow pipe tipped with a ring-shaped diamond chip-studded bit that can cut a plug and bring it to the surface. Often the plug breaks while drilling, usually in shales or fractures and the core barrel jams, slowly grinding the rocks in front of it to powder. This signals the driller to give up on getting a full length core and to pull up the pipe.\n\nTaking a full core is an expensive operation that usually stops or slows drilling for at least the better part of a day. A full core can be invaluable for later reservoir evaluation. Once a section of well has been drilled, there is, of course, no way to core it without drilling another well.\n\nAnother, cheaper, technique for obtaining samples of the formation is \"Sidewall Coring\". One type of sidewall cores is percussion cores. In this method, a steel cylinder—a coring gun—has hollow-point steel bullets mounted along its sides and moored to the gun by short steel cables. The coring gun is lowered to the bottom of the interval of interest and the bullets are fired individually as the gun is pulled up the hole. The mooring cables ideally pull the hollow bullets and the enclosed plug of formation loose and the gun carries them to the surface. Advantages of this technique are low cost and the ability to sample the formation after it has been drilled. Disadvantages are possible non-recovery because of lost or misfired bullets and a slight uncertainty about the sample depth. Sidewall cores are often shot \"on the run\" without stopping at each core point because of the danger of differential sticking. Most service company personnel are skilled enough to minimize this problem, but it can be significant if depth accuracy is important.\n\nA second method of sidewall coring is rotary sidewall cores. In this method, a circular-saw assembly is lowered to the zone of interest on a wireline, and the core is sawed out. Dozens of cores may be taken this way in one run. This method is roughly 20 times as expensive as percussion cores, but yields a much better sample.\n\nA serious problem with cores is the change they undergo as they are brought to the surface. It might seem that cuttings and cores are very direct samples but the problem is whether the formation at depth will produce oil or gas. Sidewall cores are deformed and compacted and fractured by the bullet impact. Most full cores from any significant depth expand and fracture as they are brought to the surface and removed from the core barrel. Both types of core can be invaded or even flushed by mud, making the evaluation of formation fluids difficult. The formation analyst has to remember that all tools give indirect data.\n\nMud logging (or Wellsite Geology) is a well logging process in which drilling mud and drill bit cuttings from the formation are evaluated during drilling and their properties recorded on a strip chart as a visual analytical tool and stratigraphic cross sectional representation of the well. The drilling mud which is analyzed for hydrocarbon gases, by use of a gas chromatograph, contains drill bit cuttings which are visually evaluated by a mudlogger and then described in the mud log. The total gas, chromatograph record, lithological sample, pore pressure, shale density,D-exponent, etc. (all lagged parameters because they are circulated up to the surface from the bit) are plotted along with surface parameters such as rate of penetration (ROP), Weight On Bit (WOB),rotation per minute etc. on the mudlog which serve as a tool for the mudlogger, drilling engineers, mud engineers, and other service personnel charged with drilling and producing the well.\n\nThe oil and gas industry uses wireline logging to obtain a continuous record of a formation's rock properties. Wireline logging can be defined as being \"The acquisition and analysis of geophysical data performed as a function of well bore depth, together with the provision of related services.\" Note that \"wireline logging\" and \"mud logging\" are not the same, yet are closely linked through the integration of the data sets. The measurements are made referenced to \"TAH\" - True Along Hole depth: these and the associated analysis can then be used to infer further properties, such as hydrocarbon saturation and formation pressure, and to make further drilling and production decisions.\n\nWireline logging is performed by lowering a 'logging tool' - or a string of one or more instruments - on the end of a wireline into an oil well (or borehole) and recording petrophysical properties using a variety of sensors. Logging tools developed over the years measure the natural gamma ray, electrical, acoustic, stimulated radioactive responses, electromagnetic, nuclear magnetic resonance, pressure and other properties of the rocks and their contained fluids. For this article, they are broadly broken down by the main property that they respond to.\n\nThe data itself is recorded either at surface (real-time mode), or in the hole (memory mode) to an electronic data format and then either a printed record or electronic presentation called a \"well log\" is provided to the client, along with an electronic copy of the raw data. Well logging operations can either be performed during the drilling process (see Logging While Drilling), to provide real-time information about the formations being penetrated by the borehole, or once the well has reached Total Depth and the whole depth of the borehole can be logged.\n\nReal-time data is recorded directly against measured cable depth. Memory data is recorded against time, and then depth data is simultaneously measured against time. The two data sets are then merged using the common time base to create an instrument response versus depth log. Memory recorded depth can also be corrected in exactly the same way as real-time corrections are made, so there should be no difference in the attainable TAH accuracy.\n\nThe measured cable depth can be derived from a number of different measurements, but is usually either recorded based on a calibrated wheel counter, or (more accurately) using magnetic marks which provide calibrated increments of cable length. The measurements made must then be corrected for elastic stretch and temperature.[1]\n\nThere are many types of wireline logs and they can be categorized either by their function or by the technology that they use. \"Open hole logs\" are run before the oil or gas well is lined with pipe or cased. \"Cased hole logs\" are run after the well is lined with casing or production pipe.[2]\n\nWireline logs can be divided into broad categories based on the physical properties measured.\n\nIn 1928, the Schlumberger brothers in France developed the workhorse of all formation evaluation tools: the electric log. Electric logs have been improved to a high degree of precision and sophistication since that time, but the basic principle has not changed. Most underground formations contain water, often salt water, in their pores. The resistance to electric current of the total formation—rock and fluids—around the borehole is proportional to the sum of the volumetric proportions of mineral grains and conductive water-filled pore space. If the pores are partially filled with gas or oil, which are resistant to the passage of electric current, the bulk formation resistance is higher than for water filled pores. For the sake of a convenient comparison from measurement to measurement, the electrical logging tools measure the resistance of a cubic meter of formation. This measurement is called \"resistivity\".\n\nModern resistivity logging tools fall into two categories, Laterolog and Induction, with various commercial names, depending on the company providing the logging services.\n\nLaterolog tools send an electric current from an electrode on the sonde directly into the formation. The return electrodes are located either on surface or on the sonde itself. Complex arrays of electrodes on the sonde (guard electrodes) focus the current into the formation and prevent current lines from fanning out or flowing directly to the return electrode through the borehole fluid. Most tools vary the voltage at the main electrode in order to maintain a constant current intensity. This voltage is therefore proportional to the resistivity of the formation. Because current must flow from the sonde to the formation, these tools only work with conductive borehole fluid. Actually, since the resistivity of the mud is measured in series with the resistivity of the formation, laterolog tools give best results when mud resistivity is low with respect to formation resistivity, i.e., in salty mud.\nInduction logs use an electric coil in the sonde to generate an alternating current loop in the formation by induction. This is the same physical principle as is used in electric transformers. The alternating current loop, in turn, induces a current in a receiving coil located elsewhere on the sonde. The amount of current in the receiving coil is proportional to the intensity of current loop, hence to the conductivity (reciprocal of resistivity) of the formation. Multiple transmitting and receiving coils are used to focus formation current loops both radially (depth of investigation) and axially (vertical resolution). Until the late 80’s, the workhorse of induction logging has been the 6FF40 sonde which is made up of six coils with a nominal spacing of . Since the 90’s all major logging companies use so-called array induction tools. These comprise a single transmitting coil and a large number of receiving coils. Radial and axial focusing is performed by software rather than by the physical layout of coils. Since the formation current flows in circular loops around the logging tool, mud resistivity is measured in parallel with formation resistivity. Induction tools therefore give best results when mud resistivity is high with respect to formation resistivity, i.e., fresh mud or non-conductive fluid. In oil-base mud, which is non conductive, induction logging is the only option available.\n\nUntil the late 1950s electric logs, mud logs and sample logs comprised most of the oilman's armamentarium. Logging tools to measure porosity and permeability began to be used at that time. The first was the microlog. This was a miniature electric log with two sets of electrodes. One measured the formation resistivity about 1/2\" deep and the other about 1\"-2\" deep. The purpose of this seemingly pointless measurement was to detect permeability. Permeable sections of a borehole wall develop a thick layer of mudcake during drilling. Mud liquids, called filtrate, soak into the formation, leaving the mud solids behind to -ideally- seal the wall and stop the filtrate \"invasion\" or soaking. The short depth electrode of the microlog sees mudcake in permeable sections. The deeper 1\" electrode sees filtrate invaded formation. In nonpermeable sections both tools read alike and the traces fall on top of each other on the stripchart log. In permeable sections they separate.\n\nAlso in the late 1950s porosity measuring logs were being developed. The two main types are: nuclear porosity logs and sonic logs.\n\nThe two main nuclear porosity logs are the Density and the Neutron log.\n\nDensity logging tools contain a caesium-137 gamma ray source which irradiates the formation with 662 keV gamma rays. These gamma rays interact with electrons in the formation through Compton scattering and lose energy. Once the energy of the gamma ray has fallen below 100 keV, photolectric absorption dominates: gamma rays are eventually absorbed by the formation. The amount of energy loss by Compton scattering is related to the number electrons per unit volume of formation. Since for most elements of interest (below Z = 20) the ratio of atomic weight, A, to atomic number, Z, is close to 2, gamma ray energy loss is related to the amount of matter per unit volume, i.e., formation density.\n\nA gamma ray detector located some distance from the source, detects surviving gamma rays and sorts them into several energy windows. The number of high-energy gamma rays is controlled by compton scattering, hence by formation density. The number of low-energy gamma rays is controlled by photoelectric absorption, which is directly related to the average atomic number, Z, of the formation, hence to lithology. Modern density logging tools include two or three detectors, which allow compensation for some borehole effects, in particular for the presence of mud cake between the tool and the formation.\n\nSince there is a large contrast between the density of the minerals in the formation and the density of pore fluids, porosity can easily be derived from measured formation bulk density if both mineral and fluid densities are known.\n\nNeutron porosity logging tools contain an americium-beryllium neutron source, which irradiates the formation with neutrons. These neutrons lose energy through elastic collisions with nuclei in the formation. Once their energy has decreased to thermal level, they diffuse randomly away from the source and are ultimately absorbed by a nucleus. Hydrogen atoms have essentially the same mass as the neutron; therefore hydrogen is the main contributor to the slowing down of neutrons. A detector at some distance from the source records the number of neutron reaching this point. Neutrons that have been slowed down to thermal level have a high probability of being absorbed by the formation before reaching the detector. The neutron counting rate is therefore inversely related to the amount of hydrogen in the formation. Since hydrogen is mostly present in pore fluids (water, hydrocarbons) the count rate can be converted into apparent porosity. Modern neutron logging tools usually include two detectors to compensate for some borehole effects. Porosity is derived from the ratio of count rates at these two detectors rather than from count rates at a single detector.\n\nThe combination of neutron and density logs takes advantage of the fact that lithology has opposite effects on these two porosity measurements. The average of neutron and density porosity values is usually close to the true porosity, regardless of lithology. Another advantage of this combination is the \"gas effect.\" Gas, being less dense than liquids, translates into a density-derived porosity that is too high. Gas, on the other hand, has much less hydrogen per unit volume than liquids: neutron-derived porosity, which is based on the amount of hydrogen, is too low. If both logs are displayed on compatible scales, they overlay each other in liquid-filled clean formations and are widely separated in gas-filled formations.\n\nSonic logs use a pinger and microphone arrangement to measure the velocity of sound in the formation from one end of the sonde to the other. For a given type of rock, acoustic velocity varies indirectly with porosity. If the velocity of sound through solid rock is taken as a measurement of 0% porosity, a slower velocity is an indication of a higher porosity that is usually filled with formation water with a slower sonic velocity.\n\nBoth sonic and density-neutron logs give porosity as their primary information. Sonic logs read farther away from the borehole so they are more useful where sections of the borehole are caved. Because they read deeper, they also tend to average more formation than the density-neutron logs do. Modern sonic configurations with pingers and microphones at both ends of the log, combined with computer analysis, minimize the averaging somewhat. Averaging is an advantage when the formation is being evaluated for seismic parameters, a different area of formation evaluation. A special log, the Long Spaced Sonic, is sometimes used for this purpose. Seismic signals (a single undulation of a sound wave in the earth) average together tens to hundreds of feet of formation, so an averaged sonic log is more directly comparable to a seismic waveform.\n\nDensity-neutron logs read the formation within about four to seven inches (178 mm) of the borehole wall. This is an advantage in resolving thin beds. It is a disadvantage when the hole is badly caved. Corrections can be made automatically if the cave is no more than a few inches deep. A caliper arm on the sonde measures the profile of the borehole and a correction is calculated and incorporated in the porosity reading. However, if the cave is much more than four inches deep, the density-neutron log is reading little more than drilling mud.\n\nThere are two other tools, the SP log and the Gamma Ray log, one or both of which are almost always used in wireline logging. Their output is usually presented along with the electric and porosity logs described above. They are indispensable as additional guides to the nature of the rock around the borehole.\n\nThe SP log, known variously as a \"Spontaneous Potential\", \"Self Potential\" or \"Shale Potential\" log is a voltmeter measurement of the voltage or electrical potential difference between the mud in the hole at a particular depth and a copper ground stake driven into the surface of the earth a short distance from the borehole. A salinity difference between the drilling mud and the formation water acts as a natural battery and will cause several voltage effects. This \"battery\" causes a movement of charged ions between the hole and the formation water where there is enough permeability in the rock. The most important voltage is set up as a permeable formation permits ion movement, reducing the voltage between the formation water and the mud. Sections of the borehole where this occurs then have a voltage difference with other nonpermeable sections where ion movement is restricted. Vertical ion movement in the mud column occurs much more slowly because the mud is not circulating while the drill pipe is out of the hole. The copper surface stake provides a reference point against which the SP voltage is measured for each part of the borehole. There can also be several other minor voltages, due for example to mud filtrate streaming into the formation under the effect of an overbalanced mud system. This flow carries ions and is a voltage generating current. These other voltages are secondary in importance to the voltage resulting from the salinity contrast between mud and formation water.\n\nThe nuances of the SP log are still being researched. In theory, almost all porous rocks contain water. Some pores are completely filled with water. Others have a thin layer of water molecules wetting the surface of the rock, with gas or oil filling the rest of the pore. In sandstones and porous limestones there is a continuous layer of water throughout the formation. If there is even a little permeability to water, ions can move through the rock and decrease the voltage difference with the mud nearby. Shales do not allow water or ion movement. Although they may have a large water content, it is bound to the surface of the flat clay crystals comprising the shale. Thus mud opposite shale sections maintains its voltage difference with the surrounding rock. As the SP logging tool is drawn up the hole it measures the voltage difference between the reference stake and the mud opposite shale and sandstone or limestone sections. The resulting log curve reflects the permeability of the rocks and, indirectly, their lithology. SP curves degrade over time, as the ions diffuse up and down the mud column. It also can suffer from stray voltages caused by other logging tools that are run with it. Older, simpler logs often have better SP curves than more modern logs for this reason. With experience in an area, a good SP curve can even allow a skilled interpreter to infer sedimentary environments such as deltas, point bars or offshore tidal deposits.\n\nThe gamma ray log is a measurement of naturally occurring gamma radiation from the borehole walls. Sandstones are usually nonradioactive quartz and limestones are nonradioactive calcite. Shales however, are naturally radioactive due to potassium isotopes in clays, and adsorbed uranium and thorium. Thus the presence or absence of gamma rays in a borehole is an indication of the amount of shale or clay in the surrounding formation. The gamma ray log is useful in holes drilled with air or with oil based muds, as these wells have no SP voltage. Even in water-based muds, the gamma ray and SP logs are often run together. They comprise a check on each other and can indicate unusual shale sections which may either not be radioactive, or may have an abnormal ionic chemistry. The gamma ray log is also useful to detect coal beds, which, depending on the local geology, can have either low radiation levels, or high radiation levels due to adsorption of uranium. In addition, the gamma ray log will work inside a steel casing, making it essential when a cased well must be evaluated.\n\nThe immediate questions that have to be answered in deciding to complete a well or to plug and abandon (P&A) it are:\n\nThe elementary approach to answering these questions uses the Archie Equation.\n"}
{"id": "19539938", "url": "https://en.wikipedia.org/wiki?curid=19539938", "title": "Fréchet distance", "text": "Fréchet distance\n\nIn mathematics, the Fréchet distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. It is named after Maurice Fréchet.\n\nImagine a man traversing a finite curved path while walking his dog on a leash, with the dog traversing a separate one. Assume that the dog varies its speed to keep as much slack in her leash as possible: the Fréchet distance between the two curves is the length of the shortest leash sufficient for both to traverse their separate paths. Note that the definition is symmetric with respect to the two curves—the Frechet distance would be the same if the dog was walking its owner.\n\nLet formula_1 be a metric space. A curve formula_2 in formula_1 is a continuous map from the unit interval into formula_1, i.e., formula_5. A reparameterization formula_6 of formula_7 is a continuous, non-decreasing, surjection formula_8.\n\nLet formula_2 and formula_10 be two given curves in formula_1. Then, the Fréchet distance between formula_2 and formula_10 is defined as the infimum over all reparameterizations formula_6 and formula_15 of formula_7 of the maximum over all formula_17 of the distance in formula_1 between formula_19 and formula_20. In mathematical notation, the Fréchet distance formula_21 is\n\nformula_22\n\nwhere formula_23 is the distance function of formula_1.\n\nInformally, we can think of the parameter formula_25 as \"time\". Then, formula_19 is the position of the dog and formula_20 is the position of the dog's owner at time formula_25 (or vice versa). The length of the leash between them at time formula_25 is the distance between formula_19 and formula_20. Taking the infimum over all possible reparametrizations of formula_7 corresponds to choosing the walk along the given paths where the maximum leash length is minimized. The restriction that formula_6 and formula_15 be non-decreasing means that neither the dog nor its owner can backtrack.\n\nThe Fréchet metric takes into account the flow of the two curves because the pairs of points whose distance contributes to the Fréchet distance sweep continuously along their respective curves. This makes the Fréchet distance a better measure of similarity for curves than alternatives, such as the Hausdorff distance, for arbitrary point sets. It is possible for two curves to have small Hausdorff distance but large Fréchet distance.\n\nThe Fréchet distance and its variants find application in several problems, from morphing and handwriting recognition to protein structure alignment. Alt and Godau were the first to describe a polynomial-time algorithm to compute the Fréchet distance between two polygonal curves in Euclidean space, based on the principle of parametric search. The running time of their algorithm is formula_35 for two polygonal curves with \"m\" and \"n\" segments.\n\nAn important tool for calculating the Fréchet distance of two curves is the free-space diagram, which was introduced by Alt and Godau.\nThe free-space diagram between two curves for a given distance threshold ε is a two-dimensional region in the parameter space that consist of all point pairs on the two curves at distance at most ε:\n\nformula_36\n\nThe Fréchet distance formula_21 is at most ε if and only if the free-space diagram formula_38 contains a path from the lower left corner to the upper right corner, which is monotone both in the horizontal and in the vertical direction.\n\nThe weak Fréchet distance is a variant of the classical Fréchet distance without the requirement that the endpoints move monotonically along their respective curves — the dog and its owner are allowed to backtrack to keep the leash between them short. Alt and Godau describe a simpler algorithm to compute the weak Fréchet distance between polygonal curves, based on computing minimax paths in an associated grid graph.\n\nThe discrete Fréchet distance, also called the coupling distance, is an approximation of the Fréchet metric for polygonal curves, defined by Eiter and Mannila. The discrete Fréchet distance considers only positions of the leash where its endpoints are located at vertices of the two polygonal curves and never in the interior of an edge. This special structure allows the discrete Fréchet distance to be computed in polynomial time by an easy dynamic programming algorithm.\n\nWhen the two curves are embedded in a metric space other than Euclidean space, such as a polyhedral terrain or some Euclidean space with obstacles, the distance between two points on the curves is most naturally defined as the length of the shortest path between them. The leash is required to be a geodesic joining its endpoints. The resulting metric between curves is called the geodesic Fréchet distance. Cook and Wenk describe a polynomial-time algorithm to compute the geodesic Fréchet distance between two polygonal curves in a simple polygon.\n\nIf we further require that the leash must move continuously in the ambient metric space, then we obtain the notion of the homotopic Fréchet distance between two curves. The leash cannot switch discontinuously from one position to another — in particular, the leash cannot jump over obstacles, and can sweep over a mountain on a terrain only if it is long enough. The motion of the leash describes a homotopy between the two curves. Chambers \"et al.\" describe a polynomial-time algorithm to compute the homotopic Fréchet distance between polygonal curves in the Euclidean plane with obstacles.\n\nThe Fréchet distance between two concentric circles of radius formula_39 and formula_40 respectively is formula_41\nThe longest leash is required when the owner stands still and the dog travels to the opposite side of the circle (formula_42), and the shortest leash when both owner and dog walk at a constant angular velocity around the circle (formula_43).\n\n"}
{"id": "3626951", "url": "https://en.wikipedia.org/wiki?curid=3626951", "title": "Grounding (discipline technique)", "text": "Grounding (discipline technique)\n\nGrounding is a common discipline technique used with children and teenagers, in which the child or teen is not allowed to leave their place of residence \"or\" bedroom except for required activities such as school or work. Children and teenagers are generally not allowed to leave the house for leisure during this time.\n\nOther possible consequences can also include removing positive reinforcements, privileges and freedom such as video games, toys, television, computers, mobile devices, Internet, fidget spinners, shopping malls and hanging out with or having friends over. \n\nGrounding has been suggested as an alternative to physical discipline or spanking for behavior management in the home. According to a 2000 review on child outcomes, \"Grounding has been replicated as a more effective disciplinary alternative than spanking with teenagers.\" Grounding can backfire if the type and duration of restrictions are too severe relative to the behavior meant to be corrected, or if the restrictions are too difficult for the parent to enforce.\n\nThis term was used originally in aviation: when a pilot is prevented from flying an aircraft due to misconduct, illness, technical problems with the aircraft, or other reasons, the pilot is \"grounded\".\n"}
{"id": "243916", "url": "https://en.wikipedia.org/wiki?curid=243916", "title": "Head louse", "text": "Head louse\n\nThe head louse (\"Pediculus humanus capitis\") is an obligate ectoparasite of humans that causes head lice infestation (pediculosis capitis).\n\nHead lice are wingless insects spending their entire lives on the human scalp and feeding exclusively on human blood. Humans are the only known hosts of this specific parasite, while chimpanzees host a closely related species, \"Pediculus schaeffi\". Other species of lice infest most orders of mammals and all orders of birds, as well as other parts of the human body.\n\nLice differ from other hematophagic ectoparasites such as fleas in spending their entire lifecycle on a host. Head lice cannot fly, and their short, stumpy legs render them incapable of jumping, or even walking efficiently on flat surfaces.\n\nThe nondisease-carrying head louse differs from the related disease-carrying body louse (\"Pediculus humanus humanus\") in preferring to attach eggs to scalp hair rather than to clothing. The two subspecies are morphologically almost identical, but do not normally interbreed, although they will do so in laboratory conditions. From genetic studies, they are thought to have diverged as subspecies about 30,000–110,000 years ago, when many humans began to wear a significant amount of clothing. A much more distantly related species of hair-clinging louse, the pubic or crab louse (\"Pthirus pubis\"), also infests humans. It is visually different from the other two species and is much closer in appearance to the lice which infest other primates. Lice infestation of any part of the body is known as pediculosis.\n\nHead lice (especially in children) have been, and still are, subject to various eradication campaigns. Unlike body lice, head lice are not the vectors of any known diseases. Except for rare secondary infections that result from scratching at bites, head lice are harmless, and they have been regarded by some as essentially a cosmetic rather than a medical problem. Head lice infestations might be beneficial in helping to foster a natural immune response against lice which helps humans in defense against the far more dangerous body louse, which is capable of transmission of dangerous diseases.\n\nLike other insects of the suborder Anoplura, adult head lice are small (2.5–3 mm long), dorsoventrally flattened (see anatomical terms of location), and entirely wingless. The thoracic segments are fused, but otherwise distinct from the head and abdomen, the latter being composed of seven visible segments. Head lice are grey in general, but their precise color varies according to the environment in which they were raised. After feeding, consumed blood causes the louse body to take on a reddish color.\n\nOne pair of antennae, each with five segments, protrudes from the insect's head. Head lice also have one pair of eyes. Eyes are present in all species within the Pediculidae (the family of which the head louse is a member), but are reduced or absent in most other members of the Anoplura suborder. Like other members of the Anoplura, head lice mouth parts are highly adapted for piercing skin and sucking blood. These mouth parts are retracted into the insect's head except during feeding.\n\nSix legs project from the fused segments of the thorax. As is typical in the Anoplura, these legs are short and terminate with a single claw and opposing \"thumb\". Between its claw and thumb, the louse grasps the hair of its host. With their short legs and large claws, lice are well adapted to clinging to the hair of their host. These adaptations leave them incapable of jumping, or even walking efficiently on flat surfaces. Lice can climb up strands of hair very quickly, allowing them to move quickly and reach another host.\n\nSeven segments of the louse abdomen are visible. The first six segments each have a pair of spiracles through which the insect breathes. The last segment contains the anus and (separately) the genitalia.\n\nIn male lice, the front two legs are slightly larger than the other four. This specialized pair of legs is used for holding the female during copulation. Males are slightly smaller than females and are characterized by a pointed end of the abdomen and a well-developed genital apparatus visible inside the abdomen. Females are characterized by two gonopods in the shape of a W at the end of their abdomens.\n\nLike most insects, head lice are oviparous. Females lay about three or four eggs per day. Louse eggs are attached near the base of a host hair shaft. Egg-laying behavior is temperature dependent and likely seeks to place the egg in a location that will be conducive to proper embyro development (which is, in turn, temperature dependent). In cool climates, eggs are generally laid within 3–5 mm of the scalp surface. In warm climates, and especially the tropics, eggs may be laid or more down the hair shaft.\n\nTo attach an egg, the adult female secretes a glue from her reproductive organ. This glue quickly hardens into a \"nit sheath\" that covers the hair shaft and large parts of the egg except for the operculum, a cap through which the embryo breathes. The glue was previously thought to be chitin-based, but more recent studies have shown it to be made of proteins similar to hair keratin.\n\nEach egg is oval-shaped and about 0.8 mm in length. They are bright, transparent, and tan to coffee-colored so long as they contain an embryo, but appear white after hatching. Head lice hatch typically six to nine days after oviposition.\n\nAfter hatching, the louse nymph leaves behind its egg shell (usually known as a \"nit\", see below), still attached to the hair shaft. The empty egg shell remains in place until physically removed by abrasion or the host, or until it slowly disintegrates, which may take 6 or more months.\n\nThe term \"nit\" refers to an egg without embryo or a dead egg. With respect to eggs, this rather broad definition includes the following:\nAccordingly, on the head of an infested individual, these eggs could be found:\n\nThis has produced some confusion in, for example, school policy (see The \"no-nit\" policy) because, of the three items listed above, only eggs containing viable embryos have the potential to infest or reinfest a host. Some authors have reacted to this confusion by restricting the definition of nit to describe only a hatched or nonviable egg:\n\nOthers have retained the broad definition, while simultaneously attempting to clarify its relevance to infestation:\n\nHead lice, like other insects of the order Phthiraptera, are hemimetabolous. Newly hatched nymphs will moult three times before reaching the sexually-mature adult stage. Thus, mobile head lice populations contain members of up to four developmental stages: three nymphal instars, and the adult (imago). Metamorphosis during head lice development is subtle. The only visible differences between different instars and the adult, other than size, is the relative length of the abdomen, which increases with each molt. Aside from reproduction, nymph behavior is similar to the adult. Nymphs feed only on human blood (hematophagia), and cannot survive long away from a host.\n\nThe time required for head lice to complete their nymph development to the imago depends on feeding conditions. At minimum, eight to nine days are required for lice having continuous access to a human host. This experimental condition is most representative of head lice conditions in the wild. Experimental conditions where the nymph has more limited access to blood produces more prolonged development, ranging from 12 to 24 days.\n\nNymph mortality in captivity is high—about 38%—especially within the first two days of life. In the wild, mortality may instead be highest in the third instar. Nymph hazards are numerous. Failure to completely hatch from the egg is invariably fatal and may be dependent on the humidity of the egg's environment. Death during molting can also occur, although it is reportedly uncommon. During feeding, the nymph gut can rupture, dispersing the host's blood throughout the insect. This results in death within a day or two. Whether the high mortality recorded under experimental conditions is representative of conditions in the wild is unclear.\n\nAdult head lice reproduce sexually, and copulation is necessary for the female to produce fertile eggs. Parthenogenesis, the production of viable offspring by virgin females, does not occur in \"Pediculus humanus\". Pairing can begin within the first 10 hours of adult life. After 24 hours, adult lice copulate frequently, with mating occurring during any period of the night or day. Mating attachment frequently lasts more than an hour. Young males can successfully pair with older females, and vice versa.\n\nExperiments with \"P. h. humanus\" (body lice) emphasize the attendant hazards of lice copulation. A single young female confined with six or more males will die in a few days, having laid very few eggs. Similarly, death of a virgin female was reported after admitting a male to her confinement. The female laid only one egg after mating, and her entire body was tinged with red—a condition attributed to rupture of the alimentary canal during the sexual act. Old females frequently die following, if not during, intercourse.\n\nA single louse has a 30-day lifecycle beginning from the moment the nit is laid until the adult louse dies.\n\nThe number of children per family, the sharing of beds and closets, hair washing habits, local customs and social contacts, healthcare in a particular area (e.g. school), and socioeconomic status were found to be significant factors in head louse infestation. Girls are two to four times more frequently infested than boys. Children between 4 and 14 years of age are the most frequently infested group.\n\nAll stages are blood-feeders and bite the skin four to five times daily to feed. They inject saliva which contains an anticoagulant and suck blood. The digested blood is excreted as dark red frass.\n\nAlthough any part of the scalp may be colonized, lice favor the nape of the neck and the area behind the ears, where the eggs are usually laid. Head lice are repelled by light and move towards shadows or dark-coloured objects in their vicinity.\n\nLice have no wings or powerful legs for jumping, so they move using their claw-like legs to transfer from hair to hair. Normally, head lice infest a new host only by close contact between individuals, making social contacts among children and parent-child interactions more likely routes of infestation than shared combs, hats, brushes, towels, clothing, beds, or closets. Head-to-head contact is by far the most common route of lice transmission.\n\nAbout 6–12 million people, mainly children, are treated annually for head lice in the United States alone. In the UK, it is estimated that two thirds of children will experience at least one case of head lice before leaving primary school. High levels of louse infestations have also been reported from all over the world, including Australia, Denmark, France, Ireland, Israel, and Sweden. Head lice can live off the head, for example on soft furnishings such as pillow cases, on hairbrushes, or on coat hoods for up to 48 hours.\n\nAnalysis of the DNA of lice found on Peruvian mummies may indicate that some diseases (such as typhus) may have passed from the New World to the Old World, instead of the other way around.\n\nThe sequencing of the genome of the body louse was first proposed in the mid-2000s and the annotated genome was published in 2010. An analysis of the body and head louse transcriptomes revealed these two organisms are extremely similar genetically.\n\nHuman lice are divided into three deeply divergent mitochrondrial clades known as A, B, and C. Two subclades have been identified, D (a sister clade of A) and E (a sister clade of C).\n\n\n\n\n\n\n\n"}
{"id": "36595472", "url": "https://en.wikipedia.org/wiki?curid=36595472", "title": "History of the function concept", "text": "History of the function concept\n\nThe mathematical concept of a function emerged in the 17th century in connection with the development of the calculus; for example, the slope formula_1 of a graph at a point was regarded as a function of the \"x\"-coordinate of the point. Functions were not explicitly considered in antiquity, but some precursors of the concept can perhaps be seen in the work of medieval philosophers and mathematicians such as Oresme.\n\nMathematicians of the 18th century typically regarded a function as being defined by an analytic expression. In the 19th century, the demands of the rigorous development of analysis by Weierstrass and others, the reformulation of geometry in terms of analysis, and the invention of set theory by Cantor, eventually led to the much more general modern concept of a function as a single-valued mapping from one set to another.\n\nAlready in the 12th century, mathematician Sharaf al-Din al-Tusi analyzed the equation in the form stating that the left hand side must at least equal the value of for the equation to have a solution. He then determined the maximum value of this expression. It is arguable that the isolation of this expression is an early approach to the notion of a \"function\". A value less than means no positive solution; a value equal to corresponds to one solution, while a value greater than corresponds to two solutions. Sharaf al-Din's analysis of this equation was a notable development in Islamic mathematics, but his work was not pursued any further at that time, neither in the Muslim world nor in Europe.\n\nAccording to Dieudonné and Ponte, the concept of a function emerged in the 17th century as a result of the development of analytic geometry and the infinitesimal calculus. Nevertheless, Medvedev suggests that the implicit concept of a function is one with an ancient lineage. Ponte also sees more explicit approaches to the concept in the Middle Ages:\n\nThe development of analytical geometry around 1640 allowed mathematicians to go between geometric problems about curves and algebraic relations between \"variable coordinates \"x\" and \"y\".\" Calculus was developed using the notion of variables, with their associated geometric meaning, which persisted well into the eighteenth century. However, the terminology of \"function\" came to be used in interactions between Leibniz and Bernoulli towards the end of the 17th century.\n\nThe term \"function\" was literally introduced by Gottfried Leibniz, in a 1673 letter, to describe a quantity related to points of a curve, such as a coordinate or curve's slope.\nJohann Bernoulli started calling expressions made of a single variable \"functions.\"\nIn 1698, he agreed with Leibniz that any quantity formed \"in an algebraic and transcendental manner\" may be called a function of \"x\".\nBy 1718, he came to regard as a function \"any expression made up of a variable and some constants.\" Alexis Claude Clairaut (in approximately 1734) and Leonhard Euler introduced the familiar notation formula_2 for the value of a function.\n\nThe functions considered in those times are called today differentiable functions. For this type of function, one can talk about limits and derivatives; both are measurements of the output or the change in the output as it depends on the input or the change in the input. Such functions are the basis of calculus.\n\nIn the first volume of his fundamental text \"Introductio in Analysin Infinitorum\", published in 1748, Euler gave essentially the same definition of a function as his teacher Bernoulli, as an expression or formula involving variables and constants e.g., formula_3. Euler's own definition reads:\n\nEuler also allowed multi-valued functions whose values are determined by an implicit equation.\n\nIn 1755, however, in his \"Institutiones Calculi Differentialis,\" Euler gave a more general concept of a function:\n\nMedvedev considers that \"In essence this is the definition that became known as Dirichlet's definition.\" Edwards also credits Euler with a general concept of a function and says further that\n\nIn his \"Théorie Analytique de la Chaleur,\" Fourier claimed that an arbitrary function could be represented by a Fourier series. Fourier had a general conception of a function, which included functions that were neither continuous nor defined by an analytical expression. Related questions on the nature and representation of functions, arising from the solution of the wave equation for a vibrating string, had already been the subject of dispute between d'Alembert and Euler, and they had a significant impact in generalizing the notion of a function. Luzin observes that:\n\nDuring the 19th century, mathematicians started to formalize all the different branches of mathematics. One of the first to do so was Cauchy; his somewhat imprecise results were later made completely rigorous by Weierstrass, who advocated building calculus on arithmetic rather than on geometry, which favoured Euler's definition over Leibniz's (see arithmetization of analysis). According to Smithies, Cauchy thought of functions as being defined by equations involving real or complex numbers, and tacitly assumed they were continuous:\n\nNikolai Lobachevsky and Peter Gustav Lejeune Dirichlet are traditionally credited with independently giving the modern \"formal\" definition of a function as a relation in which every first element has a unique second element.\n\nLobachevsky (1834) writes that\n\nwhile Dirichlet (1837) writes\n\nEves asserts that \"the student of mathematics usually meets the Dirichlet definition of function in his introductory course in calculus.\n\nDirichlet's claim to this formalization has been disputed by Imre Lakatos:\n\nHowever, Gardiner says\n\"...it seems to me that Lakatos goes too far, for example, when he asserts that 'there is ample evidence that [Dirichlet] had no idea of [the modern function] concept'.\"\nMoreover, as noted above, Dirichlet's paper does appear to include a definition along the lines of what is usually ascribed to him, even though (like Lobachevsky) he states it only for continuous functions of a real variable.\n\nSimilarly, Lavine observes that:\n\nBecause Lobachevsky and Dirichlet have been credited as among the first to introduce the notion of an arbitrary correspondence, this notion is sometimes referred to as the Dirichlet or Lobachevsky-Dirichlet definition of a function. A general version of this definition was later used by Bourbaki (1939), and some in the education community refer to it as the \"Dirichlet–Bourbaki\" definition of a function.\n\nDieudonné, who was one of the founding members of the Bourbaki group, credits a precise and general modern definition of a function to Dedekind in his work\n\"Was sind und was sollen die Zahlen\", which appeared in 1888 but had already been drafted in 1878. Dieudonné observes that instead of confining himself, as in previous conceptions, to real (or complex) functions, Dedekind defines a function as a single-valued mapping between any two sets:\n\n defined a function as a relation between two variables \"x\" and \"y\" such that \"to some values of \"x\" at any rate correspond values of \"y\".\" He neither required the function to be defined for all values of \"x\" nor to associate each value of \"x\" to a single value of \"y\". This broad definition of a function encompasses more relations than are ordinarily considered functions in contemporary mathematics. For example, Hardy's definition includes multivalued functions and what in computability theory are called partial functions.\n\nLogicians of this time were primarily involved with analyzing syllogisms (the 2000-year-old Aristotelian forms and otherwise), or as Augustus De Morgan (1847) stated it: \"the examination of that part of reasoning which depends upon the manner in which inferences are formed,\nand the investigation of general maxims and rules for constructing arguments\". At this time the notion of (logical) \"function\" is not explicit, but at least in the work of De Morgan and George Boole it is implied: we see abstraction of the argument forms, the introduction of variables, the introduction of a symbolic algebra with respect to these variables, and some of the notions of set theory.\n\nDe Morgan's 1847 \"FORMAL LOGIC OR, The Calculus of Inference, Necessary and Probable\" observes that \"[a] logical truth depends upon the \"structure of the statement\", and not upon the particular matters spoken of\"; he wastes no time (preface page i) abstracting: \"In the form of the proposition, the copula is made as abstract as the terms\". He immediately (p. 1) casts what he calls \"the proposition\" (present-day propositional \"function\" or \"relation\") into a form such as \"X is Y\", where the symbols X, \"is\", and Y represent, respectively, the \"subject\", \"copula\", and \"predicate.\" While the word \"function\" does not appear, the notion of \"abstraction\" is there, \"variables\" are there, the notion of inclusion in his symbolism \"all of the Δ is in the О\" (p. 9) is there, and lastly a new symbolism for logical analysis of the notion of \"relation\" (he uses the word with respect to this example \" X)Y \" (p. 75) ) is there:\n\nIn his 1848 \"The Nature of Logic\" Boole asserts that \"logic . . . is in a more especial sense the science of reasoning by signs\", and he briefly discusses the notions of \"belonging to\" and \"class\": \"An individual may possess a great variety of attributes and thus belonging to a great variety of different classes\" . Like De Morgan he uses the notion of \"variable\" drawn from analysis; he gives an example of \"represent[ing] the class oxen by \"x\" and that of horses by \"y\" and the conjunction \"and\" by the sign + . . . we might represent the aggregate class oxen and horses by \"x + y\"\".\n\nIn the context of \"the Differential Calculus\" Boole defined (circa 1849) the notion of a function as follows:\n\nEves observes \"that logicians have endeavored to push down further the starting level of the definitional development of mathematics and to derive the theory of sets, or classes, from a foundation in the logic of propositions and propositional functions\". But by the late 19th century the logicians' research into the foundations of mathematics was undergoing a major split. The direction of the first group, the Logicists, can probably be summed up best by – \"to fulfil two objects, first, to show that all mathematics follows from symbolic logic, and secondly to discover, as far as possible, what are the principles of symbolic logic itself.\"\n\nThe second group of logicians, the set-theorists, emerged with Georg Cantor's \"set theory\" (1870–1890) but were driven forward partly as a result of Russell's discovery of a paradox that could be derived from Frege's conception of \"function\", but also as a reaction against Russell's proposed solution. Zermelo's set-theoretic response was his 1908 \"Investigations in the foundations of set theory I\" – the first axiomatic set theory; here too the notion of \"propositional function\" plays a role.\n\nIn his \"An Investigation into the laws of thought\" Boole now defined a function in terms of a symbol \"x\" as follows:\nBoole then used \"algebraic\" expressions to define both algebraic and \"logical\" notions, e.g., 1−\"x\" is logical NOT(\"x\"), \"xy\" is the logical AND(\"x\",\"y\"), \"x + y\" is the logical OR(\"x\", \"y\"), \"x\"(\"x\"+\"y\") is \"xx\"+\"xy\", and \"the special law\" \"xx\" = \"x\" = \"x\".\n\nIn his 1881 \"Symbolic Logic\" Venn was using the words \"logical function\" and the contemporary symbolism ( x = f(y), y = f(x), cf page xxi) plus the circle-diagrams historically associated with Venn to describe \"class relations\", the notions \"'quantifying' our predicate\", \"propositions in respect of their extension\", \"the relation of inclusion and exclusion of two classes to one another\", and \"propositional function\" (all on p. 10), the bar over a variable to indicate not-x (page 43), etc. Indeed he equated unequivocally the notion of \"logical function\" with \"class\" [modern \"set\"]: \"... on the view adopted in this book, f(x) never stands for anything but a logical class. It may be a compound class aggregated of many simple classes; it may be a class indicated by certain inverse logical operations, it may be composed of two groups of classes equal to one another, or what is the same thing, their difference declared equal to zero, that is, a logical equation. But however composed or derived, f(x) with us will never be anything else than a general expression for such logical classes of things as may fairly find a place in ordinary Logic\".\n\nGottlob Frege's Begriffsschrift (1879) preceded Giuseppe Peano (1889), but Peano had no knowledge of until after he had published his 1889. Both writers strongly influenced . Russell in turn influenced much of 20th-century mathematics and logic through his \"Principia Mathematica\" (1913) jointly authored with Alfred North Whitehead.\n\nAt the outset Frege abandons the traditional \"concepts \"subject\" and \"predicate\"\", replacing them with \"argument\" and \"function\" respectively, which he believes \"will stand the test of time. It is easy to see how regarding a content as a function of an argument leads to the formation of concepts. Furthermore, the demonstration of the connection between the meanings of the words \"if, and, not, or, there is, some, all,\" and so forth, deserves attention\".\n\nFrege begins his discussion of \"function\" with an example: Begin with the expression \"Hydrogen is lighter than carbon dioxide\". Now remove the sign for hydrogen (i.e., the word \"hydrogen\") and replace it with the sign for oxygen (i.e., the word \"oxygen\"); this makes a second statement. Do this again (using either statement) and substitute the sign for nitrogen (i.e., the word \"nitrogen\") and note that \"This changes the meaning in such a way that \"oxygen\" or \"nitrogen\" enters into the relations in which \"hydrogen\" stood before\". There are three statements:\nNow observe in all three a \"stable component, representing the totality of [the] relations\"; call this the function, i.e.,\nFrege calls the argument of the function \"[t]he sign [e.g., hydrogen, oxygen, or nitrogen], regarded as replaceable by others that denotes the object standing in these relations\". He notes that we could have derived the function as \"Hydrogen is lighter than . . ..\" as well, with an argument position on the \"right\"; the exact observation is made by Peano (see more below). Finally, Frege allows for the case of two (or more arguments). For example, remove \"carbon dioxide\" to yield the invariant part (the function) as:\nThe one-argument function Frege generalizes into the form Φ(A) where A is the argument and Φ( ) represents the function, whereas the two-argument function he symbolizes as Ψ(A, B) with A and B the arguments and Ψ( , ) the function and cautions that \"in general Ψ(A, B) differs from Ψ(B, A)\". Using his unique symbolism he translates for the reader the following symbolism:\n\nPeano defined the notion of \"function\" in a manner somewhat similar to Frege, but without the precision. First Peano defines the sign \"K means \"class\", or aggregate of objects\", the objects of which satisfy three simple equality-conditions, \"a = a\", (\"a = b\") = (\"b = a\"), IF ((\"a = b\") AND (\"b = c\")) THEN (\"a = c\"). He then introduces φ, \"a sign or an aggregate of signs such that if \"x\" is an object of the class \"s\", the expression φx denotes a new object\". Peano adds two conditions on these new objects: First, that the three equality-conditions hold for the objects φx; secondly, that \"if \"x\" and \"y\" are objects of class \"s\" and if \"x\" = \"y\", we assume it is possible to deduce \"φx = φy\"\". Given all these conditions are met, φ is a \"function presign\". Likewise he identifies a \"function postsign\". For example if \"φ\" is the function presign \"a\"+, then \"φx\" yields \"a\"+\"x\", or if \"φ\" is the function postsign +\"a\" then \"xφ\" yields \"x\"+\"a\".\n\nWhile the influence of Cantor and Peano was paramount, in Appendix A \"The Logical and Arithmetical Doctrines of Frege\" of \"The Principles of Mathematics\", Russell arrives at a discussion of Frege's notion of \"function\", \"...a point in which Frege's work is very important, and requires careful examination\". In response to his 1902 exchange of letters with Frege about the contradiction he discovered in Frege's \"Begriffsschrift\" Russell tacked this section on at the last moment.\n\nFor Russell the bedeviling notion is that of \"variable\": \"6. Mathematical propositions are not only characterized by the fact that they assert implications, but also by the fact that they contain \"variables\". The notion of the variable is one of the most difficult with which logic has to deal. For the present, I openly wish to make it plain that there are variables in all mathematical propositions, even where at first sight they might seem to be absent. . . . We shall find always, in all mathematical propositions, that the words \"any\" or \"some\" occur; and these words are the marks of a variable and a formal implication\".\n\nAs expressed by Russell \"the process of transforming constants in a proposition into variables leads to what is called generalization, and gives us, as it were, the formal essence of a proposition ... So long as any term in our proposition can be turned into a variable, our proposition can be generalized; and so long as this is possible, it is the business of mathematics to do it\"; these generalizations Russell named \"propositional functions\"\". Indeed he cites and quotes from Frege's \"Begriffsschrift\" and presents a vivid example from Frege's 1891 \"Function und Begriff\": That \"the essence of the arithmetical function 2\"x\" + \"x\" is what is left when the x is taken away, i.e., in the above instance 2( ) + ( ). The argument \"x\" does not belong to the function but the two taken together make the whole\". Russell agreed with Frege's notion of \"function\" in one sense: \"He regards functions – and in this I agree with him – as more fundamental than predicates and relations\" but Russell rejected Frege's \"theory of subject and assertion\", in particular \"he thinks that, if a term \"a\" occurs in a proposition, the proposition can always be analysed into \"a\" and an assertion about \"a\"\".\n\nRussell would carry his ideas forward in his 1908 \"Mathematical logical as based on the theory of types\" and into his and Whitehead's 1910–1913 \"Principia Mathematica\". By the time of \"Principia Mathematica\" Russell, like Frege, considered the propositional function fundamental: \"Propositional functions are the fundamental kind from which the more usual kinds of function, such as \"sin \"x\"\" or log x or \"the father of x\" are derived. These derivative functions . . . are called \"descriptive functions\". The functions of propositions . . . are a particular case of propositional functions\".\n\nPropositional functions: Because his terminology is different from the contemporary, the reader may be confused by Russell's \"propositional function\". An example may help. Russell writes a propositional function in its raw form, e.g., as \"φŷ\": \"\"ŷ\" is hurt\". (Observe the circumflex or \"hat\" over the variable \"y\"). For our example, we will assign just 4 values to the variable \"ŷ\": \"Bob\", \"This bird\", \"Emily the rabbit\", and \"y\". Substitution of one of these values for variable \"ŷ\" yields a proposition; this proposition is called a \"value\" of the propositional function. In our example there are four values of the propositional function, e.g., \"Bob is hurt\", \"This bird is hurt\", \"Emily the rabbit is hurt\" and \"\"y\" is hurt.\" A proposition, if it is significant—i.e., if its truth is determinate—has a truth-value of \"truth\" or \"falsity\". If a proposition's truth value is \"truth\" then the variable's value is said to satisfy the propositional function. Finally, per Russell's definition, \"a \"class\" [set] is all objects satisfying some propositional function\" (p. 23). Note the word \"all'\" – this is how the contemporary notions of \"For all ∀\" and \"there exists at least one instance ∃\" enter the treatment (p. 15).\n\nTo continue the example: Suppose (from outside the mathematics/logic) one determines that the propositions \"Bob is hurt\" has a truth value of \"falsity\", \"This bird is hurt\" has a truth value of \"truth\", \"Emily the rabbit is hurt\" has an indeterminate truth value because \"Emily the rabbit\" doesn't exist, and \"\"y\" is hurt\" is ambiguous as to its truth value because the argument \"y\" itself is ambiguous. While the two propositions \"Bob is hurt\" and \"This bird is hurt\" are \"significant\" (both have truth values), only the value \"This bird\" of the \"variable\" \"ŷ\" \"satisfies\" the propositional function \"φŷ\": \"\"ŷ\" is hurt\". When one goes to form the class α: \"φŷ\": \"\"ŷ\" is hurt\", only \"This bird\" is included, given the four values \"Bob\", \"This bird\", \"Emily the rabbit\" and \"y\" for variable \"ŷ\" and their respective truth-values: falsity, truth, indeterminate, ambiguous.\n\nRussell defines functions of propositions with arguments, and truth-functions \"f(p)\". For example, suppose one were to form the \"function of propositions with arguments\" p: \"NOT(p) AND q\" and assign its variables the values of \"p\": \"Bob is hurt\" and \"q\": \"This bird is hurt\". (We are restricted to the logical linkages NOT, AND, OR and IMPLIES, and we can only assign \"significant\" propositions to the variables \"p\" and \"q\"). Then the \"function of propositions with arguments\" is p: NOT(\"Bob is hurt\") AND \"This bird is hurt\". To determine the truth value of this \"function of propositions with arguments\" we submit it to a \"truth function\", e.g., \"f(p)\": \"f\"( NOT(\"Bob is hurt\") AND \"This bird is hurt\" ), which yields a truth value of \"truth\".\n\nThe notion of a \"many-one\" functional relation\": Russell first discusses the notion of \"identity\", then defines a descriptive function (pages 30ff) as the unique value \"ιx\" that satisfies the (2-variable) propositional function (i.e., \"relation\") \"φŷ\".\nRussell symbolizes the descriptive function as \"the object standing in relation to \"y\": R'y = (\"ιx\")(\"x R y\"). Russell repeats that \"R'y\" is a function of \"y\", but not a propositional function [sic]; we shall call it a \"descriptive\" function. All the ordinary functions of mathematics are of this kind. Thus in our notation \"sin \"y\"\" would be written \" sin \" 'y\" \", and \"sin\" would stand for the relation sin \" 'y\" has to \"y\"\".\n\nDavid Hilbert set himself the goal of \"formalizing\" classical mathematics \"as a formal axiomatic theory, and this theory shall be proved to be consistent, i.e., free from contradiction\". In \"The Foundations of Mathematics\" he frames the notion of function in terms of the existence of an \"object\":\nHilbert then illustrates the three ways how the ε-function is to be used, firstly as the \"for all\" and \"there exists\" notions, secondly to represent the \"object of which [a proposition] holds\", and lastly how to cast it into the choice function.\n\nRecursion theory and computability: But the unexpected outcome of Hilbert's and his student Bernays's effort was failure; see Gödel's incompleteness theorems of 1931. At about the same time, in an effort to solve Hilbert's Entscheidungsproblem, mathematicians set about to define what was meant by an \"effectively calculable function\" (Alonzo Church 1936), i.e., \"effective method\" or \"algorithm\", that is, an explicit, step-by-step procedure that would succeed in computing a function. Various models for algorithms appeared, in rapid succession, including Church's lambda calculus (1936), Stephen Kleene's μ-recursive functions(1936) and Alan Turing's (1936–7) notion of replacing human \"computers\" with utterly-mechanical \"computing machines\" (see Turing machines). It was shown that all of these models could compute the same class of computable functions. Church's thesis holds that this class of functions exhausts all the number-theoretic functions that can be calculated by an algorithm. The outcomes of these efforts were vivid demonstrations that, in Turing's words, \"there can be no general process for determining whether a given formula \"U\" of the functional calculus K [\"Principia Mathematica\"] is provable\"; see more at Independence (mathematical logic) and Computability theory.\n\nSet theory began with the work of the logicians with the notion of \"class\" (modern \"set\") for example , Jevons (1880), , and . It was given a push by Georg Cantor's attempt to define the infinite in set-theoretic treatment (1870–1890) and a subsequent discovery of an antinomy (contradiction, paradox) in this treatment (Cantor's paradox), by Russell's discovery (1902) of an antinomy in Frege's 1879 (Russell's paradox), by the discovery of more antinomies in the early 20th century (e.g., the 1897 Burali-Forti paradox and the 1905 Richard paradox), and by resistance to Russell's complex treatment of logic and dislike of his axiom of reducibility (1908, 1910–1913) that he proposed as a means to evade the antinomies.\n\nIn 1902 Russell sent a letter to Frege pointing out that Frege's 1879 \"Begriffsschrift\" allowed a function to be an argument of itself: \"On the other hand, it may also be that the argument is determinate and the function indeterminate . . ..\" From this unconstrained situation Russell was able to form a paradox:\nFrege responded promptly that \"Your discovery of the contradiction caused me the greatest surprise and, I would almost say, consternation, since it has shaken the basis on which I intended to build arithmetic\".\n\nFrom this point forward development of the foundations of mathematics became an exercise in how to dodge \"Russell's paradox\", framed as it was in \"the bare [set-theoretic] notions of set and element\".\n\nThe notion of \"function\" appears as Zermelo's axiom III—the Axiom of Separation (Axiom der Aussonderung). This axiom constrains us to use a propositional function Φ(x) to \"separate\" a subset M from a previously formed set M:\n\nAs there is no universal set—sets originate by way of Axiom II from elements of (non-set) \"domain B\" – \"...this disposes of the Russell antinomy so far as we are concerned\". But Zermelo's \"definite criterion\" is imprecise, and is fixed by Weyl, Fraenkel, Skolem, and von Neumann.\n\nIn fact Skolem in his 1922 referred to this \"definite criterion\" or \"property\" as a \"definite proposition\":\n\nvan Heijenoort summarizes:\n\nIn this quote the reader may observe a shift in terminology: nowhere is mentioned the notion of \"propositional function\", but rather one sees the words \"formula\", \"predicate calculus\", \"predicate\", and \"logical calculus.\" This shift in terminology is discussed more in the section that covers \"function\" in contemporary set theory.\n\nThe history of the notion of \"ordered pair\" is not clear. As noted above, Frege (1879) proposed an intuitive ordering in his definition of a two-argument function Ψ(A, B). Norbert Wiener in his 1914 (see below) observes that his own treatment essentially \"revert(s) to Schröder's treatment of a relation as a class of ordered couples\". considered the definition of a relation (such as Ψ(A, B)) as a \"class of couples\" but rejected it:\n\nBy 1910–1913 and \"Principia Mathematica\" Russell had given up on the requirement for an intensional definition of a relation, stating that \"mathematics is always concerned with extensions rather than intensions\" and \"Relations, like classes, are to be taken in \"extension\"\". To demonstrate the notion of a relation in extension Russell now embraced the notion of \"ordered couple\": \"We may regard a relation ... as a class of couples ... the relation determined by φ(\"x, y\") is the class of couples (\"x, y\") for which φ(\"x, y\") is true\". In a footnote he clarified his notion and arrived at this definition:\nBut he goes on to say that he would not introduce the ordered couples further into his \"symbolic treatment\"; he proposes his \"matrix\" and his unpopular axiom of reducibility in their place.\n\nAn attempt to solve the problem of the antinomies led Russell to propose his \"doctrine of types\" in an appendix B of his 1903 \"The Principles of Mathematics\". In a few years he would refine this notion and propose in his 1908 \"The Theory of Types\" two axioms of reducibility, the purpose of which were to reduce (single-variable) propositional functions and (dual-variable) relations to a \"lower\" form (and ultimately into a completely extensional form); he and Alfred North Whitehead would carry this treatment over to \"Principia Mathematica\" 1910–1913 with a further refinement called \"a matrix\". The first axiom is *12.1; the second is *12.11. To quote Wiener the second axiom *12.11 \"is involved only in the theory of relations\". Both axioms, however, were met with skepticism and resistance; see more at Axiom of reducibility. By 1914 Norbert Wiener, using Whitehead and Russell's symbolism, eliminated axiom *12.11 (the \"two-variable\" (relational) version of the axiom of reducibility) by expressing a relation as an ordered pair \"using the null set. At approximately the same time, Hausdorff (1914, p. 32) gave the definition of the ordered pair (a, b) as { {a,1}, {b, 2} }. A few years later Kuratowski (1921) offered a definition that has been widely used ever since, namely { {a, b}, {a} }\". As noted by \"This definition . . . was historically important in reducing the theory of relations to the theory of sets.\n\nObserve that while Wiener \"reduced\" the relational *12.11 form of the axiom of reducibility he \"did not\" reduce nor otherwise change the propositional-function form *12.1; indeed he declared this \"essential to the treatment of identity, descriptions, classes and relations\".\n\nWhere exactly the \"general\" notion of \"function\" as a many-one correspondence derives from is unclear. Russell in his 1920 \"Introduction to Mathematical Philosophy\" states that \"It should be observed that all mathematical functions result form one-many [sic – contemporary usage is many-one] relations . . . Functions in this sense are \"descriptive\" functions\". A reasonable possibility is the \"Principia Mathematica\" notion of \"descriptive function\" – \"R 'y\" = (ι\"x\")(\"x R y\"): \"the singular object that has a relation \"R\" to \"y\"\". Whatever the case, by 1924, Moses Schönfinkel expressed the notion, claiming it to be \"well known\":\n\nAccording to Willard Quine, \"provide[s] for ... the whole sweep of abstract set theory. The crux of the matter is that Schönfinkel lets functions stand as arguments. For Schönfinkel, substantially as for Frege, classes are special sorts of functions. They are propositional functions, functions whose values are truth values. All functions, propositional and otherwise, are for Schönfinkel one-place functions\". Remarkably, Schönfinkel reduces all mathematics to an extremely compact \"functional calculus\" consisting of only three functions: Constancy, fusion (i.e., composition), and mutual exclusivity. Quine notes that Haskell Curry (1958) carried this work forward \"under the head of combinatory logic\".\n\nBy 1925 Abraham Fraenkel (1922) and Thoralf Skolem (1922) had amended Zermelo's set theory of 1908. But von Neumann was not convinced that this axiomatization could not lead to the antinomies. So he proposed his own theory, his 1925 \"An axiomatization of set theory\". It explicitly contains a \"contemporary\", set-theoretic version of the notion of \"function\":\n\nAt the outset he begins with \"I-objects\" and \"II-objects\", two objects \"A\" and \"B\" that are I-objects (first axiom), and two types of \"operations\" that assume ordering as a structural property obtained of the resulting objects [\"x\", \"y\"] and (\"x\", \"y\"). The two \"domains of objects\" are called \"arguments\" (I-objects) and \"functions\" (II-objects); where they overlap are the \"argument functions\" (he calls them I-II objects). He introduces two \"universal two-variable operations\" – (i) the operation [x, y]: \". . . read 'the value of the function \"x\" for the argument \"y\" . . . it itself is a type I object\", and (ii) the operation (\"x, y\"): \". . . (read 'the ordered pair x, y') whose variables \"x\" and \"y\" must both be arguments and that itself produces an argument (\"x, y\"). Its most important property is that \"x\" = \"x\" and \"y\" = \"y\" follow from (\"x\" = \"y\") = (\"x\" = \"y\")\". To clarify the function pair he notes that \"Instead of \"f\"(\"x\") we write [\"f,x\"] to indicate that \"f\", just like \"x\", is to be regarded as a variable in this procedure\". To avoid the \"antinomies of naive set theory, in Russell's first of all . . . we must forgo treating certain functions as arguments\". He adopts a notion from Zermelo to restrict these \"certain functions\".\n\nSuppes observes that von Neumann's axiomatization was modified by Bernays \"in order to remain nearer to the original Zermelo system . . . He introduced two membership relations: one between sets, and one between sets and classes\". Then Gödel [1940] further modified the theory: \"his primitive notions are those of set, class and membership (although membership alone is sufficient)\". This axiomatization is now known as von Neumann–Bernays–Gödel set theory.\n\nIn 1939, Bourbaki, in addition to giving the well-known ordered pair definition of a function as a certain subset of the cartesian product E x F, gave the following:\n\n\"Let E and F be two sets, which may or may not be distinct. A relation between a variable element x of E and a variable element y of F is called a functional relation in y if, for all x ∈ E, there exists a unique y ∈ F which is in the given relation with x.\n\"We give the name of function to the operation which in this way associates with every element x ∈ E the element y ∈ F which is in the given relation with x, and the function is said to be determined by the given functional relation. Two equivalent functional relations determine the same function.\"\n\nBoth axiomatic and naive forms of Zermelo's set theory as modified by Fraenkel (1922) and Skolem (1922) \"define\" \"function\" as a relation, \"define\" a relation as a set of ordered pairs, and \"define\" an ordered pair as a set of two \"dissymetric\" sets.\n\nWhile the reader of \"Axiomatic Set Theory\" or \"Naive Set Theory\" observes the use of function-symbolism in the \"axiom of separation\", e.g., φ(x) (in Suppes) and S(x) (in Halmos), they will see no mention of \"proposition\" or even \"first order predicate calculus\". In their place are \"\"expressions\" of the object language\", \"atomic formulae\", \"primitive formulae\", and \"atomic sentences\".\n\nIn 1954, Bourbaki, on p. 76 in Chapitre II of Theorie des Ensembles (theory of sets), gave a definition of a function as a triple \"f\" = (\"F\", \"A\", \"B\"). Here \"F\" is a \"functional graph\", meaning a set of pairs where no two pairs have the same first member. On p. 77 (\"op. cit.\") Bourbaki states (literal translation): \"Often we shall use, in the remainder of this Treatise, the word \"function\" instead of \"functional graph\".\"\n\nThe reason for the disappearance of the words \"propositional function\" e.g., in , and , is explained by together with further explanation of the terminology:\nFor his part Tarski calls the relational form of function a \"FUNCTIONAL RELATION or simply a FUNCTION\". After a discussion of this \"functional relation\" he asserts that:\nSee more about \"truth under an interpretation\" at Alfred Tarski.\n\n\n\n"}
{"id": "49357959", "url": "https://en.wikipedia.org/wiki?curid=49357959", "title": "Human Rights City", "text": "Human Rights City\n\nA Human Rights City is a municipality that refers explicitly to the Universal Declaration of Human Rights and other international human rights standards and/or law in their policies, statements, and programs. Analysts have observed growing numbers of such cities since 2000... The Human Rights City initiative emerged from the global human rights movement, and it reflects efforts of activist groups to improve respect for human rights principles by governments and other powerful actors who operate at the local/community level. Because of their focus on local contexts, Human Rights Cities tend to emphasize economic, social, and cultural rights as they affect the lives of residents of cities and other communities and their ability to enjoy civil and political human rights.\n\nHuman rights advocates describe a Human Rights City as “One whose residents and local authorities, through learning about the relevance of human rights to their daily lives (guided by a steering committee), join in ongoing learning, discussions, systematic analysis and critical thinking at the community level, to pursue a creative exchange of ideas and the joint planning of actions to realize their economic, social, political, civil and cultural human rights.” Human rights cities were defined at the 2011 World Human Rights Cities Forum of Gwangju (South Korea) as \"both a local community and a socio-political process in a local context where human rights play a key role as fundamental values and guiding principles.\" This framework has generated various practices in different cities.\n\nThe Human Rights City initiative is the result of long-standing efforts of popular groups to defend and promote human rights, and thus represents an aspect of global human rights struggles.\n\nContemporary human rights city initiatives grow directly from earlier organizing around rights claims in urban settings. The widespread nature of urban problems affecting peoples’ everyday lives and survival have generated similar types of responses in places around the world, helping account for the simultaneous emergence and consolidation of popular claims to the “right to the city.\" According to David Harvey, “to invoke rights to the city means ‘to claim some kind of shaping power over the processes of urbanization, over the ways in which our cities are made and remade and to do so in a fundamental and radical way’.\"\n\nIdeas inspiring this movement first emerged in the 1970s, with many influenced by Henri Lefebvre’s 1968 book, \"Le Droit à la ville\". The movement has expanded and gained momentum around the world since the mid-1990s. The proliferation of global financial crises, urban austerity, and environmental damage has contributed to the rise of a growing number of cities around the world that are referring more explicitly to international human rights in their policies, statements, and programs.\n\nThe formally named “Human Rights Cities” initiative was launched by the People’s Movement for Human Rights Learning (PDHRE), which was formerly known as People’s Decade for Human Rights Education, in the wake of the 1993 World Conference on Human Rights in Vienna, Austria. The initiative aims to mobilize people in communities to “pursue a community-wide dialogue and to launch actions to improve the life and security of women, men and children based on human rights norms and standards.” This approach is different from the traditional way that human rights are enforced and applied because of its emphasis on popular education, engagement, and culture as a necessary complement to government enforcement.\n\nHuman Rights Cities have grown in part because of the enhanced efforts by international agencies like UN Habitat to connect international legal regimes with municipal programs. As a result of globalized economic development processes, cities around the world are facing a similar host of urban problems, including a lack of affordable housing, traffic congestion and insufficient public services. Cities have looked to international forums like the UN Conferences on Human Settlements and the World Associations of Cities and Local Authorities to help address these problems. Shulamith Koning, founder of the People’s Movement for Human Rights Learning (PDHRE), worked closely with human rights organizers in some of the first formally designated human rights cities, including Rosario Argentina, which became the first Human Rights City in the world in 1997 and the first U.S.-based Human Rights City of Washington, D.C.\n\n1998 would suppose a breakthrough for the human rights movement both in terms of institutional consolidation as well as global outreach. At the European level, the Barcelona Conference Cities for Human Rights gathered more than 400 hundred local authorities uniting their voice and calling for a stronger political acknowledgment as key actors in safeguarding human rights. The Conference process would culminate in 2 years after in the French city of Saint Denis by the adoption of the European Charter for the Safeguarding of Human Rights in the City (2000). By the same time, the movement of human rights cities was also taking ground in Asia, as regional civil society organizations were about to launch the Asia Human Rights Charter (Gwangju, 1998). Both charters would highlight the increasing role of local actors in promoting human rights as a way to reinforce local democracy and the place of human rights in the city in an increasingly urbanized world.\n\nAfter the establishment of Rosario as the first Human Rights City back in 1997, other local authorities in South America effectively embraced the human rights-based approach by placing a special emphasis on its link with the notion of right to the city. In 2001, the City Statue of Brazil offered a renewed framework for promoting human rights and the social function of the city at a national scale. Mexico City is also among the pioneers in the development of the human rights cities notion: in the last decade, it has promoted the Mexico City Charter for the Right to the City, created mechanisms to monitor human rights at the local level and adopted a new constitution specifically based on the human rights approach. The city of Bogotá was also at the forefront in the implementation of the human rights approach, with the implementation of the Bogotá Humana policy (2013-2016) which took a specific emphasis on the rights of homeless people, women and elderly population.\n\nIn North America, Montreal was a regional pioneer with the establishment of its local Montreal Charter of Rights and Responsibilities (2006). San Francisco has implemented policy translates and implements locally the rights of women as defined in the UN Convention on the Elimination of All Forms of Discrimination against Women since 1998. Human rights organizers in United States have nonetheless faced particular challenges due to the role of the U.S. in the world and to the U.S. failure to ratify most major international human rights treaties. However, in the 2000s, more U.S.-based activists have been working to raise international awareness of U.S. human rights violations, including racial discrimination in the criminal justice system, economic human rights violations, and the rights of children. In 2014 residents of Detroit who were losing access to clean water brought their case to the United Nations, which sent a Special Rapporteur to the city and issued a statement condemning practices that inhibited residents’ right to water. This issue and others have encouraged more U.S. cities, including Baltimore Maryland, Mountain View California, Columbia South Carolina, to consider the human rights city model.\n\nThroughout the last years, several examples around the world show a deepening on the concept and implications of human rights cities. In South Korea, Gwangju pioneered the establishment of a human rights municipal system (2009) that was quickly followed by cities such as Seoul (2012) and Busan. Gwangju has also been the main organizer of a World Human Rights Cities Forum that has gathered hundreds of human rights cities on a yearly basis. All around Europe, cities such as Barcelona, Madrid, Graz or Utrecht have established mechanisms to guarantee human rights and monitor their responsibilities under international human rights standards. Particular examples in this regard can be found in local government measures such as Barcelona’s “City of Rights” programme (2016) or Madrid’s “Strategic Plan for Human Rights Cities” (2017).\n\nAll international human rights law is based in the Universal Declaration of Human Rights, which was adopted in 1948. This document outlines the inalienable and fundamental rights of humankind that are protected regardless of gender, race, class, sexual orientation, religion, or any other social, economic, or political factor. The articles in the UDHR are not legally binding, but they are recognized as part of customary international law, and they authorize the development of binding international treaties, which countries may choose to sign and ratify. International human rights treaties and monitoring processes, however, privileges national governments and limits the role of local officials, whose cooperation in the implementation of international law is critical. The day-to-day work of implementing human rights standards often rests on the shoulders of local and regional authorities. They too are bound by these agreements. Local and regional authorities are often directly responsible for services related to health care, education, housing, water supply, environment, policing and also, in many cases, taxation.\n\nHow cities implement human rights ideals varies from city to city. This allows for each city to develop a plan that is specific to its capacities, needs, problems, and concerns. Formally designated “Human Rights Cities” typically create a leadership body made up of community activists, residents, and public officials (or their appointees) working in partnership. Other cities may adopt human rights language and standards without officially adopting the name of Human Rights City. For instance, Barcelona is a leading human right city in Europe, and it created an Office of Non-Discrimination to implement the EU anti-racial discrimination policy within its borders as part of becoming a Human Rights City.\n\nSan Francisco is another such example, since its 1998 adoption of a city ordinance reflecting the principles of the Convention for the Elimination of all forms of Discrimination Against Women. The San Francisco example has helped shape work by activists organizing a “Cities for CEDAW” campaign to convince cities around the United States to implement the CEDAW convention despite the failure of the national government to ratify the treaty.\n\nIn the last years, the advance of the human rights cities movement has been assessed by several resolutions and statements of international organizations such as the Human Rights Council or the Council of Europe. A breakthrough in this regard was the adoption by the Human Rights Council of the Advisory Committee Report A/HRC/30/49 on the “Role of local governments in the promotion and the protection of human rights”. The report assessed local governments responsibilities in the light of international human rights law, but most importantly, emphasized the opportunities posed by the human rights cities movement in promoting and protecting human rights due to their close relation with the needs and aspirations of city inhabitants. The Report promoted local governments participation in the drafting of national human rights strategies, and called to secure the necessary powers and financial resources for local administration to be able to comply with their human rights responsibilities. It enshrined as best practices the initiatives of various local governments and promoted networking as a way to advance the human rights cities movement, including some examples and best practices such as the World Human Rights Cities Forum of Gwangju or the Global Charter-Agenda for Human Rights in the City.\n\nThe 2015 Report has received an intense follow up by the Human Rights Council constituency and other international and local governments organizations working for the advance of the human rights cities movement. As the world representative of local and regional governments, the UCLG Committee on Social Inclusion, Participatory Democracy and Human Rights has for instance presented various statements to the Council and shared the Report’s recommendations among its constituency. In 2016, the Human Rights Council adopted a resolution (A/HRC/RES/33/8) “Recognizing the role of local government in the promotion and protection of human rights” and that “given its proximity to people and being at the grass-roots level, one of the important functions of local government is to provide public services that address local needs and priorities related to the realization of human rights at the local level”. On 2017, the same Council organized an intersessional panel discussion on the Report and the most recent advances in its implementation.\n\nWork by international human rights activists and by policymakers in the United Nations has helped spread ideas about how city governments can improve human rights implementation. In 2004, UNESCO helped establish the International Coalition of Cities against Racism to help municipal leaders exchange ideas and improve policies to fight racism, discrimination, xenophobia and exclusion. The European Coalition of Cities against Racism (ECCAR) grew out of that effort, and it now has more than 104 municipalities in its membership and has adopted a ten-point action plan.\n\nIn 2005, the UCLG Committee on Social Inclusion, Participatory Democracy and Human Rights was created in the framework of the largest organization and world representative of local governments: United Cities and Local Governments. The Committee represents and facilitates the exchange between worldwide local authorities having a strong agenda on human rights (Mexico City and Gwangju were for instance two of its co-chairs in 2018). As relevant outcomes of more than 10 years of work for advancing the notion and recognition of human rights cities, the Committee has been in charge of the follow-up of the European Charter for the Safeguarding of Human Rights in the City and has created and promoted the Global Charter Agenda for Human Rights in the City. It has also carried out a strong political advocacy at a UN level for the recognition of local governments as key actors in the promotion and protection of human rights, and co-organized yearly a World Human Rights Cities Forum.\n\nAnother relevant example in regards to concrete partnerships for advancing the human rights cities agenda are the so-called “action-research centers”. In this case, a local research center is generally at the forefront of mobilizing local authorities for the implementation of human rights at the local level. Relevant examples in this regard can be found at a European and North American levels. One of the most advanced examples on how can a local research center scale up and become a global actor in the promotion of the human rights agenda is the Raoul Wallenberg Institute. Although being affiliated to Lund University since 1984, the Institute's outreach has nonetheless gone beyond its local or even national level, as it now carries out projects in different regions of the world.\n\nThe following cities have been formally designated as Human Rights Cities:\n\n\n\n\n\n\n"}
{"id": "1164272", "url": "https://en.wikipedia.org/wiki?curid=1164272", "title": "Lambda 10 Project", "text": "Lambda 10 Project\n\nThe Lambda 10 Project is a national clearinghouse of information pertaining to LGBT issues in American fraternities and sororities. The organization works to heighten the visibility of out members (chiefly university students), and offers educational resources related to sexual orientation and the fraternity/sorority experience.\n\nThe Lambda 10 Project was founded in the autumn of 1995 at Indiana University in Bloomington, Indiana; it is currently headquartered in the University's Office of Student Ethics and Anti-harassment Programs.\n\nAmong the organization's offerings are an index of lecturers available for events; lists of forums concerning LGBT \"Greek life\"; and a repository of magazine articles, websites, and pertinent news stories. Working with the renowned LGBT publisher Alyson Publications, Lambda 10 also compiled and released three anthologies of homosexual and bisexual students' Greek experiences: \"Out on Fraternity Row: Personal Accounts of Being Gay in a College Fraternity\" (1998), \"Secret Sisters: Stories of Being Lesbian and Bisexual in a College Sorority\" (2001) and \"Brotherhood: Gay Life in College Fraternities\" (2005).\n\nLambda 10 Project is an educational initiative of Campus Pride, an American national nonprofit 501(c)(3) organization which serves lesbian, gay, bisexual and transgender (LGBT) and ally student leaders and/or campus organization. It is an associate member of the Association of Fraternity Advisors and serves as a resource for the leaders of numerous national and international fraternities and sororities.\n\nThe Project created an educational resource titled Out on Fraternity Row: Personal Accounts of Being Gay in a College Fraternity released by Alyson Publications, Inc in 1998, Secret Sisters: Stories of Being Lesbian & Bisexual in a College Sorority released by Alyson Publications, Inc. in 2001 and recently Brotherhood: Gay Life in College Fraternities released by Alyson Publications, Inc. in October 2005.\n\n\n\n"}
{"id": "46935853", "url": "https://en.wikipedia.org/wiki?curid=46935853", "title": "Liquid fly-back booster", "text": "Liquid fly-back booster\n\nLiquid Fly-back Booster (LFBB) was a German Aerospace Center's (DLR's) project concept to develop a reusable liquid rocket booster for Ariane 5 in order to significantly reduce the high cost of space transportation and increase environmental friendliness. LFBB would replace existing solid rocket boosters, providing main thrust during the liftoff. Once separated, two winged boosters would perform an atmospheric entry, fly back autonomously to the French Guiana, and land horizontally on the airport like an airplane.\n\nAdditionally a family of derivative launch vehicles was proposed in order to take an advantage of economies of scale, further reducing launch costs. These derivatives include a Reusable First Stage in a class of small and medium size launch vehicles like Vega and Arianespace Soyuz, the Super-Heavy Lift Launcher capable of lifting nearly 70 tonnes to the orbit, and a Two-Stage-To-Orbit system operating a dedicated reusable orbiter.\n\nGerman Aerospace Center studied Liquid Fly-back Boosters as a part of future launcher research program from 1999 to 2004. After the cancellation of the project, publications at DLR continued until 2009.\n\nThe German Aerospace Center (DLR) studied potential future launch vehicles of the European Union under the \"Ausgewählte Systeme und Technologien für Raumtransport\" (ASTRA; English: Systems and Technologies for Space Transportation Applications) programme from 1999 to 2005, with additional studies continuing until 2009. The LFBB design was one of two projects within the ASTRA program, the other being Phoenix RLV. During development, scale models were constructed for testing various configurations in DLR's supersonic \"Trisonische Messstrecke Köln\" (TMK; English: Trisonic measuring section at Cologne) and in their \"Hyperschallwindkanal 2 Köln\" (H2K; English: Hypersonic wind canal at Cologne) wind tunnels. The preliminary mechanical design of other major elements was done by the companies EADS Space Transportation and MAN.\n\nThe advantages of reusable boosters include simplicity from using only one type of fuel, environmental friendliness, and lower reoccurring costs. Studies concluded that reusable fly-back boosters would be the most affordable and the least risky way for European space launch systems to start becoming reusable. These fly-back boosters had the potential to reduce launch costs. However, when other projects, such as Space Shuttle or VentureStar, undertook this objective, they failed to meet their goals. Supporting technologies needed for LFBB construction can be developed within 10 years, and additional launchers can be developed based on fly-back boosters to minimise costs and provide maintenance synergy across multiple classes of launch vehicles. \n\nEventually, the hardware grew too large and the LFBB project was scrapped, with one member of the French space agency (CNES) remarking:\n\nThe overall concept of the liquid boosters in the LFBB programme was to retain the Ariane 5's core and upper stages, along with the payload fairings, and replace its solid rocket boosters (EAP P241, from French \"Étages d’Accélération à Poudre\") with reusable liquid rocket boosters. These boosters would provide the main thrust during take-off. After separation, they would return to a spaceport in French Guiana for landing. This vertical take-off, horizontal landing (VTHL) mode of operation would allow liquid fly-back boosters to continue operating from the Guiana Space Centre, thus avoiding any major changes to the ascend profile of Ariane 5. Launch vehicle payload performance of the Cryogenic Evolution type-A (ECA) variant would increase from to .\n\nIn the reference design, each LFBB consists of three engines installed in a circular arrangement at the aft of the vehicle. Each engine is a Vulcain engine with reduced expansion ratio. An additional three turbofan air-breathing engines, installed in the nose section, provide power for fly-back. The fuselage is long, with an outer tank diameter of , specifically designed to match the existing Ariane 5 core stage and to reduce manufacturing costs. A low-wing V-tail canard configuration was selected, with a wingspan of approximately and an area of . The aerofoil was based on a transonic profile from the Royal Aircraft Establishment (RAE 2822). The gross lift-off mass (GLOW) of each booster is , with upon separation and dry mass. In comparison, the GLOW for EAP P241 is .\n\nThe booster was designed to have four independent propulsion systems, the first of which – main rocket propulsion – would be based on three gimbaled Vulcain engines fueled by of propellant. Second, Eurojet EJ200 fly-back turbofan engines would be propelled with hydrogen to reduce fuel mass. Further, ten thrusters placed on each side of the vehicle would be used by the reaction control system. Finally, the fourth propulsion system would be based on solid rocket motors that separate the boosters from the core stage. An up-scaled version of the motors used in existing EAP boosters would be mounted in the attachment ring and inside the wing's main structure.\n\nA typical mission profile would begin with the ignition of a main stage and both boosters, followed by an acceleration to and then a separation at the altitude of . As the main stage continues its flight into orbit, the boosters follow a ballistic trajectory, reaching an altitude of . After low-energy atmospheric entry, the boosters reach denser layers of the atmosphere where they perform a banking turn toward the target airfield. Gliding continues until they achieve an altitude that is optimal for engaging turbofan engines and entering cruise flight. At this point, about from the launch point, the boosters would be flying over the Atlantic Ocean. The cruise back to the airport requires about of hydrogen fuel and takes over two hours to complete. An undercarriage is deployed and each booster lands autonomously. After separation, the boosters are not under threat of collision until they land due to small differences in their initial flight trajectories.\n\nThe development of liquid fly-back boosters has the potential to enable three additional space transportation systems with an objective of increasing production and creating economies of scale. The aim of the LFBB project at DLR was to reduce Ariane 5 operational costs and to develop future derivatives, including a reusable first stage of a small-to-medium launch vehicle, a super-heavy launch vehicle capable of lifting to Low Earth orbit, and a reusable two-stage-to-orbit launch vehicle. Initially, LFBBs would be used only on Ariane 5. Over time, alternative configurations could phase out Arianespace Soyuz and Vega.\n\nThe LFBB was studied with the three upper stage composites, to attain a Reusable First Stage (RFS) configuration. The first was a Vega derivative, with a Zefiro 23 second stage, a Zefiro 9 third stage and an AVUM upper stage. With the LFBB replacing the P80 stage, the payload to sun-synchronous orbit (SSO) would increase to , compared to the of the Vega. The second was an Ariane 4 derivative called H-25. It was based on an H10 upper stage with a Vinci rocket engine and of cryogenic fuel. Depending on the method of deceleration, the payload to SSO is between . The third was a large cryogenic upper stage, called H-185, based on an alternative, yet-to-be-developed Ariane 5 main stage with of cryogenic fuel. Its payload to SSO is .\n\nTwo of the lighter configurations (the Zefiro 23 and the H-25) use upper stages mounted on top of the booster. Due to the lower weight, it might have been necessary to lower the amount of fuel in a booster to ensure that the separation velocity, the flight path, and the reentry do not exceed design bounds. In the case of H-25, it might be necessary to accelerate the fly-back boosters to above to help the upper stage achieve its desired orbit. Consequently, two solutions were proposed to decelerate the boosters after separation. The first option was to actively decelerate them using of fuel and reduce the velocity by . However, launch performance would drop below that of the Vega derivative. Another option is to use aerodynamic forces to decelerate. However, a hypersonic parachute was deemed too expensive and too complex. As a result, an alternative ballute was proposed. Flight dynamics simulation revealed that a ballute with a cross-section of offered the best compromise between loads on the booster and deceleration by aerodynamic forces. In this configuration, a launch performance of up to could be achieved, partly thanks to a higher separation velocity.\n\nThe heaviest configuration uses a single booster with an asymmetrically mounted, large, expendable cryogenic stage designated H-185. It was proposed as a future variant of the Ariane 5 core stage (H158), eventually meant to phase out the main stage in a standard launch configuration with LFBB. H-185 would use a new Vulcain 3 main engine, with increased vacuum thrust. When launched with a single booster, both stages would be operated in parallel, and be delivered to a orbit before separation. The remaining upper stage composite would weigh , with a payload performance to SSO. When launching to Low Earth orbit, payload mass can be increased to over .\n\nThe Super-Heavy Lift Launcher (SHLL) would consist of a new cryogenic main stage, five liquid fly-back boosters, and a re-ignitable injection stage. This configuration was designed to provide increased capabilities for complex missions, including manned explorations to the Moon and to Mars, as well as the launch of large solar-powered satellites.\n\nThe new core stage would stand tall and have a diameter of , feeding of LOX/LH to three Vulcain 3 engines. The increased circumference of the main stage allows five LFBBs to be integrated with either retractable or variable-geometry wings. The upper stage would be a derivative of the Ariane 5 ESC-B, with the size upped to , and strengthened to bear higher loads. The Vinci engine was proofed to be sufficiently powerful for orbital insertion. Payload would be enclosed in an fairing. The launch vehicle would have a total height of and a mass of . The payload to LEO would be .\n\nWhen launched to a Low Earth transfer orbit, the LFBBs would separate at an altitude of , at a speed of . To avoid simultaneous separation of all boosters, either a cross-feed to the main stage, or throttling could be used. The return flight of the boosters would require an estimated of fuel, including a 30% reserve.\n\nA reusable Two-Stage-To-Orbit (TSTO) launch vehicle was planned to be implemented about 15 years after the addition of LFBBs to Ariane 5. However, only a preliminary analysis of TSTO was completed. The proposed configuration consisted of two boosters with retractable wings attached to the external fuel tank, and a reusable orbiter with fixed wings carrying payload on top of it. During geostationary transfer orbit (GTO) missions, an additional, expandable upper stage would be used.\n\nThe external tank, being a core of the system, would have a diameter of and a height of , carrying of propellant. The attached orbiter would be tall and in diameter, carrying of propellant. The payload fairing mount atop the orbiter would be . For LEO missions, the launch vehicle would be tall, with a gross lift-off mass of . The payload to LEO would be , with an increase to to GTO when using an expandable upper stage.\n\n"}
{"id": "17794738", "url": "https://en.wikipedia.org/wiki?curid=17794738", "title": "List of rampage killers", "text": "List of rampage killers\n\nA rampage killer has been defined as follows:\n\nThis list should contain, for each category, the first fifteen cases with at least one of the following features:\nThe separate articles for the different categories have more extensive lists.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (Africa and the Middle East)\n\nThis section contains cases that occurred in Africa and the Middle East. Not included are school massacres, workplace killings, hate crimes or familicides, which form their own categories.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (Americas)\n\nThis section contains cases that occurred in the Americas.\n\nNot included are school massacres, workplace killings, hate crimes or familicides, which form their own categories.\n\n\"Only the first 15 entries are shown here. For the entire list see\": List of rampage killers (Asia) and List of rampage killers (China)\n\nThis section contains cases that occurred in Asia. Not included are school massacres, workplace killings, hate crimes, or familicides, which form their own categories.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (Europe)\n\nThis section contains cases that occurred in Europe.\n\nNot included are school massacres, workplace killings, hate crimes or familicides, which form their own categories.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (Oceania and Maritime Southeast Asia)\n\nThis section contains cases that occurred in Oceania and the Maritime Southeast Asia.\n\nNot included are school massacres, workplace killings, hate crimes or familicides, which form their own categories.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (workplace killings)\n\nPeople killing their (former) co-workers; also includes soldiers killing their comrades.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (school massacres) \n\"See also\" List of school-related attacks\n\nMassacres at kindergartens, schools and universities\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (religious, political or racial crimes) <br>\nMass murders, committed by lone wolf perpetrators, that have a foremost religious, racial or political background.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (home intruders), List of familicides, List of familicides in the United States and List of familicides in Europe.\n\nThis section contains cases that could be considered non-public, which means mass murders perpetrated in a domestic environment. The section is divided into two sub-categories; the first encompasses the lists of familicides and contains those incidents where most of the victims were relatives of the perpetrator, while the second, paraphrased as home intruders, contains those cases where the targeted families were not related to the perpetrator.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (vehicular homicide) <br>\nThis section contains those cases where only vehicles were used to attack people. Since it may be quite difficult to distinguish accidents, or cases of reckless driving from those incidents where the driver, or pilot, had the intention to harm others, only those cases are included where it is clear that the vehicle was applied as a weapon and crashed deliberately into people, other vehicles, or buildings. Also, those cases where a rampage killer used an armed vehicle, such as a tank, or a fighter aircraft, to shoot others are listed here. Airliners are not included in this section but in other incidents.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (mass murders committed using grenades) <br>\nThis section lists mass murders where the perpetrator used only hand grenades or comparable explosive devices, like pipe bombs or dynamite sticks, for the attack. As it is sometimes difficult to distinguish cases of grenade attacks from acts of terrorism or gang-related attacks, incidents are only included where there is at least some indication that it was neither committed in the context of a political, ethnic, or religious conflict, nor part of an assault with more than one participating offender.\n\n\"Only the first 15 entries are shown here. For the entire list see:\" List of rampage killers (other incidents) <br>\nThis section lists mass murders by single perpetrators that do not fit into the upper categories, like arson fires, poisonings, and bombings.\n\nThe W-column gives a basic description of the weapons used in the murders\n\n\n\n"}
{"id": "58582887", "url": "https://en.wikipedia.org/wiki?curid=58582887", "title": "Locus suicide recombination", "text": "Locus suicide recombination\n\nLocus suicide recombination (LSR) constitutes a variant form of class switch recombination that eliminates all immunoglobulin heavy chain constant genes. It thus terminates immunoglobulin and B-cell receptor (BCR) expression in B-lymphocytes and results in B-cell death since survival of such cells requires BCR expression. This process is initiated by the enzyme activation-induced deaminase upon B-cell activation. LSR is thus one of the pathways that can result into activation-induced cell death in the B-cell lineage.\n"}
{"id": "20590", "url": "https://en.wikipedia.org/wiki?curid=20590", "title": "Mathematical model", "text": "Mathematical model\n\nA mathematical model is a description of a system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in the social sciences (such as economics, psychology, sociology, political science). \n\nA model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models. These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models. In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments. Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\n\nMathematical models are usually composed of relationships and \"variables\". Relationships can be described by \"operators\", such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\n\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. Newton's laws accurately describe many everyday phenomena, but at certain limits relativity theory and quantum mechanics must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the speed of light. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the de Broglie wavelength of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, Maxwell's equations and the Schrödinger equation. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\n\nSince prehistorical times simple models such as maps and diagrams have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\n\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\n\nDecision variables are sometimes known as independent variables. Exogenous variables are sometimes known as parameters or constants.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables. Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables. The objective functions will depend on the perspective of the model's user. Depending on the context, an objective function is also known as an \"index of performance\", as it is some measure of interest to the user. Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\n\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\nSometimes it is useful to incorporate subjective information into a mathematical model. This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads. After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use. Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam's razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification .\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton's classical mechanics is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\n\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called \"training\", while the optimization of model hyperparameters is called \"tuning\" and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \"curve fitting\".\n\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately. This question can be difficult to answer as it involves several different types of evaluation.\n\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data. In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters. An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as cross-validation in statistics.\n\nDefining a metric to measure distances between observed and predicted data is a useful tool of assessing model fit. In statistics, decision theory, and some economic models, a loss function plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model. In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations. Tools from non-parametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.\n\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward. If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light. Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\nMany types of modeling implicitly involve claims about causality. This is usually (but not always) true of models involving differential equations. As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\n\n\"M\" = (\"Q\", Σ, δ, \"q\", \"F\") where\n\nThe state \"S\" represents that there has been an even number of 0s in the input so far, while \"S\" signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \"M\" will finish in state \"S\", an accepting state, so the input string will be accepted.\n\nThe language recognized by \"M\" is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\n\n\nthat can be written also as:\n\n\n\n\n\n\n\n"}
{"id": "31638066", "url": "https://en.wikipedia.org/wiki?curid=31638066", "title": "Media controls", "text": "Media controls\n\nIn digital electronics, analogue electronics and entertainment, the user interface of media may include media controls or player controls, to enact and change or adjust the process of watching film or listening to audio. These widely known symbols can be found in a multitude of software products, exemplifying what is known as dominant design.\n\nThese are common icons on physical devices and application software. They are commonly found on portable media players, VCRs, DVD players, record players, remote controls, tape players and multimedia keyboards. Their application is described in ISO/IEC 18035.\n\nThe main symbols date back to the 1960s, with the Pause symbol having reportedly been invented at Ampex during that decade for use on reel-to-reel audio recorder controls, due to the difficulty of translating the word \"pause\" into some languages used in foreign markets. The Pause symbol was designed as a variation on the existing square Stop symbol and was intended to evoke the concept of an interruption or \"stutter stop\".\n\nIn popular culture, the play triangle is arguably the most widely used of the media control symbols. In many ways, the symbol has become synonymous with music culture and more broadly the digital download era. As such, there are now a multitude of items such as T-shirts, posters, and tattoos that feature this symbol. Similar cultural references can be observed with the Power symbol which is especially popular among video gamers and technology enthusiasts.\n\nMedia symbols can be found on an array of advertisements, from live music venues to streaming services. In 2012, Google rebranded its digital download store to Google Play, using a play symbol in its logo. The play symbol also serves as a logo for the Google-owned video sharing site YouTube since 2017.\n\nIn recent years, there has been a proliferation of devices that use media symbols in order to represent the Run/Stop/Pause functions. Likewise, user interface programing pertaining to these functions has also been influenced by that of media players. For example, some washers and dryers with a common illuminated play/pause button are programmed so that when the appliance is off, the play/pause light stays off. When the device is running the light stays on, and when the washer/dryer is in a paused state, the button flashes. This type of programing is similar to that of earlier CD players, which are also set to flash in this manner in the pause state.\n\nAside from appliances, there are many other instances when run/stop/pause functionality is needed and media symbols could theoretically be used instead of or in addition to words. In recent years, some exercise machine manufactures have chosen to do this. A notable difference in the use of these symbols on exercise equipment is that these machines usually contain an emergency stop button. Typically, this function is denoted with a red octagonal stop sign symbol, as opposed to a square, when the button may also be used for emergencies.\n\n"}
{"id": "11647860", "url": "https://en.wikipedia.org/wiki?curid=11647860", "title": "Minkowski diagram", "text": "Minkowski diagram\n\nThe Minkowski diagram, also known as a spacetime diagram, was developed in 1908 by Hermann Minkowski and provides an illustration of the properties of space and time in the special theory of relativity. It allows a qualitative understanding of the corresponding phenomena like time dilation and length contraction without mathematical equations.\n\nMinkowski diagrams are two-dimensional graphs that depict events as happening in a universe consisting of one space dimension and one time dimension. Unlike a regular distance-time graph, the distance will be displayed on the horizontal axis and time on the vertical axis. Additionally, the time and space units of measurement are chosen in such a way that an object moving at the speed of light is depicted as following a 45° angle to the diagram's axes.\n\nIn this way, each object, like an observer or a vehicle, traces a certain line in the diagram, which is called its world line. Also, each point in the diagram represents a certain position in space and time, and is called an event, regardless of whether anything relevant happens there.\n\nIn the study of 1-dimensional kinematics, position vs. time graphs (also called distance vs. time graphs, or p-t graphs) provide a useful means to describe motion. The specific features of the motion of objects are demonstrated by the shape and the slope of the lines. In the accompanying figure, the plotted object moves away from the origin at a uniform speed of 1.66 m/s for six seconds, halts for five seconds, then returns to the origin over a period of seven seconds at a non-constant speed.\n\nAt its most basic level, a spacetime diagram is merely a time vs position graph, with the directions of the axes in a usual p-t graph exchanged, that is, the vertical axis refers to temporal and the horizontal axis to spatial coordinate values. Especially when used in STR, the temporal axes of a spacetime diagram are scaled with the speed of light , and thus are often labeled by This changes the dimension of the addressed physical quantity from <\"Time\"> to <\"Length\">, in accordance to the dimension associated to the spatial axes, which are frequently labeled \n\nTo ease insight in how spacetime coordinates, measured by observers in different reference frames, compare with each other, it is useful to work with a simplified setup. With care, this allows simplification of the math with no loss of generality in the conclusions that are reached. Setting the temporal component aside for the moment, two Galilean reference frames (i.e. conventional 3-space frames), S and S' (pronounced \"S prime\"), each with observers O and O' at rest in their respective frames, but measuring the other as moving with speeds ±\"v\" are said to be in \"standard configuration\", when:\n\nThis spatial setting is displayed in the accompanying figure, in which the temporal coordinates are separately annotated as quantities \"t\" and \"t\"'.\n\nIn a further step of simplification it is often possible to consider just the direction of the observed motion and ignore the other two spatial components, allowing \"x\" and \"ct\" to be plotted in 2-dimensional spacetime diagrams, as introduced above.\n\nThe term Minkowski diagram refers to a specific form of spacetime diagram frequently used in special relativity. The term is used in both a generic and particular sense. In general, a Minkowski diagram is a two-dimensional graphical depiction of a portion of Minkowski space, usually where space has been curtailed to a single dimension. The units of measurement in these diagrams are taken such that the light cone at an event consists of the lines of slope plus or minus one through that event. The horizontal lines correspond to the usual notion of \"simultaneous events\" for a stationary observer at the origin.\n\nA particular Minkowski diagram illustrates the result of a Lorentz transformation. The Lorentz transformation relates two inertial frames of reference, where an observer stationary at the event makes a change of velocity along the -axis. The new time axis of the observer forms an angle with the previous time axis, with (see the first figure on the right). In the new frame of reference the simultaneous events lie parallel to a line inclined by to the previous lines of simultaneity. This is the new -axis. Both the original set of axes and the primed set of axes have the property that they are orthogonal with respect to the Minkowski inner product or \"relativistic dot product\".\n\nWhatever the magnitude of , the line forms the universal bisector.\n\nThe space and time units of measurement on the axes may, for example, be taken as one of the following pairs:\n\nThis way, light paths are represented by lines parallel to the bisector between the axes.\n\nThe black axes labelled and on the adjoining diagram are the coordinate system of an observer, referred to as 'at rest', and who is positioned at . This observer's world line is identical with the time axis. Each parallel line to this axis would correspond also to an object at rest but at another position. The blue line describes an object moving with constant speed to the right, such as a moving observer.\n\nThis blue line labelled may be interpreted as the time axis for the second observer. Together with the axis, which is identical for both observers, it represents their coordinate system. Since the reference frames are in standard configuration, both observers agree on the location of the origin of their coordinate systems. The axes for the moving observer are not perpendicular to each other and the scale on their time axis is stretched. To determine the coordinates of a certain event, two lines, each parallel to one of the two axes, must be constructed passing through the event, and their intersections with the axes read off.\n\nDetermining position and time of the event A as an example in the diagram leads to the same time for both observers, as expected. Only for the position different values result, because the moving observer has approached the position of the event A since . Generally stated, all events on a line parallel to the axis happen simultaneously for both observers. There is only one universal time , modelling the existence of one common position axis. On the other hand, due to two different time axes the observers usually measure different coordinates for the same event. This graphical translation from and to and and vice versa is described mathematically by the so-called Galilean transformation.\n\nAlbert Einstein (1905) discovered that the Newtonian description is wrong, with Hermann Minkowski in 1908 providing the graphical representation. Space and time have properties which lead to different rules for the translation of coordinates in case of moving observers. In particular, events which are estimated to happen simultaneously from the viewpoint of one observer, happen at different times for the other.\n\nIn the Minkowski diagram this relativity of simultaneity corresponds with the introduction of a separate formula_1 for the moving observer. Following the rule described above, each observer interprets all events on a line parallel to his formula_2 or formula_1 axis as simultaneous. The sequence of events from the viewpoint of an observer can be illustrated graphically by shifting this line in the diagram from bottom to top.\n\nIf instead of is assigned on the time axes, the angle between the and axes will be identical with that between the time axes and . This follows from the second postulate of special relativity, which says that the speed of light is the same for all observers, regardless of their relative motion (see below). The angle is given by\n\nThe corresponding translation from and to and and vice versa is described mathematically by the Lorentz transformation. Whatever space and time axes arise through such transformation, in a Minkowski diagram they correspond to conjugate diameters of a pair of hyperbolas. The scales on the axes are given as follows: If is the unit length on the axes of and respectively, the unit length on the axes of and is:\n\nThe -axis represents the worldline of a clock resting in , with representing the duration between two events happening on this worldline, also called the proper time between these events. Length upon the -axis represents the rest length or proper length of a rod resting in . The same interpretation can also be applied to distance upon the - and -axes for clocks and rods resting in .\n\nIn Minkowski’s 1908 paper there were three diagrams, first to illustrate the Lorentz transformation, then the partition of the plane by the light-cone, and finally illustration of worldlines. The first diagram used a branch of the unit hyperbola formula_6 to show the locus of a unit of proper time depending on velocity, thus illustrating time dilation. The second diagram showed the conjugate hyperbola to calibrate space, where a similar stretching leaves the impression of FitzGerald contraction. In 1914 Ludwik Silberstein included a diagram of \"Minkowski’s representation of the Lorentz transformation\". This diagram included the unit hyperbola, its conjugate, and a pair of conjugate diameters. Since the 1960s a version of this more complete configuration has been referred to as The Minkowski Diagram, and used as a standard illustration of the transformation geometry of special relativity. E. T. Whittaker has pointed out that the principle of relativity is tantamount to the arbitrariness of what hyperbola radius is selected for time in the Minkowski diagram. In 1912 Gilbert N. Lewis and Edwin B. Wilson applied the methods of synthetic geometry to develop the properties of the non-Euclidean plane that has Minkowski diagrams.\n\nWhile the rest frame has space and time axes at right angles, the moving frame has primed axes which form an acute angle. Since the frames are meant to be equivalent, the asymmetry may be disturbing. However, several authors showed that there is a frame of reference between the resting and moving ones where their symmetry would be apparent (\"median frame\"). In this frame, the two other frames are moving in opposite directions with equal speed. Using such coordinates makes the units of length and time the same for both axes. If and is given between S and S′, then these expressions are connected with the values in their median frame S as follows:\n\nFor instance, if between S and S′, then by (2) they are moving in their median frame S with approximately each in opposite directions. On the other hand, if in S, then by (1) the relative velocity between S and S′ in their own rest frames is . The construction of the axes of S and S′ is done in accordance with the ordinary method using with respect to the orthogonal axes of the median frame (Fig. 1).\n\nHowever, it turns out that, when drawing such a symmetric diagram, it is possible to derive the diagram's relations even without mentioning the median frame and at all. Instead, the relative velocity between S and S′ can directly be used in the following construction, providing the same result: If is the angle between the axes of and (or between and ), and between the axes of and , it is given:\n\nTwo methods of construction are obvious from Fig. 2: (a) The -axis is drawn perpendicular to the -axis, the and -axes are added at angle ; (b) the \"x\"′-axis is drawn at angle with respect to the -axis, the -axis is added perpendicular to the -axis and the -axis perpendicular to the -axis.\n\nAlso the components of a vector can be vividly demonstrated by such diagrams (Fig. 3): The parallel projections of vector are its contravariant components, its covariant components.\n\n\nRelativistic time dilation means that a clock (indicating its proper time) that moves relative to an observer is observed to run slower. In fact, time itself in the frame of the moving clock is observed to run slower. This can be read immediately from the adjoining Loedel diagram quite straightforwardly because unit lengths in the two system of axes are identical. Thus, in order to compare reading between the two systems, we can simply compare lengths as they appear on the page: we do not need to consider the fact that unit lengths on each axis are warped by the factor\nwhich we would have to account for in the corresponding Minkowski diagram.\n\nThe observer whose reference frame is given by the black axes is assumed to move from the origin O towards A. The moving clock has the reference frame given by the blue axes and moves from O to B. For the black observer, all events happening simultaneously with the event at A are located on a straight line parallel to its space axis. This line passes through A and B, so A and B are simultaneous from the reference frame of the observer with black axes. However, the clock that is moving relative to the black observer marks off time along the blue time axis. This is represented by the distance from O to B. Therefore, the observer at A with the black axes notices their clock as reading the distance from O to A while they observe the clock moving relative him or her to read the distance from O to B. Due to the distance from O to B being smaller than the distance from O to A, they conclude that the time passed on the clock moving relative to them is smaller than that passed on their own clock.\n\nA second observer, having moved together with the clock from O to B, will argue that the other clock has reached only C until this moment and therefore this clock runs slower. The reason for these apparently paradoxical statements is the different determination of the events happening synchronously at different locations. Due to the principle of relativity, the question of who is right has no answer and does not make sense.\n\nRelativistic length contraction means that the proper length of an object moving relative to an observer is decreased and finally also the space itself is contracted in this system. The observer is assumed again to move along the -axis. The world lines of the endpoints of an object moving relative to him are assumed to move along the -axis and the parallel line passing through A and B. For this observer the endpoints of the object at are O and A. For a second observer moving together with the object, so that for him the object is at rest, it has the proper length OB at . Due to . the object is contracted for the first observer.\n\nThe second observer will argue that the first observer has evaluated the endpoints of the object at O and A respectively and therefore at different times, leading to a wrong result due to his motion in the meantime. If the second observer investigates the length of another object with endpoints moving along the -axis and a parallel line passing through C and D he concludes the same way this object to be contracted from OD to OC. Each observer estimates objects moving with the other observer to be contracted. This apparently paradoxical situation is again a consequence of the relativity of simultaneity as demonstrated by the analysis via Minkowski diagram.\n\nFor all these considerations it was assumed, that both observers take into account the speed of light and their distance to all events they see in order to determine the actual times at which these events happen from their point of view.\n Another postulate of special relativity is the constancy of the speed of light. It says that any observer in an inertial reference frame measuring the vacuum speed of light relative to himself obtains the same value regardless of his own motion and that of the light source. This statement seems to be paradoxical, but it follows immediately from the differential equation yielding this, and the Minkowski diagram agrees. It explains also the result of the Michelson–Morley experiment which was considered to be a mystery before the theory of relativity was discovered, when photons were thought to be waves through an undetectable medium.\n\nFor world lines of photons passing the origin in different directions and holds. That means any position on such a world line corresponds with steps on - and -axes of equal absolute value. From the rule for reading off coordinates in coordinate system with tilted axes follows that the two world lines are the angle bisectors of the - and -axes. The Minkowski diagram shows, that they are angle bisectors of the - and -axes as well. That means both observers measure the same speed for both photons.\n\nFurther coordinate systems corresponding to observers with arbitrary velocities can be added to this Minkowski diagram. For all these systems both photon world lines represent the angle bisectors of the axes. The more the relative speed approaches the speed of light the more the axes approach the corresponding angle bisector. The formula_2 axis is always more flat and the time axis more steep than the photon world lines. The scales on both axes are always identical, but usually different from those of the other coordinate systems.\n\nStraight lines passing the origin which are steeper than both photon world lines correspond with objects moving more slowly than the speed of light. If this applies to an object, then it applies from the viewpoint of all observers, because the world lines of these photons are the angle bisectors for any inertial reference frame. Therefore, any point above the origin and between the world lines of both photons can be reached with a speed smaller than that of the light and can have a cause-and-effect relationship with the origin. This area is the absolute future, because any event there happens later compared to the event represented by the origin regardless of the observer, which is obvious graphically from the Minkowski diagram.\n\nFollowing the same argument the range below the origin and between the photon world lines is the absolute past relative to the origin. Any event there belongs definitely to the past and can be the cause of an effect at the origin.\n\nThe relationship between any such pairs of event is called \"timelike\", because they have a time distance greater than zero for all observers. A straight line connecting these two events is always the time axis of a possible observer for whom they happen at the same place. Two events which can be connected just with the speed of light are called \"lightlike\".\n\nIn principle a further dimension of space can be added to the Minkowski diagram leading to a three-dimensional representation. In this case the ranges of future and past become cones with apexes touching each other at the origin. They are called light cones.\n\nFollowing the same argument, all straight lines passing through the origin and which are more nearly horizontal than the photon world lines, would correspond to objects or signals moving faster than light regardless of the speed of the observer. Therefore, no event outside the light cones can be reached from the origin, even by a light-signal, nor by any object or signal moving with less than the speed of light. Such pairs of events are called \"spacelike\" because they have a finite spatial distance different from zero for all observers. On the other hand, a straight line connecting such events is always the space coordinate axis of a possible observer for whom they happen at the same time. By a slight variation of the velocity of this coordinate system in both directions it is always possible to find two inertial reference frames whose observers estimate the chronological order of these events to be different.\n\nTherefore, an object moving faster than light, say from O to A in the adjoining diagram, would imply that, for any observer watching the object moving from O to A, another observer can be found (moving at less than the speed of light with respect to the first) for whom the object moves from A to O. The question of which observer is right has no unique answer, and therefore makes no physical sense. Any such moving object or signal would violate the principle of causality.\n\nAlso, any general technical means of sending signals faster than light would permit information to be sent into the originator's own past. In the diagram, an observer at O in the system sends a message moving faster than light to A. At A, it is received by another observer, moving so as to be in the system, who sends it back, again faster than light, arriving at B. But B is in the past relative to O. The absurdity of this process becomes obvious when both observers subsequently confirm that they received no message at all, but all messages were directed towards the other observer as can be seen graphically in the Minkowski diagram. Furthermore, if it were possible to accelerate an observer to the speed of light, their space and time axes would coincide with their angle bisector. The coordinate system would collapse, in concordance with the fact that due to time dilation, time would effectively stop passing for them.\n\nThese considerations show that the speed of light as a limit is a consequence of the properties of spacetime, and not of the properties of objects such as technologically imperfect space ships. The prohibition of faster-than-light motion, therefore, has nothing in particular to do with electromagnetic waves or light, but comes as a consequence of the structure of spacetime.\nWhen Taylor and Wheeler composed \"Spacetime Physics\" (1966), they did \"not\" use the term \"Minkowski diagram\" for their spacetime geometry. Instead they included an acknowledgement of Minkowski’s contribution to philosophy by the totality of his innovation of 1908.\n\nWhen abstracted to a line drawing, then any figure showing conjugate hyperbolas, with a selection of conjugate diameters, falls into this category. Students making drawings to accompany the exercises in George Salmon’s \"A Treatise on Conic Sections\" (1900) at pages 165–171 (on conjugate diameters) will be making Minkowski diagrams.\n\nThe momentarily co-moving inertial frames along the world line of a rapidly accelerating observer (center). The vertical direction indicates time, while the horizontal indicates distance, the dashed line is the spacetime trajectory (\"world line\") of the observer. The small dots are specific events in spacetime. If one imagines these events to be the flashing of a light, then the events that pass the two diagonal lines in the bottom half of the image (the past light cone of the observer in the origin) are the events visible to the observer. The slope of the world line (deviation from being vertical) gives the relative velocity to the observer. Note how the momentarily co-moving inertial frame changes when the observer accelerates.\n\n\n"}
{"id": "4740018", "url": "https://en.wikipedia.org/wiki?curid=4740018", "title": "Normalization (people with disabilities)", "text": "Normalization (people with disabilities)\n\n\"The normalization principle means making available to all people with disabilities patterns of life and conditions of everyday living which are as close as possible to the regular circumstances and ways of life or society.\" Normalization is a rigorous theory of human services that can be applied to disability services. Normalization theory arose in the early 1970s, towards the end of the institutionalisation period in the US; it is one of the strongest and long lasting integration theories for people with severe disabilities.\n\nNormalization involves the acceptance of some people with disabilities, with their disabilities, offering them the same conditions as are offered to other citizens. It involves an awareness of the normal rhythm of life – including the normal rhythm of a day, a week, a year, and the life-cycle itself (e.g., celebration of holidays; workday and weekends). It involves the normal conditions of life – housing, schooling, employment, exercise, recreation and freedom of choice previously denied to individuals with severe, profound, or significant disabilities.\n\nWolfensberger's definition is based on a concept of cultural normativeness: \"Utilization of a means which are as culturally normative as possible, in order to establish and/or maintain personal behaviors and characteristics that are as culturally normative as possible.\" Thus, for example, \"medical procedures\" such as shock treatment or restraints, are not just punitive, but also not \"culturally normative\" in society. His principle is based upon social and physical integration, which later became popularized, implemented and studied in services as community integration encompassing areas from work to recreation and living arrangement.\n\nThis theory includes \"the dignity of risk\", rather than an emphasis on \"protection\" and is based upon the concept of integration in community life. The theory is one of the first to examine comprehensively both the individual and the service systems, similar to theories of human ecology which were competitive in the same period.\n\nThe theory undergirds the deinstitutionalization and community integration movements, and forms the legal basis for affirming rights to education, work, community living, medical care and citizenship. In addition, self-determination theory could not develop without this conceptual academic base to build upon and critique.\n\nThe theory of social role valorization is closely related to the principle of normalization having been developed with normalization as a foundation. This theory retains most aspects of normalization concentrating on socially valued roles and means, in socially valued contexts to achieve integration and other core quality of life values.\n\nThe principle of normalization was developed in Scandinavia during the sixties and articulated by Bengt Nirje of the Swedish Association for Retarded Children with the US human service system a product of Wolf Wolfensberger formulation of normalization and evaluations of the early 1970s. According to the history taught in the 1970s, although the \"exact origins are not clear\", the names Bank-Mikkelson (who moved the principle to Danish law), Grunewald, and Nirje from Scandinavia (later Ministry of Community and Social Services in Toronto, Canada) are associated with early work on this principle. Wolfensberger is credited with authoring the first textbook as a \"well-known scholar, leader, and scientist\" and Rutherford H. (Rud) Turnbull III reports that integration principles are incorporated in US laws.\n\nThe principle was developed and taught at the university level and in field education during the seventies, especially by Wolf Wolfensberger of the United States, one of the first clinical psychologists in the field of mental retardation, through the support of Canada and the National Institute on Mental Retardation (NIMR) and Syracuse University in New York State. PASS and PASSING marked the quantification of service evaluations based on normalization, and in 1991 a report was issued on the quality of institutional and community programs in the US and Canada based on a sample of 213 programs in the US, Canada and the United Kingdom.\n\nNormalization has had a significant effect on the way services for people with disabilities have been structured throughout the UK, Europe, especially Scandinavia, North America, Israel, Australasia (e.g., New Zealand) and increasingly, other parts of the world. It has led to a new conceptualisation of disability as not simply being a medical issue (the medical model which saw the person as indistinguishable from the disorder, though Wolfensberger continued to use the term into the 2000s, but as a social situation as described in social role valorization.\n\nGovernment reports began from the 1970s to reflect this changing view of disability (Wolfensberger uses the term devalued people), e.g. the NSW Anti-Discrimination Board report of 1981 made recommendations on \"the rights of people with intellectual handicaps to receive appropriate services, to assert their rights to independent living so far as this is possible, and to pursue the principle of normalization.\" The New York State Quality of Care Commission also recommended education based upon principles of normalization and social role valorization addressing \"deep-seated negative beliefs of and about people with disabilities\". Wolfensberger's work was part of a major systems reform in the US and Europe of how individuals with disabilities whould be served, resulting in the growth in community services in support of homes, families and community living.\n\nNormalization is often described in articles and education texts that reflect deinstitutionalization, family care or community living as the ideology of human services. Its roots are European-American, and as discussed in education fields in the 1990s, reflect a traditional gender relationship-position (Racino, 2000), among similar diversity critiques of the period (i.e., multiculturalism). Normalization has undergone extensive reviews and critiques, thus increasing its stature through the decades often equating it with school mainstreaming, life success and normalization,and deinstitutionalization.\n\nIn the United States, large public institutions housing adults with developmental disabilities began to be phased out as a primary means of delivering services in the early 1970s and the statistics have been documented until the present day (2015) by David Braddock and his colleagues. As early as the late 1960s, the normalization principle was described to change the pattern of residential services, as exposes occurred in the US and reform initiatives began in Europe. These proposed changes were described in the leading text by the President's Committee on Mental Retardation (PCMR) titled: \"Changing Patterns in Residential Services for the Mentally Retarded\" with leaders Burton Blatt, Wolf Wolfensberger, Bengt Nirje, Bank-Mikkelson, Jack Tizard, Seymour Sarason, Gunnar Dybwad, Karl Gruenwald, Robert Kugel, and lesser known colleagues Earl Butterfield, Robert E. Cooke, David Norris, H. Michael Klaber, and Lloyd Dunn.\n\nThe impetus for this mass deinstitutionalization was typically complaints of systematic abuse of the patients by staff and others responsible for the care and treatment of this traditionally vulnerable population with media and political exposes and hearings. These complaints, accompanied by judicial oversight and legislative reform, resulted in major changes in the education of personnel and the development of principles for conversion models from institutions to communities, known later as the community paradigms. In many states the recent process of deinstitutionalization has taken 10–15 years due to a lack of community supports in place to assist individuals in achieving the greatest degree of independence and community integration as possible. Yet, many early recommendations from 1969 still hold such as financial aid to keep children at home, establishment of foster care services, leisure and recreation, and opportunities for adults to leave home and attain employment (Bank-Mikkelsen, p. 234-236, in Kugel & Wolfensberger, 1969).\n\nA significant obstacle in developing community supports has been ignorance and resistance on the part of \"typically developed\" community members who have been taught by contemporary culture that \"those people\" are somehow fundamentally different and flawed and it is in everyone's best interest if they are removed from society (this developing out of 19th Century ideas about health, morality, and contagion). Part of the normalization process has been returning people to the community and supporting them in attaining as \"normal\" as life as possible, but another part has been broadening the category of \"normal\" (sometimes taught as \"regular\" in community integration, or below as \"typical\") to include all human beings. In part, the word \"normal\" continues to be used in contrast to \"abnormal\", a term also for differentness or out of the norm or accepted routine (e.g., middle class).\n\nIn 2015, public views and attitudes continue to be critical both because personnel are sought from the broader society for fields such as mental health and contemporary community services continue to include models such as the international \"emblem of the group home\" for individuals with significant disabilities moving to the community. Today, the US direct support workforce, associated with the University of Minnesota, School of Education, Institute on Community Integration can trace its roots to a normalization base which reflected their own education and training at the next generation levels.\n\nPeople with disabilities are not to be viewed as sick, ill, abnormal, subhuman, or unformed, but as people who require significant supports in certain (but not all) areas of their life from daily routines in the home to participation in local community life. With this comes an understanding that all people require supports at certain times or in certain areas of their life, but that most people acquire these supports informally or through socially acceptable avenues. The key issue of support typically comes down to productivity and self-sufficiency, two values that are central to society's definition of self-worth. If we as a society were able to broaden this concept of self-worth perhaps fewer people would be labeled as \"disabled.\"\n\nDuring the mid to late 20th century people with disabilities where met with fear, stigma, and pity. Their opportunities for a full productive life where minimal at best and often emphasis was placed more on personal characterizes that could be enhanced so the attention was taken from their disability Linkowski developed the Acceptance of Disability Scale (ADS) during this time to help measure a person’s struggle to accept disability. He developed the ADS to reflect the value change process associated with the acceptance of loss theory. In contrast to later trends, the current trend shows great improvement in the quality of life for those with disabilities. Sociopolitical definitions of disability, the independent living movement, improved media and social messages, observation and consideration of situational and environmental barriers, passage of the Americans with Disabilities Act of 1990 have all come together to help a person with disability define their acceptance of what living with a disability means.\n\nBogdan and Taylor's (1993) acceptance of sociology, which states that a person need not be defined by personal characterizes alone, has become influential in helping persons with disabilities to refuse to accept exclusion from mainstream society. According to some disability scholar’s, disabilities are created by oppressive relations with society, this has been called the social creationist view of disability. In this view, it is important to grasp the difference between physical impairment and disability. In the article The Mountain written by Eli Clare, Michael Oliver defines impairment as lacking part of or all of a limb, or having a defective limb, organism or mechanism of the body and the societal construct of disability; Oliver defines disability as the disadvantage or restriction of activity caused by a contemporary social organization which takes no or little account of people who have physical (and/or cognitive/developmental/mental) impairments and thus excludes them from the mainstream of society. In society, language helps to construct reality, for instance, societies way of defining disability which implies that a disabled person lacks a certain ability, or possibility, that could contribute to her personal well-being and enable her to be a contributing member of society versus abilities and possibilities that are considered to be good and useful . Society needs to destruct the language that is used and build a new one that does not place those with disabilities in the “other” category.\n\nHowever, the perspective of Wolfensberger, who served as associated faculty with the Rehabiltation Research and Training Center on Community Integration (despite concerns of federal funds), is that people he has known in institutions have \"suffered deep wounds\". This view, reflected in his early overheads of PASS ratings, is similar to other literature that has reflected the need for hope in situations where aspirations and expectations for quality of life had previously been very low (e.g., brain injury, independent living). Normalization advocates were among the first to develop models of residential services, and to support contemporary practices in recognizing families and supporting employment. Wolfensberger himself found the new term social role valorization to better convey his theories (and his German Professorial temperament, family life and beliefs)than the constant \"misunderstandings\" of the term normalization!\n\nRelated theories on integration in the subsequent decades have been termed community integration, self-determination or empowerment theory, support and empowerment paradigms, community building, functional-competency, family support, often not independent living (supportive living),and in 2015, the principle of inclusion which also has roots in service fields in the 1980s.\n\nNormalization is so common in the fields of disability, especially intellectual and developmental disabilities, that articles will critique normalization without ever referencing one of three international leaders: Wolfensberger, Nirje, and Bank Mikkelson or any of the women educators (e.g., Wolfensberger's Susan Thomas; Syracuse University colleagues Taylor, Biklen or Bogdan; established women academics (e.g., Sari Biklen); or emerging women academics, Traustadottir, Shoultz or Racino in national research and education centers (e.g., Hillyer, 1993). Thus it is important to discuss common misconceptions about the principle of normalization and its implications among the provider-academic sectors:\n\nWolfensberger himself, in 1980, suggested \"Normalizing measures can be offered in some circumstances, and imposed in others.\" This view is not accepted by most people in the field, including Nirje. Advocates emphasize that the \"environment\", not the \"person\", is what is normalized, or as known for decades a person-environment interaction.\n\nNormalization is very complex theoretically, and Wolf Wolfensberger's educators explain his positions such as the conservatism corollary, deviancy unmaking, the developmental model (see below) and social competency, and relevance of social imagery, among others.\n\nNormalization has been blamed for the closure of services (such as institutions) leading to a lack of support for children and adults with disabilities. Indeed, normalization personnel are often affiliated with human rights groups. Normalization is not deinstitutionalization, though institutions have been found to not \"pass\" in service evaluations and to be the subject of exposes. Normalization was described early as alternative special education by leaders of the deinstitutionalization movement.\n\nHowever support services which facilitate normal life opportunities for people with disabilities – such as special education services, housing support, employment support and advocacy – are not incompatible with normalization, although some particular services (such as special schools) may actually detract from rather than enhance normal living bearing in mind the concept of normal 'rhythms' of life.\n\nSome misconceptions and confusions about normalization are removed by understanding a context for this principle. There has been a general belief that 'special' people are best served if society keeps them apart, puts them together with 'their own kind, and keep them occupied. The principle of normalization is intended to refute this idea, rather than to deal with subtlety around the question of 'what is normal?' The principle of normalization is congruent in many of its features with \"community integration\" and as been described by educators as supporting early mainstreaming in community life.\n\nArguments about choice and individuality, in connection with normalization, should also take into account whether society, perhaps through paid support staff, has encouraged them into certain behaviours. For example, in referring to normalization, a discussion about an adult's choice to carry a doll with them must be influenced by a recognition that they have previously been encouraged in childish behaviours, and that society currently expects them to behave childishly. Most people who find normalization to be a useful principle would hope to find a middle way - in this case, an adult's interest in dolls being valued, but with them being actively encouraged to express it in an age-appropriate way (e.g., viewing museums and doll collections), with awareness of gender in toy selection (e.g., see cars and motorsports), and discouraged from behaving childishly and thus accorded the rights and routines only of a \"perpetual child\". However, the principle of normalization is intended also to refer to the means by which a person is supported, so that (in this example) any encouragement or discouragement offered in a patronising or directive manner is itself seen to be inappropriate.\n\nNormalization principles were designed to be measured and ranked on all aspects through the development of measures related to homes, facilities, programs, location (i.e. community development), service activities, and life routines, among others. These service evaluations have been used for training community services personnel, both in institutions and in the community.\n\nNormalization as the basis for education of community personnel in Great Britain is reflected in a 1990s reader, highlighting Wolf Wolfensberger's moral concerns as a Christian, right activist, side-by-side (\"How to Function with Personal Model Coherency in a Dysfunctional (Human Service) World\") with the common form of normalization training for evaluations of programs. Community educators and leaders in Great Britain and the US of different political persuasions include John O'Brien and Connie Lyle O'Brien, Paul Williams and Alan Tyne, Guy Caruso and Joe Osborn, Jim Mansell and Linda Ward, among many others.\n\n\n\n\n\n"}
{"id": "88369", "url": "https://en.wikipedia.org/wiki?curid=88369", "title": "Obfuscation", "text": "Obfuscation\n\nObfuscation is the obscuring of the intended meaning of communication by making the message difficult to understand, usually with confusing and ambiguous language. The obfuscation might be either unintentional or intentional (although intent usually is connoted), and is accomplished with circumlocution (talking around the subject), the use of jargon (technical language of a profession), and the use of an argot (ingroup language) of limited communicative value to outsiders.\n\nIn expository writing, unintentional obfuscation usually occurs in draft documents, at the beginning of composition; such obfuscation is illuminated with critical thinking and editorial revision, either by the writer or by an editor. Etymologically, the word \"obfuscation\" derives from the Latin \"obfuscatio\", from \"obfuscāre\" (to darken); synonyms include the words beclouding and abstrusity.\n\nDoctors are faulted for using jargon to conceal unpleasant facts from a patient; the American author and physician Michael Crichton said that medical writing is a \"highly skilled, calculated attempt to confuse the reader\". The psychologist B. F. Skinner said that medical notation is a form of multiple audience control, which allows the doctor to communicate to the pharmacist things which the patient might oppose if he or she could understand medical jargon.\n\n\"Eschew obfuscation\", also stated as \"eschew obfuscation, espouse elucidation\", is a humorous fumblerule used by English teachers and professors when lecturing about proper writing techniques. Literally, the phrase means \"avoid being unclear\" or \"avoid being unclear, support being clear\", but the use of relatively uncommon words causes confusion in much of the audience (those lacking the vocabulary), making the statement an example of irony, and more precisely a heterological phrase. The phrase has appeared in print at least as early as 1959, when it was used as a section heading in a NASA document.\n\nAn earlier similar phrase appears in Mark Twain's \"Fenimore Cooper's Literary Offenses\", where he lists rule fourteen of good writing as \"eschew surplusage\".\n\nIn white-box cryptography, obfuscation refers to the protection of cryptographic keys from extraction when they are under the control of the adversary, e.g., as part of a DRM scheme.\n\nIn network security, obfuscation refers to methods used to obscure an attack payload from inspection by network protection systems.\n"}
{"id": "17900572", "url": "https://en.wikipedia.org/wiki?curid=17900572", "title": "Overall equipment effectiveness", "text": "Overall equipment effectiveness\n\nOverall equipment effectiveness (OEE) is a term coined by Seiichi Nakajima in the 1960s to evaluate how effectively a manufacturing operation is utilized. It is based on the Harrington Emerson way of thinking regarding labor efficiency. The results are stated in a generic form which allows comparison between manufacturing units in differing industries. It is not however an absolute measure and is best used to identify scope for process performance improvement, and how to get the improvement. If for example the cycle time is reduced, the OEE will increase i.e. more product is produced for less resource. Another example is if one enterprise serves a high volume, low variety market, and another enterprise serves a low volume, high variety market. More changeovers (set-ups) will lower the OEE in comparison, but if the product is sold at a premium, there could be more margin with a lower OEE.\n\nOEE measurement is also commonly used as a key performance indicator (KPI) in conjunction with lean manufacturing efforts to provide an indicator of success. OEE can be illustrated by a brief discussion of the six metrics that comprise the system. The hierarchy consists of two top-level measures and four underlying measures.\n\nOverall equipment effectiveness (OEE) and total effective equipment performance (TEEP) are two closely related metrics that report the overall utilization of facilities, time and material for manufacturing operations. These top view metrics directly indicate the gap between actual and ideal performance.\n\n\nIn addition to the above measures, there are four underlying metrics that provide understanding as to why and where the OEE and TEEP gaps exist.\n\nThe measurements are described below\n\n\nWhat follows is a detailed presentation of each of the six OEE / TEEP Metrics and examples of how to perform calculations. The calculations are not particularly complicated, but care must be taken as to standards that are used as the basis. Additionally, these calculations are valid at the work center or part number level but become more complicated if rolling down to aggregate levels.\n\nOEE breaks the performance of a manufacturing unit into three separate but measurable components: Availability, Performance, and Quality. Each component points to an aspect of the process that can be targeted for improvement. OEE may be applied to any individual Work Center, or rolled up to Department or Plant levels. This tool also allows for drilling down for very specific analysis, such as a particular Part Number, Shift, or any of several other parameters.\nIt is unlikely that any manufacturing process can run at 100% OEE. Many manufacturers benchmark their industry to set a challenging target; 85% is not uncommon.\n\nAlternatively, and often easier, OEE is calculated by dividing the minimum time needed to produce the parts under optimal conditions by the actual time needed to produce the parts. For example:\n\nWhere OEE measures effectiveness based on scheduled hours, TEEP measures effectiveness against calendar hours, i.e.: 24 hours per day, 365 days per year.\n\nTEEP, therefore, reports the 'bottom line' utilization of assets.\n\nTEEP = Loading * OEE\n\nThe Loading portion of the TEEP Metric represents the percentage of time that an operation is scheduled to operate compared to the total Calendar Time that is available. The Loading Metric is a pure measurement of Schedule Effectiveness and is designed to exclude the effects how well that operation may perform.\n\nCalculation: Loading = Scheduled Time / Calendar Time\n\n\"Example:\"\n\nA given Work Center is scheduled to run 5 Days per Week, 24 Hours per Day.\n\nFor a given week, the Total Calendar Time is 7 Days at 24 Hours.\n\nLoading = (5 days x 24 hours) / (7 days x 24 hours) = 71.4%\n\nThe Availability portion of the OEE Metric represents the percentage of scheduled time that the operation is available to operate. The Availability Metric is a pure measurement of Uptime that is designed to exclude the effects of Quality and Performance. The losses due to wasted availability are called \"availability losses\".\n\n\"Example:\"\nA given Work Center is scheduled to run for an 8-hour (480 minute) shift with a 30-minute scheduled break and during the break the lines stop, and unscheduled downtime is 60 minutes.\n\nThe scheduled time = 480 minutes - 30 minutes = 450 minutes.\n\nOperating Time = 480 Minutes – 30 Minutes Scheduled Downtime – 60 Minutes Unscheduled Downtime = 390 Minutes\n\nCalculation: Availability = operating time / scheduled time\n\nAvailability = 390 minutes / 450 minutes = 86.6%\n\nAlso known as \"process rate\", the Performance portion of the OEE Metric represents the speed at which the Work Center runs as a percentage of its designed speed. The Performance Metric is a pure measurement of speed that is designed to exclude the effects of Quality and Availability. The losses due to wasted performance are also often called \"speed losses\". In practice it is often difficult to determine speed losses, and a common approach is to merely assign the remaining unknown losses as speed losses.\n\nCalculation: Performance (Productivity) = (Parts Produced * Ideal Cycle Time) / Operating time \n\n\"Example:\"\n\nA given Work Center is scheduled to run for an 8-hour (480 minute) shift with a 30-minute scheduled break.\n\nOperating Time = 450 Min Scheduled – 60 Min Unscheduled Downtime = 390 Minutes\n\nThe Standard Rate for the part being produced is 40 Units/Hour or 1.5 Minutes/Unit\n\nThe Work Center produces 242 Total Units during the shift. Note: The basis is Total Units, not Good Units. The Performance metric does not penalize for Quality.\n\nTime to Produce Parts = 242 Units * 1.5 Minutes/Unit = 363 Minutes\n\nPerformance (Productivity) = 363 Minutes / 390 Minutes = 93.1%\n\nThe Quality portion of the OEE Metric represents the Good Units produced as a percentage of the Total Units Started. The Quality Metric is a pure measurement of Process Yield that is designed to exclude the effects of Availability and Performance. The losses due to defects and rework are called \"quality losses\".\n\nCalculation: Quality = (Units produced - defective units) / (Units produced)\n\n\"Example:\"\n\n242 Units are produced. 21 are defective.\n\n(242 units produced - 21 defective units) = 221 units\n\n221 good units / 242 total units produced = 91.32%\n\nTo be able to better determine what is contributing to the greatest loss and so what areas should be targeted to improve the performance, these categories (Availability, Performance and Quality) have been subdivided further into what is known as the ‘Six Big Losses’ to OEE.\n\nThese are categorized as follows:\n\nThe reason for identifying the losses in these categories is so that specific countermeasures can be applied to reduce the loss and improve the overall OEE.\n\nOEE is useful as a heuristic, but can break down in several circumstances. For example, it may be far more costly to\nrun a facility at certain times. Performance and quality may not be independent of each other or of availability and loading.\nExperience may develop over time. Since the performance of shop floor managers is at least sometimes compared to the OEE, these numbers are often not reliable, and there are numerous ways to fudge these numbers.\n\nOEE has properties of a geometric mean. As such it punishes variability among its subcomponents. For example, 20% * 80% = 16%,\nwhereas 50% * 50% = 25%. When there are asymmetric costs associated with one or more of the components, then the model may become less appropriate.\n\nConsider a system where the cost of error is exceptionally high. In such a condition, higher quality may be far more important\nin a proper evaluation of effectiveness than performance or availability. OEE also to some extent assumes a closed system and a potentially static one. If one can bring in additional resources (or lease out unused resources to other projects or business units) then it may be more appropriate for example to use an expected net present value analysis.\n\nVariability in flow can also introduce important costs and risks that may merit further modeling. Sensitivity analysis and measures of change may be helpful.\n\n\n"}
{"id": "1330680", "url": "https://en.wikipedia.org/wiki?curid=1330680", "title": "Permission (philosophy)", "text": "Permission (philosophy)\n\nPermission, in philosophy, is the attribute of a person whose performance of a specific action, otherwise ethically wrong or dubious, would thereby involve no ethical fault. The term \"permission\" is more commonly used to refer to consent. Consent is the legal embodiment of the concept, in which approval is given to another party.\n\nPermissions depend on norms or institutions.\n\nMany permissions and obligations are complementary to each other, and deontic logic is a tool sometimes used in reasoning about such relationships.\n\n"}
{"id": "41121058", "url": "https://en.wikipedia.org/wiki?curid=41121058", "title": "Place attachment", "text": "Place attachment\n\nPlace attachment is the emotional bond between person and place, and is a main concept in environmental psychology. It is highly influenced by an individual and his or her personal experiences. There is a considerable amount of research dedicated to defining what makes a place \"meaningful\" enough for place attachment to occur. Schroeder (1991) notably discussed the difference between \"meaning\" and \"preference,\" defining meaning as \"the thoughts, feelings, memories and interpretations evoked by a landscape\" and preference as \"the degree of liking for one landscape compared to another.\"\n\nPlace attachment is multi-dimensional and cannot be explained simply through a cause and effect relationship. Instead, it depends on a reciprocal relationship between behavior and experiences. Due to numerous varying opinions on the definition and components of place attachment, organizational models have been scarce until recent years. A noteworthy conceptual framework is the Tripartite Model, developed by Scannell and Gifford (2010), which defines the variables of place attachment as the three P’s: Person, Process, and Place.\n\nLittle is known about the neurological changes that make place attachment possible because of the exaggerated focus on social aspects by environmental psychologists, the difficulties in measuring place attachment over time, and the heavy influence of individualistic experiences and emotions on the degree of attachment.\n\nThe Person dimension addresses the question of, \"who is attached?\"\n\nWhen examined individually, places often gain meaning because of personal experiences, life milestones, and occurrences of personal growth. With communities, however, places derive religious, historical, or other cultural meanings. Community behaviors contribute not only to place attachment experienced by citizens of that community as a group, but also to those citizens individually. For example, desires to preserve ecological or architectural characteristics of a place have a direct impact on the strength of place attachment felt by individuals, notably through self-pride and self-esteem. People experience stronger attachments to places that they can identify with or otherwise feel proud to be a part of.\n\nThe process dimension answers the question “How does the attachment exist?” Similar to other concepts in social psychology, this dimension relies on the collective effects of affective, cognitive, and behavioral aspects.\n\nThe most common emotions associated with people-place bonding are positive, such as happiness and love. Yi-Fu Tuan, a noteworthy human geographer and pioneer in place attachment research, coined the term topophilia to describe the love that people feel for particular places. Negative emotions and experiences are also capable of giving places significance; however, negative emotions are usually not associated with people-place bonding since place attachment represents individuals’ yearnings to replicate positive experiences and emotions.\n\nCognition incorporates the knowledge, memories, and meanings that individuals or groups have associated with places of attachment. Specifically, these cognitive elements represent what makes specific places important enough for people-place bonding to develop. Environmental psychologists additionally use the term schema to describe how people organize their beliefs and knowledge in regards to places and has led some researchers to note familiarity as a central cognitive element in place attachment. This idea of familiarity has been used in explaining why people mark themselves as “city people” or why they develop preferences for certain types of homes. Researchers have coined a number of terms based on familiarity, including “settlement identity” and “generic place dependence.”\n\nBehavior is the physical manifestation of place attachment and can represent the cognitive and affective elements that an individual possesses in their person-place bonds. Proximity-maintaining behaviors have been noted as common behaviors among people who have attachment of place, similar to those who have interpersonal attachments. Many individuals unknowingly experience the effects of place attachment through homesickness and will carry out proximity-maintaining behaviors to satisfy their desires to relieve it by returning home or reinventing their current environments to match the characteristics of home. This reinvention of current environments has been coined as reconstruction of place and is a notable place attachment behavior. Reconstruction of place often occurs when communities are rebuilding after natural disasters or war. As counterintuitive as it may seem, trips and even pilgrimages away from places can enhance a person-place bond because individuals grow an increased appreciation for the places they have left behind, contributing to feelings of nostalgia that often accompany attachment and the memories that places evoke.\n\nThe Place dimension addresses the question of, “what is attached?” and can be applied to any geographic type. Many researchers stress that place attachment relies on both physical and social aspects.\n\nThere is debate among environmental psychologists that place attachment occurs due to the social relationships that exist within the realm of an individual's significant place rather than the physical characteristics of the place itself. Hidalgo and Hernández (2001) studied levels of attachment based on different dimensions and found that while social aspects were stronger than physical ones, both affected the overall person-place bond.\n\nNatural and built environments can both be subjects of person-place bonds. The resources that these environments provide are the most tangible aspects that can induce attachment. These resources can lead to the development of place dependence. Place dependence negatively correlates with environmental press, which can be defined as the demands and stresses that an environment puts on people physically, interpersonally, or socially. Conversely, intangible aspects of environments can also promote attachment. In particular, the characteristics and symbolic representations that an individual associates with his or her perceptions of self are pivotal in the person-place bond.\n\nThere is very little research dedicated to the underlying developmental and neurological processes for place attachment, which is a major criticism of the field. Suggested developmental theories include the mere-exposure effect and the security-exploration cycle. Environmental psychologists have recognized parallels between the attachment theory and the development of place attachment, but the attachment theory at times fails to recognize place as a playing piece and instead classifies it as a background for attachment relationships.\n\nThe security-exploration cycle indicates that a place can become the target of attachment when it incorporates both security and exploration. For example, the home, a popular object of attachment, typically possesses a safe or familiar indoor environment and an outdoor space that satisfies desires to explore and expand knowledge. This example is capitalized upon by Morgan (2010), who proposed a combination of human attachment and place attachment in a model called the Exploration-Assertion Motivational System, which suggests that the strongest attachments originate during childhood. The model states that place attachment forms due to a cycle of repeated arousals and behaviors that are linked to both places and attachment figures. As a result of this balance between exploration and attachment behavior, children receive positive reinforcements in the forms of connectedness and a sense of adventure and mastery.\n\nDespite the absence of a well-established developmental theory and understanding of the neurological changes that accompany place attachment, most researchers agree that some form of place attachment occurs for each person at some point in his or her lifetime, with childhood homes being the most prevalent object of attachment.\n\nThere is a desire among researchers to create a list of concrete variables that account for differing extents of place attachment among individuals. It is believed that increased length of residence in a location increases the attachment a person has to that location. Place attachment tends to develop fast in early years of life and in a short time of residence, it helps that a person visits constantly the same place and no others, making, place attachment and place identity settle and grow. Over extended periods of time, place identity can develop. Place identity is defined as an individual's perception of self as a member of a particular environment. \nOther proposed positively correlated variables are ownership (i.e. of home, land) and social interactions. Some inversely related variables that have been suggested are building size and age.\n\nThis construct is usually mistaken with place attachment, even if they develop from different causes. Place attachment develops from positive experiences and the satisfactory relationship between a person and a place, while place identity comes from beliefs, meanings, emotions, ideas and attitudes assigned to a place. \n\nDefined as a functional relationship based on the conditions provided for a place in order to satisfy the needs of a person or accomplish a specific goal. Also depends on how is evaluated in other places, answering the question, \"What can I do in this place that I can't do in other places?\". Functional attachment depends on certain characteristics related to the place, as how much this area is compatible with a person, as they can carry out their activities and goals, this type of attachment is common in those who practice green exercise. This kind of attachment grows exponentially in relation to the number of visits. \n\nThe benefits related to place attachment include the ability to build memories as the person is capable to connect with his ancestors past, the feeling of belonging to a place, the personal growth as it provides the opportunity to experience positive emotions as a result from a healthy relationship with the place, so the person can feel security and freedom. \n\nPsychometric and Likert scales are the most commonly used quantitative methods for different dimensions of place attachment, such as belongingness and identity. Meanings of places are often quantitatively studied by asking participants to score a set list of places on the basis of 12 categories: aesthetic, heritage, family connection, recreation, therapeutic, biological diversity, wilderness, home, intrinsic, spiritual, economic, life-sustaining, learning, and future. Another example of quantitative measurements are frequency counts with word associations.\nQualitative research has been conducted with the intention to gain insight into meanings that places possess. Some of the techniques used for qualitative research are free association tasks, in-depth interviews, and verbal reports from focus groups.\n\nMemories combine sensations and perceptions to create images that can be used to retain and recall information or past experiences. Over time, memories collectively allow an individual to develop feelings of familiarity that comprise a sense of place. When an interactive experience with three-dimensional space does not match these developed expectations, an individual adapts his or her understanding of place through learning and sensory input. For this reason, places are frequently associated with memories that can evoke physical senses.\n\nThe plasticity of memories means that place identities are subject to change. Memories of places and individual preferences for specific places both change over time. Adults tend to focus on the emotions, meanings, and contextual implications of feelings in association with places. Children, however, focus on physical aspects of environments and what can be performed in various environments, which can be seen in the popularity of “pretend” and imagination games among children. Consequently, childhood memories of places are typically oriented around heavily emotional, intense, or euphoric events.\n\nA healthy emotional relationship between a city or neighborhood and its inhabitants maintains the culture and positive attitudes despite any detrimental events that may be occurring in that city or neighborhood (i.e. people having an increased sense of security even if they live in a war zone). When forced relocation occurs, refugees experience a grieving process similar to when loved ones are lost. Through reconstruction of place, the familiarity of an attachment to places lost can be mimicked to relieve stress and grief.\n\nAn understanding of the psychological factors responsible for place attachment is important for the effective development of place loyalty that allows cities and towns to flourish. Additionally, successful places address the needs or maintain the cultural integrities and meanings that communities have placed upon them. More specifically, additions of buildings or monuments and creation of outdoor recreational spaces must be well-aligned with a community’s place attachment to prevent backlash from inhabitants that do not agree with the intended land developments.\n\nEnvironmental press is often considered with elderly populations transitioning into assisted living or senior communities. Improving the overall community psychology and sense of community can allow place attachment to develop for both individuals and groups. Developing new place attachment is usually more difficult with increased age, and as a result, recently transitioned senior adults and senior communities are popular subjects for research in order to test the efficacy of various community-building techniques, such as celebrations and neighborhood political organizations.\n\n"}
{"id": "1509761", "url": "https://en.wikipedia.org/wiki?curid=1509761", "title": "Polytomous choice", "text": "Polytomous choice\n\nIn economics, polytomous choice is a setting (model) with more than \"two\" choices; contrast to dichotomous choice. The use of the term polychotomous is also in common usage in the prior research literature; however, polytomous is the more technically correct spelling.\n"}
{"id": "46285661", "url": "https://en.wikipedia.org/wiki?curid=46285661", "title": "Progress chart", "text": "Progress chart\n\nProgress charts are tools used in classrooms, in child care centers, and in homes across the world. They are used to promote good behaviors and reward children for those behaviors, which is why they are also known as behavior charts. They can be used in a variety of situations and they can come in a variety of styles. Progress charts are easy to use and promote positive reinforcement.\n\nA progress chart is a reward system. It involves stickers or stars, and a chart that can be either printed off or made by hand. The main goal of a progress chart is to track children's learning or behavior. It can be used to curb bad behaviors and to encourage good behaviors. It is inexpensive and can be changed to fit different situations. The child earns stickers for the desired behavior and after so many stars, earns a reward.\n\nThe Raising Children Network agrees that you can either make a chart or you can find them just about anywhere on the internet. Children all have different needs when learning, so these charts can help children of any age with anything from potty training to reading books. Progress charts can be used at home as well as in daycares and in schools. These charts can be individualized throughout classrooms, where children can learn to make their own specialized ones. There are several types of charts that parents, caregivers, and educators can use.\n\n\nSingle behavior charts can be used for all ages and is best used for learning one skill at a time. Multiple behavior charts also can be used for a variety of ages and can be used for processes that require several steps. Chore charts and behavior charts can each be used for several different situations. On the other hand, Homework charts and toilet training charts are used for what their titles suggest. Experts advise only using one chart at a time though, otherwise children can get confused and so can the adult. Charts like these can be found on several websites which have pages of downloadable charts that you can print off. They have a big variety of styles for their charts that can be used for different ages. All of these charts can be manipulated in order to fit each child's needs easily as well.\n\nThe benefits of progress charts include motivation for a certain task, and clear expectations for that task or skill. It provides a visual picture of goal setting and helps the child to achieve the goal and be able to receive a reward. It's a solid basis to a skill that children will have to know in the future—setting goals and achieving them. The charts give children immediate feedback and this usually invokes fewer consequences.\n\nRewards don't have to be very elaborate, but can be simple. It's best if rewards are given right away as stickers should be given right away, so that the child knows what the reward is for exactly. The goal of the reward is to keep the child continuing this behavior or skill. Dr. Virginia Shiller, a psychologist and instructor at the Yale Child Study Center and coauthor of the book Rewards for Kids, rewards can help parents teach their children new habits. Shiller says the key is in how the incentives are given; in setting appropriate, realistic goals; and in figuring out a strategy to achieve them.\n\n\n"}
{"id": "24387025", "url": "https://en.wikipedia.org/wiki?curid=24387025", "title": "Pullapart", "text": "Pullapart\n\nPullApart is a UK-based, independent packaging recycling classification system. Applied at the kerbside, it combines environmental and consumer packaging surveys to provide customers with a measurement of the ease with which specific types of packaging may be recycled locally. The process was invented by Michael Butler of Dawlish in 2005, and is operated for free.\n\nAs PullApart is applied to existing local authority-installed recycling bin refuse collection systems, its scoring scheme is dependent on individual local authorities’ own packaging disassembly practices. Sample packaging is disassembled, according to the Local Authority’s process, rearranged and its components graded for ease of recycling. The raw information from this exercise is also made available to the public.\n\nA final, consumer-oriented \"PAC\" (PullApart Code) score is achieved by measuring what proportion of a product's components is recyclable from the kerbside. The PAC score is represented by 13 stages of ‘traffic light’ grading.\n\nPullApart’s stated aims are to encourage, manufactures, retailers, food and agricultural producers to give greater weight to the ease of disposal and recycling in their packaging designs. Weighting the consumers point of view equally to that of packaging manufactures, retailers and recyclers, in the handling of domestic waste products for kerbside collections. To provide consumers with information enabling product choice (ethical consumerism), that's easy, local and totally kerbside recyclable. Furnishing an unambiguous tool, that measures the differences between those mentioned above, assisting in the optimisation of products for the goal of near Zero waste.\n\nAccording to PullApart’s current Teignbridge (2011) survey of over 2000 products, 2.84% are ideally suited for kerbside recycling and a further 29.32% are good, whilst the rest fail. The sample area, Teignbridge, and therefore Teignbridge District Council, has a current recycling rate of 57% (2008/2009), (by weight). Quoting from their periodical, “Teignbridge Life” explaining to local people how PullApart works: “The online packaging recycling guide features a free search function which classifies ordinary consumer products, like cereal boxes, with a 'PullApart' rating. The rating breaks the product down into its components, explaining which parts can be recycled in Teignbridge.”\n\nPullApart is considered to be of “Environmental Best Practice” by The Green Organisation.\n\nWorldwide there are broader packaging scoring systems that address the full environmental impact of packaging. Recycling being one factor, other vital considerations include the use of recycled materials in the package, avoidance of toxic substances, minimization of packaging, effects on atmosphere (greenhouse gas, VOC, etc.), use of renewable resources, etc. Efforts involve methodologies such as life cycle assessment to inventory the all environmental impacts and factors.\nThere are many Sustainability metrics and indices, some specifically for packaging.\n\n\n"}
{"id": "6394087", "url": "https://en.wikipedia.org/wiki?curid=6394087", "title": "Revelation principle", "text": "Revelation principle\n\nThe revelation principle is a fundamental principle in mechanism design. It states that if a social choice function can be implemented by an arbitrary mechanism (i.e. if that mechanism has an equilibrium outcome that corresponds to the outcome of the social choice function), then the same function can be implemented by an incentive-compatible-direct-mechanism (i.e. in which players truthfully report type) with the same equilibrium outcome (payoffs).\n\nIn mechanism design, the revelation principle is of utmost importance in finding solutions. The researcher need only look at the set of equilibrium characterized by incentive compatibility. That is, if the mechanism designer wants to implement some outcome or property, he can restrict his search to mechanisms in which agents are willing to reveal their private information to the mechanism designer that has that outcome or property. If no such direct and truthful mechanism exists, no mechanism can implement this outcome/property. By narrowing the area needed to be searched, the problem of finding a mechanism becomes much easier.\n\nThe principle comes in two variants corresponding to the two flavors of incentive-compatibility:\n\nConsider the following example. There is a certain item that Alice values as formula_1 and Bob values as formula_2. The government needs to decide who will receive that item and in what terms. \n\nSuppose we have an arbitrary mechanism Mech that implements Soc.\n\nWe construct a direct mechanism Mech' that is truthful and implements Soc.\n\nMech' simply simulates the equilibrium strategies of the players in Game(Mech). I.e:\n\nReporting the true valuations in Mech' is like playing the equilibrium strategies in Mech. Hence, reporting the true valuations is a Nash equilibrium in Mech', as desired. Moreover, the equilibrium payoffs are the same, as desired.\n\nThe revelation principle says that for every arbitrary \"coordinating device\" a.k.a. correlating there exists another direct device for which the state space equals the action space of each player. Then the coordination is done by directly informing each player of his action.\n\n"}
{"id": "48136123", "url": "https://en.wikipedia.org/wiki?curid=48136123", "title": "Romina Ressia", "text": "Romina Ressia\n\nRomina Ressia (born March 11, 1981) is a multi-award winning photographer and artist from Buenos Aires, Argentina. She is well known for her anachronisms and her strong Renaissance influence. Her works have been featured by \"The Huffington Post\", \"Interview\", \"Vanity Fair\", \"Vogue Italia\" and \"The Wild Magazine\", among others. \"The Wall Street International\" wrote, \"Romina's photography provokes and challenges us to confront things which may seem uncomfortable however manages to provide a reassuring quality. This juxtaposition between comfort and uncomfortable is a powerful one which resonates throughout her practice.\"\n\nRessia was born on March 11, 1981, in Azul, Buenos Aires, Argentina. At the age of 19 she moved to Capital Federal to study, where she graduated as an accountant with a Bachelor of Business Administration at the University of Buenos Aires. She later abandoned those professions to devote herself to photography. She studied photography, art direction and scenery at several institutions, including the Teatro Colon.\n\nShe started in fashion photography but was gradually moving to fine arts, venturing beyond photography into mixed media. Her works are represented by galleries in the UK, New York, Switzerland and Italy, and they have also been exhibited in cities such as Milan, Zurich, Paris and Buenos Aires.\n\n\nThe artist is characterized by the use of anachronisms and juxtapositions that allow to draw a timeline from which to explore human evolution and their behavior as individuals and as a collective.\n\n\"Sus imágenes se parecen más a un cuadro del Renacimiento que a una foto moderna y ahí radica su belleza. Romina usa con ironía la confrontación pasado/presente\" \n\n\n\n \n"}
{"id": "55730249", "url": "https://en.wikipedia.org/wiki?curid=55730249", "title": "Sarina Prabasi", "text": "Sarina Prabasi\n\nSarina Prabasi (born 1973/1974) is a Nepalese charity executive specialising in international development. She has been the Chief Executive Officer (CEO) of WaterAid America since May 2014. She was previously Deputy Chief of Programs at Orbis International, and had been a Country Representative at WaterAid Ethiopia.\n\nIn addition to her charity work, Prabasi co-founded Buunni Coffee with her husband in 2011. This is an independent company that sells its own roasted Ethiopian coffee; the coffee is organic and fair trade. They also have their own coffee shop called Café Buunni which is located in Upper Manhattan, New York.\n\nPrabasi was born in The Hague, Netherlands, and was brought up in Nepal. She moved to the United States to attend university. She studied at Smith College, and graduated with a Bachelor of Arts (BA) degree having majored in economics. She later attended the School of Oriental and African Studies, University of London, graduating with a Master of Science (MSc) degree in development studies.\n\nPrabasi is married to Elias, an Ethiopian. Together they have two daughters.\n\nIn 2015, Prabasi was named was of the 'Most Innovative Women in Food and Drink' by \"Food & Wine\". In 2016, she was named a 'Woman of Influence' by the \"New York Business Journal\".\n"}
{"id": "12155645", "url": "https://en.wikipedia.org/wiki?curid=12155645", "title": "Separation of mechanism and policy", "text": "Separation of mechanism and policy\n\nThe separation of mechanism and policy is a design principle in computer science. It states that mechanisms (those parts of a system implementation that control the authorization of operations and the allocation of resources) should not dictate (or overly restrict) the policies according to which decisions are made about which operations to authorize, and which resources to allocate.\n\nThis is most commonly discussed in the context of security mechanisms (authentication and authorization), but is actually applicable to a much wider range of resource allocation\nproblems (e.g. CPU scheduling, memory allocation, quality of service), and the general \nquestion of good object abstraction.\n\nPer Brinch Hansen introduced the concept of separation of policy and mechanism in operating systems in the RC 4000 multiprogramming system. Artsy and Livny, in a 1987 paper, discussed an approach for an operating system design having an \"extreme separation of mechanism and policy\". In a 2000 article, Chervenak et al. described the principles of \"mechanism neutrality\" and \"policy neutrality\".\n\nThe separation of mechanism and policy is the fundamental approach of a microkernel that distinguishes it from a monolithic one. In a microkernel the majority of operating system services are provided by user-level server processes. It is important for an operating system to have the flexibility of providing adequate mechanisms to support the broadest possible spectrum of real-world security policies.\n\nIt is almost impossible to envision all of the different ways in which a system might be used by different types of users over the life of the product. This means that any hard-coded policies are likely to be inadequate or inappropriate for some (or perhaps even most) potential users.\nDecoupling the mechanism implementations from the policy specifications makes it possible for different applications to use the same mechanism implementations with different policies. This means that those mechanisms are likely to better meet the needs of a wider range of users, for a longer period of time.\n\nIf it is possible to enable new policies without changing the implementing mechanisms, the costs and risks of such policy changes can be greatly reduced. In the first instance, this could be accomplished merely by segregating mechanisms and their policies into distinct modules: by replacing the module which dictates a policy (e.g. CPU scheduling policy) without changing the module which executes this policy (e.g. the scheduling mechanism), we can change the behaviour of the system. Further, in cases where a wide or variable range of policies are anticipated depending on applications' needs, it makes sense to create some non-code means for specifying policies, i.e. policies are not hardcoded into executable code but can be specified as an independent description. For instance, file protection policies (e.g. Unix's \"user/group/other read/write/execute\") might be parametrized. Alternatively an implementing mechanism could be designed to include an interpreter for a new policy specification language. In both cases, the systems are usually accompanied by a mechanism (e.g. configuration files, or APIs) that permits policy specifications to be incorporated to the system or replaced by another after it has been delivered to the customer.\n\nAn everyday example of mechanism/policy separation is the use of card keys to gain access to locked doors. The mechanisms (magnetic card readers, remote controlled locks, connections to a security server) do not impose any limitations on entrance policy (which people should be allowed to enter which doors, at which times). These decisions are made by a centralized security server, which (in turn) probably makes its decisions by consulting a database of room access rules. Specific authorization decisions can be changed by updating a room access database. If the rule schema of that database proved too limiting, the entire security server could be replaced while leaving the fundamental mechanisms (readers, locks, and connections) unchanged.\n\nContrast this with issuing physical keys: if you want to change who can open a door, you have to issue new keys and change the lock. This intertwines the unlocking mechanisms with the access policies. For a hotel, this is significantly less effective than using key cards.\n\n\n\n"}
{"id": "10480026", "url": "https://en.wikipedia.org/wiki?curid=10480026", "title": "Shock collar", "text": "Shock collar\n\nThe term shock collar is a term used in order to describe a family of training collars (also called e-collars, Ecollars, remote training collars, Zap collars, or electronic collars) that deliver electrical shocks of varying intensity and duration to the neck of a dog (they can also be applied to other places on the dog's body, to achieve various training effects) via a radio-controlled electronic device incorporated into a dog collar. Some collar models also include a tone or vibrational setting, as an alternative to or in conjunction with the shock. Others include integration with Internet mapping capabilities and GPS to locate the dog or alert an owner of his/her whereabouts.\n\nOriginally used in the late 1960s to train hunting dogs, early collars were very high powered. Many modern versions are capable of delivering very low levels of shock. Shock collars are now readily available and have been used in a range of applications, including behavioral modification, obedience training, and pet containment, as well as military, police and service training. While similar systems are available for other animals, the most common are the collars designed for domestic dogs.\n\nThe most common use of shock collars is pet containment systems that are used to keep a dog inside the perimeter of the residence without the construction of a physical barrier. This use of shock collars is increasingly popular in areas where local laws or homeowners' associations prohibit the construction of a physical fence. Available systems include: in-ground installation to preserve the aesthetics of the yard; above ground installation to reinforce an existing barrier that was not sufficient in containing the dog; and wireless systems to allow for indoor use. Most pet containment systems work by installing a wire around the perimeter of the yard. The wire carries no current (as opposed to electric fences which do carry a current at high voltage that may be lethal in the event of unauthorized or defective installation or equipment) but forms a closed loop with a circuit box that transmits a radio signal to the receiver collar worn by the dog. As the dog approaches the perimeter the collar will activate.\n\nBark control collars are used to curb excessive or nuisance barking by delivering a shock at the moment the dog begins barking. Bark collars can be activated by microphone or vibration, and some of the most advanced collars use both sound and vibration to eliminate the possibility of extraneous noises activating a response.\n\nTraining collars can be activated by a handheld device. Better quality remote trainers have a large variety of levels and functions, can give varying duration of stimulation, better quality stimulation, and have a beep or vibration option useful for getting the dog’s attention. Proper training is an imperative for remote collar use, as misuse can cause negative behavioral fallout. Many recommend consulting a behaviorist or a certified training professional who is experienced with shock collars for successful usage and application.\n\nShock collars may be used in conjunction with positive reinforcement and / or utilizing other principles of operant conditioning, depending on the trainer's methods either as a form of positive punishment, where the shock is applied at the moment an undesired behavior occurs, in order to reduce the frequency of that behavior; or as a form of negative reinforcement, where a continuous stimulation is applied until the moment a desired behavior occurs, in order to increase the frequency of that behavior.\n\nElectrical shock is the physiological reaction, sensation, or injury caused by electric current passing through the body. It occurs upon contact of a body part with any source of electricity that causes a sufficient current through the skin, muscles, or hair.\n\nCommenting in his textbook on training and behaviour, Steven Lindsay wrote about the public perception of the term \"shock\" and its application in the description of training aids; \"At low levels, the term shock is hardly fitting to describe the effects produced by electronic training collars, since there is virtually no effect beyond a pulsing tingling or tickling sensation on the surface of the skin ... the word shock is loaded with biased connotations, images of convulsive spasms and burns, and implications associated with extreme physical pain, emotional trauma, physiological collapse, and laboratory abuses ... the stimulus or signal generated by most modern devices is highly controlled and presented to produce a specific set of behavioral and motivational responses to it.\" \n\nIn an article for the trade magazine \"Office for veterinary service and food control\", Dr. Dieter Klein compared the effects of shock collars with other electrical stimulation products; \"Modern devices ... are in a range in which normally no organic damage is being inflicted. The electric properties and performances of the modern low current remote stimulation devices ... are comparable to the electric stimulation devices used in human medicine. Organic damage, as a direct impact of the applied current, can be excluded.” \n\n\"At 0.914 joules the electric muscle stimulation and contractions a human receives from an 'abdominal energizer' fitness product is exponentially stronger — more than 1,724 times stronger— than the impulse a dog receives from a pet containment collar set at its highest level.\"\n\nElectric shock can be characterised in terms of voltage, current, waveform, frequency (of waveform), pulse rate and duration. \nAlthough voltage, current and duration of shock can be used to calculate the amount of energy applied (in Joules), these are not indicators of the intensity of the stimulus or how it may be perceived by the recipient. Static electric shocks that are experienced in daily life are of the order of 20,000 to 25,000 volts, and yet are not painful or physically damaging because they are of very low current.\n\nDepending on design, shock collars can be set so that the shock delivered is only mildly uncomfortable. Variable settings of this kind are essential, so that the shock collar can be adjusted to the level that the dog requires, as situations change.\n\nShock collars are sometimes referred to as delivering a \"static shock\"; however, static electricity is direct current and carries little energy (order of millijoules). Shock collars make use of alternating current. It is therefore inappropriate to refer to shock collars as delivering a static shock.\n\nIn order to deliver consistent shocks, good contact must be made between the collar electrodes and the dog's skin (the collar must be fitted according to the manufacturer's instructions). Local humidity and individual variation in coat density, skin thickness and surface conductivity, will also affect the delivery of the shock.\n\nThe waveform, its frequency, the pulse rate, current, voltage and impedance are important determinants of likely response. \"Many e−collars appear to shift intensity levels by altering the pulse duration or repetition rate while keeping the output current and voltage relatively constant, depending on the electrode−skin load.\" (Lindsay 2005, p. 573).\n\nIndividual variations in temperament, pain sensitivity and susceptibility to startle of dogs, means that shock settings must be carefully adjusted to produce a shock that is perceived by the dog as only just aversive enough to stop the dog engaging in the unwanted behaviour. Normally salient stimuli, such as noises, commands and even shocks, may have no effect on a dog that is highly aroused and focused on an activity such as hunting.\n\nIn 1980 (revised 1987), the US Center for Veterinary Medicine (CVM), a branch of the U.S. Food and Drug Administration (FDA), concurred in regulatory action against a manufacturer of a bark collar, stating \"Complaints received, which were later corroborated by our own testing, included severe burns in the collar area and possible personality adjustment injuries to the dogs. The shocking mechanism was found to be activated not only by barking but by vehicle horns, slamming doors or any other loud noise. CVM concurred in regulatory action against the device since it was deemed to be dangerous to the health of the animal.\". The standing policy of the US FDA is that \"Dog collars which are activated by the noise of barking to produce an electric shock are considered as hazardous to the health of the animal.\"\n\nNo regulations exist specifying the performance characteristics or reliability of these devices, so there is considerable variation in shock level and waveform characteristics between manufacturers, and perhaps even between batches of collars from a single manufacturer. The lack of regulation or standards, and the fact that some of the safety features of shock collars are patented by specific manufacturers, means that the safety and operational characteristics of individual products cannot be verified.\n\nThe debate over the effectiveness of shock collars is highly debated, however it is not one sided. Each side has a wide variety of backers and activists from professors, to activists, and trainers. \n\nAccording to Pat Nolan, who has been training dogs for over thirty years, shock training is a key and effective use to train dogs (Nolan, 2011). According to his methods described in his book regular and fair use is key in shock training. He goes into detail about what fair use is, stating that keeping a regular training schedule is key and to set some boundaries your pet will understand (Nolan, 2011).\n\nThe Wildlife Society article addresses the use of shock collars as a way to prevent sheep from being preyed upon by wild coyotes. According to (Phillips, 1999) they tested these collars on coyotes for a four-month period and found that the collars stopped thirteen attacks on sheep herds. This also is said to deter future attacks by the tested coyotes. Collars have also been used on wolves for similar reasons. This document is the assessment of the shock collar on wolves’ long-term behavior. The article talks about trying to alter wolves’ behavior over an extended period of time using the collar. The consensus was while it did have an effect while in use and temporally after it was removed, the study concluded that longer exposure would be needed in order to have any substantial evidence (Hawley, 2008). As far as non-lethal alternatives these two sources both concluded that shock collars are the most effective deterrence to predators. Both groups continued their research and the Wildlife Society has developed a new and improved version that eliminates the risk of neck injury when used on animals that previous versions caused. They have increased battery life and the durability of the unit. They devised a unit that is worn like a back pack for the animal. Previous versions caused excessive rubbing and soreness as well as being irritating for the animal to the point were they would try to take the harness off.\n\nChristiansen et al., looked at behavioural differences between three breeds of dogs when confronted by domestic sheep (138 dogs; Elkhounds, hare hunting dogs and English setters). Two testing procedures were used and shock collars were used to deter attacks on sheep. The first, a path test, involved observing the dogs' reactions to a set of novel stimuli (rag pulled across the track, bundle of cans thrown down, tethered sheep at 5m) as it was walked. The second test involved monitoring the dog's reaction to a free-roaming sheep flock in a field. In this study they identified several factors that predicted a high hunting motivation and attack severity. These were lack of previous opportunity to chase sheep, low fearfulness towards gunshots and unfamiliar people and general interest in sheep when encountering them. Younger dogs (<3 years of age) showed more pronounced initial hunting motivation and more frequent attacks. Elkhounds showed more hunting behaviour, more attacks and were more frequently given electric shocks during the tests. A shock collar was used to deter attacks on the sheep during the experiments. Shocks (3000V, 0.4A, duration 1 second) were delivered when dogs came within a distance of 1-2m of the sheep, and were repeated until the dogs left the area. The objective was to suppress an attack, but not to damage the hunting ability of the dogs. Despite frequently initiated chases and attacks, few shocks were delivered. This was because few dogs approached closer than 1–2 m, and the intention was to deter proximity to sheep rather than to associate hunting behaviour with an aversive shock, which would impair future hunting behaviour in other contexts.\n\nThe dogs used in the first study were re-tested using the same procedures in order to assess the long-term impact of the training on their reaction to sheep. Again, in the free-running tests the dogs were fitted with a shock collar, which was used to deter approaches to within 1-2m of the sheep. Dogs that had previously been shocked in year 1 showed a significant increased in latency to approach a person during the path test (p<0.001), even though this was not a condition under which shocks had been delivered. Owners reported behavioral differences between year 1 and 2 in 24 of the dogs. 18 of the 24 dogs had shown no interest in sheep during that period, even though they had been interested in them during the first year tests. However, only one of those dogs had received shocks, so the change in behaviour could not be attributed to the use of the shock collar. When comparing owners’ reports for the two years, the dogs showed a weaker inclination for chasing sheep and other prey than previously (p < 0:001), but this variable was not affected by shock experience. Dogs that had shown interest in sheep in year 1 showed a persistent interest in year 2. No dogs chased or attacked sheep as their first response, while half of them did so the first year. During the entire test period, the proportion of dogs attacking sheep was reduced to almost one fourth. The number of shocks administered per dog was reduced by the second year, and only one of the dogs which received el. shocks the first year needed el. shocks also the second year.The observations that both receivers and non-receivers of el. shocks the first year showed a reduction in the probability of chasing sheep, but the receivers showing a larger reduction, show that el. shock treatment provides an additional learning response. No adverse effects on the dogs were observed with this training procedure, but in their discussion the authors commented \"In order to ensure no negative effects, we recommend that the electronic dog collar may be used for such purposes only if it is used by skilled trainers with special competence on dog behaviour, learning mechanisms, and of this particular device.\"\n\nThe aim of Salgirli's study was \"...to investigate whether any stress is caused by the use of specific conditioned signal, quitting signal, and/or pinch collars as alternatives to electric training collars, and if they do so, whether the stress produced in the process is comparable to the one with electric training collars.\". The study population were a group of 42 adult police dogs. The quitting signal was a conditioned frustration equivalent to negative punishment. It was conditioned by associating failure to obtain an anticipated food reward with a specific vocal signal. In the test, dogs were walked past a \"provocateur\" who attempted to taunt the dog into a reaction. If the dog reacted, it was punished, and if it failed to react on subsequent provocations then the punishment was deemed to have had a learning effect. The study is therefore a comparison of negative and positive punishment methods, and not a comparison of punishment with positive reinforcement. Learning effect was measured by assessing the number of dogs that learned to quit a behaviour after application of the punishing stimulus. There was no statistical difference in learning effect between the pinch and shock collar, but the quitting signal produced a significantly poorer learning effect compared to shock or pinch collars (p < 0.01 in both cases). \"Although the pinch collar caused more behavioral reactions, in the form of distress, than the electronic training collar, the electronic training collar elicits more vocal reactions in dogs than the pinch collars\"; the explanation for increased vocalisation in the shock collar group was that this was due to a startle response rather than pain reactions.\n\nSalivary cortisol was monitored to measure the stress levels of the dogs, but this data was not presented in the dissertation; behavioral observation was the sole measure of stress. The study concluded that the electronic training collar induces less distress and shows stronger “learning effect” in dogs in comparison to the pinch collar. Commenting on the quitting signal, the author stated \"It should particularly be mentioned, that the quitting signal training was implied only on adult dogs within the frame of this study. Therefore, the results should not be interpreted as that the quitting signal can not be a suitable method in police dog training. As previously stated training of the quitting signal requires a hard and a structured procedure. Thus, if the training, namely the conditioning, begins in puppyhood, the quitting signal can also be an effective method in police dog training\". Comparing the effects of the three punishment methods; \"These results can probably be explained by that electronic training collar complies completely with the punishment criteria, which were defined by TORTORA (1982), in case of proof of the proficient and experienced user. On the other hand when applying the pinch collar, these criteria can not be met even though perfect timing is applied since reactions of the dog and effectiveness of the method depends on several different factors such as the willingness, strength and motivation of the handler, as well as his/her proficiency. In addition to that, the visibility of the administrator and, thus, of the punishment is another important factor influencing the efficiency of the pinch collar because the dog directly links the punishment with its owner. Therefore this method does not satisfy the ‘‘punishment criteria’’ at all. The quitting signal on the other hand requires criteria, such as good timing and structured training procedure, on account of complete conditioning in order to achieve effective results. Even if these criteria are met, the personality trait of the dog is another factor, which influences the efficiency of the signal.\"\n\nSchalke et al. conducted a 7-month study to investigate the effect of shock collars on stress parameters, in a series of different training situations. Heart rate and saliva cortisol were used to determine the stress levels in three groups of dogs. Group A received the electric shock when they touched the \"prey\" (a rabbit dummy attached to a motion device), Group H (\"here\" command) received the electric shock when they did not obey a previously trained recall command during hunting, and Group R (random) received random shocks that were unpredictable and out of context. Group A did not show a significant rise in cortisol levels; the other two groups (R & H) did show a significant rise, with group R showing the highest level of cortisol. Salivary cortisol was measured, as this procedure is less likely to cause stress related rise in cortisol.\n\nFrom this the researchers concluded that the dogs who could clearly associate the shock with their action (i.e. touching the prey) and as a result were able to predict and control whether they received a shock, did not show considerable or persistent stress. The evidence of increased stress in the other groups was felt to support earlier findings that poor timing and/or inappropriate use of a shock collar puts the dog at high risk of severe and ongoing stress. They conclude that \"The results of this study suggest that poor timing in the application of high level electric pulses, such as those used in this study, means there is a high risk that dogs will show severe and persistent stress symptoms. We recommend that the use of these devices should be restricted with proof of theoretical and practical qualification required and then the use of these devices should only be allowed in strictly specified situations.\"\n\nSchilder and van der Borg conducted a study to compare the behavior of police service dogs that had previously been trained using a shock collar (Group S) with those which had not (Group C). In the training test no shocks were applied, but the animal's behavior was observed during training tasks. The intention was to investigate whether shock collar based training might have a long-term effect on stress-related behavior even in the absence of shock, and whether this related to specific features of the training context. Behaviors recorded included recognised indicators of stress (panting, lip-licking, yawning, paw lifting and body posture) as well as yelping, squealing, snapping and avoidance. During free walks on the training grounds, groups S dogs showed significantly more stress related behaviors and a lower body posture than group C dogs. During training, the same differences were found. The difference between the groups was more significant when training took place on the familiar training ground, indicating a contextual effect. The presence of the trainer was considered to be part of this context. The authors concluded \"We concluded that shocks received during training are not only unpleasant but also painful and frightening.\"\n\nLindsay says of this study, \"Schilder and Van der Borg (2004) have published a report of disturbing findings regarding the short-term and long- term effects of shock used in the context of working dogs that is destined to become a source of significant controversy... The absence of reduced drive or behavioral suppression with respect to critical activities associated with shock (e.g., bite work) makes one skeptical about the lasting adverse effects the authors claim to document. Although they offer no substantive evidence of trauma or harm to dogs, they provide loads of speculation, anecdotes, insinuations of gender and educational inadequacies, and derogatory comments regarding the motivation and competence of IPO trainers in its place.\" \n\nSteiss, et al., conducted a four-week study of adult shelter dogs’ physiological and behavioral responses to bark control collars. Plasma cortisol was used as the stress measure. Dogs were randomly assigned to either a shock collar, a spray collar, or a dummy collar (control group). Dogs that were known to bark at an unfamiliar dog were used for the study. Test conditions involved presentation of an unfamiliar dog. Dogs wore activated collars for period of 30 minutes per day for three days in two consecutive weeks. The amount of barking was significantly reduced starting on the second day with both the spray and shock collars. There was no significant difference in effect between the two collar types. The treatment group dogs showed a mild yet statistically significant increase in blood cortisol level (an indicator of stress) only on the first day of wearing the collars (as compared to the Control Group.) At the conclusion of the study, Dr. Steiss and her team concluded that \"In the present study, with dogs wearing bark control collars intermittently over a 2-week period, the collars effectively deterred barking without statistically significant elevations in plasma cortisol, compared to controls, at any of the time points measured.\"\n\nTortora applied a method called \"safety training\" to treat aggression in 36 cases exhibiting a form of \"instrumental aggression\", selected after screening a population of 476 cases. \"Instrumental aggression\" was defined as describing aggressive acts that \"do not have a clear evolutionary significance, are not directly related to emotional arousal, do not have specific releasing stimuli, are not directly modulated by hormones, and do not have an identifiable focus in the brain\". Tortora states that in the context of the article \"instrumental aggression\" was specifically defined as \"aggressive responses that have \"a specifiable learning history, show a growth function over time and are modulated by their consequences. These dogs had few operant alternatives to gain reinforcement by compliance and were channeled down a path that allowed their innate aggressiveness to come under the control of the negatively reinforcing contingencies in the environment\". The dogs initially behaved as though they \"expected\" aversive events and that the only way to prevent these events was through aggression. The dogs were therefore a highly selected subset that had not learned strategies for coping with threat.\n\nEach dog was trained to respond to a set of 15 commands taken from the AKC standard for CDX obedience. The commands were selected to provide control over the dog, and included \"heel\", \"stand\" \"go\", \"come\", \"hold\", \"drop\" and \"sit\". These behaviors were termed \"safety behaviors\". Training was divided into 9 stages, each of which was composed of 5-20 twice daily training sessions. Dogs could only progress to the next stage after passing a test. On average, dogs took 10-15 sessions to complete each stage. After training basic commands, the dogs were trained to perform the behaviors they had already learned in order to avoid progressively increasing electric shock. After that, they were conditioned to perform a safety behavior in order to avoid a \"safety tone\" that allowed them to anticipate the shock. In the later stages of training, dogs were exposed to provocation by a distractor dog, and were punished using full intensity shock if they failed to perform a safety behavior or if they showed aggression. After training was complete, and the dogs were choosing to perform the safety behaviors instead of aggression, owners were taught to use the shock collar and the training was transferred into everyday situations. The training resulted in a long-lasting and complete suppression of aggressive behaviour in the dogs. Dogs were followed up 3 years after the end of training, and the reduction in aggression were maintained.\n\nPETA (People for the Ethical Treatment of Animals) opposes the use of shock collars, stating \"Dogs wearing shock collars can suffer from physical pain and injury (ranging from burns to cardiac fibrillation) and psychological stress, including severe anxiety and displaced aggression. Individual animals vary in their temperaments and pain thresholds; a shock that seems mild to one dog may be severe to another. The anxiety and confusion caused by repeated shocks can lead to changes in the heart and respiration rate or gastrointestinal disorders. Electronic collars can also malfunction, either administering nonstop shocks or delivering no shocks at all\".\n\nCABTSG (The Companion Animal Behaviour Therapy Study Group), an affiliate group of the BSAVA (British Small Animal Veterinary Association), produced a policy statement on the use of shock collars, stating \"Their effectiveness depends upon the pain and fear experienced by the animal, but to use them correctly requires detailed understanding of behaviour and its motivation, as well as very precise timing. Few operators are able to achieve any reliable success with these devices and the consequences of failure can be a worsening of the problem behaviour. The indiscriminate use of shock collars therefore poses a threat to the safety of the general public, as well as to the welfare of the animal. We believe that sufficient alternative methods of treatment exist that such electronic training devices are redundant. Therefore, as an association affiliated to BSAVA, it is our duty to recommend that shock collars and all other related training and control aids should be banned from sale or use\". CABTSG has been renamed the British Veterinary Behaviour Association.\n\nThe BSAVA (British Small Animal Veterinary Association) produced a statement on the risks associated with collars \"In principle, the BSAVA opposes the use of electronic shock collars for training and containment of animals. Shocks received during training may not only be acutely stressful, painful and frightening for the animal but also may produce long term adverse effects on behavioural and emotional responses.\".\n\nOn the advice of the RSPCA (Royal Society for the Prevention of Cruelty to Animals) and other welfare groups, the ACPO (Association of Chief Police Officers)banned the use of shock collars for police dog training by all UK police forces. The current ACPO Police Dogs Manual of Guidance states \"Equipment that is not approved for use in the training of police dogs includes remote training collars designed to give an electric shock and Pinch Collars\" .\n\nThe RSPCA commissioned a review of the effects of shock collars from the Department of Veterinary Medicine at Bristol University, which is available online. It states \"Given the lack of scientific evidence for the efficacy of behavioural modification using shock collars, particularly in the long term, in addition to the potential for mistakes or deliberate abuse and the difficulty in correcting such errors, the widespread use of these devices must be carefully considered.\" \n\nThe UK Kennel Club has an ongoing campaign to achieve a ban on the sale and use of shock collars; \"The Kennel Club is against the use of any negative training methods or devices. The Kennel Club believes that there are many positive training tools and methods that can produce dogs that are trained just as quickly and reliably, with absolutely no fear, pain, or potential damage to the relationship between dog and handler.\" \"The Kennel Club in calling upon the Government and Scottish Parliament to introduce an outright ban on this barbaric method of training dogs.\".\n\nThe two British members of the World Union of German Shepherd Clubs (WUSV) have joined the Kennel Club in calling for a complete ban on shock collars, and passed a motion to exclude this equipment from any of its training branches during official club training times.\n\nThe HSUS (Humane Society of the United States) provides the following comment on the use of aversive collars (choke chains, pinch collars and shock collars): \"Some trainers use aversive collars to train \"difficult\" dogs with correction or punishment. These collars rely on physical discomfort or even pain to teach the dog what not to do. They suppress the unwanted behavior but don't teach him what the proper one is. At best, they are unpleasant for your dog, and at worst, they may cause your dog to act aggressively and even bite you. Positive training methods should always be your first choice.\" They go on to comment on shock collars specifically: \"The least humane and most controversial use of the shock collar is as a training device. The trainer can administer a shock to a dog at a distance through a remote control. There is a greater chance for abuse (delivery of shocks as punishment) or misuse (poor timing of shocks). Your dog also may associate the painful shock with people or other experiences, leading to fearful or aggressive behavior\".\n\nThe NCAE (Norwegian Council on Animal Ethics) \"recommends the introduction of a ban electric training collars and similar remote-controlled or automatic electronic devices that cause your dog substantial discomfort. It should nevertheless be granted an exemption for such training carried out by authorized persons in order to prevent hunting of livestock and wildlife.\" \n\nThe APDT (Association of Professional Dog Trainers) says, \"[Electronic] training collars should not be used by novice dog owners or by trainers who are not properly instructed in their use. Use of electronic training collars can result in trauma to your dog and generally are not recommended by positive reinforcement trainers\".\n\nThe AVSAB (American Veterinary Society of Animal Behavior) has produced a position statement titled \"The use of punishment for behavior modification in animals\", the opening paragraph of which reads \"AVSAB’s position is that punishment (e.g. choke chains, pinch collars, and electronic collars) should not be used as a first-line or early-use treatment for behavior problems. This is due to the potential adverse effects which include but are not limited to: inhibition of learning, increased fear-related and aggressive behaviors, and injury to animals and people interacting with animals.\" \n\nIn his 2005 textbook on training and behavior, Steven Lindsay writes \"Instead of instilling social aversion and anxiety ... animal and human research supports the notion that competent shock [collar] training appears to promote positive social attachment, safety, and reward effects that may be provided and amplified via affectionate petting and reassuring praise. The preponderance of scientific evidence suggests that [electrical stimulation] escape/avoidance and pain reduction should promote long-term effects that are incompatible with fear and stress, making the trainer an object of significant extrinsic reward that actually enhances the dog's welfare via an improved capacity for social coping, learning, and adaptation\". Steven Lindsay states \"If minimizing the intensity, duration, and frequency of aversive stimulation during training is recognized as a significant factor in the definition of humane dog training, then the radio controlled e-collar must be ranked as one of the most humane dog-training tools currently available\" \n\n\"The International Association of Canine Professionals (IACP) strongly opposes legislation that bans or limits the humane use of any training tool, saying It is our conviction that limiting the humane use of training tools would result in a higher incidence of nuisance and dangerous dog behavior, and more dogs being surrendered to already over-burdened public shelters... Dog training is a very diverse field with a single common thread: communication. Dogs are trained for many different tasks such as assisting the disabled, police work, herding, hunting, protection, competition and companionship. Professional trainers achieve these training goals by using a wide variety of tools to communicate with the dog, both at close range, and over long distances. Done effectively, this communication increases desirable behaviors and reduces the incidence of problem behaviors in dogs... Any efforts to ban or limit the use of training tools would hinder this communication, and our ability to train dogs would suffer. Working dogs would no longer be able to achieve highly specialized tasks, and families with pet dogs would have fewer options available to correct behavioral problems... Training tools, when properly utilized, are safe and humane\".\n\nRandall Lockwood PhD, Senior Vice President, Anti-cruelty Initiatives and Legislative Services, The American Society for the Prevention of Cruelty to Animals (ASPCA) was quoted in a 2007 White Paper titled \"The Facts About Modern Electronic Training Devices,\" produced by Radio Systems, a manufacturer of shock collars, \"We recognize that older products were often unreliable and difficult to use humanely. But we feel that new technology employed by responsible manufacturers has led to products that can be and are being used safely and effectively to preserve the safety and well-being of many dogs and strengthen the bond with their human companions.\"\n\nThe use of shock collars is banned in Scotland, Denmark, Norway, Sweden, Austria, Switzerland, Slovenia, and Germany, and in some territories of Australia, including New South Wales and Southern Australia\n\nIn March 2010, the Welsh Assembly voted to ban the use of shock collars in Wales. The ban was unsuccessfully challenged by Petsafe, a manufacturer of these devices, and the Electronic Collar Manufacturers' Association, who claimed that it breached Article 1 of the First Protocol of the European Convention of Human Rights.\n\nIn August 2018, it was announced that shock collars for cats and dogs would be banned in England. The Environment secretary Michael Gove said that shock collars caused unacceptable \"harm and suffering\". Animal charities, including the Kennel Club, the RSPCC and the Dogs Trust, welcomed the move.\n\nThe potential for shock collars to have a negative impact on behaviour has been recognised by the UK courts. In 2001 Ostarra Langridge was prosecuted after one of her dogs attacked and killed a Shih Tzu whilst on a walk. A control order, rather than a destruction order, was imposed as the magistrates accepted the defense that Ms. Langridge's dog's aggressive behaviour was attributable to the effects of the shock collar. \"Ms. Langridge sought the help of a behaviourist when her dogs started to run away from her on their walks along the beach. The dogs were given shock collars, which Miss Langridge was told to keep on for three months and activate whenever they misbehaved. But the first time the dogs got a shock was by mistake, after a small dog they were walking past made Miss Langridge jump. From then on her pets associated the shocks with small dogs and became afraid of them. When Miss Langridge described the day in July when her dogs turned on a Shih Tzu she had tears in her eyes.\". She stated \"\"They connected the pain of the electric shock with little dogs because of the first time I used the collar. The day that machine came in this house I regret.\" \n\nOn April 11, 2011, a 48-year-old man from Ogmore-by-Sea became the first person convicted of illegal use of a shock collar in Wales. He was subsequently fined £2,000 and assessed £1,000 for court costs.\n\nIn 2002, the RSPCA, Victoria (Royal Society for the Prevention of Cruelty to Animals) lost a defamation lawsuit to Orion Pet Products, a shock collar manufacturer, and was ordered to pay $100,00 in damages. The RSPCA had falsely claimed that shock collars can cause burns and delivered 3,000 volt shocks to dogs. They made the \"fanciful claim\" that the current from a shock collar had caused a 60 Kilogram dog to perform backflips and resulted in brain damage. Justice Weinberg also found that the RSPCA claims that these collars caused epileptic fits, vomiting, seizures, burning and bleeding were misleading. The RSPCA's senior inspector had falsified evidence in an attempt to demonstrate that shock collars can cause burns.\n"}
{"id": "316842", "url": "https://en.wikipedia.org/wiki?curid=316842", "title": "Social distance", "text": "Social distance\n\nSocial distance describes the distance between different groups in society and is opposed to \"locational distance\". The notion includes differences such as social class, race/ethnicity, gender or sexuality, but also the fact that the different groups mix less than members of the same group. The term is often applied in cities, but its use is not limited to that. \n\nAn old concept, in 1924 Robert E. Park defined social distance as \"an attempt to reduce to something like measurable terms the grade and degrees of understanding and intimacy which characterizes personal and social relations generally\". It is the measure of nearness or intimacy that an individual or group feels towards another individual or group in a social network or the level of trust one group has for another and the extent of perceived likeness of beliefs..\n\nThe concept of social distance as applied to the study of racial attitudes and racial relations. Journal of Applied Sociology, 8, 339-344.</ref> In the sociological literature, social distance is conceptualized in several different ways.\n\n\nIt is possible to view these different conceptions as \"dimensions\" of social distance, that do not necessarily overlap. The members of two groups might interact with each other quite frequently, but this does not always mean that they will feel \"close\" to each other or that normatively they will consider each other as the members of the same group. In other words, interactive, normative and affective dimensions of social distance might not be linearly associated.\n\nSocial distance was also used in a different meaning by anthropologist and cross-cultural researcher, Edward T. Hall, to describe the psychological distance which an animal can stand to be away from its group before beginning to feel anxious. This phenomenon can be seen in human babies and toddlers who can only walk or crawl so far from their parents or guardians before becoming anxious and quickly returning to the safe space. The babies' social distance is quite small.\n\nHall also notes that this concept of social distance has been extended by technological advances such as the telephone, walkie talkie, and television, among others. Hall’s analysis of social distance came before the development of the internet, which has expanded social distance exponentially. Social distance is now even expanding beyond our planet as we send people into outer space on space missions and even personal trips to space.\n\nIt is said|date=June 2016}} that every individual regards his or her own culture as being superior to all other cultures, and other cultures as being inferior due to their differences from his or her own culture. The social distance between two cultures may ultimately manifest in the form of hatred. A consequence of this distance and hatred is prejudices, that different cultural groups assume to be true for differing social groups. For example, the Brahmins are believed to possess the highest, and the shudras the lowest status in Hindu society. If a Brahmin child ever touches the child of some shudra, the former is given a bath to rid him of his supposed defilement caused by his touch. As a result of this strict formulation of his activities, the Brahmin child forms a prejudice in his mind that shudras are untouchable and impure.\n\nSome ways social distance can be measured include: direct observation of people interacting, questionnaires, speeded decision making tasks, route planning exercises, or other social drawing tasks (see sociogram).\n\nIn questionnaires, respondents are typically asked members of which groups they would accept in particular relationships. For example, to check whether or not they would accept a member of each group as a neighbor, as a fellow worker as a marriage partner. The social distance questionnaires may not accurately measure what people actually would do if a member of another group sought to become a friend or neighbour. The social distance scale is only an attempt to measure one's feeling of unwillingness to associate equally with a group. What a person will actually do in a situation also depends upon the circumstances of the situation.\n\nIn speeded decision making tasks, studies have suggested a systematic relationship between social distance and physical distance. When asked to either indicate the spatial location of a presented word or verify a word’s presence, people respond more quickly when \"we\" was displayed in a spatially proximate versus spatially distant location and when \"others\" was displayed in a\nspatially distant versus a spatially proximate location. This suggests that social distance and physical distance are conceptually related.\n\nRoute planning exercises have also hinted at a conceptual link between social distance and physical distance. When asked to draw a route on a map, people tend to draw routes closer to friends they pass along the way and further away from strangers. This effect is robust even after controlling for how easy it is for the people passing one another to communicate. \n\nThere is some evidence that reasoning about social distance and physical distance draw on shared processing resources in the human parietal cortex.\n\n\"Social periphery\" is a term often used in conjunction with social distance. It refers to people being 'distant' with regard to social relations. It is often implied that it is measured from the dominant \"city élite\". The social periphery of a city is often located in the centre.\n\n\"Locational periphery\" in contrast is used to describe places physically distant from the heart of the city. These places often include suburbs which are socially close to the core of the city. In some cases the locational periphery overlaps with the social periphery, such as in Paris' \"banlieues\".\n\nIn 1991, Mulgan stated that \"The centres of two cities are often for practical purposes closer to each other than to their own peripheries.\" This reference to social distance is especially true for global cities.\n\n"}
{"id": "15669613", "url": "https://en.wikipedia.org/wiki?curid=15669613", "title": "Spinning Dancer", "text": "Spinning Dancer\n\nThe Spinning Dancer, also known as the silhouette illusion, is a kinetic, bistable optical illusion resembling a pirouetting female dancer. The illusion, created in 2003 by web designer Nobuyuki Kayahara, involves the apparent direction of motion of the figure. Some observers initially see the figure as spinning clockwise (viewed from above) and some counterclockwise. Additionally, some may see the figure suddenly spin in the opposite direction.\n\nThe illusion derives from the lack of visual cues for depth. For instance, as the dancer's arms move from viewer's left to right, it is possible to view her arms passing between her body and the viewer (that is, in the foreground of the picture, in which case she would be circling counterclockwise on her right foot) and it is also possible to view her arms as passing behind the dancer's body (that is, in the background of the picture, in which case she is seen circling clockwise on her left foot).\n\nWhen she is facing to the left or to the right, her breasts and ponytail clearly define the direction she is facing, although there is ambiguity in which leg is which. However, as she moves away from facing to the left (or from facing to the right), the dancer can be seen (by different viewers, not by a single individual) facing in either of two directions. At first, these two directions are fairly close to each other (both left, say, but one facing slightly forward, the other facing slightly backward) but they become further and further away from each other until we reach a position where her ponytail and breasts are in line with the viewer (so that neither her breasts nor her ponytail are seen so readily). In this position, she could be facing either away from the viewer or towards the viewer, so that the two positions the two different viewers could see are 180 degrees apart.\n\nIt has been established that the silhouette is more often seen rotating clockwise than counterclockwise. According to an online survey of over 1600 participants, approximately two thirds of observers initially perceived the silhouette to be rotating clockwise. In addition, observers who initially perceived a clockwise rotation had more difficulty experiencing the alternative.\n\nThese results can be explained by a psychological study providing evidence for a viewing-from-above bias that influences observers' perceptions of the silhouette. Kayahara's dancer is presented with a camera elevation slightly above the horizontal plane. Consequently, the dancer may also be seen from above or below in addition to spinning clockwise or counterclockwise, and facing toward or away from the observer. Upon inspection, one may notice that in Kayahara's original illusion, seeing the dancer spin clockwise is paired with constantly holding an elevated viewpoint and seeing the dancer from above. The opposite is also true; an observer maintaining an anti-clockwise percept has assumed a viewpoint below the dancer. If observers report perceiving Kayahara's original silhouette as spinning clockwise more often than counterclockwise, there are two chief possibilities. They may have a bias to see it spinning clockwise, or they may have a bias to assume a viewpoint from above. To tease these two apart, the researchers created their own versions of Kayahara's silhouette illusion by recreating the dancer and varying the camera elevations. This allowed for clockwise/from-above (like Kayahara's original) and clockwise/from-below pairings. The results indicated that there was no clockwise bias, but rather viewing-from-above bias. Furthermore, this bias was dependent upon camera elevation. In other words, the greater the camera elevation, the more often an observer saw the dancer from above.\n\nIn popular psychology, the illusion has been incorrectly identified as a personality test that supposedly reveals which hemisphere of the brain is dominant in the observer. Under this wrong interpretation, it has been popularly called the Right Brain–Left Brain test, and was widely circulated on the Internet during late 2008 to early 2009.\n\nA 2014 paper describes the brain activation related to the switching of perception. Utilizing fMRI in a volunteer capable to switch at will the direction of rotation, it was found that a part of the right parietal lobe is responsible for the switching. The authors relate this brain activation to the recently described spontaneous brain fluctuations.\n\nThere are other optical illusions that depend on the same or a similar kind of visual ambiguity known as multistable, in that case bistable, perception. One example is the Necker Cube.\n\nDepending on the perception of the observer, the apparent direction of spin may change any number of times, a typical feature of so-called bistable percepts such as the Necker cube which may be perceived from time to time as seen from above or below. These alternations are spontaneous and may randomly occur without any change in the stimulus or intention by the observer. However some observers may have difficulty perceiving a change in motion at all.\n\nOne way of changing the direction perceived is to use averted vision and mentally look for an arm going behind instead of in front, then carefully move the eyes back. Some may perceive a change in direction more easily by narrowing visual focus to a specific region of the image, such as the spinning foot or the shadow below the dancer and gradually looking upwards. One can also try to tilt one's head to perceive a change in direction. Another way is to watch the base shadow foot, and perceive it as the toes always pointing away from you and it can help with direction change. You can also close your eyes and try and envision the dancer going in a direction then reopen them and the dancer should change directions. Still another way is to wait for the dancer's legs to cross in the projection and then try to perceive a change in the direction in what follows. You could also try using your peripheral vision to distract the dominant part of the brain, slowly look away from the ballerina and you may begin to see it spin in the other direction. Perhaps the easiest method is to blink rapidly (slightly varying the rate if necessary) until consecutive images are going in the 'new' direction. Then open your eyes and the new rotational direction is maintained. It is even possible to see the illusion in a way that the dancer is not spinning at all, but simply rotating back and forth 180 degrees.\n\nSlightly altered versions of the animation have been created with an additional visual cue to assist viewers who have difficulty 'seeing' one rotation direction or the other. Labels and white edges have been added to the legs, to make it clear which leg is passing in front of the other. \"(see below)\" Looking at one of these can sometimes then make the original dancer image above spin in the corresponding direction.\n"}
{"id": "8199698", "url": "https://en.wikipedia.org/wiki?curid=8199698", "title": "Stationary sequence", "text": "Stationary sequence\n\nIn probability theory – specifically in the theory of stochastic processes, a stationary sequence is a random sequence whose joint probability distribution is invariant over time. If a random sequence \"X\" is stationary then the following holds:\n\nwhere \"F\" is the joint cumulative distribution function of the random variables in the subscript.\n\nIf a sequence is stationary then it is wide-sense stationary.\n\nIf a sequence is stationary then it has a constant mean (which may not be finite):\n\n\n"}
{"id": "22242796", "url": "https://en.wikipedia.org/wiki?curid=22242796", "title": "Susan Smalley", "text": "Susan Smalley\n\nSusan Smalley, Ph.D. is a behavioral geneticist, writer and activist. The co-author of \"Fully Present: The Science, Art, and Practice of Mindfulness\", she is the founder of the UCLA Mindful Awareness Research Center at the Jane and Terry Semel Institute for Neuroscience and Human Behavior (MARC), and professor emerita in the department of psychiatry and biobehavioral sciences at UCLA. Her research centers on the genetic basis of childhood-onset behavior disorders, such as ADHD, and the cognitive and emotional impact of mindfulness meditation on health and wellbeing. She has published more than 100 peer-reviewed papers and lectured globally on the genetics of human behavior and the science of mindfulness. \n\nSmalley is on the board of directors for Equality Now, an international human rights organization focused on women and girls; the scientific advisory board for Stop Breathe and Think, a meditation app developed by Tools for Peace, a non-profit organization that teaches mindfulness and meditation to inner city teenagers; and the board of directors for Roswell Biotechnologies, a molecular electronics company that uses genomic information to provide precision medical treatment. In 2010, she co-founded Cell-Ed, a platform to bring basic literacy to adults in underserved populations using mobile phones.\n\nSmalley majored in biological anthropology at the University of Michigan, and as an undergraduate became interested in population genetics and human evolution. She received a BA in anthropology in 1976, and in 1981 earned an MA in anthropology from UCLA. She was awarded a Ph.D in anthropology with specialization in population genetics from UCLA in 1985. Her dissertation examined the genetics of spatial ability. \nSmalley joined the faculty at UCLA after she completed post-doctoral fellowships in medical genetics and childhood psychopathology, moving from assistant to full professor until her retirement to emeritus in 2011.\n\nIn 1988 she published a review paper on the genetics of autism in \"JAMA Psychiatry\". Following its publication, she received a National Institute of Health (NIH) grant to investigate genetic determinants in autism, and pioneered an approach to behavioral genetics by studying known genetic disorders with behavioral sequelae, specifically, the study of tuberous sclerosis complex, (TSC), a genetic disorder in which autistic disorder occurs at higher rates than the general population. She continued to research autism for the following ten years, producing numerous papers on the genetics and subclinical variants of autism beyond the diagnostic classification as well as genetic and behavioral studies of TSC.\n\nSmalley's focus subsequently shifted to ADHD. Her lab produced more than 40 publications on the disorder, including the first genome-wide scan (in conjunction with investigators at Oxford University), candidate gene investigations, and a series of papers on ADHD among a northern Finnish birth cohort. She wrote extensively on the strengths of those with ADHD, characterizing it as a different way of thinking rather than a deficit. \n\nSmalley was diagnosed with an early stage melanoma in 2002. She took a leave of absence to explore non-western wellness practices, and in addition to significant lifestyle changes, she developed a meditation practice. When she returned to UCLA, she began researching mindfulness meditation and its impact on ADHD and other disorders. She subsequently led seminal studies that demonstrated the relationship between mindfulness and common personality traits of those with ADHD, mindfulness as an intervention in ADHD, and school-based approaches to bringing mindfulness to children. and founded the Mindful Awareness Research Center at the UCLA Semel Institute for Neuroscience and Human Behavior (MARC) to bring meditation practices to the general public through research and education. In an article for the \"Huffington Post\", she wrote: \"As a scientist, I love the challenge of understanding my mind, from the inside, while learning what science tells us from the outside. The merging of these two approaches will yield knowledge far greater than either can alone.\"\n\nIn 2010 Smalley and Diana Winston, a former Buddhist nun and the director of education at MARC, wrote \"Fully Present: The Science, Art, and Practice of Mindfulness.\" It explores the science of meditation and provides guidance to develop a mindfulness practice.\n\nSmalley writes regularly for the \"Huffington Post\" and \"Psychology Today.\" She was the keynote speaker at the UCLA Department of Anthropology commencement ceremony in 2013, and the 2017 Children and Adults with Attention Deficit Hyperactivity Disorder (CHADD) annual conference.\nSmalley and her husband, Kevin Wall, were married in 1974. They have three children.\n\n\nOfficial website \n"}
{"id": "147831", "url": "https://en.wikipedia.org/wiki?curid=147831", "title": "Tomáš Garrigue Masaryk", "text": "Tomáš Garrigue Masaryk\n\nTomáš Garrigue Masaryk (), sometimes anglicised to Thomas Masaryk (7 March 1850 – 14 September 1937), was a Czechoslovak politician, statesman, sociologist and philosopher.\n\nAfter trying to reform the Austro-Hungarian monarchy into a federal state, with the help of the Allied Powers, he eventually succeeded in gaining Czechoslovak independence as a republic after World War I. He both founded and was the first President of Czechoslovakia and so is called the \"President Liberator\". (Prezident osvoboditel)\n\nMasaryk was born to a poor working-class family in the predominantly Catholic city of Hodonín, Moravia (in the region of Moravian Slovakia, today in the Czech Republic but then part of the Austro-Hungarian Empire.) Another tradition claims the nearby Slovak village of Kopčany, the home of his father, as his birthplace. He subsequently grew up in the village of Čejkovice, in South Moravia, before he moved to Brno to study.\n\nHis father Jozef Masárik, born in Kopčany in Slovakia (then the Hungarian part of Austria-Hungary), was a carter and later the steward and coachman at the Imperial Estate of nearby Hodonín. Tomáš's mother, Teresie Masaryková (née Kropáčková), was a Moravian of Slavic origin but German education. She worked as a cook at the Estate where she met Masárik, and they married on 15 August 1849.\n\nAfter grammar school in Brno and Vienna, from 1872 to 1876, Masaryk attended the University of Vienna, where he was a student of Franz Brentano. He received his Ph.D. at the University of Vienna in 1876 and completed his habilitation thesis at the same university in 1879, entitled \"Der Selbstmord als sociale Massenerscheinung der modernen Civilisation\" (\"Suicide as a Social Mass Phenomenon of Modern Civilization\"). Between 1876 and 1879, he studied in Leipzig with Wilhelm Wundt and Edmund Husserl.\n\nOn 15 March 1878, he married Charlotte Garrigue in Brooklyn, whom he had met at Leipzig. They lived in Vienna until 1881, when they moved to Prague.\n\nIn 1882, he was appointed Professor of Philosophy in the Czech part of Charles University of Prague. The following year he founded \"Athenaeum\", a magazine devoted to Czech culture and science. \"Athenaeum\" issued in October 15, 1883 (editor was Jan Otto).\n\nHe challenged the validity of the epic poems \"Rukopisy královedvorský a zelenohorský\", supposedly dating from the early Middle Ages and providing a false nationalistic basis of Czech chauvinism to which he was continuously opposed. Further enraging Czech sentiment, he fought against the old superstition of Jewish blood libel during the Hilsner Trial of 1899.\n\nMasaryk served in the \"Reichsrat\" from 1891 to 1893 in the Young Czech Party and again from 1907 to 1914 in the Realist Party, which he had founded in 1900, but he did not yet campaign for the independence of Czechs and Slovaks from Austria-Hungary. In 1909, he helped Hinko Hinković in Vienna in the defense during the fabricated trial against members of the Croato-Serb Coalition (such as Frano Supilo and Svetozar Pribićević) and others, who were sentenced to more than 150 years, with a number of death penalties.\n\nWhen the First World War broke out in 1914, Masaryk concluded that the best course was to seek an independent country for Czechs and Slovaks, outside Austria-Hungary. He went into exile with his daughter, Olga, in December 1914, to Rome, then to Geneva and then to London via Paris in 1915, the Russian Empire in May 1917 and the United States via Vladivostok and Tokyo in April 1918. From Geneva onwards, he started organizing Czechs and Slovaks living outside Austria-Hungary, primarily in Switzerland, France, England, Russia and the United States, establishing the contacts that would prove crucial for Czechoslovak independence. He also gave lectures and wrote numerous articles and official memoranda supporting the Czechoslovak cause. In Russia, he was pivotal in establishing Czechoslovak Legions as an effective fighting force on the side of the Allies in World War I. During the war, he held a Serbian passport.\n\nIn 1915, he was one of the first members of staff of the new School of Slavonic and East European Studies, now part of University College London, where the Student Society and Senior Common Room are named after him. He became Professor of Slavic Research at King's College in London, lecturing on \"the problem of small nations\".\n\nDuring the war, Masaryk's intelligence network of Czech revolutionaries provided important and critical intelligence to the Allies. Masaryk's European network worked with an American counterespionage network of nearly 80 members, headed by E.V. Voska, who, as Habsburg subjects were presumed to be German supporters but were involved in spying on German and Austrian diplomats. Among other things, the intelligence from these networks was critical in uncovering the Hindu-German Conspiracy in San Francisco. T.G. Masaryk started teaching at London University in October 1915. There, he published \"Racial Problems in Hungary\" in which he also expressed thoughts on Czechoslovak's independence. In 1916, Masaryk went to France to convince the French government of the necessity of dismantling Austria-Hungary. After the February Revolution in 1917, he proceeded to Russia (he left London for St. Petersburg in May) to help organize Slavic resistance to the Austrians, the Czechoslovak Legion.\n\nOn 5 August 1914, the Russian High Command authorized the formation of a battalion, recruited from Czechs and Slovaks in Russia. The unit went to the front in October 1914 and was attached to the Russian Third Army.\n\nFrom its start, Masaryk desired to grow the Družina from a battalion into a formidable military formation. To do so, however, he recognized that they would need to recruit from Czech and Slovak prisoners of war (POWs) in Russian camps. In late 1914, Russian military authorities permitted the Družina to enlist Czech and Slovak POWs from the Austro-Hungarian Army, but the order was rescinded after a few weeks because of opposition in other areas of the Russian government. Despite continuous efforts to persuade the Russian authorities to change their mind, the Czechs and the Slovaks were officially barred from recruiting POWs until the summer of 1917.\n\nUnder such conditions the Czechoslovak armed unit in Russia grew very slowly from 1914 and 1917. In early 1916, the Družina was reorganized as the First Czecho-Slovak Rifle Regiment. Following the Czechoslovak soldiers' stellar performance in July 1917 at the Battle of Zborov, when the Czecho-Slovak troops overran Austrian trenches, the Russian Provisional Government finally granted to Masaryk and the Czechoslovak National Council permission to recruit and mobilize Czech and Slovak volunteers from the POW camps. Later that summer, a fourth regiment was added to the brigade, which was renamed the First Division of the Czechoslovak Corps in Russia (Československý sbor na Rusi), also known as the Czechoslovak Legion (Československá legie) in Russia. A second division consisting of four regiments was added to the legion in October 1917, raising its strength to about 40,000 troops by 1918.\n\nIn 1918, he traveled to the United States (starting from Moscow 7 March to Vladivostok, Fusan and Tokyo and then to Vancouver by steamliner, and from Canada to Chicago), where he convinced President Woodrow Wilson of the righteousness of his cause. Speaking on 26 October 1918 from the steps of Independence Hall in Philadelphia, as head of the Mid-European Union, Masaryk called for the independence of the Czechoslovaks and the other oppressed peoples of Central Europe. On May 5, 1918, more than 150,000 Chicagoans filled the streets to give a triumphant welcome to the future President of Czechoslovakia. Chicago was then the center of Czechoslovak immigration to the United States and the city gave Masaryk an enthusiastic reception, which echoed Masaryk's earlier visits to the city and his visiting professorship at the University of Chicago in 1902. Masaryk had lectured at the University of Chicago in 1902 and 1907. He also had strong personal links with the US since 1878 by his marriage with an American citizen and his friendship with Charles R. Crane. a Chicago industrialist. Crane got Masaryk invited to the University of Chicago and introduced into the highest political circles, including to Wilson.\n\nWith the fall of the Austro-Hungarian Empire in 1918, the Allies recognized Masaryk as head of the Provisional Czechoslovak government (on October 14), and on November 14, 1918, he was elected President of the Czechoslovak Republic by the National Assembly in Prague while he was in New York. He came back to Prague Castle on December 21, 1918.\n\nMasaryk was re-elected as president three times: in May 1920, 1927, and 1934. A provision in the Czechoslovak Constitution of 1920 exempted him from the two-term limit. He visited many countries like France, Belgium, England, Egypt and the Mandate for Palestine in 1923 and 1927. With Herbert Clark Hoover, he guaranteed the 1. PIMCO – First Prague International Management Congress, organized Masaryks Academy of Labour and 120 experts around the world in Prague in July 1924. In March 1930, the National Assembly approved the law: \"T.G. Masaryk, he deserved on the State\" (law No.22/1930 Sb., from March 6, Czech: \"T.G. Masaryk, zasloužil se o stát\"). After the rise of Hitler, he was one of the first political figures in Europe to voice concern. He resigned from office on December 14, 1935 on the grounds of old age and poor health, and Edvard Beneš succeeded him.\n\nOn paper, Masaryk's powers as president were limited; the framers of the constitution intended for the Prime Minister and Cabinet to hold the real power. However, he provided a considerable measure of stability in the face of frequent changes of government (there were ten cabinets headed by nine Prime Ministers during his tenure). The stability that he ensured, as well as his great prestige both inside and outside the country, made Masaryk enjoy almost-legendary authority by the people. He used his authority to create an extensive informal political network called \"Hrad\" (the Castle). Under his watch, Czechoslovakia became the strongest democracy in central Europe.\n\nMasaryk died less than two years after leaving office, at the age of 87, in Lány, Czechoslovakia, now the Czech Republic. He died before the Munich Agreement and the Nazi occupation of his country. He was known as \"The Great Old Man of Europe\". Commemorations of Masaryk, state institutions and democratic societies have taken place annually in Lány cemetery on March 7 and September 14, since 1989.\n\nMasaryk wrote several books, including \"The Czech Question\" (1895), \"The Problems of Small Nations in the European Crisis\" (1915), \"The New Europe\" (1917) and \"The World Revolution\" (1925), in Czech, published in English as \"The Making of a State\" (1927), and the two following Volumes). The writer Karel Čapek wrote a series of articles entitled 'Hovory s T.G.M.' (Conversations with T.G.M.) which were later collected as a form of autobiography.\n\nMasaryk's life motto was \"Do not fear and do not steal\" (). He was a philosopher and an outspoken rationalist and humanist, he emphasised practical ethics, reflecting the influence of Anglo-Saxon philosophers, French philosophy, and especially the work of 18th Century German philosopher, Johann Gottfried Herder, who is considered the founder of nationalism. He was critical of German idealistic philosophy \n\nMasaryk married Charlotte Garrigue in 1878 and took her family name as his middle name. They had met in Leipzig, Germany, and were engaged in 1877. She was born in Brooklyn in a Protestant family with French Huguenots among their ancestors. She came to learn Czech perfectly and published studies in \"Bedřich Smetana\", a Czech magazine. Hardships during the war took their toll on her, and she died in 1923 from an unspecified illness. Their son, Jan Masaryk, served as Foreign Minister in the Czechoslovak government-in-exile (1940–1945) and in the governments of 1945 to 1948. Charlotte gave birth to four other children, Herbert, Alice, Eleanor (Anna, or Hana), and Olga.\n\nAlthough born a Catholic, Masaryk eventually became a non-practising Protestant, influenced in part by both the declaration of papal Infallibility in 1870 and his wife, who was raised as a Unitarian.\n\nAs the principal founding father of Czechoslovakia, Masaryk was regarded in a way as George Washington is regarded in the United States. Czechs and Slovaks alike still regard him as a symbol of democracy.\n\nThe Order of Tomáš Garrigue Masaryk is a state decoration established in 1990 that is awarded to individuals who have made outstanding contributions to humanity, democracy and human rights.\n\nMasaryk University in Brno, founded in 1919 as the second university of Czechoslovakia, is named after him, first when it was founded and again since 1990, after it had carried the name \"Univerzita Jana Evangelisty Purkyně v Brně\" for 30 years\n\nNumerous statues, busts and plaques commemorate Masaryk. Most are located in the Czech Republic and Slovakia, but some are in other countries; notably the Masaryk Statue on Embassy Row in Washington, D.C., as well as in Chicago on the Midway, and in San Francisco's Golden Gate Park rose garden. A plaque with a portrait of Masaryk is set on a wall of Rachiv, Ukraine, on the site of a hotel where he reportedly resided for a time (1917 to 1918). There is also a bust of Masaryk, erected in 2002, at Druzhba Narodiv Square (Friendship of Nations Square) in Uzhhorod, in the far-western corner of Ukraine.\n\nAvenida Presidente Masaryk (President Masaryk Avenue), a main avenue in Mexico City, where are located some of the most exclusive garment retailers takes its name from him, as does Masaryktown, Florida. So too does the Kfar Masaryk kibbutz near Haifa, Israel, founded largely by Czechoslovak immigrants. Also Tel Aviv has a Masaryk Square - Masaryk had visited Tel Aviv in 1927. There is a street in Zagreb, the capital of Croatia, named \"Masarykova ulica\", as well in many other Croatian towns such as Dubrovnik, Daruvar, Varaždin and Split. \"Masarikova ulica\" in Belgrade, Serbia, although one of the smallest in the city, has the address of the tallest building in Belgrade, the Beograđanka palace. One of the streets in the centre of Novi Sad, Serbia is named \"Masarikova ulica\". One of the biggest streets in the capital of Slovenia, Ljubljana, is also named after Masaryk. There is \"Rue Thomas Masaryk\" in Geneva as well, as well as in downtown Bucharest, Romania.\n\nA United Nations Expeditionary Force starship in Joe Haldeman's 1974 science fiction novel \"The Forever War\" is named \"Masaryk.\"\n\nA photograph of Masaryk leaning out of a train window, waving to and shaking hands with supporters, is the front cover for alternative metal band, Faith No More's 1997 album \"Album of the Year\". The liner notes for the album jacket depicts the funeral of an old man, with the words \"pravda vítězí\" (truth prevails) adorning the coffin. The statement is the motto of the Czech Republic and is seen as a symbol of democracy.\n\nAsteroid 1841 Masaryk, discovered by Lubos Kohoutek is named after him.\n\n\n\n\n"}
{"id": "10886610", "url": "https://en.wikipedia.org/wiki?curid=10886610", "title": "Trivialism", "text": "Trivialism\n\nTrivialism () is the logical theory that all statements (also known as propositions) are true and that all contradictions of the form \"p and not p\" (e.g. the ball is red and not red) are true. In accordance with this, a trivialist is a person who believes everything is true.\n\nIn classical logic, trivialism is in direct violation of Aristotle's law of noncontradiction. In philosophy, trivialism is considered by some to be the complete opposite of skepticism. Paraconsistent logics may use \"the law of non-triviality\" to abstain from trivialism in logical practices that involve true contradictions.\n\nTheoretical arguments and anecdotes have been offered for trivialism to contrast it with theories such as modal realism, dialetheism and paraconsistent logics.\n\nTrivialism, as a term, is derived from the Latin word \"trivialis,\" meaning something that can be found everywhere. From this, \"trivial\" was used to suggest something was introductory or simple. In logic, from this meaning, a \"trivial\" theory is something regarded as defective in the face of a complex phenomenon that needs to be completely represented. Thus, literally, the trivialist theory is something expressed in the simplest possible way.\n\nIn symbolic logic, trivialism may be expressed as the following:\n\nThe above would be read as \"given any proposition, it is a true proposition\" through universal quantification (∀).\n\nA claim of trivialism may always apply its fundamental truth, otherwise known as a truth predicate:\n\nThe above would be read as a \"proposition if and only if a true proposition,\" meaning that all propositions are believed to be inherently proven as true. Without consistent use of this concept, a claim of advocating trivialism may not be seen as genuine and complete trivialism; as to claim a proposition is true but deny it as probably true may be considered inconsistent with the assumed theory.\n\nLuis Estrada-González in \"Models of Possiblism and Trivialism\" lists four types of trivialism through the concept of possible worlds, with a \"world\" being a possibility and \"the actual world\" being reality. It is theorized a trivialist simply designates a value to all propositions in equivalence to seeing all propositions and their negations as true. This taxonomy is used to demonstrate the different strengths and plausibility of trivialism in this context:\n\nThe consensus among the majority of philosophers is descriptively a denial of trivialism, termed as non-trivialism or anti-trivialism. This is due to it being unable to produce a sound argument through the principle of explosion and it being considered an absurdity (reductio ad absurdum).\n\nAristotle's law of noncontradiction and other arguments are considered to be against trivialism. Luis Estrada-González in \"Models of Possiblism and Trivialism\" has interpreted Aristotle's \"Metaphysics Book IV\" as such: \"...A family of arguments between 1008a26 and 1007b12 of the form 'If trivialism is right, then X is the case, but if X is the case then all things are one. But it is impossible that all things are one, so trivialism is impossible.'(...)these Aristotelian considerations are the seeds of virtually all subsequent suspicions against trivialism: Trivialism has to be rejected because it identifies what should not be identified, and is undesirable from a logical point of view because it identifies what is not identical, namely, truth and falsehood.\"\n\nIt is implicitly claimed by Graham Priest, a professor of philosophy, that a position for trivialism is unsubstantial: \"...a substantial case can be made for [dialetheism]; belief in [trivialism], though, would appear to be grounds for certifiable insanity.\"\n\nHe has coined his rejection of trivialism \"the law of non-triviality\" as a replacement for the law of non-contradiction in paraconsistent logic and dialetheism.\n\nThere are theoretical arguments for trivialism argued from the position of a devil's advocate:\n\nPaul Kabay has argued for trivialism in \"On the Plenitude of Truth\" from the following:\n\n\nAbove, possibilism (modal realism; related to possible worlds) is the barely accepted theory that every proposition is possible. With this assumed to be true, trivialism can be assumed to be true as well according to Kabay.\n\nThe liar's paradox, Curry's paradox alongside the principle of explosion all can be asserted as valid and not required to be resolved and used to defend trivialism.\n\nIn Paul Kabay's comparison of trivialism to schools of philosophical skepticism (in \"On the Plenitude of Truth\")—such as Pyrrhonism—who seek to attain a form of ataraxia, or state of imperturbability; it is purported the figurative trivialist inherently attains this state. This is claimed to be justified by the figurative trivialist seeing every state of affairs being true, even in a state of anxiety. Once universally accepted as true, the trivialist is free from any further anxieties regarding whether any state of affairs is true.\n\nKabay compares the Pyrrhonian skeptic to the figurative trivialist and claims that as the skeptic reportedly attains a state of imperturbability through a suspension of belief, the trivialist may attain such a state through an abundance of belief.\n\nIn this case—and according to independent claims by Graham Priest—trivialism is considered the complete opposite of skepticism. However, insofar as the trivialist affirms all states of affairs as universally true, the Pyrrhonist neither affirms nor denies the truth (or falsity) of such affairs.\n\nIt is asserted by both Priest and Kabay that it is impossible for a trivialist to truly choose and thus act. Priest argues this by the following in \"Doubt Truth to Be a Liar\": \"One cannot intend to act in such a way as to bring about some state of affairs, \"s\", if one believes \"s\" already to hold. Conversely, if one acts with the purpose of bringing \"s\" about, one cannot believe that \"s\" already obtains.\" Ironically, due to their suspension of determination upon striking equipollence between claims, the Pyrrhonist has also remained subject to apraxia charges.\n\nPaul Kabay, an Australian philosopher, in his book \"A Defense of Trivialism\" has argued that various philosophers in history have held views resembling trivialism, although he stops short of calling them trivialists. He mentions various pre-Socratic Greek philosophers as philosophers holding views resembling trivialism. He mentions that Aristotle in his book \"Metaphysics\" appears to suggest that Heraclitus and Anaxagoras advocated trivialism. He quotes Anaxagoras as saying that all things are one. Kabay also suggests Heraclitus' ideas are similar to trivialism because Heraclitus believed in a union of opposites, shown in such quotes as \"the way up and down is the same\". Kabay also mentions a fifteen century Roman Catholic cardinal Nicholas of Cusa, stating that what Cusa wrote in \"De Docta Ignorantia\" is interpreted as stating that God contained every fact, which Kabay argues would result in trivialism, but Kabay admits that mainstream Cusa scholars would not agree with interpreting Cusa as a trivialist. Kabay also mentions Spinoza as a philosopher whose views resemble trivialism. Kabay argues Spinoza was a trivialist because Spinoza believed everything was made of one substance which had infinite attributes. Kabay also mentions Hegel as a philosophers whose views resemble trivialism, quoting Hegel as stating in \"The Science of Logic\" \"everything is inherently contradictory.\"\n\nJody Azzouni is a purported advocate of trivialism in his article \"The Strengthened Liar\" by claiming that natural language is trivial and inconsistent through the existence of the liar paradox (\"This sentence is false\"), and claiming that natural language has developed without central direction. It is heavily implied by Azzouni that every sentence in any natural language is true.\n\nThe Greek philosopher Anaxagoras is suggested as a possible trivialist by Graham Priest in his 2005 book \"Doubt Truth to Be a Liar\". Priest writes, \"He held that, at least at one time, everything was all mixed up so that no predicate applied to any one thing more than a contrary predicate.\"\n\nLuis Estrada-González in \"Models of Possiblism and Trivialism\" lists eight types of anti-trivialism (or non-trivialism) through the use of possible worlds:\n\n\n"}
{"id": "1344849", "url": "https://en.wikipedia.org/wiki?curid=1344849", "title": "Weltschmerz", "text": "Weltschmerz\n\nWeltschmerz (from the German, literally \"world-pain\", also \"world weariness\", ) is a term coined by the German author Jean Paul and denotes the kind of feeling experienced by someone who believes that physical reality can never satisfy the demands of the mind. This kind of world view was widespread among several romantic and decadent authors such as Lord Byron, Oscar Wilde, William Blake, the Marquis de Sade, Charles Baudelaire, Giacomo Leopardi, Paul Verlaine, François-René de Chateaubriand, Alfred de Musset, Mikhail Lermontov, Nikolaus Lenau, Hermann Hesse, and Heinrich Heine.\n\nFrederick C. Beiser defines \"Weltschmerz\" more broadly as \"a mood of weariness or sadness about life arising from the acute awareness of evil and suffering\", and notes that by the 1860s the word was used ironically in Germany to refer to oversensitivity to those same concerns.\n\nJohn Steinbeck wrote about this feeling in \"The Winter of Our Discontent\" and referred to it as the \"Welshrats\"; and in \"East of Eden\", Samuel Hamilton feels it after meeting Cathy Trask for the first time. Ralph Ellison uses the term in \"Invisible Man\" with regard to the pathos inherent in the singing of spirituals: \"beneath the swiftness of the hot tempo there was a slower tempo and a cave and I entered it and looked around and heard an old woman singing a spiritual as full of Weltschmerz as flamenco\". Kurt Vonnegut references this feeling in his novel \"Player Piano\"; it is felt by Doctor Paul Proteus and his father. In Henry Miller's \"Tropic of Cancer\", he describes an acquaintance, \"Moldorf\", who has prescriptions for Weltschmerz on scraps of paper in his pocket.\n"}
{"id": "45337232", "url": "https://en.wikipedia.org/wiki?curid=45337232", "title": "World Woman", "text": "World Woman\n\nWORLD WOMAN is a festival of art and activism in Oslo founded in 2015 by filmmaker and activist Deeyah Khan. The festival highlights the voices of artists and activists from around the world and promotes courage, creativity and compassion through human rights, freedom of expression, equality and peace. The event was produced by Fuuse and it took place on the 30th and 31 January 2015 in the Riksscenen with the support of Norway's Ministry of Foreign Affairs. WORLD WOMAN is planned as an annual event.\n\nWorld Woman 2015 theme was Courage and Creativity.\n\nThe inaugural World Woman festival featured performances by Mari Boine, Marilyn Mazur, John McLaughlin, Mahsa Vahdat, Sister Fa, Pakistani dancer Sheema Kermani, founder of Tehrik-e-Niswan, Fahmida Riaz, Viktoria Mullova, Gary Husband, Bugge Wesseltoft, Eivind Aarset, Hela Fattoumi, Arve Henriksen.\n\nWorld Woman conversations and talks included Shirin Ebadi, Nawal El Saadawi, Hina Jilani, Sanam Naraghi-Anderlini, International Civil Society Action Network (ICAN), Gro Brundtland, Scilla Elworthy, Şafak Pavey, Mona Eltahawy, Gurpreet Kaur Bhatti, Kenan Malik, Ayşe Önal, Sister Rosemary Nyirumbe, Fawzia Koofi, Farida Shaheed, Natalia Koliada of Belarus Free Theatre, Leyla Hussein, Yanar Mohammed, Gabrielle Rifkind, Rana Husseini.\n\nMessages of support and solidarity were recorded by Patrick Stewart, Erna Solberg, Sinead O'Connor, Kofi Annan, Sting and Richard Branson. \n\nWORLD WOMAN was covered in \"Volkskrant\", \"The Guardian\", \"Huffington Post\", \"The Independent\" and on BBC4's \"Woman's Hour\" as well as the Norwegian press.\n\n"}
