{"id": "47645554", "url": "https://en.wikipedia.org/wiki?curid=47645554", "title": "ARC3 Survey", "text": "ARC3 Survey\n\nThe ARC3 (Administrator-Researcher Campus Climate Collaborative) Survey is a campus climate survey developed to assess perpetration and victimization of sexual misconduct on college campuses in the United States. In addition to measuring rates of sexual assault on campus, the survey also gathers data on those who are engaging in sexual assault. It was developed by a group of sexual assault researchers and student affairs professionals in response to the White House Task Force to Protect Students from Sexual Assault. The survey is free for college campuses to use. The study has been used to assess both graduate and undergraduate students.\n\nCollege sexual assault is common. Estimates of the number of women assaulted on college campuses have shown that some 15 to 20 percent of college women report rape or attempted rape during their college career, and that over 50 percent report experiencing some form of unwanted sexual contact. Not only are survivors of sexual violence at an increased risk for anxiety, depression, PTSD, substance abuse, and various other physical health problems, but they are also at risk of poorer academic performance. Additionally, according to the \"AAUP Report on Sexual Assault,\" other academic consequences of sexual assault include: significant declines in academic achievement; impaired ability to carry a normal course load; increased frequency of missing classes; reduced capacity to contribute to the campus community; and an increased likelihood of dropping courses, leaving school, or transferring.\n\nStudents in the United States may apply Title IX of the Education Amendments of 1972 in order to successfully sue universities for \"indifference to known situations of harassment.\" In addition, Title IX is used to support initiatives to prevent sexual assault on college campuses in the United States. Because of various federal investigations, many universities turned to surveys and apps that were often expensive, spending \"six figures on a product that promised to address the problem.\"\n\nWith the increase of attention on college sexual misconduct, the White House presented on September 19, 2014 the launch of the \"It's On Us\" Campaign. President Barack Obama and Vice President Joe Biden introduced the campaign by describing the importance of ending sexual assault on college campuses. The \"It's On Us\" Campaign is leading a pledge to \"not be a bystander to the problem [of sexual assault], but to be a part of the solution.\" The campaign emphasizes the idea that everyone is responsible for taking a stand in ending sexual violence.\n\nThe ARC3 Survey was developed in response to the White House initiative in hopes to better evaluate data about sexual misconduct on college campuses.\n\nThe ARC3 survey was also created in order to ensure that there was no profit motive behind the campus surveys, many of which have been very expensive for colleges to purchase and implement. Jennifer Freyd, a researcher at the University of Oregon, has been instrumental in spear-heading the ARC3 survey initiative.\n\nThis survey is the result of ongoing efforts by student and legal affairs professionals, campus advocates, students, campus law enforcement, and sexual assault and harassment researchers, groups of whom met in Atlanta, Georgia in October 2014 for the Georgia State University Forum on Campus Sexual Assault and in February 2015 in Madison, Wisconsin for the Madison Summit on Campus Climate and Sexual Misconduct. The collaborators who collectively designed the survey and their titles and institutions are listed below:\n\nAccording to the ARC3 Survey website hosted by Georgia State University the ARC3 Survey was created with the following principles in mind:\n\nThe ARC3 Survey takes approximately 30 minutes to complete and has a module-based structure which makes it flexible to campus needs and legislative mandates moving forward, while maintaining validity of measurement. The following is a list of module topics obtained from the ARC3 Website:\n"}
{"id": "22655119", "url": "https://en.wikipedia.org/wiki?curid=22655119", "title": "Abhibhavayatana", "text": "Abhibhavayatana\n\nAbhibhāvayatana (, ), or abhibhāyatana (, , \"control of perceptions\"), is a concept in Buddhism through which meditation is achieved in eight stages by mastering the senses. During this process, the practitioner separates himself from the physical world, frees himself from attachments to physical forms, and begins freeing himself from pain and pleasure of the material world tied to suffering in Buddhism.\n\nThe eight stages of abhibhāvayatana are:\n\n"}
{"id": "60491", "url": "https://en.wikipedia.org/wiki?curid=60491", "title": "Abstraction (computer science)", "text": "Abstraction (computer science)\n\nIn software engineering and computer science, abstraction is:\n\n\nAbstraction, in general, is a fundamental concept to computer science and software development. The process of abstraction can also be referred to as modeling and is closely related to the concepts of \"theory\" and \"design\". Models can also be considered types of abstractions per their generalization of aspects of reality.\n\nAbstraction in computer science is also closely related to abstraction in mathematics due to their common focus on building abstractions as objects, but is also related to other notions of abstraction used in other fields such as art.\n\nAbstractions may also refer to vehicles, features, or rules of computational systems or programming languages that carry or utilize features of or abstraction itself, such as:\n\n\nComputing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others. The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.\n\nA central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. \"Modeling languages\" help in planning. \"Computer languages\" can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.\n\nWithin a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.\n\nSome abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are \"leaky\" — that they can never completely hide the details below; however, this does not negate the usefulness of abstraction.\n\nSome abstractions are designed to inter-operate with other abstractions - for example, a programming language may contain a foreign function interface for making calls to the lower-level language.\n\nIn simple terms, abstraction is removing irrelevant data so a program is easier to understand.\n\nDifferent programming languages provide different types of abstraction, depending on the intended applications for the language. For example:\n\n\nAnalysts have developed various methods to formally specify software systems. Some known methods include:\n\n\nSpecification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The UML specification language, for example, allows the definition of \"abstract\" classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.\n\nProgramming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:\n\nTo a human, this seems a fairly simple and obvious calculation (\"one plus two is three, times five is fifteen\"). However, the low-level steps necessary to carry out this evaluation, and return the value \"15\", and then assign that value to the variable \"a\", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of \"15\" to the variable labeled \"a\", so that \"a\" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of \"15\" to that memory location, etc.\n\nWithout control abstraction, a programmer would need to specify \"all\" the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:\n\n\nStructured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.\n\nIn a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.\n\nIn a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:\n\n\nThese layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.\n\nData abstraction enforces a clear separation between the \"abstract\" properties of a data type and the \"concrete\" details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the \"interface\" to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.\n\nFor example, one could define an abstract data type called \"lookup table\" which uniquely associates \"keys\" with \"values\", and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.\n\nOf course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a \"contract\" on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.\n\nIn object-oriented programming theory, abstraction involves the facility to define objects that represent abstract \"actors\" that can perform work, report on and change their state, and \"communicate\" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of \"data type\" from earlier programming languages to associate \"behavior\" most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction. When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.\n\nVarious object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime. This would leave only a minimum of such bindings to change at run-time.\n\nCommon Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.\n\nC++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.\n\nAlthough these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.\n\nConsider for example a sample Java fragment to represent some common farm \"animals\" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an codice_5 class to represent both the state of the animal and its functions:\nWith the above definition, one could create objects of type Animal and call their methods like this:\nIn the above example, the class \"codice_5\" is an abstraction used in place of an actual animal, \"codice_7\" is a further abstraction (in this case a generalisation) of \"codice_5\".\n\nIf one requires a more differentiated hierarchy of animals — to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives — that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.\n\nSuch an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.\n\nDecisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysis—actually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.\n\nIn general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.\n\nWhen discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.\n\nAbstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth modulo \"n\", we need only perform all operations modulo \"n\" (a familiar form of this abstraction is casting out nines).\n\nAbstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from them—even though the abstraction may simply yield a result of undecidability. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer \"I don't know\".\n\nThe level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of \"abstraction gradient\" in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.\n\nAbstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer \"I don't know\" to some questions).\n\nAbstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.\n\nComputer science commonly presents \"levels\" (or, less commonly, \"layers\") of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.\n\nEach relatively abstract, \"higher\" level builds on a relatively concrete, \"lower\" level, which tends to provide an increasingly \"granular\" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.\n\nSince many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:\n\nPhysical level: The lowest level of abstraction describes \"how\" a system actually stores data. The physical level describes complex low-level data structures in detail.\n\nLogical level: The next higher level of abstraction describes \"what\" data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.\n\nView level: The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.\n\nThe ability to provide a design of different levels of abstraction can\n\n\nSystems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.\n\nLayered architecture partitions the concerns of the application into stacked groups (layers).\nIt is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.\n\n\n"}
{"id": "34299735", "url": "https://en.wikipedia.org/wiki?curid=34299735", "title": "Accountability in Research", "text": "Accountability in Research\n\nAccountability in Research is a peer-reviewed academic journal, published by Taylor & Francis, examining systems for ensuring integrity in the conduct of biomedical research. The editor-in-chief is Adil E. Shamoo (University of Maryland School of Medicine).\n"}
{"id": "5376779", "url": "https://en.wikipedia.org/wiki?curid=5376779", "title": "Active shooter", "text": "Active shooter\n\nActive shooter or active killer names the perpetrator of a type of mass murder marked by rapidity, scale, randomness, and often suicide. The United States Department of Homeland Security defines an \"active shooter\" as \"an individual actively engaged in killing or attempting to kill people in a confined and populated area; in most cases, active shooters use firearms and there is no pattern or method to their selection of victims.\"\n\nMost incidents occur at locations in which the killers find little impediment in pressing their attack. Locations are generally described as \"soft targets\", that is, they carry limited security measures to protect members of the public. In most instances, shooters commit suicide, are shot by police, or surrender when confrontation with responding law enforcement becomes unavoidable, and active shooter events are often over in 10 to 15 minutes. According to New York City Police Department (NYPD) statistics, 46percent of active shooter incidents are ended by the application of force by police or security, 40percent end in the shooter's suicide, 14percent of the time the shooter surrenders and, in less than 1percent of cases, the violence ends with the attacker fleeing.\n\nIn police training manuals, the police response to an active shooter scenario is different from hostage rescue and barricaded suspect situations. Police officers responding to an armed barricaded suspect often deploy with the intention of containing the suspect within a perimeter, gaining information about the situation, attempting negotiation with the suspect, and waiting for specialist teams like SWAT.\n\nIf police officers believe that a gunman intends to kill as many people as possible before committing suicide, they may use a tactic like Immediate Action Rapid Deployment.\n\nThe terminology \"active shooter\" is critiqued by some academics. There have been several mass stabbings that have high casualty counts, for instance in Belgium (Dendermonde nursery attack), Canada (2014 Calgary stabbing), China (2008 Beijing Drum Tower stabbings), Japan (Osaka School Massacre and Sagamihara stabbings), and Pennsylvania (Franklin Regional High School stabbing). Ron Borsch recommends the phrase \"rapid mass murder\". Due to a worldwide increase in firearm and non-firearm based Mass Casualty Attacks (including attacks with vehicles, explosives, incendiary devices, stabbings, slashing, acid attacks), Dr. Tau Braun and the Violence Prevention Agency (VPA) has encouraged the use of the more accurate descriptor Mass Casualty Attacker (MCA).\n\nActive shooters do not negotiate, killing as many people as possible, often to gain notoriety. Active shooters generally do not lie in wait to battle responding law enforcement officers. Few law enforcement officers have been injured responding to active shooter incidents; fewer still have been killed. As noted, more often than not, when the prospect of confrontation with responding law enforcement becomes unavoidable, the active shooter commits suicide. And when civilians—even unarmed civilians—resist, the active shooter crumbles.\n\nBorsch's statistical analysis recommends a tactic: aggressive action. For law enforcement, the tactical imperative is to respond and engage the killer without delaythe affected orthodoxy of cumbersome team formations fails to answer the rapid temporal dynamics of active shooter events and fails to grasp the nature of the threat involved. For civilians, when necessity or obligation calls, the tactical mandate is to attack the attacker—a strategy that has proved successful across a range of incidents from Norina Bentzel (William Michael Stankewicz) in Pennsylvania and Bill Badger in Arizona (2011 Tucson shooting) to David Benke in Colorado.\n\nThe active shooter incidents do not always meet the classification of a mass murder. Mass murders result when three or more individuals are killed. The definition of an active shooter does not require that victims are killed, only that they are injured or wounded by the act. Not all mass murderers are active shooters. Noting the similarities and differences among several types of mass murder will help to isolate and define what is meant by the term \"active shooter\".\n\nMass murderers defy traditional criminal categorization. The goal of the mass murderer is neither to defend nor appropriate turf or territory, neither to initiate himself into nor elevate his status within a criminal organization. The mass murderer does not kill for drugs or money. The serial killer is one kind of mass murderer. He claims many lives in multiple events across time. The events are discontinuous, punctuated by \"a cooling-off period\". By contrast, the active shooter claims many lives in a single event along a compressed frame of time. In practice, this appears to carry a corollary: broadly, the serial killer seeks anonymity; the active shooter, notoriety. Repetition through multiple events across time answers the pathology of the serial killer. Savage as they are, his acts are not designed to excite publicity. The serial killer will conceal a corpse or bury evidence. He wants to kill again. By contrast, the active shooter seeks infamy through slaughter. He means to fuse his name forever to a place, a date, an event. Thus, his acts are designed to maximize publicity. Accordingly, he (generally) plans no escape.\n\nThe serial killer murders at close quarters, delighting in experiencing the horror of his victims as he shares their space. In his distorted estimation, his victims \"mean\" something to him, and he may secure keepsakes from victims to memorialize the \"relationship\". The active shooter also murders at close quarters. He delights in experiencing the horror of his victims as he shares their space. Crucially, however, while the victims of the serial killer \"mean\" something to him, to the active shooter they mean nothing. The active shooter moves rapidly from one victim to the next.\n\nModell speculates that the contrast is rooted in the peculiar form of abuse each suffers. The serial killer is a victim of physical/sexual/emotional abuse. Such abuse is administered at length, over time, by those who, by relation or connection, should care for the abused. The serial killer, after a manner, models this behavior. The active shooter is a victim of bullying. Though bullying may persist over time, it is delivered in discrete, relatively short-lived acts, often by multiple actors with no special relation or connection to the abused. The active shooter, after a manner, models this behavior.\n\nA portrait of the active shooter may be sharpened by contrasting him with another type of mass murderer, the ideological killer. The Oklahoma City bombing exemplifies the type: in 1995, a man with ties to a disorganized militia movement detonated a truck bomb in front of the Alfred P. Murrah Building in downtown Oklahoma City, and the resulting blast ended 168lives while wounding680.\n\nThe ideological killer is driven by adherence to ethico-political or religious orthodoxy. His actions are an expression of that orthodoxy. By contrast, while the active shooter may conceive of himself as \"making a statement\" of sorts, his motives appear more personal and desultory.\n\nLike the active shooter, the ideological killer plans multiple murders within the confines of a single event. But the active shooter seeks to experience the horror of his victims at close quarters. The ideological killer does not. He kills at a distance. He plants explosive devices or takes up position as a sniper. Killing at a distance suits his primary motive—the expression of adherence to an abstract orthodoxy. For the ideological killer, victims are of incidental significance. He need not \"know\" them, as the serial killer must, nor experience their horror, as the active shooter must. For the ideological killer, abstraction is reality, the individual but a construct in an interminable struggle of dogmas.\n\nSince he is motivated by adherence to orthodoxy, the ideological killer typically seeks notoriety for his cause through carnage. While the active shooter too seeks notoriety, and while he too maps a careful plan of the event, there is a crucial difference in the pursuit. The ideological killer generally means to elude capture, to live beyond the event (as does the serial killer). The active shooter merges his identity with the event and sees nothing beyond.\n\nIntegrating the elements elicited by comparison and contrast, the \"active shooter (killer)\" may be defined as a mass murderer who kills (or attempts to kill) at close-quarters, in multiples, at random in a single, planned event.\n\nAccounts of why active shooters do what they do vary. Some contend that the motive, at least proximately, is vengeance. Others argue that bullying breeds the problem, and sometimes the active shooter is a victim of bullying, directly or derivatively. Still others such as Grossman and DeGaetano argue that the pervasiveness of violent imagery girding modern culture hosts the phenomenon.\n\nSome argue that a particular interpretation of the world, a conscious or subconscious ontology, accounts for the phenomenon. They argue that the active shooter lives in a world of victims and victimizers, that all are one or the other. The ontology accommodates no nuance, no room between the categories for benevolence, friendship, decency, nor indeed, for a mixture of good and bad. His interpretation of the world may grow out of or be fed by bullying or violent imagery (hence the common obsession with violent movies, books or video games), but it is the absolutist interpretation of his world that drives him both to kill and to die.\n\nIn \"The Psychology of the Active Killer\", Daniel Modell writes that \"The world conceived by the active killer is a dark dialectic of victim and victimizer. His impoverished ontology brooks no nuance, admits no resolution. The two categories, isolated and absolute, exhaust and explain his world. And the peculiar logic driving the dialectic yields a fatal inference: in a world of victims and victimizers, success means victimization.\"\n\nMany government installations, businesses, campuses have camera and sensor based security systems, some analog and, more recently, digital. Some also include formal Command Centers or at least are monitored by personnel in any organization. These provide very good forensic support in documenting an active shooter. However they offer only mixed results in the timely interdiction of an active shooter.\n\n\n"}
{"id": "44959719", "url": "https://en.wikipedia.org/wiki?curid=44959719", "title": "Adhyavasaya", "text": "Adhyavasaya\n\nAdhyavasāya (Sanskrit: अध्यवसाय ) generally means – 'apprehension', 'clinging to', 'mental effort' and also 'perseverance', 'having decided'.\n\nNārāyana Tīrtha, in his \"Sāmkhya-Candrikā\", explains \"adhyavasāya\" as \" a modified condition of the intellect (\"buddhi\") the determination in such a form as – \"such an act is to be done by me\" \". Gaudapāda explains it as \" intellectual determination of the object of perception as belonging to a definite class \". Vācaspatimiśra explains it as \" ascertainment or determinate knowledge consequent upon the manifestation of the essence (\"sattva\") of the intellect, when the inertia of the intellect is overcome by the operation of the sense-organs in apprehending their objects, as the intention which follows self-appropriated knowledge or volition of the agent to react to the object of perception in a definite way \". Ascertainment also means definite awareness which according to the Samkhya school is associated in \"buddhi\" (अध्यवसायो बुद्धिः), in perception which is definite ascertainment involving the senses with respect to specific kinds of empirical objects, and which is an activity of \"buddhi\". It implies determination of objects (\"avidhāranā\") which by itself is decisive knowledge (\"niścaya\").\n\nThe sage of the Maitri Upanishad (VI.30) explains:-\n\nthe opposite of this is liberated. \"\n\nVindhyāvasin holds the view that \"sankalpa\", \"abhimāna\" and \"adhyavasāya\" are not different functions but the modifications of the mind or intellect. Samkhyas hold the view that perception is the reflective discerning (\"adhyavasāya\"), a kind of awareness, an operation of the intellect in its \"sattva\" modality; to be aware of perception as perception is to define and distinguish it from others which definition (\"adhyavasāya\") can come only through conception (\"vikalpa\"). Sadyojyoti defines discernment as the ascertaining cognitive activity (\"adhyavasāya\") which is understanding (\"jñāna\").\n\nAccording to the Yoga school and Ayurveda, \"adhyavasāya\" i.e. apprehension of clear and clean knowledge of the knowable, is one of the sixteen qualities of the empirical soul (\"jīvātmān\"). Lakshmi Tantra (XVI.4) explains that the intellect (\"buddhi\") is the incentive to mental effort (\"adhyavasāya\"); \"prana\" is the incentive to endeavour (\"prayatna\") and time (\"kāla\") is the incentive to transform in the form of impulse and effective development; whereas ego (\"ahamkara\") results from a modification of \"mahat\" in which cognition is indeterminate cognition (\"nirvikalpa adhyavasāya\").\n\nThe followers of Jainism consider \"adhyavasāya\" referring to good or bad sentiments, and of the nature of \"Bhava-Yoga\" which is made up of 'inauspicious combined meditation' and 'meditation'. Utpala in his following statement:-\n\nterms - \"adhyavasāya\" as mental apprehension as he proceeds to explain \"māyā-shakti\" to be the differentiating power of Brahman that affects consciousness resulting in the false perception of duality. And, Ratnakīrti holds the view that non-momentary existence is a figment of imaginative intuition projected as real by the process of intellection called \"adhyavasāya\" which is impulsive movement of the mind generated by the force of immediately preceding cognition. The Buddhist view is that the judgment of the object of cognition ('spatio-temporal objects') owing to special synthesis of moments gains sensible qualities etc., to become universal (\"sāmānaya lakshana\", \"ekatva adhyavasāya\"). Jñanasrimitra holds the view that only deluded persons interpret an image by conceptual thinking. (\"adhyavasāya\", \"vikalpa\").\n\nCommon characteristics (\"sādhāranadharma\") in poetry can either be in the form of existence (\"bhāva\") or negation (\"abhāva\"), and can involve supersession (\"adhyavasāya\"). In \"apahnuti\" (concealment of truth, denial), when a thing is concealed and something else is described in its place, the \"āropa\" (raising, elevating, superimposition) that is based on negation amounts to \"adhyavasāya\", which view is refuted by Jayaratha who agrees with Ruyyaka that \"adhyavasāya\" is the phenomenon where the \"visayah\" is concealed and its \"non-difference (abheda) \" with the \"vishayin\" is known, and that doubt (\"bhrānti\") is always based on \"adhyavasāya\" and not on \"āropa\". In Sanskrit literature, \"Alankāraśāstra\" deals with the beauty which poetry (\"kāvya\") alone can display. \"Adhyavasāya\" is considered as \"siddha\" (accomplished) when the object is not expressed in words but is lost in comparison, it is considered as \"sādhya\" (perfect) when the object in the process of being represented is identical with the object of comparison. It distinguishes \"utprekśa\" ('alliteration') and \"atiśayokti\" ('hyperbole') from \"rūpaka\" ('dramatic representation', 'form') and \"āropa\" ('superimposition').\n"}
{"id": "41986762", "url": "https://en.wikipedia.org/wiki?curid=41986762", "title": "Akrodha", "text": "Akrodha\n\nAkrodha (Sanskrit: अक्रोध) literally means \"free from anger\". It is considered an important virtue in Indian philosophy.\n\nAkrodha is a fusion word \"a\" (अ, without, non) and \"krodha\" (क्रोध, anger), or 'without anger'. A related word is \"Akrodhah\" (Sanskrit: अक्रोध:), which also means 'absence of anger'.\n\n\"Akrodha\" is considered a virtue and desirable ethical value in Hinduism. When there is cause of getting angry but even then there is absence of anger, it is non-anger or \"akrodha\". Absence of anger (\"akrodha\") means being calm even when insulted, rebuked or despite great provocation. \"Akrodha\" does not mean absence of causes of anger, it means not getting angry and keeping an even, calm temper despite the circumstances.\n\n\"Krodha\" ('anger') is excessive mental turmoil on account of the obstacles in the gratification of some desire; it is manifestation of the quality of tamas (dark, negative, destructive), an undesirable psychological state. The opposite of \"Krodha\" is \"Akrodha\", and this is a productive, positive and constructive state.\n\nBhawuk states that \"akrodha\" is necessary to any process of peace. Peace and happiness is a state of contentment (santustah), where there is absence of spite or envy (advestah), absence of anger (akrodhah), and absence of violence (ahimsa). \"Dharma\" relies on \"Akrodha\", because it creates an environment of serenity, a rational principle of life, and because it is a moral virtue inspired by love.\n\nAccording to Vedic sages, when work becomes akin to a \"yajna\" (a worship ceremony), the effect of that work is transformed into \"apurva\", that is, it becomes something unique, unprecedented and empowering. In contrast, anger clouds reason, which results in the loss of discrimination between right and wrong and virtue and vice. When the discriminating faculty is ruined, the person loses self-identity and the inner good perishes. With freedom from anger, a person reaches an \"apurva\" state.\n\nNarada Parivrajaka Upanishad states the nature of \"akrodha\" for a person who seeks self-knowledge and liberation (kaivalya) as follows,\n\n\"Akrodha\", states Manickam, is related to the concept \"Sahya\" (Sanskrit: सह्य) in the Upanishads. \"Sahya\" means, depending on the context, to bear, endure, suffer, and put up with. The quality to \"Sahya\" is considered an ethical value in Hinduism, not out of weakness to react, but for the cause of the ultimate \"Truth\". It is the attribute by which a person willingly bears negative cognitive inputs in order to \"win over\" the opponent or whatever is offensive, in the pursuit of holding on to Truth, in order to achieve oneness with Brahman, the ultimate Truth. This endurance, this strive to overcome the adversaries, through \"akrodha\" and \"ahimsa\", is recommended as the constructive way in one's pursuit of \"Truth\".\n\nThe Hindu epic Mahabharata repeatedly emphasizes the virtue of \"akrodha\". For example, in Adi Parva, it states\n\nIn Vana Parva, the Mahabharata states\n\nIn Shanti Parva, the Mahabharata states\n\nThe Bhagavad Gita (Slokas XVI.1–3), in the Mahabharata, gives a list of twenty-six divine attributes beginning with \"abhayam\" ('fearlessness') and \"sattva sansuddhih\" ('purity of mind'), ending with \"adroha\" ('bearing enmity to none') and \"naatimaanita\" ('absence of arrogance'):\n\n\"Akrodha\" is one of the twenty six divine attributes a person can have, states Bhagavad Gita.\n\nManu has listed \"Akrodha\" ('absence of anger') among the ten primary virtues. The \"Apastambhadharmasutra\" (I.iii.22) rules that a student be not given to anger, and that a house-holder is required to abstain from anger and abstain from action or words that would provoke someone else to anger (II.xviii.2). The \"Baudhayanadharmasutra\" (I.xv.30) requires a house-holder never to be angry, and the \"Gautamdharmasutra\" (II.13) advises that he must not feel angry. The \"Vashisthadharmasutra\" (IV.4) avers that refraining from anger is a virtue like truthfulness, charity among others.\n\nManu mentions ten \"Dharma Lakshanas\", \"akrodha\" is one of these \"lakshana\" (attribute, sign of a dharmic person). The other nine are: Dhriti (patience), Kshama (forgiveness), Damah (temperance), Asteya (non-stealing), Shaucham (purity), Indriyaigraha (freedom from sensual craving), Dhi (reason), Vidya (knowledge), and Satyam (truth).\n\nThe Shaivite doctrine considers four \"yamas\" for the \"Pashupata\" ascetic who smears on his body \"bhasam\"; the four \"yamas\" are – non-injury, celibacy, truthfulness and non-stealing; the \"niyamas\" consist of non-irritability (\"akrodha\"), attendance on the teachers, purity, lightness of diet and carefulness (\"apramada\"). \"Akrodha\" is a virtue.\n\nHinduism and Buddhism both suggest ten freedoms needed for good life. These are – \"Ahimsa\" ('freedom from violence'), \"Asteya\" ('freedom from want, stealing'), \"Aparigraha\" ('freedom from exploitation'), \"Amritava\" ('freedom from early death') and \"Arogya\" ('freedom from disease'), \"Akrodha\" ('freedom of anger'), \"Jnana\" or \"Vidya\" (\"freedom from ignorance\"), \"Pravrtti\" (\"freedom of conscience\"), \"Abhaya\" ('freedom from fear') and \"Dhrti\" ('freedom from frustration and despair').\n\n"}
{"id": "4118207", "url": "https://en.wikipedia.org/wiki?curid=4118207", "title": "Anticipatory bail", "text": "Anticipatory bail\n\nUnder Indian criminal law, there is a provision for anticipatory bail under Section 438 of the Criminal Procedure Code. Law Commission of India in its 41st report recommended to incorporate this provision in procedure code. This provision allows a person to seek bail in anticipation of an arrest on accusation of having committed a non-bailable offence.\n\nOn filing anticipatory bail, the opposing party is notified about the bail application and the opposition can then contest the bail application in court (public prosecutor can also be used to do this).\n\nAnticipatory bail is a direction to release a person on bail, issued even before the person is arrested.\n\nWhen any person apprehends that there is a move to get him arrested on false or trumped up charges, or due to enmity with someone, or he fears that a false case is likely to be built up against him, he has the right to move the court of Session or the High Court under section 438 of the code of Criminal Procedure for grant of bail in the event of his arrest, and the court may, if it thinks fit, direct that in the event of such arrest, he shall be released on bail.Anticipatory Bail can be granted by Sessions Court, High Court and Supreme Court.\n\nThe High Court or the court of session may include such conditions in the light of the facts of the particular case, as it may think fit, including: \n\n\nIf such person is thereafter arrested, and is prepared either at the time of arrest or at any time while in the custody of such officer to give bail, he shall be released on bail and the magistrate taking cognizance of such offence decides that warrant should be issued against that person, he shall issue a bailable warrant in conformity with the direction of the court granting anticipatory bail.\nSupreme Court while dealing the case of Sidhram Mhetre, held certain conditions imposed by High Court to be not required & contrary to provisions of anticipatory bail.\n\nThe applicant must show by disclosing special facts and events that he has reason to believe, that he may be arrested for a non-bailable offence so that the court may take care to specify the offence or offences in respect of which alone the order will be effective and it is not a blanket order covering all other offences.\n\nAn accused is free on bail as long as the same is not cancelled. The High Court or Court of Session may direct that any person who has been released on bail be arrested and commit him to custody on an application moved by the complainant or the prosecution.\n\n"}
{"id": "19712256", "url": "https://en.wikipedia.org/wiki?curid=19712256", "title": "Arborloo", "text": "Arborloo\n\nAn arborloo is a simple type of composting toilet in which feces are collected in a shallow pit and a fruiting tree is later planted in the fertile soil of the full pit. Arborloos have a pit like a pit latrine but less deep, a concrete slab, superstructure (toilet house or outhouse) to provide privacy and possibly a ring beam to protect the pit from collapsing.\n\nThe arborloo works by temporarily putting the slab and superstructure above a shallow pit while this pit fills. When the pit is nearly full, the superstructure and slab is moved to a newly dug pit and the old pit is covered with the earth got by digging the new pit and left to compost. The old pit serves as a bed for a fruit tree or some other useful vegetation, which is preferably planted during the rainy season.\n\nThe arborloo is a type of dry toilet. In using the nutrient-rich soil of a retired pit, the arborloo, in effect, treats feces as a resource rather than a waste product. Arborloos are used in rural areas of many developing countries, for example in Zimbabwe, Malawi and Ethiopia.\n\nThe defecation pit may be circular or square and this may depend on the slab and superstructure. A circular pit is less likely to collapse. The pit of the arborloo is shallow (between 1-1.5 meter).\n\nIf the pit is dug by hand it must have a diameter of at least 0.9 meters to accommodate effective digging. The pit should not be wider than the slab and must allow for 0.1 meter bearing around the edge.\n\n"}
{"id": "26276222", "url": "https://en.wikipedia.org/wiki?curid=26276222", "title": "Bateman's principle", "text": "Bateman's principle\n\nBateman's principle, in evolutionary biology, is that in most species, variability in reproductive success (or reproductive variance) is greater in males than in females. It was first proposed by Angus John Bateman (1919–1996), an English geneticist. Bateman suggested that, since males are capable of producing millions of sperm cells with little effort, while females invest much higher levels of energy in order to nurture a relatively small number of eggs, the female plays a significantly larger role in their offspring's reproductive success. Bateman’s paradigm thus views females as the limiting factor of parental investment, over which males will compete in order to copulate successfully.\n\nAlthough Bateman's principle served as a cornerstone for the study of sexual selection for many decades, it has recently been subject to criticism. Attempts to reproduce Bateman's experiments in 2012 and 2013 were unable to support his conclusions. Some scientists have criticized Bateman's experimental and statistical methods, or pointed out conflicting evidence, while others have defended the veracity of the principle and cited evidence in support of it.\n\nTypically it is the females who have a relatively larger investment in producing each offspring. Bateman attributed the origin of the unequal investment to the differences in the production of gametes: sperm are cheaper than eggs. A single male can easily fertilize all females' eggs; she will not produce more offspring by mating with more than one male. A male is capable of fathering more offspring if he mates with several females. By and large, a male's potential reproductive success is limited by the number of females he mates with, whereas a female's potential reproductive success is limited by how many eggs she can produce. According to Bateman's principle, this results in sexual selection, in which males compete with each other, and females become choosy in which males to mate with. Thus, as a result of being anisogamous, males are fundamentally promiscuous, and females are fundamentally selective.\n\nBateman initially published his review in 1948. He was a botanist, contributing to the literature of sexual selection only once in his lifetime. Bateman initially saw his study on Drosophila to be a test of Charles Darwin’s doctrine of sexual selection. He saw Darwin’s theory of natural selection not as flawed, but as incomplete. He felt that if he were to provide a concrete demonstration of how sexual selection played a role in the reproductive success of certain species, he could explain the gap between Darwin’s ideas and sexual dimorphism.\n\nAlthough it is common to confuse Bateman's ideas with those of later scientists, his principle can be expressed in three simple statements. The first is that male reproductive success increases with the number of mates they attempt to copulate with, while female reproductive success does not. The second is that male reproductive success will show greater variance than female. The third is that sexual selection will have a greater effect on the sex with greater variance in reproductive success.\n\nThroughout his research, Bateman conducted experiments using fruit flies in order to observe their copulation and sexual behavior. A total of six series of experiments were conducted with the fruit fly \"Drosophila melanogaster\", using three to five individuals of each sex. Each trial ran for three or four days. Some ran to completion without the transfer of the Drosophila from one environment (bottle) to another. In the others, Bateman transferred the flies and their eggs to a new bottle every day. Bateman also varied the age of the flies depending on the experiment, with an age gap between one and six days total. He never watched the flies' copulations. The flies used were from several inbred strains, which meant they could be identified by their specific inbred strain. Therefore, he inferred the number of involved mates based on the number of offspring that were later found to have mutations from both a male and a female. The difficulty that arose was that if a female Drosophila had copulated with five males and only one larva survived, Bateman would not be able to account for the other four copulations.\n\nAnalysis of the data collected in sets one through four showed that the males' reproductive success, estimated as the number of sired offspring, increased at a steady rate until a total of three mates were reached. It is important to note that Bateman kept the sex ratio of males to females completely even throughout his trials. But after surpassing three mates, male reproductive success began to fall. Female reproductive success also increased with number of mates, but much more gradually than that of the males. The second series of data collected in sets five and six illustrated a dramatically different outcome. Male reproductive success increased at a steady and steep rate, never dropping. Female reproductive success, on the other hand, plateaued after a single mate. Bateman focused mainly on the second series of data when discussing his results. His main conclusion was that the reproductive success of females does not increase with an influx of mates, as one fit mate was enough to successfully complete fertilization. This is often referred to as Bateman’s Gradient.\n\nThroughout 2012 and 2013, Gowaty, Kim, and Anderson took it upon themselves to repeat Bateman's experiment in its entirety. While reproducing his tests, the same fly strains and mutations were used in order to maintain the same methodology. However, one of the 11 strains that Bateman used had gone extinct, and was thus replaced. (Tang-Martinez 2012)\n\nGowaty, Kim, and Anderson found that upon combining certain strains with one another, the offspring were unable to survive to adulthood. (Gowaty, Kim, & Anderson 2012) Thus, Bateman’s numbers regarding the number of individuals not having mated was higher than the actual number. Likewise, his estimate of those that mated with one or more mates was too low. This was valid for both the males and females of this species.\n\nGowaty desired to further explore the reasoning behind the premature death of the Drosophila. She began doing so by running monogamy trials between different strains of flies and found that 25% of the offspring died due to becoming double mutants. (Gowaty 2013) Bateman thought his work fit within the lines of Mendel’s laws of genetics, while Gowaty proved otherwise. The 1948 experiments inferred reproductive success based on the number of adults living by the end of the trial. In reality, many factors were left out of the equation when calculating reproductive success as a function of the number of mates, which had the ability to completely dislodge the accuracy behind Bateman's results. Gowaty was not able to confirm Bateman's conclusions and found no evidence for sexual selection in the experiment. (Gowaty 2013; Tang-Martinez 2012)\n\nNevertheless, some modern experiments between the relationship of number of mates and the reproductive success of males and females support Bateman's principle. Julie Collet conducted an experiment with a population of red jungle fowl. A total of thirteen replicate groups of three males and four females were monitored for ten days. In this experiment, the sex ratio was biased toward females. A male's reproductive success was calculated using the proportion of embryos fathered to the total number of embryos produced by all the females he mated with. The total sexual selection opportunity was calculated using the following formula.\n\nThe \"σ\" represents the variance in RS, while the is the square mean of reproductive success of members of one sex in a group.\n\nIn 2013, Fritzsche and Arnqvist tested Bateman's principle by estimating sexual selection between males and females in four seed beetles. They used a unique experimental design that showed sexual selection to be greater in males than in females. In contrast, sexual selection was also shown to be stronger for females in role-reversed species. They suggested that the Bateman gradient is typically the most accurate and informative measure of sexual selection between different sexes and species (Fritzsche, K. & Arnqvist, G).\n\nMore than 60 years later, Bateman's principle has received considerable attention. Sutherland argued that males' higher variance in reproductive success may result from random mating and coincidence. Hubbell and Johnson suggested that variance in reproductive success can be greatly influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies are subject to change depending on certain strategies. They argued that there are cases in which males can be more selective than females, whereas Bateman suggested that his paradigm would be “almost universal” among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.\n\nExperimental and statistical criticisms followed. Until approximately a decade ago, critics of Bateman’s model focused on his experimental design. In recent years, they have shifted attention to the actual experimental and statistical calculations Bateman published throughout his trials. Birkhead wrote a 2000 review arguing that since Bateman’s experiments lasted only three to four days, the female fruit fly, \"Drosophila melanogaster\", may not have needed to mate repeatedly, as it can store sperm for up to four days; if Bateman had used a species in which females had to copulate more often to fertilize their eggs, the results might have been different. Snyder and Gowaty conducted the first in-depth analysis of the data in Bateman’s 1948 paper. They found sampling biases, mathematical errors, and selective presentation of data.\n\nA 2012 review by Zuleyma Tang-Martínez concluded that various empirical and theoretical studies, especially Gowaty's reproduction of Bateman's original experiment, pose a major challenge to Bateman's conclusions, and that Bateman's principle should be considered an unproven hypothesis in need of further reexamination. According to Tang-Martínez, \"modern data simply don't support most of Bateman's and Trivers's predictions and assumptions.\"\n\nA 2016 review confirmed Darwinian sex roles across the animal kingdom, concluding that \"sexual selection, as captured by standard Bateman metrics, is indeed stronger in males than in females and that it is evolutionarily tied to sex biases in parental care and sexual dimorphism.\"\n\nOne error source that have been shown to give an illusion of greater differential in reproductive success in males than in females genetically is that chromosome effects cause a greater percentage of mutations to be lethal before even reaching sexual maturity in males than in females.\n\nThe assumption that any differential in reproductive success between males and females among the individuals that do reach sexual maturity must be due to sexual selection in the current population is also subject to criticism, such as the possibility of remnants of sexually selected traits in a previous species from which a new species have evolved can be negatively selected due to costs in nutrients and weakened immune systems and that such negative selection would cause a higher difference in reproductive success in males than in females even without any still ongoing sexual selection. Since lower degrees of selection during times of stable environment allows genetic variation to build up by random mutations and allow some individuals in a population to survive environmental change while strong constant selection offsets the effect and increases the risk of the entire population dying out during catastrophic environmental change due to less genetic variation, constant loss of genetic variation caused by sexual selection have been suggested as a factor contributing to higher extinction rates in more sexually dimorphic species besides the nutrient, immunity and other costs of the ornaments themselves. While the ornament cost risk would only be removed when the ornaments have been eliminated by selection, the genetic variation model predicts that the species ability to survive would improve significantly even at an early stage of reduction of sexual dimorphism due to other adaptive mutations arising and surviving due to minimal selection during times of stable environment while the genes causing sexually dimorphic anatomy have only in small part been affected by the mutations. Applied to human evolution, this model can explain why early Homo sapiens display a significantly increased adaptability to environmental change already at its early divergence from Homo erectus that had a high muscular sexual dimorphism, as well as why human anatomy through the history of Homo sapiens show a diversification during times of stable climate and a selective loss of the more robust male forms during environmental change that does not recover during later stability, continuing through the loss of many robust characteristics in regional bottlenecks as recent as the end of the Ice Age and the time around the agricultural revolution. It also explains genetic evidence of human genetic diversity increasing during stable environmental periods and being reduced during bottlenecks related to changes in the environment.\n\nRecently, DNA testing has permitted more detailed investigation of mating behavior in numerous species. The results, in many cases, have been cited as evidence against Bateman's principle.\n\nUntil recently, most bird species were believed to be sexually monogamous. DNA paternity testing, however, has shown that in nearly 90% of bird species, females copulate with multiple males during each breeding season. The superb fairy wren is socially monogamous, but 95% of its clutches contain young fathered by extra-pair males. Up to 87% of tree swallow clutches, 75% of coal tit clutches, and 70% of reed bunting clutches contain young fathered by extra-pair males. Even female waved albatrosses, which typically mate for life, are sexually promiscuous, with 17% of young fathered by extra-pair males.\n\nIn many primate species, females solicit sex from males and may mate with more than one male in quick succession. Female lions may mate 100 times per day with different males while they are in estrus. Females of the pseudoscorpion species, \"Cordylochernes scorpioides\", have been shown to have higher reproductive success when mated with more than one male.\n\nThere are a number of claims about sex differences in humans said by evolutionary psychologists to be well supported that are not logically predicted by Bateman's principle. One example that have been pointed out is language ability that is said to be more developed in women than in men. It is argued that language, being a derived human trait, is not predicted by a model that assumes males to be more selected than females to be present to a higher degree in women than in men but the opposite. The objection about a trade-off between spatial ability and language is found lacking due to the existence of animals that outperform humans on spatial tasks such as birds of prey and bats while having much smaller and more rapidly maturing brains than humans, while great ape language is limited and ungrammatical at best despite great apes being much closer to humans in brain capacity than bats or birds of prey are, showing that the system requirements for spatial ability are much lower than those for language and therefore not a serious trade-off for them. The generalization of Bateman's principle to societies lacking modern birth control is also argued to be incompatible with such generalization of the \"nuns more successful at celibacy than monks\" clause of the female erotic plasticity model, as absence of modern contraceptives would mean that absence of reproduction for many men required that many men had no sexual relationships with women while male sexual exclusivity to a particular gender, if present, would have precluded same-sex relationships as a form of non-celibacy for men who could not find a female mate as well. It is also mentioned that the evolutionary psychologists themselves claim that women are sexually picky due to an evolutionary history without modern contraception, while it is pointed out that the notion of celibacy being impossible for men in general with only a small number of individual exceptions is incompatible with the notion of evolution making celibacy inevitable for a significant percentage of men.\n\nThe most well-known exceptions to Bateman's principle are the existence of sex-role reversed species such as pipefish (seahorses), phalaropes and jacanas in which the males perform the majority of the parental care, and are cryptic while the females are highly ornamented and territorially aggressive (; ; ). \n\nIn these species, however, the typical fundamental sex differences are reversed: females have a faster reproductive rate than males (and thus greater reproductive variance), and males have greater assurance of genetic parentage than do females . Consequently, reversals in sex roles and reproductive variance are consistent with Bateman's principle, and with Robert Trivers's parental investment theory.\n\n\n\n"}
{"id": "16112716", "url": "https://en.wikipedia.org/wiki?curid=16112716", "title": "Brewing Justice", "text": "Brewing Justice\n\nBrewing Justice: Fair Trade Coffee, Sustainability and Survival is a book by American academic Daniel Jaffee.\n\nThe book, by studying coffee farmers in Mexico, offers an investigation of the social, economic, and environmental benefits of fair trade. Based on research in Zapotec indigenous communities in the state of Oaxaca, Brewing Justice follows the members of the cooperative Michiza, whose organic coffee is sold on the international fair trade market. It compares these families to conventional farming families in the same region, who depend on local middlemen and are vulnerable to the fluctuations of the world coffee market.\n\nThe book carries readers into the lives of these coffee producer households and their communities, offering a nuanced analysis of both the effects of fair trade on everyday life and the limits of its impact. Brewing Justice paints a clear picture of the complex dynamics of the fair trade market and its relationship to the global economy. Drawing on interviews with dozens of fair trade leaders, the book also explores the changing politics of this international movement, including the challenges posed by the entry of transnational corporations into the fair trade system. It concludes by offering recommendations for strengthening and protecting the integrity of fair trade.\n"}
{"id": "15304844", "url": "https://en.wikipedia.org/wiki?curid=15304844", "title": "Charlotte Selver", "text": "Charlotte Selver\n\nCharlotte Selver (April 4, 1901 in Ruhrort (Duisburg), Germany – August 22, 2003 in Muir Beach, California; née \"Wittgenstein\") was a German music educator.\n\nThe central point of Charlotte Selver's work was \"experience through the senses\". Charlotte Selver was convinced that the well-being of the individual, the society as a whole and even the worries about our environment depend on how far we find new confidence in organic processes.\n\nWith her work, \"Sensory Awareness\", Charlotte Selver had a deciding influence on the \"Human Potential Movement\", which also came out of the Esalen Institute, where she taught as of 1963. Because of that, she also had influence on Humanistic Psychology and the therapies based on it. Aspects of her work, especially the conscious sensing of the body and the following of physical sensations (Sensory Awareness), flowed into many of the methods of physical work, physical therapy, physical psychotherapy and psychotherapy which still exist today.\n\nIn the 1920s, Charlotte Selver encountered Elsa Gindler in Berlin, who together with the students in her courses, researched how the natural gifts of people could be developed, even at an adult age. Up until she emigrated to New York in 1938, she studied with Elsa Gindler and the music teacher, Heinrich Jacoby, and she reestablished contact with them in the 1950s.\n\nIn 1971, the \"Sensory Awareness Foundation\" was brought into being. Its goal is to preserve and document Charlotte Selver's life work. In 1995, the \"California Institute of Integral Studies\" in San Francisco awarded her an honorary doctorate.\n\nCharlotte Selver died on August 22, 2003, at her home in Muir Beach, California, among her closest friends and students at the age of 102.\n\nSensory Awareness has its origin in the work of the gymnastics and exercise teacher, Elsa Gindler (1885–1961), and the Swiss music teacher, Heinrich Jacoby (1889–1964). They never gave their 'work' a formal name. The fundamental objective of the Jacoby/Gindler approach is the development of the person (integral unfolding as development and growth to meaningful being). Charlotte Selver was Gindler's student in Berlin before she emigrated to the United States in 1938 and she introduced this work under the name of Sensory Awareness.\n\nCharlotte Selver touched and encouraged thousands of people in the USA, Mexico and Europe in her 80 years of work, among them influential personalities, such as: \n\n\n\n"}
{"id": "1589660", "url": "https://en.wikipedia.org/wiki?curid=1589660", "title": "Circular flow of income", "text": "Circular flow of income\n\nThe circular flow of income or circular flow is a model of the economy in which the major exchanges are represented as flows of money, goods and services, etc. between economic agents. The flows of money and goods exchanged in a closed circuit correspond in value, but run in the opposite direction. The circular flow analysis is the basis of national accounts and hence of macroeconomics.\n\nThe idea of the circular flow was already present in the work of Richard Cantillon. François Quesnay developed and visualized this concept in the so-called Tableau économique. Important developments of Quesnay's tableau were Karl Marx' reproduction schemes in the second volume of \"\", and John Maynard Keynes' \"General Theory of Employment, Interest and Money\". Richard Stone further developed the concept for the United Nations (UN) and the Organisation for Economic Co-operation and Development to the system, which is now used internationally.\n\nThe circular flow of income is a concept for better understanding of the economy as a whole and for example the National Income and Product Accounts (NIPAs). In its most basic form it considers a simple economy consisting solely of businesses and individuals, and can be represented in a so-called \"circular flow diagram.\" In this simple economy, individuals provide the labour that enables businesses to produce goods and services. These activities are represented by the green lines in the diagram.\nAlternatively, one can think of these transactions in terms of the monetary flows that occur. Businesses provide individuals with income (in the form of compensation) in exchange for their labor. That income is spent on the goods and services businesses produce.\nThese activities are represented by the blue lines in the diagram above.\n\nThe circular flow diagram illustrates the interdependence of the “flows,” or activities, that occur in the economy, such as the production of goods and services (or the “output” of the economy) and the income generated from that production. The circular flow also illustrates the equality between the income earned from production and the value of goods and services produced.\n\nOf course, the total economy is much more complicated than the illustration above. An economy involves interactions between not only individuals and businesses, but also Federal, state, and local governments and residents of the rest of the world. Also not shown in this simple illustration of the economy are other aspects of economic activity such as investment in capital (produced—or fixed—assets such as structures, equipment, research and development, and software), flows of financial capital (such as stocks, bonds, and bank deposits),\nand the contributions of these flows to the accumulation of fixed assets.\n\nOne of the earliest ideas on the circular flow was explained in the work of 18th century Irish-French economist Richard Cantillon, who was influenced by prior economists, especially William Petty. Cantillon described the concept in his 1730 \"Essay on the Nature of Trade in General,\" in chapter 11, entitled \"The Par or Relation between the Value of Land and Labor\" to chapter 13, entitled \"The Circulation and Exchange of Goods and Merchandise, as well as their Production, are Carried On in Europe by Entrepreneurs, and at a Risk.\" Thornton eds. (2010) further explained:\nCantillon distinguished at least five types of economic agents: property owners, farmers, entrepreneurs, labors and artisans, as expressed in the contemporary diagram of the Cantillon’s Circular Flow Economy.\n\nFrançois Quesnay further developed these concepts, and was the first to visualize these interactions over time in the so-called Tableau économique. Quesnay believed that trade and industry were not sources of wealth, and instead in his 1758 book \"Tableau économique\" (Economic Table) argued that agricultural surpluses, by flowing through the economy in the form of rent, wages, and purchases were the real economic movers, for two reasons.\nThe model Quesnay created consisted of three economic agents: The \"Proprietary\" class consisted of only landowners. The \"Productive\" class consisted of all agricultural laborers. The \"Sterile\" class is made up of artisans and merchants. The flow of production and/or cash between the three classes started with the Proprietary class because they own the land and they buy from both of the other classes. Quesnay visualised the steps in the process in the Tableau économique.\n\nIn Marxian economics, economic reproduction refers to recurrent (or cyclical) processes by which the initial conditions necessary for economic activity to occur are constantly re-created.\n\nEconomic reproduction involves the physical \"production\" and \"distribution\" of goods and services, the \"trade\" (the circulation via exchanges and transactions) of goods and services, and the \"consumption\" of goods and services (both productive or intermediate consumption and final consumption).\n\nKarl Marx developed the original insights of Quesnay to model the circulation of capital, money, and commodities in the second volume of \"Das Kapital\" to show how the reproduction process that must occur in any type of society can take place in capitalist society by means of the circulation of capital.\n\nMarx distinguishes between \"simple reproduction\" and \"expanded (or enlarged) reproduction\". In the former case, no economic growth occurs, while in the latter case, more is produced than is needed to maintain the economy at the given level, making economic growth possible. In the capitalist mode of production, the difference is that in the former case, the new surplus value created by wage-labour is spent by the employer on consumption (or hoarded), whereas in the latter case, part of it is reinvested in production.\n\nAn important development was John Maynard Keynes' 1933 publication of the \"General Theory of Employment, Interest and Money\". Keynes' assistant Richard Stone further developed the concept for the United Nations (UN) and the Organisation for Economic Co-operation and Development to the systems, which is now used internationally.\n\nThe first to visualize the modern circular flow of income model was Frank Knight in 1933 publication of \"The Economic Organization.\" Knight (1933) explained:\n\nKnight pictured a circulation of money and circulation of economic value between people (individuals, families) and business enterprises as a group, explaining: \"The general character of an enterprise system, reduced to its very simplest terms, can be illustrated by a diagram showing the exchange of productive power for consumption goods between individuals and business units, mediated by the \"circulation\" of money, and suggesting the familiar figure of the \"wheel of wealth\".\"\n\nIn the basic circular flow of income, or two sector circular flow of income model, the state of equilibrium is defined as a situation in which there is no tendency for the levels of income (Y), expenditure (E) and output (O) to change, that is:\n\nY = E = O\n\nThis means that the expenditure of buyers (households) becomes income for sellers (firms). The firms then spend this income on factors of production such as labour, capital and raw materials, \"transferring\" their income to the factor owners. The factor owners spend this income on goods which leads to a circular flow of income.\n\nThis basic circular flow of income model consists of six assumptions:\n\nIt includes household sector, producing sector and government sector. It will study a circular flow income in these sectors excluding rest of the world i.e. closed economy income. Here flows from household sector and producing sector to government sector are in the form of taxes. The income received from the government sector flows to producing and household sector in the form of payments for government purchases of goods and services as well as payment of subsidies and transfer payments. Every payment has a receipt in response of it by which aggregate expenditure of an economy becomes identical to aggregate income and makes this circular flow unending.\n\nA modern monetary economy comprises a network of four sector economy these are:\n\nEach of the above sectors receives some payments from the other in lieu of goods and services which makes a regular flow of goods and physical services. Money facilitates such an exchange smoothly. A residual of each market comes in capital market as saving which in turn is invested in firms and government sector. Technically speaking, so long as lending is equal to the borrowing i.e. leakage is equal to injections, the circular flow will continue indefinitely. However, this job is done by financial institutions in the economy.\n\nIn the five sector model the economy is divided into five sectors:\nThe five sector model of the circular flow of income is a more realistic representation of the economy. Unlike the two sector model where there are six assumptions the five sector circular flow relaxes all six assumptions. Since the first assumption is relaxed there are three more sectors introduced.\n\nIn the five sector model, there are leakages and injections\nLeakages and injections can occur in the financial sector, government sector and overseas sector:\n\nIn terms of the circular flow of income model, the leakage that financial institutions provide in the economy is the option for households to save their money. This is a leakage because the saved money can not be spent in the economy and thus is an idle asset that means not all output will be purchased. The injection that the financial sector provides into the economy is investment (I) into the business/firms sector. An example of a group in the finance sector includes banks such as Westpac or financial institutions such as Suncorp.\n\nThe leakage that the Government sector provides is through the collection of revenue through Taxes (T) that is provided by households and firms to the government. This is a leakage because it is a leakage out of the current income thus reducing the expenditure on current goods and services. The injection provided by the government sector is Government spending (G) that provides collective services and welfare payments to the community. An example of a tax collected by the government as a leakage is income tax and an injection into the economy can be when the government redistributes this income in the form of welfare payments, that is a form of government spending back into the economy.\n\nThe main leakage from this sector are imports (M), which represent spending by residents into the rest of the world. The main injection provided by this sector is the exports of goods and services which generate income for the exporters from overseas residents. An example of the use of the overseas sector is Australia exporting wool to China, China pays the exporter of the wool (the farmer) therefore more money enters the economy thus making it an injection. Another example is China processing the wool into items such as coats and Australia importing the product by paying the Chinese exporter; since the money paying for the coat leaves the economy it is a leakage.\n\n\nIn terms of the \"five sector circular flow of income model\" the state of equilibrium occurs when the total leakages are equal to the total injections that occur in the economy. This can be shown as:\n\nSavings + Taxes + Imports = Investment + Government Spending + Exports\n\nOR\n\nS + T + M = I + G + X.\n\nThis can be further illustrated through a fictitious economy where:<br>\n\nS + T + M = I + G + X<br>\n$100 + $150 + $50 = $50 + $100 + $150<br>\n$300 = $300<br>\n\nTherefore, since the leakages are equal to the injections the economy is in a stable state of equilibrium. This state can be contrasted to the state of disequilibrium where unlike that of equilibrium the sum of total leakages does not equal the sum of total injections. By giving values to the leakages and injections the circular flow of income can be used to show the state of disequilibrium. Disequilibrium can be shown as:\n\nS + T + M ≠ I + G + X\n\nTherefore, it can be shown as one of the below equations where:\n\nTotal leakages > Total injections\n$150 (S) + $250 (T) + $150 (M) > $75 (I) + $200 (G) + $150 (X)\n\nOr\n\nTotal Leakages < Total injections\n$50 (S) + $200 (T) + $125 (M) < $75 (I) + $200 (G) + $150 (X)\n\nThe effects of disequilibrium vary according to which of the above equations they belong to.\n\nIf S + T + M > I + G + X the levels of income, output, expenditure and employment will fall causing a recession or contraction in the overall economic activity. But if S + T + M < I + G + X the levels of income, output, expenditure and employment will rise causing a boom or expansion in economic activity.\n\nTo manage this problem, if disequilibrium were to occur in the five sector circular flow of income model, changes in expenditure and output will lead to equilibrium being regained. An example of this is if:\n\nS + T + M > I + G + X the levels of income, expenditure and output will fall causing a contraction or recession in the overall economic activity. As the income falls households will cut down on all leakages such as saving, they will also pay less in taxation and with a lower income they will spend less on imports. This will lead to a fall in the leakages until they equal the injections and a lower level of equilibrium will be the result.\n\nThe other equation of disequilibrium, if S + T + M < I + G + X in the five sector model the levels of income, expenditure and output will greatly rise causing a boom in economic activity. As the households income increases there will be a higher opportunity to save therefore saving in the financial sector will increase, taxation for the higher threshold will increase and they will be able to spend more on imports. In this case when the leakages increase the situation will be a higher level of equilibrium.\n\nThe book A General Approach to Macroeconomic Policy identifies four possible areas of significance:\n\n\nThe circular flow diagram is an abstraction of the economy as a whole. The diagram suggests that the economy can reproduce itself. The idea is that as households spend money of goods and services from firms, the firms have the means to purchase labor from the households, which the households to then purchase goods and services. Suggesting that this process can and will continuously go on as a perpetual motion machine. However, according to the Laws of Thermodynamics perpetual motion machines do not exist. The First Laws says matter and energy cannot be created or destroyed, and the Second Laws says that matter and energy move from a low entropy, useful, state towards a less useful higher entropy state. Thus, no system can continue without inputs of new energy that exit as high entropy waste. Just as no animal can live on its own waste, no economy can recycle the waste it produces without the input of new energy to reproduce itself. The economy therefore cannot be the whole. It must be a subsystem of the larger ecosystem.\n\nThe abstraction ignores the linear throughput of matter and energy that must power the continuous motion of money, goods and services, and factors of production. Matter and energy enter the economy in the form of low entropy natural capital, such as solar energy, oil wells, fisheries, and mines. These materials and energy are used by households and firms a like to create products and wealth. After the material are used up the energy and matter leaves the economy in the form of high entropy waste that is no longer valuable to the economy. The natural materials that power the motion of the circular flow of the economy come from the environment, and the waste must be absorbed by the larger ecosystem in which the economy exists.\n\nThis is not to say that the circular flow diagram isn't useful in understanding the basics of an economy, such as leakages and injections. However, it cannot be ignored that the economy intrinsically requires natural resources and the creation of waste that must be absorbed in some manner. The economy can only continuing churning if it has matter and energy to power it and the ability to absorb the waste it creates. This matter and low entropy energy and the ability to absorb waste exists in a finite amount, and thus there is a finite amount of inputs to the flow and outputs of the flow that the environment can handle, implying there is a sustainable limit to motion, and therefore growth, of the economy.\n\n\nThis article incorporates text from Bureau of Economic Analysis. \"Measuring the Economy : A Primer on GDP and the National Income and Product Accounts,\" 2014, a publication now in the public domain.\n\n\n"}
{"id": "402048", "url": "https://en.wikipedia.org/wiki?curid=402048", "title": "Closed system", "text": "Closed system\n\nA closed system is a physical system that does not allow certain types of transfers (such as transfer of mass and energy transfer) in or out of the system. The specification of what types of transfers are excluded varies in the closed systems of physics, chemistry or engineering.\n\nIn nonrelativistic classical mechanics, a closed system is a physical system that doesn't exchange any matter with its surroundings, and isn't subject to any net force whose source is external to the system. A closed system in classical mechanics would be considered an isolated system in thermodynamics. Closed systems are often used to limit the factors that can affect the results of specific problem or experiment.\n\nIn thermodynamics, a closed system can exchange energy (as heat or work) but not matter, with its surroundings.\nAn isolated system cannot exchange any heat, work, or matter with the surroundings, while an open system can exchange energy and matter. (This scheme of definition of terms is not uniformly used, though it is convenient for some purposes. In particular, some writers use 'closed system' where 'isolated system' is used here.)\n\nFor a simple system, with only one type of particle (atom or molecule), a closed system amounts to a constant number of particles. However, for systems which are undergoing a chemical reaction, there may be all sorts of molecules being generated and destroyed by the reaction process. In this case, the fact that the system is closed is expressed by stating that the total number of each elemental atom is conserved, no matter what kind of molecule it may be a part of. Mathematically:\n\nformula_1\n\nwhere formula_2 is the number of j-type molecules, formula_3 is the number of atoms of element \"i\" in molecule \"j\" and \"b\" is the total number of atoms of element \"i\" in the system, which remains constant, since the system is closed. There will be one such equation for each different element in the system.\n\nIn thermodynamics, a closed system is important for solving complicated thermodynamic problems. It allows the elimination of some external factors that could alter the results of the experiment or problem thus simplifying it. A closed system can also be used in situations where thermodynamic equilibrium is required to simplify the situation.\n\nThis equation, called Schrödinger's equation, describes the behavior of an isolated or closed quantum system, that is, by definition, a system which does not interchange information (i.e. energy and/or matter) with another system. So if an isolated system is in some pure state |ψ(t) ∈ H at time t, where H denotes the Hilbert space of the system, the time evolution of this state (between two consecutive measurements).\n\nformula_4\n\nwhere is the imaginary unit, is the Planck constant divided by , the symbol indicates a partial derivative with respect to time , (the Greek letter psi) is the wave function of the quantum system, and is the Hamiltonian operator (which characterizes the total energy of any given wave function and takes different forms depending on the situation).\n\nIn chemistry, a closed system is where no reactants or products can escape, only heat can be exchanged freely (e.g. an ice cooler). A closed system can be used when conducting chemical experiments where temperature is not a factor (i.e. reaching thermal equilibrium).\n\nIn an engineering context, a closed system is a bound system, i.e. defined, in which every input is known and every resultant is known (or can be known) within a specific time.\n\n"}
{"id": "33956397", "url": "https://en.wikipedia.org/wiki?curid=33956397", "title": "Cognitive holding power", "text": "Cognitive holding power\n\nCognitive holding power is a concept measured by John C. Stevenson in 1994 using a questionnaire, the Cognitive Holding Power Questionnaire (CHPQ). This tool is assesses first- or second-order cognitive processing preferences.\n\nStudies using holding power have suggest improvements to mathematical education.\n"}
{"id": "2110010", "url": "https://en.wikipedia.org/wiki?curid=2110010", "title": "Collioure", "text": "Collioure\n\nCollioure (; , ) is a commune in the Pyrénées-Orientales department in southern France.\n\nThe town of Collioure is on the Côte Vermeille, in the canton of La Côte Vermeille and in the arrondissement of Céret.\n\nCollioure is named \"Cotlliure\" in Catalan.\n\nThere is a record of the castle at \"Castrum Caucoliberi\" having been mentioned as early as 673, indicating that the settlement here was of strategic and commercial importance during the Visigoth ascendancy.\n\nCollioure used to be divided into two villages separated by the river Douy, the old town to the south named \"Port d'Avall\" (in French known as \"Le Faubourg\") and the upstream port, \"Port d'Amunt\" (in French known as \"La Ville\").\n\nCollioure was taken in 1642 by the French troops of Maréchal de la Meilleraye. A decade later, the town was officially surrendered to France by the 1659 Treaty of Pyrenees. Because of its highly strategic importance, the town's fortifications, the Château Royal de Collioure and the Fort Saint-Elme stronghold, were improved by the military engineer Vauban during the reign of Louis XIV. Nevertheless, Collioure was besieged and occupied by the Spanish troops in 1793, marking the last Spanish attempt to take the city. The blockade was broken a year later by general Jacques François Dugommier.\n\nIn 1823, the territory of Port-Vendres became a commune, taking parts from the communes of Collioure and Banyuls-sur-Mer.\n\nOn 21 January 1870, an exceptional climatic phenomenon occurred in Collioure, as observed by Charles Naudin at the time; more than of snow fell in one day on the town. Many orchards as well as cork oak woodlands were damaged.\n\n\nCollioure is the name of an Appellation d'Origine Contrôlée (AOC) situated around the town, (Collioure AOC), producing red, rosé and a few white wines.\nThe ancient terraced vines in the hills behind the town also provide grapes for the apéritif and dessert wines of the (Banyuls) appellation, which shares its boundaries with the Collioure appellation.\n\nCollioure is also famous for its anchovies, and its once-thriving fisheries is referenced in Mark Kurlansky's book \"Salt\".\n\nAs the town has a strong Catalan culture, its own motto has been adopted by one of the local Catalan rugby teams (USA Perpignan, France): \"Sempre endavant, mai morirem\" (\"Always forward, We'll never die\"). Under 's leadership, the town has an alternative motto, \"Collioure sera toujours Collioure\" (\"Collioure shall always be Collioure\") quoting French singer Maurice Chevalier's famous song titled \"Paris sera toujours Paris\".\n\nThe annual Saint Vincent festival is held around August 15, attracting twice the town's population in visitors for several days of celebration with music and fireworks.\n\nIn the early 20th century Collioure became a center of artistic activity, with several Fauve artists making it their meeting place. André Derain, Georges Braque, Othon Friesz, Henri Matisse, Pablo Picasso, Charles Rennie Mackintosh, James Dickson Innes and Tsuguharu Fujita have all been inspired by Collioure's royal castle, medieval streets, its lighthouse converted into the church of Notre-Dame-des-Anges and its typical Mediterranean bay. Collioure's cemetery contains the tomb of Spanish poet Antonio Machado, who fled here to escape advancing Francoist troops at the end of the Spanish Civil War in 1939.\n\nThe British novelist Patrick O'Brian, lived in the town from 1949 until his death in 2000, and his novel \"The Catalans\" graphically describes Collioure life before major changes took place. He also wrote a biography of Picasso, who was an acquaintance. O'Brian and his wife Mary are also buried in the town cemetery.\n\nPart of the action in Stephen Clarke's fourth comic novel featuring Paul West, \"Dial M for Merde\", takes place in Collioure.\n\nNinety-eight reproductions of Matisse’s and Derain’s works are exposed exactly where these two masters of Fauvism painted the originals, in the early 20th century.\n\n\n\n"}
{"id": "11797804", "url": "https://en.wikipedia.org/wiki?curid=11797804", "title": "Comparison (grammar)", "text": "Comparison (grammar)\n\nComparison is a feature in the morphology or syntax of some languages, whereby adjectives and adverbs are inflected or modified to indicate the relative degree of the property defined by the adjective or adverb. The comparative expresses a comparison between two (or more) entities or groups of entities in quality, quantity, or degree; the superlative is the form of an adverb or adjective that is the greatest degree of a given descriptor.\n\nThe grammatical category associated with comparison of adjectives and adverbs is degree of comparison. The usual degrees of comparison are the \"positive\", which simply denotes a property (as with the English words \"big\" and \"fully\"); the \"comparative\", which indicates \"greater degree (as \"bigger\" and \"more fully\"); and the \"superlative\", which indicates \"greatest degree (as \"biggest\" and \"most fully\"). Some languages have forms indicating a very large degree of a particular quality (called elative in Semitic linguistics). Other languages (e.g. English) can express lesser degree, e.g. \"beautiful\", \"less beautiful\", \"least beautiful\".\n\nThe comparative is frequently associated with adjectives and adverbs because these words take the \"-er\" suffix or modifying word \"more\" or \"less\" (e.g., \"faster\", \"more intelligent\", \"less wasteful\"); it can also, however, appear when no adjective or adverb is present, for instance with nouns (e.g., \"more men than women\"). One preposition, \"near\", also has a superlative form, as in \"Find the restaurant nearest your house\".\n\nComparatives and superlatives may be formed morphologically, by inflection, as with the English and German \"-er\" and \"-(e)st\" forms, or syntactically, as with the English \"more...\" and \"most...\" and the French \"plus...\" and \"le plus...\" forms. Common adjectives and adverbs often produce irregular forms, such as \"better\" and \"best\" (from \"good\") and \"less\" and \"least\" (from \"little/few\") in English, and \"meilleur\" (from \"bon\") and \"mieux\" (from the adverb \"bien\") in French.\n\nMost if not all languages have some means of forming the comparative, although these means can vary significantly from one language to the next.\n\nComparatives are often used with a conjunction or other grammatical means to indicate with what the comparison is being made, as with \"than\" in English, \"als\" in German, etc. In Russian and Greek (Ancient, Koine and Modern) this can be done by placing the compared noun in the genitive case. With superlatives, the class of things being considered for comparison may be indicated, as in \"the best swimmer out of all the girls\".\n\nLanguages also possess other structures for comparing adjectives and adverbs; English examples include \"as... as\" and \"less/least...\".\n\nА few languages apply comparison to nouns and even verbs. One such language is Bulgarian, where expressions like \"по̀ човек (po chovek), най човек (nay chovek), по-малко човек (po malko chovek)\" (literally \"more person\", \"most person\", \"less person\" but normally \"better kind of a person\", \"best kind of person\", \"not that good kind of a person\") and \"по̀ обичам (po obicham), най-малко обичам (nay malko obicham)\" (\"I like more\", \"I like the least\") are quite usual.\n\nIn many languages, including English, traditional grammar requires the comparative form to be used when exactly two things are being considered, even in constructions where the superlative would be used when considering a larger number. For instance, \"May the better man win\" would be considered correct if there are only two individuals competing. However, this rule is not always observed in informal usage; the form \"May the best man win\" will often be used in that situation, as it would if there were three or more competitors involved.\n\nIn some contexts, such as advertising or political speeches, absolute and relative comparatives are intentionally employed in a way that invites a comparison, and yet the basis of comparison is not established. This is a common rhetorical device used to create an implication of significance where one may not actually be present. Although such usage is common, it is sometimes considered ungrammatical.\n\nFor example:\n\nEnglish has two parallel systems of comparison, a morphological one formed using the suffixes \"-er\" (the \"comparative\") and \"-est\" (the \"superlative\"), with some irregular forms; and a syntactic one, formed with the adverbs \"more\" and \"most\".\n\nAs a general rule, words with one syllable require the suffix (except for the four words: fun, real, right, wrong), words with three or more syllables require \"more\" or \"most\", and words with two syllables may use one system or the other; which words use which system is a matter of idiom. Some adjectives, \"e.g.\" 'polite', can use either form, with different frequencies according to context.\n\nMorphological comparison uses the suffixes \"-er\" (the \"comparative\") and \"-est\" (the \"superlative\"). These inflections are of Germanic origin and are cognate with the Latin suffixes -\"ior\" and -\"issimus\" and Ancient Greek -\"īōn\" and -\"istos\". They are typically added to shorter words, words of Anglo-Saxon origin, and borrowed words which have been fully assimilated into the English vocabulary. Usually the words which take these inflections have fewer than three syllables.\n\nThis system also contains a number of irregular forms, some of which, like \"good\", \"better\", and \"best\", contain suppletive forms. These irregular forms include:\n\nThe second system of comparison in English appends the grammatical particles \"more\" and \"most\", themselves the irregular comparatives of \"many\" and \"much\", to the adjective or adverb being modified. This series can be compared to a system containing the diminutives \"less\" and \"least\".\n\nThis system is most commonly used with words of French or Latin derivation; with adjectives and adverbs formed with suffixes other than \"-ly\" (e.g., \"beautiful\"); and with longer, technical, or infrequently used words. For example:\n\nSome adjectives, the absolute or ungradable adjectives do not appear to logically allow degrees. Some qualities are either \"present\" or \"absent\", such as being Cretaceous or igneous, so it appears illogical to call anything \"very Cretaceous\", or to characterize something as \"more igneous\" than something else.\n\nSome grammarians object to the use of the superlative or comparative with words such as \"full\", \"complete\", \"unique\", or \"empty\", which by definition already denote either a totality, an absence, or an absolute. However, such words are routinely and frequently qualified in contemporary speech and writing. This type of usage conveys more of a figurative than a literal meaning, because in a strictly literal sense, something cannot be more or less unique or empty to a greater or lesser degree.\n\nMany prescriptive grammars and style guides include adjectives for inherently superlative qualities to be ungradable. Thus, they reject expressions such as \"more perfect\", \"most unique\", and \"most parallel\" as illogical pleonasms: after all, if something is unique, it is one of a kind, so nothing can be \"very unique\", or \"more unique\" than something else.\n\nOther style guides argue that terms like \"perfect\" and \"parallel\" never apply \"exactly\" to things in real life, so they are commonly used to mean \"nearly perfect\", \"nearly parallel\", and so on; in this sense, \"more perfect\" (\"i.e.\", more nearly perfect, closer to perfect) and \"more parallel\" (\"i.e.\", more nearly parallel, closer to parallel) are meaningful.\n\nIn most Balto-Slavic languages (such as Czech, Polish, Lithuanian and Latvian), the comparative and superlative forms are also declinable adjectives.\n\nIn Bulgarian, comparative and superlative forms are formed with the clitics \"по-\" (\"more\") and \"най-\" (\"most\"):\n\nIn Czech, Polish, Slovak and Slovene, comparative is formed from the base form of an adjective with a suffix and superlative is formed with a circumfix (equivalent to adding a prefix to the comparative).\n\nIn Russian, comparative and superlative forms are usually formed with a suffix:\n\nIn contrast to English, the relative and the superlative are joined into the same degree (the superlative), which can be of two kinds: comparative (e.g. \"the most beautiful\") and absolute (e.g. \"very beautiful\").\n\nFrench: The superlative is created from the comparative by inserting the definitive article (la, le, or les), or the possessive article (\"mon\", \"ton\", \"son\", etc.), before \"plus\" or \"moins\" and the adjective determining the noun. For instance: \"Elle est la plus belle femme\" → (she is the most beautiful woman); \"Cette ville est la moins chère de France\" → (this town is the least expensive in France); \"C'est sa plus belle robe\" → (It is her most beautiful dress). It can also be created with the suffix \"-issime\" but only with certain words, for example: \"C'est un homme richissime\" → (That is the most rich man). Its use is often rare and ironic.\n\nPortuguese and Italian distinguish comparative superlative \"(superlativo relativo)\" and absolute superlative \"(superlativo absoluto/assoluto).\nFor the comparative superlative they use the words \"mais\" and \"più\" between the article and the adjective, like \"most\" in English.\nFor the absolute superlative they either use \"muito\"/\"molto\" and the adjective or modify the adjective by taking away the final vowel and adding \"issimo\" (singular masculine), \"issima\" (singular feminine), \"íssimos\"/\"issimi\" (plural masculine), or \"íssimas\"/\"issime\" (plural feminine). For example:\nThere are some irregular forms for some words ending in \"-re\" and \"-le\" (deriving from Latin words ending in \"-er\" and \"-ilis\") that have a superlative form similar to the Latin one. In the first case words lose the ending \"-re\" and they gain the endings \"errimo\" (singular masculine), \"errima\" (singular feminine), \"érrimos\"/\"errimi\" (plural masculine), or \"érrimas\"/\"errime\" (plural feminine); in the second case words lose the \"-l\"/\"-le\" ending and gain \"ílimo\"/\"illimo\" (singular masculine), \"ílima\"/\"illima\" (singular feminine), \"ílimos\"/\"illimi\" (plural masculine), or \"íli\nRomanian, similar to Portuguese and Italian, distinguishes comparative and absolute superlatives. The comparative uses the word \"mai\" before the adjective, which operates like \"more\" or \"-er\" in English. For example: \"luminos\" → bright, \"mai luminos\" → brighter. To weaken the adjective, the word \"puțin\" (little) is added between \"mai\" and the adjective, for example \"mai puțin luminos\" → less bright. For absolute superlatives, the gender-dependent determinant \"cel\" precedes \"mai,\" conjugated as \"cel / cei\" for male singular / plural and \"cea / cele\" for female singular / plural. For example: \"cea mai luminoasă stea\" → the brightest star; \"cele mai frumoase fete\" → the most beautiful girls; \"cel mai mic morcov\" → the smallest carrot.\n\nScottish Gaelic: When comparing one entity to another in the present or the future tense, the adjective is changed by adding an \"e\" to the end and \"i\" before the final consonant(s) if the final vowel is broad. Then, the adjective is preceded by \"nas\" to say \"more,\" and \"as\" to say \"most.\" (The word \"na\" is used to mean \"than\".) Adjectives that begin with \"f\" are lenited. and \"as\" use different syntax constructions. For example:\nTha mi nas àirde na mo pheathraichean.\" → I am taller than my sisters.\nIs mi as àirde.\" → I am the tallest.\n\nAs in English, some forms are irregular, i.e. nas fheàrr (better), nas miosa (worse), etc.\n\nIn other tenses, \"nas\" is replaced by \"na bu\" and \"as\" by \"a bu,\" both of which lenite the adjective if possible. If the adjective begins with a vowel or an \"f\" followed by a vowel, the word \"bu\" is reduced to \"b\"'. For example:\n\n\nWelsh is similar to English in many respects. The ending \"-af\" is added onto regular adjectives in a similar manner to the English \"-est\", and with (most) long words \"mwyaf\" precedes it, as in the English \"most\". Also, many of the most common adjectives are irregular. Unlike English, however, when comparing just two things, the superlative \"must\" be used, e.g. of two people - \"John ydy'r talaf\" (John is the tallest).\n\nIn Akkadian cuneiform, (on a 12 paragraph clay tablet), from the time period of the 1350 BC Amarna letters (a roughly 20-year body of letters), two striking examples of the superlative extend the common grammatical use. The first is the numeral \"10,\" as well as \"7 and 7.\" The second is a verb-spacement adjustment.\n\nThe term \"7 and 7\" means 'over and over'. The phrase itself is a superlative, but an addition to some of the Amarna letters adds \"more\" at the end of the phrase (EA 283, \"Oh to see the King-(pharaoh)):\" \"... I fall at the feet of the king, my lord. I fall at the feet of the king, my lord, 7 and 7 times\" more, \"...\". The word 'more' is Akkadian \"mila\", and by Moran is 'more' or 'overflowing'. The meaning in its letter context is \"...over and over again, overflowing,\" (as 'gushingly', or 'obsequiously', as an underling of the king).\n\nThe numeral 10 is used for \"ten times greater\" in EA 19, \"Love and Gold\", one of King Tushratta's eleven letters to the Pharaoh-(Amenhotep IV-\"Akhenaton\"). The following quote using 10, also closes out the small paragraph by the second example of the superlative, where the verb that ends the last sentence is spread across the letter in s-p-a-c-i-n-g, to accentuate the last sentence, and the verb itself (i.e. the relational kingly topic of the paragraph):\n\nThe actual last paragraph line contains three words: 'may it be', 'flourish', and 'us'. The verb flourish (from napāhu?, \"to light up, to rise\"), uses: -e-le-né-ep-pi-, and the spaces. The other two words on the line, are made from two characters, and then one: \"...may it be, flourish-our (relations).\"\n\nIn Estonian, the superlative form can usually be formed in two ways. One is a periphrastic construction with \"kõige\" followed by the comparative form. This form exists for all adjectives. For example: the comparative form of \"sinine\" 'blue' is \"sinisem\" and therefore the periphrastic superlative form is \"kõige sinisem\". There is also a synthetic (\"short\") superlative form, which is formed by adding \"-m\" to the end of the plural partitive case. For \"sinine\" the plural partitive form is \"siniseid\" and so \"siniseim\" is the short superlative. The short superlative does not exist for all adjectives and, in contrast to the \"kõige\"-form, has a lot of exceptions.\n\n"}
{"id": "1554404", "url": "https://en.wikipedia.org/wiki?curid=1554404", "title": "De Finetti diagram", "text": "De Finetti diagram\n\nA de Finetti diagram is a ternary plot used in population genetics. It is named after the Italian statistician Bruno de Finetti (1906–1985) and is used to graph the genotype frequencies of populations, where there are two alleles and the population is diploid. It is based on an equilateral triangle, and Viviani's theorem concerning any point within the triangle, and the three lines from that point that are perpendicular to the sides of the triangle. The sum of the lengths of the lines is a fixed value, regardless of the position of the point. This value (the sum of the lengths) is set to be 1.\n\nThe de Finetti diagram has been put to extensive use in population genetics by A.W.F. Edwards in his book \"Foundations of Mathematical Genetics\". In its simplest form the diagram can be used to show the range of genotype frequencies for which Hardy-Weinberg equilibrium is satisfied (the curve within the diagram). A.W.F. Edwards and Chris Cannings extended its use to demonstrate the changes that occur in allele frequencies under natural selection.\n\n\n\n"}
{"id": "4606683", "url": "https://en.wikipedia.org/wiki?curid=4606683", "title": "Diagonal functor", "text": "Diagonal functor\n\nIn category theory, a branch of mathematics, the diagonal functor formula_1 is given by formula_2, which maps objects as well as morphisms. This functor can be employed to give a succinct alternate description of the product of objects \"within\" the category formula_3: a product formula_4 is a universal arrow from formula_5 to formula_6. The arrow comprises the projection maps.\n\nMore generally, given a small index category formula_7, one may construct the functor category formula_8, the objects of which are called diagrams. The diagonal functor is one particular diagram: for each object formula_9 in formula_3, there is a constant functor with fixed object formula_9: formula_12. The diagonal functor formula_13 assigns to each object formula_9 of formula_3 the functor formula_16, and to each morphism formula_17 in formula_3 the obvious natural transformation formula_19 in formula_8 (given by formula_21). Thus, for example, in the case that formula_7 is a discrete category with two objects, the diagonal functor formula_1 is recovered.\n\nDiagonal functors provide a way to define limits and colimits of diagrams. Given the functor formula_16, the natural transform from this functor to any other diagram formula_25 is called a cone. Among all such cones is a universal cone; this cone is the limit of the diagram formula_26. That is, the limit of any functor formula_25 \nis a universal arrow formula_28; the colimit is a universal arrow formula_29. \n\nIf every functor from formula_7 to formula_3 has a limit (which will be the case if formula_3 is complete), then the operation of taking limits is itself a functor from formula_8 to formula_3. The limit functor is the right-adjoint of the diagonal functor. Similarly, the colimit functor (which exists if the category is cocomplete) is the left-adjoint of the diagonal functor. \n\nFor example, the diagonal functor formula_1 described above is the left-adjoint of the binary product functor and the right-adjoint of the binary coproduct functor. Other well-known examples include the pushout, which is the limit of the span, and the terminal object, which is the limit of the empty category.\n\n"}
{"id": "42255549", "url": "https://en.wikipedia.org/wiki?curid=42255549", "title": "Eurasian Observatory for Democracy and Elections", "text": "Eurasian Observatory for Democracy and Elections\n\nEurasian Observatory for Democracy and Elections (EODE) is a Russia-based Eurasianist non-governmental organization which on its website claims that it monitors elections.\nAccording to its website, it specializes in the \"self-proclaimed republics\" (Abkhazia, Transdnistria, Nagorno-Karabakh).\nIt is led by the Belgian activist Luc Michel and Jean-Pierre Vandersmissen. Both Michel and Vandermissen are followers of the Belgian Neo-Nazi politician Jean-François Thiriart.\n\nOliver Bullough of \"New Republic\" reported in 2014 that the organization stated on its website that it shared the values of \"the current Russian leadership and V. V. Putin.\" The organization states that is committed to a multi-polar world and is critical of what it sees as a unipolar world dominated in the twenty-first century by the United States and NATO. It is opposed to the imposition of western values and the identification of the western-style parliamentary system of democracy as a universal norm not open to criticism. \n\nEODE visited Crimea during the 2014 Crimean referendum and claimed that the referendum was conducted in a legitimate manner.\n\nThe organization has offices in Moscow, Paris, Brussels, Sochi and Chișinău.\n\n"}
{"id": "35932202", "url": "https://en.wikipedia.org/wiki?curid=35932202", "title": "European Pathway to Zero Waste", "text": "European Pathway to Zero Waste\n\nEuropean Pathway to Zero Waste is an EU demonstration project part funded by The LIFE Programme (LIFE+), the EU’s funding instrument for the environment and is a partnership between the Environment Agency and Waste & Resources Action Programme (WRAP). The project has been set up to investigate practical ways to achieve zero waste to landfill in the South East of England. It will share the results with colleagues both in the UK and in relevant EU Member States. Zero waste in this context is an approach to supporting the sustainable use of resources by business to benefit both the economy and the environment. It concentrates on reducing, re-using, recycling and recovering energy from waste. The project will run to March 2013.\n\n"}
{"id": "13901408", "url": "https://en.wikipedia.org/wiki?curid=13901408", "title": "Implied repeal", "text": "Implied repeal\n\nThe doctrine of implied repeal is a concept in constitutional theory which states that where an Act of Parliament or an Act of Congress (or of some other legislature) conflicts with an earlier one, the later Act takes precedence and the conflicting parts of the earlier Act becomes legally inoperable. This doctrine is expressed in the Latin phrase \"leges posteriores priores contrarias abrogant\".\n\nImplied repeal is to be contrasted with the express repeal of legislation by the legislative body.\n\nUnder United States law, \"implied repeal\" is a disfavored doctrine. That is, if a court can reconcile the two statutes with any reasonable interpretation, that interpretation is preferred to one that treats the earlier statute as invalidated by the later one.\n\nIn Canadian law, it is possible for a law to be protected from implied repeal by way of a \"primacy clause\" which state that the act in question supersedes all other statutes until it is specifically repealed. Acts with such primacy clauses are called quasi-constitutional.\n\nIn the 2002 English case \"Thoburn v Sunderland City Council\" (the so-called \"Metric Martyrs\" case), Lord Justice Laws held that some constitutionally significant statutes hold a higher status in UK law and are not be subject to the doctrine of implied repeal. The case specifically dealt with s.2(2) of the European Communities Act, but in his judgment Lord Justice Laws also held the view that the Parliament Acts and the Human Rights Act are \"constitutional statutes\" and in his opinion may not be subject to the doctrine of implied repeal. \n\nA decade later in 2012, in a case before the United Kingdom Supreme Court, \"BH v The Lord Advocate (Scotland)\", Lord Hope held (in Paragraph 30 of the judgment) that \"the Scotland Act can only be expressly repealed; it cannot be impliedly repealed; that is because of its 'fundamental constitutional nature'.\"\n\n"}
{"id": "876262", "url": "https://en.wikipedia.org/wiki?curid=876262", "title": "Incivility", "text": "Incivility\n\nIncivility is a general term for social behaviour lacking in civility or good manners, on a scale from rudeness or lack of respect for elders, to vandalism and hooliganism, through public drunkenness and threatening behaviour. The word \"incivility\" is derived from the Latin \"incivilis\", meaning \"not of a citizen\".\n\nThe distinction between plain rudeness, and perceived incivility as threat, will depend on some notion of civility as structural to society; incivility as anything more ominous than bad manners is therefore dependent on appeal to notions like its antagonism to the complex concepts of civic virtue or civil society. It has become a contemporary political issue in a number of countries.\n\nCivil behavior requires that people communicate with respect, restraint, and responsibility, and uncivil communication occurs when people fail to do so. Universal pragmatics, a term coined by Jürgen Habermas, suggests that human conflict arises from miscommunication, so communicative competence is needed to reduce conflict. Communication competence \"involves the ability to communicate in such a way that: (1) the truth claim of an utterance is shared by both speaker and hearer; (2) the hearer is led to understand and accept the speaker’s intention; and (3) the speaker adapts to the hearer’s world view.\" If people disagree about the truth or appropriateness of their interaction, conflict will occur.\n\nAccording to Habermas, we should establish communicative norms that lead to rational conversations by creating the social coordination needed for interactants to pursue their goals while recognizing the truth or appropriateness of their interaction. Such norms, or social rules, include: \"all participants must be allowed to speak freely, all participants must be allowed to speak for themselves (to enable them to establish their own ethos or \"selfhood\"), and that communication should be equal, with no one participant commanding more attention from the others than is afforded to them on their turn.\"\n\nSome examples of uncivil communication include rude gestures, vulgar language, interrupting, and loudly having private discussions in public spaces. Recent poll data suggests that Americans believe uncivil communication is a serious problem, and believe it has led to an increase in physical violence. The 2013 study on \"Civility in America: A Nationwide Survey\", conducted by global public relations firm Weber Shandwick and public affairs firm Powell Tate in partnership with KRC Research found that 70 percent of Americans believe incivility has reached crisis proportions. Of those who expect civility to rise, 34 percent blame Twitter. The study found that Americans encounter incivility, on average, 17.1 times per week, or 2.4 times per day. Some studies suggest that uncivil communication may have real consequences, including increased health problems due to stress, decreased work productivity, more auto accidents caused by aggressive driving, and vandalism.\n\nPolitical incivility is different from the everyday incivility described above. According to face negotiation theory, politeness norms require us to avoid challenging others, but political incivility is different because, since it is specific to the political sphere, contestation of views and confrontation are required for a democracy to occur. According to Thomas Benson, \"Where there is disagreement, there is a risk of incivility; in many cases, incivility is itself a tactic in political discourse, employed as an indicator of sincerity, as the marker of the high stakes in a disagreement.\"\n\nCivil discourse is \"the free and respectful exchange of different ideas\". Eight out of 10 Americans believe that the lack of civil discourse in the political system is a serious problem. Eighty-two percent of American respondents to a 2011 survey felt that political advertisements were too \"nasty\" and 72 percent believed that political commercials that attacked the opponent were \"inappropriate\". Research has linked political incivility to reduced trust in the legitimacy of political candidates, political polarization, and policy gridlock. Campaigns and politicians are not the only avenues for incivility, however. The public also participates in civil discourse, and incivility. Incivility in these contexts can lead to the breakdown of political discourse, and exclude certain people or groups from the discussion. If people or groups are systematically excluded from the discussion, the democratic nature of that discussion is called into question. \n\nIn his article \"\", Habermas explains that the public sphere is \"a realm of our social life in which something approaching public opinion can be formed… Although state authority is so to speak the executor of the political public sphere, it is not a part of it.\" Political incivility threatens the features of the rhetorical model of the public sphere, which include:\n\nPolitical incivility threatens the future of the rhetorical model of the public sphere because it fractures that sphere into counter-publics, which may or may not interact with each other. According to Papacharissi (2004), \"Incivility can then be operationalized as the set of behaviors that threaten democracy, deny people their personal freedoms, and stereotype social groups.\", all of which could result from the violation of the features of the rhetorical model of the public sphere. People or groups may be systematically shut out of the mainstream political discourse, which makes that discourse less democratic, as certain voices are then missing from that discourse. Examples of incivility in political discourse include, but are not limited to, name calling, derisive or disrespectful speech and vulgarity, intentional lies, and misrepresentation. Another type of uncivil behavior is \"outrage speech\", which includes name calling, insulting, character assassination, mockery, and emotional displays. There are disagreements among researchers about whether or not emotional speech – using anger, fear, or hatred – should be considered uncivil. Some researchers view some emotional speech as civil unless it threatens democracy in some way, while other researchers view emotional speech itself as a disruption to democracy, and push for a purely rational view of civility. \n\nStryker et al. assert that \"political incivility is usefully distinguished from interpersonal politeness outside of politics.\" Their research found consensus among survey respondents concerning the types of political speech and behavior that \"count\" as \"political incivility\". Papacharissi echoes this sentiment, stating that \"civility should be redefined as a construct that encompasses, but also goes beyond, politeness.\" \n\nA 2011 report in USA Today defined Workplace incivility as \"a form of organizational deviance... characterized by low-intensity behaviors that violate respectful workplace norms, appearing vague as to intent to harm.\" The article asserts further that researchers had announced at the annual meeting of the American Psychological Association that \"Workplace incivility is on the rise. Uncivil behaviors are characteristically rude and discourteous, displaying a lack of regard for others.\" Incivility is distinct from violence. Examples of workplace incivility include insulting comments, denigration of the target's work, spreading false rumors, and social isolation.\n\nAt one time, a number of automotive audio manufacturers engaged in marketing incivility with their products, which included Sony with its \"Disturb The Peace\" tagline.\n\n\n\n"}
{"id": "18985062", "url": "https://en.wikipedia.org/wiki?curid=18985062", "title": "Information", "text": "Information\n\nInformation is any entity or form that provides the answer to a question of some kind or resolves uncertainty. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts. As it regards data, the information's existence is not necessarily coupled to an observer (it exists beyond an event horizon, for example), while in the case of knowledge, the information requires a cognitive observer.\n\nInformation is conveyed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message.\n\nInformation can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.\n\nInformation reduces uncertainty. The uncertainty of an event is measured by its probability of occurrence and is inversely proportional to that. The more uncertain an event, the more information is required to resolve uncertainty of that event. The bit is a typical unit of information, but other units such as the nat may be used. For example, the information encoded in one \"fair\" coin flip is log(2/1) = 1 bit, and in two fair coin flips is\nlog(4/1) = 2 bits.\n\nThe concept that \"information is the message\" has different meanings in different contexts. Thus the concept of information becomes closely related to notions of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.\n\nThe English word apparently derives from the Latin stem (\"information-\") of the nominative (\"informatio\"): this noun derives from the verb \"informare\" (to inform) in the sense of \"to give form to the mind\", \"to discipline\", \"instruct\", \"teach\". \"Inform\" itself comes (via French \"informer\") from the Latin verb \"informare\", which means to give form, or to form an idea of. Furthermore, Latin itself already contained the word \"informatio\" meaning concept or idea, but the extent to which this may have influenced the development of the word \"information\" in English is not clear.\n\nThe ancient Greek word for \"form\" was (\"morphe\"; cf. morph) and also εἶδος (\"eidos\") \"kind, idea, shape, set\", the latter word was famously used in a technical philosophical sense by Plato (and later Aristotle) to denote the ideal identity or essence of something (see Theory of Forms). 'Eidos' can also be associated with thought, proposition, or even concept.\n\nThe ancient Greek word for \"information\" is , which transliterates (\"plērophoria\") from \nπλήρης (\"plērēs\") \"fully\" and φέρω (\"phorein\") frequentative of (\"pherein\") \"to carry through\". It literally means \"bears fully\" or \"conveys fully\". In modern Greek the word is still in daily use and has the same meaning as the word \"information\" in English. In addition to its primary meaning, the word as a symbol has deep roots in Aristotle's semiotic triangle. In this regard it can be interpreted to communicate information to the one decoding that specific type of sign. This is something that occurs frequently with the etymology of many words in ancient and modern Greek where there is a very strong denotative relationship between the signifier, e.g. the word symbol that conveys a specific encoded interpretation, and the signified, e.g. a concept whose meaning the interpreter attempts to decode.\n\nIn English, “information” is an uncountable mass noun.\n\nIn information theory, \"information\" is taken as an ordered sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ. Information processing consists of an input-output function that maps any input sequence from χ into an output sequence from ϒ. The mapping may be probabilistic or deterministic. It may have memory or be memoryless.\n\nOften information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book \"Sensory Ecology\" Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input. In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is mainly (but not only, e.g. plants can grow in the direction of the lightsource) a causal input to plants but for animals it only provides information. The colored light reflected from a flower is too weak to do much photosynthetic work but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.\n\nThe cognitive scientist and applied mathematician Ronaldo Vigo argues that information is a concept that requires at least two related entities to make quantitative sense. These are, any dimensionally defined category of objects S, and any of its subsets R. R, in essence, is a representation of S, or, in other words, conveys representational (and hence, conceptual) information about S. Vigo then defines the amount of information that R conveys about S as the rate of change in the complexity of S whenever the objects in R are removed from S. Under \"Vigo information\", pattern, invariance, complexity, representation, and information—five fundamental constructs of universal science—are unified under a novel mathematical framework. Among other things, the framework aims to overcome the limitations of Shannon-Weaver information when attempting to characterize and measure subjective information.\n\nInformation is any type of pattern that influences the formation or transformation of other patterns. In this sense, there is no need for a conscious mind to perceive, much less appreciate, the pattern. Consider, for example, DNA. The sequence of nucleotides is a pattern that influences the formation and development of an organism without any need for a conscious mind. One might argue though that for a human to consciously define a pattern, for example a nucleotide, naturally involves conscious information processing.\n\nSystems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines \"information\" as a \"difference that makes a difference\".\n\nIf, however, the premise of \"influence\" implies that information has been perceived by a conscious mind and also interpreted by it, the specific context associated with this interpretation may cause the transformation of the information into knowledge. Complex definitions of both \"information\" and \"knowledge\" make such semantic and logical analysis difficult, but the condition of \"transformation\" is an important point in the study of information as it relates to knowledge, especially in the business discipline of knowledge management. In this practice, tools and processes are used to assist a knowledge worker in performing research and making decisions, including steps such as:\n\n\nStewart (2001) argues that transformation of information into knowledge is critical, lying at the core of value creation and competitive advantage for the modern enterprise.\n\nThe Danish Dictionary of Information Terms argues that information only provides an answer to a posed question. Whether the answer provides knowledge depends on the informed person. So a generalized definition of the concept should be: \"Information\" = An answer to a specific question\".\n\nWhen Marshall McLuhan speaks of media and their effects on human cultures, he refers to the structure of artifacts that in turn shape our behaviors and mindsets. Also, pheromones are often said to be \"information\" in this sense.\n\nInformation has a well-defined meaning in physics. In 2003 J. D. Bekenstein claimed that a growing trend in physics was to define the physical world as being made up of information itself (and thus information is defined in this way) (see Digital physics). Examples of this include the phenomenon of quantum entanglement, where particles can interact without reference to their separation or the speed of light. Material information itself cannot travel faster than light even if that information is transmitted indirectly. This could lead to all attempts at physically observing a particle with an \"entangled\" relationship to another being slowed down, even though the particles are not connected in any other way other than by the information they carry.\n\nThe mathematical universe hypothesis suggests a new paradigm, in which virtually everything, from particles and fields, through biological entities and consciousness, to the multiverse itself, could be described by mathematical patterns of information. By the same token, the cosmic void can be conceived of as the absence of material information in space (setting aside the virtual particles that pop in and out of existence due to quantum fluctuations, as well as the gravitational field and the dark energy). Nothingness can be understood then as that within which no matter, energy, space, time, or any other type of information could exist, which would be possible if symmetry and structure break within the manifold of the multiverse (i.e. the manifold would have tears or holes).\n\nAnother link is demonstrated by the Maxwell's demon thought experiment. In this experiment, a direct relationship between information and another physical property, entropy, is demonstrated. A consequence is that it is impossible to destroy information without increasing the entropy of a system; in practical terms this often means generating heat. Another more philosophical outcome is that information could be thought of as interchangeable with energy. Toyabe et al. experimentally showed in nature that information can be converted into work. Thus, in the study of logic gates, the theoretical lower bound of thermal energy released by an \"AND gate\" is higher than for the \"NOT gate\" (because information is destroyed in an \"AND gate\" and simply converted in a \"NOT gate\"). Physical information is of particular importance in the theory of quantum computers.\n\nIn thermodynamics, information is any kind of event that affects the state of a dynamic system that can interpret the information.\n\nThe information cycle (addressed as a whole or in its distinct components) is of great concern to information technology, information systems, as well as information science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc. Information does not cease to exist, it may only get scrambled beyond any possibility of retrieval (within information theory, see lossy compression; in physics, the black hole information paradox gets solved with the aid of the holographic principle).\n\nInformation visualization (shortened as InfoVis) depends on the computation and digital representation of data, and assists users in pattern recognition and anomaly detection.\nInformation security (shortened as InfoSec) is the ongoing process of exercising due diligence to protect information, and information systems, from unauthorized access, use, disclosure, destruction, modification, disruption or distribution, through algorithms and procedures focused on monitoring and detection, as well as incident response and repair.nalysis is the process of inspecting, transforming, and modelling information, by converting raw data into actionable knowledge, in support of the decision-making process.\n\nInformation quality (shortened as InfoQ) is the potential of a dataset to achieve a specific (scientific or practical) goal using a given empirical analysis method.\n\nInformation communication represents the convergence of informatics, telecommunication and audio-visual media & content.\n\nIt is estimated that the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 – which is the informational equivalent to less than one 730-MB CD-ROM per person (539 MB per person) – to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of almost 61 CD-ROM per person in 2007.\n\nThe world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was the informational equivalent of 6 newspapers per person per day in 2007.\n\nAs of 2007, an estimated 90% of all new information is digital, mostly stored on hard drives.\n\nRecords are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management ensures that the integrity of records is preserved for as long as they are required.\n\nThe international standard on records management, ISO 15489, defines records as \"information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business\". The International Committee on Archives (ICA) Committee on electronic records defined a record as, \"a specific piece of recorded information generated, collected or received in the initiation, conduct or completion of an activity and that comprises sufficient content, context and structure to provide proof or evidence of that activity\".\n\nRecords may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis expressed the view that sound management of business records and information delivered \"...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information.\"\n\nMichael Buckland has classified \"information\" in terms of its uses: \"information as process\", \"information as knowledge\", and \"information as thing\".\n\nBeynon-Davies explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.\n\nPragmatics is concerned with the purpose of communication. Pragmatics links the issue of signs with the context within which signs are used. The focus of pragmatics is on the intentions of living agents underlying communicative behaviour. In other words, pragmatics link language to action.\n\nSemantics is concerned with the meaning of a message conveyed in a communicative act. Semantics considers the content of communication. Semantics is the study of the meaning of signs - the association between signs and behaviour. Semantics can be considered as the study of the link between symbols and their referents or concepts – particularly the way that signs relate to human behavior.\n\nSyntax is concerned with the formalism used to represent a message. Syntax as an area studies the form of communication in terms of the logic and grammar of sign systems. Syntax is devoted to the study of the form rather than the content of signs and sign-systems.\n\nNielsen (2008) discusses the relationship between semiotics and information in relation to dictionaries. He introduces the concept of lexicographic information costs and refers to the effort a user of a dictionary must make to first find, and then understand data so that they can generate information.\n\nCommunication normally exists within the context of some social situation. The social situation sets the context for the intentions conveyed (pragmatics) and the form of communication. In a communicative situation intentions are expressed through messages that comprise collections of inter-related signs taken from a language mutually understood by the agents involved in the communication. Mutual understanding implies that agents involved understand the chosen language in terms of its agreed syntax (syntactics) and semantics. The sender codes the message in the language and sends the message as signals along some communication channel (empirics). The chosen communication channel has inherent properties that determine outcomes such as the speed at which communication can take place, and over what distance.\n\nKenett, R.S. and Shmueli, G. (2016) Information Quality: The Potential of Data and Analytics to Generate Knowledge, John Wiley and Sons, Chichester, UK\n\n\n"}
{"id": "12265948", "url": "https://en.wikipedia.org/wiki?curid=12265948", "title": "International breastfeeding symbol", "text": "International breastfeeding symbol\n\nThe International breastfeeding symbol was created by Matt Daigle, a graphic artist and father. He created the symbol in response to a contest hosted by \"Mothering\" magazine. The winner was chosen in November 2006 out of a total of more than 500 entries. Daigle, who says his wife and son were the inspiration behind the symbol, signed the symbol over to the public domain.\n\nIncreasing cultural diversity and personal mobility have created a growing need for universal modes of communication. The International Breastfeeding Symbol was created in the style of the AIGA symbol signs commonly seen in public places. These signs need to be designed carefully because they need to be understood at a glance by most people without written descriptions explaining what they mean.\n\nThe International Breastfeeding Symbol was created specifically to address the perceived problem of not having a universally accepted and understood symbol for breastfeeding available in public places. The modern iconography representing infancy usually involves artificial feeding or soothing objects, like a nurser bottle icon or pacifier symbol. Nursing rooms traditionally use a baby bottle symbol to designate what they are instead of a symbol of a mother nursing a child. The International Breastfeeding Symbol may be helpful in shifting the bottle-feeding cultural paradigm toward the biological norm of breastfeeding.\n\nIn July 2007, the International Breastfeeding Symbol site, dedicated to the new symbol, was launched. Examples of uses of the symbol include:\n\n\n"}
{"id": "41297122", "url": "https://en.wikipedia.org/wiki?curid=41297122", "title": "Jeong Yol", "text": "Jeong Yol\n\nJeong Yol (; born October 19, 1978) is a South Korean LGBT rights activist. Since the foundation of Solidarity for LGBT Human Rights of Korea in 1997, he has been an early member of SLRK. From 1998 to 2012, he had been a representative of SLRK. His real name is Jeong Min-suk (정민석).\n\nDuring the college time, he acknowledged that himself was gay. Later, while serving his mandatory military service, he came out as a homosexual. Then, his commanding officer took him to a mental hospital. After his discharge, he worked as a pastry chef for six years.\n\nIn 1997, he joined SLRK. In 2000, he spent more time primarily for SLRK, From 2002 to 2012, he has been a representative of SLRK. In February 2003, the word \"homosexual\" was pointed as harmful word for youth by South Korean government. So he struggled against the government with Kwak Yi-kyong, Chang Pyong-kwon and others. In April 2004, banning of word \"homosexual\" for youth was eliminated in South Korea.\n\nIn 2003, he led a movement for the Elimination of Discrimination against LGBT. In 2004, he joined anti-war and peace movements. Since 2006, he has fought for people living with HIV's rights.\n\n\n"}
{"id": "423729", "url": "https://en.wikipedia.org/wiki?curid=423729", "title": "Libertarianism (metaphysics)", "text": "Libertarianism (metaphysics)\n\nLibertarianism is one of the main philosophical positions related to the problems of free will and determinism, which are part of the larger domain of metaphysics. In particular, libertarianism, which is an incompatibilist position, argues that free will is logically incompatible with a deterministic universe and that agents have free will, and that, therefore, determinism is false. In the early modern period, some of the most important metaphysical libertarians were René Descartes, George Berkeley, Immanuel Kant, and Thomas Reid. Roderick Chisholm was a prominent defender of libertarianism in the 20th century, and contemporary libertarians include Robert Kane, Peter van Inwagen and Robert Nozick.\n\nThe first recorded use of the term \"libertarianism\" was in 1789 by William Belsham in a discussion of free will and in opposition to \"necessitarian\" (or determinist) views.\n\nMetaphysical libertarianism is one philosophical view point under that of incompatibilism. Libertarianism holds onto a concept of free will that requires the agent to be able to take more than one possible course of action under a given set of circumstances.\n\nAccounts of libertarianism subdivide into non-physical theories and physical or naturalistic theories. Non-physical theories hold that the events in the brain that lead to the performance of actions do not have an entirely physical explanation, and consequently the world is not closed under physics. Such interactionist dualists believe that some non-physical mind, will, or soul overrides physical causality.\n\nExplanations of libertarianism that do not involve dispensing with physicalism require physical indeterminism, such as probabilistic subatomic particle behavior – a theory unknown to many of the early writers on free will. Physical determinism, under the assumption of physicalism, implies there is only one possible future and is therefore not compatible with libertarian free will. Some libertarian explanations involve invoking panpsychism, the theory that a quality of mind is associated with all particles, and pervades the entire universe, in both animate and inanimate entities. Other approaches do not require free will to be a fundamental constituent of the universe; ordinary randomness is appealed to as supplying the \"elbow room\" believed to be necessary by libertarians.\n\nFree volition is regarded as a particular kind of complex, high-level process with an element of indeterminism. An example of this kind of approach has been developed by Robert Kane, where he hypothesises that,\nAlthough at the time quantum mechanics (and physical indeterminism) was only in the initial stages of acceptance, in his book \"Miracles: A preliminary study\" C. S. Lewis stated the logical possibility that if the physical world were proved indeterministic this would provide an entry point to describe an action of a non-physical entity on physical reality. Indeterministic physical models (particularly those involving quantum indeterminacy) introduce random occurrences at an atomic or subatomic level. These events might affect brain activity, and could seemingly allow incompatibilist free will if the apparent indeterminacy of some mental processes (for instance, subjective perceptions of control in conscious volition) map to the underlying indeterminacy of the physical construct. This relationship, however, requires a causative role over probabilities that is questionable, and it is far from established that brain activity responsible for human action can be affected by such events. Secondarily, these incompatibilist models are dependent upon the relationship between action and conscious volition, as studied in the neuroscience of free will. It is evident that observation may disturb the outcome of the observation itself, rendering limited our ability to identify causality. Niels Bohr, one of the main architects of quantum theory, suggested, however, that no connection could be made between indeterminism of nature and freedom of will.\n\nIn non-physical theories of free will, agents are assumed power to intervene in the physical world, a view known as agent causation. Proponents of agent causation include George Berkeley, Thomas Reid, and Roderick Chisholm.\n\nMost events can be explained as the effects of prior events. When a tree falls, it does so because of the force of the wind, its own structural weakness, and so on. However, when a person performs a free act, agent causation theorists say that the action was not caused by any other events or states of affairs, but rather was caused by the agent. Agent causation is ontologically separate from event causation. The action was not uncaused, because the agent caused it. But the agent's causing it was not determined by the agent's character, desires, or past, since that would just be event causation. As Chisholm explains it, humans have \"a prerogative which some would attribute only to God: each of us, when we act, is a prime mover unmoved. In doing what we do, we cause certain events to happen, and nothing – or no one – causes us to cause those events to happen.\" \n\nThis theory involves a difficulty which has long been associated with the idea of an unmoved mover. If a free action was not caused by any event, such as a change in the agent or an act of the will, then what is the difference between saying that an agent caused the event and simply saying that the event happened on its own? As William James put it, \"If a 'free' act be a sheer novelty, that comes not from me, the previous me, but ex nihilo, and simply tacks itself on to me, how can I, the previous I, be responsible? How can I have any permanent character that will stand still long enough for praise or blame to be awarded?\" \n\nAgent causation advocates respond that agent causation is actually more intuitive than event causation. They point to David Hume's argument that when we see two events happen in succession, our belief that one event caused the other cannot be justified rationally (known as the problem of induction). If that is so, where does our belief in causality come from? According to Thomas Reid, \"the conception of an efficient cause may very probably be derived from the experience we have had...of our own power to produce certain effects.\" Our everyday experiences of agent causation provide the basis for the idea of event causation.\n\nEvent-causal accounts of incompatibilist free will typically rely upon physicalist models of mind (like those of the compatibilist), yet they presuppose physical indeterminism, in which certain indeterministic events are said to be caused by the agent. A number of event-causal accounts of free will have been created, referenced here as \"deliberative indeterminism\", \"centred accounts\", and \"efforts of will theory\". The first two accounts do not require free will to be a fundamental constituent of the universe. Ordinary randomness is appealed to as supplying the \"elbow room\" that libertarians believe necessary. A first common objection to event-causal accounts is that the indeterminism could be destructive and could therefore diminish control by the agent rather than provide it (related to the problem of origination). A second common objection to these models is that it is questionable whether such indeterminism could add any value to deliberation over that which is already present in a deterministic world.\n\n\"Deliberative indeterminism\" asserts that the indeterminism is confined to an earlier stage in the decision process. This is intended to provide an indeterminate set of possibilities to choose from, while not risking the introduction of \"luck\" (random decision making). The selection process is deterministic, although it may be based on earlier preferences established by the same process. Deliberative indeterminism has been referenced by Daniel Dennett and John Martin Fischer. An obvious objection to such a view is that an agent cannot be assigned ownership over their decisions (or preferences used to make those decisions) to any greater degree than that of a compatibilist model.\n\n\"Centred accounts\" propose that for any given decision between two possibilities, the strength of reason will be considered for each option, yet there is still a probability the weaker candidate will be chosen. An obvious objection to such a view is that decisions are explicitly left up to chance, and origination or responsibility cannot be assigned for any given decision.\n\n\"Efforts of will theory\" is related to the role of will power in decision making. It suggests that the indeterminacy of agent volition processes could map to the indeterminacy of certain physical events – and the outcomes of these events could therefore be considered caused by the agent. Models of volition have been constructed in which it is seen as a particular kind of complex, high-level process with an element of physical indeterminism. An example of this approach is that of Robert Kane, where he hypothesizes that \"in each case, the indeterminism is functioning as a hindrance or obstacle to her realizing one of her purposes – a hindrance or obstacle in the form of resistance within her will which must be overcome by effort.\" According to Robert Kane such \"ultimate responsibility\" is a required condition for free will. An important factor in such a theory is that the agent cannot be reduced to physical neuronal events, but rather mental processes are said to provide an equally valid account of the determination of outcome as their physical processes (see non-reductive physicalism).\n\nEpicurus, an ancient Greek philosopher, argued that as atoms moved through the void, there were occasions when they would \"swerve\" (\"clinamen\") from their otherwise determined paths, thus initiating new causal chains. Epicurus argued that these swerves would allow us to be more responsible for our actions, something impossible if every action was deterministically caused.\n\nEpicurus did not say the swerve was directly involved in decisions. But following Aristotle, Epicurus thought human agents have the autonomous ability to transcend necessity and chance (both of which destroy responsibility), so that praise and blame are appropriate. Epicurus finds a \"tertium quid\", beyond necessity (Democritus' physics) and beyond chance. His \"tertium quid\" is agent autonomy, what is \"up to us.\"\n\nLucretius (1st century BC), a strong supporter of Epicurus, saw the randomness as enabling free will, even if he could not explain exactly how, beyond the fact that random swerves would break the causal chain of determinism.\n\nHowever, the interpretation of Greek philosophers is controversial. Tim O'Keefe has argued that Epicurus and Lucretius were not libertarians at all, but compatibilists.\n\nRobert Nozick put forward an indeterministic theory of free will in \"Philosophical Explanations\" (1981). \n\nWhen human beings become agents through reflexive self-awareness, they express their agency by having reasons for acting, to which they assign weights. Choosing the dimensions of one's identity is a special case, in which the assigning of weight to a dimension is partly self-constitutive. But all acting for reasons is constitutive of the self in a broader sense, namely, by its shaping one's character and personality in a manner analogous to the shaping that law undergoes through the precedent set by earlier court decisions. Just as a judge does not merely apply the law but to some degree makes it through judicial discretion, so too a person does not merely discover weights but assigns them; one not only weighs reasons but also weights them. Set in train is a process of building a framework for future decisions that we are tentatively committed to.\n\nThe lifelong process of self-definition in this broader sense is construed indeterministically by Nozick. The weighting is \"up to us\" in the sense that it is undetermined by antecedent causal factors, even though subsequent action is fully caused by the reasons one has accepted. He compares assigning weights in this deterministic sense to \"the currently orthodox interpretation of quantum mechanics\", following von Neumann in understanding a quantum mechanical system as in a superposition or probability mixture of states, which changes continuously in accordance with quantum mechanical equations of motion and discontinuously via measurement or observation that \"collapses the wave packet\" from a superposition to a particular state. Analogously, a person before decision has reasons without fixed weights: he is in a superposition of weights. The process of decision reduces the superposition to a particular state that causes action.\n\nOne particularly influential contemporary theory of libertarian free will is that of Robert Kane. Kane argues that \"(1) the existence of alternative possibilities (or the agent's power to do otherwise) is a necessary condition for acting freely, and that (2) determinism is not compatible with alternative possibilities (it precludes the power to do otherwise)\". It is important to note that the crux of Kane's position is grounded not in a defense of alternative possibilities (AP) but in the notion of what Kane refers to as ultimate responsibility (UR). Thus, AP is a necessary but insufficient criterion for free will. It is necessary that there be (metaphysically) real alternatives for our actions, but that is not enough; our actions could be random without being in our control. The control is found in \"ultimate responsibility\".\n\nUltimate responsibility entails that agents must be the ultimate creators (or originators) and sustainers of their own ends and purposes. There must be more than one way for a person's life to turn out (AP). More importantly, whichever way it turns out must be based in the person's willing actions. As Kane defines it,\n\nIn short, \"an agent must be responsible for anything that is a sufficient reason (condition, cause or motive) for the action's occurring.\"\n\nWhat allows for ultimacy of creation in Kane's picture are what he refers to as \"self-forming actions\" or SFAs—those moments of indecision during which people experience conflicting wills. These SFAs are the undetermined, regress-stopping voluntary actions or refraining in the life histories of agents that are required for UR. UR does not require that \"every\" act done of our own free will be undetermined and thus that, for every act or choice, we could have done otherwise; it requires only that certain of our choices and actions be undetermined (and thus that we could have done otherwise), namely SFAs. These form our character or nature; they inform our future choices, reasons and motivations in action. If a person has had the opportunity to make a character-forming decision (SFA), they are responsible for the actions that are a result of their character.\n\nRandolph Clarke objects that Kane's depiction of free will is not truly libertarian but rather a form of compatibilism. The objection asserts that although the outcome of an SFA is not determined, one's history up to the event \"is\"; so the fact that an SFA will occur is also determined. The outcome of the SFA is based on chance, and from that point on one's life is determined. This kind of freedom, says Clarke, is no different than the kind of freedom argued for by compatibilists, who assert that even though our actions are determined, they are free because they are in accordance with our own wills, much like the outcome of an SFA.\n\nKane responds that the difference between causal indeterminism and compatibilism is \"ultimate control—the originative control exercised by agents when it is 'up to them' which of a set of possible choices or actions will now occur, and up to no one and nothing else over which the agents themselves do not also have control\". UR assures that the sufficient conditions for one's actions do not lie before one's own birth.\n\nGalen Strawson holds that there is a fundamental sense in which free will is impossible, whether determinism is true or not. He argues for this position with what he calls his \"basic argument\", which aims to show that no-one is ever ultimately morally responsible for their actions, and hence that no one has free will in the sense that usually concerns us.\n\nIn his book defending compatibilism, \"Freedom Evolves\", Daniel Dennett spends a chapter criticising Kane's theory. Kane believes freedom is based on certain rare and exceptional events, which he calls self-forming actions or SFA's. Dennett notes that there is no guarantee such an event will occur in an individual's life. If it does not, the individual does not in fact have free will at all, according to Kane. Yet they will seem the same as anyone else. Dennett finds an essentially \"indetectable\" notion of free will to be incredible.\n\n\n\n"}
{"id": "4430241", "url": "https://en.wikipedia.org/wiki?curid=4430241", "title": "Loss of chance in English law", "text": "Loss of chance in English law\n\nLoss of chance in English law refers to a particular problem of causation, which arises in tort and contract. The law is invited to assess hypothetical outcomes, either affecting the claimant or a third party, where the defendant's breach of contract or of the duty of care for the purposes of negligence deprived the claimant of the opportunity to obtain a benefit and/or avoid a loss. For these purposes, the remedy of damages is normally intended to compensate for the claimant's loss of expectation (alternative rationales include restitution and reliance). The general rule is that while a loss of chance is compensable when the chance was something promised on a contract it is not generally so in the law of tort, where most cases thus far have been concerned with medical negligence in the public health system.\n\nIn contract cases, the court is usually interested in securing the performance of what was agreed. Where one party is about to or has suffered loss as a result of the other's breach, the court offers practical protection to his or her expectations as to performance (in some cases, the use of injunction or specific performance may be appropriate). Where a party proves that he or she has sustained loss flowing from any breach (potentially including non-pecuniary or intangible losses, e.g. for disappointment, damage to reputation, etc.), the purpose of damages is, so far as money can do it, to place the claimant in the same situation as if the contract had been performed. Thus, the most relevant basis upon which to calculate any loss is to examine the economic potential of the contract as worded. This will provide a measure of what the claimant expected to gain, and so quantify what has been lost by the breach.\n\nAs a matter of public policy, the law aims to respect the reasonable expectations of all parties involved in the dispute. The fundamental approach is therefore to uphold the validity of the contract wherever possible. Thus, there is no general protection offered to those who find they have entered into a bad bargain. All must accept the real outcomes of agreements entered into voluntarily (see freedom of contract). Even when there is a breach, the court will not penalise the \"guilty\" party (see \"Addis v Gramophone Co Ltd\" [1909] AC 488 which prevents the award of punitive or exemplary damages in a purely contractual action), nor will it strip away all profits made at the expense of the other unless the breach is exceptional as in \"Attorney General v Blake\" [2000] 3 WLR 635 which appears to create a wholly novel form of contractual remedy, namely the restitutionary remedy of an account of profits for breach of contract where the normal remedies are inadequate. The standard remedy is damages which are usually calculated by reference to the claimant only and do not reflect any form of penalty on the other(s) for exploiting the gullibility or innocence of the claimant. The law also recognises that unfairness may flow from inequality in bargaining power and addresses oppressive exemption clauses.\n\nThe primary difficulty in the calculation of damages is the question of causation. Remoteness will defeat a claim if it depends on very hypothetical possibilities. In \"McRae v Commonwealth Disposals Commission\" relying on rumours, the Commission sold to McRae the right to salvage an oil tanker thought to be marooned at the specified location. Unfortunately, the tanker did not exist. The Commission argued the contract was void because of a common mistake as to the existence of the subject matter, but the court noted that the Commission \"took no steps to verify what they were asserting and any 'mistake' that existed was induced by their own culpable conduct.\" McRae wasted money searching for the non-existent wreck. His claim for the loss of profits expected from a successful salvage was dismissed as too speculative, but reliance damages were awarded for wasted expenses. Nevertheless, the courts have been prepared to speculate. In \"Chaplin v Hicks\" (1911) 2 KB 786 the defendant in breach of contract prevented the claimant from taking part in the final stage of a beauty contest where twelve of the final fifty (out of 6,000 original entrants) would be rewarded with places in a chorus line. The claimant was awarded damages for the loss of a chance, assessed at 25% of winning the competition. The court seemed to proceed on the claimant's statistical chance of winning (as if she were a lottery player) without any actual assessment of her physical attributes against any particular criteria of beauty.\n\nYet \"Allied Maples Group Ltd v. Simmons & Simmons\" [1995] 1 WLR 1602 has partly restricted \"Chaplin v. Hicks\". A solicitor's negligence deprived the claimant of an opportunity to negotiate a better bargain. The Court of Appeal held that if the client could show on the balance of probabilities that: (a) they would have sought renegotiation with the third party, and (b) that they had a substantial chance of negotiating (not necessarily that they would on balance of probabilities have negotiated) a better deal from the third party, then the court should quantify and award compensation for their loss of chance of doing so. Stuart-Smith LJ, at p1611, accepted the 'loss of chance' approach and regarded the case as one of those where \"the plaintiff's loss depends on the hypothetical action of a third party, either in addition to action by the plaintiff … or independently of it.\" This inclusion of a third party in the equation to quantify loss could have been taken as a general precondition to all claim of loss cases, but Lord Nicholls in \"Gregg v Scott\" [2005] UKHL 2 said, \"It is clear that Stuart-Smith LJ. did not intend this to be a precise or exhaustive statement of the circumstances where loss of a chance may constitute actionable damage and his observation has not been so understood.\"\n\nIn \"Bank of Credit and Commerce International SA v Ali\" [2002] 1 AC 251 an employee made redundant by BCCI, claimed the usual statutory payments and, under the aegis of ACAS, signed an agreement to accept a sum \"in full and final settlement of all or any claims of whatsoever nature that exist or may exist against BCCI.\" The House of Lords held that this exclusion clause did not prevent employees from reopening their agreements when, following BCCI's collapse, it became clear that a significant part of the bank's business had been run dishonestly and the employees found that they were stigmatised for having worked there. When the parties signed the release, they could not have realistically supposed that a claim for damages in respect of disadvantage and stigma was a possibility. Accordingly, they claimed they could not have intended the release to apply to such a claim. But in earlier proceedings on the question of damages, the formidable practical obstacles presented by the limiting principles of causation, remoteness, and the duty of the claimant to mitigate any losses proved insurmountable. In 1999 Lightman J. tried five representative cases out of the 369 which had been initiated by former BCCI employees. None of them succeeded in proving that their unemployment was attributable to stigma. Indeed, subject to the anti-discrimination laws, a prospective employer is under no particular duty to employ anyone who attends for interview. Four of the cases tried by Lightman J. appear to have concerned employees who were dismissed by the liquidators when the bank collapsed in 1991. Those made redundant in 1990 faced the additional hurdle of having to explain why their unemployment was attributable to stigma when they were unable to find jobs for a year before any stigma attached to them.\n\nIn this context, \"Johnson (A.P.) v. Unisys Limited\" [2001] UKHL 13 rejects any interpretation of \"Addis v Gramophone Co Ltd\" that might have prevented an action for damage to reputation or for psychiatric injury arising from dismissal, but confirms formidable evidential difficulties on causation: How, for example, would the employee prove that his psychiatric condition was caused by the manner of the dismissal rather than the fact of the dismissal which is within an employer's power for cause? More generally, the case holds that claims for breach of contractual terms cannot be used to avoid statutory preconditions to making claims for unfair dismissal. Recently, in \"Harper v. Virgin Net\" [2004] EWCA Civ 271 the Court of Appeal decided that an employee who was summarily dismissed, cannot bring a claim for damages for the loss of the opportunity to initiate a claim for unfair dismissal. If she had served the minimum three-month period of notice stipulated in the contract, she would have been able to bring a claim for unfair dismissal. But although there was a breach of this term as to notice, there was no loss of chance to claim. She had not gained the chance by actually serving the minimum statutory period of twelve months to qualify and the action for breach of a contractual term could not be used to defeat Parliament's intention in specifying a minimum period of actual service.\n\nWhile the award of damages in tort may protect pre-existing expectations (e.g. of earning capacity or of business profits), a claimant cannot be seen to benefit from the breach of the duty of care. The measure of damages is therefore to ensure that the claimant is \"no worse off\" having suffered the breach of the duty of care. In each case, the claimant must prove the cause of action on the balance of probabilities. For these purposes, the court is required to speculate on what would have happened had there been no negligence. In many cases, loss and damage might have been sustained even if all had gone as planned. But there might always have been a chance that no long-term loss and damage would occur. For example, a person may attend a hospital with an existing injury. The only effect of any negligence in the treatment may be that the patient loses the chance of a full recovery, i.e. what was merely threatened becomes inevitable. Thus, actions by claimants whose chances of recovery from illness or injury have been reduced due to the professional negligence of their doctors have failed when they could not establish that, with proper treatment, their chances of recovery would have exceeded 50%. In \"Gregg v Scott\" [2005] UKHL 2; [2005] 2 WLR 268 a man whose chances of surviving non-Hodgkins Lymphoma for ten years were reduced from 42% to 25% by a delay in diagnosis could not claim damages because his chances were already too slim for the delay to have worsened his position. This was complicated by the fact that the case was brought before the court following an extended delay at which point the plaintiff was still alive. In judgement this was cited as a significant weakness in his claim. The principle is that a claimant must have had a more than 50% chance of survival to establish causation in order to satisfy the balance of probability test. However, in some Australian states, claims for loss of chance have been succeeded in medical negligence cases. Their approach argues that a patient would rather have a 42% than a 25% chance of survival. If negligence reduces the percentage, common sense justice rejects a black-and-white approach to accepting or rejecting a claim based on an expert's opinion as to whether there was ever a 50% chance of survival, and prefers to offer mitigated damages to represent the loss of chance.\nIn cases of economic loss, the rule that a claimant cannot normally recover for a lost chance is modified. In \"Kitchen v. Royal Airforce Association\" [1958] 2 All ER 241 a solicitor failed to issue a writ within the period of limitation in respect of a fatal accident. The surviving spouse sued for damages as she was unable to pursue her claim. There was no doubt that the loss was caused by the solicitors’ negligence and the only argument related to quantification of her claim. Although it was argued on behalf of the solicitors that the claimant might not have won her case, and may therefore have lost nothing, the court held that she had lost a chance and, as this was a valuable right, she should be compensated for it. Similarly, in \"Stovold v. Barlows\" (1996) PNLR 91 a solicitor acting for a vendor failed to use the appropriate system for sending the title deeds to a purchaser. Consequently, the claimant lost his chance to sell the property at a higher price. But damages were reduced by 50% as the court held that the purchaser might have bought another property even if the documents had arrived on time. In \"First Interstate Bank of California v Cohen Arnold & Co.\" (1996) PNLR 17 the claimant bank had loaned money to a client of the defendant accountants who negligently overstated the net worth of their clients. The bank then became concerned about the amount of the loan outstanding but, relying on the representations made by the defendant accountants, the bank delayed in calling in the loan. As a result of the delay in placing the property on the market, the price obtained was £1.45 million whereas the bank contended that it could have realised £3 million in an earlier sale. The Court of Appeal valued the chance at 66.66% on the assumption that “but for” the negligence, the property would actually have been sold for 66.66% of £3 million.\n\nIn commercial cases, damages are assessed not on the outcome which the claimant would have sought, but on the economic opportunity which he has lost. The claimant must prove on the balance of probabilities that he or she would have taken action to obtain the relevant benefit or avoid the relevant risk. Once this has been established, the claimant need only show that the chance which he or she has lost was real or substantial. In \"Coudert Brothers v. Normans Bay Ltd. (formerly Illingworth, Morris Ltd.)\" [2004] EWCA Civ 215 the court reviewed two earlier authorities:\"Allied Maples Group Ltd v Simmons & Simmons\" and \"Equitable Life Assurance Society v Ernst & Young\" (2003) EWCA Civ 1114. The claimant, Normans Bay Ltd. was advised by Coudert Brothers in a tender for 49% of the shares in a Russian company, Bolshevichka, in 1993, but the investment was lost. NBL claimed that, \"but for\" Coudert's negligence, the tender would have survived. At first instance, Buckley J assessed that chance of survival at 70%. The prior cases establish that loss of chance claims require proof on the balance of probabilities that:\nIf both of these are proved, the court must assess that chance lost. If the chance was low, the court will award a low percentage of the value of the chance in damages; if the chance had a high probability of success, a high percentage will be awarded. On appeal the award was reduced to 40%. The court also dismissed Coudert's argument that its own negligence had broken the chain of causation because, to allow such an argument, would be to allow a party to benefit from their own unlawful act.\n\n\n\n"}
{"id": "766488", "url": "https://en.wikipedia.org/wiki?curid=766488", "title": "Marketplace of ideas", "text": "Marketplace of ideas\n\nThe marketplace of ideas is a rationale for freedom of expression based on an analogy to the economic concept of a free market. The marketplace of ideas holds that the truth will emerge from the competition of ideas in free, transparent public discourse, and concludes that ideas and ideologies will be culled according to their superiority or inferiority and widespread acceptance among the population. The concept is often applied to discussions of patent law as well as freedom of the press and the responsibilities of the media in a liberal democracy.\n\nThe marketplace of ideas metaphor is founded in the philosophy of John Milton in his work \"Areopagitica\" in 1644 and also John Stuart Mill in his book, \"On Liberty\" in 1859. It was later used in opinions by the Supreme Court of the United States. The first reference to the \"free trade in ideas\" within \"the competition of the market\" appears in Justice Oliver Wendell Holmes, Jr.'s dissent in \"Abrams v. United States\". The phrase \"marketplace of ideas\" first appears in a concurring opinion by Justice William O. Douglas in the Supreme Court decision \"United States v. Rumely\" in 1953: \"Like the publishers of newspapers, magazines, or books, this publisher bids for the minds of men in the market place of ideas.\" The Court's 1969 decision in \"Brandenburg v. Ohio\" enshrined the marketplace of ideas as the dominant public policy in American free speech law (that is, against which narrow exceptions to freedom of speech must be justified by specific countervailing public policies). While the previous cases dealt with natural persons, the 1976 decision \"Virginia State Pharmacy Board v. Virginia Citizens Consumer Council\" expanded it to corporations by creating a curtailed corporate commercial speech right, striking down a government regulation of advertising in the process. It has not been seriously questioned since in United States jurisprudence, but the legacy of those decisions have lead to subsequent decisions like \"Citizens United v. FEC\" that curtailed the government's ability to regulate corporate speech and much more expansive advertising campaigns, commercial and political, than Americans had experienced previously.\n\nThe general idea that free speech should be tolerated because it will lead toward the truth has a long history. The English poet John Milton suggested that restricting speech was not necessary because \"in a free and open encounter\", truth would prevail. U.S. President Thomas Jefferson argued that it is safe to tolerate \"error of opinion ... where reason is left free to combat it\". Fredrick Siebert echoed the idea that free expression is self-correcting in \"Four Theories of the Press\": \"Let all with something to say be free to express themselves. The true and sound will survive. The false and unsound will be vanquished. Government should keep out of the battle and not weigh the odds in favor of one side or the other.\" These writers did not rely on the economic analogy to a market.\n\nIf beliefs such as religions are considered as ideas, the marketplace of ideas concept favors a marketplace of religions rather than forcing a state religion or forbidding incompatible beliefs. In this sense, it provides a rationale for freedom of religion.\n\nIn recent years questions have arisen regarding the existence of markets in ideas. Several scholars have noted differences between the way ideas are produced and consumed and the way more traditional goods are produced and consumed. It has also been argued that the idea of the marketplace of ideas as applied to religion \"incorrectly assumes a level playing field\" among religions. In addition, the idea of a marketplace of ideas has been applied to the study of scientific research as a social institution.\n\n"}
{"id": "2091242", "url": "https://en.wikipedia.org/wiki?curid=2091242", "title": "McIntyre Final Eight System", "text": "McIntyre Final Eight System\n\nThe McIntyre Final Eight System was devised by Ken McIntyre in addition to the McIntyre Four, Five and Six systems. It is a playoff system of the top 8 finishers in a competition to determine which two teams will play in the grand final. The teams play each other over three weeks, with two teams eliminated each week. Teams who finish in a higher position in the competition are given an easier route to the grand final.\n\nIn top level sport, the system was used by the Australian Football League from 1994 until 1999, and by the National Rugby League from 1999 to 2011.\n\nWeek 1\n\n\nThe organisation of the rest of the finals series is dependent upon whether teams won or lost in week 1 and their final ranking on the ladder before the finals. The two lowest-ranked losers are eliminated from the finals, while the two highest-ranked winners progress straight to week 3.\n\nWeek 2\n\n\nThe two losing teams are eliminated, the two winning teams progress to week 3.\n\nWeek 3\n\n\nThe two losing teams are eliminated, the two winning teams progress to week four.\n\nWeek 4\n\n\nScheduling\n\nA key element of an effective McIntyre system is scheduling in week 1. In the first week games must be played in the following order: 4 v 5, 3 v 6, 2 v 7, 1 v 8. Teams in the first two games are playing for the chance of a bye in the second week of the finals. If the final two games ultimately go as predicted, then the chance of a bye or the risk of elimination disappears. Therefore, those games need to be played last so that there is never a situation where two teams know that their result would not matter.\n\n1st: Advances with a win to preliminary final (week 3). Must play the semi final with a loss, cannot be eliminated in week 1, and has a 18,75% of winning the tournament.\n\n2nd: Same as 1st, but has a more difficult opponent in week 1 (7th instead 8th).\n\n3rd: Advances to preliminary final with a win AND at least one upset in one of the two last qualifying finals (1st or 2nd loses his qualifying final). Must play the semi final with a win and no upset in last two qualifying finals or a loss if there is at least one hopeful result in two last qualifying finals. Is eliminated if he loses AND 1st and 2nd too. (Has a 15.625% of winning the tournament).\n\n4th: Advances to preliminary final with a win AND there are at least two upsets in remaining qualifying finals. It must play the semi final if there are, at least, two hopeful results in other qualifying finals, regardless his result. Is eliminated with a loss AND, at least, two upsets (He has a 12,5% of winning the tournament).\n\n5th: Same as 4th (but play away his qualifying final against 4th).\n\n6th: Advances to preliminary final with a win AND 7th and 8th win their qualifying finals. It must play the semi final with a win AND, if there is, at least, one hopeful result, OR, if he loses, if there are BOTH hopeful results in remaining qualifying finals. Is eliminated with a loss AND one upset, at least, in remaining qualifying finals. (He has a 9,375% of winning the tournament).\n\n7th: It can't advance straight to preliminary final. Advances to semi final with a win and is eliminated with a loss (He has a 6,25% of winning the tournament).\n\n8th: Same as 7th (but he has a more difficult opponent in week 1 -1st instead 2nd).\n\n1st, 2nd, 7th and 8th depend on themselves. 3rd, 4th, 5th and 6th depend on their results and other qualifying finals results\n\nThe 2010 NRL Final Series:\nAt the qualifying finals, team 1 won and so went straight through to the preliminary finals. Team 4 also went through to the preliminary finals, because they won while teams 2 and 3 lost. However, teams 2 and 3 were not eliminated, but played again in the semi finals, because two teams ranking lower than them on the ladder also lost. Those two losing teams ranking lower than them, teams 5 and 8, were eliminated.\n\nThe major advantages of the system are the number of different combinations of teams which could make the final game and that no matches are repeated in the first three weeks. When compared to other final eight systems, many of which split the participants into two groups, the McIntyre system means only two of the 28 possible combinations (1v7 and 2v8) are impossible in the grand final.\n\nThe top two teams after the regular season are rewarded by being given a 'second life' within the finals. If either of these two teams lose to their much lower-ranked opponents in the first week, then one of the two losing teams ranked lower than them are eliminated, which means the 1st- and 2nd-ranked teams can withstand a loss in the finals and their season will continue, albeit with significant disadvantages.\n\nThis 'second life' advantage for the highest-ranked teams on the ladder can flow on to teams 3 and 4 (and possibly teams 5 and 6.) If only one of the top two on the ladder loses in the first week, then team 3 is not eliminated if it has lost. Similarly, if only one of the top three on the ladder loses in the first week, then team 4 is saved from elimination if it has lost.\n\nIf the top two teams win in the first week, then teams 7 and 8 will be the ones to be eliminated. Only two teams drop out, so even if they were to lose their games, teams 5 and 6 would still have another chance in the second week.\n\nWith its adoption by the NRL, debate has arisen over its fairness. The McIntyre system rewards teams who have form coming into the finals rather than during the whole season. The advantages given to a victor in the first week of the finals, even if that team is initially ranked 6th to 8th, includes a home final in the second week against a team ranked 3rd to 6th coming off a loss. This advantage given to lower-ranked teams that win in the first week are significant compared to the alternate final 8 system used by the AFL, which protects teams coming 1st to 4th from elimination and never give home finals to teams ranked 7th or 8th, regardless of whether they win or lose their matches.\n\nIn 2008, the first week of the NRL finals saw the then reigning grand finalists the Melbourne Storm lose to the 8th placed New Zealand Warriors. Granted a home final as a week 1 winner, the Warriors then defeated the Sydney Roosters in the second week and proceeded to the final 4, the first team ever to make it that far from 8th position, whereas the Storm had to travel to Brisbane and win away to continue on. This scenario was exceeded in 2009 when the Parramatta Eels, who had finished 8th defeated St George Illawarra Dragons. Parramatta, with a home advantage, proceeded to defeat the Gold Coast Titans in week 2, whereas the Dragons were eliminated from the competition in week 2 in their away match against the Brisbane Broncos. This gave them the dubious distinction of being the first minor premiers to be eliminated after two consecutive losses since the inception of the McIntyre System. Parramatta became the first team ranked last of the finalists to contest the grand final, only to lose to the 4th-placed Melbourne Storm, although this Premiership has been struck from the record due to salary cap breaches by the Storm. For the record, at least one top four team has lost its qualifying final in every year since this system was introduced in 1999, until 2011 when all top four teams won their qualifying final.\n\nAnother criticism is that, like many other top-8 systems, there is the possibility of games in the first week that are effectively meaningless, where teams have no risk of elimination and results only determine respective opponents and home ground advantage in the second week. In the McIntyre system if first-week results go as planned, then first defeats eighth and second defeats seventh. This leaves the teams who finished from third to sixth effectively playing \"dead rubbers\" in the first week, with the results merely reshuffling the order of these four teams. There is a requirement therefore that the 4v5 contest be played first. This raises a further criticism that the fate of the loser in this match is dependent upon the performance of the higher-ranked teams. A good example of this was the Dragons v Penrith final in 2004. The Dragons lost by a mere point, however, due to the higher ranked Bulldogs and Broncos losing the Dragons were eliminated.\n\nThis also makes scheduling much less flexible, since the first vs eighth game must be the last game played, in order to prevent the teams between third and sixth entering their qualifying finals knowing that the game is already a dead rubber. The newly formed ARL Commission accepted a recommendation to scrap the system as of 2012 on the 22nd of February, 2012. It was replaced by the final eight system currently used by the AFL.\n\nAnother anomaly of scheduling is that in the second week a team may play a higher-ranked opponent than the team they defeated, and similarly a first week loser may play an easier opponent than the team that defeated them. In the second week of the McIntyre system the third highest winner (i.e. the strongest winner of those playing) plays the highest-ranked loser rather than the second highest loser (i.e. the weakest loser). This may ensure no repetition of games in the second week, but it means higher-ranked teams end up with more difficult opponents simply for the sake of more interesting scheduling. An example of this happened in 2005 when the Wests Tigers (4th) defeated the North Queensland Cowboys (5th) in week one of the finals and were 'rewarded' in week 2 with a game against the 3rd placed Brisbane Broncos. Meanwhile, the Cowboys (ranked 5th and first week losers) played the Melbourne Storm who were ranked 6th.\n\nIn 2011, the 6th-placed New Zealand Warriors were beaten convincingly by the Brisbane Broncos (ranked 3rd) in its qualifying final by the score of 40–10. Following that match, then-Warriors coach Ivan Cleary was quoted as saying \"the way we played tonight, we don't deserve to be in the finals\". That loss saw them at risk of being eliminated initially, but were granted a reprieve after the two lower-ranking teams, North Queensland and Newcastle, also lost their finals. A sudden reverse in form would see them advance to the grand final, thus becoming the third team (after the St. George Illawarra Dragons in 1999 and the Sydney Roosters the season previous), to advance to the grand final after finishing sixth at the end of the season. Ultimately, the Warriors would lose to the second-placed Manly-Warringah Sea Eagles who had a much easier run to the decider.\n\nIn addition to the NRL, the McIntyre Final Eight System is also used in the Rugby League National League Three in Great Britain, the NSWRL Premier League and Jersey Flegg competitions. It will be used in the final heads up in Circuito Nacional de Poker (Spain).\n\n\n"}
{"id": "19391912", "url": "https://en.wikipedia.org/wiki?curid=19391912", "title": "Media naturalness theory", "text": "Media naturalness theory\n\nMedia naturalness theory is also known as the psychobiological model. The theory was developed by Ned Kock and attempts to apply Darwinian evolutionary principles to suggest which types of computer-mediated communication will best fit innate human communication capabilities. Media naturalness theory argues that natural selection has resulted in face-to-face communication becoming the most effective way for two people to exchange information.\n\nThe theory has been applied to human communication outcomes in various contexts, such as: education, knowledge transfer, communication in virtual environments, e-negotiation, business process improvement, trust and leadership in virtual teamwork, online learning, maintenance of distributed relationships, performance in experimental tasks using various media, and modular production. Media naturalness theory can be considered a Darwinian theory of behavior toward certain types of communication media. Its development is also consistent with ideas from the field of evolutionary psychology.\n\nThe media naturalness theory builds on the media richness theory's arguments that face-to-face interaction is the richest type of communication medium by providing an evolutionary explanation for the medium's richness. Media naturalness theory argues that since our Stone Age hominid ancestors have communicated primarily face-to-face, evolutionary pressures have led to the development of a brain that is consequently designed for that form of communication. Kock points out that computer-mediated communication is far too recent a phenomenon to have had the time necessary to shape human cognition and language capabilities via natural selection. In turn, Kock argues that using communication media that suppress key elements found in face-to-face communication, as many electronic communication media do, thus ends up posing cognitive obstacles to communication, and particularly in the case of complex tasks (e.g., business process redesign, new product development, online learning), because such tasks seem to require more intense communication over extended periods of time than simple tasks.\n\nA simple thought experiment highlights the biological basis of media naturalness theory, and the fundamental difference between the media naturalness and media richness theories. Let us assume that the human species had evolved in an ancestral environment without light. If that were the case, modern humans would all be blind, and therefore a communication medium's ability to convey facial expressions and body language would be irrelevant for effective communication. Conversely, a medium's ability to convey smell might be fairly important for effective communication. This illustrates the fact that one cannot define a medium's ability to support effective communication without taking into consideration characteristics of the communicators. Of these, biological characteristics often have an evolutionary basis.\n\nThe naturalness of a communication medium is defined, in media naturalness theory, as the degree of similarity of the medium with the face-to-face medium. The face-to-face medium is presented as the medium enabling the highest possible level of communication naturalness, which is characterized by the following five key elements: (1) a high degree of co-location, which would allow the individuals engaged in a communication interaction to see and hear each other; (2) a high degree of synchronicity, which would allow the individuals to quickly exchange communicative stimuli; (3) the ability to convey and observe facial expressions; (4) the ability to convey and observe body language; and (5) the ability to convey and listen to speech.\n\nMedia naturalness theory predicts that any electronic communication medium allowing for the exchange of significantly less or more communicative stimuli per unit of time than the face-to-face medium will pose cognitive obstacles to communication. In other words, media naturalness theory places the face-to-face medium at the center of a one-dimensional scale of naturalness, where deviations to the left or right are associated with decreases in naturalness (see Figure 1).\n\nElectronic media that enable the exchange of significantly more communicative stimuli per unit of time than the face-to-face medium are classified by media naturalness theory as having a lower degree of naturalness than the face-to-face medium. As such, those media are predicted to be associated with higher cognitive effort; in this case due primarily to a phenomenon known as information overload, which is characterized by individuals having more communicative stimuli to process than they are able to.\n\nMedia naturalness effects on cognitive effort, communication ambiguity, and physiological arousal. Media naturalness theory's main prediction is that, other things being equal, a decrease in the degree of naturalness of a communication medium leads to the following effects in connection with communication interactions in complex tasks: (a) an increase in cognitive effort, (b) an increase in communication ambiguity, and (c) a decrease in physiological arousal.\n\nNaturalness of electronic communication media. Electronic communication media often suppress key face-to-face communication elements, with the goal of creating other advantages. For example, Web-based bulletin boards and discussion groups enable asynchronous (or time-disconnected) communication, but at the same time make it difficult to have the same level of feedback immediacy found in face-to-face communication. That often leads to frustration from users who expect immediate feedback on their postings.\n\nThe high importance of speech. Media naturalness theory predicts that the degree to which an electronic communication medium supports an individual's ability to convey and listen to speech is particularly significant in determining its naturalness. The theory predicts, through its speech imperative proposition, that speech enablement influences naturalness significantly more than a medium's degree of support for the use of facial expressions and body language.\n\nCompensatory adaptation. According to media naturalness theory, electronic communication media users can adapt their behavior in such a way as to overcome some of the limitations of those media. That is, individuals who choose to use electronic communication media to accomplish complex collaborative tasks may compensate for the cognitive obstacles associated with the lack of naturalness of the media. One of the ways in which this can be achieved through email is by users composing messages that are redundant and particularly well organized, compared to face-to-face communication. This often contributes to improving the effectiveness of communication, sometimes even beyond that of the face-to-face medium.\n\nHuman beings possess specialized brain circuits that are designed for the recognition of faces and the generation and recognition of facial expressions, which artificial intelligence research suggests require complex computations that are difficult to replicate even in powerful computers. The same situation is found in connection with speech generation and recognition. Generation and recognition of facial expressions, and speech generation and recognition, are performed effortlessly by humans.\n\nCognitive effort is defined in media naturalness theory as the amount of mental activity, or, from a biological perspective, the amount of brain activity involved in a communication interaction. It can be assessed directly, with the use of techniques such as magnetic resonance imaging. Cognitive effort can also be assessed indirectly, based on perceptions of levels of difficulty associated with communicative tasks, as well as through indirect measures such as that of fluency. Fluency is defined as the amount of time taken to convey a certain number of words through different communication media, which is assumed to correlate (and serve as a surrogate measure of) the amount of time taken to convey a certain number of ideas through different media. According to media naturalness theory, a decrease in the degree of naturalness of a communication medium leads to an increase in the amount of cognitive effort required to use the medium for communication.\n\nIndividuals brought up in different cultural environments usually possess different information processing schemas that they have learned over their lifetimes. Different schemas make individuals interpret information in different ways, particularly when information is expected but not actually provided.\n\nWhile different individuals are likely to look for the same types of communicative stimuli, their interpretation of the message being communicated in the absence of those stimuli will be largely based on their learned schemas, which are likely to differ from those held by other individuals (no two individuals, not even identical twins raised together, go through exactly the same experiences during their lives). According to media naturalness theory, a decrease in medium naturalness, caused by the selective suppression of media naturalness elements in a communication medium, leads to an increase in the probability of misinterpretations of communicative cues, and thus an increase in communication ambiguity.\n\nTo say that our genes influence the formation of a phenotypic trait (i.e., a biological trait that defines a morphological, behavioral, physiological, etc. characteristic) does not mean the same as saying that the trait in question is innate. Very few phenotypic traits are innate (e.g., blood type); the vast majority, including most of those in connection with our biological communication apparatus, need interaction with the environment to be fully and properly developed.\n\nWhile there is substantial evidence suggesting that our biological communication apparatus is designed for face-to-face communication, there is also ample evidence that such an apparatus (including the neural functional language system) cannot be fully developed without a significant amount of practice. Thus, according to media naturalness theory, evolution must have shaped brain mechanisms to compel human beings to practice the use of their biological communication apparatus; mechanisms that are similar to those compelling animals to practice those skills that play a key role in connection with survival and mating. Among these mechanisms, one of the most important is that of physiological arousal, which is often associated with excitement and pleasure. Engaging in communication interactions, particularly in face-to-face situations, triggers physiological arousal in human beings. Suppression of media naturalness elements makes communication interactions duller than if those elements were present.\n\nComplex speech was enabled by the evolution of a larynx located relatively low in the neck, which considerably increased the variety of sounds that our species could generate; this is actually one of the most important landmarks in the evolution of the human species. However, that adaptive design also significantly increased our ancestors' chances of choking on ingested food and liquids, and suffering from aerodigestive tract diseases such as gastroesophageal reflux. This leads to an interesting conclusion, which is that complex speech must have been particularly important for effective communication in our evolutionary past, otherwise the related evolutionary costs would prevent it from evolving through natural selection. This argument is similar to that made by Amotz Zahavi in connection with evolutionary handicaps. If a trait evolves to improve the effectiveness in connection with a task, in spite of imposing a survival handicap, then the trait should be a particularly strong determinant of the performance in the task to offset the survival cost it imposes.\n\nMedia naturalness theory builds on this evolutionary handicap conclusion to predict that the degree to which an electronic communication medium supports an individual's ability to convey and listen to speech is particularly significant in defining its naturalness. Media naturalness theory predicts, through its speech imperative proposition, that speech enablement influences naturalness significantly more than a medium's degree of support for the use of facial expressions and body language. This prediction is consistent with past research showing that removing speech from an electronic communication medium significantly increases the perceived mental effort associated with using the medium to perform knowledge-intensive tasks. According to this prediction, a medium such as audio conferencing is relatively close to the face-to-face medium in terms of naturalness (see Figure 2).\n\nIncreases in cognitive effort and communication ambiguity are usually accompanied by an interesting behavioral phenomenon, called compensatory adaptation. The phenomenon is characterized by voluntary and involuntary attempts by the individuals involved in a communicative act to compensate for the obstacles posed by an unnatural communication medium. One of the key indications of compensatory adaptation is a decrease in communication fluency, which can be measured through the number of words conveyed per minute through a communication medium. That is, communication fluency is believed to go down as a result of individuals making an effort to adapt their behavior in a compensatory way.\n\nFor example, an empirical study suggests that when individuals used instant messaging and face-to-face media to perform complex and knowledge-intensive tasks, the use of the electronic (i.e., instant messaging) medium caused several effects. Those effects were consistent with media naturalness theory, and the compensatory adaptation notion. Among those effects, the electronic medium increased perceived cognitive effort by approximately 40% and perceived communication ambiguity by approximately 80% – as predicted by media naturalness theory. The electronic medium also reduced actual fluency by approximately 80%, and the quality of the task outcomes was not affected, suggesting compensatory adaptation.\n\n\n"}
{"id": "777042", "url": "https://en.wikipedia.org/wiki?curid=777042", "title": "Mercy", "text": "Mercy\n\nMercy (Middle English, from Anglo-French \"merci\", from Medieval Latin \"merced-\", \"merces\", from Latin, \"price paid, wages\", from \"merc-\", \"merxi\" \"merchandise\") is benevolence, forgiveness, and kindness in a variety of ethical, religious, social, and legal contexts.\n\nThe concept of a merciful God appears in various religions, including Hinduism, Christianity, Judaism and Islam. Performing acts of mercy as a component of religious beliefs is also emphasized through actions such as the giving of alms, and care for the sick and Works of Mercy.\n\nIn the social and legal context, mercy may refer both to compassionate behavior on the part of those in power (e.g. mercy shown by a judge toward a convict), or on the part of a humanitarian third party, e.g., a mission of mercy aiming to treat war victims.\n\n\"Mercy\" can be defined as \"compassion or forbearance shown especially to an offender or to one subject to one's power\"; and also \"a blessing that is an act of divine favor or compassion.\" \"To be at someone's mercy\" indicates a person being \"without defense against someone.\"\n\nIn a judicial context mercy is often termed \"clemency\". It is a sovereign prerogative that resides in the executive and is entirely discretionary. John Locke defined it as \"the power to act according to discretion, for the public good, without the prescription of the Law, and sometimes even against it.\" The U.S. Court of Appeals for the Sixth Circuit explained that \"The very nature of clemency is that it is grounded solely in the will of the dispenser of clemency. He need give no reasons for granting it or for denying it.\"\n\nHebrews 4:16 says, \"So let us confidently approach the throne of grace to receive mercy and to find grace for timely help.\" Grace and mercy are similar in that both are free gifts of God and both are dispensed absent any merit on the part of the recipient. Grace is the favor of God, a divine assistance. Grace is what one receives that they do not deserve while mercy is what one receives when they do \"not\" get that which they deserve. \n\nAn emphasis on mercy appears in the New Testament, for example in the Magnificat and Benedictus (Song of Zechariah), in Luke's Gospel, and in the Beatitudes in : \"Blessed are the merciful: for they shall obtain mercy\". In Apostle Paul refers to the mercy of God in terms of salvation: \"God, being rich in mercy... even when we were dead through our sins, made us alive together with Christ\".\n\nPsalm 117 calls upon all nations to praise the Lord, and that on account of his \"merciful kindness\". This is quoted by the Apostle Paul in Romans 15:11 to show that God has now fulfilled this prophecy and promise through Jesus Christ, who has been merciful in giving his life as a sacrifice for his people, both Jew and Gentile. Thus St Peter writes in 1 Peter 2:9,10:\n\nThis devotional element of mercy as part of the Christian tradition was echoed by Saint Augustine who called mercy \"ever ancient, ever new\". The Works of Mercy (seven corporal and seven spiritual works) are part of the Catholic and Eastern Orthodox traditions.\n\nIn the encyclical \"Dives in misericordia\" (\"Rich in Mercy\") Pope John Paul II examines the role of mercy—both God's mercy, and also the need for human mercy. He sees in the Parable of the Prodigal Son () \"the essence of the divine mercy\". Having squandered his patrimony, justice would dictate that the prodigal should only expect to be received back as a hireling. The figure of the father is analogous to God as Father, who goes beyond the requirements of justice to welcome his son with compassion.\n\nThe Catechism of the Catholic Church emphasizes the importance of the Works of Mercy (item 2447) and in Roman Catholic teachings, the mercy of God flows through the work of the Holy Spirit. Roman Catholic liturgy includes frequent references to mercy, e.g., as in \"Kyrie eleison, Christe eleison\": Lord have mercy, Christ have mercy.\n\nMercy has also been an important subject of Christian iconography. Since the Middle Ages, many representations in art encouraged people to practice the works of mercy and, as the art historian Ralf van Bühren explains using the example of Caravaggio, helped \"the audience to explore mercy in their own lives\".\n\nIn the 20th century, there was new focus on mercy in the Roman Catholic Church, partly due to the Divine Mercy devotion. The primary focus of the Divine Mercy devotion is the merciful love of God and the desire to let that love and mercy flow through one's own heart towards those in need of it. \nPope John Paul II was a follower of the Divine Mercy devotion, due to Saint Mary Faustina Kowalska (1905–1938), who is known as the \"Apostle of Mercy\". \n\nA number of Roman Catholic shrines are specifically dedicated to Divine Mercy, e.g.\nthe Basilica of Divine Mercy in Krakow Poland, and the National Shrine of The Divine Mercy (Stockbridge, Massachusetts). During the dedication of the Basilica of Divine Mercy John Paul II quoted and called mercy the \"greatest attribute of God Almighty\".\n\nThe first \"World Apostolic Congress on Mercy\" was held in Rome in April 2008 and was inaugurated by Pope Benedict XVI.\n\nOn 11 April, 2015, at St. Peter's Basillica, in a Papal Bull of Indiction entitled \"Misericordiae Vultus\" (\"The Face of Mercy\"), Pope Francis proclaimed a Special and Extraordinary Holy Year Jubilee Year of Mercy, from December 8, 2015: Solemnity of the Immaculate Conception of the Blessed Virgin Mary, until November 21, 2016: the Solemnity of Our Lord Jesus Christ the King. The theme of the Extraordinary Jubilee is taken from Luke 6:36, \"Merciful, Like the Father\".\n\nIn Islam the title \"Most Merciful\" (al-Rahman) is one of the names of Allah and Compassionate (al-Rahim), is the most common name occurring in the Quran. Rahman and Rahim both derive from the root Rahmat, which refers to tenderness and benevolence. As a form of mercy, the giving of alms (zakat) is the fourth of the Five Pillars of Islam and one of the requirements for the faithful.\n\nThe Hebrew word for mercy is \"Rachamim\" which is always in plural form so that it literally means \"mercies\". \"Mercy includes showing kindness to those who don’t deserve it, and forgiving those that deserve punishment.\"\n\nMercy is one of the defining characteristics of God. Exodus 34:6 says: \"The Lord, the Lord, a God merciful and gracious, slow to anger, and abounding in steadfast love and faithfulness.\" This is also emphasized in the context of the Babylonian exile in Isaiah: \"For the Lord has comforted his people, and will have compassion on his suffering ones. But Zion said, 'The Lord has forsaken me, my Lord has forgotten me.' Can a woman forget her nursing child, or show no compassion for the child of her womb? Even these may forget, yet I will not forget you.\" (Isaiah 49:13-15) Also: \"It is good to pray and fast, to be merciful and just.\" (Tobit 12:8) \n\nWhen David, because of his sin, was told to choose between a three-year famine, pursuit by his enemies for three months, or a three-day pestilence, he chose the pestilence saying, \"Let us fall by the hand of God, for he is most merciful; but let me not fall by the hand of man.\" Psalm 103:8 praises God for his mercy.\n\nKwan Yin the bodhisattva of mercy and compassion, is one of the best known and most venerated Bodhisattva in Asia.\n\nKaruṇā (often translated as \"compassion\") is part of the beliefs of both Buddhism and Jainism. Karuṇā is present in all schools of Buddhism and in Jainism it is viewed as one of the reflections of universal friendship.\n\nThe spiritual teacher Meher Baba described God as being \"all-merciful and eternally benevolent\" in his O Parvardigar prayer, and he held that we can approach God through the \"invocation of His mercy.\"\n\n\n\n"}
{"id": "195795", "url": "https://en.wikipedia.org/wiki?curid=195795", "title": "Metric tensor", "text": "Metric tensor\n\nIn the mathematical field of differential geometry, a metric tensor is a type of function which takes as input a pair of tangent vectors and at a point of a surface (or higher dimensional differentiable manifold) and produces a real number scalar in a way that generalizes many of the familiar properties of the dot product of vectors in Euclidean space. In the same way as a dot product, metric tensors are used to define the length of and angle between tangent vectors. Through integration, the metric tensor allows one to define and compute the length of curves on the manifold.\n\nA metric tensor is called \"positive-definite\" if it assigns a positive value to every nonzero vector . A manifold equipped with a positive-definite metric tensor is known as a Riemannian manifold. On a Riemannian manifold, the curve connecting two points that (locally) has the smallest length is called a geodesic, and its length is the distance that a passenger in the manifold needs to traverse to go from one point to the other. Equipped with this notion of length, a Riemannian manifold is a metric space, meaning that it has a distance function whose value at a pair of points and is the distance from to . Conversely, the metric tensor itself is the derivative of the distance function (taken in a suitable manner). Thus the metric tensor gives the \"infinitesimal\" distance on the manifold.\n\nWhile the notion of a metric tensor was known in some sense to mathematicians such as Carl Gauss from the early 19th century, it was not until the early 20th century that its properties as a tensor were understood by, in particular, Gregorio Ricci-Curbastro and Tullio Levi-Civita, who first codified the notion of a tensor. The metric tensor is an example of a tensor field.\n\nThe components of a metric tensor in a coordinate basis take on the form of a symmetric matrix whose entries transform covariantly under changes to the coordinate system. Thus a metric tensor is a covariant symmetric tensor. From the coordinate-independent point of view, a metric tensor field is defined to be a nondegenerate symmetric bilinear form on each tangent space that varies smoothly from point to point.\n\nCarl Friedrich Gauss in his 1827 \"Disquisitiones generales circa superficies curvas\" (\"General investigations of curved surfaces\") considered a surface parametrically, with the Cartesian coordinates , , and of points on the surface depending on two auxiliary variables and . Thus a parametric surface is (in today's terms) a vector-valued function\n\ndepending on an ordered pair of real variables , and defined in an open set in the -plane. One of the chief aims of Gauss's investigations was to deduce those features of the surface which could be described by a function which would remain unchanged if the surface underwent a transformation in space (such as bending the surface without stretching it), or a change in the particular parametric form of the same geometrical surface.\n\nOne natural such invariant quantity is the length of a curve drawn along the surface. Another is the angle between a pair of curves drawn along the surface and meeting at a common point. A third such quantity is the area of a piece of the surface. The study of these invariants of a surface led Gauss to introduce the predecessor of the modern notion of the metric tensor.\n\nIf the variables and are taken to depend on a third variable, , taking values in an interval , then will trace out a parametric curve in parametric surface . The arc length of that curve is given by the integral\n\nwhere formula_3 represents the Euclidean norm. Here the chain rule has been applied, and the subscripts denote partial derivatives:\n\nThe integrand is the restriction to the curve of the square root of the (quadratic) differential\n\nwhere\n\nThe quantity in () is called the line element, while is called the first fundamental form of . Intuitively, it represents the principal part of the square of the displacement undergone by when is increased by units, and is increased by units.\n\nUsing matrix notation, the first fundamental form becomes\n\nSuppose now that a different parameterization is selected, by allowing and to depend on another pair of variables and . Then the analog of () for the new variables is\nThe chain rule relates , , and to , , and via the matrix equation\n\nwhere the superscript T denotes the matrix transpose. The matrix with the coefficients , , and arranged in this way therefore transforms by the Jacobian matrix of the coordinate change\n\nA matrix which transforms in this way is one kind of what is called a tensor. The matrix\n\nwith the transformation law () is known as the metric tensor of the surface.\n\n first observed the significance of a system of coefficients , , and , that transformed in this way on passing from one system of coordinates to another. The upshot is that the first fundamental form () is \"invariant\" under changes in the coordinate system, and that this follows exclusively from the transformation properties of , , and . Indeed, by the chain rule,\n\nso that\n\nAnother interpretation of the metric tensor, also considered by Gauss, is that it provides a way in which to compute the length of tangent vectors to the surface, as well as the angle between two tangent vectors. In contemporary terms, the metric tensor allows one to compute the dot product of tangent vectors in a manner independent of the parametric description of the surface. Any tangent vector at a point of the parametric surface can be written in the form\n\nfor suitable real numbers and . If two tangent vectors are given:\n\nthen using the bilinearity of the dot product,\n\nThis is plainly a function of the four variables , , , and . It is more profitably viewed, however, as a function that takes a pair of arguments and which are vectors in the -plane. That is, put\n\nThis is a symmetric function in and , meaning that\n\nIt is also bilinear, meaning that it is linear in each variable and separately. That is,\n\nfor any vectors , , , and in the plane, and any real numbers and .\n\nIn particular, the length of a tangent vector is given by\n\nand the angle between two vectors and is calculated by\n\nThe surface area is another numerical quantity which should depend only on the surface itself, and not on how it is parameterized. If the surface is parameterized by the function over the domain in the -plane, then the surface area of is given by the integral\n\nwhere denotes the cross product, and the absolute value denotes the length of a vector in Euclidean space. By Lagrange's identity for the cross product, the integral can be written\n\nwhere is the determinant.\n\nLet be a smooth manifold of dimension ; for instance a surface (in the case ) or hypersurface in the Cartesian space . At each point there is a vector space , called the tangent space, consisting of all tangent vectors to the manifold at the point . A metric tensor at is a function which takes as inputs a pair of tangent vectors and at , and produces as an output a real number (scalar), so that the following conditions are satisfied:\n\nA metric tensor field on assigns to each point of a metric tensor in the tangent space at in a way that varies smoothly with . More precisely, given any open subset of manifold and any (smooth) vector fields and on , the real function\n\nis a smooth function of .\n\nThe components of the metric in any basis of vector fields, or frame, are given by\n\nThe functions form the entries of an symmetric matrix, . If\nare two vectors at , then the value of the metric applied to and is determined by the coefficients () by bilinearity:\n\nDenoting the matrix by and arranging the components of the vectors and into column vectors and ,\n\nwhere and denote the transpose of the vectors and , respectively. Under a change of basis of the form\n\nfor some invertible matrix , the matrix of components of the metric changes by as well. That is,\n\nor, in terms of the entries of this matrix,\n\nFor this reason, the system of quantities is said to transform covariantly with respect to changes in the frame .\n\nA system of real-valued functions , giving a local coordinate system on an open set in , determines a basis of vector fields on \n\nThe metric has components relative to this frame given by\n\nRelative to a new system of local coordinates, say\n\nthe metric tensor will determine a different matrix of coefficients,\n\nThis new system of functions is related to the original by means of the chain rule\n\nso that\n\nOr, in terms of the matrices and ,\n\nwhere denotes the Jacobian matrix of the coordinate change.\n\nAssociated to any metric tensor is the quadratic form defined in each tangent space by\n\nIf is positive for all non-zero , then the metric is positive-definite at . If the metric is positive-definite at every , then is called a Riemannian metric. More generally, if the quadratic forms have constant signature independent of , then the signature of is this signature, and is called a pseudo-Riemannian metric. If is connected, then the signature of does not depend on .\n\nBy Sylvester's law of inertia, a basis of tangent vectors can be chosen locally so that the quadratic form diagonalizes in the following manner\n\nfor some between 1 and . Any two such expressions of (at the same point of ) will have the same number of positive signs. The signature of is the pair of integers , signifying that there are positive signs and negative signs in any such expression. Equivalently, the metric has signature if the matrix of the metric has positive and negative eigenvalues.\n\nCertain metric signatures which arise frequently in applications are:\n\nLet be a basis of vector fields, and as above let be the matrix of coefficients\nOne can consider the inverse matrix , which is identified with the inverse metric (or \"conjugate\" or \"dual metric\"). The inverse metric satisfies a transformation law when the frame is changed by a matrix via\n\nThe inverse metric transforms \"contravariantly\", or with respect to the inverse of the change of basis matrix . Whereas the metric itself provides a way to measure the length of (or angle between) vector fields, the inverse metric supplies a means of measuring the length of (or angle between) covector fields; that is, fields of linear functionals.\n\nTo see this, suppose that is a covector field. To wit, for each point , determines a function defined on tangent vectors at so that the following linearity condition holds for all tangent vectors and , and all real numbers and :\n\nAs varies, is assumed to be a smooth function in the sense that\n\nis a smooth function of for any smooth vector field .\n\nAny covector field has components in the basis of vector fields . These are determined by\n\nDenote the row vector of these components by\n\nUnder a change of by a matrix , changes by the rule\n\nThat is, the row vector of components transforms as a \"covariant\" vector.\n\nFor a pair and of covector fields, define the inverse metric applied to these two covectors by\n\nThe resulting definition, although it involves the choice of basis , does not actually depend on in an essential way. Indeed, changing basis to gives\n\nSo that the right-hand side of equation () is unaffected by changing the basis to any other basis whatsoever. Consequently, the equation may be assigned a meaning independently of the choice of basis. The entries of the matrix are denoted by , where the indices and have been raised to indicate the transformation law ().\n\nIn a basis of vector fields , any smooth tangent vector field can be written in the form\n\nfor some uniquely determined smooth functions . Upon changing the basis by a nonsingular matrix , the coefficients change in such a way that equation () remains true. That is,\n\nConsequently, . In other words, the components of a vector transform \"contravariantly\" (with respect to the inverse) under a change of basis by the nonsingular matrix . The contravariance of the components of is notationally designated by placing the indices of in the upper position.\n\nA frame also allows covectors to be expressed in terms of their components. For the basis of vector fields define the dual basis to be the linear functionals such that\n\nThat is, , the Kronecker delta. Let\n\nUnder a change of basis for a nonsingular matrix , transforms via\n\nAny linear functional on tangent vectors can be expanded in terms of the dual basis \n\nwhere denotes the row vector . The components transform when the basis is replaced by in such a way that equation () continues to hold. That is,\n\nwhence, because , it follows that . That is, the components transform \"covariantly\" (by the matrix rather than its inverse). The covariance of the components of is notationally designated by placing the indices of in the lower position.\n\nNow, the metric tensor gives a means to identify vectors and covectors as follows. Holding fixed, the function\n\nof tangent vector defines a linear functional on the tangent space at . This operation takes a vector at a point and produces a covector . In a basis of vector fields , if a vector field has components , then the components of the covector field in the dual basis are given by the entries of the row vector\n\nUnder a change of basis , the right-hand side of this equation transforms via\n\nso that : transforms covariantly. The operation of associating to the (contravariant) components of a vector field the (covariant) components of the covector field , where\n\nis called lowering the index.\n\nTo \"raise the index\", one applies the same construction but with the inverse metric instead of the metric. If are the components of a covector in the dual basis , then the column vector\n\nhas components which transform contravariantly:\n\nConsequently, the quantity does not depend on the choice of basis in an essential way, and thus defines a vector field on . The operation () associating to the (covariant) components of a covector the (contravariant) components of a vector given is called raising the index. In components, () is\n\nLet be an open set in , and let be a continuously differentiable function from into the Euclidean space , where . The mapping is called an immersion if its differential is injective at every point of . The image of is called an immersed submanifold.\n\nSuppose that is an immersion onto the submanifold . The usual Euclidean dot product in is a metric which, when restricted to vectors tangent to , gives a means for taking the dot product of these tangent vectors. This is called the induced metric.\n\nSuppose that is a tangent vector at a point of , say\n\nwhere are the standard coordinate vectors in . When is applied to , the vector goes over to the vector tangent to given by\n\n(This is called the pushforward of along .) Given two such vectors, and , the induced metric is defined by\n\nIt follows from a straightforward calculation that the matrix of the induced metric in the basis of coordinate vector fields is given by\n\nwhere is the Jacobian matrix:\n\nThe notion of a metric can be defined intrinsically using the language of fiber bundles and vector bundles. In these terms, a metric tensor is a function\n\nfrom the fiber product of the tangent bundle of with itself to such that the restriction of to each fiber is a nondegenerate bilinear mapping\n\nThe mapping () is required to be continuous, and often continuously differentiable, smooth, or real analytic, depending on the case of interest, and whether can support such a structure.\n\nBy the universal property of the tensor product, any bilinear mapping () gives rise naturally to a section of the dual of the tensor product bundle of with itself\n\nThe section is defined on simple elements of by\n\nand is defined on arbitrary elements of by extending linearly to linear combinations of simple elements. The original bilinear form is symmetric if and only if\nwhere\nis the braiding map.\n\nSince is finite-dimensional, there is a natural isomorphism\n\nso that is regarded also as a section of the bundle of the cotangent bundle with itself. Since is symmetric as a bilinear mapping, it follows that is a symmetric tensor.\n\nMore generally, one may speak of a metric in a vector bundle. If is a vector bundle over a manifold , then a metric is a mapping\n\nfrom the fiber product of to which is bilinear in each fiber:\n\nUsing duality as above, a metric is often identified with a section of the tensor product bundle . (See metric (vector bundle).)\n\nThe metric tensor gives a natural isomorphism from the tangent bundle to the cotangent bundle, sometimes called the musical isomorphism. This isomorphism is obtained by setting, for each tangent vector ,\n\nthe linear functional on which sends a tangent vector at to . That is, in terms of the pairing between and its dual space ,\n\nfor all tangent vectors and . The mapping is a linear transformation from to . It follows from the definition of non-degeneracy that the kernel of is reduced to zero, and so by the rank–nullity theorem, is a linear isomorphism. Furthermore, is a symmetric linear transformation in the sense that\n\nfor all tangent vectors and .\n\nConversely, any linear isomorphism defines a non-degenerate bilinear form on by means of\n\nThis bilinear form is symmetric if and only if is symmetric. There is thus a natural one-to-one correspondence between symmetric bilinear forms on and symmetric linear isomorphisms of to the dual .\n\nAs varies over , defines a section of the bundle of vector bundle isomorphisms of the tangent bundle to the cotangent bundle. This section has the same smoothness as : it is continuous, differentiable, smooth, or real-analytic according as . The mapping , which associates to every vector field on a covector field on gives an abstract formulation of \"lowering the index\" on a vector field. The inverse of is a mapping which, analogously, gives an abstract formulation of \"raising the index\" on a covector field.\n\nThe inverse defines a linear mapping\n\nwhich is nonsingular and symmetric in the sense that\n\nfor all covectors , . Such a nonsingular symmetric mapping gives rise (by the tensor-hom adjunction) to a map\n\nor by the double dual isomorphism to a section of the tensor product\n\nSuppose that is a Riemannian metric on . In a local coordinate system , , the metric tensor appears as a matrix, denoted here by , whose entries are the components of the metric tensor relative to the coordinate vector fields.\n\nLet be a piecewise-differentiable parametric curve in , for . The arclength of the curve is defined by\n\nIn connection with this geometrical application, the quadratic differential form\n\nis called the first fundamental form associated to the metric, while is the line element. When is pulled back to the image of a curve in , it represents the square of the differential with respect to arclength.\n\nFor a pseudo-Riemannian metric, the length formula above is not always defined, because the term under the square root may become negative. We generally only define the length of a curve when the quantity under the square root is always of one sign or the other. In this case, define\n\nNote that, while these formulas use coordinate expressions, they are in fact independent of the coordinates chosen; they depend only on the metric, and the curve along which the formula is integrated.\n\nGiven a segment of a curve, another frequently defined quantity is the (kinetic) energy of the curve:\n\nThis usage comes from physics, specifically, classical mechanics, where the integral can be seen to directly correspond to the kinetic energy of a point particle moving on the surface of a manifold. Thus, for example, in Jacobi's formulation of Maupertuis' principle, the metric tensor can be seen to correspond to the mass tensor of a moving particle.\n\nIn many cases, whenever a calculation calls for the length to be used, a similar calculation using the energy may be done as well. This often leads to simpler formulas by avoiding the need for the square-root. Thus, for example, the geodesic equations may be obtained by applying variational principles to either the length or the energy. In the latter case, the geodesic equations are seen to arise from the principle of least action: they describe the motion of a \"free particle\" (a particle feeling no forces) that is confined to move on the manifold, but otherwise moves freely, with constant momentum, within the manifold.\n\nIn analogy with the case of surfaces, a metric tensor on an -dimensional paracompact manifold gives rise to a natural way to measure the -dimensional volume of subsets of the manifold. The resulting natural positive Borel measure allows one to develop a theory of integrating functions on the manifold by means of the associated Lebesgue integral.\n\nA measure can be defined, by the Riesz representation theorem, by giving a positive linear functional on the space of compactly supported continuous functions on . More precisely, if is a manifold with a (pseudo-)Riemannian metric tensor , then there is a unique positive Borel measure such that for any coordinate chart ,\nfor all supported in . Here is the determinant of the matrix formed by the components of the metric tensor in the coordinate chart. That is well-defined on functions supported in coordinate neighborhoods is justified by Jacobian change of variables. It extends to a unique positive linear functional on by means of a partition of unity.\n\nIf is in addition oriented, then it is possible to define a natural volume form from the metric tensor. In a positively oriented coordinate system the volume form is represented as\nwhere the are the coordinate differentials and denotes the exterior product in the algebra of differential forms. The volume form also gives a way to integrate functions on the manifold, and this geometric integral agrees with the integral obtained by the canonical Borel measure.\n\nThe most familiar example is that of elementary Euclidean geometry: the two-dimensional Euclidean metric tensor. In the usual coordinates, we can write\n\nThe length of a curve reduces to the formula:\n\nThe Euclidean metric in some other common coordinate systems can be written as follows.\n\nPolar coordinates :\n\nSo\nby trigonometric identities.\n\nIn general, in a Cartesian coordinate system on a Euclidean space, the partial derivatives are orthonormal with respect to the Euclidean metric. Thus the metric tensor is the Kronecker delta δ in this coordinate system. The metric tensor with respect to arbitrary (possibly curvilinear) coordinates is given by\n\nThe unit sphere in comes equipped with a natural metric induced from the ambient Euclidean metric. In standard spherical coordinates , with the colatitude, the angle measured from the -axis, and the angle from the -axis in the -plane, the metric takes the form\n\nThis is usually written in the form\n\nIn flat Minkowski space (special relativity), with coordinates\n\nthe metric is, depending on choice of metric signature,\n\nFor a curve with—for example—constant time coordinate, the length formula with this metric reduces to the usual length formula. For a timelike curve, the length formula gives the proper time along the curve.\n\nIn this case, the spacetime interval is written as\n\nThe Schwarzschild metric describes the spacetime around a spherically symmetric body, such as a planet, or a black hole. With coordinates\n\nwe can write the metric as\n\nwhere (inside the matrix) is the gravitational constant and represents the total mass-energy content of the central object.\n\n\n"}
{"id": "6230269", "url": "https://en.wikipedia.org/wiki?curid=6230269", "title": "Military meteorology", "text": "Military meteorology\n\nMilitary meteorology is meteorology applied to military purposes, by armed forces or other agencies. It is one of the most common fields of employment for meteorologists. \n\nWorld War II brought great advances in meteorology as large-scale military land, sea, and air campaigns were highly dependent on weather, particularly forecasts provided by the Royal Navy, Met Office and USAAF for the Normandy landing and strategic bombing. \n\nUniversity meteorology departments grew rapidly as the military services sent cadets to be trained as weather officers. Wartime technological developments such as radar also proved to be valuable meteorological observing systems. More recently, the use of satellites in space has contributed extensively to military meteorology.\n\nMilitary meteorologists currently operate with a wide variety of military units, from aircraft carriers to special forces.\n\n\nEnlisted meteorology and oceanography forecasters are called Aerographer's Mates.\n\nNaval meteorology and oceanography officers are restricted line officers in the Information Dominance Corps.\n\n\n\n\n"}
{"id": "12553397", "url": "https://en.wikipedia.org/wiki?curid=12553397", "title": "Mind at Large", "text": "Mind at Large\n\nMind at Large is a concept from \"The Doors of Perception\" and \"Heaven and Hell\" by Aldous Huxley. This philosophy was influenced by the ideas of C. D. Broad. Psychedelic drugs are thought to disable filters which inhibit or quell signals related to mundane functions from reaching the conscious mind. In the aforementioned books, Huxley explores the idea that the human mind filters reality, partly because handling the details of all of the impressions and images coming in would be unbearable, partly because it has been taught to do so. He believes that psychoactive drugs can partly remove this filter, which leaves the drug user exposed to Mind at Large.\n\nDuring an experiment with British psychiatrist, Humphrey Osmond, Huxley was administered mescaline, and was prompted by Osmond to comment on the various stimuli around him, such as books and flowers. The conversation that was recorded in Huxley's book mainly concerned his thoughts on what he said in the recordings. He observed that everyday objects lose their functionality, and suddenly exist \"as such\"; space and dimension become irrelevant, with perceptions seemingly being enlarged, and at times even overwhelming.\n\nAccording to \"The Doors of Perception\" by Aldous Huxley,\nIn \"The Doors of Perception\", Huxley also stated: \"In the final stage of egolessness there is an 'obscure knowledge' that All is in all—that All is actually each. This is as near, I take it, as a finite mind can ever come to 'perceiving everything that is happening everywhere in the universe.'\"\n\nThe Mind at Large is also the name of a psychedelic blues band, from Greater Manchester, who have gained an audience due to their underground free acid parties.\n\n"}
{"id": "11373842", "url": "https://en.wikipedia.org/wiki?curid=11373842", "title": "Mooers's law", "text": "Mooers's law\n\nMooers's law is an empirical observation of behavior made by American computer scientist Calvin Mooers in 1959. The observation is made in relation to information retrieval and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.\n\nMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the user's personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.\n\nThe more commonly used interpretation of Mooers's law is considered to be a derivation of the principle of least effort first stated by George Kingsley Zipf. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker \"gives up\", and the law is often paraphrased to increase the focus on the retrieval system:\n\nIn this interpretation, \"painful and troublesome\" comes from \"using\" the retrieval system.\n\n\n\n"}
{"id": "5815494", "url": "https://en.wikipedia.org/wiki?curid=5815494", "title": "Ojha", "text": "Ojha\n\nThe term \"Ojha\"' or \"Oza\" is a Hindu Brahmin caste that has settled in north and central India and are found in the states of Rajasthan, Uttar Pradesh and some parts of Bihar and Madhya Pradesh. \n\nOjha is a surname for Hindu Sikhwal Brahmins, Shrimali Brahmins, Atri Gotra Brahmin, Saraswat Brahmin, Bhumihar Brahmins, Kanya Kubj Brahmin, Saryuparin Brahmin, Maithil Brahmin of Nepal, Nepali, Nagar Brahmin, and Bengali Brahmin of India and Nepal. Ojhas are considered to be worshipers of Durga, Saraswati, Hanuman, Mahalakshmi and Shiva . The surname is used amongst speakers of Nepali, Bhojpuri, Rajasthani, Gujarati, Hindi, Oriya, Maithili, Kumaoni and Bengali as well as the Santhals. The high population of Ojhas are found in Central Nepal, and Rajasthan in India within the districts of Gonda, Gorakhpur, Pithoragarh, Uttarakhand, Pratapgarh, Ballia, Buxar (Bihar), and Ara.\n\nThe term Ojha is derived from Sanskrit. Literal translations vary, but one meaning is \"he who is doing meditation for whole life\". According to some, the Ojhas were the top class Brahmin preachers who earned their livelihood through teaching. In Rajasthan, some consider Sharma and Ojha to be the same clan. The Jhas are Maithilis originated in Bihar and Nepal in Janakpura and call their sons-in-laws Ojhajis. This has given credence to this belief. \n\nOjhas are basically divided into two parts – Ojha and the \"Bavan-gaon ke Ojhas\" (the Ojhas of the 52 villages), as they got 52 villages in charity from one of [Rajasthan]-region rulers for having protected those rulers by fighting themselves . They were also given the title of 'Sharma' for the valour and aggression with which they fought. Mata Kaalratri Mata , Bharmani Mata is the \"Kuldevi\" of the Bavan-gaon ke Ojhas. Some Ojhas in Rajasthan have become \"Bhumihars\", and usually use Saraswa or Sharma as their surnames. Some although have not become Bhumihars but still have the title of Singh – to use it or not to use is individual choice. \n\nOjha Brahmin from Rajasthan had excellence in Martial Art, Spirituality, Veda and Science. They have many sub-branches under the surname Ojha. A sub- branch is known as Khairi Ojha which played a significant role in Indian history. Kairi Ojha are active in shaping and binding Hindu rituals and knowledge before the Shankaracharya and formation of “ Four Mathh” Sathapna. Khairi Ojha were spiritual leader for uttar Pradesh and Bihar region. They perform as “Rajguru”, “Army Trainer”, “Thinker” and Philosopher. In few parts of India Ojha ware ruling the Riyasats. \n\nThe Ojha of Rajasthan Brahmins are mainly vegetarians.\n\nAs Brahmins, the Ojhas are spiritual leaders, teachers, and members of the highest ritual rank in the varna system of Hinduism. \n\nMost of the Ojhas are concentrated in the Rajasthan Gujarat (India) and Nepal, a few in Mauritius and can also be found in many regions of India and all over world. \n\nNotable people with surname Ojha Include:\n"}
{"id": "10418624", "url": "https://en.wikipedia.org/wiki?curid=10418624", "title": "Renewable energy commercialization", "text": "Renewable energy commercialization\n\nRenewable energy commercialization involves the deployment of three generations of renewable energy technologies dating back more than 100 years. First-generation technologies, which are already mature and economically competitive, include biomass, hydroelectricity, geothermal power and heat. Second-generation technologies are market-ready and are being deployed at the present time; they include solar heating, photovoltaics, wind power, solar thermal power stations, and modern forms of bioenergy. Third-generation technologies require continued R&D efforts in order to make large contributions on a global scale and include advanced biomass gasification, hot-dry-rock geothermal power, and ocean energy. As of 2012, renewable energy accounts for about half of new nameplate electrical capacity installed and costs are continuing to fall.\n\nPublic policy and political leadership helps to \"level the playing field\" and drive the wider acceptance of renewable energy technologies. Countries such as Germany, Denmark, and Spain have led the way in implementing innovative policies which has driven most of the growth over the past decade. As of 2014, Germany has a commitment to the \"Energiewende\" transition to a sustainable energy economy, and Denmark has a commitment to 100% renewable energy by 2050. There are now 144 countries with renewable energy policy targets.\n\nRenewable energy continued its rapid growth in 2015, providing multiple benefits. There was a new record set for installed wind and photovoltaic capacity (64GW and 57GW) and a new high of US$329 Billion for global renewables investment. A key benefit that this investment growth brings is a growth in jobs. The top countries for investment in recent years were China, Germany, Spain, the United States, Italy, and Brazil. Renewable energy companies include BrightSource Energy, First Solar, Gamesa, GE Energy, Goldwind, Sinovel, Targray, Trina Solar, Vestas, and Yingli.\n\nClimate change concerns are also driving increasing growth in the renewable energy industries. According to a 2011 projection by the (IEA) International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, reducing harmful greenhouse gas emissions.\n\nRenewable power has been more effective in creating jobs than coal or oil in the United States.\n\nClimate change, pollution, and energy insecurity are significant problems, and addressing them requires major changes to energy infrastructures. Renewable energy technologies are essential contributors to the energy supply portfolio, as they contribute to world energy security, reduce dependency on fossil fuels, and provide opportunities for mitigating greenhouse gases. Climate-disrupting fossil fuels are being replaced by clean, climate-stabilizing, non-depletable sources of energy:\n...the transition from coal, oil, and gas to wind, solar, and geothermal energy is well under way. In the old economy, energy was produced by burning something — oil, coal, or natural gas — leading to the carbon emissions that have come to define our economy. The new energy economy harnesses the energy in wind, the energy coming from the sun, and heat from within the earth itself.\nIn international public opinion surveys there is strong support for a variety of methods for addressing the problem of energy supply. These methods include promoting renewable sources such as solar power and wind power, requiring utilities to use more renewable energy, and providing tax incentives to encourage the development and use of such technologies. It is expected that renewable energy investments will pay off economically in the long term.\n\nEU member countries have shown support for ambitious renewable energy goals. In 2010, Eurobarometer polled the twenty-seven EU member states about the target \"to increase the share of renewable energy in the EU by 20 percent by 2020\". Most people in all twenty-seven countries either approved of the target or called for it to go further. Across the EU, 57 percent thought the proposed goal was \"about right\" and 16 percent thought it was \"too modest.\" In comparison, 19 percent said it was \"too ambitious\".\n\nAs of 2011, new evidence has emerged that there are considerable risks associated with traditional energy sources, and that major changes to the mix of energy technologies is needed: \n\nSeveral mining tragedies globally have underscored the human toll of the coal supply chain. New EPA initiatives targeting air toxics, coal ash, and effluent releases highlight the environmental impacts of coal and the cost of addressing them with control technologies. The use of fracking in natural gas exploration is coming under scrutiny, with evidence of groundwater contamination and greenhouse gas emissions. Concerns are increasing about the vast amounts of water used at coal-fired and nuclear power plants, particularly in regions of the country facing water shortages. Events at the Fukushima nuclear plant have renewed doubts about the ability to operate large numbers of nuclear plants safely over the long term. Further, cost estimates for \"next generation\" nuclear units continue to climb, and lenders are unwilling to finance these plants without taxpayer guarantees.\nThe 2014 REN21 Global Status Report says that renewable energies are no longer just energy sources, but ways to address pressing social, political, economic and environmental problems:\n\nToday, renewables are seen not only as sources of energy, but also as tools to address many other pressing needs, including: improving energy security; reducing the health and environmental impacts associated with fossil and nuclear energy; mitigating greenhouse gas emissions; improving educational opportunities; creating jobs; reducing poverty; and increasing gender equality... Renewables have entered the mainstream.\n\nIn 2008 for the first time, more renewable energy than conventional power capacity was added in both the European Union and United States, demonstrating a \"fundamental transition\" of the world's energy markets towards renewables, according to a report released by REN21, a global renewable energy policy network based in Paris. In 2010, renewable power consisted about a third of the newly built power generation capacities.\n\nBy the end of 2011, total renewable power capacity worldwide exceeded 1,360 GW, up 8%. Renewables producing electricity accounted for almost half of the 208 GW of capacity added globally during 2011. Wind and solar photovoltaics (PV) accounted for almost 40% and 30% . Based on REN21's 2014 report, renewables contributed 19 percent to our energy consumption and 22 percent to our electricity generation in 2012 and 2013, respectively. This energy consumption is divided as 9% coming from traditional biomass, 4.2% as heat energy (non-biomass), 3.8% hydro electricity and 2% electricity from wind, solar, geothermal, and biomass.\n\nDuring the five-years from the end of 2004 through 2009, worldwide renewable energy capacity grew at rates of 10–60 percent annually for many technologies, while actual production grew 1.2% overall. In 2011, UN under-secretary general Achim Steiner said: \"The continuing growth in this core segment of the green economy is not happening by chance. The combination of government target-setting, policy support and stimulus funds is underpinning the renewable industry's rise and bringing the much needed transformation of our global energy system within reach.\" He added: \"Renewable energies are expanding both in terms of investment, projects and geographical spread. In doing so, they are making an increasing contribution to combating climate change, countering energy poverty and energy insecurity\".\n\nAccording to a 2011 projection by the International Energy Agency, solar power plants may produce most of the world's electricity within 50 years, significantly reducing the emissions of greenhouse gases that harm the environment. The IEA has said: \"Photovoltaic and solar-thermal plants may meet most of the world's demand for electricity by 2060 – and half of all energy needs – with wind, hydropower and biomass plants supplying much of the remaining generation\". \"Photovoltaic and concentrated solar power together can become the major source of electricity\".\n\nIn 2013, China led the world in renewable energy production, with a total capacity of 378 GW, mainly from hydroelectric and wind power. As of 2014, China leads the world in the production and use of wind power, solar photovoltaic power and smart grid technologies, generating almost as much water, wind and solar energy as all of France and Germany's power plants combined. China's renewable energy sector is growing faster than its fossil fuels and nuclear power capacity. Since 2005, production of solar cells in China has expanded 100-fold. As Chinese renewable manufacturing has grown, the costs of renewable energy technologies have dropped. Innovation has helped, but the main driver of reduced costs has been market expansion.\n\nSee also renewable energy in the United States for US-figures.\n\nRenewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2011 IEA report said: \"A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support,\" and added that \"cost reductions in critical technologies, such as wind and solar, are set to continue.\" , there have been substantial reductions in the cost of solar and wind technologies:\nThe price of PV modules per MW has fallen by 60 percent since the summer of 2008, according to Bloomberg New Energy Finance estimates, putting solar power for the first time on a competitive footing with the retail price of electricity in a number of sunny countries. Wind turbine prices have also fallen – by 18 percent per MW in the last two years – reflecting, as with solar, fierce competition in the supply chain. Further improvements in the levelised cost of energy for solar, wind and other technologies lie ahead, posing a growing threat to the dominance of fossil fuel generation sources in the next few years.\nHydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. \nRenewable energy is also the most economic solution for new grid-connected capacity in areas with good resources. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where \"oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today\". As of 2012, renewable power generation technologies accounted for around half of all new power generation capacity additions globally. In 2011, additions included 41 gigawatt (GW) of new wind power capacity, 30 GW of PV, 25 GW of hydro-electricity, 6 GW of biomass, 0.5 GW of CSP, and 0.1 GW of geothermal power.\n\nRenewable energy includes a number of sources and technologies at different stages of commercialization. The International Energy Agency (IEA) has defined three generations of renewable energy technologies, reaching back over 100 years:\n\n\nFirst-generation technologies are widely used in locations with abundant resources. Their future use depends on the exploration of the remaining resource potential, particularly in developing countries, and on overcoming challenges related to the environment and social acceptance.\n\nBiomass for heat and power is a fully mature technology which offers a ready disposal mechanism for municipal, agricultural, and industrial organic wastes. However, the industry has remained relatively stagnant over the decade to 2007, even though demand for biomass (mostly wood) continues to grow in many developing countries. One of the problems of biomass is that material directly combusted in cook stoves produces pollutants, leading to severe health and environmental consequences, although improved cook stove programmes are alleviating some of these effects. First-generation biomass technologies can be economically competitive, but may still require deployment support to overcome public acceptance and small-scale issues.\n\nHydroelectricity is the term referring to electricity generated by hydropower; the production of electrical power through the use of the gravitational force of falling or flowing water. In 2015 hydropower generated 16.6% of the worlds total electricity and 70% of all renewable electricity and is expected to increase about 3.1% each year for the next 25 years. Hydroelectric plants have the advantage of being long-lived and many existing plants have operated for more than 100 years.\n\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity plants larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela. The cost of hydroelectricity is low, making it a competitive source of renewable electricity. The average cost of electricity from a hydro plant larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour.\n\nGeothermal power plants can operate 24 hours per day, providing baseload capacity. Estimates for the world potential capacity for geothermal power generation vary widely, ranging from 40 GW by 2020 to as much as 6,000 GW.\n\nGeothermal power capacity grew from around 1 GW in 1975 to almost 10 GW in 2008. The United States is the world leader in terms of installed capacity, representing 3.1 GW. Other countries with significant installed capacity include the Philippines (1.9 GW), Indonesia (1.2 GW), Mexico (1.0 GW), Italy (0.8 GW), Iceland (0.6 GW), Japan (0.5 GW), and New Zealand (0.5 GW). In some countries, geothermal power accounts for a significant share of the total electricity supply, such as in the Philippines, where geothermal represented 17 percent of the total power mix at the end of 2008.\n\nGeothermal (ground source) heat pumps represented an estimated 30 GWth of installed capacity at the end of 2008, with other direct uses of geothermal heat (i.e., for space heating, agricultural drying and other uses) reaching an estimated 15 GWth. , at least 76 countries use direct geothermal energy in some form.\n\nSecond-generation technologies have gone from being a passion for the dedicated few to a major economic sector in countries such as Germany, Spain, the United States, and Japan. Many large industrial companies and financial institutions are involved and the challenge is to broaden the market base for continued growth worldwide.\n\nSolar heating systems are a well known second-generation technology and generally consist of solar thermal collectors, a fluid system to move the heat from the collector to its point of usage, and a reservoir or tank for heat storage. The systems may be used to heat domestic hot water, swimming pools, or homes and businesses. The heat can also be used for industrial process applications or as an energy input for other uses such as cooling equipment.\n\nIn many warmer climates, a solar heating system can provide a very high percentage (50 to 75%) of domestic hot water energy. , China has 27 million rooftop solar water heaters.\n\nPhotovoltaic (PV) cells, also called solar cells, convert light into electricity. In the 1980s and early 1990s, most photovoltaic modules were used to provide remote-area power supply, but from around 1995, industry efforts have focused increasingly on developing building integrated photovoltaics and photovoltaic power stations for grid connected applications.\n\nMany solar photovoltaic power stations have been built, mainly in Europe. As of July 2012, the largest photovoltaic (PV) power plants in the world are the Agua Caliente Solar Project (USA, 247 MW), Charanka Solar Park (India, 214 MW), Golmud Solar Park (China, 200 MW), Perovo Solar Park (Russia 100 MW), Sarnia Photovoltaic Power Plant (Canada, 97 MW), Brandenburg-Briest Solarpark (Germany 91 MW), Solarpark Finow Tower (Germany 84.7 MW), Montalto di Castro Photovoltaic Power Station (Italy, 84.2 MW), Eggebek Solar Park (Germany 83.6 MW), Senftenberg Solarpark (Germany 82 MW), Finsterwalde Solar Park (Germany, 80.7 MW), Okhotnykovo Solar Park (Russia, 80 MW), Lopburi Solar Farm (Thailand 73.16 MW), Rovigo Photovoltaic Power Plant (Italy, 72 MW), and the Lieberose Photovoltaic Park (Germany, 71.8 MW).\n\nThere are also many large plants under construction. The Desert Sunlight Solar Farm under construction in Riverside County, California and Topaz Solar Farm being built in San Luis Obispo County, California are both 550 MW solar parks that will use thin-film solar photovoltaic modules made by First Solar. The Blythe Solar Power Project is a 500 MW photovoltaic station under construction in Riverside County, California. The California Valley Solar Ranch (CVSR) is a 250 megawatt (MW) solar photovoltaic power plant, which is being built by SunPower in the Carrizo Plain, northeast of California Valley. The 230 MW Antelope Valley Solar Ranch is a First Solar photovoltaic project which is under construction in the Antelope Valley area of the Western Mojave Desert, and due to be completed in 2013. The Mesquite Solar project is a photovoltaic solar power plant being built in Arlington, Maricopa County, Arizona, owned by Sempra Generation. Phase 1 will have a nameplate capacity of 150 megawatts.\n\nMany of these plants are integrated with agriculture and some use innovative tracking systems that follow the sun's daily path across the sky to generate more electricity than conventional fixed-mounted systems. There are no fuel costs or emissions during operation of the power stations.\n\nSome of the second-generation renewables, such as wind power, have high potential and have already realised relatively low production costs. Wind power could become cheaper than nuclear power. Global wind power installations increased by 35,800 MW in 2010, bringing total installed capacity up to 194,400 MW, a 22.5% increase on the 158,700 MW installed at the end of 2009. The increase for 2010 represents investments totalling €47.3 billion (US$65 billion) and for the first time more than half of all new wind power was added outside of the traditional markets of Europe and North America, mainly driven, by the continuing boom in China which accounted for nearly half of all of the installations at 16,500 MW. China now has 42,300 MW of wind power installed. Wind power accounts for approximately 19% of electricity generated in Denmark, 9% in Spain and Portugal, and 6% in Germany and the Republic of Ireland. In Australian state of South Australia wind power, championed by Premier Mike Rann (2002–2011), now comprises 26% of the state's electricity generation, edging out coal fired power. At the end of 2011 South Australia, with 7.2% of Australia's population, had 54%of the nation's installed wind power capacity. Wind power's share of worldwide electricity usage at the end of 2014 was 3.1%.\nThese are some of the largest wind farms in the world:\n\nAs of 2014, the wind industry in the USA is able to produce more power at lower cost by using taller wind turbines with longer blades, capturing the faster winds at higher elevations. This has opened up new opportunities and in Indiana, Michigan, and Ohio, the price of power from wind turbines built 300 feet to 400 feet above the ground can now compete with conventional fossil fuels like coal. Prices have fallen to about 4 cents per kilowatt-hour in some cases and utilities have been increasing the amount of wind energy in their portfolio, saying it is their cheapest option.\n\nSolar thermal power stations include the 354 megawatt (MW) Solar Energy Generating Systems power plant in the USA, Solnova Solar Power Station (Spain, 150 MW), Andasol solar power station (Spain, 100 MW), Nevada Solar One (USA, 64 MW), PS20 solar power tower (Spain, 20 MW), and the PS10 solar power tower (Spain, 11 MW). The 370 MW Ivanpah Solar Power Facility, located in California's Mojave Desert, is the world's largest solar-thermal power plant project currently under construction. Many other plants are under construction or planned, mainly in Spain and the USA. In developing countries, three World Bank projects for integrated solar thermal/combined-cycle gas-turbine power plants in Egypt, Mexico, and Morocco have been approved.\n\nGlobal ethanol production for transport fuel tripled between 2000 and 2007 from 17 billion to more than 52 billion litres, while biodiesel expanded more than tenfold from less than 1 billion to almost 11 billion litres. Biofuels provide 1.8% of the world's transport fuel and recent estimates indicate a continued high growth. The main producing countries for transport biofuels are the USA, Brazil, and the EU.\n\nBrazil has one of the largest renewable energy programs in the world, involving production of ethanol fuel from sugar cane, and ethanol now provides 18 percent of the country's automotive fuel. As a result of this and the exploitation of domestic deep water oil sources, Brazil, which for years had to import a large share of the petroleum needed for domestic consumption, recently reached complete self-sufficiency in liquid fuels.\n\nNearly all the gasoline sold in the United States today is mixed with 10 percent ethanol, a mix known as E10, and motor vehicle manufacturers already produce vehicles designed to run on much higher ethanol blends. Ford, DaimlerChrysler, and GM are among the automobile companies that sell flexible-fuel cars, trucks, and minivans that can use gasoline and ethanol blends ranging from pure gasoline up to 85% ethanol (E85). The challenge is to expand the market for biofuels beyond the farm states where they have been most popular to date. The Energy Policy Act of 2005, which calls for of biofuels to be used annually by 2012, will also help to expand the market.\n\nThe growing ethanol and biodiesel industries are providing jobs in plant construction, operations, and maintenance, mostly in rural communities. According to the Renewable Fuels Association, \"the ethanol industry created almost 154,000 U.S. jobs in 2005 alone, boosting household income by $5.7 billion. It also contributed about $3.5 billion in tax revenues at the local, state, and federal levels\".\n\nThird-generation renewable energy technologies are still under development and include advanced biomass gasification, biorefinery technologies, hot-dry-rock geothermal power, and ocean energy. Third-generation technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research and development funding.\n\nAccording to the International Energy Agency, cellulosic ethanol biorefineries could allow biofuels to play a much bigger role in the future than organizations such as the IEA previously thought. Cellulosic ethanol can be made from plant matter composed primarily of inedible cellulose fibers that form the stems and branches of most plants. Crop residues (such as corn stalks, wheat straw and rice straw), wood waste, and municipal solid waste are potential sources of cellulosic biomass. Dedicated energy crops, such as switchgrass, are also promising cellulose sources that can be sustainably produced in many regions.\n\nOcean energy is all forms of renewable energy derived from the sea including wave energy, tidal energy, river current, ocean current energy, offshore wind, salinity gradient energy and ocean thermal gradient energy.\n\nThe Rance Tidal Power Station (240 MW) is the world's first tidal power station. The facility is located on the estuary of the Rance River, in Brittany, France. Opened on 26 November 1966, it is currently operated by Électricité de France, and is the largest tidal power station in the world, in terms of installed capacity.\n\nFirst proposed more than thirty years ago, systems to harvest utility-scale electrical power from ocean waves have recently been gaining momentum as a viable technology. The potential for this technology is considered promising, especially on west-facing coasts with latitudes between 40 and 60 degrees:\n\nIn the United Kingdom, for example, the Carbon Trust recently estimated the extent of the economically viable offshore resource at 55 TWh per year, about 14% of current national demand. Across Europe, the technologically achievable resource has been estimated to be at least 280 TWh per year. In 2003, the U.S. Electric Power Research Institute (EPRI) estimated the viable resource in the United States at 255 TWh per year (6% of demand).\n\nThere are currently nine projects, completed or in-development, off the coasts of the United Kingdom, United States, Spain and Australia to harness the rise and fall of waves by Ocean Power Technologies. The current maximum power output is 1.5 MW (Reedsport, Oregon), with development underway for 100 MW (Coos Bay, Oregon).\n\n, geothermal power development was under way in more than 40 countries, partially attributable to the development of new technologies, such as Enhanced Geothermal Systems. The development of binary cycle power plants and improvements in drilling and extraction technology may enable enhanced geothermal systems over a much greater geographical range than \"traditional\" Geothermal systems. Demonstration EGS projects are operational in the USA, Australia, Germany, France, and The United Kingdom.\n\nBeyond the already established solar photovoltaics and solar thermal power technologies are such advanced solar concepts as the solar updraft tower or space-based solar power. These concepts have yet to (if ever) be commercialized. \n\nThe Solar updraft tower (SUT) is a renewable-energy power plant for generating electricity from low temperature solar heat. Sunshine heats the air beneath a very wide greenhouse-like roofed collector structure surrounding the central base of a very tall chimney tower. The resulting convection causes a hot air updraft in the tower by the chimney effect. This airflow drives wind turbines placed in the chimney updraft or around the chimney base to produce electricity. Plans for scaled-up versions of demonstration models will allow significant power generation, and may allow development of other applications, such as water extraction or distillation, and agriculture or horticulture.\n\nA more advanced version of a similarly themed technology is the Vortex engine (AVE) which aims to replace large physical chimneys with a vortex of air created by a shorter, less-expensive structure.\n\nSpace-based solar power (SBSP) is the concept of collecting solar power in space (using an \"SPS\", that is, a \"solar-power satellite\" or a \"satellite power system\") for use on Earth. It has been in research since the early 1970s. SBSP would differ from current solar collection methods in that the means used to collect energy would reside on an orbiting satellite instead of on Earth's surface. Some projected benefits of such a system are a higher collection rate and a longer collection period due to the lack of a diffusing atmosphere and night time in space.\n\nTotal investment in renewable energy reached $211 billion in 2010, up from $160 billion in 2009. The top countries for investment in 2010 were\nChina, Germany, the United States, Italy, and Brazil. Continued growth for the renewable energy sector is expected and promotional policies helped the industry weather the 2009 economic crisis better than many other sectors.\n\n, Vestas (from Denmark) is the world's top wind turbine manufacturer in terms of percentage of market volume, and Sinovel (from China) is in second place. Together Vestas and Sinovel delivered 10,228 MW of new wind power capacity in 2010, and their market share was 25.9 percent. GE Energy (USA) was in third place, closely followed by Goldwind, another Chinese supplier. German Enercon ranks fifth in the world, and is followed in sixth place by Indian-based Suzlon.\n\nThe solar PV market has been growing for the past few years. According to solar PV research company, PVinsights, worldwide shipment of solar modules in 2011 was around 25 GW, and the shipment year over year growth was around 40%. The top 5 solar module players in 2011 in turns are Suntech, First Solar, Yingli, Trina, and Sungen. The top 5 solar module companies possessed 51.3% market share of solar modules, according to PVinsights' market intelligence report.\n\nThe PV industry has seen drops in module prices since 2008. In late 2011, factory-gate prices for crystalline-silicon photovoltaic modules dropped below the $1.00/W mark. The $1.00/W installed cost, is often regarded in the PV industry as marking the achievement of grid parity for PV. These reductions have taken many stakeholders, including industry analysts, by surprise, and perceptions of current solar power economics often lags behind reality. Some stakeholders still have the perspective that solar PV remains too costly on an unsubsidized basis to compete with conventional generation options. Yet technological advancements, manufacturing process improvements, and industry re-structuring, mean that further price reductions are likely in coming years.\n\nMany energy markets, institutions, and policies have been developed to support the production and use of fossil fuels. Newer and cleaner technologies may offer social and environmental benefits, but utility operators often reject renewable resources because they are trained to think only in terms of big, conventional power plants. Consumers often ignore renewable power systems because they are not given accurate price signals about electricity consumption. Intentional market distortions (such as subsidies), and unintentional market distortions (such as split incentives) may work against renewables. Benjamin K. Sovacool has argued that \"some of the most surreptitious, yet powerful, impediments facing renewable energy and energy efficiency in the United States are more about \"culture\" and \"institutions\" than engineering and science\".\n\nThe obstacles to the widespread commercialization of renewable energy technologies are primarily political, not technical, and there have been many studies which have identified a range of \"non-technical barriers\" to renewable energy use. These barriers are impediments which put renewable energy at a marketing, institutional, or policy disadvantage relative to other forms of energy. Key barriers include:\n\n\n\"National grids are usually tailored towards the operation of centralised power plants and thus favour their performance. Technologies that do not easily fit into these networks may struggle to enter the market, even if the technology itself is commercially viable. This applies to distributed generation as most grids are not suited to receive electricity from many small sources. Large-scale renewables may also encounter problems if they are sited in areas far from existing grids.\"\n\n\nWith such a wide range of non-technical barriers, there is no \"silver bullet\" solution to drive the transition to renewable energy. So ideally there is a need for several different types of policy instruments to complement each other and overcome different types of barriers.\n\nA policy framework must be created that will level the playing field and redress the imbalance of traditional approaches associated with fossil fuels. The policy landscape must keep pace with broad trends within the energy sector, as well as reflecting specific social, economic and environmental priorities. Some resource-rich countries struggle to move away from fossil fuels and have failed thus far to adopt regulatory frameworks necessary for developing renewable energy (e.g. Russia).\n\nPublic policy has a role to play in renewable energy commercialization because the free market system has some fundamental limitations. As the Stern Review points out:\n\nIn a liberalised energy market, investors, operators and consumers should face the full cost of their decisions. But this is not the case in many economies or energy sectors. Many policies distort the market in favour of existing fossil fuel technologies.\nThe International Solar Energy Society has stated that \"historical incentives for the conventional energy resources continue even today to bias markets by burying many of the real societal costs of their use\".\n\nFossil-fuel energy systems have different production, transmission, and end-use costs and characteristics than do renewable energy systems, and new promotional policies are needed to ensure that renewable systems develop as quickly and broadly as is socially desirable.\n\nLester Brown states that the market \"does not incorporate the indirect costs of providing goods or services into prices, it does not value nature's services adequately, and it does not respect the sustainable-yield thresholds of natural systems\". It also favors the near term over the long term, thereby showing limited concern for future generations. Tax and subsidy shifting can help overcome these problems, though is also problematic to combine different international normative regimes regulating this issue.\n\nTax shifting has been widely discussed and endorsed by economists. It involves lowering income taxes while raising levies on environmentally destructive activities, in order to create a more responsive market. For example, a tax on coal that included the increased health care costs associated with breathing polluted air, the costs of acid rain damage, and the costs of climate disruption would encourage investment in renewable technologies. Several Western European countries are already shifting taxes in a process known there as environmental tax reform.\n\nIn 2001, Sweden launched a new 10-year environmental tax shift designed to convert 30 billion kroner ($3.9 billion) of income taxes to taxes on environmentally destructive activities. Other European countries with significant tax reform efforts are France, Italy, Norway, Spain, and the United Kingdom. Asia's two leading economies, Japan and China, are considering carbon taxes.\n\nJust as there is a need for tax shifting, there is also a need for subsidy shifting. Subsidies are not an inherently bad thing as many technologies and industries emerged through government subsidy schemes. The Stern Review explains that of 20 key innovations from the past 30 years, only one of the 14 was funded entirely by the private sector and nine were totally publicly funded. In terms of specific examples, the Internet was the result of publicly funded links among computers in government laboratories and research institutes. And the combination of the federal tax deduction and a robust state tax deduction in California helped to create the modern wind power industry.\n\nLester Brown has argued that \"a world facing the prospect of economically disruptive climate change can no longer justify subsidies to expand the burning of coal and oil. Shifting these subsidies to the development of climate-benign energy sources such as wind, solar, biomass, and geothermal power is the key to stabilizing the earth's climate.\" The International Solar Energy Society advocates \"leveling the playing field\" by redressing the continuing inequities in public subsidies of energy technologies and R&D, in which the fossil fuel and nuclear power receive the largest share of financial support.\n\nSome countries are eliminating or reducing climate-disrupting subsidies and Belgium, France, and Japan have phased out all subsidies for coal. Germany is reducing its coal subsidy. The subsidy dropped from $5.4 billion in 1989 to $2.8 billion in 2002, and in the process Germany lowered its coal use by 46 percent. China cut its coal subsidy from $750 million in 1993 to $240 million in 1995 and more recently has imposed a high-sulfur coal tax. However, the United States has been increasing its support for the fossil fuel and nuclear industries.\n\nIn November 2011, an IEA report entitled \"Deploying Renewables 2011\" said \"subsidies in green energy technologies that were not yet competitive are justified in order to give an incentive to investing into technologies with clear environmental and energy security benefits\". The IEA's report disagreed with claims that renewable energy technologies are only viable through costly subsidies and not able to produce energy reliably to meet demand.\n\nA fair and efficient imposition of subsidies for renewable energies and aiming at sustainable development, however, require coordination and regulation at a global level, as subsidies granted in one country can easily disrupt industries and policies of others, thus underlining the relevance of this issue at the World Trade Organization.\n\nSetting national renewable energy targets can be an important part of a renewable energy policy and these targets are usually defined as a percentage of the primary energy and/or electricity generation mix. For example, the European Union has prescribed an indicative renewable energy target of 12 per cent of the total EU energy mix and 22 per cent of electricity consumption by 2010. National targets for individual EU Member States have also been set to meet the overall target. Other developed countries with defined national or regional targets include Australia, Canada, Israel, Japan, Korea, New Zealand, Norway, Singapore, Switzerland, and some US States.\n\nNational targets are also an important component of renewable energy strategies in some developing countries. Developing countries with renewable energy targets include China, India, Indonesia, Malaysia, the Philippines, Thailand, Brazil, Egypt, Mali, and South Africa. The targets set by many developing countries are quite modest when compared with those in some industrialized countries.\n\nRenewable energy targets in most countries are indicative and nonbinding but they have assisted government actions and regulatory frameworks. The United Nations Environment Program has suggested that making renewable energy targets legally binding could be an important policy tool to achieve higher renewable energy market penetration.\n\nThe IEA has identified three actions which will allow renewable energy and other clean energy technologies to \"more effectively compete for private sector capital\".\n\nIn response to the global financial crisis in the late 2000s, the world's major governments made \"green stimulus\" programs one of their main policy instruments for supporting economic recovery. Some in green stimulus funding had been allocated to renewable energy and energy efficiency, to be spent mainly in 2010 and in 2011.\n\nPublic policy determines the extent to which renewable energy (RE) is to be incorporated into a developed or developing country's generation mix. Energy sector regulators implement that policy—thus affecting the pace and pattern of RE investments and connections to the grid. Energy regulators often have authority to carry out a number of functions that have implications for the financial feasibility of renewable energy projects. Such functions include issuing licenses, setting performance standards, monitoring the performance of regulated firms, determining the price level and structure of tariffs, establishing uniform systems of accounts, arbitrating stakeholder disputes (like interconnection cost allocations), performing management audits, developing agency human resources (expertise), reporting sector and commission activities to government authorities, and coordinating decisions with other government agencies. Thus, regulators make a wide range of decisions that affect the financial outcomes associated with RE investments. In addition, the sector regulator is in a position to give advice to the government regarding the full implications of focusing on climate change or energy security. The energy sector regulator is the natural advocate for efficiency and cost-containment throughout the process of designing and implementing RE policies. Since policies are not self-implementing, energy sector regulators become a key facilitator (or blocker) of renewable energy investments.\n\nThe \"Energiewende\" (German for \"energy transition\") is the transition by Germany to a low carbon, environmentally sound, reliable, and affordable energy supply. The new system will rely heavily on renewable energy (particularly wind, photovoltaics, and biomass) energy efficiency, and energy demand management. Most if not all existing coal-fired generation will need to be retired. The phase-out of Germany's fleet of nuclear reactors, to be complete by 2022, is a key part of the program.\n\nLegislative support for the \"Energiewende\" was passed in late 2010 and includes greenhouse gas (GHG) reductions of 80–95% by 2050 (relative to 1990) and a renewable energy target of 60% by 2050. These targets are ambitious. The Berlin-based policy institute Agora Energiewende noted that \"while the German approach is not unique worldwide, the speed and scope of the \"Energiewende\" are exceptional\". The \"Energiewende\" also seeks a greater transparency in relation to national energy policy formation.\n\nGermany has made significant progress on its GHG emissions reduction target, achieving a 27% decrease between 1990 and 2014. However Germany will need to maintain an average GHG emissions abatement rate of 3.5% per annum to reach its \"Energiewende\" goal, equal to the maximum historical value thus far.\n\nGermany spends €1.5billion per annum on energy research (2013 figure) in an effort to solve the technical and social issues raised by the transition. This includes a number of computer studies that have confirmed the feasibility and a similar cost (relative to business-as-usual and given that carbon is adequately priced) of the \"Energiewende\".\n\nThese initiatives go well beyond European Union legislation and the national policies of other European states. The policy objectives have been embraced by the German federal government and has resulted in a huge expansion of renewables, particularly wind power. Germany's share of renewables has increased from around 5% in 1999 to 22.9% in 2012, surpassing the OECD average of 18% usage of renewables.\nProducers have been guaranteed a fixed feed-in tariff for 20 years, guaranteeing a fixed income. Energy co-operatives have been created, and efforts were made to decentralize control and profits. The large energy companies have a disproportionately small share of the renewables market. However, in some cases poor investment designs have caused bankruptcies and low returns, and unrealistic promises have been shown to be far from reality.\nNuclear power plants were closed, and the existing nine plants will close earlier than planned, in 2022.\n\nOne factor that has inhibited efficient employment of new renewable energy has been the lack of an accompanying investment in power infrastructure to bring the power to market. It is believed 8,300 km of power lines must be built or upgraded. The different German States have varying attitudes to the construction of new power lines. Industry has had their rates frozen and so the increased costs of the \"Energiewende\" have been passed on to consumers, who have had rising electricity bills.\n\nVoluntary markets, also referred to as green power markets, are driven by consumer preference. Voluntary markets allow a consumer to choose to do more than policy decisions require and reduce the environmental impact of their electricity use. Voluntary green power products must offer a significant benefit and value to buyers to be successful. Benefits may include zero or reduced greenhouse gas emissions, other pollution reductions or other environmental improvements on power stations.\nThe driving factors behind voluntary green electricity within the EU are the liberalized electricity markets and the RES Directive. According to the directive, the EU Member States must ensure that the origin of electricity produced from renewables can be guaranteed and therefore a \"guarantee of origin\" must be issued (article 15). Environmental organisations are using the voluntary market to create new renewables and improving sustainability of the existing power production. In the US the main tool to track and stimulate voluntary actions is Green-e program managed by Center for Resource Solutions. In Europe the main voluntary tool used by the NGOs to promote sustainable electricity production is EKOenergy label.\n\nA number of events in 2006 pushed renewable energy up the political agenda, including the US mid-term elections in November, which confirmed clean energy as a mainstream issue. Also in 2006, the Stern Review made a strong economic case for investing in low carbon technologies now, and argued that economic growth need not be incompatible with cutting energy consumption. According to a trend analysis from the United Nations Environment Programme, climate change concerns coupled with recent high oil prices and increasing government support are driving increasing rates of investment in the renewable energy and energy efficiency industries.\n\nInvestment capital flowing into renewable energy reached a record US$77 billion in 2007, with the upward trend continuing in 2008. The OECD still dominates, but there is now increasing activity from companies in China, India and Brazil. Chinese companies were the second largest recipient of venture capital in 2006 after the United States. In the same year, India was the largest net buyer of companies abroad, mainly in the more established European markets.\n\nNew government spending, regulation, and policies helped the industry weather the 2009 economic crisis better than many other sectors. Most notably, U.S. President Barack Obama's American Recovery and Reinvestment Act of 2009 included more than $70 billion in direct spending and tax credits for clean energy and associated transportation programs. This policy-stimulus combination represents the largest federal commitment in U.S. history for renewables, advanced transportation, and energy conservation initiatives. Based on these new rules, many more utilities strengthened their clean-energy programs. Clean Edge suggests that the commercialization of clean energy will help countries around the world deal with the current economic malaise. Once-promising solar energy company, Solyndra, became involved in a political controversy involving U.S. President Barack Obama's administration's authorization of a $535 million loan guarantee to the Corporation in 2009 as part of a program to promote alternative energy growth. The company ceased all business activity, filed for Chapter 11 bankruptcy, and laid-off nearly all of its employees in early September 2011.\n\nIn his 24 January 2012, State of the Union address, President Barack Obama restated his commitment to renewable energy. Obama said that he \"will not walk away from the promise of clean energy.\" Obama called for a commitment by the Defense Department to purchase 1,000 MW of renewable energy. He also mentioned the long-standing Interior Department commitment to permit 10,000 MW of renewable energy projects on public land in 2012.\n\nAs of 2012, renewable energy plays a major role in the energy mix of many countries globally. Renewables are becoming increasingly economic in both developing and developed countries. Prices for renewable energy technologies, primarily wind power and solar power, continued to drop, making renewables competitive with conventional energy sources. Without a level playing field, however, high market penetration of renewables is still dependent on robust promotional policies. Fossil fuel subsidies, which are far higher than those for renewable energy, remain in place and quickly need to be phased out.\n\nUnited Nations' Secretary-General Ban Ki-moon has said that \"renewable energy has the ability to lift the poorest nations to new levels of prosperity\". In October 2011, he \"announced the creation of a high-level group to drum up support for energy access, energy efficiency and greater use of renewable energy. The group is to be co-chaired by Kandeh Yumkella, the chair of UN Energy and director general of the UN Industrial Development Organisation, and Charles Holliday, chairman of Bank of America\".\n\nWorldwide use of solar power and wind power continued to grow significantly in 2012. Solar electricity consumption increased by 58 percent, to 93 terawatt-hours (TWh). Use of wind power in 2012 increased by 18.1 percent, to 521.3 TWh. Global solar and wind energy installed capacities continued to expand even though new investments in these technologies declined during 2012. Worldwide investment in solar power in 2012 was $140.4 billion, an 11 percent decline from 2011, and wind power investment was down 10.1 percent, to $80.3 billion. But due to lower production costs for both technologies, total installed capacities grew sharply. This investment decline, but growth in installed capacity, may again occur in 2013. Analysts expect the market to triple by 2030. In 2015, investment in renewables exceeded fossils.\n\nThe incentive to use 100% renewable energy for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of the total global energy demand. In reviewing 164 recent scenarios of future renewable energy growth, the report noted that the majority expected renewable sources to supply more than 17% of total energy by 2030, and 27% by 2050; the highest forecast projected 43% supplied by renewables by 2030 and 77% by 2050. Renewable energy use has grown much faster than even advocates anticipated. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply.\n\nMark Z. Jacobson, professor of civil and environmental engineering at Stanford University and director of its Atmosphere and Energy Program says producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs.\n\nSimilarly, in the United States, the independent National Research Council has noted that \"sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\"\n\nThe most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies are primarily political and not technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.\n\nMoving towards energy sustainability will require changes not only in the way energy is supplied, but in the way it is used, and reducing the amount of energy required to deliver various goods or services is essential. Opportunities for improvement on the demand side of the energy equation are as rich and diverse as those on the supply side, and often offer significant economic benefits.\n\nA sustainable energy economy requires commitments to both renewables and efficiency. Renewable energy and energy efficiency are said to be the \"twin pillars\" of sustainable energy policy. The American Council for an Energy-Efficient Economy has explained that both resources must be developed in order to stabilize and reduce carbon dioxide emissions:\n\nEfficiency is essential to slowing the energy demand growth so that rising clean energy supplies can make deep cuts in fossil fuel use. If energy use grows too fast, renewable energy development will chase a receding target. Likewise, unless clean energy supplies come online rapidly, slowing demand growth will only begin to reduce total emissions; reducing the carbon content of energy sources is also needed.\nThe IEA has stated that renewable energy and energy efficiency policies are complementary tools for the development of a sustainable energy future, and should be developed together instead of being developed in isolation.\n\n\n"}
{"id": "29107", "url": "https://en.wikipedia.org/wiki?curid=29107", "title": "Semantics", "text": "Semantics\n\nSemantics (from \"sēmantikós\", \"significant\") is the linguistic and philosophical study of meaning, in language, programming languages, formal logics, and semiotics. It is concerned with the relationship between \"signifiers\"—like words, phrases, signs, and symbols—and what they stand for, their denotation.\n\nIn international scientific vocabulary semantics is also called \"semasiology\". The word \"semantics\" was first used by Michel Bréal, a French philologist. It denotes a range of ideas—from the popular to the highly technical. It is often used in ordinary language for denoting a problem of understanding that comes down to word selection or connotation. This problem of understanding has been the subject of many formal enquiries, over a long period of time, especially in the field of formal semantics. In linguistics, it is the study of the interpretation of signs or symbols used in agents or communities within particular circumstances and contexts. Within this view, sounds, facial expressions, body language, and proxemics have semantic (meaningful) content, and each comprises several branches of study. In written language, things like paragraph structure and punctuation bear semantic content; other forms of language bear other semantic content.\n\nThe formal study of semantics intersects with many other fields of inquiry, including lexicology, syntax, pragmatics, etymology and others. Independently, semantics is also a well-defined field in its own right, often with synthetic properties. In the philosophy of language, semantics and reference are closely connected. Further related fields include philology, communication, and semiotics. The formal study of semantics can therefore be manifold and complex.\n\nSemantics contrasts with syntax, the study of the combinatorics of units of a language (without reference to their meaning), and pragmatics, the study of the relationships between the symbols of a language, their meaning, and the users of the language. Semantics as a field of study also has significant ties to various representational theories of meaning including truth theories of meaning, coherence theories of meaning, and correspondence theories of meaning. Each of these is related to the general philosophical study of reality and the representation of meaning. In 1960s psychosemantic studies became popular after Osgood's massive cross-cultural studies using his semantic differential (SD) method that used thousands of nouns and adjective bipolar scales. A specific form of the SD, Projective Semantics method uses only most common and neutral nouns that correspond to the 7 groups (factors) of adjective-scales most consistently found in cross-cultural studies (Evaluation, Potency, Activity as found by Osgood, and Reality, Organization, Complexity, Limitation as found in other studies). In this method, seven groups of bipolar adjective scales corresponded to seven types of nouns so the method was thought to have the object-scale symmetry (OSS) between the scales and nouns for evaluation using these scales. For example, the nouns corresponding to the listed 7 factors would be: Beauty, Power, Motion, Life, Work, Chaos, Law. Beauty was expected to be assessed unequivocally as “very good” on adjectives of Evaluation-related scales, Life as “very real” on Reality-related scales, etc. However, deviations in this symmetric and very basic matrix might show underlying biases of two types: scales-related bias and objects-related bias. This OSS design meant to increase the sensitivity of the SD method to any semantic biases in responses of people within the same culture and educational background.\n\nIn linguistics, semantics is the subfield that is devoted to the study of meaning, as inherent at the levels of words, phrases, sentences, and larger units of discourse (termed \"texts\", or \"narratives\"). The study of semantics is also closely linked to the subjects of representation, reference and denotation. The basic study of semantics is oriented to the examination of the meaning of signs, and the study of relations between different linguistic units and compounds: homonymy, synonymy, antonymy, hypernymy, hyponymy, meronymy, metonymy, holonymy, paronyms. A key concern is how meaning attaches to larger chunks of text, possibly as a result of the composition from smaller units of meaning. Traditionally, semantics has included the study of \"sense\" and denotative \"reference\", truth conditions, argument structure, thematic roles, discourse analysis, and the linkage of all of these to syntax.\n\nIn the late 1960s, Richard Montague proposed a system for defining semantic entries in the lexicon in terms of the lambda calculus. In these terms, the syntactic parse of the sentence \"John ate every bagel\" would consist of a subject (\"John\") and a predicate (\"ate every bagel\"); Montague demonstrated that the meaning of the sentence altogether could be decomposed into the meanings of its parts and in relatively few rules of combination. The logical predicate thus obtained would be elaborated further, e.g. using truth theory models, which ultimately relate meanings to a set of Tarskian universals, which may lie outside the logic. The notion of such meaning atoms or primitives is basic to the language of thought hypothesis from the 1970s.\n\nDespite its elegance, Montague grammar was limited by the context-dependent variability in word sense, and led to several attempts at incorporating context, such as:\n\nIn Chomskyan linguistics there was no mechanism for the learning of semantic relations, and the nativist view considered all semantic notions as inborn. Thus, even novel concepts were proposed to have been dormant in some sense. This view was also thought unable to address many issues such as metaphor or associative meanings, and semantic change, where meanings within a linguistic community change over time, and qualia or subjective experience. Another issue not addressed by the nativist model was how perceptual cues are combined in thought, e.g. in mental rotation.\n\nThis view of semantics, as an innate finite meaning inherent in a lexical unit that can be composed to generate meanings for larger chunks of discourse, is now being fiercely debated in the emerging domain of cognitive linguistics\nand also in the non-Fodorian camp in philosophy of language.\nThe main challenge is motivated by:\n\nA concrete example of the latter phenomenon is semantic underspecification – meanings are not complete without some elements of context. To take an example of one word, \"red\", its meaning in a phrase such as \"red book\" is similar to many other usages, and can be viewed as compositional. However, the colours implied in phrases such as \"red wine\" (very dark), and \"red hair\" (coppery), or \"red soil\", or \"red skin\" are very different. Indeed, these colours by themselves would not be called \"red\" by native speakers. These instances are contrastive, so \"red wine\" is so called only in comparison with the other kind of wine (which also is not \"white\" for the same reasons). This view goes back to de Saussure:\nand may go back to earlier Indian views on language, especially the Nyaya view of words as indicators and not carriers of meaning.\n\nAn attempt to defend a system based on propositional meaning for semantic underspecification can be found in the generative lexicon model of James Pustejovsky, who extends contextual operations (based on type shifting) into the lexicon. Thus meanings are generated \"on the fly\" (as you go), based on finite context.\n\nAnother set of concepts related to fuzziness in semantics is based on prototypes. The work of Eleanor Rosch in the 1970s led to a view that natural categories are not characterizable in terms of necessary and sufficient conditions, but are graded (fuzzy at their boundaries) and inconsistent as to the status of their constituent members. One may compare it with Jung's archetype, though the concept of archetype sticks to static concept. Some post-structuralists are against the fixed or static meaning of the words. Derrida, following Nietzsche, talked about slippages in fixed meanings.\n\nSystems of categories are not objectively \"out there\" in the world but are rooted in people's experience. These categories evolve as learned concepts of the world – meaning is not an objective truth, but a subjective construct, learned from experience, and language arises out of the \"grounding of our conceptual systems in shared embodiment and bodily experience\".\nA corollary of this is that the conceptual categories (i.e. the lexicon) will not be identical for different cultures, or indeed, for every individual in the same culture. This leads to another debate (see the Sapir–Whorf hypothesis or Eskimo words for snow).\n\nOriginates from Montague's work (see above). A highly formalized theory of natural language semantics in which expressions are assigned denotations (meanings) such as individuals, truth values, or functions from one of these to another. The truth of a sentence, and its logical relation to other sentences, is then evaluated relative to a model.\n\nPioneered by the philosopher Donald Davidson, another formalized theory, which aims to associate each natural language sentence with a meta-language description of the conditions under which it is true, for example: 'Snow is white' is true if and only if snow is white. The challenge is to arrive at the truth conditions for any sentences from fixed meanings assigned to the individual words and fixed rules for how to combine them. In practice, truth-conditional semantics is similar to model-theoretic semantics; conceptually, however, they differ in that truth-conditional semantics seeks to connect language with statements about the real world (in the form of meta-language statements), rather than with abstract models.\n\nThis theory is an effort to explain properties of argument structure. The assumption behind this theory is that syntactic properties of phrases reflect the meanings of the words that head them. With this theory, linguists can better deal with the fact that subtle differences in word meaning correlate with other differences in the syntactic structure that the word appears in. The way this is gone about is by looking at the internal structure of words. These small parts that make up the internal structure of words are termed \"semantic primitives\".\n\nA linguistic theory that investigates word meaning. This theory understands that the meaning of a word is fully reflected by its context. Here, the meaning of a word is constituted by its contextual relations. Therefore, a distinction between degrees of participation as well as modes of participation are made. In order to accomplish this distinction any part of a sentence that bears a meaning and combines with the meanings of other constituents is labeled as a semantic constituent. Semantic constituents that cannot be broken down into more elementary constituents are labeled minimal semantic constituents.\n\nComputational semantics is focused on the processing of linguistic meaning. In order to do this concrete algorithms and architectures are described. Within this framework the algorithms and architectures are also analyzed in terms of decidability, time/space complexity, data structures that they require and communication protocols.\n\nIn computer science, the term \"semantics\" refers to the meaning of language constructs, as opposed to their form (syntax). According to Euzenat, semantics \"provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared.\" In ontology engineering, the term \"semantics\" refers to the meaning of concepts, properties, and relationships that formally represent real-world entities, events, and scenes in a logical underpinning, such as a description logic, and typically implemented in the Web Ontology Language. The meaning of description logic concepts and roles is defined by their model-theoretic semantics, which are based on interpretations. The concepts, properties, and relationships defined in OWL ontologies can be deployed directly in the web site markup as RDFa, HTML5 Microdata, or JSON-LD, in graph databases as RDF triples or quads, and dereferenced in LOD datasets.\n\nThe semantics of programming languages and other languages is an important issue and area of study in computer science. Like the syntax of a language, its semantics can be defined exactly.\n\nFor instance, the following statements use different syntaxes, but cause the same instructions to be executed, namely, perform an arithmetical addition of 'y' to 'x' and store the result in a variable called 'x':\nVarious ways have been developed to describe the semantics of programming languages formally, building on mathematical logic:\n\nThe Semantic Web refers to the extension of the World Wide Web via embedding added semantic metadata, using semantic data modeling techniques such as Resource Description Framework (RDF) and Web Ontology Language (OWL).\nOn the Semantic Web, terms such as \"semantic network\" and \"semantic data model\" are used to describe particular types of data model characterized by the use of directed graphs in which the vertices denote concepts or entities in the world and their properties, and the arcs denote relationships between them. These can formally be described as description logic concepts and roles, which correspond to OWL classes and properties.\n\nIn psychology, \"semantic memory\" is memory for meaning – in other words, the aspect of memory that preserves only the \"gist\", the general significance, of remembered experience – while episodic memory is memory for the ephemeral details – the individual features, or the unique particulars of experience. The term 'episodic memory' was introduced by Tulving and Schacter in the context of 'declarative memory' which involved simple association of factual or objective information concerning its object. Word meaning is measured by the company they keep, i.e. the relationships among words themselves in a semantic network. The memories may be transferred intergenerationally or isolated in one generation due to a cultural disruption. Different generations may have different experiences at similar points in their own time-lines. This may then create a vertically heterogeneous semantic net for certain words in an otherwise homogeneous culture. In a network created by people analyzing their understanding of the word (such as Wordnet) the links and decomposition structures of the network are few in number and kind, and include \"part of\", \"kind of\", and similar links. In automated ontologies the links are computed vectors without explicit meaning. Various automated technologies are being developed to compute the meaning of words: latent semantic indexing and support vector machines as well as natural language processing, artificial neural networks and predicate calculus techniques.\n\nIdeasthesia is a psychological phenomenon in which activation of concepts evokes sensory experiences. For example, in synesthesia, activation of a concept of a letter (e.g., that of the letter \"A\") evokes sensory-like experiences (e.g., of red color).\n\n"}
{"id": "1812151", "url": "https://en.wikipedia.org/wiki?curid=1812151", "title": "Skeleton (category theory)", "text": "Skeleton (category theory)\n\nIn mathematics, a skeleton of a category is a subcategory which, roughly speaking, does not contain any extraneous isomorphisms. In a certain sense, the skeleton of a category is the \"smallest\" equivalent category which captures all \"categorical properties\". In fact, two categories are equivalent if and only if they have isomorphic skeletons. A category is called skeletal if isomorphic objects are necessarily identical.\n\nA skeleton of a category \"C\" is an equivalent category \"D\" in which no two distinct objects are isomorphic. It is generally considered to be a subcategory. In detail, a skeleton of \"C\" is a category \"D\" such that:\n\nfor every pair of objects \"d\" and \"d\" of \"D\", the morphisms in \"D\" are morphisms in \"C\", i.e.\nand the identities and compositions in \"D\" are the restrictions of those in \"C\".\n\nIt is a basic fact that every small category has a skeleton; more generally, every accessible category has a skeleton. (This is equivalent to the axiom of choice.) Also, although a category may have many distinct skeletons, any two skeletons are isomorphic as categories, so up to isomorphism of categories, the skeleton of a category is unique.\n\nThe importance of skeletons comes from the fact that they are (up to isomorphism of categories), canonical representatives of the equivalence classes of categories under the equivalence relation of equivalence of categories. This follows from the fact that any skeleton of a category \"C\" is equivalent to \"C\", and that two categories are equivalent if and only if they have isomorphic skeletons.\n\n\n\n"}
{"id": "12447991", "url": "https://en.wikipedia.org/wiki?curid=12447991", "title": "Social entropy", "text": "Social entropy\n\nSocial entropy is a sociological theory that evaluates social behaviours using a method based on the second law of thermodynamics. The equivalent of entropy in a social system is considered to be wealth or residence location. The theory was introduced by Kenneth D. Bailey (sociologist) in 1990.\n\n"}
{"id": "17653819", "url": "https://en.wikipedia.org/wiki?curid=17653819", "title": "SolarAid", "text": "SolarAid\n\nSolarAid is an international development charity which is working to create a sustainable market for solar lights in Africa. The organisation's aim is to reduce global poverty and climate change. SolarAid wholly owns an African social enterprise, SunnyMoney, the largest seller of solar lights in Africa. SolarAid was founded by Solarcentury, a solar energy company based in the UK.\n\nSolarAid aims to eradicate the kerosene lamp from Africa through the creation of a sustainable market for solar lights. The charity's social enterprise, SunnyMoney, operates in Uganda, Zambia and Malawi. A pilot project has also been conducted in Senegal in West Africa.\n\nSolarAid is the recipient of a 2013 Google Global Impact Award, a 2013 \"Guardian\" Sustainable Business Award. and the 2013 Ashden Gold Award.\n\nIn 2015, Indian Canadian singer Raghav released the single \"Until the Sun Comes Up\" in support of the SolarAid efforts. The single features also vocals from Indian film superstar actor Abhishek Bachchan and American rapper and singer Nelly.\n\n\n"}
{"id": "17642768", "url": "https://en.wikipedia.org/wiki?curid=17642768", "title": "Statement (logic)", "text": "Statement (logic)\n\nIn logic, the term statement is variously understood to mean either: \nIn the latter case, a statement is distinct from a sentence in that a sentence is only one formulation of a statement, whereas there may be many other formulations expressing the same statement.\n\nPhilosopher of language, Peter Strawson advocated the use of the term \"statement\" in sense (b) in preference to proposition. Strawson used the term \"Statement\" to make the point that two declarative sentences can make the same statement if they say the same thing in different ways. Thus in the usage advocated by Strawson, \"All men are mortal.\" and \"Every man is mortal.\" are two different sentences that make the same statement.\n\nIn either case a statement is viewed as a truth bearer.\n\nExamples of sentences that are (or make) statements:\n\n\nExamples of sentences that are not (or do not make) statements:\n\n\nThe first two examples are not declarative sentences and therefore are not (or do not make) statements.\nThe third and fourth are declarative sentences but, lacking meaning, are neither true nor false and therefore are not (or do not make) statements. The fifth and sixth examples are meaningful declarative sentences, but are not statements but rather matters of opinion or taste. Whether or not the sentence \"Pegasus exists.\" is a statement is a subject of debate among philosophers. Bertrand Russell held that it is a (false) statement. Strawson held it is not a statement at all.\n\nIn some treatments \"statement\" is introduced in order to distinguish a sentence from its informational content. A statement is regarded as the information content of an information-bearing sentence. Thus, a sentence is related to the statement it bears like a numeral to the number it refers to. Statements are abstract logical entities, while sentences are grammatical entities.\n\n\n"}
{"id": "1701956", "url": "https://en.wikipedia.org/wiki?curid=1701956", "title": "Sturmian word", "text": "Sturmian word\n\nIn mathematics, a Sturmian word (Sturmian sequence or billiard sequence), named after Jacques Charles François Sturm, is a certain kind of infinitely long sequence of characters. Such a sequence can be generated by considering a game of English billiards on a square table. The struck ball will successively hit the vertical and horizontal edges labelled 0 and 1 generating a sequence of letters. This sequence is a Sturmian word.\n\nSturmian sequences can be defined strictly in terms of their combinatoric properties or geometrically as cutting sequences for lines of irrational slope or codings for irrational rotations. They are traditionally taken to be infinite sequences on the alphabet of the two symbols 0 and 1.\n\nFor an infinite sequence of symbols \"w\", let \"σ\"(\"n\") be the complexity function of \"w\"; i.e., \"σ\"(\"n\") = the number of distinct subwords in \"w\" of length \"n\". \"w\" is Sturmian if \"σ\"(n) = \"n\" + 1 for all \"n\".\n\nA set \"X\" of binary strings is called \"balanced\" if the Hamming weight of elements of \"X\" takes at most two distinct values. That is, for any formula_1 |\"s\"| = \"k\" or |\"s\"| = \"k\"' where |\"s\"| is the number of 1s in \"s\".\n\nLet \"w\" be an infinite sequence of 0s and 1s and let formula_2 denote the set of all length-\"n\" subwords of \"w\". The sequence \"w\" is Sturmian if formula_2 is balanced for all \"n\" and \"w\" is not eventually periodic.\n\nLet \"w\" be an infinite sequence of 0s and 1s. The sequence \"w\" is Sturmian if for some formula_4 and some irrational formula_5, \"w\" is realized as the cutting sequence of the line formula_6.\n\nLet \"w\" = (\"w\") be an infinite sequence of 0s and 1s. The sequence \"w\" is Sturmian if it is the difference of non-homogeneous Beatty sequences, that is, for some formula_4 and some irrational formula_8\nfor all formula_10 or\nfor all formula_10.\n\nFor formula_13, define formula_14 by formula_15. For formula_4 define the \"θ\"-coding of \"x\" to be the sequence (\"x\") where\n\nLet \"w\" be an infinite sequence of 0s and 1s. The sequence \"w\" is Sturmian if for some formula_4 and some irrational formula_5, \"w\" is the \"θ\"-coding of \"x\".\n\nA famous example of a (standard) Sturmian word is the Fibonacci word; its slope is formula_20, where formula_21 is the golden ratio.\n\nA set \"S\" of finite binary words is \"balanced\" if for each \"n\" the subset \"S\" of words of length \"n\" has the property that the Hamming weight of the words in \"S\" takes at most two distinct values. A balanced sequence is one for which the set of factors is balanced. A balanced sequence has at most \"n\"+1 distinct factors of length \"n\". An aperiodic sequence is one which does not consist of a finite sequence followed by a finite cycle. An aperiodic sequence has at least \"n\" + 1 distinct factors of length \"n\". A sequence is Sturmian if and only if it is balanced and aperiodic.\n\nA sequence formula_22 over {0,1} is a Sturmian word if and only if there exist two real numbers, the \"slope\" formula_23 and the \"intercept\" formula_24, with formula_23 irrational, such that\n\nfor all formula_10. Thus a Sturmian word provides a discretization of the straight line with slope formula_23 and intercept \"ρ\". Without loss of generality, we can always assume formula_29, because for any integer \"k\" we have\n\nAll the Sturmian words corresponding to the same slope formula_23 have the same set of factors; the word formula_32 corresponding to the intercept formula_33 is the standard word or characteristic word of slope formula_23. Hence, if formula_29, the characteristic word formula_32 is the first difference of the Beatty sequence corresponding to the irrational number formula_23.\n\nThe standard word formula_32 is also the limit of a sequence of words formula_39 defined recursively as follows:\n\nLet formula_40 be the continued fraction expansion of formula_23, and define\nwhere the product between words is just their concatenation. Every word in the sequence formula_45 is a prefix of the next ones, so that the sequence itself converges to an infinite word, which is formula_32.\n\nThe infinite sequence of words formula_39 defined by the above recursion is called the standard sequence for the standard word formula_32, and the infinite sequence \"d\" = (\"d\", \"d\", \"d\", ...) of nonnegative integers, with \"d\" ≥ 0 and \"d\" > 0 (\"n\" ≥ 2), is called its directive sequence.\n\nA Sturmian word \"w\" over {0,1} is characteristic if and only if both 0\"w\" and 1\"w\" are Sturmian.\n\nIf \"s\" is an infinite sequence word and \"w\" is a finite word, let μ(\"w\") denote the number of occurrences of \"w\" as a factor in the prefix of \"s\" of length \"N\" + |\"w\"| − 1. If \"μ\"(\"w\") has a limit as \"N\"→∞, we call this the frequency of \"w\", denoted by \"μ\"(\"w\").\n\nFor a Sturmian word \"s\", every finite factor has a frequency. The three-gap theorem implies that the factors of fixed length \"n\" have at most three distinct frequencies, and if there are three values then one is the sum of the other two.\n\nFor words over an alphabet of size \"k\" greater than 2, we define a Sturmian word to be one with complexity function \"n\" + \"k\" − 1. They can be described in terms of cutting sequences for \"k\"-dimensional space. An alternative definition is as words of minimal complexity subject to not being ultimately periodic.\n\nA real number for which the digits with respect to some fixed base form a Sturmian word is a transcendental number.\n\nAlthough the study of Sturmian words dates back to Johann III Bernoulli (1772), it was Gustav A. Hedlund and Marston Morse in 1940 who coined the term \"Sturmian\" to refer to such sequences, in honor of the mathematician Jacques Charles François Sturm due to the relation with the Sturm comparison theorem.\n\n"}
{"id": "227053", "url": "https://en.wikipedia.org/wiki?curid=227053", "title": "Superorganism", "text": "Superorganism\n\nA superorganism or supraorganism (the latter is less frequently used but more etymologically correct) or extended organism is a group of synergistically interacting organisms of the same species. A community of synergistically interacting organisms of different species is called a holobiont.\n\nThe term superorganism is used most often to describe a social unit of eusocial animals, where division of labour is highly specialised and where individuals are not able to survive by themselves for extended periods. Ants are the best-known example of such a superorganism. A superorganism can be defined as \"a collection of agents which can act in concert to produce phenomena governed by the collective\", phenomena being any activity \"the hive wants\" such as ants collecting food and avoiding predators, or bees choosing a new nest site. Superorganisms tend to exhibit homeostasis, power law scaling, persistent disequilibrium and emergent behaviours.\n\nThe term was coined in 1789 by James Hutton, the \"father of geology\", to refer to Earth in the context of geophysiology. The Gaia hypothesis of James Lovelock, and Lynn Margulis as well as the work of Hutton, Vladimir Vernadsky and Guy Murchie, have suggested that the biosphere itself can be considered a superorganism, although this has been disputed. This view relates to systems theory and the dynamics of a complex system.\n\nThe concept of a superorganism raises the question of what is to be considered an individual. Toby Tyrrell's critique of the Gaia hypothesis argues that Earth's climate system does not resemble an animal's physiological system. Planetary biospheres are not tightly regulated in the same way that animal bodies are: \"planets, unlike animals, are not products of evolution. Therefore we are entitled to be highly skeptical (or even outright dismissive) about whether to expect something akin to a \"superorganism\"\". He concludes that \"the superorganism analogy is unwarranted\". However, as Gaia is another systemic level of integration in Nature, precisely has properties that cannot be inferred from their components.\n\nSome scientists have suggested that individual human beings can be thought of as \"superorganisms\"; as a typical human digestive system contains 10 to 10 microorganisms whose collective genome, the microbiome studied by the Human Microbiome Project, contains at least 100 times as many genes as the human genome itself. Salvucci wrote that superorganism is another level of integration that it is observed in nature. These levels include the genomic, the organismal and the ecological levels. The genomic structure of organism reveals the fundamental role of integration and gene shuffling along evolution.\n\nThe nineteenth century thinker Herbert Spencer coined the term \"super-organic\" to focus on social organization (the first chapter of his \"Principles of Sociology\" is entitled \"Super-organic Evolution\"), though this was apparently a distinction between the organic and the social, \"not\" an identity: Spencer explored the holistic nature of society as a social organism while distinguishing the ways in which society did not behave like an organism. For Spencer, the super-organic was an emergent property of interacting organisms, that is, human beings. And, as has been argued by D. C. Phillips, there is a \"difference between emergence and reductionism\".\n\nThe economist Carl Menger expanded upon the evolutionary nature of much social growth, but without ever abandoning methodological individualism. Many social institutions arose, Menger argued, not as \"the result of socially teleological causes, but the unintended result of innumerable efforts of economic subjects pursuing 'individual' interests\".\n\nSpencer and Menger both argued that because it is individuals who choose and act, any social whole should be considered less than an organism, though Menger emphasized this more emphatically. Spencer used the organistic idea to engage in extended analysis of social structure, conceding that it was primarily an analogy. So, for Spencer, the idea of the super-organic best designated a distinct level of social reality above that of biology and psychology, and not a one-to-one identity with an organism. Nevertheless, Spencer maintained that \"every organism of appreciable size is a society\", which has suggested to some that the issue may be terminological.\n\nThe term \"superorganic\" was adopted by the anthropologist Alfred L. Kroeber in 1917. Social aspects of the superorganism concept are analysed in Marshall (2002). Finally, recent work in social psychology has offered the superorganism metaphor as a unifying framework to understand diverse aspects of human sociality, such as religion, conformity, and social identity processes.\n\nSuperorganisms are important in cybernetics, particularly biocybernetics. They exhibit a form of \"distributed intelligence\", a system in which many individual agents with limited intelligence and information are able to pool resources to accomplish a goal beyond the capabilities of the individuals. Existence of such behavior in organisms has many implications for military and management applications, and is being actively researched.\n\n\n\n"}
{"id": "49021319", "url": "https://en.wikipedia.org/wiki?curid=49021319", "title": "TALE-likes", "text": "TALE-likes\n\nTranscription Activator Like Effector Likes (TALE-likes) are a group of bacterial DNA binding proteins named for the first and still best studied group, the TALEs of \"Xanthomonas\" bacteria. TALEs are important factors in the plant diseases caused by \"Xanthomonas\" bacteria, but are known primarily for their role in biotechnology as programmable DNA binding proteins, particularly in the context of TALE nucleases. TALE-likes have additionally been found in many strains of the \"Ralstonia solanacearum\" bacterial species complex, in \"Burkholderia rhizoxinica\" strain HKI 454, and in two unknown marine bacteria. Whether or not all these proteins from a single phylogenetic grouping is as yet unclear.\n\nThe unifying feature of the TALE-likes are their tandem arrays of DNA binding repeats. These repeats are, with few exceptions, 33-35 amino acids in length, and composed of two alpha-helices on either side of a flexible loop containing the DNA base binding residues and with neighbouring repeats joined by flexible linker loops. Evidence for this common structure comes in part from solved crystal structures of TALEs and a \"Burkholderia\" TALE-like, but also from the conservation of the code that all TALE-likes use to recognise DNA-sequences.\n\nTALEs are the first identified, best-studied and largest group within the TALE-likes. TALEs are found throughout the bacterial genus \"Xanthomonas\", comprising mostly plant pathogens. Those TALEs which have been studied have all been shown to be secreted as part of the Type III secretion system into host plant cells. Once inside the host cell they translocate to the nucleus, bind specific DNA sequences within host promoters and turn on downstream genes. Every part of this process is thought to be conserved across all TALEs. The single meaningful difference between individual TALEs, based on current understanding, is the specific DNA sequence that each TALE binds. TALEs from even closely related strains differ in the composition of repeats that make up their DNA binding domain. Repeat composition determines DNA binding preference. In particular position 13 of each repeat confers the DNA base preference of each repeat. During early research it was noted that almost all the differences between repeats of a single TALE repeat array are found in positions 12 and 13 and this finding led to the hypothesis that these residues determine base preference. In fact repeat positions 12 and 13, referred to jointly as the Repeat Variable Diresidue (RVD) are commonly said to confer base specificity despite clear evidence that position 13 is the base determining residue. In addition to the repeat domain TALEs also possess a number of conserved features in the domains flanking the repeats. These include domains for type-III-secretion, nuclear localization and transcriptional activation. This allows TALEs to carry out their biological role as effector proteins secreted into host plant cells to activate expression of specific host genes.\n\nDiversity and evolution\n\nWhilst the RVD positions are commonly the only variable positions within a single TALE repeat array it should be noted that there are more differences when comparing repeat arrays of different TALEs. The diversity of TALEs across the Xanthomonas genus is considerable, but a particularly striking finding is that the evolutionary history one arrives at by comparing repeat compositions differs from that found when comparing non-repeat sequences. Repeat arrays of TALEs are thought to evolve rapidly, with a number of recombinatorial processes suggested to shape repeat array evolution. Recombination of TALE repeat arrays has been demonstrated in a forced-selection experiment. This evolutionary dynamism is though to be made possible by the very high sequence identity of TALE repeats, which is a unique feature of TALEs as opposed to other TALE-likes.\n\nT-zero\n\nAnother unique feature of TALEs is a set of four repeat structures at the N-terminal flank of the core repeat array. These structures, termed non-canonical or degenerate repeats have been shown to be vital for DNA binding, though all but one do not contact DNA bases and thus make no contribution to sequence preference. The one exception is repeat -1, which encodes a fixed T-zero preference to all TALEs. This means that the target sequences of TALEs are always preceded by a thymine base. This is thought to be common to all TALEs, with the possible exception of TalC from \"Xanthomonas oryzae pv. oryzae\" strain AXO1947.\n\nDiscovery and molecular properties\n\nIt was noted in the 2002 publication of the genome of reference strain \"Ralstonia solanacearum\" GMI1000 that its genome encodes a protein similar to \"Xanthomonas\" TALEs. Based on similar domain structure and repeat sequences it was presumed that this gene and homologs in other \"Ralstonia\" strains would encode proteins with the same molecular properties as TALEs, including sequence-specific DNA binding. In 2013 this was confirmed by two studies. These genes and the proteins they encode are referred to as RipTALs (Ralstonia injected protein TALE-like) in line with the standard nomenclature of Ralstonia effectors. Whilst the DNA binding code of the core repeats is conserved with TALEs, RipTALs do not share the T-zero preference, instead they have a strict G-zero requirement. In addition repeats within a single RipTAL repeat array have multiple sequence differences beyond the RVD positions, unlike the near-identical repeats of TALEs.\n\nBiological role\n\nSeveral lines of evidence support the idea that RipTALs function as effector proteins, promoting bacterial growth or disease by manipulating the expression of plant genes. They are secreted into plant cells by the Type III secretion system, which is the main delivery system for effector proteins. They are able to function as sequence-specific transcription factors in plant cells. In addition a strain lacking its RipTAL was shown to grow slower inside eggplant leaf tissue than the wild type. Furthermore, a study based on DNA polymorphisms in \"ripTAL\" repeat domain sequences and host plants found a statistically significant connection between host plant and repeat domain variants. This is expected if the RipTALs of different strains are adapted to target genes in specific host plants. Despite this to date no target genes have been identified for any RipTAL.\n\nDiscovery\n\nThe publication of the genome of bacterial strain \"Bukrholderia rhizoxinica\" HKI 454, in 2011 led to the discovery of a set of TALE-like genes that differed considerably in nature from the TALEs and RipTALS. The proteins encoded by these genes were studied for their DNA binding properties by two groups independently and named the Bats (Burkholderia TALE-likes ) or BurrH. This research showed that the repeat units of the \"Burkholderia\" TALE-likes bind DNA with the same code as TALEs, governed by position 13 of each repeat. There are, however, a number of differences.\n\nBiological role\n\n\"Burkholderia\" TALE-likes are composed almost entirely of repeats, lacking the large non-repetitive domains found flanking the repeats in TALEs and RpTALs. Those domains are key to the functions of TALEs and RipTALs allowing them to infiltrate the plant nucleus and turn on gene expression. It is therefore currently unclear what the biological roles of \"Burkholderia\" TALE-likes are. What is clear is that they are not effector proteins secreted into plant cells to act as transcription factors, the biological role of TALEs and RipTALs. It is not unexpected that they may differ in biological roles from TALEs and RipTALs since the life style of the bacterium they derive from is very unlike that of TALE and RipTAL bearing bacteria. \"B. rhizoxinica\" is an endosymbiont, living inside a fungus, \"Rhizopus microsporus\", a plant pathogen. The same fungus is also an opportunistic human pathogen in immuno-compromised patients, but whereas \"B. rhizoxinica\" is necessary for pathogenicity on plant hosts it is irrelevant to human infection. It is unclear whether the \"Burkholderia\" TALE-likes are ever secreted either into the fungus, let alone into host plants.\nUses in Biotechnology\n\nAs noted in the publications on \"Burkholderia\" TALE-likes there may be some advantages to using these proteins as a scaffold for programmable DNA-binding proteins to function as transcription factors or designer-nucleases, compared to TALEs. These advantages are a shorter repeat size, more compact domain structure (no large non-repeat domains), greater repeat sequence diversity enabling the use of PCR on the genes encoding them and making them less vulnerable to recombinatorial repeat loss. In addition Burkholderia TALE-likes have no T-zero requirement relaxing the constraints on DNA target selection. However, to uses of Burkholderia TALE-likes as programmable DNA binding proteins have been published, outside of the original characterization publications.\n\nDiscovery\n\nIn 2007 the results of a sweep of the world's oceans by the J. Craig Venter Institute were made publicly available. The paper in 2014 on \"Burkholderia\" TALE-likes was also the first to report that two entries from that database resembled TALE-likes, based on sequence similarity. These were further characterized and assessed for their DNA-binding potential in 2015. The repeat units encoded by these sequences were found to mediate DNA binding with base preference matching the TALE code, and judged likely to form structures nearly identical to Bat1 repeats based on molecular dynamics simulations. The proteins encoded by these DNA sequences were therefore designated Marine Organism TALE-likes (MOrTLs) 1 and 2.\n\nEvolutionary relationship to other TALE-likes\n\nWhilst repeats of MOrTL1 and 2 both conform structurally and functionally to the TALE-like norm, they differ considerably at the sequence level both from all other TALE-likes and from one another. It is not known whether they are truly homologous to the other TALE-likes, and thus constitute together with the TALEs, RipTALs and Bats a true protein-family. Alternatively they may have evolved independently. It is particularly difficult to judge the relationship to the other TALE-likes because almost nothing is known of the organisms that MOrTL1 and MOrTL2 come from. It is known only that they were found in two separate sea-water samples from the Gulf of Mexico and are likely to be bacteria based on size-exclusion before DNA sequencing.\n"}
{"id": "279701", "url": "https://en.wikipedia.org/wiki?curid=279701", "title": "Type variable", "text": "Type variable\n\nIn type theory and programming languages, a type variable is a mathematical variable ranging over types. Even in programming languages that allow mutable variables, a type variable remains an abstraction, in the sense that it does not correspond to some memory locations.\n\nProgramming languages that support parametric polymorphism make use of universally quantified type variables. Languages that support existential types make use of existentially quantified type variables. For example, the following OCaml code defines a polymorphic identity function that has a universally quantified type, which is printed by the interpreter on the second line:\n\nIn mathematical notation, the type of the function codice_1 is formula_1, where formula_2 is a type variable.\n\n"}
{"id": "7859407", "url": "https://en.wikipedia.org/wiki?curid=7859407", "title": "Weakly o-minimal structure", "text": "Weakly o-minimal structure\n\nIn model theory, a weakly o-minimal structure is a model theoretic structure whose definable sets in the domain are just finite unions of convex sets.\n\nA linearly ordered structure, \"M\", with language \"L\" including an ordering relation <, is called weakly o-minimal if every parametrically definable subset of \"M\" is a finite union of convex (definable) subsets. A theory is weakly o-minimal if all its models are weakly o-minimal.\n\nNote that, in contrast to o-minimality, it is possible for a theory to have models which are weakly o-minimal and to have other models which are not weakly o-minimal.\n\nIn an o-minimal structure formula_1 the definable sets in formula_2 are finite unions of points and intervals, where \"interval\" stands for a sets of the form formula_3. For weakly o-minimal structures formula_1 this is relaxed so that the definable sets in \"M\" are finite unions of convex definable sets. A set formula_5 is convex if whenever \"a\" and \"b\" are in formula_5, \"a\" < \"b\" and \"c\" ∈  formula_2 satisfies that \"a\" < \"c\" < \"b\", then \"c\" is in \"C\". Points and intervals are of course convex sets, but there are convex sets which are not either points or intervals, as explained below.\n\nIf we have a weakly o-minimal structure expanding (R,<), the real ordered field, then the structure will be o-minimal. The two notions are different in other settings though. For example, let \"R\" be the ordered field of real algebraic numbers with the usual ordering < inherited from R. Take a transcendental number, say \"π\", and add a unary relation \"S\" to the structure given by the subset (−\"π\",\"π\") ∩ \"R\". Now consider the subset \"A\" of \"R\" defined by the formula\n\nSince we have a definable set that isn't a finite union of points and intervals, this structure is not o-minimal. However, it is known that the structure is weakly o-minimal, and in fact the theory of this structure is weakly o-minimal.\n"}
