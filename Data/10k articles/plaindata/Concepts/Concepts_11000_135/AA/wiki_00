{"id": "24713479", "url": "https://en.wikipedia.org/wiki?curid=24713479", "title": "Cambridge change", "text": "Cambridge change\n\nA Cambridge change is a philosophical concept of change according to which an entity \"x\" has changed \"if and only if\" there is some predicate F that is true (not true) of \"x\" at a time \"t1\" but not true (true) of \"x\" at some later time \"t2\".\n\nThe term \"Cambridge change\" was coined by Peter Geach in the late 1960s, in reference to Russell and McTaggart, philosophers active at Cambridge University.\n\nSuppose that at \"t1\", person A is 180 cm tall and person B is 175 cm tall, while at time \"t2\" A is still 180 cm tall but B has grown to be 185 cm tall. Since the predicate 'is taller than B' is true of A at \"t1\" but not true of A at \"t2\", A has changed according to the Cambridge change definition of \"change\"—he has gone from being taller than B to not being taller than B.\n\nIntuitively, however, it is only person B, and not person A, who has changed: B has grown by 10 cm, but A has stayed the same. This problem with Cambridge changes is usually thought to call for a distinction between intrinsic and extrinsic, or natural and non-natural, properties. Given such a distinction, it is possible to define \"real\" change by requiring that the respective predicates express an intrinsic change in a property, such as a change in height from 175 cm to 185 cm, rather than an extrinsic change in a property, such as being now taller than B.\n\nBut this assumes that there \"really\" are strictly unary (non-relational) properties that, as such, are thus intrinsic. Namely, a property is intrinsic if and only if it is actually (really, analytically, fundamentally, necessarily, ontologically) unary.\n\nBut imagine that all \"meter sticks\" as of time \"t2\" have contracted at a rate such that B's height at time \"t2\" would be measured as having increased by 10 cm. Imagine further that A's height has actually (really) shrunk so that it would be measured with a respectively shrinking meter stick as remaining constant from time \"t1\" to time \"t2\".\n\nIntuitively, B's height will have remained the same whereas A's height and all ways of measuring height will have changed. In this case, it is A's height and all ways of measuring height that will have changed. The problem with the notion of \"Cambridge change\" is its failure to acknowledge that B's or anything else's height is relational (relative to ways of measuring height). \"Height\" is thus not unary and thus is extrinsic, not intrinsic.\n\n"}
{"id": "2123148", "url": "https://en.wikipedia.org/wiki?curid=2123148", "title": "Civic virtue", "text": "Civic virtue\n\nCivic virtue is the cultivation of habits important for the success of the community. Closely linked to the concept of citizenship, civic virtue is often conceived as the dedication of citizens to the common welfare of their community even at the cost of their individual interests. The identification of the character traits that constitute civic virtue has been a major concern of political philosophy. The term \"civility\" refers to behavior between persons and groups that conforms to a social mode (that is, in accordance with the civil society), as itself being a foundational principle of society and law.\n\nCivic virtues have historically taught as a matter of chief concern in nations under republican forms of government, and societies with cities. When final decisions on public matters are made by a monarch, it is the \"monarch's\" virtues which influence those decisions. When a broader class of people become the decision-makers, it is then their virtues which characterize the types of decisions made. This form of decision-making is considered superior in determining what best protects the interests of the majority. Aristocratic oligarchies may also develop traditions of public lists of virtues they believe appropriate in the governing class, but these virtues differ significantly from those who are generally identified under the category of civic virtue, stressing martial courage over commercial honesty. Constitutions became important in defining the public virtue of republics and constitutional monarchies. The earliest forms of constitutional development can be seen in late medieval Germany (see Communalism before 1800) and in the Dutch and English revolts of the 16th and 17th centuries.\n\nIn the classical culture of Western Europe and those places that follow its political tradition, concern for civic virtue starts with the oldest republics of which we have extensive records, Athens and Rome. Attempting to define the virtues needed to successfully govern the Athenian \"polis\" was a matter of significant concern for Socrates and Plato; a difference in civic vision ultimately was one of the factors that led to the trial of Socrates and his conflict with the Athenian democracy. The \" Politics\" of Aristotle viewed citizenship as consisting, not of political rights, but rather of political duties. Citizens were expected to put their private lives and interests aside and serve the state in accordance with duties defined by law.\n\nRome, even more than Greece, produced a number of moralistic philosophers such as Cicero, and moralistic historians such as Tacitus, Sallust, Plutarch and Livy. Many of these figures were either personally involved in power struggles that took place in the late Roman Republic, or wrote elegies to liberty which was lost during their transition to the Roman Empire. They tended to blame this loss of liberty on the perceived lack of civic virtue in their contemporaries, contrasting them with idealistic examples of virtue drawn from Roman history, and even non-Roman \"barbarians\".\n\nTexts of antiquity became very popular by the Renaissance. Scholars tried to gather as many of them as they could find, especially in monasteries, from Constantinople, and from the Muslim world. Aided by the rediscovery of the virtue ethics and metaphysics of Aristotle by Avicenna and Averroes, Thomas Aquinas fused Aristotle's cardinal virtues with Christianity in his \"Summa Theologica\" (1273).\n\nHumanists wanted to reinstate the ancient ideal of civic virtue through education. Instead of punishing sinners, it was believed that sin could be prevented by raising virtuous children. Living in the city became important for the elite, because people in the city are forced to behave themselves when communicating with others. A problem was that the proletarianization of peasants created an environment in cities where such workers were hard to control. Cities tried to keep the proletarians out or tried to civilize them by forcing them to work in poor houses. Important aspects of civic virtue were: civic conversation (listening to others, trying to reach an agreement, keeping yourself informed so you can have a relevant contribution), civilized behavior (decent clothing, accent, containing feelings and needs), work (people had to make a useful contribution to the society). Religion changed. It became more focused on individual behavior instead of a communion of people. The people who believed in civic virtue belonged to a small majority surrounded by \"barbarity\". Parental authority was popular, especially the authority of the monarch and the state.\n\nCivic virtue was very popular during the Enlightenment but it had changed dramatically. Parental authority began to wane. Freedom became popular. But people can only be free by containing their emotions in order to keep some space for others. Trying to keep proletarians out or putting them in a poor house was not done anymore. The focus was now on educating. Work was an important virtue during the Middle Ages and the Renaissance, but the people who worked were treated with contempt by the non-working elite. The 18th century brought an end to this. The advancing rich merchants class emphasized the importance of work and contributing to society for all people including the elite. Science was popular. The government and the elites tried to change the world and humanity positively by expanding the bureaucracy. Leading thinkers thought that education and the breach of barriers would liberate everybody from stupidity and oppression. Civic conversations were held in societies and scientific journals.\n\nCivic virtue also became a matter of public interest and discussion during the 18th century, in part because of the American Revolutionary War. An anecdote first published in 1906 has Benjamin Franklin answer a woman who asked him, \"Well, Doctor, what have we got – a Republic or a Monarchy?\" He responded: \"A Republic, if you can keep it.\" The current use for this quotation is to bolster with Franklin's authority the opinion that republics require the cultivation of specific political beliefs, interests, and habits among their citizens, and that if those habits are not cultivated, they are in danger of falling back into some sort of authoritarian rule, such as a monarchy.\n\nAmerican historian Gordon S. Wood called it a universal 18th-century assumption that, while no form of government was more beautiful than a republic, monarchies had various advantages: the pomp and circumstances surrounding them cultivated a sense that the rulers were in fact superior to the ruled and entitled to their obedience, and maintained order by their presence. By contrast, in a republic, the rulers were the servants of the public, and there could therefore be no sustained coercion from them. Laws had to be obeyed for the sake of conscience, rather than fear of the ruler's wrath. In a monarchy, people might be restrained by force to submit their own interest to their government's. In a republic, by contrast, people must be \"persuaded\" to submit their own interests to the government, and this voluntary submission constituted the 18th century's notion of civic virtue. In the absence of such persuasion, the authority of the government would collapse, and tyranny or anarchy were imminent.\n\nAuthority for this ideal was found once more among the classical, and especially the Roman, political authors and historians. But since the Roman writers wrote during a time when the Roman republican ideal was fading away, its forms but not its spirit or substance being preserved in the Roman Empire, the 18th-century American and French revolutionaries read them with a spirit to determine how the Roman republic failed, and how to avoid repeating that failure. In his \"Reflections on the Rise and Fall of the Antient Republicks\", the English Whig historian Edward Wortley Montagu sought to describe \"the principal causes of that degeneracy of manners, which reduc'd those once brave and free people into the most abject slavery.\" Following this reading of Roman ideals, the American revolutionary Charles Lee envisioned a Spartan, egalitarian society where every man was a soldier and master of his own land, and where people were \"instructed from early infancy to deem themselves property of the State... (and) were ever ready to sacrifice their concerns to her interests.\" The agrarianism of Thomas Jefferson represents a similar belief system; Jefferson believed that the ideal republic was composed of independent, rural agriculturalists rather than urban tradesmen.\nThese widely held ideals led American revolutionaries to found institutions such as the Society of the Cincinnati, named after the Roman farmer and dictator Cincinnatus, who according to Livy left his farm to lead the army of the Roman republic during a crisis, and voluntarily returned to his plow once the crisis had passed. About Cincinnatus, Livy writes:\n\nCivic virtues were especially important during the 19th and 20th century. Class and profession greatly affected the virtues of the individual, and there was a general division about what the best civic virtues were. Additionally several major ideologies came into being, each with their own ideas about civic virtues.\n\nConservatism emphasized family values and obedience to the father and the state. Nationalism carried by masses of people made patriotism an important civic virtue. Liberalism combined republicanism with a belief in progress and liberalization based on capitalism. Civic virtues focused on individual behavior and responsibility were very important. Many liberals turned into socialists or conservatives in the end of the 19th century and early 20th century. Others became social liberals, valuing capitalism with a strong government to protect the poor. A focus on agriculture and landed nobility was supplanted by a focus on industry and civil society.\n\nAn important civic virtue for socialists was that people be conscious of oppression within society and the forces which uphold the status quo. This consciousness should result in action to change the world for the good, so that everybody can become respectful citizens in a modern society.\n\nNational Socialism, the German variant of twentieth century fascism whose precepts were laid out in Adolf Hitler's Mein Kampf, classified inhabitants of the ideal nation into three main hierarchical categories, each of which had different rights and duties in relation to the state: citizens, subjects, and aliens. The first category, citizens, were to possess full civic rights and responsibilities. Citizenship would be conferred only on those males of pure racial stock who had completed military service, and could be revoked at any time by the state. Only women who worked independently or who married a citizen could obtain citizenship for themselves. The second category, subjects, referred to all others who were born within the nation's boundaries who did not fit the racial criteria for citizenship. Subjects would have no voting rights, could not hold any position within the state, and possessed none of the other rights and civic responsibilities conferred on citizens. The final category, aliens, referred to those who were citizens of another state, who also had no rights:\n\nA number of institutions and organizations promote the idea of civic virtue in the older democracies. Among such organizations are the Boy Scouts of America, and Civil Air Patrol whose U.S. oath, Cadet Oath and Cadet Honor Code reflect a goal to foster habits aimed at serving a larger community:\n\nBoy Scouts of America Scout Oath:\n\nBoy Scouts of America Scout Law:\nCadet Oath:\nAir Force Academy Cadet Honor Code:\nInstitutions that might be said to encourage civic virtue include the school, particularly with social studies courses, and the prison, namely in its rehabilitative function.\n\nOther, later phenomena associated with the concept of civic virtue include \"McGuffey's Eclectic Readers\", a series of primary school textbooks whose compiler, William Holmes McGuffey, deliberately sought out patriotic and religious sentiments to instill these values in the children who read them. William Bennett, a Reagan administration cabinet member turned conservative commentator, produced \"\" in 1993, another anthology of literary materials that might be considered an attempt to update McGuffey's concept.\n\nConfucianism, which specifies cultural virtues and traditions which all members of society are to observe, in particular the heads of households and those who govern, was the basis of Chinese society for more than 2000 years and is still influential in modern China. Its related concepts can be compared to the Western idea of civic virtue.\n\n\n\n"}
{"id": "89246", "url": "https://en.wikipedia.org/wiki?curid=89246", "title": "Curve", "text": "Curve\n\nIn mathematics, a curve (also called a curved line in older texts) is, generally speaking, an object similar to a line but that need not be straight. Thus, a curve is a generalization of a line, in that its curvature need not be zero.\n\nVarious disciplines within mathematics have given the term different meanings depending on the area of study, so the precise meaning depends on context. However, many of these meanings are special instances of the definition which follows. A curve is a topological space which is locally homeomorphic to a line. In everyday language, this means that a curve is a set of points which, near each of its points, looks like a line, up to a deformation. A simple example of a curve is the parabola, shown to the right. A large number of other curves have been studied in multiple mathematical fields.\n\nA closed curve is a curve that forms a path whose starting point is also its ending point—that is, a path from any of its points to the same point.\n\nClosely related meanings include the graph of a function (for example, Phillips curve) and a two-dimensional graph.\n\nInterest in curves began long before they were the subject of mathematical study. This can be seen in numerous examples of their decorative use in art and on everyday objects dating back to prehistoric\ntimes. Curves, or at least their graphical representations, are simple to create, for example by a stick in the sand on a beach.\n\nHistorically, the term \"line\" was used in place of the more modern term \"curve\". Hence the phrases \"straight line\" and \"right line\" were used to distinguish what are today called lines from \"curved lines\". For example, in Book I of Euclid's Elements, a line is defined as a \"breadthless length\" (Def. 2), while a \"straight\" line is defined as \"a line that lies evenly with the points on itself\" (Def. 4). Euclid's idea of a line is perhaps clarified by the statement \"The extremities of a line are points,\" (Def. 3). Later commentators further classified lines according to various schemes. For example:\nThe Greek geometers had studied many other kinds of curves. One reason was their interest in solving geometrical problems that could not be solved using standard compass and straightedge construction.\nThese curves include:\nA fundamental advance in the theory of curves was the advent of analytic geometry in the seventeenth century. This enabled a curve to be described using an equation rather than an elaborate geometrical construction. This not only allowed new curves to be defined and studied, but it enabled a formal distinction to be made between curves that can be defined using algebraic equations, algebraic curves, and those that cannot, transcendental curves. Previously, curves had been described as \"geometrical\" or \"mechanical\" according to how they were, or supposedly could be, generated.\n\nConic sections were applied in astronomy by Kepler.\nNewton also worked on an early example in the calculus of variations. Solutions to variational problems, such as the brachistochrone and tautochrone questions, introduced properties of curves in new ways (in this case, the cycloid). The catenary gets its name as the solution to the problem of a hanging chain, the sort of question that became routinely accessible by means of differential calculus.\n\nIn the eighteenth century came the beginnings of the theory of plane algebraic curves, in general. Newton had studied the cubic curves, in the general description of the real points into 'ovals'. The statement of Bézout's theorem showed a number of aspects which were not directly accessible to the geometry of the time, to do with singular points and complex solutions.\n\nSince the nineteenth century there has not been a separate theory of curves, but rather the appearance of curves as the one-dimensional aspect of projective geometry, and differential geometry; and later topology, when for example the Jordan curve theorem was understood to lie quite deep, as well as being required in complex analysis. The era of the space-filling curves finally provoked the modern definitions of curve.\n\nIn general, a curve is defined through a continuous function formula_1 from an interval of the real numbers into a topological space . Depending on the context, it is either formula_2 or its image formula_3 which is called a curve.\n\nIn general topology, when non-differentiable functions are considered, it is the map formula_2, which is called a curve, because its image may look very differently from what is commonly called a curve. For example, the image of the Peano curve completely fills the square. On the other hand, when one considers curves defined by a differentiable function (or, at least, a piecewise differentiable function), this is commonly the image of the function which is called a curve.\n\nA plane curve is a curve for which formula_22 is the Euclidean plane—these are the examples first encountered—or in some cases the projective plane. A space curve is a curve for which formula_22 is of three dimensions, usually Euclidean space; a skew curve is a space curve which lies in no plane. These definitions of plane, space and skew curves apply also to real algebraic curves, although the above definition of a curve does not apply (a real algebraic curve may be disconnected).\n\nThis definition of curve captures our intuitive notion of a curve as a connected, continuous geometric figure that is \"like\" a line, without thickness and drawn without interruption, although it also includes figures that can hardly be called curves in common usage. For example, the image of a curve can cover a square in the plane (space-filling curve). The image of simple plane curve can have Hausdorff dimension bigger than one (see Koch snowflake) and even positive Lebesgue measure (the last example can be obtained by small variation of the Peano curve construction). The dragon curve is another unusual example.\n\nRoughly speaking a differentiable curve is a curve that is defined as being locally the image of an injective differentiable function formula_1 from an interval of the real numbers into a differentiable manifold , often formula_25\n\nMore precisely, a differentiable curve is a subset of where every point of has a neighborhood such that formula_26 is diffeomorphic to an interval of the real numbers. In other words, a differentiable curve is a differentiable manifold of dimension one.\n\nIf formula_27 is the formula_28-dimensional Euclidean space, and if formula_29 is an injective and continuously differentiable function, then the length of formula_30 is defined as the quantity\nThe length of a curve is independent of the parametrization formula_30.\n\nIn particular, the length formula_33 of the graph of a continuously differentiable function formula_34 defined on a closed interval formula_35 is\n\nMore generally, if formula_37 is a metric space with metric formula_38, then we can define the length of a curve formula_39 by\nwhere the supremum is taken over all formula_41 and all partitions formula_42 of formula_43.\n\nA rectifiable curve is a curve with finite length. A curve formula_39 is called natural (or unit-speed or parametrized by arc length) if for any formula_45 such that formula_46, we have\n\nIf formula_39 is a Lipschitz-continuous function, then it is automatically rectifiable. Moreover, in this case, one can define the speed (or metric derivative) of formula_30 at formula_50 as\nand then show that\n\nWhile the first examples of curves that are met are mostly plane curves (that is, in everyday words, \"curved lines\" in \"two-dimensional space\"), there are obvious examples such as the helix which exist naturally in three dimensions. The needs of geometry, and also for example classical mechanics are to have a notion of curve in space of any number of dimensions. In general relativity, a world line is a curve in spacetime.\n\nIf formula_22 is a differentiable manifold, then we can define the notion of \"differentiable curve\" in formula_22. This general idea is enough to cover many of the applications of curves in mathematics. From a local point of view one can take formula_22 to be Euclidean space. On the other hand, it is useful to be more general, in that (for example) it is possible to define the tangent vectors to formula_22 by means of this notion of curve.\n\nIf formula_22 is a smooth manifold, a \"smooth curve\" in formula_22 is a smooth map\n\nThis is a basic notion. There are less and more restricted ideas, too. If formula_22 is a formula_61 manifold (i.e., a manifold whose charts are formula_62 times continuously differentiable), then a formula_61 curve in formula_22 is such a curve which is only assumed to be formula_61 (i.e. formula_62 times continuously differentiable). If formula_22 is an analytic manifold (i.e. infinitely differentiable and charts are expressible as power series), and formula_2 is an analytic map, then formula_2 is said to be an \"analytic curve\".\n\nA differentiable curve is said to be \"regular\" if its derivative never vanishes. (In words, a regular curve never slows to a stop or backtracks on itself.) Two formula_61 differentiable curves\n\nare said to be \"equivalent\" if there is a bijective formula_61 map\n\nsuch that the inverse map\n\nis also formula_61, and\n\nfor all formula_78. The map formula_79 is called a \"reparametrisation\" of formula_80; and this makes an equivalence relation on the set of all formula_61 differentiable curves in formula_22. A formula_61 \"arc\" is an equivalence class of formula_61 curves under the relation of reparametrisation.\n\nAlgebraic curves are the curves considered in algebraic geometry. A plane algebraic curve is the set of the points of coordinates \"x\", \"y\" such that \"f\"(\"x\", \"y\") = 0, where \"f\" is a polynomial in two variables defined over some field \"F\". Algebraic geometry normally looks not only on points with coordinates in \"F\" but on all the points with coordinates in an algebraically closed field \"K\". If \"C\" is a curve defined by a polynomial \"f\" with coefficients in \"F\", the curve is said to be defined over \"F\". The points of the curve \"C\" with coordinates in a field \"G\" are said to be rational over \"G\" and can be denoted \"C\"(\"G\")). When \"G\" is the field of the rational numbers, one simply talks of \"rational points\". For example, Fermat's Last Theorem may be restated as: \"For n\" > 2, \"every rational point of the Fermat curve of degree n has a zero coordinate\".\n\nAlgebraic curves can also be space curves, or curves in a space of higher dimension, say . They are defined as algebraic varieties of dimension one. They may be obtained as the common solutions of at least polynomial equations in variables. If polynomials are sufficient to define a curve in a space of dimension , the curve is said to be a complete intersection. By eliminating variables (by any tool of elimination theory), an algebraic curve may be projected onto a plane algebraic curve, which however may introduce new singularities such as cusps or double points.\n\nA plane curve may also be completed in a curve in the projective plane: if a curve is defined by a polynomial \"f\" of total degree \"d\", then \"w\"\"f\"(\"u\"/\"w\", \"v\"/\"w\") simplifies to a homogeneous polynomial \"g\"(\"u\", \"v\", \"w\") of degree \"d\". The values of \"u\", \"v\", \"w\" such that \"g\"(\"u\", \"v\", \"w\") = 0 are the homogeneous coordinates of the points of the completion of the curve in the projective plane and the points of the initial curve are those such \"w\" is not zero. An example is the Fermat curve \"u\" + \"v\" = \"w\", which has an affine form \"x\" + \"y\" = 1. A similar process of homogenization may be defined for curves in higher dimensional spaces\n\nImportant examples of algebraic curves are the conics, which are nonsingular curves of degree two and genus zero, and elliptic curves, which are nonsingular curves of genus one studied in number theory and which have important applications to cryptography. Because algebraic curves in fields of characteristic zero are most often studied over the complex numbers, algebraic curves in algebraic geometry may be considered as real surfaces. In particular, the nonsingular complex projective algebraic curves are called Riemann surfaces.\n\n\n"}
{"id": "13954122", "url": "https://en.wikipedia.org/wiki?curid=13954122", "title": "Deicide", "text": "Deicide\n\nDeicide is the killing (or the killer) of a god. The concept may be used for any act of killing a god, including a life-death-rebirth deity who is killed and then resurrected.\n\nThe term deicide was coined in the 17th century from medieval Latin \"*deicidium\", from \"de-us\" \"god\" and \"-cidium\" \"cutting, killing.\"\n\nAccording to the New Testament accounts, the Judean (or Jewish) authorities in Jerusalem, the Pharisees, charged Jesus with blasphemy, a capital crime under biblical law, and sought his execution. According to , the Judean (Jewish) authorities claimed to lack the authority to have Jesus put to death, though it is doubtful what legal basis such a claim would have had; the Jesus Seminar historicity project notes for : \"\"it's illegal for us:\" The accuracy of this claim is doubtful.\" in their \"Scholars Version\". Additionally, records them asking Jesus about stoning the adulteress and records them ordering the stoning of Saint Stephen.\n\nThey brought Jesus to Pontius Pilate, the Roman Prefect of Judea, who was hesitant and let the people decide if Jesus were to be executed. According to the Bible, Pontius Pilate only ordered Jesus to be flogged. Washing his hands, Pilate said he would not take the blame for Jesus' death, to which the crowd replied, \"His blood is upon us and our children.\"\n\nPilate is portrayed in the Gospel accounts as a reluctant accomplice to Jesus' death. Modern scholars say it is most likely that a Roman Governor such as Pilate would have no problem in executing any leader whose followers posed a potential threat to Roman rule. It has also been suggested that the Gospel accounts may have downplayed the role of the Romans in Jesus' death during a time when Christianity was struggling to gain acceptance in the Roman world.\n\nThe Catholic Church and other Christian denominations suggest that Jesus' death was necessary to take away the collective sin of the human race. The crucifixion is seen as an example of Christ's eternal love for mankind and as a self-sacrifice on the part of God for humanity.\n\nThe Gnostic Gospel of Judas contends that Jesus commanded Judas Iscariot to set in motion the chain of events that would lead to his death.\n\nThe following is a verse from a hymn written in 1892 for use in the Church of England to call upon God to convert the Jews to Christianity:\n\nAgainst certain Christian movements, some of which rejected the use of Hebrew Scripture, Augustine countered that God had chosen the Jews as a special people, and he considered the scattering of Jewish people by the Roman Empire to be a fulfillment of prophecy. He rejected homicidal attitudes, quoting part of the same prophecy, namely \"Slay them not, lest they should at last forget Thy law\" (Psalm 59:11). Augustine, who believed Jewish people would be converted to Christianity at \"the end of time\", argued that God had allowed them to survive their dispersion as a warning to Christians; as such, he argued, they should be permitted to dwell in Christian lands. The sentiment sometimes attributed to Augustine that Christians should let the Jews \"survive but not thrive\" (it is repeated by author James Carroll in his book \"Constantine's Sword\", for example) is apocryphal and is not found in any of his writings.\n\n\n\n"}
{"id": "25910555", "url": "https://en.wikipedia.org/wiki?curid=25910555", "title": "Demonic composition", "text": "Demonic composition\n\nIn mathematics, demonic composition is an operation on binary relations that is somewhat comparable to ordinary composition of relations but is robust to refinement of the relations into (partial) functions or injective relations.\n\nUnlike ordinary composition of relations, demonic composition is not associative.\n\nSuppose \"R\" is a binary relation between \"X\" and \"Y\" and \"S\" is a relation between \"Y\" and \"Z\". Their right demonic composition \"R\" ; \"S\" is a relation between \"X\" and \"Z\". Its graph is defined as\n\nConversely, their left demonic composition \"R\" ; \"S\" is defined by\n"}
{"id": "46600473", "url": "https://en.wikipedia.org/wiki?curid=46600473", "title": "Descent along torsors", "text": "Descent along torsors\n\nIn mathematics, given a \"G\"-torsor \"X\" → \"Y\" and a stack \"F\", the descent along torsors says there is a canonical equivalence between \"F\"(\"Y\"), the category of \"Y\"-points and \"F\"(\"X\"), the category of \"G\"-equivariant \"X\"-points. It is a basic example of descent, since it says the \"equivariant data\" (which is an additional data) allows one to \"descend\" from \"X\" to \"Y\".\n\nWhen \"G\" is the Galois group of a finite Galois extension \"L\"/\"K\", for the \"G\"-torsor formula_1, this generalizes classical Galois descent (cf. field of definition).\n\nFor example, one can take \"F\" to be the stack of quasi-coherent sheaves (in an appropriate topology). Then \"F\"(\"X\") consists of equivariant sheaves on \"X\"; thus, the descent in this case says that to give an equivariant sheaf on \"X\" is to give a sheaf on the quotient \"X\"/\"G\".\n\n\n"}
{"id": "18038088", "url": "https://en.wikipedia.org/wiki?curid=18038088", "title": "Determinancy diagramming", "text": "Determinancy diagramming\n\nA determinancy diagram, sometimes known as a dependency diagram, is a diagram which documents the determinancy or dependency between a set of data items. Determinancy diagrams are particularly used as an aid to database normalization.\n\n"}
{"id": "1664235", "url": "https://en.wikipedia.org/wiki?curid=1664235", "title": "Explanatory style", "text": "Explanatory style\n\nExplanatory style is a psychological attribute that indicates how people explain to themselves why they experience a particular event, either positive or negative. \n\nThis aspect covers the degree to which a person attributes an event to internal or external causes. An optimist might attribute a bad experience to luck whereas a pessimist might consider it his or her fault. Another person might also attribute an event to external forces in an unhealthy way (e.g. \"I had no choice but to get violent.\")\n\nThis aspect covers characteristics considered stable versus unstable (across time). An optimist would tend to define his or her failures as unstable (I just didn't study enough for this particular test) whereas a pessimist might think, for example, \"I'm never good at tests\".\n\nThis distinction covers global versus local and/or specific and the extent of the effect. A pessimist might, for example, think that \"Everywhere there is misery\" and an optimist think that, \"I have had dealings mostly with honest people\".\n\nPeople who generally tend to blame themselves for negative events, believe that such events will continue indefinitely, and let such events affect many aspects of their lives display what is called a \"pessimistic explanatory style\". Conversely, people who generally tend to blame outside forces for negative events, believe that such events will end soon, and do not let such events affect too many aspects of their lives display what is called an \"optimistic explanatory style\".\n\nSome research has suggested a pessimistic explanatory style may be correlated with depression and physical illness. The concept of explanatory style encompasses a wide range of possible responses to both positive and negative occurrences, rather than a black-white difference between optimism and pessimism. Also, an individual does not necessarily show a uniform explanatory style in all aspects of life, but may exhibit varying responses to different types of events.\n\nAttributional style emerged from research on depression, with Abramson, Seligman and Teasdale (1978) arguing that a characteristic way of attributing negative outcomesto internal, stable and global causeswould be associated with depression in response to negative events happened to them. As a diathesis–stress model of depression, the model does not predict associations of attributional style with depression in the absence of objective negative events (stressors). A meta-analysis of 104 empirical studies of the theory indicates that the predictions are supported. Data have, however, been ambiguous, and some researchers believe that the theory is well-supported, some believe that it has not had impressive empirical support and some believe that, at least in the early days of the theory, the theory was never adequately tested. One factor accounting for ambiguity in research into the model is whether researchers have assessed attributions for hypothetical events or for real events. Those studies that have looked at attributions for hypothetical events have been more supportive of the model, possibly because these studies are more likely to have controlled for event severity.\n\nThe \"learned helplessness\" model formed the theoretical basis of the original Abramson, Seligman, and Teasdale statement on attributional style. More recently, Abramson, Metalsky and Alloy proposed a modified \"hopelessness theory\". This distinguished hopeless depression and more circumscribed pessimism. It emphasizes the dimensions of stability and globality rather than internality, and suggests that stable and global attributions (rather than internal cause attributions) are associated with hopelessness depression. Hopelessness theory also highlights perceived importance and consequences of a negative outcome in addition to causal attributions as factors in clinical depression.\n\nDevelopmentally, it has been suggested that attributional style originates in experiences of trust or lack of trust in events Along with evidence from twin studies for some heredity basis to attributional style., Eisner argues that repeated exposure to controllable events may foster an optimistic explanatory style, whereas repeated exposure to uncontrollable events may foster a negative attributional style. Trust in interpersonal relationships is argued to build an optimistic explanatory style.\n\nAttributional style is typically assessed using questionnaires such as the Attributional Style Questionnaire or ASQ, which assesses attributions for six negative and six positive hypothetical events, the Expanded Attributional Style Questionnaire or EASQ, which assesses attributions for eighteen hypothetical negative events, and various scales that assess attributions for real events, such as the Real Events Attributional Style Questionnaire or the Attributions Questionnaire. Although these scales provide empirical methodology for study of attributional style, and considerable empirical data support the Abramson-Seligman-Teasdale model of depression, there has been dispute about whether this concept really exists. Cutrona, Russell and Jones, for example, found evidence for considerable cross-situational variation and temporal change of attributional style in women suffering from post-partum depression. Xenikou notes, however, that Cutrona, Russell and Jones found more evidence for the cross-situational consistency of stability and globalism than of internalization. More data in support of long-term stability of attributional style has come from a diary study by Burns and Seligman. Using a technique called Content Analysis of Verbatim Explanation (CAVE), these authors found stable patterns of attributional style over a long time period.\n\nAttributional style may be domain-specific. Using the Attributional Style Assessment Test, Anderson and colleagues found some evidence for domain-specificity of style, for instance work-related attributions vs interpersonal attributions.\n\nModelling of the items of the ASQ suggests that the positive and negative event information (e.g. getting a promotion, losing a job) and the causal nature of attributions - whether events are seen as global or local in scope, or as temporally stable or unstable, for instance - assess distinct factors. A global focus tends to emerge, for instance, independent of the valence of an event. Such effects are found more broadly in cognition, where they are referred to as Global versus local precedence. Optimistic and Pessimistic attributions emerged as independent of each other, supporting models in which these styles have distinct genetic and environmental origins.\n\nAttributional style is, at least superficially, similar to locus of control. However, the locus of control is concerned with expectancies about the future while attribution style is concerned with attributions for the past. Whereas locus of control cuts across both positive and negative outcomes, authors in the attributional style field have distinguished between a Pessimistic Explanatory Style, in which failures are attributed to internal, stable, and global factors and successes to external, unstable, and specific causes, and an Optimistic Explanatory Style, in which successes are attributed to internal, stable, and global factors and failures to external, unstable, and specific causes.\n\n\n"}
{"id": "21313650", "url": "https://en.wikipedia.org/wiki?curid=21313650", "title": "Exponential field", "text": "Exponential field\n\nIn mathematics, an exponential field is a field that has an extra operation on its elements which extends the usual idea of exponentiation.\n\nA field is an algebraic structure composed of a set of elements, \"F\", two binary operations, addition (+) such that \"F\" forms an abelian group with identity 0 and multiplication (·), such that \"F\" excluding 0 forms an abelian group under multiplication with identity 1, and such that multiplication is distributive over addition, that is for any elements \"a\", \"b\", \"c\" in \"F\", one has . If there is also a function \"E\" that maps \"F\" into \"F\", and such that for every \"a\" and \"b\" in \"F\" one has\nthen \"F\" is called an exponential field, and the function \"E\" is called an exponential function on \"F\". Thus an exponential function on a field is a homomorphism between the additive group of \"F\" and its multiplicative group.\n\nThere is a trivial exponential function on any field, namely the map that sends every element to the identity element of the field under multiplication. Thus every field is trivially also an exponential field, so the cases of interest to mathematicians occur when the exponential function is non-trivial.\n\nExponential fields are sometimes required to have characteristic zero as the only exponential function on a field with nonzero characteristic is the trivial one. To see this first note that for any element \"x\" in a field with characteristic \"p\" > 0,\nHence, taking into account the Frobenius endomorphism,\nAnd so \"E\"(\"x\") = 1 for every \"x\".\n\n\nThe underlying set \"F\" may not be required to be a field but instead allowed to simply be a ring, \"R\", and concurrently the exponential function is relaxed to be a homomorphism from the additive group in \"R\" to the multiplicative group of units in \"R\". The resulting object is called an exponential ring.\n\nAn example of an exponential ring with a nontrivial exponential function is the ring of integers Z equipped with the function \"E\" which takes the value +1 at even integers and −1 at odd integers, i.e., the function formula_4 This exponential function, and the trivial one, are the only two functions on Z that satisfy the conditions.\n\nExponential fields are much-studied objects in model theory, occasionally providing a link between it and number theory as in the case of Zilber's work on Schanuel's conjecture. It was proved in the 1990s that R is model complete, a result known as Wilkie's theorem. This result, when combined with Khovanskiĭ's theorem on pfaffian functions, proves that R is also o-minimal. On the other hand, it is known that C is not model complete. The question of decidability is still unresolved. Alfred Tarski posed the question of the decidability of R and hence it is now known as Tarski's exponential function problem. It is known that if the real version of Schanuel's conjecture is true then R is decidable.\n\n"}
{"id": "44806545", "url": "https://en.wikipedia.org/wiki?curid=44806545", "title": "Feud letter", "text": "Feud letter\n\nA feud letter ( or \"Absagebrief\") was a document in which a feud was announced – usually with few words - in medieval Europe. The letter had to be issued three days in advance in order to be legally valid.\n\nSo that the feud did not became a case of murder and thus become punishable by law, those involved had to abide by the following rules:\n\n\n\n\"Wettet, biscop Dierich van Moeres, dat wy den vesten Junker Johan can Cleve lever hebbet alls Juwe, unde wert Juwe hiermit affgesaget\"<br>(\"Know this, Bishop Dietrich of Moers, that we prefer the steadfast Junker, John of Cleves, to you, and hereby give you notice thereof.\")\n\n\n"}
{"id": "19098122", "url": "https://en.wikipedia.org/wiki?curid=19098122", "title": "Finality (law)", "text": "Finality (law)\n\nFinality, in law, is the concept that certain disputes must achieve a resolution from which no further appeal may be taken, and from which no collateral proceedings may be permitted to disturb that resolution. For example, in some jurisdictions, a person convicted of a crime may not sue their defence attorney for incompetence or legal malpractice if the civil lawsuit would call into question the finality of the criminal conviction. Finality is considered to be important because, absent this there would be no certainty as to the meaning of the law, or the outcome of any legal process. The principle is an aspect of the separation of powers, that being a distinction between the executive and the judicial power. This concept was defined within NSW v Kable [2013]. In this case, the court explained that unless orders were valid until set aside, \"the exercise of judicial power could yield no adjudication of right and liability to which immediate effect could be given\". \n\nThe importance of finality is the source of the concept of \"res judicata\" - that the decisions of one court are settled law, and may not be retried in another case brought in a different court. \n"}
{"id": "185427", "url": "https://en.wikipedia.org/wiki?curid=185427", "title": "Function (mathematics)", "text": "Function (mathematics)\n\nIn mathematics, a function was originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a \"function\" of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept. \n\nA function is a process or a relation that associates each element of a set , the \"domain\" of the function, to a single element of another set (possibly the same set), the \"codomain\" of the function. If the function is called , this relation is denoted (read of ), the element is the \"argument\" or \"input\" of the function, and is the \"value of the function\", the \"output\", or the \"image\" of by . The symbol that is used for representing the input is the variable of the function (one often says that is a function of the variable ).\n\nA function is uniquely represented by its graph which is the set of all pairs . When the domain and the codomain are sets of numbers, each such pair may be considered as the Cartesian coordinates of a point in the plane. In general, these points form a curve, which is also called the graph of the function. This is a useful representation of the function, which is commonly used everywhere, for example in newspapers.\n\nFunctions are widely used in science, and in most fields of mathematics. Their role is so important that it has been said that they are \"the central objects of investigation\" in most fields of mathematics.\n\nIntuitively, a function is a process that associates to each element of a set a unique element of a set .\n\nFormally, a function from a set to a set is defined by a set of ordered pairs such that , , and every element of is the first component of exactly one ordered pair in . In other words, for every in , there is exactly one element such that the ordered pair belongs to the set of pairs defining the function . The set is called the graph of the function. Formally speaking, it may be identified with the function, but this hides the usual interpretation of a function as a process. Therefore, in common usage, the function is generally distinguished from its graph. Functions are also called \"maps\" or \"mappings\". However, some authors reserve the word \"mapping\" to the case where the codomain \"Y\" belongs explicitly to the definition of the function. In this sense, the graph of the mapping recovers the function as the set of pairs.\n\nIn the definition of function, and are respectively called the \"domain\" and the \"codomain\" of the function . If belongs to the set defining , then is the \"image\" of under , or the \"value\" of applied to the \"argument\" . Especially in the context of numbers, one says also that is the value of for the \"value of its variable\", or, still shorter, is the \"value of\" \"of\" , denoted as .\n\nThe domain and codomain are not always explicitly given when a function is defined, and, without some (possibly difficult) computation, one knows only that the domain is contained in a larger set. Typically, this occurs in mathematical analysis, where \"a function often refers to a function that may have a proper subset of as domain. For example, a \"function from the reals to the reals\" may refer to a real-valued function of a real variable, and this phrase does not mean that the domain of the function is the whole set of the real numbers, but only that the domain is a set of real numbers that contains a non-empty open interval. For example, if is a function that has the real numbers as domain and codomain, then a function mapping the value to the value formula_1 is a function from the reals to the reals, whose domain is the set of the reals , such that . In many cases, the exact domains are difficult to determine, but this is rarely a problem for working with such functions.\n\nThe range of a function is the set of the images of all elements in the domain. However, \"range\" is sometimes used as a synonym of codomain, generally in old textbooks.\n\nAny subset of the Cartesian product of a domain formula_2 and a codomain formula_3 is said to define a binary relation formula_4 between these two sets. It is immediate that an arbitrary relation may contain pairs that violate the necessary conditions for a function, given above. \n\nA univalent relation is a relation such that\nUnivalent relations may be identified to functions whose domain is a subset of .\n\nA left-total relation is a relation such that \nFormal functions may be strictly identified to relations that are both univalent and left total. Violating the left-totality is similar to giving a convenient encompassing set instead of the true domain, as explained above.\n\nVarious properties of functions and function composition may be reformulated in the language of relations. For example, a function is injective if the converse relation formula_7 is univalent, where the converse relation is defined as formula_8\n\nThere are various standard ways for denoting functions. The most commonly used notation is functional notation, which defines the function using an equation that gives the names of the function and the argument explicitly. This gives rise to a subtle point, often glossed over in elementary treatments of functions: \"functions\" are distinct from their \"values\". Thus, a function should be distinguished from its value at the value in its domain. To some extent, even working mathematicians will conflate the two in informal settings for convenience, and to avoid the use of pedantic language. However, strictly speaking, it is an abuse of notation to write \"let formula_9 be the function \", since and should both be understood as the \"value\" of \"f\" at \"x\", rather than the function itself. Instead, it is correct, though pedantic, to write \"let formula_9 be the function defined by the equation valid for all real values of \". \n\nThis distinction in language and notation becomes important in cases where functions themselves serve as inputs for other functions. (A function taking another function as an input is termed a \"functional\".) Other approaches to denoting functions, detailed below, avoid this problem but are less commonly used.\n\nFirst used by Leonhard Euler in 1734, it is often useful to use a symbol for denoting a function. This symbol consists generally of a single letter in italic font, most often the lower-case letters . Some widely used functions are represented by a symbol consisting of several letters (usually two or three, generally an abbreviation of their name). By convention, the symbol for standard functions is set in roman type, such as \"\" for the sine function, in contrast to functions defined on an \"ad hoc\" basis.\n\nThe notation (read: \" equals of \")\nmeans that the pair belongs to the set of pairs defining the function . If is the domain of , the set of pairs defining the function is thus, using set-builder notation,\n\nOften, a definition of the function is given by what \"f\" does to the explicit argument \"x.\" For example, a function \"f\" can be defined by the equation\n\nfor all real numbers \"x.\" In this example, \"f\" can be thought of as the composite of several simpler functions: squaring, adding 1, and taking the sine. However, only the sine function has a common explicit symbol (sin), while the combination of squaring and then adding 1 is described by the polynomial expression formula_14. In order to explicitly reference functions such as squaring or adding 1 without introducing new function names (e.g., by defining function \"g\" and \"h\" by formula_15 and formula_16), one of the methods below (arrow notation or dot notation) could be used. \n\nSometimes the parentheses of functional notation are omitted when the symbol denoting the function consists of several characters and no ambiguity may arise. For example, formula_17 can be written instead of formula_18\n\nFor explicitly expressing domain and the codomain of a function , the arrow notation is often used (read: or ):\n\nor\n\nThis is often used in relation with the arrow notation for elements (read: \" maps to \"), often stacked immediately below the arrow notation giving the function symbol, domain, and codomain: \n\nFor example, if a multiplication is defined on a set , then the square function formula_22 on is unambiguously defined by (read: \"the function formula_22 from to that maps to \")\n\nthe latter line being more commonly written \n\nOften, the expression giving the function symbol, domain and codomain is omitted. Thus, the arrow notation is useful for avoiding introducing a symbol for a function that is defined, as it is often the case, by a formula expressing the value of the function in terms of its argument. As a common application of the arrow notation, suppose formula_26 is a two-argument function, and we want to refer to a partially applied function formula_27 produced by fixing the second argument to the value without introducing a new function name. The map in question could be denoted formula_28 using the arrow notation for elements. Note that the expression formula_28 (read: \"the map taking to formula_30\") represents this new function with just one argument, whereas the expression formula_31 refers to the value of the function at the \n\nIndex notation is often used instead of functional notation. That is, instead of writing , one writes formula_32 \n\nThis is typically the case for functions whose domain is the set of the natural numbers. Such a function is called a sequence, and, in this case the element formula_33 is called the th element of sequence.\n\nThe index notation is also often used for distinguishing some variables called parameters from the \"true variables\". In fact, parameters are specific variables that are considered as being fixed during the study of a problem. For example, the map formula_34 (see above) would be denoted formula_35 using index notation, if we define the collection of maps formula_35 by the formula formula_37 for all formula_38.\n\nIn the notation \nformula_39\nthe symbol does not represent any value, it is simply a placeholder meaning that, if is replaced by any value on the left of the arrow, it should be replaced by the same value on the right of the arrow. Therefore, may be replaced by any symbol, often an interpunct \"\". This may be useful for distinguishing the function from its value at . \n\nFor example, formula_40 may stand for the function formula_41, and formula_42 may stand for a function defined by an integral with variable upper bound: formula_43.\n\nThere are other, specialized notations for functions in sub-disciplines of mathematics. For example, in linear algebra and functional analysis, linear forms and the vectors they act upon are denoted using a dual pair to show the underlying duality. This is similar to the use of bra–ket notation in quantum mechanics. In logic and the theory of computation, the function notation of lambda calculus is used to explicitly express the basic notions of function abstraction and application. In category theory and homological algebra, networks of functions are described in terms of how they and their compositions commute with each other using commutative diagrams that extend and generalize the arrow notation for functions described above.\n\nAccording to the definition of a function, a specific function is, in general, defined by associating to every element of its domain one element of its codomain. When the domain and the codomain are sets of numbers, this association may take the form of a computation taking as input any element of the domain and producing an output in the codomain. This computation may be described by a formula. (This is the starting point of algebra, where many similar numerical computations can be replaced by a single formula that describes these computations by means of variables that represent computation inputs as unspecified numbers). This type of specification of a function frequently uses previously defined auxiliary functions. \n\nFor example, the function from the reals to the reals, defined by the formula \nformula_44\nemploys, as auxiliary functions, the square function (mapping all the reals to the non-negative reals), the square root function (mapping the non-negative reals to the non-negative reals), and the addition of real numbers. The whole set of real numbers may be taken as the domain of , even though the domain of the square root function is restricted to the non-negative real numbers; the image of consists of the reals that are not less than one.\n\nA computation that defines a function may often be described by an algorithm, and any kind of algorithm may be used. Sometimes, the definition of a function may involve elements or properties that can be defined, but not computed. For example, if one considers the set formula_45 of the programs in a given programming language that take an integer as input. The \"terminating function\" is the function that returns 1 if a program of formula_45 runs forever when executed on a given integer input, and returns 0 otherwise. It is a basic theorem of computability theory that there does not exist an algorithm for computing this function. More generally, computability theory is the study of computable functions, that is, the functions that can be computed by an algorithm.\n\nThe above ways of defining functions define them \"pointwise\", that is, each value is defined independently of the other values. This is not necessarily the case.\n\nWhen the domain of a function is the set of nonnegative integers or, more generally, when the domain is a well ordered set, a function may be defined by induction or recursion, meaning (roughly) that the calculation of the value of the function for some given input requires values of the function for \"lesser\" inputs. For example, the Fibonacci sequence is a function from the natural numbers into themselves that is defined by two starting values and a formula, recurring to the two immediately preceding arguments (see above for the use of indices for the argument of a function): \n\nIn calculus, the usual functions considered have extensive regularities. That is, the value of the function at a point is related to the values of the function at neighboring points. This allows defining them by functional equations (for example, the gamma function is the unique meromorphic function such that formula_48, and formula_49 for any complex that is not a non-positive integer), by differential equations (for example, the natural logarithm is the solution of the differential equation formula_50 such that ), by integral equations or by analytic continuation.\n\nAs functions may be complicated objects, it is often useful to draw the graph of a function for getting a global view of its properties. Some functions may also represented histograms\n\nGiven a function formula_51 its \"graph\" is, formally, the set \n\nIn the frequent case where and are subsets of the real numbers (or may be identified to such subsets), an element formula_53 may be identified with the point of coordinates in the Cartesian plane. Marking these points provides a drawing, generally a curve, that is also called the \"graph of the function\". For example the graph of the square function\n\nis a parabola that consists of all points of coordinates formula_55 for formula_56\n\nIt is possible to draw effectively the graph of a function only if the function is sufficiently regular, that is, either if the function is differentiable (or piecewise differentiable) or if its domain may be identified with the integers or a subset of the integers.\n\nIf either the domain or the codomain of the function is a subset of formula_57 the graph is a subset of a Cartesian space of higher dimension, and various technics have been developed for drawing it, including the use of colors for representing one of the dimensions.\n\nHistograms are often used for representing functions whose domain is finite, or is the natural numbers or the integers. In this case, an element of the domain is represented by an interval of the -axis, and a point of the graph is represented by a rectangle with basis the interval corresponding to and height .\n\nIn statistic, histogram are often used for representing very irregular functions. For example, for representing the function that associates his weight to each member of some population, one draws the histogram of the function that associates to each weight interval the number of people, whose weights belong to this interval. \n\nThere are many variants of this method, see Histogram for details.\n\nThis section describes general properties of functions, that are independent of specific properties of the domain and the codomain.\n\nSome functions are uniquely defined by their domain and codomain, and are sometimes called \"canonical\": \n\n\nTwo functions \"f\" and \"g\" are equal if their domain and codomain sets agree and their output values agree on the whole domain. Formally, \"f\"=\"g\" if \"f\"(\"x\")=\"g\"(\"x\") for all \"x\"∈\"X\", where \"f\":\"X\"→\"Y\" and \"g\":\"X\"→\"Y\".\n\nGiven two functions formula_61 and formula_62 such that the domain of is the codomain of , their \"composition\" is the function formula_63 defined by\n\nThat is, the value of formula_65 is obtained by first applying to to obtain and then applying to the result to obtain . In the notation the function that is applied first is always written on the right.\n\nThe composition formula_66 is an operation on functions that is defined only if the codomain of the first function is the domain of the second one. Even when both formula_65 and formula_68 satisfy these conditions, the composition is not necessarily commutative, that is, the functions formula_65 and formula_70 need not be equal, but may deliver different values for the same argument. For example, let and , then formula_71 and formula_72 agree just for formula_73\n\nThe function composition is associative in the sense that, if one of formula_74 and formula_75 is defined, then the other is also defined, and they are equal. Thus, one writes \n\nThe identity functions formula_60 and formula_78 are respectively a right identity and a left identity for functions from to . That is, if is a function with domain , and codomain , one has \nformula_79\n\nLet formula_80 The \"image\" by of an element of the domain is . If is any subset of , then the \"image\" of by , denoted is the subset of the codomain consisting of all images of elements of , that is,\n\nThe \"image\" of is the image of the whole domain, that is . It is also called the range of , although the term may also refer to the codomain.\n\nOn the other hand, the \"inverse image\", or \"preimage\" by of a subset of the codomain is the subset of the domain consisting of all elements of whose images belong to . It is denoted by formula_82 That is \nFor example, the preimage of {4, 9} under the square function is the set {−3,−2,2,3}. \n\nBy definition of a function, the image of an element of the domain is always a single element of the codomain. However, the preimage of a single element , denoted formula_84 may be empty or contain any number of elements. For example, if is the function from the integers to themselves that map every integer to 0, then . \n\nIf formula_61 is a function, and are subsets of , and and are subsets of , then one has the following properties:\n\nThe preimage by of an element of the codomain is sometimes called, in some contexts, the fiber of under . \n\nIf a function has an inverse (see below), this inverse is denoted formula_92 In this case formula_93 may denote either the image by formula_94 or the preimage by of . This is not a problem, as these sets are equal. The notation formula_95 and formula_93 may be ambiguous in the case of sets that contain some subsets as elements, such as formula_97 In this case, some care may be needed, for example, by using square brackets formula_98 for images and preimages of subsets, and ordinary parentheses for images and preimages of elements.\n\nLet formula_61 be a function.\n\nThe function is \"injective\" (or \"one-to-one\", or is an \"injection\") if for any two different elements and of . Equivalently, is injective if, for any formula_100 the preimage formula_101 contains at most one element. An empty function is always injective. If is not the empty set, and if, as usual, the axiom of choice is assumed, then is injective if and only if there exists a function formula_102 such that formula_103 that is, if has a left inverse. The axiom of choice is needed, because, if is injective, one defines by formula_104 if formula_105 and by formula_106, if formula_107 where formula_108 is an \"arbitrarily chosen\" element of .\n\nThe function is \"surjective\" (or \"onto\", or is a \"surjection\") if the range equals the codomain, that is, if . In other words, the preimage formula_101 of every formula_110 is nonempty. If, as usual, the axiom of choice is assumed, then is surjective if and only if there exists a function formula_102 such that formula_112 that is, if has a right inverse. The axiom of choice is needed, because, if is injective, one defines by formula_113 where formula_114 is an \"arbitrarily chosen\" element of formula_115\n\nThe function is \"bijective\" (or is \"bijection\" or a \"one-to-one correspondence\") if it is both injective and surjective. That is is bijective if, for any formula_100 the preimage formula_101 contains exactly one element. The function is bijective if and only if it admits an inverse function, that is a function formula_102 such that formula_103 and formula_120 (Contrarily to the case of injections and surjections, this does not require the axiom of choice.)\n\nEvery function formula_61 may be factorized as the composition of a surjection followed by an injection, where is the canonical surjection of onto , and is the canonical injection of into . This is the \"canonical factorization\" of .\n\n\"One-to-one\" and \"onto\" are terms that were more common in the older English language literature; \"injective\", \"surjective\", and \"bijective\" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English. As a word of caution, \"a one-to-one function\" is one that is injective, while a \"one-to-one correspondence\" refers to a bijective function. Also, the statement \" maps \"onto\" \" differs from \" maps \"into\" \" in that the former implies that is surjective), while the latter makes no assertion about the nature of the mapping. In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage to be more symmetrical.\n\nIf formula_61 is a function, and is a subset of , then the \"restriction\" of to , denoted , is the function from to that is defined by \n\nThis often used for define partial inverse functions: if there is a subset of a function such that is injective, then the canonical surjection of on its image is a bijection, which has an inverse function from to . This is in this way that inverse trigonometric functions are defined. The cosine function, for example, is injective, when restricted to the interval ; the image of this restriction is the interval ; this defines thus an inverse function from to , which is called arccosine and denoted .\n\nFunction restriction may also be used for \"gluing\" functions together: let formula_124 be the decomposition of as a union of subsets. Suppose that a function formula_125 is defined on each formula_126 such that, for each pair of indices, the restrictions of formula_127 and formula_128 to formula_129 are equal. Then, this defines a unique function formula_61 such that formula_131 for every . This is generally in this way that functions on manifolds are defined.\n\nAn \"extension\" of a function is a function such that is a restriction of . A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.\n\nHere is another classical example of a function extension that is encountered when studying homographies of the real line. An \"homography\" is a function formula_132 such that . Its domain is the set of all real numbers different from formula_133 and its image is the set of all real numbers different from formula_134 If one extends the real line to the projectively extended real line by adding to the real numbers, one may extend for being a bijection of the extended real line to itself, by setting formula_135 and formula_136\n\nA multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time and its speed.\n\nMore formally, a function of variables is a function whose domain is a set of -tuples.\nFor example, multiplication of integers is a function of two variables, or bivariate function, whose domain is the set of all pairs (2-tuples) of integers, and whose codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.\n\nThe Cartesian product formula_137 of sets formula_138 is the set of all -tuples formula_139 such that formula_140 for every with formula_141. Therefore, a function of variables is a function\nwhere the domain has the form\nWhen using function notation, one usually omits the parentheses surrounding tuples, writing formula_144 instead of formula_145\n\nIn the case where all the formula_146 are equal to the set formula_147 of real numbers, one has a function of several real variables. If the formula_146 are equal to the set formula_149 of complex numbers, one has a function of several complex variables.\n\nIt is common to also consider functions whose codomain is a product of sets. For example, Euclidean division maps every pair of integers with to a pair of integers called the \"quotient\" and the \"remainder\":\nThe codomain may also be a vector space. In this case, one talks of a vector-valued function. If the domain is contained in a Euclidean space, or more generally a manifold, a vector-valued function is often called a vector field.\n\nThe idea of function, starting in the 17th century, was fundamental to the new infinitesimal calculus (see History of the function concept). At that time, only real-valued functions of a real variable were considered, and all functions were assumed to be smooth. But the definition was soon extended to functions of several variables and to function of a complex variable. In the second half of 19th century, the mathematically rigorous definition of a function was introduced, and functions with arbitrary domains and codomains were defined. \n\nFunctions are now used throughout all areas of mathematics. In introductory calculus, when the word \"function\" is used without qualification, it means a real-valued function of a single real variable. The more general definition of a function is usually introduced to second or third year college students with STEM majors, and in their senior year they are introduced to calculus in a larger, more rigorous setting in courses such as real analysis and complex analysis.\n\nA \"real function\" is a real-valued function of a real variable, that is, a function whose codomain is the field of real numbers and whose domain is a set of real numbers that contains an interval. In this section, these functions are simply called \"functions\".\n\nThe functions that are most commonly considered in mathematics and its applications have some regularity, that is they are continuous, differentiable, and even analytic. This regularity insures that these functions can be visualized by their graphs. In this section, all functions are differentiable in some interval.\n\nFunctions enjoy pointwise operations, that is, if and are functions, their sum, difference and product are functions defined by \nThe domains of the resulting functions are the intersection of the domains of and . The quotient of two functions is defined similarly by \nbut the domain of the resulting function is obtained by removing the zeros of from the intersection of the domains of and .\n\nThe polynomial functions are defined by polynomials, and their domain is the whole set of real numbers. They include constant functions, linear functions and quadratic functions. Rational functions are quotients of two polynomial functions, and their domain is the real numbers with a finite number of them removed to avoid division by zero. The simplest rational function is the function formula_153 whose graph is an hyperbola, and whose domain is the whole real line except for 0.\n\nThe derivative of a real differentiable function is a real function. An antiderivative of a continuous real function is a real function that is differentiable in any open interval in which the original function is continuous. For example, the function formula_154 is continuous, and even differentiable, on the positive real numbers. Thus one antiderivative, which takes the value zero for , is a differentiable function called the natural logarithm.\n\nA real function is monotonic in an interval if the sign of formula_155 does not depend of the choice of and in the interval. If the function is differentiable in the interval, it is monotonic if the sign of the derivative is constant in the interval. If a real function is monotonic in an interval , it has an inverse function, which is a real function with domain and image . This is how inverse trigonometric functions are defined in terms of trigonometric functions, where the trigonometric functions are monotonic. Another example: the natural logarithm is monotonic on the positive real numbers, and its image is the whole real line; therefore it has an inverse function that is a bijection between the real numbers and the positive real numbers. This inverse is the exponential function.\n\nMany other real functions are defined either by the implicit function theorem (the inverse function is a particular instance) or as solutions of differential equations. For example the sine and the cosine functions are the solutions of the linear differential equation \nsuch that \n\nWhen working with complex numbers different types of functions are used: \n\nThe study of complex functions is a vast subject in mathematics with many applications, and that can claim to be an ancestor to many other areas of mathematics, like homotopy theory, and manifolds.\n\nIn mathematical analysis, and more specifically in functional analysis, a function space is a set of scalar-valued or vector-valued functions, which share a specific property and form a topological vector space. For example, the real smooth functions with a compact support (that is, they are zero outside some compact set) form a function space that is at the basis of the theory of distributions.\n\nFunction spaces play a fundamental role in advanced mathematical analysis, by allowing the use of their algebraic and topological properties for studying properties of functions. For example, all theorems of existence and uniqueness of solutions of ordinary or partial differential equations result of the study of function spaces.\n\nIt is rather frequent that a function with domain may be naturally extended to a function whose domain is a set that is built from .\n\nFor example, for any set , its power set is the set of all subsets of . Any function formula_51 may be extended to a function on power sets by \nwhere is the image by of the subset of .\n\nAccording to the definition, a function maps each element from its domain to some element of its codomain . It is often convenient to extend this meaning to apply to arbitrary subsets of the domain, which are, as immediately can be checked, mapped to subsets of the codomain, thus considering a function formula_164 mapping its domain, the powerset () of 's domain , to its codomain, a subset of the powerset () of 's codomain .\nUnder slight abuse of notation this function on subsets is often denoted also by .\n\nAnother example is the following. If the function\nformula_166\nis a ring homomorphism, it may be extended to a function on polynomial rings\nwhich is also a ring homomorphism.\n\nSeveral methods for specifying functions of real or complex variables start from a local definition of the funcion at a point or on a neighbourhood of a point, and then extend by continuity the function to a much larger domain. Frequently, for a starting point formula_168 there are several possible starting values for the function. \n\nFor example, in defining the square root as the inverse function of the square function, for any positive real number formula_168 there are two choices for the value of the square root, one of which is positive and denoted formula_170 and another which is negative and denoted formula_171 These choices define two continuous functions, both having the nonnegative real numbers as a domain, and having either the nonnegative or the nonpositive real numbers as images. When looking at the graphs of these functions, one can see that, together, they form a single smooth curve. It is therefore often useful to consider these two square root functions as a single function that has two values for positive , one value for 0 and no value for negative .\n\nIn the preceding example, one choice, the positive square root, is more natural than the other. This is not the case in general. For example, let consider the implicit function that maps to a root of formula_172 (see the figure on the right). For one may choose either formula_173 for By the implicit function theorem, each choice defines a function; for the first one, the (maximal) domain is the interval and the image is ; for the second one, the domain is and the image is ; for the last one, the domain is and the image is . As the three graphs together form a smooth curve, and there is no reason for preferring one choice, these three functions are often considered as a single \"multi-valued function\" of that has three values for , and only one value for and .\n\nUsefulness of the concept of multi-valued functions is clearer when considering complex functions, typically analytic functions. The domain to which a complex function may be extended by analytic continuation generally consists of almost the whole complex plane. However, when extending the domain through two different paths, one often gets different values. For example, when extending the domain of the square root function, along a path of complex numbers with positive imaginary parts, one gets for the square root of –1; while, when extending through complex numbers with negative imaginary parts, one gets . There are generally two ways of solving the problem. One may define a function that is not continuous along some curve, called a branch cut. Such a function is called the principal value of the function. The other way is to consider that one has a \"multi-valued function\", which is analytic everywhere except for isolated singularities, but whose value may \"jump\" if one follows a closed loop around a singularity. This jump is called the monodromy.\n\nThe definition of a function that is given in this article requires the concept of set, since the domain and the codomain of a function must be a set. This is not a problem in usual mathematics, as it is generally not difficult to consider only functions whose domain and codomain are sets, which are well defined, even if the domain is not explicitly defined. However, it is sometimes useful to consider more general functions. \n\nFor example the singleton set may be considered as a function formula_174 Its domain would include all sets, and therefore would not be a set. In usual mathematics, one avoids this kind of problem by specifying a domain, which means that one has many singleton functions. However, when establishing foundations of mathematics, one may have to use functions whose domain, codomain or both are not specified, and some authors, often logicians, give precise definition for these weakly specified functions.\n\nThese generalized functions may be critical in the development of a formalization of foundations of mathematics. For example, the Von Neumann–Bernays–Gödel set theory, is an extension of the set theory in which the collection of all sets is a class. This theory includes the replacement axiom, which may be interpreted as \"if is a set, and is a function, then is a set\".\n\n\n\n"}
{"id": "342058", "url": "https://en.wikipedia.org/wiki?curid=342058", "title": "Gimbal lock", "text": "Gimbal lock\n\nGimbal lock is the loss of one degree of freedom in a three-dimensional, three-gimbal mechanism that occurs when the axes of two of the three gimbals are driven into a parallel configuration, \"locking\" the system into rotation in a degenerate two-dimensional space.\n\nThe word \"lock\" is misleading: no gimbal is restrained. All three gimbals can still rotate freely about their respective axes of suspension. Nevertheless, because of the parallel orientation of two of the gimbals' axes there is no gimbal available to accommodate rotation about one axis.\n\nA gimbal is a ring that is suspended so it can rotate about an axis. Gimbals are typically nested one within another to accommodate rotation about multiple axes.\n\nThey appear in gyroscopes and in inertial measurement units to allow the inner gimbal's orientation to remain fixed while the outer gimbal suspension assumes any orientation. In compasses and flywheel energy storage mechanisms they allow objects to remain upright. They are used to orient thrusters on rockets.\n\nSome coordinate systems in mathematics behave as if there were real gimbals used to measure the angles, notably Euler angles.\n\nFor cases of three or fewer nested gimbals, gimbal lock inevitably occurs at some point in the system due to properties of covering spaces (described below).\n\nWhile only two specific orientations produce exact gimbal lock, practical mechanical gimbals encounter difficulties near those orientations. When a set of gimbals is close to the locked configuration, small rotations of the gimbal platform require large motions of the surrounding gimbals. Although the ratio is infinite only at the point of gimbal lock, the practical speed and acceleration limits of the gimbals—due to inertia (resulting from the mass of each gimbal ring), bearing friction, the flow resistance of air or other fluid surrounding the gimbals (if they are not in a vacuum), and other physical and engineering factors—limit the motion of the platform close to that point.\n\nGimbal lock can occur in gimbal systems with two degrees of freedom such as a theodolite with rotations about an azimuth and elevation in two dimensions. These systems can gimbal lock at zenith and nadir, because at those points azimuth is not well-defined, and rotation in the azimuth direction does not change the direction the theodolite is pointing.\n\nConsider tracking a helicopter flying towards the theodolite from the horizon. The theodolite is a telescope mounted on a tripod so that it can move in azimuth and elevation to track the helicopter. The helicopter flies towards the theodolite and is tracked by the telescope in elevation and azimuth. The helicopter flies immediately above the tripod (i.e. it is at zenith) when it changes direction and flies at 90 degrees to its previous course. The telescope cannot track this maneuver without a discontinuous jump in one or both of the gimbal orientations. There is no continuous motion that allows it to follow the target. It is in gimbal lock. So there is an infinity of directions around zenith for which the telescope cannot continuously track all movements of a target. Note that even if the helicopter does not pass through zenith, but only \"near\" zenith, so that gimbal lock does not occur, the system must still move exceptionally rapidly to track it, as it rapidly passes from one bearing to the other. The closer to zenith the nearest point is, the faster this must be done, and if it actually goes through zenith, the limit of these \"increasingly rapid\" movements becomes \"infinitely\" fast, namely discontinuous.\n\nTo recover from gimbal lock the user has to go around the zenith – explicitly: reduce the elevation, change the azimuth to match the azimuth of the target, then change the elevation to match the target.\n\nMathematically, this corresponds to the fact that spherical coordinates do not define a coordinate chart on the sphere at zenith and nadir. Alternatively, the corresponding map \"T\"→\"S\" from the torus \"T\" to the sphere \"S\" (given by the point with given azimuth and elevation) is not a covering map at these points.\n\nConsider a case of a level-sensing platform on an aircraft flying due north with its three gimbal axes mutually perpendicular (i.e., roll, pitch and yaw angles each zero). If the aircraft pitches up 90 degrees, the aircraft and platform's yaw axis gimbal becomes parallel to the roll axis gimbal, and changes about yaw can no longer be compensated for.\n\nThis problem may be overcome by use of a fourth gimbal, intelligently driven by a motor so as to maintain a large angle between roll and yaw gimbal axes. Another solution is to rotate one or more of the gimbals to an arbitrary position when gimbal lock is detected and thus reset the device.\n\nModern practice is to avoid the use of gimbals entirely. In the context of inertial navigation systems, that can be done by mounting the inertial sensors directly to the body of the vehicle (this is called a strapdown system) and integrating sensed rotation and acceleration digitally using quaternion methods to derive vehicle orientation and velocity. Another way to replace gimbals is to use fluid bearings or a flotation chamber.\n\nA well-known gimbal lock incident happened in the Apollo 11 Moon mission. On this spacecraft, a set of gimbals was used on an inertial measurement unit (IMU). The engineers were aware of the gimbal lock problem but had declined to use a fourth gimbal. Some of the reasoning behind this decision is apparent from the following quote:\n\nThey preferred an alternate solution using an indicator that would be triggered when near to 85 degrees pitch.\n\nRather than try to drive the gimbals faster than they could go, the system simply gave up and froze the platform. From this point, the spacecraft would have to be manually moved away from the gimbal lock position, and the platform would have to be manually realigned using the stars as a reference.\n\nAfter the Lunar Module had landed, Mike Collins aboard the Command Module joked \"How about sending me a fourth gimbal for Christmas?\"\n\nIn robotics, gimbal lock is commonly referred to as \"wrist flip\", due to the use of a \"triple-roll wrist\" in robotic arms, where three axes of the wrist, controlling yaw, pitch, and roll, all pass through a common point.\n\nAn example of a wrist flip, also called a wrist singularity, is when the path through which the robot is traveling causes the first and third axes of the robot's wrist to line up. The second wrist axis then attempts to spin 180° in zero time to maintain the orientation of the end effector. The result of a singularity can be quite dramatic and can have adverse effects on the robot arm, the end effector, and the process.\n\nThe importance of avoiding singularities in robotics has led the American National Standard for Industrial Robots and Robot Systems — Safety Requirements to define it as \"a condition caused by the collinear alignment of two or more robot axes resulting in unpredictable robot motion and velocities\".\n\nThe problem of gimbal lock appears when one uses Euler angles in applied mathematics; developers of 3D computer programs, such as 3D modeling, embedded navigation systems, and video games must take care to avoid it.\n\nIn formal language, gimbal lock occurs because the map from Euler angles to rotations (topologically, from the 3-torus \"T\" to the real projective space RP which is the same as the space of 3d rotations SO3) is not a covering map – it is not a local homeomorphism at every point, and thus at some points the rank (degrees of freedom) must drop below 3, at which point gimbal lock occurs. Euler angles provide a means for giving a numerical description of any rotation in three-dimensional space using three numbers, but not only is this description not unique, but there are some points where not every change in the target space (rotations) can be realized by a change in the source space (Euler angles). This is a topological constraint – there is no covering map from the 3-torus to the 3-dimensional real projective space; the only (non-trivial) covering map is from the 3-sphere, as in the use of quaternions.\n\nTo make a comparison, all the translations can be described using three numbers formula_1, formula_2, and formula_3, as the succession of three consecutive linear movements along three perpendicular axes formula_4, formula_5 and formula_6 axes. The same holds true for rotations: all the rotations can be described using three numbers formula_7, formula_8, and formula_9, as the succession of three rotational movements around three axes that are perpendicular one to the next. This similarity between linear coordinates and angular coordinates makes Euler angles very intuitive, but unfortunately they suffer from the gimbal lock problem.\n\nA rotation in 3D space can be represented numerically with matrices in several ways. One of these representations is:\n\nAn example worth examining happens when formula_11. Knowing that formula_12 and formula_13, the above expression becomes equal to:\n\nCarrying out matrix multiplication:\n\nAnd finally using the trigonometry formulas:\n\nChanging the values of formula_7 and formula_9 in the above matrix has the same effects: the rotation angle formula_19 changes, but the rotation axis remains in the formula_6 direction: the last column and the first row in the matrix won't change. The only solution for formula_7 and formula_9 to recover different roles is to change formula_8.\n\nIt is possible to imagine an airplane rotated by the above-mentioned Euler angles using the X-Y-Z convention. In this case, the first angle - formula_7 is the pitch. Yaw is then set to formula_25 and the final rotation - by formula_9 - is again the airplane's pitch. Because of gimbal lock, it has lost one of the degrees of freedom - in this case the ability to roll.\n\nIt is also possible to choose another convention for representing a rotation with a matrix using Euler angles than the X-Y-Z convention above, and also choose other variation intervals for the angles, but in the end there is always at least one value for which a degree of freedom is lost.\n\nThe gimbal lock problem does not make Euler angles \"invalid\" (they always serve as a well-defined coordinate system), but it makes them unsuited for some practical applications.\n\nThe cause of gimbal lock is representing an orientation as three axial rotations with Euler angles. A potential solution therefore is to represent the orientation in some other way. This could be as a rotation matrix, a quaternion (see quaternions and spatial rotation), or a similar orientation representation that treats the orientation as a value rather than three separate and related values. Given such a representation, the user stores the orientation as a value. To apply angular changes, the orientation is modified by a delta angle/axis rotation. The resulting orientation must be re-normalized to prevent floating-point error from successive transformations from accumulating. For matrices, re-normalizing the result requires converting the matrix into its nearest orthonormal representation. For quaternions, re-normalization requires performing quaternion normalization.\n\n"}
{"id": "20533589", "url": "https://en.wikipedia.org/wiki?curid=20533589", "title": "Implicit attitude", "text": "Implicit attitude\n\nImplicit attitudes are evaluations that occur without conscious awareness towards an attitude object or the self. These evaluations are generally either favorable or unfavorable. They come about from various influences in the individual experience. The commonly used definition of implicit attitude within cognitive and social psychology comes from Anthony Greenwald and Mahzarin Banaji's template for definitions of terms related to implicit cognition (see also implicit stereotype and implicit self-esteem for usage of this template): \"Implicit attitudes are introspectively unidentified (or inaccurately identified) traces of past experience that mediate favorable or unfavorable feeling, thought, or action toward social objects\". These thoughts, feelings or actions have an influence on behavior that the individual may not be aware of.\n\nAn attitude is differentiated from the concept of a stereotype in that it functions as a broad favorable or unfavorable characteristic towards a social object, whereas a stereotype is a set of favorable and/or unfavorable characteristics which are applied to an individual based on social group membership.\n\nA number of different theories have been proposed relating to the formation, development, and influence of implicit attitudes.\n\nBased on many empirical findings, Greenwald and Banaji et al. (1995) generated the fundamental idea of implicit attitude definitively for the first time, disambiguating attitude into explicit and implicit types. Halo effects are an example of the empirical research used by Greenwald and Banaji in their chapter on implicit social cognition. Understanding halo effects set the foundation for understanding other theories regarding implicit attitudes. For example, it is possible to explain implicit partisanship or implicit egotism in terms of a halo effect, however these concepts will discussed more in subsequent sections.\n\nPioneered by Edward Thorndike in 1920, the halo effect is the judgement of attribute \"A\" being influenced by a known but irrelevant attribute \"B\". For example, subsequent replications commonly use physical attractiveness as attribute \"B\" and attribute \"A\" being a judgement of the subject. More specifically a study Landy and Sigall et al. (1974) found that essays written by female essayists were found to be of higher quality when a photo showed the essayist as being attractive (rather than unattractive) when rated by male judges.\n\nGreenwald and Banaji et al. (1995) have suggested that attribute \"B\" is in fact an implicit attitude when the judge or subject cannot identify attribute \"B\" as the source of the judgement for attribute \"A\". Moreover, when attribute \"B\" is associated with a positive or negative attitude and additionally is unknowingly and automatically transferred onto attribute \"A\", that attitude of attribute \"B\" is known to be an implicit attitude.\n\nEarlier research findings on implicit attitudes show that socialization and reflections of past experiences may be responsible for the development or manifestation of longer lasting implicit attitudes. As an example, Rudman and Goodwin et al. (2004) found that individuals who were primarily raised by their mothers showed a more positive implicit attitude towards women rather than men. Furthermore, Olson and Fazio et al. (2001, 2002) suggest that these implicit attitudes are a result of repeated pairings of positive or negative stimuli with an object; more pairings of positive stimuli would result in a more positive implicit attitude and vice versa. This finding supports the fundamental principals of classical conditioning.\n\nImplicit attitudes are also developed by more recent experiences as well. For example, Rudmore, Ashmore, & Gary et al. (2001) found that implicit attitude of prejudice against African Americans could be shaped through diversity training intervention using variables at an emotional level rather than increased awareness of bias which helped explicit attitude more.\n\nSelf-related objects are anything that pertains to the self; including in-groups and self-esteem (attitude towards the self).\n\nEarly research by Nuttin et al. (1985) suggested that people generally have an implicit preference for letters in their own name, known as the Name letter effect. Further replications of this same effect with varying independent variables (e.g., attractiveness to people with the same letters contained in their names) suggest that people have an implicit preference towards themselves. This manifestation of implicit attitude has come to be known as Implicit egotism. Implicit egotism additionally manifests itself in in-groups.\n\n\"Implicit partisanship \"is the heightened attractiveness and identification to a self-related group and negative or neutral attitudes towards non-self-related groups. Greenwald, Pickrell, and Farnham et al. (2002) demonstrated this effect even when the groups were cooperative and when the members of the groups were non-human. Much of the research on implicit partisanship suggests that this is an uncontrollable process, or an implicit attitude towards self-related groups.\n\nGenerally speaking, culture and social norms have an effect on implicit attitude in the same way experiences and socialization have an effect on implicit attitude. However, culture has a very noticeable effect on implicit attitude in the way implicit attitude differs from one's explicit attitude. Livingston et al. (2002) examined the effect of mainstream culture on one's implicit attitude towards their social group. Implicitly, one will follow the cultural attitudes towards their social group that they perceive from mainstream culture in their society whether that be positive or negative. With that said, a strong cultural disadvantage (e.g., negative attitude) can effectively eliminate in-group favoritism when tested at the implicit level. However it may be important to note that at the explicit attitude level, these individuals still showed positive attitudes towards their social group. Olson and Fazio et al. (2004) have suggested that at an implicit level one's personal attitude can be influenced by the social or cultural norms that one perceives. Furthermore, this may be due to a weak distinction between one's personal attitude and extrapersonal associations (e.g., one's cultural evaluations) towards an attitude object at the implicit level. From this we can conclude that implicit attitudes are indeed reflective of experiences but can also be shaped by the cultural context.\n\nCurrent research supports the idea that there are three different aspects of attitudes captured by current indirect measures that could be outside of conscious awareness: the source, the content, and the impact of an attitude. Source awareness is described as the “awareness of the origin of a particular attitude” (emphasis added). Content awareness is differentiated from source awareness by the lack of awareness about the attitude, rather than simply its origin. Finally, one may have awareness of both the attitude and its source but the attitude may still have influences on thought or behavior beyond ones awareness; this can be thought of as impact awareness. Conclusions have been made that both indirectly assessed and self-reported attitudes can be characterized by lack of source awareness, there is no evidence for lack of content awareness of indirectly assessed attitudes, and there is some evidence showing that indirectly assessed, but not self-reported, attitudes can be characterized by lack of impact awareness. The most compelling evidence for content awareness of implicit attitudes has showed that people are highly accurate in predicting their scores on the Implicit Association Test.\n\nRecent research indicates the possibility of the malleability of implicit attitudes based on situational context. That is, implicit attitudes are not believed to be stable representations of memory, rather they are constructed based on the type of available information in a given situation. Available information can vary in context to the individual, though it is believed to serve as a prime to their behaviors. Flexibility of implicit attitudes is best demonstrated through measures that include accessibility effects. For example, it has been demonstrated that the information given to an individual prior to completing an implicit measure directly effects their response based on the information they were given. Therefore, if an individual is primed with information regarding the positive, or negative, attributions of a different race and then asked to complete an implicit measures task, the participants will most likely use the information that was presented during the prime and not their own experienced information to assess the situation. This occurs because the information that was primed is most available for the participant to access without having to use conscious resources.\n\nThe fundamental goal of measuring implicit attitudes is to use it to predict behavior; behaviors that can't be predicted by knowledge of explicitly held attitudes. Numerous studies, such as research conducted by Chen and Bargh (1999) show that automatic evaluations triggered by various attitudes towards objects directly affected behavioral predispositions towards that object. Stimuli that elicited positive attitudes produced immediate positive behavior whilst stimuli that elicited negative attitudes triggered immediate avoidance behavior. The individuals are completely unaware of the operations that their behavioral responses because they are automatic and unconscious. In Bassenoff and Sherman et al. (2000) they found that automatic negative attitudes about overweight individuals directly predicted how far participants choose to sit from a fat woman, who they were expected to interact with. We see this phenomenon also with implicitly held racial attitudes as shown by McConnell and Leibold et al. (2001). These implicit attitudes affected how long they interacted for, how much participants smiled, how many speech errors they made and how many social comments were made. All automatic behavioral responses that measuring explicit attitudes could not predict.\n\nImplicit attitudes aren't always better at predicting behavior than explicit measurements, they both play a systematic role in predicting behavior. Implicit attitudes are typically better than explicit attitudes at predicting behavior that is automatic and spontaneous. In line with Dual process theories such as Fazio's MODE model, automatic attitudes determine spontaneous actions, whereas deliberative actions reflect a contribution of multiple processes, including more controlled processes (e.g., a person's motivation to overcome prejudiced responses). As demonstrated by Dasgupta an Rivera et al. (2006), individuals who endorsed traditional beliefs about gender and sexuality were friendlier towards gay confederates verbally but showed negative non-verbal behavior, this suggested that this individuals were consciously over-correcting their behavior but their prejudice leaked out through automatic responses like blinking and eye contact.\n\nAlthough, research has shown that motivation and an opportunity to react carefully can affect how much implicit attitudes influence behavioral response, Fazio (2001). When individuals are highly motivated to control their responses and processing abilities are not lacking or preoccupied, behavioral responses tend to reflect intentional processes. In Towles-schwen and Fazio (2003) they measured anticipated willingness and discomfort of participants to interact with a black person. Individuals who were motivated to avoid interracial conflicts and where not concerned about seeming biased expressed their discomfort; whereas individuals who were concerned about not appearing biased reported less anticipated discomfort, in an attempt to hide their prejudice. Motivation to control our responses can minimize the influence of implicit attitudes on behavioral responses as shown by that example.\n\nThere is an assortment of different experimental tests that assess for the presence of implicit attitudes, including the implicit association test, evaluative and semantic priming tasks, the Extrinsic Affective Simon Task, Go/No-Go Association Task, and the Affect Misattribution Procedure. Though these tests vary in administration, and content, the basis of each is to \"allow investigators to capture attitudes that individuals are unwilling to report.\" Unwillingness and lack of ability are intertwined considering most individuals are unaware that these attitudes even exist. The following are brief descriptions about these measurements, which are most commonly used to assess implicit attitudes, and the empirical evidence that supports them.\n\nThe Implicit Association Test is a latency-based measure of the relative associations between two concepts. In a series of tasks, participants sort words or images representing a target concept such as race (white/black) and stimuli with known positive/negative valence into two categories (usually indicated by right or left location on a computer screen). Each category of concept words or images is paired with both positive and negative stimuli. The faster the categorization occurs, the stronger the association is between words and/or images that are grouped together (ex. faster categorization of dogs when paired with positive rather than negative words), which would indicate and implicit attitude towards that object. A full demonstration of the IAT procedure can be found at the Project Implicit link and the IAT Inquisit link below.\n\nResearch using the IAT measure of implicit attitudes has demonstrated consistent experimental and population-based attitudes with respect to concepts such as gender, race, and age. A recent analysis from the Project Implicit database found that science-gender stereotypes are predictive of differences in gender related math and science performance across countries in an international sample. Research has also successfully used the IAT in consumer research.\n\nResearch using the evaluative priming task has been frequently used in research on eating and attitudes towards food. In clinical studies, the procedure was used to study attitudes of those diagnosed with eating disorders such as anorexia nervosa. Along with many of the other methods presented here, researchers have used the procedure to measure the effects of stereotypes, including measurement of the effectiveness of stereotype reduction treatments.\n\nIn the semantic priming task paradigm described by Wittenbrink et al. (1997), participants are shown a word prime at intervals which are too brief for reported awareness (see subliminal stimuli). The word prime consists of two groups of words representing the concept in question (such as black sounding names or white sounding names). Participants were then asked to complete a lexical decision task (LDT) to identify if target stimuli are words or a non-words. The target stimuli consist of words with known positive or negative valence. When words with positive valence are categorized more quickly in the presence of one group of word primes (such as black sounding names), this indicates positive attitudes towards the group.\n\nIn the Extrinsic Affective Simon Task (EAST), participants categorized stimuli which consisted of words that either had positive or negative valence that were presented in either the color white or two different colors. When the words are presented in white, participants categorize based words on their perceived positive or negative valence. When the words are presented in color, participants are asked to categorize based on color alone and ignore word meaning. When colored words are presented, categorization accuracy and speed are facilitated when, for words which the respondent has a positive implicit attitude, the response was the same as was expected for white words with obvious positive valence. A full demonstration of the EAST procedure can be found in the external links below.\n\nThe EAST has been used in research of attitudes of those who have specific phobias and/or anxiety. Additionally, the test has been recently used to measure implicit attitudes towards alcohol in populations who have substance abuse problems; and the test has been cited as having relatively high predictive value for problem substance use.\n\nIn practice, the GNAT appears similar to the Implicit Association Test in that participants are asked to categorize targets representing either a concept (such as race; ex. white or black names) or words which have obvious positive or negative valence. Participants are asked to respond ('go') or decline to respond ('no-go') during a short interval after each of the stimuli are presented. In test trials, participants are asked to respond to one of the concepts (white or black) and words with either positive or negative valence; these are then switched so that the concept is then paired with the opposite valence category. When paired with words with positive valence, faster and more accurate responding indicates greater association, and therefore positive attitude towards the target concept (either white or black race). A full demonstration of the GNAT procedure can be found in the external links below.\n\nLike the EAST, the GNAT has been used in populations who have been diagnosed with acute phobias to measure fear associations in addition to research on stereotypes and discrimination.\n\nThe Affect Misattribution Procedure relies on participant ratings of neutral stimuli as an indirect measure of implicit attitudes rather than latency or accuracy measures. In the procedure, participants are first presented with a stimulus (usually an image or word), for either a brief visible period or subliminally, which is suspected to elicit a positive or negative attitude. Directly afterwards, participants are presented with a neutral stimulus (most often a Chinese pictograph) which they are asked to rate as either more or less, in this case visually, pleasing than an average stimulus. During these trials, the positive or negative affect in response to the priming image is misattributed or 'projected' onto the neutral stimulus such that it is rated as more or less pleasing than would be expected from solitary presentation. Neutral stimuli which are rated as more visually pleasing indicate that the preceding concept presented in the prime stimuli are associated with positive valence. A full demonstration of the AMP procedure can be found in the external links below.\n\nThe AMP has been used to study attitudes towards political candidates and has proven useful in predicting voting behavior. Also, the procedure is frequently used in the study of substance use; for example, attitudes towards cigarettes among smokers and non-smokers and attitudes towards alcohol among heavy drinkers.\n\nThe following are some examples of how implicit attitude and explicit attitude are moderated by each other and how they interact with each other.\n\nIndividuals will alter a response when questioned for personal or social purposes. This typically happens in situations where individuals are not willing to report or express their \"affective response toward an object\" because they don't want others to know how they feel about something (they don't consciously accept or endorse their evaluation). Since implicit measures are not as vulnerable to control as explicit measures are, the correlation between implicit and explicit attitudes should decrease as self presentation concerns increase. For example, Nosek (2005) found that there was more overlap in explicit and implicit measures when people rated Pepsi vs. Coca-Cola (low self presentation concern). However, when they rated thin vs. fat people (high self presentation concern), the correlation (or overlap) of implicit and explicit measure decreased.\n\nThe strength of an attitude has an influence on explicit attitudes the stronger an implicit attitude the more likely it is that it will show up in an explicit attitude. Strong attitudes are stable and not easily changed due to persuasion and can therefore help predict behaviors. The more an individual expresses or acts on an attitude the stronger the attitude becomes and the more automated the attitude becomes. Attitude strength should increase the correspondence between implicit and explicit attitudes. Conscious thinking about the attitude should create more of an overlap between both implicit and explicit attitude.\n\nMuch of the literature within the field of social psychology has focused on explicit constructions of the attitude construct. Until more recently, examination of attitudes beyond reported awareness has lagged far behind that of explicit attitudes. This point is driven home in a review of research in the mid 1990s which found that among attitudinal research published in 1989, approximately only 1 in 9 experimental paradigms utilized an indirect measure of attitude (necessary for determining contributions of implicit attitudes) while all of the reviewed studies employed direct measures such as self report of attitudes which were explicitly aware to participants.\n\nNewer research has called into question the distinction between implicit and explicit attitudes. Fazio & Olson ask whether a person who is being primed to detect implicit attitudes is necessarily blind to their implicit beliefs. In their paper they bring up the question; just because a person is primed on an unconscious level and may indeed be answering on an unconscious level, does that not mean that they could still be aware of their attitudes nonetheless. \"A second troublesome aspect of the implicit-explicit distinction is that it implies preexisting dual attitudes\". They go on to say there is not a known test capable of measuring explicit attitudes solely without the influence of implicit attitudes as well. However, they do go on to say that context can have a significant effect on this particular line of research. People's explicitly stated and implicitly tested attitudes are more likely to be in sync for trivial matters such as preference in a presidential election than for highly charged issues such as predispositions towards a certain race. They exert that \"The more sensitive the domain, the greater the likelihood that motivational factors will be evoked and exert some influence on overt responses to an explicit measure\". In other words, it is easier to compare explicit and implicit attitudes on safe subjects than subjects where people are likely to mask their beliefs.\n\nA prominent dual process theory specifying the relation between implicit and explicit attitudes is Gawronski and Bodenhausen's associative-propositional evaluation (APE) model. A central assumption of the APE model is that implicit and explicit evaluations are the product of two functionally distinct mental processes. Whereas implicit evaluations are assumed to be the outcome of associative processes, explicit evaluations are assumed to be the outcome of propositional processes. Associative processes are conceptualized as the activation of associations on the basis of feature similarity and spatio-temporal contiguity during learning. Propositional processes are defined as the validation of activated information on the basis of cognitive consistency. A central assumption of the APE model is that people tend to rely on their implicit evaluations when making explicit evaluative judgments to the extent that the implicit evaluative response is consistent with other momentarily considered propositional information. However, people may reject implicit evaluations for making explicit evaluative judgments when the implicit evaluative response is inconsistent with other momentarily considered propositional information. In addition to explaining the relation between implicit and explicit evaluations, the APE model accounts for diverging patterns of attitude change, including (a) changes in implicit but not explicit evaluations, (b) changes in explicit but not implicit evaluations, (c) corresponding changes in implicit and explicit evaluations, and (d) opposite changes in implicit and explicit evaluations.\n"}
{"id": "300602", "url": "https://en.wikipedia.org/wiki?curid=300602", "title": "Internet access", "text": "Internet access\n\nInternet access is the ability of individuals and organizations to connect to the Internet using computer terminals, computers, and other devices; and to access services such as email and the World Wide Web. Various technologies, at a wide range of speeds have been used by Internet service providers (ISPs) to provide this service.\n\nInternet access was once rare, but has grown rapidly. In 1995, only percent of the world's population had access, with well over half of those living in the United States, and consumer use was through dial-up. By the first decade of the 21st century, many consumers in developed nations used faster broadband technology, and by 2014, 41 percent of the world's population had access, broadband was almost ubiquitous worldwide, and global average connection speeds exceeded 1 Mbit/s..\n\nThe Internet developed from the ARPANET, which was funded by the US government to support projects within the government and at universities and research laboratories in the US – but grew over time to include most of the world's large universities and the research arms of many technology companies. Use by a wider audience only came in 1995 when restrictions on the use of the Internet to carry commercial traffic were lifted.\n\nIn the early to mid-1980s, most Internet access was from personal computers and workstations directly connected to local area networks or from dial-up connections using modems and analog telephone lines. LANs typically operated at 10 Mbit/s, while modem data-rates grew from 1200 bit/s in the early 1980s, to 56 kbit/s by the late 1990s. Initially, dial-up connections were made from terminals or computers running terminal emulation software to terminal servers on LANs. These dial-up connections did not support end-to-end use of the Internet protocols and only provided terminal to host connections. The introduction of network access servers supporting the Serial Line Internet Protocol (SLIP) and later the point-to-point protocol (PPP) extended the Internet protocols and made the full range of Internet services available to dial-up users; although slower, due to the lower data rates available using dial-up.\nBroadband Internet access, often shortened to just broadband, is simply defined as \"Internet access that is always on, and faster than the traditional dial-up access\" and so covers a wide range of technologies. Broadband connections are typically made using a computer's built in Ethernet networking capabilities, or by using a NIC expansion card.\n\nMost broadband services provide a continuous \"always on\" connection; there is no dial-in process required, and it does not interfere with voice use of phone lines. Broadband provides improved access to Internet services such as:\n\nIn the 1990s, the National Information Infrastructure initiative in the U.S. made broadband Internet access a public policy issue. In 2000, most Internet access to homes was provided using dial-up, while many businesses and schools were using broadband connections. In 2000 there were just under 150 million dial-up subscriptions in the 34 OECD countries and fewer than 20 million broadband subscriptions. By 2004, broadband had grown and dial-up had declined so that the number of subscriptions were roughly equal at 130 million each. In 2010, in the OECD countries, over 90% of the Internet access subscriptions used broadband, broadband had grown to more than 300 million subscriptions, and dial-up subscriptions had declined to fewer than 30 million.\n\nThe broadband technologies in widest use are ADSL and cable Internet access. Newer technologies include VDSL and optical fibre extended closer to the subscriber in both telephone and cable plants. Fibre-optic communication, while only recently being used in premises and to the curb schemes, has played a crucial role in enabling broadband Internet access by making transmission of information at very high data rates over longer distances much more cost-effective than copper wire technology.\n\nIn areas not served by ADSL or cable, some community organizations and local governments are installing Wi-Fi networks. Wireless and satellite Internet are often used in rural, undeveloped, or other hard to serve areas where wired Internet is not readily available.\n\nNewer technologies being deployed for fixed (stationary) and mobile broadband access include WiMAX, LTE, and fixed wireless, e.g., Motorola Canopy.\n\nStarting in roughly 2006, mobile broadband access is increasingly available at the consumer level using \"3G\" and \"4G\" technologies such as HSPA, EV-DO, HSPA+, and LTE.\n\nIn addition to access from home, school, and the workplace Internet access may be available from public places such as libraries and Internet cafes, where computers with Internet connections are available. Some libraries provide stations for physically connecting users' laptops to local area networks (LANs).\n\nWireless Internet access points are available in public places such as airport halls, in some cases just for brief use while standing. Some access points may also provide coin-operated computers. Various terms are used, such as \"public Internet kiosk\", \"public access terminal\", and \"Web payphone\". Many hotels also have public terminals, usually fee based.\n\nCoffee shops, shopping malls, and other venues increasingly offer wireless access to computer networks, referred to as hotspots, for users who bring their own wireless-enabled devices such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based. A Wi-Fi hotspot need not be limited to a confined location since multiple ones combined can cover a whole campus or park, or even an entire city can be enabled.\n\nAdditionally, Mobile broadband access allows smart phones and other digital devices to connect to the Internet from any location from which a mobile phone call can be made, subject to the capabilities of that mobile network.\n\nThe bit rates for dial-up modems range from as little as 110 bit/s in the late 1950s, to a maximum of from 33 to 64 kbit/s (V.90 and V.92) in the late 1990s. Dial-up connections generally require the dedicated use of a telephone line. Data compression can boost the effective bit rate for a dial-up modem connection to from 220 (V.42bis) to 320 (V.44) kbit/s. However, the effectiveness of data compression is quite variable, depending on the type of data being sent, the condition of the telephone line, and a number of other factors. In reality, the overall data rate rarely exceeds 150 kbit/s.\n\nBroadband technologies supply considerably higher bit rates than dial-up, generally without disrupting regular telephone use. Various minimum data rates and maximum latencies have been used in definitions of broadband, ranging from 64 kbit/s up to 4.0 Mbit/s. In 1988 the CCITT standards body defined \"broadband service\" as requiring transmission channels capable of supporting bit rates greater than the primary rate which ranged from about 1.5 to 2 Mbit/s. A 2006 Organisation for Economic Co-operation and Development (OECD) report defined broadband as having download data transfer rates equal to or faster than 256 kbit/s. And in 2015 the U.S. Federal Communications Commission (FCC) defined \"Basic Broadband\" as data transmission speeds of at least 25 Mbit/s downstream (from the Internet to the user’s computer) and 3 Mbit/s upstream (from the user’s computer to the Internet). The trend is to raise the threshold of the broadband definition as higher data rate services become available.\n\nThe higher data rate dial-up modems and many broadband services are \"asymmetric\"—supporting much higher data rates for download (toward the user) than for upload (toward the Internet).\n\nData rates, including those given in this article, are usually defined and advertised in terms of the maximum or peak download rate. In practice, these maximum data rates are not always reliably available to the customer. Actual end-to-end data rates can be lower due to a number of factors. In late June 2016, internet connection speeds averaged about 6 Mbit/s globally. Physical link quality can vary with distance and for wireless access with terrain, weather, building construction, antenna placement, and interference from other radio sources. Network bottlenecks may exist at points anywhere on the path from the end-user to the remote server or service being used and not just on the first or last link providing Internet access to the end-user.\n\nUsers may share access over a common network infrastructure. Since most users do not use their full connection capacity all of the time, this aggregation strategy (known as contended service) usually works well and users can burst to their full data rate at least for brief periods. However, peer-to-peer (P2P) file sharing and high-quality streaming video can require high data-rates for extended periods, which violates these assumptions and can cause a service to become oversubscribed, resulting in congestion and poor performance. The TCP protocol includes flow-control mechanisms that automatically throttle back on the bandwidth being used during periods of network congestion. This is fair in the sense that all users that experience congestion receive less bandwidth, but it can be frustrating for customers and a major problem for ISPs. In some cases the amount of bandwidth actually available may fall below the threshold required to support a particular service such as video conferencing or streaming live video–effectively making the service unavailable.\n\nWhen traffic is particularly heavy, an ISP can deliberately throttle back the bandwidth available to classes of users or for particular services. This is known as traffic shaping and careful use can ensure a better quality of service for time critical services even on extremely busy networks. However, overuse can lead to concerns about fairness and network neutrality or even charges of censorship, when some types of traffic are severely or completely blocked.\n\nAn Internet blackout or outage can be caused by local signaling interruptions. Disruptions of submarine communications cables may cause blackouts or slowdowns to large areas, such as in the 2008 submarine cable disruption. Less-developed countries are more vulnerable due to a small number of high-capacity links. Land cables are also vulnerable, as in 2011 when a woman digging for scrap metal severed most connectivity for the nation of Armenia. Internet blackouts affecting almost entire countries can be achieved by governments as a form of Internet censorship, as in the blockage of the Internet in Egypt, whereby approximately 93% of networks were without access in 2011 in an attempt to stop mobilization for anti-government protests.\n\nOn April 25, 1997, due to a combination of human error and software bug, an incorrect routing table at MAI Network Service (a Virginia Internet service provider) propagated across backbone routers and caused major disruption to Internet traffic for a few hours.\n\nWhen the Internet is accessed using a modem, digital data is converted to analog for transmission over analog networks such as the telephone and cable networks. A computer or other device accessing the Internet would either be connected directly to a modem that communicates with an Internet service provider (ISP) or the modem's Internet connection would be shared via a Local Area Network (LAN) which provides access in a limited area such as a home, school, computer laboratory, or office building.\n\nAlthough a connection to a LAN may provide very high data-rates within the LAN, actual Internet access speed is limited by the upstream link to the ISP. LANs may be wired or wireless. Ethernet over twisted pair cabling and Wi-Fi are the two most common technologies used to build LANs today, but ARCNET, Token Ring, Localtalk, FDDI, and other technologies were used in the past.\n\nEthernet is the name of the IEEE 802.3 standard for physical LAN communication and Wi-Fi is a trade name for a wireless local area network (WLAN) that uses one of the IEEE 802.11 standards. Ethernet cables are interconnected via switches & routers. Wi-Fi networks are built using one or more wireless antenna called access points.\n\nMany \"modems\" provide the additional functionality to host a LAN so most Internet access today is through a LAN, often a very small LAN with just one or two devices attached. And while LANs are an important form of Internet access, this raises the question of how and at what data rate the LAN itself is connected to the rest of the global Internet. The technologies described below are used to make these connections.\n\nThe term broadband includes a broad range of technologies, all of which provide higher data rate access to the Internet. The following technologies use wires or cables in contrast to wireless broadband described later.\n\nDial-up Internet access uses a modem and a phone call placed over the public switched telephone network (PSTN) to connect to a pool of modems operated by an ISP. The modem converts a computer's digital signal into an analog signal that travels over a phone line's local loop until it reaches a telephone company's switching facilities or central office (CO) where it is switched to another phone line that connects to another modem at the remote end of the connection.\n\nOperating on a single channel, a dial-up connection monopolizes the phone line and is one of the slowest methods of accessing the Internet. Dial-up is often the only form of Internet access available in rural areas as it requires no new infrastructure beyond the already existing telephone network, to connect to the Internet. Typically, dial-up connections do not exceed a speed of 56 kbit/s, as they are primarily made using modems that operate at a maximum data rate of 56 kbit/s downstream (towards the end user) and 34 or 48 kbit/s upstream (toward the global Internet).\n\nMultilink dial-up provides increased bandwidth by channel bonding multiple dial-up connections and accessing them as a single data channel. It requires two or more modems, phone lines, and dial-up accounts, as well as an ISP that supports multilinking – and of course any line and data charges are also doubled. This inverse multiplexing option was briefly popular with some high-end users before ISDN, DSL and other technologies became available. Diamond and other vendors created special modems to support multilinking.\n\nIntegrated Services Digital Network (ISDN) is a switched telephone service capable of transporting voice and digital data, as well as one of the oldest Internet access methods. ISDN has been used for voice, video conferencing, and broadband data applications. ISDN was very popular in Europe, but less common in North America. Its use peaked in the late 1990s before the availability of DSL and cable modem technologies.\n\nBasic rate ISDN, known as ISDN-BRI, has two 64 kbit/s \"bearer\" or \"B\" channels. These channels can be used separately for voice or data calls or bonded together to provide a 128 kbit/s service. Multiple ISDN-BRI lines can be bonded together to provide data rates above 128 kbit/s. Primary rate ISDN, known as ISDN-PRI, has 23 bearer channels (64 kbit/s each) for a combined data rate of 1.5 Mbit/s (US standard). An ISDN E1 (European standard) line has 30 bearer channels and a combined data rate of 1.9 Mbit/s.\n\nLeased lines are dedicated lines used primarily by ISPs, business, and other large enterprises to connect LANs and campus networks to the Internet using the existing infrastructure of the public telephone network or other providers. Delivered using wire, optical fiber, and radio, leased lines are used to provide Internet access directly as well as the building blocks from which several other forms of Internet access are created.\n\nT-carrier technology dates to 1957 and provides data rates that range from 56 and (DS0) to (DS1 or T1), to (DS3 or T3). A T1 line carries 24 voice or data channels (24 DS0s), so customers may use some channels for data and others for voice traffic or use all 24 channels for clear channel data. A DS3 (T3) line carries 28 DS1 (T1) channels. Fractional T1 lines are also available in multiples of a DS0 to provide data rates between 56 and . T-carrier lines require special termination equipment that may be separate from or integrated into a router or switch and which may be purchased or leased from an ISP. In Japan the equivalent standard is J1/J3. In Europe, a slightly different standard, E-carrier, provides 32 user channels () on an E1 () and 512 user channels or 16 E1s on an E3 ().\n\nSynchronous Optical Networking (SONET, in the U.S. and Canada) and Synchronous Digital Hierarchy (SDH, in the rest of the world) are the standard multiplexing protocols used to carry high-data-rate digital bit-streams over optical fiber using lasers or highly coherent light from light-emitting diodes (LEDs). At lower transmission rates data can also be transferred via an electrical interface. The basic unit of framing is an OC-3c (optical) or STS-3c (electrical) which carries . Thus an OC-3c will carry three OC-1 (51.84 Mbit/s) payloads each of which has enough capacity to include a full DS3. Higher data rates are delivered in OC-3c multiples of four providing OC-12c (), OC-48c (), OC-192c (), and OC-768c (39.813 Gbit/s). The \"c\" at the end of the OC labels stands for \"concatenated\" and indicates a single data stream rather than several multiplexed data streams.\n\nThe 1, 10, 40, and 100 gigabit Ethernet (GbE, 10 GbE, 40/100 GbE) IEEE standards (802.3) allow digital data to be delivered over copper wiring at distances to 100 m and over optical fiber at distances to .\n\nCable Internet provides access using a cable modem on hybrid fiber coaxial wiring originally developed to carry television signals. Either fiber-optic or coaxial copper cable may connect a node to a customer's location at a connection known as a cable drop. In a cable modem termination system, all nodes for cable subscribers in a neighborhood connect to a cable company's central office, known as the \"head end.\" The cable company then connects to the Internet using a variety of means – usually fiber optic cable or digital satellite and microwave transmissions. Like DSL, broadband cable provides a continuous connection with an ISP.\n\nDownstream, the direction toward the user, bit rates can be as much as 400 Mbit/s for business connections, and 320 Mbit/s for residential service in some countries. Upstream traffic, originating at the user, ranges from 384 kbit/s to more than 20 Mbit/s. Broadband cable access tends to service fewer business customers because existing television cable networks tend to service residential buildings and commercial buildings do not always include wiring for coaxial cable networks. In addition, because broadband cable subscribers share the same local line, communications may be intercepted by neighboring subscribers. Cable networks regularly provide encryption schemes for data traveling to and from customers, but these schemes may be thwarted.\n\nDigital Subscriber Line (DSL) service provides a connection to the Internet through the telephone network. Unlike dial-up, DSL can operate using a single phone line without preventing normal use of the telephone line for voice phone calls. DSL uses the high frequencies, while the low (audible) frequencies of the line are left free for regular telephone communication. These frequency bands are subsequently separated by filters installed at the customer's premises.\n\nDSL originally stood for \"digital subscriber loop\". In telecommunications marketing, the term digital subscriber line is widely understood to mean Asymmetric Digital Subscriber Line (ADSL), the most commonly installed variety of DSL. The data throughput of consumer DSL services typically ranges from 256 kbit/s to 20 Mbit/s in the direction to the customer (downstream), depending on DSL technology, line conditions, and service-level implementation. In ADSL, the data throughput in the upstream direction, (i.e. in the direction to the service provider) is lower than that in the downstream direction (i.e. to the customer), hence the designation of asymmetric. With a symmetric digital subscriber line (SDSL), the downstream and upstream data rates are equal.\n\nVery-high-bit-rate digital subscriber line (VDSL or VHDSL, ITU G.993.1) is a digital subscriber line (DSL) standard approved in 2001 that provides data rates up to 52 Mbit/s downstream and 16 Mbit/s upstream over copper wires and up to 85 Mbit/s down- and upstream on coaxial cable. VDSL is capable of supporting applications such as high-definition television, as well as telephone services (voice over IP) and general Internet access, over a single physical connection.\n\nVDSL2 (ITU-T G.993.2) is a second-generation version and an enhancement of VDSL. Approved in February 2006, it is able to provide data rates exceeding 100 Mbit/s simultaneously in both the upstream and downstream directions. However, the maximum data rate is achieved at a range of about 300 meters and performance degrades as distance and loop attenuation increases.\n\nDSL Rings (DSLR) or Bonded DSL Rings is a ring topology that uses DSL technology over existing copper telephone wires to provide data rates of up to 400 Mbit/s.\n\nFiber-to-the-home (FTTH) is one member of the Fiber-to-the-x (FTTx) family that includes Fiber-to-the-building or basement (FTTB), Fiber-to-the-premises (FTTP), Fiber-to-the-desk (FTTD), Fiber-to-the-curb (FTTC), and Fiber-to-the-node (FTTN). These methods all bring data closer to the end user on optical fibers. The differences between the methods have mostly to do with just how close to the end user the delivery on fiber comes. All of these delivery methods are similar to hybrid fiber-coaxial (HFC) systems used to provide cable Internet access.\n\nThe use of optical fiber offers much higher data rates over relatively longer distances. Most high-capacity Internet and cable television backbones already use fiber optic technology, with data switched to other technologies (DSL, cable, POTS) for final delivery to customers.\n\nAustralia began rolling out its National Broadband Network across the country using fiber-optic cables to 93 percent of Australian homes, schools, and businesses. The project was abandoned by the subsequent LNP government, in favour of a hybrid FTTN design, which turned out to be more expensive and introduced delays. Similar efforts are underway in Italy, Canada, India, and many other countries (see Fiber to the premises by country).\n\nPower-line Internet, also known as Broadband over power lines (BPL), carries Internet data on a conductor that is also used for electric power transmission. Because of the extensive power line infrastructure already in place, this technology can provide people in rural and low population areas access to the Internet with little cost in terms of new transmission equipment, cables, or wires. Data rates are asymmetric and generally range from 256 kbit/s to 2.7 Mbit/s.\n\nBecause these systems use parts of the radio spectrum allocated to other over-the-air communication services, interference between the services is a limiting factor in the introduction of power-line Internet systems. The IEEE P1901 standard specifies that all power-line protocols must detect existing usage and avoid interfering with it.\n\nPower-line Internet has developed faster in Europe than in the U.S. due to a historical difference in power system design philosophies. Data signals cannot pass through the step-down transformers used and so a repeater must be installed on each transformer. In the U.S. a transformer serves a small cluster of from one to a few houses. In Europe, it is more common for a somewhat larger transformer to service larger clusters of from 10 to 100 houses. Thus a typical U.S. city requires an order of magnitude more repeaters than in a comparable European city.\n\nAsynchronous Transfer Mode (ATM) and Frame Relay are wide-area networking standards that can be used to provide Internet access directly or as building blocks of other access technologies. For example, many DSL implementations use an ATM layer over the low-level bitstream layer to enable a number of different technologies over the same link. Customer LANs are typically connected to an ATM switch or a Frame Relay node using leased lines at a wide range of data rates.\n\nWhile still widely used, with the advent of Ethernet over optical fiber, MPLS, VPNs and broadband services such as cable modem and DSL, ATM and Frame Relay no longer play the prominent role they once did.\n\nWireless broadband is used to provide both fixed and mobile Internet access with the following technologies.\n\nSatellite Internet access provides fixed, portable, and mobile Internet access. Data rates range from 2 kbit/s to 1 Gbit/s downstream and from 2 kbit/s to 10 Mbit/s upstream. In the northern hemisphere, satellite antenna dishes require a clear line of sight to the southern sky, due to the equatorial position of all geostationary satellites. In the southern hemisphere, this situation is reversed, and dishes are pointed north. Service can be adversely affected by moisture, rain, and snow (known as rain fade). The system requires a carefully aimed directional antenna.\n\nSatellites in geostationary Earth orbit (GEO) operate in a fixed position 35,786 km (22,236 miles) above the Earth's equator. At the speed of light (about 300,000 km/s or 186,000 miles per second), it takes a quarter of a second for a radio signal to travel from the Earth to the satellite and back. When other switching and routing delays are added and the delays are doubled to allow for a full round-trip transmission, the total delay can be 0.75 to 1.25 seconds. This latency is large when compared to other forms of Internet access with typical latencies that range from 0.015 to 0.2 seconds. Long latencies negatively affect some applications that require real-time response, particularly online games, voice over IP, and remote control devices. TCP tuning and TCP acceleration techniques can mitigate some of these problems. GEO satellites do not cover the Earth's polar regions. HughesNet, Exede, AT&T and Dish Network have GEO systems.\nSatellites in low Earth orbit (LEO, below 2000 km or 1243 miles) and medium Earth orbit (MEO, between 2000 and 35,786 km or 1,243 and 22,236 miles) are less common, operate at lower altitudes, and are not fixed in their position above the Earth. Lower altitudes allow lower latencies and make real-time interactive Internet applications more feasible. LEO systems include Globalstar and Iridium. The O3b Satellite Constellation is a proposed MEO system with a latency of 125 ms. COMMStellation™ is a LEO system, scheduled for launch in 2015, that is expected to have a latency of just 7 ms.\n\nMobile broadband is the marketing term for wireless Internet access delivered through mobile phone towers to computers, mobile phones (called \"cell phones\" in North America and South Africa, and \"hand phones\" in Asia), and other digital devices using portable modems. Some mobile services allow more than one device to be connected to the Internet using a single cellular connection using a process called tethering. The modem may be built into laptop computers, tablets, mobile phones, and other devices, added to some devices using PC cards, USB modems, and USB sticks or dongles, or separate wireless modems can be used.\n\nNew mobile phone technology and infrastructure is introduced periodically and generally involves a change in the fundamental nature of the service, non-backwards-compatible transmission technology, higher peak data rates, new frequency bands, wider channel frequency bandwidth in Hertz becomes available. These transitions are referred to as generations. The first mobile data services became available during the second generation (2G).\n\nThe download (to the user) and upload (to the Internet) data rates given above are peak or maximum rates and end users will typically experience lower data rates.\n\nWiMAX was originally developed to deliver fixed wireless service with wireless mobility added in 2005. CDPD, CDMA2000 EV-DO, and MBWA are no longer being actively developed.\n\nIn 2011, 90% of the world's population lived in areas with 2G coverage, while 45% lived in areas with 2G and 3G coverage.\n\nWorldwide Interoperability for Microwave Access (WiMAX) is a set of interoperable implementations of the IEEE 802.16 family of wireless-network standards certified by the WiMAX Forum. WiMAX enables \"the delivery of last mile wireless broadband access as an alternative to cable and DSL\". The original IEEE 802.16 standard, now called \"Fixed WiMAX\", was published in 2001 and provided 30 to 40 megabit-per-second data rates. Mobility support was added in 2005. A 2011 update provides data rates up to 1 Gbit/s for fixed stations. WiMax offers a metropolitan area network with a signal radius of about 50 km (30 miles), far surpassing the 30-metre (100-foot) wireless range of a conventional Wi-Fi local area network (LAN). WiMAX signals also penetrate building walls much more effectively than Wi-Fi.\n\nWireless Internet service providers (WISPs) operate independently of mobile phone operators. WISPs typically employ low-cost IEEE 802.11 Wi-Fi radio systems to link up remote locations over great distances (Long-range Wi-Fi), but may use other higher-power radio communications systems as well.\n\nTraditional 802.11a/b/g/n/ac is an unlicensed omnidirectional service designed to span between 100 and 150 m (300 to 500 ft). By focusing the radio signal using a directional antenna (where allowed by regulations), 802.11 can operate reliably over a distance of many km(miles), although the technology's line-of-sight requirements hamper connectivity in areas with hilly or heavily foliated terrain. In addition, compared to hard-wired connectivity, there are security risks (unless robust security protocols are enabled); data rates are usually slower (2 to 50 times slower); and the network can be less stable, due to interference from other wireless devices and networks, weather and line-of-sight problems.\n\nWith the increasing popularity of unrelated consumer devices operating on the same 2.4 GHz band, many providers have migrated to the 5GHz ISM band. If the service provider holds the necessary spectrum license, it could also reconfigure various brands of off the shelf Wi-Fi hardware to operate on its own band instead of the crowded unlicensed ones. Using higher frequencies carries various advantages:\n\nProprietary technologies like Motorola Canopy & Expedience can be used by a WISP to offer wireless access to rural and other markets that are hard to reach using Wi-Fi or WiMAX. There are a number of companies that provide this service.\n\nLocal Multipoint Distribution Service (LMDS) is a broadband wireless access technology that uses microwave signals operating between 26 GHz and 29 GHz. Originally designed for digital television transmission (DTV), it is conceived as a fixed wireless, point-to-multipoint technology for utilization in the last mile. Data rates range from 64 kbit/s to 155 Mbit/s. Distance is typically limited to about , but links of up to 5 miles (8 km) from the base station are possible in some circumstances.\n\nLMDS has been surpassed in both technological and commercial potential by the LTE and WiMAX standards.\n\nIn some regions, notably in rural areas, the length of the copper lines makes it difficult for network operators to provide high bandwidth services. An alternative is to combine a fixed access network, typically XDSL, with a wireless network, typically LTE. The Broadband Forum has standardised an architecture for such Hybrid Access Networks.\n\nDeploying multiple adjacent Wi-Fi access points is sometimes used to create city-wide wireless networks. It is usually ordered by the local municipality from commercial WISPs.\n\nGrassroots efforts have also led to wireless community networks widely deployed at numerous countries, both developing and developed ones. Rural wireless-ISP installations are typically not commercial in nature and are instead a patchwork of systems built up by hobbyists mounting antennas on radio masts and towers, agricultural storage silos, very tall trees, or whatever other tall objects are available.\n\nWhere radio spectrum regulation is not community-friendly, the channels are crowded or when equipment can not be afforded by local residents, free-space optical communication can also be deployed in a similar manner for point to point transmission in air (rather than in fiber optic cable).\n\nPacket radio connects computers or whole networks operated by radio amateurs with the option to access the Internet. Note that as per the regulatory rules outlined in the HAM license, Internet access and e-mail should be strictly related to the activities of hardware amateurs.\n\nThe term, a tongue-in-cheek play on \"net(work)\" as in \"Internet\" or \"Ethernet\", refers to the wearing of sneakers as the transport mechanism for the data.\n\nFor those who do not have access to or can not afford broadband at home, downloading large files and disseminating information is done by transmission through workplace or library networks, taken home and shared with neighbors by sneakernet.\n\nThere are various decentralized, delay tolerant peer to peer applications which aim to fully automate this using any available interface, including both wireless (Bluetooth, Wi-Fi mesh, P2P or hotspots) and physically connected ones (USB storage, ethernet, etc.).\n\nSneakernets may also be used in tandem with computer network data transfer to increase data security or overall throughput for big data use cases. Innovation continues in the area to this day, for example AWS has recently announced Snowball, and bulk data processing is also done in a similar fashion by many research institutes and government agencies.\n\nInternet access is limited by the relation between pricing and available resources to spend. Regarding the latter, it is estimated that 40% of the world's population has less than US$20 per year available to spend on information and communications technology (ICT). In Mexico, the poorest 30% of the society counts with an estimated US$35 per year (US$3 per month) and in Brazil, the poorest 22% of the population counts with merely US$9 per year to spend on ICT (US$0.75 per month). From Latin America it is known that the borderline between ICT as a necessity good and ICT as a luxury good is roughly around the “magical number” of US$10 per person per month, or US$120 per year. This is the amount of ICT spending people esteem to be a basic necessity. Current Internet access prices exceed the available resources by large in many countries.\n\nDial-up users pay the costs for making local or long distance phone calls, usually pay a monthly subscription fee, and may be subject to additional per minute or traffic based charges, and connect time limits by their ISP. Though less common today than in the past, some dial-up access is offered for \"free\" in return for watching banner ads as part of the dial-up service. NetZero, BlueLight, Juno, Freenet (NZ), and Free-nets are examples of services providing free access. Some Wireless community networks continue the tradition of providing free Internet access.\n\nFixed broadband Internet access is often sold under an \"unlimited\" or flat rate pricing model, with price determined by the maximum data rate chosen by the customer, rather than a per minute or traffic based charge. Per minute and traffic based charges and traffic caps are common for mobile broadband Internet access.\n\nInternet services like Facebook, Wikipedia and Google have built special programs to partner with mobile network operators (MNO) to introduce \"zero-rating\" the cost for their data volumes as a means to provide their service more broadly into developing markets.\n\nWith increased consumer demand for streaming content such as video on demand and peer-to-peer file sharing, demand for bandwidth has increased rapidly and for some ISPs the flat rate pricing model may become unsustainable. However, with fixed costs estimated to represent 80–90% of the cost of providing broadband service, the marginal cost to carry additional traffic is low. Most ISPs do not disclose their costs, but the cost to transmit a gigabyte of data in 2011 was estimated to be about $0.03.\n\nSome ISPs estimate that a small number of their users consume a disproportionate portion of the total bandwidth. In response some ISPs are considering, are experimenting with, or have implemented combinations of traffic based pricing, time of day or \"peak\" and \"off peak\" pricing, and bandwidth or traffic caps. Others claim that because the marginal cost of extra bandwidth is very small with 80 to 90 percent of the costs fixed regardless of usage level, that such steps are unnecessary or motivated by concerns other than the cost of delivering bandwidth to the end user.\n\nIn Canada, Rogers Hi-Speed Internet and Bell Canada have imposed bandwidth caps. In 2008 Time Warner began experimenting with usage-based pricing in Beaumont, Texas. In 2009 an effort by Time Warner to expand usage-based pricing into the Rochester, New York area met with public resistance, however, and was abandoned.\nOn August 1, 2012 in Nashville, Tennessee and on October 1, 2012 in Tucson, Arizona Comcast began tests that impose data caps on area residents. In Nashville exceeding the 300 Gbyte cap mandates a temporary purchase of 50 Gbytes of additional data.\n\nDespite its tremendous growth, Internet access is not distributed equally within or between countries. The digital divide refers to “the gap between people with effective access to information and communications technology (ICT), and those with very limited or no access”. The gap between people with Internet access and those without is one of many aspects of the digital divide. Whether someone has access to the Internet can depend greatly on financial status, geographical location as well as government policies. “Low-income, rural, and minority populations have received special scrutiny as the technological \"have-nots.\"\n\nGovernment policies play a tremendous role in bringing Internet access to or limiting access for underserved groups, regions, and countries. For example, in Pakistan, which is pursuing an aggressive IT policy aimed at boosting its drive for economic modernization, the number of Internet users grew from 133,900 (0.1% of the population) in 2000 to 31 million (17.6% of the population) in 2011. In North Korea there is relatively little access to the Internet due to the governments' fear of political instability that might accompany the benefits of access to the global Internet. The U.S. trade embargo is a barrier limiting Internet access in Cuba.\n\nAccess to computers is a dominant factor in determining the level of Internet access. In 2011, in developing countries, 25% of households had a computer and 20% had Internet access, while in developed countries the figures were 74% of households had a computer and 71% had Internet access. The majority of people in developing countries do not have Internet access. About 4 billion people do not have Internet access. When buying computers was legalized in Cuba in 2007, the private ownership of computers soared (there were 630,000 computers available on the island in 2008, a 23% increase over 2007).\n\nInternet access has changed the way in which many people think and has become an integral part of peoples economic, political, and social lives. The United Nations has recognized that providing Internet access to more people in the world will allow them to take advantage of the “political, social, economic, educational, and career opportunities” available over the Internet. Several of the 67 principles adopted at the World Summit on the Information Society convened by the United Nations in Geneva in 2003, directly address the digital divide. To promote economic development and a reduction of the digital divide, national broadband plans have been and are being developed to increase the availability of affordable high-speed Internet access throughout the world.\n\nAccess to the Internet grew from an estimated 10 million people in 1993, to almost 40 million in 1995, to 670 million in 2002, and to 2.7 billion in 2013. With market saturation, growth in the number of Internet users is slowing in industrialized countries, but continues in Asia, Africa, Latin America, the Caribbean, and the Middle East.\n\nThere were roughly 0.6 billion fixed broadband subscribers and almost 1.2 billion mobile broadband subscribers in 2011. In developed countries people frequently use both fixed and mobile broadband networks. In developing countries mobile broadband is often the only access method available.\n\nTraditionally the divide has been measured in terms of the existing numbers of subscriptions and digital devices (\"have and have-not of subscriptions\"). Recent studies have measured the digital divide not in terms of technological devices, but in terms of the existing bandwidth per individual (in kbit/s per capita). As shown in the Figure on the side, the digital divide in kbit/s is not monotonically decreasing, but re-opens up with each new innovation. For example, \"the massive diffusion of narrow-band Internet and mobile phones during the late 1990s\" increased digital inequality, as well as \"the initial introduction of broadband DSL and cable modems during 2003–2004 increased levels of inequality\". This is because a new kind of connectivity is never introduced instantaneously and uniformly to society as a whole at once, but diffuses slowly through social networks. As shown by the Figure, during the mid-2000s, communication capacity was more unequally distributed than during the late 1980s, when only fixed-line phones existed. The most recent increase in digital equality stems from the massive diffusion of the latest digital innovations (i.e. fixed and mobile broadband infrastructures, e.g. 3G and fiber optics FTTH). As shown in the Figure, Internet access in terms of bandwidth is more unequally distributed in 2014 as it was in the mid-1990s.\n\nIn the United States, billions of dollars have been invested in efforts to narrow the digital divide and bring Internet access to more people in low-income and rural areas of the United States. Internet availability varies widely state by state in the U.S. In 2011 for example, 87.1% of all New Hampshire residents lived in a household where Internet was available, ranking first in the nation. \nMeanwhile, 61.4% of all Mississippi residents lived in a household where Internet was available, ranking last in the nation. \nThe Obama administration continued this commitment to narrowing the digital divide through the use of stimulus funding. The National Center for Education Statistics reported that 98% of all U.S. classroom computers had Internet access in 2008 with roughly one computer with Internet access available for every three students. The percentage and ratio of students to computers was the same for rural schools (98% and 1 computer for every 2.9 students).\n\nOne of the great challenges for Internet access in general and for broadband access in particular is to provide service to potential customers in areas of low population density, such as to farmers, ranchers, and small towns. In cities where the population density is high, it is easier for a service provider to recover equipment costs, but each rural customer may require expensive equipment to get connected. While 66% of Americans had an Internet connection in 2010, that figure was only 50% in rural areas, according to the Pew Internet & American Life Project.\nVirgin Media advertised over 100 towns across the United Kingdom \"from Cwmbran to Clydebank\" that have access to their 100 Mbit/s service.\n\nWireless Internet service providers (WISPs) are rapidly becoming a popular broadband option for rural areas. The technology's line-of-sight requirements may hamper connectivity in some areas with hilly and heavily foliated terrain. However, the Tegola project, a successful pilot in remote Scotland, demonstrates that wireless can be a viable option.\n\nThe Broadband for Rural Nova Scotia initiative is the first program in North America to guarantee access to \"100% of civic addresses\" in a region. It is based on Motorola Canopy technology. As of November 2011, under 1000 households have reported access problems. Deployment of a new cell network by one Canopy provider (Eastlink) was expected to provide the alternative of 3G/4G service, possibly at a special unmetered rate, for areas harder to serve by Canopy.\n\nIn New Zealand, a fund has been formed by the government to improve rural broadband, and mobile phone coverage. Current proposals include: (a) extending fibre coverage and upgrading copper to support VDSL, (b) focussing on improving the coverage of cellphone technology, or (c) regional wireless.\n\nSeveral countries have started to Hybrid Access Networks to provide faster Internet services in rural areas by enabling network operators to efficiently combine their XDSL and LTE networks.\n\nThe actions, statements, opinions, and recommendations outlined below have led to the suggestion that Internet access itself is or should become a civil or perhaps a human right.\n\nSeveral countries have adopted laws requiring the state to work to ensure that Internet access is broadly available and/or preventing the state from unreasonably restricting an individual's access to information and the Internet:\n\nIn December 2003, the World Summit on the Information Society (WSIS) was convened under the auspice of the United Nations. After lengthy negotiations between governments, businesses and civil society representatives the WSIS Declaration of Principles was adopted reaffirming the importance of the Information Society to maintaining and strengthening human rights:\n\nThe WSIS Declaration of Principles makes specific reference to the importance of the right to freedom of expression in the \"Information Society\" in stating:\n\nA poll of 27,973 adults in 26 countries, including 14,306 Internet users, conducted for the BBC World Service between 30 November 2009 and 7 February 2010 found that almost four in five Internet users and non-users around the world felt that access to the Internet was a fundamental right. 50% strongly agreed, 29% somewhat agreed, 9% somewhat disagreed, 6% strongly disagreed, and 6% gave no opinion.\n\nThe 88 recommendations made by the Special Rapporteur on the promotion and protection of the right to freedom of opinion and expression in a May 2011 report to the Human Rights Council of the United Nations General Assembly include several that bear on the question of the right to Internet access:\n\nNetwork neutrality (also net neutrality, Internet neutrality, or net equality) is the principle that Internet service providers and governments should treat all data on the Internet equally, not discriminating or charging differentially by user, content, site, platform, application, type of attached equipment, or mode of communication. Advocates of net neutrality have raised concerns about the ability of broadband providers to use their last mile infrastructure to block Internet applications and content (e.g. websites, services, and protocols), and even to block out competitors. Opponents claim net neutrality regulations would deter investment into improving broadband infrastructure and try to fix something that isn't broken. In April 2017, a recent attempt to compromise net neutrality in the United States is being considered by the newly appointed FCC chairman, Ajit Varadaraj Pai. The vote on whether or not to abolish net neutrality was passed on December 14, 2017, and ended in a 3-2 split in favor of abolishing net neutrality.\n\nNatural disasters disrupt internet access in profound ways. This is important—not only for telecommunication companies who own the networks and the businesses who use them, but for emergency crew and displaced citizens as well. The situation is worsened when hospitals or other buildings necessary to disaster response lose their connection. Knowledge gained from studying past internet disruptions by natural disasters could be put to use in planning or recovery. Additionally, because of both natural and man-made disasters, studies in network resiliency are now being conducted to prevent large-scale outages.\n\nOne way natural disasters impact internet connection is by damaging end sub-networks (subnets), making them unreachable. A study on local networks after Hurricane Katrina found that 26% of subnets within the storm coverage were unreachable. At Hurricane Katrina’s peak intensity, almost 35% of networks in Mississippi were without power, while around 14% of Louisiana’s networks were disrupted. Of those unreachable subnets, 73% were disrupted for four weeks or longer and 57% were at “network edges where important emergency organizations such as hospitals and government agencies are mostly located”. Extensive infrastructure damage and inaccessible areas were two explanations for the long delay in returning service. The company Cisco has revealed a Network Emergency Response Vehicle (NERV), a truck that makes portable communications possible for emergency responders despite traditional networks being disrupted.\n\nA second way natural disasters destroy internet connectivity is by severing submarine cables—fiber-optic cables placed on the ocean floor that provide international internet connection. A sequence of undersea earthquakes cut six out of seven international cables connected to that country and caused a tsunami that wiped out one of its cable and landing stations. The impact slowed or disabled internet connection for five days within the Asia-Pacific region as well as between the region and the United States and Europe.\n\nWith the rise in popularity of cloud computing, concern has grown over access to cloud-hosted data in the event of a natural disaster. Amazon Web Services (AWS) has been in the news for major network outages in April 2011 and June 2012. AWS, like other major cloud hosting companies, prepares for typical outages and large-scale natural disasters with backup power as well as backup data centers in other locations. AWS divides the globe into five regions and then splits each region into availability zones. A data center in one availability zone should be backed up by a data center in a different availability zone. Theoretically, a natural disaster would not affect more than one availability zone. This theory plays out as long as human error is not added to the mix. The June 2012 major storm only disabled the primary data center, but human error disabled the secondary and tertiary backups, affecting companies such as Netflix, Pinterest, Reddit, and Instagram.\n\n"}
{"id": "182655", "url": "https://en.wikipedia.org/wiki?curid=182655", "title": "Inventory", "text": "Inventory\n\nInventory (American English) or stock (British English) is the goods and materials that a business holds for the ultimate goal of resale (or repair).\n\nInventory management is a discipline primarily about specifying the shape and placement of stocked goods. It is required at different locations within a facility or within many locations of a supply network to precede the regular and planned course of production and stock of materials.\n\nThe concept of inventory, stock or work-in-process has been extended from manufacturing systems to service businesses and projects, by generalizing the definition to be \"all work within the process of production- all work that is or has occurred prior to the completion of production.\" In the context of a manufacturing production system, inventory refers to all work that has occurred – raw materials, partially finished products, finished products prior to sale and departure from the manufacturing system. In the context of services, inventory refers to all work done prior to sale, including partially process information.\n\nDefinition\n\nThe scope of inventory management concerns the balance between replenishment lead time, carrying costs of inventory, asset management, inventory forecasting, inventory valuation, inventory visibility, future inventory price forecasting, physical inventory, available physical space, quality management, replenishment, returns and defective goods, and demand forecasting. Balancing these competing requirements leads to optimal inventory levels, which is an ongoing process as the business needs shift and react to the wider environment.\n\nInventory management involves a retailer seeking to acquire and maintain a proper merchandise assortment while ordering, shipping, handling and related costs are kept in check. It also involves systems and processes that identify inventory requirements, set targets, provide replenishment techniques, report actual and projected inventory status and handle all functions related to the tracking and management of material. This would include the monitoring of material moved into and out of stockroom locations and the reconciling of the inventory balances. It also may include ABC analysis, lot tracking, cycle counting support, etc. Management of the inventories, with the primary objective of determining/controlling stock levels within the physical distribution system, functions to balance the need for product availability against the need for minimizing stock holding and handling costs.\n\nThere are five basic reasons for keeping an inventory\n\nAll these stock reasons can apply to any owner or product.\n\n\n\nAverage Daily/Weekly usage quantity X Lead time in days + Safety stock\n\nWhile accountants often discuss inventory in terms of goods for sale, organizations – manufacturers, service-providers and not-for-profits – also have inventories (fixtures, furniture, supplies, etc.) that they do not intend to sell. Manufacturers', distributors', and wholesalers' inventory tends to cluster in warehouses. Retailers' inventory may exist in a warehouse or in a shop or store accessible to customers. Inventories not intended for sale to customers or to clients may be held in any premises an organization uses. Stock ties up cash and, if uncontrolled, it will be impossible to know the actual level of stocks and therefore impossible to control them.\n\nWhile the reasons for holding stock were covered earlier, most manufacturing organizations usually divide their \"goods for sale\" inventory into:\n\nFor example:\n\nA canned food manufacturer's materials inventory includes the ingredients to form the foods to be canned, empty cans and their lids (or coils of steel or aluminum for constructing those components), labels, and anything else (solder, glue, etc.) that will form part of a finished can. The firm's work in process includes those materials from the time of release to the work floor until they become complete and ready for sale to wholesale or retail customers. This may be vats of prepared food, filled cans not yet labeled or sub-assemblies of food components. It may also include finished cans that are not yet packaged into cartons or pallets. Its finished good inventory consists of all the filled and labeled cans of food in its warehouse that it has manufactured and wishes to sell to food distributors (wholesalers), to grocery stores (retailers), and even perhaps to consumers through arrangements like factory stores and outlet centers.\n\nThe partially completed work (or work in process) is a measure of inventory built during the work execution of a capital project, such as encountered in civilian infrastructure construction or oil and gas. Inventory may not only reflect physical items (such as materials, parts, partially-finished sub-assemblies) but also knowledge work-in-process (such as partially completed engineering designs of components and assemblies to be fabricated).\n\nA \"virtual inventory\" (also known as a \"bank inventory\") enables a group of users to share common parts, especially where their availability at short notice may be critical but they are unlikely to required by more than a few bank members at any one time. Virtual inventory also allows distributors and fulfilment houses to ship goods to retailers direct from stock regardless of whether the stock is held in a retail store, stock room or warehouse.\n\nThere are several costs associated with inventory:\n\nInventory proportionality is the goal of demand-driven inventory management. The primary optimal outcome is to have the same number of days' (or hours', etc.) worth of inventory on hand across all products so that the time of runout of all products would be simultaneous. In such a case, there is no \"excess inventory,\" that is, inventory that would be left over of another product when the first product runs out. Excess inventory is sub-optimal because the money spent to obtain it could have been utilized better elsewhere, i.e. to the product that just ran out.\n\nThe secondary goal of inventory proportionality is inventory minimization. By integrating accurate demand forecasting with inventory management, rather than only looking at past averages, a much more accurate and optimal outcome is expected.\n\nIntegrating demand forecasting into inventory management in this way also allows for the prediction of the \"can fit\" point when inventory storage is limited on a per-product basis.\n\nThe technique of inventory proportionality is most appropriate for inventories that remain unseen by the consumer, as opposed to \"keep full\" systems where a retail consumer would like to see full shelves of the product they are buying so as not to think they are buying something old, unwanted or stale; and differentiated from the \"trigger point\" systems where product is reordered when it hits a certain level; inventory proportionality is used effectively by just-in-time manufacturing processes and retail applications where the product is hidden from view.\n\nOne early example of inventory proportionality used in a retail application in the United States was for motor fuel. Motor fuel (e.g. gasoline) is generally stored in underground storage tanks. The motorists do not know whether they are buying gasoline off the top or bottom of the tank, nor need they care. Additionally, these storage tanks have a maximum capacity and cannot be overfilled. Finally, the product is expensive. Inventory proportionality is used to balance the inventories of the different grades of motor fuel, each stored in dedicated tanks, in proportion to the sales of each grade. Excess inventory is not seen or valued by the consumer, so it is simply cash sunk (literally) into the ground. Inventory proportionality minimizes the amount of excess inventory carried in underground storage tanks. This application for motor fuel was first developed and implemented by Petrolsoft Corporation in 1990 for Chevron Products Company. Most major oil companies use such systems today.\n\nThe use of inventory proportionality in the United States is thought to have been inspired by Japanese just-in-time parts inventory management made famous by Toyota Motors in the 1980s.\n\nIt seems that around 1880 there was a change in manufacturing practice from companies with relatively homogeneous lines of products to horizontally integrated companies with unprecedented diversity in processes and products. Those companies (especially in metalworking) attempted to achieve success through economies of scope - the gains of jointly producing two or more products in one facility. The managers now needed information on the effect of product-mix decisions on overall profits and therefore needed accurate product-cost information. A variety of attempts to achieve this were unsuccessful due to the huge overhead of the information processing of the time. However, the burgeoning need for financial reporting after 1900 created unavoidable pressure for financial accounting of stock and the management need to cost manage products became overshadowed. In particular, it was the need for audited accounts that sealed the fate of managerial cost accounting. The dominance of financial reporting accounting over management accounting remains to this day with few exceptions, and the financial reporting definitions of 'cost' have distorted effective management 'cost' accounting since that time. This is particularly true of inventory.\n\nHence, high-level financial inventory has these two basic formulas, which relate to the accounting period:\n\n\nThe benefit of these formulas is that the first absorbs all overheads of production and raw material costs into a value of inventory for reporting. The second formula then creates the new start point for the next period and gives a figure to be subtracted from the sales price to determine some form of sales-margin figure.\n\nManufacturing management is more interested in \"inventory turnover ratio\" or \"average days to sell inventory\" since it tells them something about relative inventory levels.\n\nand its inverse\n\nThis ratio estimates how many times the inventory turns over a year. This number tells how much cash/goods are tied up waiting for the process and is a critical measure of process reliability and effectiveness. So a factory with two inventory turns has six months stock on hand, which is generally not a good figure (depending upon the industry), whereas a factory that moves from six turns to twelve turns has probably improved effectiveness by 100%. This improvement will have some negative results in the financial reporting, since the 'value' now stored in the factory as inventory is reduced.\n\nWhile these accounting measures of inventory are very useful because of their simplicity, they are also fraught with the danger of their own assumptions. There are, in fact, so many things that can vary hidden under this appearance of simplicity that a variety of 'adjusting' assumptions may be used. These include:\nInventory Turn is a financial accounting tool for evaluating inventory and it is not necessarily a management tool. Inventory management should be forward looking. The methodology applied is based on historical cost of goods sold. The ratio may not be able to reflect the usability of future production demand, as well as customer demand.\n\nBusiness models, including Just in Time (JIT) Inventory, Vendor Managed Inventory (VMI) and Customer Managed Inventory (CMI), attempt to minimize on-hand inventory and increase inventory turns. VMI and CMI have gained considerable attention due to the success of third-party vendors who offer added expertise and knowledge that organizations may not possess.\n\nInventory management in modern days is online oriented and more viable in digital. This type of dynamics order management will require end-to-end visibility, collaboration across fulfillment processes, real-time data automation among different companies, and integration among multiple systems.\n\nEach country has its own rules about accounting for inventory that fit with their financial-reporting rules.\n\nFor example, organizations in the U.S. define inventory to suit their needs within US Generally Accepted Accounting Practices (GAAP), the rules defined by the Financial Accounting Standards Board (FASB) (and others) and enforced by the U.S. Securities and Exchange Commission (SEC) and other federal and state agencies. Other countries often have similar arrangements but with their own accounting standards and national agencies instead.\n\nIt is intentional that financial accounting uses standards that allow the public to compare firms' performance, cost accounting functions internally to an organization and potentially with much greater flexibility. A discussion of inventory from standard and Theory of Constraints-based (throughput) cost accounting perspective follows some examples and a discussion of inventory from a financial accounting perspective.\n\nThe internal costing/valuation of inventory can be complex. Whereas in the past most enterprises ran simple, one-process factories, such enterprises are quite probably in the minority in the 21st century. Where 'one process' factories exist, there is a market for the goods created, which establishes an independent market value for the good. Today, with multistage-process companies, there is much inventory that would once have been finished goods which is now held as 'work in process' (WIP). This needs to be valued in the accounts, but the valuation is a management decision since there is no market for the partially finished product. This somewhat arbitrary 'valuation' of WIP combined with the allocation of overheads to it has led to some unintended and undesirable results.\n\nAn organization's inventory can appear a mixed blessing, since it counts as an asset on the balance sheet, but it also ties up money that could serve for other purposes and requires additional expense for its protection. Inventory may also cause significant tax expenses, depending on particular countries' laws regarding depreciation of inventory, as in Thor Power Tool Company v. Commissioner.\n\nInventory appears as a current asset on an organization's balance sheet because the organization can, in principle, turn it into cash by selling it. Some organizations hold larger inventories than their operations require in order to inflate their apparent asset value and their perceived profitability.\n\nIn addition to the money tied up by acquiring inventory, inventory also brings associated costs for warehouse space, for utilities, and for insurance to cover staff to handle and protect it from fire and other disasters, obsolescence, shrinkage (theft and errors), and others. Such holding costs can mount up: between a third and a half of its acquisition value per year. \n\nBusinesses that stock too little inventory cannot take advantage of large orders from customers if they cannot deliver. The conflicting objectives of cost control and customer service often put an organization's financial and operating managers against its sales and marketing departments. Salespeople, in particular, often receive sales-commission payments, so unavailable goods may reduce their potential personal income. This conflict can be minimised by reducing production time to being near or less than customers' expected delivery time. This effort, known as \"Lean production\" will significantly reduce working capital tied up in inventory and reduce manufacturing costs (See the Toyota Production System).\n\nBy helping the organization to make better decisions, the accountants can help the public sector to change in a very positive way that delivers increased value for the taxpayer’s investment. It can also help to incentive's progress and to ensure that reforms are sustainable and effective in the long term, by ensuring that success is appropriately recognized in both the formal and informal reward systems of the organization.\n\nTo say that they have a key role to play is an understatement. Finance is connected to most, if not all, of the key business processes within the organization. It should be steering the stewardship and accountability systems that ensure that the organization is conducting its business in an appropriate, ethical manner. It is critical that these foundations are firmly laid. So often they are the litmus test by which public confidence in the institution is either won or lost.\n\nFinance should also be providing the information, analysis and advice to enable the organizations’ service managers to operate effectively. This goes beyond the traditional preoccupation with budgets – how much have we spent so far, how much do we have left to spend? It is about helping the organization to better understand its own performance. That means making the connections and understanding the relationships between given inputs – the resources brought to bear – and the outputs and outcomes that they achieve. It is also about understanding and actively managing risks within the organization and its activities.\n\nWhen a merchant buys goods from inventory, the value of the inventory account is reduced by the cost of goods sold (COGS). This is simple where the cost has not varied across those held in stock; but where it has, then an agreed method must be derived to evaluate it. For commodity items that one cannot track individually, accountants must choose a method that fits the nature of the sale. Two popular methods in use are: FIFO (first in – first out) and LIFO (last in – first out).\n\nFIFO treats the first unit that arrived in inventory as the first one sold. LIFO considers the last unit arriving in inventory as the first one sold. Which method an accountant selects can have a significant effect on net income and book value and, in turn, on taxation. Using LIFO accounting for inventory, a company generally reports lower net income and lower book value, due to the effects of inflation. This generally results in lower taxation. Due to LIFO's potential to skew inventory value, UK GAAP and IAS have effectively banned LIFO inventory accounting. LIFO accounting is permitted in the United States subject to section 472 of the Internal Revenue Code.\n\nStandard cost accounting uses ratios called efficiencies that compare the labour and materials actually used to produce a good with those that the same goods would have required under \"standard\" conditions. As long as actual and standard conditions are similar, few problems arise. Unfortunately, standard cost accounting methods developed about 100 years ago, when labor comprised the most important cost in manufactured goods. Standard methods continue to emphasize labor efficiency even though that resource now constitutes a (very) small part of cost in most cases.\n\nStandard cost accounting can hurt managers, workers, and firms in several ways. For example, a policy decision to increase inventory can harm a manufacturing manager's performance evaluation. Increasing inventory requires increased production, which means that processes must operate at higher rates. When (not if) something goes wrong, the process takes longer and uses more than the standard labor time. The manager appears responsible for the excess, even though s/he has no control over the production requirement or the problem.\n\nIn adverse economic times, firms use the same efficiencies to downsize, rightsize, or otherwise reduce their labor force. Workers laid off under those circumstances have even less control over excess inventory and cost efficiencies than their managers.\n\nMany financial and cost accountants have agreed for many years on the desirability of replacing standard cost accounting. They have not, however, found a successor.\n\nEliyahu M. Goldratt developed the Theory of Constraints in part to address the cost-accounting problems in what he calls the \"cost world.\" He offers a substitute, called throughput accounting, that uses throughput (money for goods sold to customers) in place of output (goods produced that may sell or may boost inventory) and considers labor as a fixed rather than as a variable cost. He defines inventory simply as everything the organization owns that it plans to sell, including buildings, machinery, and many other things in addition to the categories listed here. Throughput accounting recognizes only one class of variable costs: the truly variable costs, like materials and components, which vary directly with the quantity produced\n\nFinished goods inventories remain balance-sheet assets, but labor-efficiency ratios no longer evaluate managers and workers. Instead of an incentive to reduce labor cost, throughput accounting focuses attention on the relationships between throughput (revenue or income) on one hand and controllable operating expenses and changes in inventory on the other.\n\nInventories also play an important role in national accounts and the analysis of the business cycle. Some short-term macroeconomic fluctuations are attributed to the inventory cycle.\n\nAlso known as distressed or expired stock, distressed inventory is inventory whose potential to be sold at a normal cost has passed or will soon pass. In certain industries it could also mean that the stock is or will soon be impossible to sell. Examples of distressed inventory include products which have reached their expiry date, or have reached a date in advance of expiry at which the planned market will no longer purchase them (e.g. 3 months left to expiry), clothing which is out of fashion, music which is no longer popular and old newspapers or magazines. It also includes computer or consumer-electronic equipment which is obsolete or discontinued and whose manufacturer is unable to support it, along with products which use that type of equipment e.g. VHS format equipment and videos.\n\nIn 2001, Cisco wrote off inventory worth US $2.25 billion due to duplicate orders. This is considered one of the biggest inventory write-offs in business history.\n\nStock rotation is the practice of changing the way inventory is displayed on a regular basis. This is most commonly used in hospitality and retail - particularity where food products are sold. For example, in the case of supermarkets that a customer frequents on a regular basis, the customer may know exactly what they want and where it is. This results in many customers going straight to the product they seek and do not look at other items on sale. To discourage this practice, stores will rotate the location of stock to encourage customers to look through the entire store. This is in hopes the customer will pick up items they would not normally see.\n\nInventory credit refers to the use of stock, or inventory, as collateral to raise finance. Where banks may be reluctant to accept traditional collateral, for example in developing countries where land title may be lacking, inventory credit is a potentially important way of overcoming financing constraints. This is not a new concept; archaeological evidence suggests that it was practiced in Ancient Rome. Obtaining finance against stocks of a wide range of products held in a bonded warehouse is common in much of the world. It is, for example, used with Parmesan cheese in Italy. Inventory credit on the basis of stored agricultural produce is widely used in Latin American countries and in some Asian countries. A precondition for such credit is that banks must be confident that the stored product will be available if they need to call on the collateral; this implies the existence of a reliable network of certified warehouses. Banks also face problems in valuing the inventory. The possibility of sudden falls in commodity prices means that they are usually reluctant to lend more than about 60% of the value of the inventory at the time of the loan.\n\n\n\n"}
{"id": "29273607", "url": "https://en.wikipedia.org/wiki?curid=29273607", "title": "Journey to the End of Coal", "text": "Journey to the End of Coal\n\nJourney to the End of Coal is a French web documentary directed by Samuel Bollendorff and Abel Segretin.\n\nBased on the \"choose your own adventure\" principle, the interactive documentary tells the story of millions of Chinese coal miners who are risking their lives to satisfy their country’s appetite for economic growth.\n\n\"Journey to the End of Coal\" won the Prix SCAM 2009 digital interactive artwork award \".\n\n\n\n"}
{"id": "6978390", "url": "https://en.wikipedia.org/wiki?curid=6978390", "title": "Korean decimal classification", "text": "Korean decimal classification\n\nThe Korean decimal classification (KDC) is a system of library classification used in South Korea. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.\n\n"}
{"id": "853189", "url": "https://en.wikipedia.org/wiki?curid=853189", "title": "Left-hand path and right-hand path", "text": "Left-hand path and right-hand path\n\nIn Western esotericism the Left-Hand Path and Right-Hand Path are the dichotomy between two opposing approaches to magic. This terminology is used in various groups involved in the occult and ceremonial magic. In some definitions, the \"Left-Hand Path\" is equated with malicious black magic and the \"Right-Hand Path\" with benevolent white magic. Other occultists have criticised this definition, believing that the Left–Right dichotomy refers merely to different kinds of working and does not necessarily connote good or bad magical actions.\n\nIn more recent definitions, which base themselves on the terms' origins in Indian Tantra, the Right-Hand Path, or RHP, is seen as a definition for those magical groups that follow specific ethical codes and adopt social convention, while the Left-Hand Path adopts the opposite attitude, espousing the breaking of taboo and the abandoning of set morality. Some contemporary occultists, such as Peter J. Carroll, have stressed that both paths can be followed by a magical practitioner, as essentially they have the same goals.\n\nThe Right-Hand Path is commonly thought to refer to magical or religious groups which adhere to a certain set of characteristics:\n\nThe occultists Dion Fortune and William G. Gray consider non-magical Abrahamic religions to be RHP.\n\nThe historian Dave Evans studied self-professed followers of the Left-Hand Path in the early 21st century, making several observations about their practices:\n\nCriticism of both terms has come from various occultists. The Magister of the Cultus Sabbati, Andrew D. Chumbley, stated that they were simply \"theoretical constructs\" that were \"without definitive objectivity\", and that nonetheless, both forms could be employed by the magician. He used the analogy of a person having two hands, a right and a left, both of which served the same master. Similar sentiments were expressed by the Wiccan High Priest John Belham-Payne, who stated that \"For me, magic is magic.\"\n\n\"Vāmācāra\" is a Sanskrit term meaning \"left-handed attainment\" and is synonymous with Left-Hand Path or Left-path (Sanskrit: \"Vāmamārga\"). It is used to describe a particular mode of worship or spiritual practice (Sanskrit: \"sadhana\") that is not only heterodox (Sanskrit: \"Nāstika\") to standard Vedic injunction, but extreme in comparison to prevailing cultural norms. These practices are often generally considered to be Tantric in orientation. The converse term to \"Vāmācāra\" is \"Dakshinachara\" (glossed \"Right-Hand Path\") which is used to refer not only to orthodox (\"Āstika\") sects but to modes of spirituality that engage in spiritual practices that not only accord with Vedic injunction but are generally agreeable to prevailing cultural norms. That said, left-handed and right-handed modes of practice may be evident in both orthodox and heterodox schools of Dharmic religions such as Hinduism, Jainism, Sikhism and Buddhism and are a matter of taste, culture, proclivity, initiation, \"sadhana\" and dharmic lineage (\"parampara\").\n\nThe occidental use of the terms \"Left-Hand Path\" and \"Right Hand-Path\" originated with Madame Blavatsky, a 19th-century occultist who founded the Theosophical Society. She had travelled across parts of southern Asia and claimed to have met with many mystics and magical practitioners in India and Tibet. She developed the term \"Left-Hand Path\" as a translation of the term Vamachara, an Indian Tantric practice that emphasised the breaking of Hindu societal taboos by having sexual intercourse in ritual, drinking alcohol, eating meat and assembling in graveyards, as a part of the spiritual practice. The term \"Vamachara\" literally meant \"the left-hand way\" in Sanskrit, and it was from this that Blavatsky first coined the term.\n\nReturning to Europe, Blavatsky began using the term. It was relatively easy for her to associate \"left\" with evil in many European countries, where it already had an association with many negative things; as the historian Dave Evans noted, homosexuals were referred to as \"left-handed\", and while in Protestant nations Roman Catholics were called \"left-footers\".\n\nIn New York, Madame Blavatsky founded the Theosophical Society with several other people in 1875. She set about writing several books, including \"Isis Unveiled\" (1877) in which she introduced the terms \"Left-Hand Path\" and \"Right-Hand Path\", firmly stating that she herself followed the RHP, and that followers of the LHP were practitioners of Black Magic who were a threat to society. The occult community soon picked up on her newly introduced duality, which, according to historian Dave Evans, \"had not been known before\" in the Western Esoteric Tradition. For instance, Dion Fortune, the founder of an esoteric magical group (the Society of the Inner Light) also took the side of the RHP, making the claim that \"black magicians\", or followers of the LHP, were homosexuals and that Indian servants might use malicious magical rites devoted to the goddess Kali against their European masters.\n\nAleister Crowley further altered and popularized the term in certain occult circles, referring to a \"Brother of the Left-Hand Path\", or a \"Black Brother\", as one who failed to attain the grade of Magister Templi in Crowley's system of ceremonial magic. Crowley also referred to the Left-Hand Path when describing the point at which the Adeptus Exemptus (such as his old Christian mentor, Macgregor Mathers) chooses to cross the Abyss, which is the location of Choronzon and the illusory eleventh Sephira, which is Da'ath or Knowledge. In this example, the adept must surrender all, including the guidance of his Holy Guardian Angel, and leap into the Abyss. If his accumulated Karma is sufficient, and if he has been utterly thorough in his own self-destruction, he becomes a \"babe of the abyss\", arising as a Star in the Crowleyan system. On the other hand, if he retains some fragment of ego, or if he fears to cross, he then becomes encysted. The layers of his self, which he could have shed in the Abyss, ossify around him. He is then titled a \"Brother of the Left-Hand Path\", who will eventually be broken up and disintegrated against his will, since he failed to choose voluntary disintegration. Crowley associated all this with \"Mary, a blasphemy against BABALON\", and with the celibacy of Christian clergy.\n\nAnother of those figures that Fortune considered to be a follower of the LHP was Arthur Edward Waite, who did not recognise these terms, and acknowledged that they were newly introduced and that in any case he believed the terms LHP and RHP to be distinct from black and white magic. However, despite Waite's attempts to distinguish the two, the equation of the LHP with Black Magic was propagated more widely in the fiction of Dennis Wheatley; Wheatley also conflated the two with Satanism and also the political ideology of communism, which he viewed as a threat to traditional British society. In one of his novels, \"Strange Conflict\" (1941), he stated that:\n\nIn the latter half of the 20th century various groups arose that self-professedly described themselves as LHP, but did not consider themselves as following Black Magic. In 1975, Kenneth Grant, a student of Aleister Crowley, explained in \"Cults of the Shadow\" that he and his group, the Typhonian Order, practiced the LHP. Grant's usage takes meaning from its roots in eastern Tantra; Grant states that it is about challenging taboos, but that it should be used in conjunction with the RHP to achieve balance.\n\nWhen Anton Szandor LaVey was developing his form of LaVeyan Satanism during the 1960s, he emphasised the rejection of traditional Christian morality and as such labelled his new philosophy to be a form of the Left-Hand Path. In his \"The Satanic Bible\", he wrote that \"Satanism is not a white light religion; it is a religion of the flesh, the mundane, the carnal—all of which are ruled by Satan, the personification of the Left Hand Path.\"\n\nIn Russia there is a tradition of Left Hand Path practices within the Rodnover community under the influence of Volhv Veleslav, and within the Odinist community with Askr Svarte, and in England with Nikarev Leshy. Veleslav has also written numerous books on Tantra and the Left Hand Path.\n\nStephen E. Flowers, Ph.D. of the Temple of Set states in his book \"Lords of the Left Hand Path: A History of Spiritual Dissent\" there are two criteria to be considered a true Lord of the Left Hand Path and they are Self-Deification and Antinomianism.\n\nTantra is a set of esoteric Indian traditions with roots in Hinduism and Buddhism. Tantra is often divided by its practitioners into two different paths: \"dakshinachara\" and \"vamachara\", translated as \"Right-Hand Path\" and \"Left-Hand Path\" respectively. Dakshinachara consists of traditional Hindu practices such as asceticism and meditation, while vamachara also includes ritual practices that conflict with mainstream Hinduism, such as sexual rituals, consumption of alcohol and other intoxicants. The two paths are viewed by Tantrists as equally valid approaches to enlightenment. Vamachara, however, is often considered to be the faster and more dangerous of the two paths, and is not suitable for all practitioners. The usage of the terms Left-Hand Path and Right-Hand Path is still current in modern Indian and Buddhist Tantra.\n\nRobert Beér's \"Encyclopedia of Tibetan Symbols and Motifs\" clarifies widespread taboos and deprecation that associate the left hand as dark, female, inferior and 'not right':\n\n\"In Buddhist tantra, the right hand symbolises the male aspect of compassion or skilful means, and the left hand represents the female aspect of wisdom or emptiness. Ritual hand-held attributes, such as the vajra and bell, vajra and lotus, damaru and bell, damaru and khatvanga, arrow and bow, curved knife and skull-cup, sword and shield, hook and rope snare, etc., placed in the right and left hands respectively, symbolise the union of the active male aspect of skilful means with the contemplative female aspect of wisdom.\n\nIn both Hinduism and Buddhism the goddess is always placed on the left side of the male deity, where she 'sits on his left thigh, while her lord places his left arm over her left shoulder and dallies with her left breast'.\n\nIn representations of the Buddha image, the right hand often makes an active mudra of skilful means—the earth-touching, protection, fearlessness, wish-granting or teaching mudra; while the left hand often remains in the passive mudra of meditative equipoise, resting in the lap and symbolising meditation on emptiness or wisdom.\"\n\nBeér's preceding explanations correspond to Yab-Yum (father-mother) symbolism and contemplation on or practice of sexual rituals associated with Vajrayogini and Anuttarayoga Tantra. Yab-yum is generally understood to represent the primordial (or mystical) union of wisdom and compassion. The metaphorical union of bliss and emptiness is commonly represented within Thangka paintings of the Cakrasaṃvara Tantra depicting the sexual union of the deity Saṃvara and his consort Dorje Pakmo.\n\n\n\n"}
{"id": "10035229", "url": "https://en.wikipedia.org/wiki?curid=10035229", "title": "Leibniz's gap", "text": "Leibniz's gap\n\nLeibniz's gap is a philosophy of mind term that is used to refer to the problem that thoughts cannot be observed or perceived solely by examining brain properties, events, and processes. Here the word 'gap' is a metaphor of a subquestion regarding the mind–body problem that allegedly must be answered in order to reach more profound understanding of consciousness and emergence. A theory that could correlate brain phenomena with psychological phenomena would \"bridge the gap.\" The term is named after Gottfried Leibniz who first presented the problem in his work \"The Monadology\" in 1714. Leibniz's passage describing the gap goes as follows:\n\nLeibniz himself sought to bridge the gap by introducing monads to explain the existence of immaterial, eternal souls. Leibniz's gap, however, applies to materialism and dualism alike. This brought late 19th century scientists to conclude that psychology must build on introspection; thus introspectionism was born. Computationalism seeks to answer the problem proposed by Leibniz's gap through functional analysis of the brain and its processes. Today the term Leibniz's gap is still in wide use in scientific debate as the mind body problem remains unsolved.\n\n"}
{"id": "8810311", "url": "https://en.wikipedia.org/wiki?curid=8810311", "title": "Ludic fallacy", "text": "Ludic fallacy\n\nThe ludic fallacy, identified by Nassim Nicholas Taleb in his book \"The Black Swan\" (2007), is \"the misuse of games to model real-life situations\". Taleb explains the fallacy as \"basing studies of chance on the narrow world of games and dice\". The adjective \"ludic\" originates from the Latin noun \"ludus\", meaning \"play, game, sport, pastime\".\n\nThe fallacy is a central argument in the book and a rebuttal of the predictive mathematical models used to predict the future – as well as an attack on the idea of applying naïve and simplified statistical models in complex domains. According to Taleb, statistics is applicable only in some domains, for instance casinos in which the odds are visible and defined. Taleb's argument centers on the idea that predictive models are based on platonified forms, gravitating towards mathematical purity and failing to take various aspects into account:\n\n\nOne example given in the book is the following thought experiment. Two people are involved:\n\nA third party asks them to \"assume that a coin is fair, i.e., has an equal probability of coming up heads or tails when flipped. I flip it ninety-nine times and get heads each time. What are the odds of my getting tails on my next throw?\"\n\nThe ludic fallacy here is to assume that in real life the rules from the purely hypothetical model (where Dr. John is correct) apply. Would a reasonable person bet on black on a roulette table that has come up red 99 times in a row (especially as the reward for a correct guess is so low when compared with the probable odds that the game is fixed)?\n\nIn classical terms, statistically significant events, i.e. unlikely events, should make one question one's model assumptions. In Bayesian statistics, this can be modelled by using a prior distribution for one's assumptions on the fairness of the coin, then Bayesian inference to update this distribution.\n\nNassim Taleb shares an example that comes from his friend and trading partner, Mark Spitznagel. \"A martial version of the ludic fallacy: organized competitive fighting trains the athlete to focus on the game and, in order not to dissipate his concentration, to ignore the possibility of what is not specifically allowed by the rules, such as kicks to the groin, a surprise knife, et cetera. So those who win gold medal might be precisely those who will be most vulnerable in real life.\"\n\nThe ludic fallacy is a specific case of the more general problem of platonicity, defined by Nassim Taleb as:\n"}
{"id": "43679677", "url": "https://en.wikipedia.org/wiki?curid=43679677", "title": "Mawza Exile", "text": "Mawza Exile\n\nThe Exile of Mawzaʻ (the expulsion of Yemenite Jews to Mawza') , ;‎ 1679–1680, is considered the single most traumatic event experienced collectively by the Jews of Yemen, in which Jews living in nearly all cities and towns throughout Yemen were banished by decree of the king, Imām al-Mahdi Ahmad, and sent to a dry and barren region of the country named Mawzaʻ to withstand their fate or to die. Only a few communities, \"viz.\", those Jewish inhabitants who lived in the far eastern quarters of Yemen (Nihm, al-Jawf, and Khawlan of the east) were spared this fate by virtue of their Arab patrons who refused to obey the king's orders. Many would die along the route and while confined to the hot and arid conditions of this forbidding terrain. After one year in exile, the exiles were called back to perform their usual tasks and labors for the indigenous Arab populations, who had been deprived of goods and services on account of their exile.\n\nWith the rise to power of the Qāsimīd Imām, al-Mutawakkil Isma'il (1644–1676), there was a crucial turning point in the condition of Jews living under the Imamate kingdom of Yemen. He endorsed the most hostile policies toward his Jewish subjects, partly due to the claim that the Jews were aiding the Ottoman Turks during the local uprising against them. The rise of the Shabbathian movement in Yemen in 1666 exacerbated the problems facing the community, calling into question their status as protected wards of the State. One decree led to another. The king initially demanded their conversion to Islam and when they refused, he made them stand out in the sun without apparel for three days, which was later followed by harsher decrees. It is said that al-Mutawakkil Isma'il consulted with the religious scholars of Islam and sought to determine whether or not the laws concerning Jews in the Arabian Peninsula applied also to Yemen, citing Muhammad who was reported as saying, \"There shall not be two religions in Arabia.\" When it was determined that these laws did indeed apply to Yemen, since the country was an indivisible part of the Arabian Peninsula, it then became incumbent upon Jews living in Yemen to either convert to Islam or to leave the country. Yet, since the king fell ill and was bedridden, he did not presently perform his ill-designs to expel the Jews from his kingdom, but commanded the heir to his throne, al-Mahdi Ahmad, to do so.\n\nAl-Mahdi Ahmad of al-Ghirās, who is also known by the epithet \"Ṣafī al-Din\" (purity of religion), succeeded al-Mutawakkil Isma'il, but perpetuated the same hostilities toward his Jewish subjects as those made by his predecessor. Everything reached its climax between the years 1677 and 1680, when he ordered the destruction of the synagogues in Sana'a and elsewhere. By early summer of 1679, he gave an ultimatum unto his Jewish subjects, namely, that they had the choice of either converting to Islam, in which they'd be allowed to remain in the country, or of being killed by the sword. He gave to them three months to decide what they would do.\n\nThe king's words led to no small consternation amongst his Jewish subjects in Yemen, who immediately declared a time of public fasting and prayer, which they did both by night and day. Their plight soon became known to the local Yemeni tribesmen, whose chiefs and principal men pitied their condition and intervened on their behalf. They came before the king and enquired concerning the decree, and insisted that the Jews had been loyal to their king and had not offended the Arab peoples, neither had they done anything worthy of death, but should only be punished a little for their \"obduracy\" in what concerns the religion of Islam. The king, agreeing to their counsel, chose not to kill his Jewish subjects, but decided to banish them from his kingdom. They were to be sent to Zeilaʻ, a place along the African coast of the Red Sea, where they would be confined for life, or else repent and accept the tenets of Islam.\n\nThe Jewish community in Sana'a was concentrated in the neighborhood of al-Sā'ilah, within the walled city, as one enters Bab al-Shaʻub (the Shaʻub Gate) on Sana'a's north side. The chief rabbi of the Jewish community at that time was an elder to whom they gave the title of Prince (\"nasi\"), Rabbi Suleiman al-Naqqāsh, while the city's chief seat of learning was under the tutelage of Rabbi and Judge, Shelomo ben Saadia al-Manzeli (\"resh methivta\"). The Jews of Sana'a were given but short notice about the things that were about to happen to them. They had been advised to sell their houses, fields and vineyards, and that all property which they were unable to sell would automatically be confiscated and accrue to the Public Treasury (Ar. \"al-māl\"), without recompense.\n\nBy late 1679, when the king saw that they were unrelenting in their fathers' faith, he then decided to follow through with what he had determined for them and issued a decree, banishing all Jews in his kingdom to the Red Sea outpost known as Zeila'. On the 2nd day of the lunar month Rajab, in the year 1090 of the Hijri calendar (corresponds with Gregorian calendar, August 10, 1679), his edict was put into effect, and he ordered the Jews of Sana'a to take leave of their places, but gave more space to the provincial governors of Yemen to begin the expulsion of all other Jews in Yemen to Zeila', and which should be accomplished by them in a time period not to exceed twelve months. The Jews of Sana'a had, meanwhile, set out on their journey, leaving behind them their homes and possessions, rather than exchange their religion for another. In doing so, they brought sanctity to God's name.\n\nRabbi Suleiman al-Naqqāsh, by his wisdom and care for his community, had preemptively made arrangements for the community's safety and upkeep by sending written notifications to the Jewish communities which lay along the route, requesting that they provide food and assistance to their poor Jewish brethren when they passed through their communities in the coming weeks or days. The king's soldiers were sent to escort the exiles unto their final destination, while the king himself had sent orders to the governors of the outlying districts and places where it was known that the Jewish exiles were to pass through while \"en route\" to Zeila', commanding them not to permit any Jew to remain in those cities when they reached them, but to send them on in their journey.\n\nMeanwhile, while columns of men, women and children were advancing by foot southward with only bare essentials, along the road leading from Sana'a to Dhamar, Yarim, 'Ibb and Ta'izz, the chiefs of the indigenous Sabaean tribes who had been the patrons of the Jews came together once again and petitioned the king, al-Mahdi, this time requesting that the king rescind his order to expel all Jews unto the Red Sea outpost of Zeila', but to be content with their banishment to the Tihama coastal town of Mawza'. The reason being for this urgent request was that, by taking into consideration their troubles in a barren wasteland, those that will remain of them will be more inclined to repent and to choose the way of Islam, in which case it will be easier to hoist them from that place and to bring them back unto their former places. The grandees reminded the king how they had been faithful in implementing his orders. At hearing this, the king agreed and sent orders to the effect that Jewish exiles should be conducted only to Mawza'.\n\nBy the time the Jews of Sana'a reached Dhamar, they had already been joined by the Jewish villagers of Siān and Tan'am (located about eastward of Bayt al-Ḥāḍir, southeast Sana'a), all of which places lie within Sana'a's periphery. The Jews had sent fifteen letters to the king in al-Ghirās, asking him to forgive them of whatever offense they may have made and to permit them to remain in their former settlements, yet none of these did he answer.\n\nAround the beginning of September 1679, approximately one month after the Jews of Sana'a had set out for Mawza‛, Jews that hailed from Dhurān – a village situate about three days' walking distance southwest of Sana'a – were also evacuated from their village. In a letter written in 1684 to the Jewish community of Hebron, only four years after the community's return to Dhurān, the author describes the sufferings of the Jews who were forced to leave their homes and to go into Mawza‛. One important revelation that emerges from his account of these events is that the Jews of Yemen had tried to pacify the king's wrath by paying large sums of money to him, but which money the king refused to accept:\n\nThe author goes on to explain how that, when they reached their destination, they wept bitterly, since many of them had perished as in a plague, and they were unable to bury them because of the excruciating heat. When some of their party had tried to escape at night, approximately seventy men, the next morning when the sun arose they were stricken down by the intense heat, and there they died. The author concludes by saying, \"Now, this decree of exile was at the beginning of \"anno mundi\" 5440 (= 1679 CE), and the blessed God redeemed us at the [year's] end; the sign of which being: 'The punishment of your iniquity has \"ended\" ' .\" Here, the author makes a play on words; the Hebrew word for \"ended\" (Heb. תם) having the numerical value of 440, the same as the year when abbreviated without the millennium.\n\nMawzaʻ is a town situated eleven-days' walking distance from Sana'a, and ca. from the port of Mocha, in the Tihama coastal plain. During their long trek there, the king's soldiers pressed them on. Many of the sick and elderly and children died along the way. Others would later succumb to the harsh weather conditions of that place. All, however, suffered from hunger and thirst. Eventually, the community of Sana'a was joined by other Jewish communities from across Yemen. In Mawzaʻ they remained for one full year, until 1680, when the king's non-Jewish subjects began to complain about their lack of farm implements which had been exclusively made by Jewish craftsmen. The governor of `Amran went personally before the king with a petition to bring back his Jewish subjects. The king acquiesced and sent emissaries bearing food and water to call them back to their former cities. Some returned only to find their homes taken by usurping occupants. Others decided to move and to settle elsewhere in Yemen.\n\nRabbi Hayim Hibshush, speaking somewhat about this time, writes: \"For the duration of one year since this decree was first issued, they went as sheep to the slaughter from all the districts of Yemen, while none remained of all those districts who did not go into exile, excepting the district of Nihm towards the east, and the district of al-Jawf, as well as the eastern district of Khawlan.\"\n\nRabbi Yiḥyah Salaḥ (who is known by the acronym Maharitz) gives a most captivating account of these harrowing events borne by the Jews of Sana'a in the years leading up to their expulsion, as also when they left their city, based on a hand-written document preserved and copied down by subsequent generations. Some have judged the sum and bearing of these events as a mere microscopic example of the sufferings experienced by the Jewish inhabitants as a whole, in each and every city throughout Yemen. Thus, he gives the following account:\n\nThose Jews who survived, who returned either to Sana'a or to the other towns and villages, were mostly ill from being exposed to the changes in climate and from the poor quality of drinking water. In Sana'a, they were required to relinquish their ownership over their houses and fields within the city's wall, in the neighborhood of al-Sā'ilah, and were directed to build humble abodes in a new area outside of the city's walls, in a place then known as the \"hyena's field\" (Ar. \"Qāʻ al-simaʻ\"), or what later became known as \"Qāʻ al-Yahud \" (the Jewish Quarter). This place attracted other migrant Jews from the other towns and villages from which they had been expelled and soon grew into a suburb, situate about one kilometer beyond the walls which then existed on the extreme west-side of the city. The first synagogue to be built in this place was the Alsheikh synagogue, in which was housed the returnees most prized possessions: Torah scrolls and old, handwritten manuscripts. Jewish houses were made \"low, seldom more than two storeys, and built of sun-baked brick dressed with mud.\" Today, the place is called \"Qāʻ al-ʻUlufi\" (Ar. قاع العلفي). The lands upon which they built the new Jewish Quarter were lands provided by the king, but the Jews were later required to pay a monthly tenancy fee for the land, and which money accrued to the Muslim \"Waqf \" (mortmain land) for the upkeep of their own places of worship. Between the new Jewish Quarter and the city walls was a suburb full of gardens called \"Bi'r alʻAzab\" (the Single's Well), being once the Turkish Quarter. In subsequent years, the Jewish Quarter was also enclosed by a wall.\n\nAt that time, the Muslims passed a new edict which forbade Jews from dwelling within Muslim neighborhoods, so as not to \"defile their habitations,\" although they were at liberty to work in the city. Those who traversed between the Jewish Quarter and the city would go by foot, while those who were either aged or ill would make use of beasts of burden to carry them into the city, the Jewish Quarter being then at a distance of about one-kilometer from the city's walls. When these who had ridden upon donkeys were spied by the gentiles, they were presently envied by the gentiles, who then went unto the king and pressed upon him to outlaw their riding upon donkeys, saying that this mode of transportation seemed to be too extravagant for the Jews and would lead to a general unrest and eventual hegemony over the indigenous people. The king then passed a series of discriminatory laws (Ar. \"ghiyār\") meant to humiliate the Jews and which not only forbade their riding upon donkeys and horses, but also from walking or passing to the right side of any Muslim. Jews were to pass only on the left side of all Muslims. They also petitioned the king that a Jew would be prohibited by an edict from raising his voice against any Muslim, but to behave in a lowly and contrite spirit, and that offenders would be made punishable by flogging. Such were the conditions of the Jews at that time.\n\nThe Exile of Mawzaʻ brought about demographic changes that could be felt all across Yemen. In Sana'a, to distinguish the original inhabitants from incoming migrant Jews, all newcomers who chose to dwell in the newly built Jewish Quarter were given surnames, each one after the place from which he was exiled, so that a man who came from the district of Sharʻab was called so-and-so, al-Sharʻabi, or he that came from the village of Maswar was called so-and-so, al-Maswari. In the words of the Jewish chronicler who wrote \"Dofi Hazeman\" (Vicissitudes of Time), being one of the earliest Jewish accounts of the expulsion (initially compiled by Yaḥyā ben Judah Ṣa'di in 1725) and which work has since undergone several recensions by later chroniclers, we read the following testimony:\n\nDanish explorer, Carsten Niebuhr, who visited the Jewish Quarter of Sana'a in 1763, some eighty-three years following the community's return to Sana'a, estimated their numbers at only two-thousand. These had built, up until 1761, fourteen synagogues within the new Jewish Quarter. In 1902, before the famine of 1905 decimated more than half of the city's Jewish population, German explorer Hermann Burchardt estimated the Jewish population of Sana'a at somewhere between six and eight thousand. G. Wyman Bury, who visited the Jewish Quarter of Sana'a in 1905 noted a decrease in the city's population from 1891, estimated at 50,000 people (Jews and Muslims alike), to only about 20,000 people in 1905. By 1934, when Carl Rathjens visited Sana'a, the Jewish population in the city had swollen to about seven thousand.\nOne of the outcomes of the king's notorious decree was that Jewish property passed into Muslim hands. A Jewish public bath house in Sana'a was relinquished and passed into the proprietorship of the Muslim Waqf. So, too, the once famous synagogue within Sana'a's walled city and which was known as \"Kenisat al-'Ulamā\" (The Synagogue of the Sages) was turned into a mosque and called \"Masjid al-Jalā\" – the Mosque of the Expulsion, or \"of those banished.\" On the frieze (Ar. \"ṭiraz\") of the \"Masjid al-Jalā\" were inscribed words with invectives, in gypsum plaster (Ar. \"al-juṣ\"):\n\nOur king, al-Mahdi, is the sun of [religious] guidance / even Aḥmad, the [grand]son of him who rose to power, al-Qasim. Unto him is ascribed dignities, such as were not accorded / before [to any other], even in part. Had he not done aught but banish / the Jews of Ṣan'ā', who are the 'scum' of the world, and turned their venerable place (Ar. \"bi'ah\" = synagogue) into a mosque, / for bowing down unto God or standing [before Him in prayer], by that decree, he would have still been most triumphant. Now the time of this event happened to concur with the date that is [alluded to] in \"ghānim\" [victorious]\"; \"Ghānm\" = (), the numerical value of which letters adds up to A.H. 1091 = 1680 CE).\n\nRabbi Amram Qorah brings down a brief history of the said mosque, taken from a book originally drawn up in Arabic and which was entitled: \"A List of the Mosques of Ṣan'ā\"'. Therein is found a vivid description of the events which transpired in that fateful year and which reads as follows: \"Among the mosques built in the vicinity of al-Sā'ilah, northwards from the path which leads from al-Sā'ilah to al-Quzālī, and the mosque [known as] Ben al-Ḥussein built by the Imam of the Qasimid dynasty, the son of Muhammad (i.e. al-Mahdi Ahmad b. al-Ḥasan b. al-Qasim b. Muhammad), in the year A.H. 1091 (= 1679 CE) in the synagogue of the Jewish Quarter, who banished them from Sana'a and removed them unto a place befitting them, [a place] now known as \"Qāʻ al-Yahud\" on the west side of Sana'a, just as it has been intimated by the scholarly judge, Muhammad b. Ibrahim al-Suḥuli, etc.\" Rabbi Amram Qorah then proceeds to bring down the words or panegyric inscribed on the frieze of the mosque in rhymed verse (see: \"supra\"), and which apparently had been composed by the said judge, in which he describes the exploits of the king who banished the Jews and who converted their synagogue into a mosque.\n\nRabbi Amram Qorah, in the same work, brings down Rabbi Pinheas ben Gad Hacohen's account of events, whose testimony he found written in the margin of the first page of a Prayer Book (\"Siddur\"), written in 1710:\n\nNow I shall inform you, my brethren, about what has happened to us at this time, since the beginning of \"anno\" 1,990 of the Seleucid Era (1678 CE) and in 1,991 [of the same] (1679 CE), how that the king made a decree and demolished all the synagogues of all the towns of Yemen, and there were some of the books and sacred writings that were desecrated at the hand of the gentiles, on account of our great iniquities, so that we could no longer make our [public] prayers, save only a very few [men] secretly within their houses. Afterwards, the king made a decree against the Jews to expel them into the wilderness of Mawzaʻ, while they, [at this time] demolished also their houses. However, there were some who managed to sell their house; what was worth one-thousand gold pieces they sold for one-hundred, and what was worth one-hundred gold pieces they sold for ten. So that, by these things, we were for a reproach amongst the nations, who continuously sought after ways by which they might cause us to change [our religion], O may God forbid! So, all of the exiles of Israel stood up and laid aside their most beloved and precious possessions as a means by which God's name might be sanctified, blessed be He, including their fields and their vineyards, and delivered themselves up as martyrs for God's name sake, blessed be He. And if one had need of going out into the marketplace, he could not avoid being the object of hatred and spite, while there were those who even attacked him or called him by abusive language, so that there was fulfilled in this, our generation, the scripture that says, \"Who will raise up Jacob, for he is too small\" to bear all the afflictions. So, too, was there fulfilled in us by reason of our iniquities the scripture that says, \"And I shall send a faintness into their hearts\" . Yet, the divine Name, blessed be He, gives us strength to bear all those troubles and travails each day.\n\nAnother man who witnessed these events, Shalem 'Ashri, also wrote a suppliant poem about the events of that year – the Exile of Mawzaʻ, now preserved in the Yemenite \"Diwān\", which same poem is meant to be chanted as a slow dirge by one or, at the most, two individuals, who are then answered by others who sit in attendance. It is sung without the accompaniment of musical instruments, although a tin drum is sometimes used, in accordance with what is customary and proper for the \"nashid\" (a rejoinder). His own name is spelt out in acrostic form in the first letters of each stanza:\n\"I shall shed my tears – like rain they shall pour down / over all the pleasant sons who have gone forth into exile.\nThey have forgotten what pertains to their happiness, and have also been diminished. / They journeyed in haste; along the parched ground they trod.\nOn the day when 'Uzal (i.e. Ṣan'ā') went into exile, they took up his burden.\nThe sun and the moon were extinguished at their departure!\nA multitude of the handmaid's sons have ruled over them. / Wrath, and also jealousy, they've poured out upon them.\nSo that they have inherited all of the glory, even their sublime honour!\nWhilst the dwelling place of God's glory, they have been given power to destroy! \nMidrash, as also the Talmud and the Torah, they have abolished. / Constable and elder were, both, drawn away by their hands.\nOrion and Pleiades, as well as the crescent moon, have become dim! / Even all the luminous lights, their light has turned into darkness!\nThe beauty of their homes and their money they had entirely looted. / Every oppressor and every governor have prepared their bow for shooting.\nPreserve, O Master of the universe, those who are your peculiar friends, / Hadoram (i.e. Dhamar), God's congregation, have been drawn after you!\nThe heads of their academies have borne patiently the exile, / to do even the will of God, having valued the commandments.\nRedeem, O Master of the universe, your friends who have inherited / the Divine Law and sound wisdom, by which they have been blest!\nFor the honour due to the writing of thine own hand on the day when they were gathered, / may you call to remembrance and deliver them during the time of their flight.\nMy name is Shalem; 'tis written in the locked rhyme. / Rejoice in God's Divine Law, and bless His name!\"\n\nOriginal: \n\nאזיל דמעותי כמטר יזלו / על כל בני חמדה בגלות הלכו.\nנשו לטובתם וגם נתדלדלו / נסעו בחפזון בציה דרכו.\nיום גלתה אוזל וסבלו סבלו / שמש וירח בצאתם נדעכו.\nשפעת בני אמה עליהם משלו / חמה וגם קנאה עליהם שפכו.\nלכלל יקר הדרת כבודם נחלו / ומעון כבוד האל להחריב נמלכו.\nמדרש וגם תלמוד ותורה בטלו / שוטר וגם זקן ידיהם משכו.\nעיש וגם כימה וסהר אפלו / גם כל מאורי אור מאורם חשכו.\nאת כל נאות ביתם וכספם שללו / כל צר וכל מושל לקשתם דרכו.\nשמרה אדון עולם ידידים נסגלו / הדורם עדת האל אחריך נמשכו.\nראשי ישיבתם לגלות סבלו / לעשות רצון האל ומצות ערכו.\nיגאל אדון עולם ידידים נחלו / תורה ותושיה ובה נתברכו.\nלכבוד כתב ידך ביום שנקהלו / תזכר ותצילם בעת יתהלכו.\nשלם שמי כתוב בחרוזים ננעלו / שמחו בתורת אל ולשמו ברכו.\nIn the following poem of the subgenre known as \"qiṣṣa\" (poetic tale), composed mostly in Judeo-Arabic with only two stanzas written in Hebrew, the author gives a long testimony about the events which transpired during that year of exile. The poem is entitled, \"Waṣalnā hātif al-alḥān\" – \"Tidings have reached us,\" and is the work of the illustrious poet, Shalom Shabazi, who was an eye-witness to these events and whose name is inscribed in the poem in acrostics. The rhyme, however, has been lost in the translation: \n\n\"Tidings have reached us on the second day of [the lunar month] \"Rajab\" (i.e. corresponds with the 2nd day of the lunar month Elul), saying, 'My companions, arise and ascribe singularity unto the Merciful One, and read [the decree] that has been inscribed! Hearken to these matters, and let not your mind be distracted, for the appointed time is at hand. Al-Mahdi the king has decreed over us that we take flight.' The Jews of Sana'a then took leave, and have wandered unto those select places, even unto the habitation of vipers and brute beasts. Even from al-Mahjam and from Dar'ān it was decreed over us to leave; by authorization of an edict which has overcome us. Now, we shall wait in Mawza'; there we shall dwell in the far reaches of the land belonging to the inhabitants of Arabia.\n\nAll of the inhabitants of 'Uzal (i.e. Ṣan'ā') were obedient, and they assembled in Dhamar. My companion, tighten the camel's gear and we'll begin moving after the ass. Let us proceed to 'Adinah, then to 'Amirah, and to al-'Ammār, while there we shall make camp. As for the young ones and those who were weak, their tears flowed like riverine brooks. 'Idaynah, receive those who are beloved! Go out to the gate of the city to welcome them! Now is the hour of testing those who are friends. Let them take pleasure in the weary fugitive, so that his fatigue might depart from him. Lo! They are the sons of the tribes and of those who are pious; those who are highborn and of gentility.\n\n\"Ṣafī al-Din\" (i.e. al-Mahdi) has already given the order that we not remain in our places. Whether rich man or poor man, or he that is respectable, together they have gone forth; let us proceed according to our ability, under the influences of Saturn's horoscope; its evil portent will bring destruction. If its light flickers, it is about to change. The wisdom of the Blessed God has decreed upon the Sages of Israel, even the chosen sons of Jacob. Our elder, Suleiman [al-Naqqāsh] the Helmsman, will be the judge of those attempting to bypass [his decree]. In his hand there is the Imām's order for all to see, while there is nothing disparaging about the matter.\n\nI am curtailed of my sleep from dismay, while tears run down my cheeks. When our elder, al-Naqqāsh, had arrived, all of the Jews [who had come out to see him] were shaken-up. 'Let us go out into the barren wasteland, a place of monstrous beasts and every kind of lion. Happy is he who returns safely from that place. We have already sold our fields, have forsaken our houses, and have submitted to the decree of our lord, [the king].' The young men wept, as also the pious men, when His anger was turned against us. Consider, O Lord, and reflect upon how many distinguished men, as well as those who were delicately raised, have been humiliated!\n\nWeep, O Rachel, in our city for [your] wandering sons! Stir up our forefathers, let them arise, standing upon their feet, so that they may make mention of our fathers who, with grace, insist upon God's unison. May God's favour accompany us, in whose shadow we fervently desire. Let him gather those who dwell in Yemen, seeing that He is a Shepherd and the Faithful God. We shall then hear the song of the sons of Heman (i.e. the sons of Zerah, the son of Judah). Let him then take away the poison of the adder, which is most bitter. Let him command Yinnon (i.e. the Messiah) and the Prefect [of the priest] (i.e. Elijah, the forerunner of the Messiah), and let him say to him: 'Draw nigh!'\n\nBy the merit of our forefathers, by the favour [with which you have favoured] Levi who is of Jacob's seed, make level [the ground] along the route in your wilderness for the son who is, both, comely and good. And by the nut tree garden may you sedate my heart which is in pain. As for Gabriel and the rooster, I have heard them in the street, whilst my pigeon is at rest; she calls out to the poor: 'Release [them] from their bonds!' In Zion there is to be found relief, whilst our portion is in the Garden of Eden, just as a son who is dearly loved. We shall then behold the house of our God, and the houses of Gischala (Heb. Gush Ḥalab).\n\nThe Mashtaite has said: O God, remove mine affliction. Our strength is brought low in Yemen, in the days of my exile. In both small and great matters, I think about my case. Now, by the abundance [of afflictions] delights have been diminished. O gracious God! He who instructs my tongue to speak, Heaven forbid that you have forgotten me! Unto Whom belong signs and wonders. Behold, it was upon us that He bestowed His bounty, and He has chosen Moses, the son of Amram, our beloved prophet!\n\nThe pampered pigeons are cooing in the tops of the citadels. The householders of al-Sā'ilah who have come to visit al-Mahdi are complaining [before him] about how destruction and evil have come over them. They recall the conversations revolving around the Divine Law spoken [between their walls], and the vines and the flowers [in their gardens]; they recall also the social gatherings where wine was served, and the chalices, and the splendour of their wedding feasts, where [a man] would delight himself in them, become inebriated, but would avoid that which is obscene or mockery; [he'd drink] pure wine, whatever kind at hand, whose colour was as gold!\n\nThe Book of the Law (i.e. Torah) calls out to all wise men, and says: 'Have you neglected the study of the Law? It is the reason for their ignorance. Let them repent before the masters and return unto their Lord. The day of redemption is nigh, and He shall gather together their dispersed. There is a time for drinking wine, together with [eating] dainties, and there is a time for delving in wisdom. He, whose wine makes him heavy laden, let him sleep [and rest] from his weariness and from his burden. Let him wake-up to drink a second cup, such as may be imposed upon him.\n\nIn conclusion, [let us pray] that He who is congenial (i.e. God) might conceal us in the covert of His mercy. The Benevolent One shall not forget us, while we shall proclaim the eminence of His bountiful grace. He that will console us, may he be merited with a good life. He that gives to us clothing, may his own wishes be fulfilled. My salutations go out unto those of my companions on this happy, but powerful night; [which shall continue unabated] until Venus comes out [in the sky]. That which my God has decreed shall come to pass, while for every thing there is a reason. The birds will once again trill at the top of the ben [nut] tree (\"Moringa peregrina\") in the fruitful orchard.\"\nAnother record of these events, composed here in poetic verse (although the rhyme has been lost in the translation), is the poem composed by Sālim ben Sa'īd, in Judeo-Arabic. The poem is written as a \"nashid\" and is entitled, Ibda' birrub al-'arsh\" (I shall commence by addressing Him who is upon the throne). \n\n\"I shall commence by addressing Him who is upon the throne [of glory], even He that is an Omniscient God, the Creator of all creatures; He who causes the dumb to speak.\n\nI was curtailed of my sleep this night, while my heart was aching on account the king's decree; he that has made a decree against us by an oath.\n\nHe has revealed his ill-intentions on a dark night, one made sullen by the shadow of death; and who has sent against us soldiers and oppressors.\n\nWe lifted up our voices unto God of heaven, [saying]: 'Remove from us the evil of this decree. Behold! You are He that governs all!'\n\nThey have destroyed all of the cities, and have cast their fear upon the Sages. There is none who takes an interest in our case, nor anyone who will take pity upon us.\n\nHe lifted up his right hand and swore, 'They have no choice but to be banished unto Mawza!'\n\nHe commanded to destroy the synagogues which were in Sana'a, the habitation of the Divine Law and the seating place of the Sages.\n\nHe forced (?) them to go out into a parched land, the Tihama and al-Mahjam.\n\nThey wandered unto Mawza' and walked along the paths, in the fierce blaze of heat and with severe thirst.\n\nOn the day in which he took them out of their houses, their eyes rained tears of blood. They had gone out a short distance in the dark of night.\n\nSeveral distinguished persons, and several disciples of the Sages [went forth into exile]; they and their little ones, who were without understanding.\n\n'You are obliged to go forth into exile; `tis from the Lord of Heaven, who once delivered us from the hand of wicked Pharaoh.'\n\nMy heart moans over my relatives who are missing. I have no pleasure in sleep, nor in bread or water!\n\nA flame burns inside of me, ever since the evil tidings [of the king's decree] reached me; I have become perplexed.\n\nPraised be the Creator of the heavenly circuits, the Ruler of all [things], unto whom none can be compared.\n\nYour covenant and your signs have been forever. You have intoxicated your people with the waters of Abraham, [made during] the covenant between the dissected halves.\n\nBut now, O king of most puissant kings, your people are sadly distressed and are deprived of all things.\n\nThey (i.e. the gentiles) cast their fear upon us, while the horsemen inflict us. No one tries to help us, nor is there anyone who takes pity upon us.\n\nThey have humiliated our religion, and have called out to us to become Muslims; even to sin and to desecrate your Divine Law.\n\nHe (i.e. the Imām) has issued against us frequent declarations; shall we not fear the punishment of God on High?\n\nOur elders have gone forth into exile by an urgent command, whether willingly or unwillingly.\n\nI have concluded my words, my brethren! Take-up my salutations and remain silent! Our hope is in God the Omniscient.\n\nRemember me, O God, on account of the Divine Law's hidden mystery! So, too, remember Jacob, 'the man of pure intentions' [dwelling in tents]!\n\nRemember Moses who built for you the Tent of Convocation in the Sinai wilderness, on the day in which your Divine Presence dwelt thereon.\n\nDo not forget Isaac, your bound [servant], on the day in which he spoke to Abraham face to face.\n\nPraise be to you, O Master of the universe! `Tis from me, Sālim ben Sa'īd, who has written rhymed verse.\" \nIn 1859, Lithuanian Jew, Jacob Saphir, visited the Jewish community in Yemen, less than two-hundred years after the Exile of Mawza', but still heard vivid accounts from the people about the things that befell their ancestors during that fateful event. Later, he made a written account of the same in his momentous ethnographic work, \"Iben Safir\". The full, unabridged account is given here (translated from the original Hebrew):\n\n\"[The Jews] dwelt securely, beneath the shadow of the kings of that country, until three-hundred (sic) years ago while they were dwelling in that chief metropolis, when the daughter of the king became pregnant outside of wedlock, and they laid the blame upon a Jewish man, one of the king's courtiers and of those who behold his countenance. However, the king's wrath wasn't assuaged until he had banished all of the Jews from that city and the surrounding regions, expelling them to the region of Tihama, a desolate wilderness (a walking distance of ten days' journey in a south-westerly direction from Sana'a), between Mocha and Aden; a salty land, and one of very fearsome heat, while they were all tender and accustomed to delicacies. Many of them died along the way, while those who came there could not bear the climate of that place and its infirmities. Two thirds of them succumbed and perished, and they had entertained the notion that all of them would perish either by the plague, by famine or by thirst, may God forbid. Now during the time of this exile and perdition, they had lost all of their precious belongings, and their handwritten books, as well as their peculiar compositions which they possessed of old. I have also seen their synagogues and places of study used by them of old in the city of the gentiles; eternal desolations 'and where demons will be found making sport' , on account of our great iniquities. Notwithstanding, it is by the mercies of the Lord that we have not perished. He (i.e. God) did not prolong the days of their exile, but sent great distempers upon the king and upon his household. (They say that this was on account of the virtue of that pious Rabbi, the kabbalist, even our teacher and Rabbi, Mori Sālim al-Shabazi, may the memory of the righteous be blessed, who brought about multiple forms of distempers upon that cruel king, who then regretted the evil [that he caused them] and sent [messengers] to call out unto them [with] a conciliatory message, [requesting] that they return to their place – with the one exception that they not dwell with them in the royal city built as a fortress. He then gave to them a possession, being a grand inheritance outside of the city, which is \"al-Qaʻa\", \"B'ir al-ʻAzāb\" – the plain wherein is the cistern known as \"ʻAzāb\", and they built there houses for their dwelling quarters and built for themselves an enclosing wall which extended as far as to the wall of the city built like unto a fortress. In only a short time God assisted them, and they built there a large city and one that was spacious. They also acquired wealth and they rose to prominence, while many of the villagers likewise seized [upon land] with them, that they might dwell in the city, until it became [a place] full of people. At that time, Mori Yiḥya Halevi was the \"Nasi\" among them and the Exilarch.)\"\nThere are several references to Jewish life in Sana'a before the expulsion of 1679. Maharitz (d. 1805) mentions in his Responsa that before the Exile of Mawza the Jews of Sana'a had an old custom to say the seven benedictions for the bridegroom and bride on a Friday morning, following the couple's wedding the day before. On Friday (Sabbath eve) they would pitch a large tent within a garden called \"al-Jowzah\", replete with pillows and cushions, and there, on the next day (Sabbath afternoon), the invited guests would repeat the seven benedictions for the bridegroom and bride, followed by prayer inside the tent, before being dismissed to eat of their third Sabbath meal, at which time some accompanied the bridegroom to his own house to eat with him there. The significance of this practice, according to Maharitz, was that they made the seven blessings even when not actually eating in that place, a practice which differs from today's custom.\n\nGerman-Jewish ethnographer, Shelomo Dov Goitein, mentions a historical note about the old synagogue in Sana'a, before the expulsion of Jews from the city in 1679, and which is written in the glosses of an old copy of the Mishnah (\"Seder Moed\"), written with Babylonian supralinear punctuation. The marginal note concerns the accurate pronunciation of the word אישות in Mishnah \"Mo'ed Ḳaṭan\" 1:4, and reads as follows: \"Now the Jews of Sana'a read it as אִישׁוּת (\"ishūth\"), with a [vowel] \"shuraq\" (\"shuruk\"). I studied with them a long time ago, during the time when the synagogue of Sana'a was still standing \"in situ\".\"\n\nUpon returning to Sana'a, the Chief Rabbis, led by R. Shelomo Manzeli and Yiḥya Halevi (called \"Alsheikh\"), came together in the newly built Alsheikh synagogue and decided to put in place a series of enactments meant at bettering the spiritual condition of the community, and which they hoped would prevent the recurrence of such harsh decrees against the Jewish community in the future. These enactments were transcribed in a document entitled \"Iggereth Ha-Besoroth\" (Letter of Tidings), and which was believed to have been disseminated amongst the community at large. Only excerpts of the letter have survived. The enactments called out for a more strict observance of certain laws which, heretofore, had been observed with leniency. Such strictures were to be incumbent upon the entire community and which, in the Rabbis' estimation, would have given to the community some merit in the face of oppression or persecution. Not all of these enactments, however, were upheld by the community, since some enactments were seen as breaking-away from tradition.\n\n\n"}
{"id": "286882", "url": "https://en.wikipedia.org/wiki?curid=286882", "title": "Merry England", "text": "Merry England\n\n\"Merry England\", or in more jocular, archaic spelling \"Merrie England\" (also styled as \"Merrie Olde England\"), refers to an English autostereotype, a utopian conception of English society and culture based on an idyllic pastoral way of life that was allegedly prevalent in Early Modern Britain at some time between the Middle Ages and the onset of the Industrial Revolution. More broadly, it connotes a putative essential Englishness with nostalgic overtones, incorporating such cultural symbols as the thatched cottage, the country inn and the Sunday roast.\n\n\"Merry England\" is not a wholly consistent vision but rather a revisited England which Oxford folklorist Roy Judge described as \"a world that has never actually existed, a visionary, mythical landscape, where it is difficult to take normal historical bearings.\" It may be treated both as a product of the sentimental nostalgic imagination and as an ideological or political construct, often underwriting various sorts of conservative world-views. Favourable perceptions of Merry England reveal a nostalgia for aspects of an earlier society that are missing in modern times.\n\nThe concept of \"Merry England\" originated in the Middle Ages, when Henry of Huntingdon around 1150 first coined the phrase \"Anglia plena jocis\". His theme was taken up in the following century by the encyclopedist Bartholomeus Anglicus, who claimed that \"England is full of mirth and of game, and men oft-times able to mirth and game\".\n\nHowever Ronald Hutton's study of churchwardens' accounts places the real consolidation of \"Merry England\" in the years between 1350 and 1520, with the newly elaborative annual festive round of the liturgical year, with candles and pageants, processions and games, boy bishops and decorated rood lofts. Hutton argued that, far from being pagan survivals, many of the activities of popular piety criticised by sixteenth-century reformers were actually creations of the later Middle Ages: \"Merry England\" thus reflects those historical aspects of rural English customs and folklore that were subsequently lost. \n\nThe same concept \"may\" have also been used to describe a utopian state of life that peasants aspired to lead (see Cockaigne). Peasant revolts, such as those led by Wat Tyler and Jack Straw invoked a visionary idea that was also egalitarian – John Ball arguing for \"wines, spices, and good bread...velvet and camlet furred with grise\" all to be held in common. Tyler's rebels wished to throw off the feudal aristocracy (though the term \"Norman yoke\" belongs to a later period) and return to a perceived time where the Saxons ruled in equality and freedom. The main arguments of Tyler's rebels were that there was no basis for aristocratic rule in the Bible, and that the plague had demonstrated by its indiscriminate nature that all people were equal under God. \n\nEven in relatively peaceful times, medieval existence was for the majority a harsh and uncertain one – Lawrence Stone describing rural life as “at the mercy of disease and the weather...with money to burn today from the sale of a bumper crop, plunged into debt tomorrow because of harvest failure”. Nevertheless, the rural community was clearly prepared to play hard, as well as work hard (even if much of the surviving evidence for this comes in the form of official censure, ecclesiastical or secular). The festival calendar provided some fifty holy days for seasonal and communal coming-together and merry-making. Complaints against the rise in levels of drunkenness and crime on holidays, of flirting in church or on pilgrimage, of grievous bodily harm from the “abominable enough...foot-ball-game” all testify (however indirectly) to a vital, if unofficial medieval existence. Langland might castigate, but also provided a vivid picture of, those who “drink all day in diverse taverns, and gossip and joke there”, of the field-workers who “sat down to drink their ale and sing songs – thinking to plough his field with a \"Hey-nonny-nonny\"”. The wandering scholar, or goliard, who posed the mock questions of whether it was better to eat meat or fish, to court Agnes or Rose, belonged to a similar fraternity.\n\nMore legitimised recreation came in the form of archery, ice-skating, wrestling, hunting and hawking, while there was also the medieval angler, who “atte the leest hath his holsom walke and mery at his ease”. Above the town or village itself stood a semi-approved of layer of nomadic entertainers – minstrels, jugglers, mummers, morris-dancers, actors and jig-makers, all adding to first stirrings of mass entertainment.\n\nThus there was certainly \"merriment\" in Medieval England, even if always found in an unidealized and conflictual social setting. If there was a period after the Black Death when labour shortages meant that agricultural workers were in stronger positions, and serfdom was consequently eroded, the growing commercialisation of agriculture – with enclosures, rising rents, and pasture displacing arable, sheep men – meant that such social and economic hardship and conflict continued in the countryside through into Tudor times.\n\nThe Reformation set in motion a debate about popular festivities that was to endure for at least a century and a half – a culture war concerning the so-called politics of mirth. As part of the move away from Catholicism, Henry VIII had slashed the number of saint day holidays, attacking the \"lycencyous vacacyon and lybertye of these holy days\", and Edward VI had reduced them further to a bare twenty-seven. The annual festal round in parish society – consolidated between 1350 and 1520 and including such customs as church ales, may games, maypoles and local plays, came under severe pressure in Elizabeth's reign. Religious austerity, opposed to catholic and pagan hangovers, and economic arguments against idleness, found common ground in attacking communal celebrations.\n\nHowever a reaction quickly set in, John Caius in 1552 deploring the loss of what he called \"the old world, when this country was called merry England\". James I in 1618 issued his Book of Sports, specifically defending the practice of sports, dancing, maypoles and the like after Sunday Service; and his son Charles took a similar line. The question of \"Merry England\" thus became a focal point dividing Puritan and Anglican, proto-Royalist and proto-Roundhead, in the lead-up to the Civil War. Unsurprisingly, the Long Parliament put an end to ales, the last of which was held in 1641, and drove Christmas underground, where it was kept privately, as a form of protest; while the Restoration saw the revival of such pastimes (if not on the Sabbath itself) widely and popularly celebrated.\n\nAt various times since the Middle Ages, authors, propagandists, romanticists, poets and others have revived or co-opted the term. The celebrated Hogarth engraving illustrating the patriotic song \"The Roast Beef of Old England\" (\"see illustration\"), is as anti-French as it is patriotic.\n\nWilliam Hazlitt's essay \"Merry England\", appended to his \"Lectures on the English Comic Writers\" (1819), popularised the specific term, introduced in tandem with an allusion to the iconic figure of Robin Hood, under the epigraph \"St George for merry England!\":\nThe beams of the morning sun shining on the lonely glades, or through the idle branches of the tangled forest, the leisure, the freedom, 'the pleasure of going and coming without knowing where', the troops of wild deer, the sports of the chase, and other rustic gambols, were sufficient to justify the appelation of 'Merry Sherwood', and in like manner, we may apply the phrase to \"Merry England\".\n\nHazlitt's subject was the traditional sports and rural diversions native to the English. In \"Die Lage der arbeitenden Klasse in England\" (1844: translated as \"The Condition of the Working Class in England\"), Friedrich Engels wrote sarcastically of Young England (a ginger-group of young aristocrats hostile to the new industrial order) that they hoped to restore \"the old 'merry England' with its brilliant features and its romantic feudalism. This object is of course unattainable and ridiculous ...\" The phrase \"merry England\" appears in English in the German text.\nWilliam Cobbett provided conservative commentary on the rapidly changing look and \"mores\" of an industrialising nation by invoking the stable social hierarchy and prosperous working class of the pre-industrial country of his youth in his \"Rural Rides\" (1822–26, collected in book form, 1830). The later works of Samuel Taylor Coleridge also subscribed to some extent to the \"Merry England\" view. Thomas Carlyle's \"Past and Present\" also made the case for Merrie England; the conclusion of \"Crotchet Castle\" by Thomas Love Peacock contrasts the mediaevalism of Mr. Chainmail to the contemporary social unrest. Barry Cornwall's patriotic poem. \"Hurrah for Merry England\", was set twice to music and printed in \"The Musical Times\", in 1861 and 1880.\n\nIn the 1830s, the Gothic revival promoted in England what once had been a truly international European style. Its stages, though, had been given purely English antiquarian labels—\"Norman\" for the Romanesque, \"Early English\", etc.—and the revival was stretched to include also the succeeding, more specifically \"English\" style: a generic English Renaissance revival, later named \"Jacobethan\". The revival was spurred by a series of lithographs by Joseph Nash (1839–1849), illustrating \"The Mansions of England in the Olden Time\" in picturesque and accurate detail. They were peopled with jolly figures in ruffs and farthingales, who personified a specific \"Merry England\" that was not Catholic (always an issue with the Gothic style in England), yet full of lively detail, in a golden pre-industrial land of Cockaigne.\n\nChildren's storybooks and fairytales written in the Victorian period often used Merry England as a setting as it is seen as a mythical utopia. They often contain nature-loving mythological creatures such as elves and fairies, as well as Robin Hood. In popular culture, the adjective \"Dickensian\" is sometimes used in reference to the same mythical era, but Charles Dickens's view of the rural past evoked nostalgia, not fantasy. Mr. Pickwick's world was that of the 1820s and 1830s, of the stagecoach before the advent of the railways.\nThe London-based Anglo-Catholic magazine of prose and verse \"Merry England\" began publication in 1879. Its issues bore a sonnet by William Wordsworth as epigraph, beginning \"They called thee 'merry England' in old time\" and characterizing \"Merry England\" \"a responsive chime to the heart's fond belief\":\n<poem>\n...Can, I ask,\n</poem>\n\nIn the late Victorian era, the Tory Young England set perhaps best reflected the vision of \"Merry England\" on the political stage. Today, in a form adapted to political conservatism, the vision of \"Merry England\" extends to embrace a few urban artisans and other cosmopolitans; a flexible and humane clergy; an interested and altruistic squirearchy, aristocracy and royalty. Solidity and good cheer would be the values of yeoman farmers, whatever the foibles of those higher in the hierarchy.\n\nThe idea of Merry England became associated on one side with the Anglo-Catholics and Catholicism, as a version of life's generosity; for example Wilfrid Meynell entitled one of his magazines \"Merrie England\". The pastoral aspects of William Blake, a Londoner and an actual craftsman, lack the same mellow quality. G. K. Chesterton in part adapted it to urban conditions. William Morris and the Arts and Crafts movement and other left-inclined improvers. Walter Crane's \"Garland for May Day 1895\" is lettered \"Merrie England\" together with progressive slogans (\"Shorten Working Day & Lengthen Life\", \"The Land for the People\", \"No Child Toilers\") with socialism (\"Production for Use Not for Profit\"). For a time, the \"Merry England\" vision was a common reference point for rhetorical Tories and utopian socialists, offering similar alternatives to an industrialising society, with its large-scale movement off the land to jerry-built cities and gross social inequality.\nThis was also the theme of the journalist Robert Blatchford, editor of the \"Clarion\", in his booklet \"Merrie England\" (1893) In it he imagined a new society much on the basis of William News from Nowhere, in which capitalism had disappeared and people lived in a small self sufficient communities. The book was deeply nostalgic for a pastoral England of the past before industrial capitalism and factory production. It was widely and enjoyed worldwide sales, and probably more working class readers to socialism than William Morris or Karl Marx. \n\nAnother variant of \"Merry England\" was promoted in the \"organic community\" of F. R. Leavis by which he seems to have meant a community with a deeply rooted and locally self-sufficient culture that he claimed existed in the villages of 17th and 18th century England and which was destroyed by the machine and mass culture introduced by the industrial revolution. Historians of the era say that the idea was based on a misreading of history and that such communities had never existed.\n\n\"Punch\" in 1951 mocked both planning, and the concept of a revived Merry England, by envisioning a 'Merrie Board' with powers to set up 'Merrie Areas' in rural England – intended to preserve \"this hard core of Merriment\".\n\n\"Deep England\" refers to an idealised view of a rural, Southern England. The term is neutral, though it reflects what English cultural conservatives would wish to conserve. The term, which alludes to \"la France profonde\", has been attributed to both Patrick Wright and Angus Calder. The concept of Deep England may imply an explicit opposition to modernism and industrialisation; and may be connected to a ruralist viewpoint typified by the writer H. J. Massingham. Major artists whose work is associated with Deep England include: the writer Thomas Hardy, the painter John Constable, the composer Ralph Vaughan Williams, and the poets Rupert Brooke and Sir John Betjeman. Examples of this conservative or village green viewpoint include the editorial line sometimes adopted by the British \"Daily Mail\" newspaper, and the ideological outlook of magazines such as \"This England\". Wartime propaganda is sometimes taken to reflect a generalised view of a rural Deep England, but this is perhaps to ignore both the competing views of ruralism, and the mix of rural and non-rural actually offered for a post-war vision of a better Britain.\n\nIn Angus Calder's re-examination of the ideological constructs surrounding \"Little England\" during the Second World War in \"The Myth of the Blitz\", he puts forward the view that the story of Deep England was central to wartime propaganda operations within the United Kingdom, and then, as now, served a clearly defined political and cultural purpose in the hands of various interested agencies.\n\nCalder cites the writer and broadcaster J. B. Priestley whom he considered to be a proponent of the Deep England world-view. Priestley's wartime BBC radio \"chats\" described the beauty of the English natural environment, this at a time when rationing was at its height, and the population of London was sheltering from the Blitz in its Underground stations. In reference to one of Priestley's bucolic broadcasts, Calder made the following point:\n\nPriestley, the socialist, gives this cottage no occupant, nor does he wonder about the size of the occupant's wage, nor ask if the cottage has internal sanitation and running water. His countryside only exists as spectacle, for the delectation of people with motor cars.\" (Angus Calder, \"The Myth of the Blitz\", London 1991)\n\nHowever, in \"Journey Through England\", Priestley identified himself as a Little Englander because he despised imperialism and the effect that the capitalist industrial revolution had on the people and environment.\n\nPart of the imagery of the 1940 patriotic song \"There'll Always Be an England\" seems to be derived from the same source:\n\nThe continuation evokes, however, the opposite image of the modern industrialised society:\n\nThe song seems therefore to offer a synthesis and combine the two Englands, the archaic bucolic one and the modern industrialised one, in the focus of patriotic loyalty and veneration.\n\nSome of the nostalgia associated with Merry England is reminiscent, both in its origins and its artistic influences, of the 19th-Century Romantic Nationalism which flourished in continental Europe and became integral to Nazi ideology. However, the Merry England vision never motivated hostility towards foreigners, nor nationalism, as did Romantic Nationalism before and during the German Nazi era.\n\nThe transition from a literary locus of \"Merry England\" to a more obviously political one cannot be placed before 1945, as the cited example of J. B. Priestley shows. Writers and artists described as having a Merry England viewpoint range from the radical visionary poet William Blake to the evangelical Christian Arthur Mee. The Rudyard Kipling of \"Puck of Pook's Hill\" is certainly one; when he wrote it, he was in transition towards his later, very conservative stance. Within art, the fabled long-lost merrie England was also a recurring theme in the Victorian-era paintings of the Pre-Raphaelite Brotherhood. The 1890 News from Nowhere by William Morris portrays a future England that has reverted to a rural idyll following a socialist revolution.\n\nReference points might be taken as children's writer Beatrix Potter, John Betjeman (more interested in Victoriana), and the fantasy author J. R. R. Tolkien, whose hobbit characters' culture in The Shire embodied many aspects of the Merry England point of view.\n\nIn his essay \"Epic Pooh\", Michael Moorcock opined:\n\nHere the shift has taken place: Tolkien was profoundly conservative with respect to cultural traditions, as Moorcock is quite aware, but not at all an imperialist. He set an area based upon the West Midlands region within Middle-earth, but made it apparent that its perimeter was maintained by external allies. The Shire seems to represent the comfort of childhood which Tolkien's characters must leave in order to grow into maturity and wisdom. In addition, in \"The Fellowship of the Ring\", both Gandalf and Frodo express frustration with hobbits' staidness and dislike of anything foreign or out of the ordinary, and even in \"Concerning Hobbits\" the narrator shows a certain degree of impatience with hobbits' general narrow-mindedness. Rather than a celebration of a narrow, anachronistic idealism, Tolkien's works hinge upon his characters moving beyond that place of idealism into a broader, more complex interaction with the world.\n\nFurthermore, in the chapters \"Homeward Bound\" and \"The Scouring of the Shire\" of Tolkien's \"The Return of the King\" the Shire is depicted as being far from idyllic or safe, largely due to the actions of lax, greedy, and venal hobbits, with the character of Sam Gamgee describing the Shire upon his homecoming as \"This is worse than Mordor.\" Of particular note with respect to the naturally corrupt nature of hobbits is the final chapter of The Hobbit, where \"The legal bother, indeed, lasted for years. It was quite a long time before Mr Baggins was admitted to be alive again. The people who had got specially good bargains at the sale took a deal of convincing; and in the end to save time Bilbo had to buy back quite a lot of his own furniture.\"\n\n\"The Pyrates\", the 1983 spoof historical novel by George MacDonald Fraser, sets its scene with a page-long sentence composed entirely of (immediately demolished) Merry England tropes:\nThe novel \"England, England\" by Julian Barnes describes an imaginary, though plausible, set of circumstances that cause modern England to return to the state of Deep England. The author's views are not made explicit, but the characters who choose to remain in the changed nation are treated more sympathetically than those who leave.\n\nIn Kingsley Amis's novel \"Lucky Jim\", Professor Welch and his friends are devotees of the Merry England legend, and Jim's \"Merrie England\" lecture somehow turns into a debunking of the whole concept (a position almost certainly reflecting that of Amis).\n\nA few popular music artists have used elements of the Merry England story as recurring themes; The Kinks and their leader Ray Davies crafted \"The Kinks are the Village Green Preservation Society\" as a homage to English country life and culture: it was described by AllMusic senior editor Stephen Thomas Erlewine as an album \"lamenting the passing of old-fashioned English traditions\"; \"Arthur (Or the Decline and Fall of the British Empire)\" also contains similar elements. Ian Anderson of Jethro Tull has often alluded to an anti-modern, pre-industrial, agrarian vision of England in his songs (the band's namesake was himself an agrarian, the inventor of the seed drill).\n\n\"Merrie England\" is a comic opera by Edward German.\n\nRichmal Crompton's \"William the Bad'\" [1930] contains a chapter, 'The Pennymans Hand On The Torch', about an idealist couple who wish to return to Merrie England, as a staging post towards their ideal of living at \"the morning of the world\", which means dressing in flowing robes and (incongrously with the Merrie England concept, bearing in mind the traditions of English Ale and The Roast Beef Of Old England) being vegetarian and teetotal. The pageant they organise becomes a fiasco, largely, needless to say, on account of William's involvement as part of the dragon who fights Mr Pennyman's St George. \"The Pennymans'... pageant for May Day which involves St. George and the Dragon ... proves to be the first time ever that the Dragon (played by William) ever came out on top in the conflict\".\n\n\n"}
{"id": "48470754", "url": "https://en.wikipedia.org/wiki?curid=48470754", "title": "Mindfulness and technology", "text": "Mindfulness and technology\n\nMindfulness and technology is a movement in research and design, that encourages the user to become aware of the present moment, rather than losing oneself in a technological device. This field encompasses multidisciplinary participation between design, psychology, computer science, and religion. Mindfulness stems from Buddhist meditation practices and refers to the awareness that arises through paying attention on purpose in the present moment, and non-judgmentally. In the field of Human-Computer Interaction, research is being done on \"Techno-spirituality\" — the study of how technology can facilitate feelings of awe, wonder, transcendence, and mindfulness and on \"Slow design\", which facilitates self-reflection. The excessive use of personal devices, such as smartphones and laptops, can lead to the deterioration of mental and physical health. This area focuses on redesigning and creating technology to improve the wellbeing of its users.\n\nIn 1979, Jon Kabat-Zinn founded the Mindfulness-Based Stress Reduction (MBSR) program at the University of Massachusetts to treat the chronically ill. He is noted to be responsible for the popularization of mindfulness in Western culture. The program uses combination of mindfulness meditation, body awareness, and yoga. These practices were derived from teachings of the Eastern World, specifically Buddhist traditions. Researchers found that enhanced mindfulness through the program mediated the association between increased daily spiritual experiences and improved mental health-related quality of life.\n\nThere are applications for desktop and mobile to help users bring themselves back to the present moment.\n\nAccording to Vietnamese Zen teacher Thich Nhat Hanh, the ringing of a bell every 15 minutes is an effective way to cultivate the mindfulness practice and connect back with the body. The Mindfulness Bell and Mindful Mynah applications simulate the bell on the user's personal device.\n\nA 2011 brain imaging study published in the \"Journal of Neuroscience\" found that even very brief instruction in mindfulness meditation (four 20-minute sessions) was effective in relieving pain by reducing the brain's emotional response to painful stimuli. To help make meditation and mindfulness more accessible, developers have created digital health platforms, such as Headspace, Insight Timer and Buddhify.\n\nThere are several wearables which measures the breath in order to connect the user back to their body. Wo.Defy is a dress which attempts to reveal the beauty of emotional communication using the common platform of the human breath; proposing the best methods of human to human communication lie within us. Spire measures your breathing patterns to give you insights into your state of mind. Being, the mindfulness tracker from Zensorium, maps user’s emotions (stressed, excited normal and calm) through heart rate variability. WellBe monitors heart rate levels and then matches them, through a patent pending algorithm, to specific moments and interactions throughout a user’s day. SmartMat is a responsive mat embedded with 21,000 sensors to detect your body’s balance, pressure and alignment. Prana's platform evaluates breath patterns, takes into account the effects of posture on breathing, and differentiates between diaphragmatic and chest breathing, three critical components of assessing the true quality of breathing, previously unaddressed by systems such as spirometers or pulse oximeters.\n\nSonic Cradle enables users to shape sound with their breath while suspended in a completely dark chamber. The researchers conducted a qualitative study with 39 participants to show how persuasive media have the potential to promote long-term psychological health by experientially introducing a stress-relieving, contemplative practice to non-practitioners.\n\nBecause the nature of chronic pain is complex, pharmacological analgesics are often not enough to achieve an ideal treatment plan. The system incorporates biofeedback sensors, an immersive virtual environment, and stereoscopic sound titled the \"Virtual Meditative Walk\" (VMW). It was designed to enable chronic pain patients to learn Mindfulness-based stress reduction (MBSR), a form of meditation. By providing real-time visual and sonic feedback, VMW enables patients to learn how to manage their pain.\n\nIntel anthropologist Genevieve Bell has urged the human-computer interaction (HCI) research community to devote more research to the use of technology in spirituality and religion. Techno-spirituality is the study of how technology can facilitate feelings of awe, wonder, transcendence, and mindfulness. Currently, there are 6,000 applications related to spirituality and religion. This area is in high demand and “important under-explored areas of HCI research”.\n\nInspired by Bell’s work, researchers (Sterling & Zimmerman) focused on how mobile phones could be incorporated in American Soto Zen Buddhist community, without conflicting with their philosophy of “the here and the now”. They were able to find three ways to use technology to help strengthen ties within the community.\n\nSlow design is a design agenda for technology aimed at reflection and moments of mental rest rather than efficiency in performance.\n\nMindful design, based on Langer’s theory of mindfulness, is a design philosophy that incorporates the idea of mindfulness into creating meaningful user oriented design. A major tenant is the behavior change of a user through awareness and responsibility of meaningful interactions between user and designed object, and this will encourage more desirable human practices. This type of mind behavior driven change has been most heavily incorporated design for sustainability. Other approaches include crime prevention or health. It is also seen in the design of safety objects and the social interaction of performative objects.\n\nPerformative objects are identified as design objects that are designed to facilitate mindful awareness of the physical and symbolic social actions and their consequences within which they are used.\n\nClasses in mindfulness practices have become part of some of Silicon Valleys major tech giants. Google has implemented a series of bimonthly \"mindfulness lunches\" as well as built a labyrinth for walking meditations. Both Twitter and Facebook have incorporated contemplative practices into their corporate culture. The desired outcome of using mindfulness in the tech workforce is to increase communication and develop the emotional intelligence of their employees.\n\nMindfulness is currently being explored by researchers as a possible treatment for technological addiction, also known as Internet addiction disorder, a form of behavioral addiction. There has been some consensus in the field of psychology on the benefits of using mindfulness to treat behavioral addiction.\nExperts in the field say in order to treat technology addiction with mindfulness, one must be non-judgmental about the behavior and pay attention in order to recognize instances in which technology is being used mindlessly. Then reflect on the helpfulness of the device, and notice the benefits of disconnecting. The three keystones of mindfulness are: Intention, Attention and Action. Technology is said to interfere with mindfulness by causing the individual to forget what matters (intention), the distracts (attention), and then keeps the individual from taking action.\n\nIn technological addiction, the reward system, located in the mid-brain and underlies addiction, evolved to rewards finding and consuming food. In complex animals this evolution also rewards the exchange of information within the social group. In humans this has developed into its current form of mass worldwide communication. The exchange of social information has demonstrated reward based reinforcement, similar to that of gamification.\n\nCritics of the mindfulness movement in technology focus on several key areas, technophobia, pacifications of genuine grievances in the workforce and disconnection from religious roots. The editor of the New Republic, Eygeny Morozov, questions the value of tech companies who advocate \"unplugging\" from the modern digital lifestyle as similar to a drug addict taking a tolerance break from the substance they are addicted to in order to then increase the vigor with which those activities are then resumed (3, 4). They also state that the concept of Mindfulness in the tech world is jargonistic and amorphous.\n\nMobile meditation applications like Calm and Headspace have over a million users and are increasing in popularity. Swedish Researchers found that downloading and using the applications for eight weeks made little to no difference for people with major depression and anxiety. They did, however, see improvements with a subgroup with mild levels of depression.\n\nCriticisms of the slow technology movement are similar to the slow-food movement; it lacks understanding of global scope, and as an individualistic response will not answer the actual problems in technology. This movement has been dubbed by critics as disconnectionists. Mindfulness in technology and has been criticized as being less about restoring self and more about stifling autonomy that technology inspires. Anti-disconnectionists state mindfulness and the expressed need to disconnect from technology and the modern world can be accused of being a nostalgia-manipulating marketing tactic and maybe a technological form of conservatism. Critics state that the labeling of digital connection as debasing and unnatural is in direct proportion to the rapidity of adoption. Thus it is depicted as a dangerous desire and toxin to be regulated. This argument itself can be tied back to rationalization, Walter Benjamin on aura, Jacques Ellul on technique, Jean Baudrillard on simulations, or Zygmunt Bauman and the Frankfurt School on modernity and the Enlightenment. Critics state that disconnectionists see the Internet as having normalized or enforced a repression of an authentic self in favor of a social media avatar. Thus reflecting the desire to connect with a deeper self, which may itself be an illusion. The pathologization of technology use then opens the door for Foucault's idea of \"normalization\" to be applied to technology in similar fashion as other social ills, which then can become a concept around which social control and management can be applied.\n\nThere is some concern among Buddhist practitioners that decoupling mediation and mindfulness from the core tenement of Buddhism may have negative effects. The wide adoption of mindfulness in technology and the tech industry has been accused of increasing passivity in the worker by creating a calm mindstate which then allows for disconnect form actual grievances. Critics of mindfulness in Cognitive Behavior Therapy also comment on this as a possible problem. However, critics of the movement fear that the secularization of mindfulness, dubbed McMindfulness, leads to reinforcement of anti-Buddhist ideas. Buddhists differentiate between Right Mindfulness (samma sati) and Wrong Mindfulness (miccha sati). The distinction is not moralistic: the issue is whether the quality of awareness is characterized by wholesome intentions and positive mental qualities that lead to human flourishing and optimal well-being for others as well as oneself. Mindfulness as adopted by the Silicon Valley Tech giants has been criticized as conveniently shifting the burden of stress and toxic work environment onto the individual employee. Obfuscated by the seemingly inherent qualities of care and humanity, mindfulness is refashioned into a way to coping with and adapting to the stresses and strains of corporate life rather than actually solve them.\n"}
{"id": "43887987", "url": "https://en.wikipedia.org/wiki?curid=43887987", "title": "Mobility analogy", "text": "Mobility analogy\n\nThe mobility analogy, also called admittance analogy or Firestone analogy, is a method of representing a mechanical system by an analogous electrical system. The advantage of doing this is that there is a large body of theory and analysis techniques concerning complex electrical systems, especially in the field of filters. By converting to an electrical representation, these tools in the electrical domain can be directly applied to a mechanical system without modification. A further advantage occurs in electromechanical systems: Converting the mechanical part of such a system into the electrical domain allows the entire system to be analysed as a unified whole.\n\nThe mathematical behaviour of the simulated electrical system is identical to the mathematical behaviour of the represented mechanical system. Each element in the electrical domain has a corresponding element in the mechanical domain with an analogous constitutive equation. All laws of circuit analysis, such as Kirchhoff's laws, that apply in the electrical domain also apply to the mechanical mobility analogy.\n\nThe mobility analogy is one of the two main mechanical-electrical analogies used for representing mechanical systems in the electrical domain, the other being the impedance analogy. The roles of voltage and current are reversed in these two methods, and the electrical representations produced are the dual circuits of each other. The mobility analogy preserves the topology of the mechanical system when transferred to the electrical domain whereas the impedance analogy does not. On the other hand, the impedance analogy preserves the analogy between electrical impedance and mechanical impedance whereas the mobility analogy does not.\n\nThe mobility analogy is widely used to model the behaviour of mechanical filters. These are filters that are intended for use in an electronic circuit, but work entirely by mechanical vibrational waves. Transducers are provided at the input and output of the filter to convert between the electrical and mechanical domains.\n\nAnother very common use is in the field of audio equipment, such as loudspeakers. Loudspeakers consist of a transducer and mechanical moving parts. Acoustic waves themselves are waves of mechanical motion: of air molecules or some other fluid medium.\n\nBefore an electrical analogy can be developed for a mechanical system, it must first be described as an abstract mechanical network. The mechanical system is broken down into a number of ideal elements each of which can then be paired with an electrical analogue. The symbols used for these mechanical elements on network diagrams are shown in the following sections on each individual element.\n\nThe mechanical analogies of lumped electrical elements are also lumped elements, that is, it is assumed that the mechanical component possessing the element is small enough that the time taken by mechanical waves to propagate from one end of the component to the other can be neglected. Analogies can also be developed for distributed elements such as transmission lines but the greatest benefits are with lumped element circuits. Mechanical analogies are required for the three passive electrical elements, namely, resistance, inductance and capacitance. What these analogies are is determined by what mechanical property is chosen to represent voltage, and what property is chosen to represent current. In the mobility analogy the analogue of voltage is velocity and the analogue of current is force. Mechanical impedance is defined as the ratio of force to velocity, thus it is not analogous to electrical impedance. Rather, it is the analogue of electrical admittance, the inverse of impedance. Mechanical admittance is more commonly called mobility, hence the name of the analogy.\n\nThe mechanical analogy of electrical resistance is the loss of energy of a moving system through such processes as friction. A mechanical component analogous to a resistor is a shock absorber and the property analogous to inverse resistance (conductance) is damping (inverse, because electrical impedance is the analogy of the inverse of mechanical impedance). A resistor is governed by the constitutive equation of Ohm's law,\n\nThe analogous equation in the mechanical domain is,\n\nElectrical conductance represents the real part of electrical admittance. Likewise, mechanical resistance is the real part of mechanical impedance.\n\nThe mechanical analogy of inductance in the mobility analogy is compliance. It is more common in mechanics to discuss stiffness, the inverse of compliance. A mechanical component analogous to an inductor is a spring. An inductor is governed by the constitutive equation,\n\nThe analogous equation in the mechanical domain is a form of Hooke's law,\n\nThe impedance of an inductor is purely imaginary and is given by,\n\nThe analogous mechanical admittance is given by,\n\nThe mechanical analogy of capacitance in the mobility analogy is mass. A mechanical component analogous to a capacitor is a large, rigid weight. A capacitor is governed by the constitutive equation,\n\nThe analogous equation in the mechanical domain is Newton's second law of motion,\n\nThe impedance of a capacitor is purely imaginary and is given by,\n\nThe analogous mechanical admittance is given by,\n\nA curious difficulty arises with mass as the analogy of an electrical element. It is connected with the fact that in mechanical systems the velocity of the mass (and more importantly, its acceleration) is always measured against some fixed reference frame, usually the earth. Considered as a two-terminal system element, the mass has one terminal at velocity <nowiki>\"u\"</nowiki>, analogous to electric potential. The other terminal is at zero velocity and is analogous to electric ground potential. Thus, mass cannot be used as the analogue of an ungrounded capacitor.\n\nThis led Malcolm C. Smith of the University of Cambridge in 2002 to define a new energy storing element for mechanical networks called \"inertance\". A component that possesses inertance is called an inerter. The two terminals of an inerter, unlike a mass, are allowed to have two different, arbitrary velocities and accelerations. The constitutive equation of an inerter is given by,\n\nInertance has the same units as mass (kilograms in the SI system) and the name indicates its relationship to inertia. Smith did not just define a network theoretic element, he also suggested a construction for a real mechanical component and made a small prototype. Smith's inerter consists of a plunger able to slide in or out of a cylinder. The plunger is connected to a rack and pinion gear which drives a flywheel inside the cylinder. There can be two counter-rotating flywheels in order to prevent a torque developing. Energy provided in pushing the plunger in will be returned when the plunger moves in the opposite direction, hence the device stores energy rather than dissipates it just like a block of mass. However, the actual mass of the inerter can be very small, an ideal inerter has no mass. Two points on the inerter, the plunger and the cylinder case, can be independently connected to other parts of the mechanical system with neither of them necessarily connected to ground.\n\nSmith's inerter has found an application in Formula One racing where it is known as the J-damper. It is used as an alternative to the now banned tuned mass damper and forms part of the vehicle suspension. It may have been first used secretly by McLaren in 2005 following a collaboration with Smith. Other teams are now believed to be using it. The inerter is much smaller than the tuned mass damper and smoothes out contact patch load variations on the tyres. Smith also suggests using the inerter to reduce machine vibration.\n\nThe difficulty with mass in mechanical analogies is not limited to the mobility analogy. A corresponding problem also occurs in the impedance analogy, but in that case it is ungrounded inductors, rather than capacitors, that cannot be represented with the standard elements.\n\nA mechanical resonator consists of both a mass element and a compliance element. Mechanical resonators are analogous to electrical LC circuits consisting of inductance and capacitance. Real mechanical components unavoidably have both mass and compliance so it is a practical proposition to make resonators as a single component. In fact, it is more difficult to make a pure mass or pure compliance as a single component. A spring can be made with a certain compliance and mass minimised, or a mass can be made with compliance minimised, but neither can be eliminated altogether. Mechanical resonators are a key component of mechanical filters.\n\nAnalogues exist for the active electrical elements of the voltage source and the current source (generators). The mechanical analogue in the mobility analogy of the constant current generator is the constant force generator. The mechanical analogue of the constant voltage generator is the constant velocity generator.\n\nAn example of a constant force generator is the constant-force spring. An example of a practical constant velocity generator is a lightly loaded powerful machine, such as a motor, driving a belt. This is analogous to a real voltage source, such as a battery, which remains near constant-voltage with load provided that the load resistance is much higher than the battery internal resistance.\n\nElectromechanical systems require transducers to convert between the electrical and mechanical domains. They are analogous to two-port networks and like those can be described by a pair of simultaneous equations and four arbitrary parameters. There are numerous possible representations, but the form most applicable to the mobility analogy has the arbitrary parameters in units of admittance. In matrix form (with the electrical side taken as port 1) this representation is,\n\nThe element formula_13 is the short circuit mechanical admittance, that is, the admittance presented by the mechanical side of the transducer when zero voltage (short circuit) is applied to the electrical side. The element formula_14, conversely, is the unloaded electrical admittance, that is, the admittance presented to the electrical side when the mechanical side is not driving a load (zero force). The remaining two elements, formula_15 and formula_16, describe the transducer forward and reverse transfer functions respectively. They are both analogous to transfer admittances and are hybrid ratios of an electrical and mechanical quantity.\n\nThe mechanical analogy of a transformer is a simple machine such as a pulley or a lever. The force applied to the load can be greater or less than the input force depending on whether the mechanical advantage of the machine is greater or less than unity respectively. Mechanical advantage is analogous to the inverse of transformer turns ratio in the mobility analogy. A mechanical advantage less than unity is analogous to a step-up transformer and greater than unity is analogous to a step-down transformer.\n\nThe figure shows a mechanical arrangement of a platform of mass \"M\" that is suspended above the substrate by a spring of stiffness \"S\" and a damper of resistance \"R\". The mobility analogy equivalent circuit is shown to the right of this arrangement and consists of a parallel resonant circuit. This system has a resonant frequency, and may have a natural frequency of oscillation if not too heavily damped.\n\nThe principal advantage of the mobility analogy over its alternative, the impedance analogy, is that it preserves the topology of the mechanical system. Elements that are in series in the mechanical system are in series in the electrical equivalent circuit and elements in parallel in the mechanical system remain in parallel in the electrical equivalent.\n\nThe principal disadvantage of the mobility analogy is that it does not maintain the analogy between electrical and mechanical impedance. Mechanical impedance is represented as an electrical admittance and a mechanical resistance is represented as an electrical conductance in the electrical equivalent circuit. Force is not analogous to voltage (generator voltages are often called electromotive force), but rather, it is analogous to current.\n\nHistorically, the impedance analogy was in use long before the mobility analogy. Mechanical admittance and the associated mobility analogy were introduced by F. A. Firestone in 1932 to overcome the issue of preserving topologies. W. Hähnle independently had the same idea in Germany. Horace M. Trent developed a treatment for analogies in general from a mathematical graph theory perspective and introduced a new analogy of his own.\n\n"}
{"id": "9050760", "url": "https://en.wikipedia.org/wiki?curid=9050760", "title": "Multi-compartment model", "text": "Multi-compartment model\n\nA multi-compartment model is a type of mathematical model used for describing the way materials or energies are transmitted among the \"compartments\" of a system. Each compartment is assumed to be a homogeneous entity within which the entities being modelled are equivalent. For instance, in a pharmacokinetic model, the compartments may represent different sections of a body within which the concentration of a drug is assumed to be uniformly equal.\n\nHence a multi-compartment model is a lumped parameters model.\n\nMulti-compartment models are used in many fields including pharmacokinetics, epidemiology, biomedicine, systems theory, complexity theory, engineering, physics, information science and social science. The circuits systems can be viewed as a multi-compartment model as well.\n\nIn systems theory, it involves the description of a network whose components are compartments that represent a population of elements that are equivalent with respect to the manner in which they process input signals to the compartment.\nMost commonly, the mathematics of multi-compartment models is simplified to provide only a single parameter—such as concentration—within a compartment.\n\nPossibly the simplest application of multi-compartment model is in the single-cell concentration monitoring (see the figure above). If the volume of a cell is \"V\", the mass of solute is \"q\", the input is \"u\"(\"t\") and the secretion of the solution is proportional to the density of it within the cell, then the concentration of the solution \"C\" within the cell over time is given by\n\nwhere \"k\" is the proportionality.\n\nAs the number of compartments increases, the model can be very complex and the solutions usually beyond ordinary calculation.\n\nThe formulae for n-cell multi-compartment models become:\n\nWhere \n\nOr in matrix forms:\n\nWhere\n\nIn the special case of a closed system (see below) i.e. where formula_9 then there is a general solution.\n\nWhere formula_11, formula_12, ... and formula_13 are the eigenvalues of formula_14; formula_15, formula_16, ... and formula_17 are the respective eigenvectors of formula_14; and formula_19, formula_20, ... and formula_21 are constants.\n\nHowever it can be shown that given the above requirement to ensure the 'contents' of a closed system are constant, then for every pair of eigenvalue and eigenvector then either formula_22 or formula_23 and also that one eigenvalue is 0, say formula_11\n\nSo\n\nWhere\n\nThis solution can be rearranged:\n\nThis somewhat inelegant equation demonstrates that all solutions of an \"n-cell\" multi-compartment model with constant or no inputs are of the form:\n\nWhere formula_30 is a \"nxn\" matrix and formula_12, formula_32, ... and formula_13 are constants.\nWhere formula_34\n\nGenerally speaking, as the number of compartments increase, it is challenging both to find the algebraic and numerical solutions of the model. However, there are special cases of models, which rarely exist in nature, when the topologies exhibit certain regularities that the solutions become easier to find. The model can be classified according to the interconnection of cells and input/output characteristics:\n\n\n\n"}
{"id": "19445663", "url": "https://en.wikipedia.org/wiki?curid=19445663", "title": "Oceanic dispersal", "text": "Oceanic dispersal\n\nOceanic dispersal is a type of biological dispersal that occurs when terrestrial organisms transfer from one land mass to another by way of a sea crossing. Often this occurs via large rafts of floating vegetation such as are sometimes seen floating down major rivers in the tropics and washing out to sea, occasionally with animals trapped on them. Dispersal via such a raft is sometimes referred to as a \"rafting event\".\n\nColonization of land masses by plants can also occur via long-distance oceanic dispersal of floating seeds.\n\nRafting has played an important role in the colonization of isolated land masses by mammals. Prominent examples include Madagascar, which has been isolated for ~120 million years (Ma), and South America, which was isolated for much of the Cenozoic. Both land masses, for example, appear to have received their primates by this mechanism. According to genetic evidence, the common ancestor of the lemurs of Madagascar appears to have crossed the Mozambique Channel by rafting between 50 and 60 Ma ago. Likewise, the New World monkeys are thought to have originated in Africa and rafted to South America by the Oligocene, when the continents were much closer than they are today. Madagascar also appears to have received its tenrecs (25–42 Ma ago), nesomyid rodents (20–24 Ma ago) and euplerid carnivorans (19–26 Ma ago) by this route and South America its caviomorph rodents (over 30 Ma ago). Simian primates (ancestral to monkeys) and hystricognath rodents (ancestral to caviomorphs) are believed to have previously rafted from Asia to Africa about 40 Ma ago.\n\nAmong reptiles, several iguanid species in the South Pacific have been hypothesized to be descended from iguanas that rafted from Central or South America (an alternative theory involves dispersal of a putative now-extinct iguana lineage from Australia or Asia). Similarly, a number of clades of American geckos seem to have rafted over from Africa during both the Paleogene and Neogene. Skinks of the related genera \"Mabuya\" and \"Trachylepis\" also apparently both floated across the Atlantic from Africa to South America and Fernando de Noronha, respectively, during the last 9 Ma. Skinks from the same group have also rafted from Africa to Cape Verde, Madagascar, the Seychelles, the Comoros and Socotra. (Among lizards, skinks and geckos seem especially capable of surviving long transoceanic journeys.) Surprisingly, even burrowing amphisbaenians and blind snakes appear to have rafted from Africa to South America.\n\nAn example of a bird that is thought to have reached its present location by rafting is the weak-flying South American hoatzin, whose ancestors apparently floated over from Africa.\n\nColonization of groups of islands can occur by an iterative rafting process sometimes called island hopping. Such a process appears to have played a role, for example, in the colonization of the Caribbean by mammals of South American origin (including caviomorphs and monkeys).\n\nA remarkable example of iterative rafting has been proposed for spiders of the genus \"Amaurobioides\". Members of this genus inhabit coastal sites and build silken cells which they seal at high tide; however, they do not balloon. DNA sequence analysis suggests that ancestors of the genus dispersed from southern South America to South Africa about 10 million years (Ma) ago, where the most basal clade is found; subsequent rafting events then took the genus eastward with the Antarctic Circumpolar Current to Australia, then to New Zealand and finally to Chile by about 2 Ma ago. Another example among spiders is the species \"Moggridgea rainbowi\", the only Australian member of a genus otherwise endemic to Africa, with a divergence date of 2 to 16 Ma ago.\n\nHowever, oceanic dispersal of terrestrial species may not always take the form of rafting; in some cases, swimming or simply floating may suffice. Tortoises of the genus \"Chelonoidis\" arrived in South America from Africa in the Oligocene; they were probably aided by their ability to float with their heads up, and to survive up to six months without food or fresh water. South American tortoises then went on to colonize the West Indies and Galápagos Islands. The dispersal of anthracotheres from Asia to Africa about 40 Ma ago, and the much more recent dispersal of hippos (relatives and probably descendants of anthracotheres) from Africa to Madagascar may have occurred by floating or swimming.\n\nThe first documented example of colonization of a land mass by rafting occurred in the aftermath of hurricanes Luis and Marilyn in the Caribbean in 1995. A raft of uprooted trees carrying fifteen or more green iguanas was observed by fishermen landing on the east side of Anguilla – an island where they had never before been recorded. The iguanas had apparently been caught on the trees and rafted two hundred miles across the ocean from Guadeloupe, where they are indigenous. Examination of the weather patterns and ocean currents indicated that they had probably spent three weeks at sea before landfall. This colony began breeding on the new island within two years of its arrival.\n\nThe advent of human civilization has created opportunities for organisms to raft on floating artifacts, which may be more durable than natural floating objects. This phenomenon was noted following the 2011 Tōhoku tsunami in Japan, with about 300 species found to have been carried on debris by the North Pacific Current to the west coast of North America (although no colonizations have been detected thus far).\n\n\n"}
{"id": "188379", "url": "https://en.wikipedia.org/wiki?curid=188379", "title": "Operant conditioning chamber", "text": "Operant conditioning chamber\n\nAn operant conditioning chamber (also known as the Skinner box) is a laboratory apparatus used to study animal behavior. The operant conditioning chamber was created by B. F. Skinner while he was a graduate student at Harvard University. It may have been inspired by Jerzy Konorski's studies. It is used to study both operant conditioning and classical conditioning.\n\nSkinner created the operant chamber as a variation of the puzzle box originally created by Edward Thorndike.\n\nAn operant conditioning chamber permits experimenters to study behavior conditioning (training) by teaching a subject animal to perform certain actions (like pressing a lever) in response to specific stimuli, such as a light or sound signal. When the subject correctly performs the behavior, the chamber mechanism delivers food or another reward. In some cases, the mechanism delivers a punishment for incorrect or missing responses. For instance, to test how operant conditioning works for certain invertebrates, such as fruit flies, psychologists use a device known as a \"heat box\". Essentially this takes up the same form as the Skinner box, but the box is composed of two sides: one side that can undergo temperature change and the other that does not. As soon as the invertebrate crosses over to the side that can undergo a temperature change, the area is heated up. Eventually, the invertebrate will be conditioned to stay on the side that does not undergo a temperature change. This goes to the extent that even when the temperature is turned to its lowest point, the fruit fly will still refrain from approaching that area of the heat box. These types of apparatuses allow experimenters to perform studies in conditioning and training through reward/punishment mechanisms.\n\nThe structure forming the shell of a chamber is a box large enough to easily accommodate the animal being used as a subject. (Commonly used model animals include rodents—usually lab rats—pigeons, and primates). It is often sound-proof and light-proof to avoid distracting stimuli.\n\nOperant chambers have at least one operandum (or \"manipulandum\"), and often two or more, that can automatically detect the occurrence of a behavioral response or action. Typical operanda for primates and rats are response levers; if the subject presses the lever, the opposite end moves and closes a switch that is monitored by a computer or other programmed device. Typical operanda for pigeons and other birds are response keys with a switch that closes if the bird pecks at the key with sufficient force. The other minimal requirement of a conditioning chamber is that it has a means of delivering a primary reinforcer (a reward, such as food, etc.) or unconditioned stimulus like food (usually pellets) or water. It can also register the delivery of a conditioned reinforcer, such as an LED (see Jackson & Hackenberg 1996 in the \"Journal of the Experimental Analysis of Behavior\" for example) signal as a \"token\".\n\nDespite such a simple configuration, one operandum and one feeder, it is possible to investigate many psychological phenomena. Modern operant conditioning chambers typically have many operanda, like many response levers, two or more feeders, and a variety of devices capable of generating many stimuli, including lights, sounds, music, figures, and drawings. Some configurations use an LCD panel for the computer generation of a variety of visual stimuli.\n\nSome operant chambers can also have electrified nets or floors so that shocks can be given to the animals; or lights of different colors that give information about when the food is available.\nAlthough the use of shock is not unheard of, approval may be needed in countries that regulate experimentation on animals.\n\nOperant conditioning chambers have become common in a variety of research disciplines including behavioral pharmacology. The results of these experiments inform many disciplines outside of psychology, such as behavioral economics.\n\nAn urban legend spread concerning Skinner putting his daughter through an experiment such as this, causing great controversy. His daughter later debunked this.\n\nSlot machines and online games are sometimes cited as examples of human devices that use sophisticated operant schedules of reinforcement to reward repetitive actions.\n\nSocial networking services such as Google, Facebook and Twitter have been identified as using the techniques, critics use terms such as \"Skinnerian Marketing\" for the way the companies use the ideas to keep users engaged and using the service.\n\nGamification, the technique of using game design elements in non-game contexts, has also been described as using operant conditioning and other behaviorist techniques to encourage desired user behaviors.\n\nSkinner is noted to have said that he did not want to be an eponym. Further, he believed that Clark Hull and his Yale students coined the expression: Skinner stated he did not use the term himself, and went so far as to ask Howard Hunt to use \"lever box\" instead of \"Skinner box\" in a published document.\n\n\n"}
{"id": "21991755", "url": "https://en.wikipedia.org/wiki?curid=21991755", "title": "Parity (charity)", "text": "Parity (charity)\n\nParity is a United Kingdom-based equal rights organisation, which describes itself as campaigning to promote and protect the equality of men and women under the law. Its main focus has been in the area of state pensions and associated benefits, and most of its notable successes have occurred in this field. The organisation was previously called Campaign for Equal State Pension Ages.\n\nParity was formed in 1986 as the \"Campaign for Equal State Pension Ages\" (CESPA) and its principal aim was to obtain for men the same [state pension] age as enjoyed by women. The organisation changed its name to \"Parity\" in 1997 to reflect growing concern regarding increased unequal treatment of men and women in other areas by the state.\n\nCESPA's inaugural meeting was held on 29 August 1986 in Committee Room 1 of Manchester Town Hall. The ten founder members were G.W. Alderton, D.Higgins, D.J.D Yarwood, J.H Bennett, E.L Anderson, J.Greenwood, M.D Davidson, J.Graham, D.G Lindsay, and J. Bradfield.\nThe original CESPA constitution was agreed. The meeting appointed David Lindsay as Chairman, Geoff Alderton as Vice-Chairman, David Yarwood as Hon Secretary and John Bennett as Treasurer.\n\nParity was denied charitable status for a number of years because it was seeking changes in the law to redress statutory sex discrimination, and thus was deemed to be political. It finally gained charitable status in 2005 following a change to the Human Rights Act.\n\nThe organisation is run entirely by volunteers and receives no public funding. It had an income of £1,016 in 2017.\n\nIn its constitution it states its objectives are \"to promote and protect the equal rights of men and women to the enjoyment of all civil, political, economic social and cultural rights under the law\" and \"to institute proceedings in the UK or appropriate European Courts for the purpose of establishing or protecting any such equal rights.\"\n\nParity has had some notable successes over the years, particularly in addressing statutory sex discrimination. Their campaigns have stopped government sex discrimination which, in the case of winter fuel payments, had denied an estimated £20 million per year to males between the ages of 60-65.\n\nParity claims successes in four main areas: prescription charges, winter fuel payments, bus travel concessions and, in association with others, widower's benefits:\n\nParity's first major success began in 1993 under its original name, when CESPA member Cyril Richardson, an asthmatic, took the government to court over sex discrimination in entitlement for free prescriptions. Eventually, in 1995 the European Court of Justice ruled that it was unlawful to charge men aged 60–65 for prescriptions when they were free to women. As a result, men now receive benefits in the order of an estimated £30 million per annum. Additionally, £10 million in charges was refunded to those who had wrongly paid for prescriptions in the previous 3 months before the ruling.\n\nIn 1998 Parity member John Taylor went to the High Court to contest the fact that the government was denying winter fuel payments to men aged 60–65 that women were able to receive. It was argued that this was a breach of European laws on equality in social security, and that such discrimination was blatant and unjustifiable. The case was referred to the European Court of Justice who ruled in Mr Taylor's favour in December 1999. The ruling meant up to £26 million per annum in benefits being given to men that otherwise would have been denied.\n\nIn 2000 Parity took the Government to the European Court of Human Rights over the fact that it denied free bus travel to men aged 60–65. Despite Parity's previous successes, the government had refused to end discrimination in this area. However, in June 2001 it became clear that Parity would win the case if it went to court and the government relented, with John Prescott announcing men would receive free bus passes from age 60. Controversially, Prescott hailed the move as \"another example of the Government providing extra help for pensioners\" whereas it had effectively been forced into the move by the European Court of Human Rights. The Travel Concessions (Eligibility) Bill finally passed in 2003 resulting in males receiving the £50 million per annum in benefits that the state had denied them.\n\nParity campaigned to end sexism in the payment of widows benefits and bereavement tax allowances. These were previously only paid to women and not men. The campaign achieved success in 2001 as a result the benefits are now available to both genders.\n\nIn 2008 Parity campaigned to stop the Crown Prosecution Service making false statements that the \"overwhelming\" majority of domestic violence victims were women, whereas in fact about one in three victims is male. With the aid of the UK Statistics Authority the organisation succeeded and the incorrect claims were removed. The organisation also forced Gillian Morgan of the Welsh Assembly to stop making the same claims in the \"Strategic Action Plan to Address Violence to Women\".\n\nParity's main current campaign is to equalise the state pension ages for men and women in the United Kingdom. Currently this is now due to take place in 2018.\n\nOther objectives include:\n\n\nParity's present patrons are Sir Peter Bottomley MP and Mr Mark Brooks.\n\n"}
{"id": "31353487", "url": "https://en.wikipedia.org/wiki?curid=31353487", "title": "Posetal category", "text": "Posetal category\n\nIn mathematics, a posetal category, or thin category, is a category whose homsets each contain at most one morphism. As such, a posetal category amounts to a preordered class (or a preordered set, if its objects form a set). As suggested by the name, the further requirement that the category be skeletal is often assumed for the definition of \"posetal\"; in the case of a category that is posetal, being skeletal is equivalent to the requirement that the only isomorphisms are the identity morphisms, equivalently that the preordered class satisfies antisymmetry and hence, if a set, is a poset.\n\nAll diagrams commute in a posetal category. When the commutative diagrams of a category are interpreted as a typed equational theory whose objects are the types, a codiscrete posetal category corresponds to an inconsistent theory understood as one satisfying the axiom \"x\" = \"y\" at all types.\n\nViewing a 2-category as an enriched category whose hom-objects are categories, the hom-objects of any extension of a posetal category to a 2-category having the same 1-cells are monoids.\n\nSome lattice-theoretic structures are definable as posetal categories of a certain kind, usually with the stronger assumption of being skeletal. For example, a poset may be defined as a posetal category, a distributive lattice as a posetal distributive category, a Heyting algebra as a posetal finitely cocomplete cartesian closed category, and a Boolean algebra as a posetal finitely cocomplete *-autonomous category. Conversely, categories, distributive categories, finitely cocomplete cartesian closed categories, and finitely cocomplete *-autonomous categories can be considered the respective categorifications of posets, distributive lattices, Heyting algebras, and Boolean algebras.\n"}
{"id": "14652078", "url": "https://en.wikipedia.org/wiki?curid=14652078", "title": "Powered Descent Initiation", "text": "Powered Descent Initiation\n\nPowered Descent Initiation (PDI) is a term used during the Apollo program Moon landing missions to describe the maneuver of the Apollo Lunar Module as it descended from lunar orbit to landing. \n\n\"\"Eagle\" was GO to ignite its descent engine, and Armstrong and Aldrin locked their eyes to the glowing numbers displayed before them. They were almost at an invisible junction of height, speed, range, and time when everything would join together for commitment. When the instruments told them that they were 192 miles from their projected landing site, and were precisely 50,174 radar~measured feet above the long shadows of the moon, they would unleash decelerating thrust and begin slowing their speed for the touchdown...this was it. PDI. Powered Descent Initiate.\"\n"}
{"id": "3735409", "url": "https://en.wikipedia.org/wiki?curid=3735409", "title": "Psychological resilience", "text": "Psychological resilience\n\nPsychological resilience is the ability to successfully cope with a crisis and to return to pre-crisis status quickly. Resilience exists when the person uses \"mental processes and behaviors in promoting personal assets and protecting self from the potential negative effects of stressors\". In simpler terms, psychological resilience exists in people who develop psychological and behavioral capabilities that allow them to remain calm during crises/chaos and to move on from the incident without long-term negative consequences. Psychological resilience is an evolutionary advantage that most people have and use to manage normal stressors.\n\nResilience is generally thought of as a \"positive adaptation\" after a stressful or adverse situation. When a person is \"bombarded by daily stress, it disrupts their internal and external sense of balance, presenting challenges as well as opportunities\". Resilience is the integrated adaptation of physical, mental and spiritual aspects in a set of \"good or bad\" circumstances, a coherent sense of self that is able to maintain normative developmental tasks that occur at various stages of life.\nThe Children's Institute of the University of Rochester explains that \"resilience research is focused on studying those who engage in life with hope and humor despite devastating losses\".\nIt is important to note that resilience is not only about overcoming a deeply stressful situation, but also coming out of the said situation with \"competent functioning\". Resiliency allows a person to rebound from adversity as a strengthened and more resourceful person.\n\nThe first research on resilience was published in 1973. The study used epidemiology, which is the study of disease prevalence, to uncover the risks and the protective factors that now help define resilience. A year later, the same group of researchers created tools to look at systems that support development of resilience.\n\nEmmy Werner was one of the early scientists to use the term \"resilience\" in the 1970s. She studied a cohort of children from Kauai, Hawaii. Kauai was quite poor and many of the children in the study grew up with alcoholic or mentally ill parents. Many of the parents were also out of work. Werner noted that of the children who grew up in these detrimental situations, two-thirds exhibited destructive behaviors in their later teen years, such as chronic unemployment, substance abuse, and out-of-wedlock births (in case of teenage girls). However, one-third of these youngsters did not exhibit destructive behaviours. Werner called the latter group 'resilient'. Thus, resilient children and their families were those who, by definition, demonstrated traits that allowed them to be more successful than non-resilient children and families.\n\nResilience also emerged as a major theoretical and research topic from the studies of children with mothers diagnosed with schizophrenia in the 1980s. In a 1989 study, the results showed that children with a schizophrenic parent may not obtain an appropriate level of comforting caregiving—compared to children with healthy parents—and that such situations often had a detrimental impact on children's development. On the other hand, some children of ill parents thrived well and were competent in academic achievement, and therefore led researchers to make efforts to understand such responses to adversity.\n\nSince the onset of the research on resilience, researchers have been devoted to discovering the protective factors that explain people's adaptation to adverse conditions, such as maltreatment, catastrophic life events, or urban poverty. The focus of empirical work then has been shifted to understand the underlying protective processes. Researchers endeavor to uncover how some factors (e.g. connection to family) may contribute to positive outcomes.\n\nIn all these instances, resilience is best understood as a process. It is often mistakenly assumed to be a trait of the individual, an idea more typically referred to as \"resiliency\". Most research now shows that resilience is the result of individuals being able to interact with their environments and the processes that either promote well-being or protect them against the overwhelming influence of risk factors. It is essential to understand the process or this cycle of resiliency. When people are faced with an adverse condition, there are three ways in which they may approach the situation.\n\n\nOnly the third approach promotes well-being. It is employed by resilient people, who become upset about the disruptive state and thus change their current pattern to cope with the issue. The first and second approaches lead people to adopt the victim role by blaming others and rejecting any coping methods even after the crisis is over. These people prefer to instinctively react, rather than respond to the situation. Those who respond to the adverse conditions by adapting themselves tend to cope, spring back, and halt the crisis. Negative emotions involve fear, anger, anxiety, distress, helplessness, and hopelessness which decrease a person's ability to solve the problems they face and weaken a person's resiliency. Constant fears and worries weaken people's immune system and increase their vulnerability to illnesses.\n\nThese processes include individual coping strategies, or may be helped by a protective environment like good families, schools, communities, and social policies that make resilience more likely to occur. In this sense \"resilience\" occurs when there are cumulative \"protective factors\". These factors are likely to play a more important role, the greater the individual's exposure to cumulative risk factors.\n\nThree notable bases for resilience—self-confidence, self-esteem and self-concept—all have roots in three different nervous systems—respectively, the somatic nervous system, the autonomic nervous system and the central nervous system.\n\nAn emerging field in the study of resilience is the neurobiological basis of resilience to stress. For example, neuropeptide Y (NPY) and 5-Dehydroepiandrosterone (5-DHEA) are thought to limit the stress response by reducing sympathetic nervous system activation and protecting the brain from the potentially harmful effects of chronically elevated cortisol levels respectively. In addition, the relationship between social support and stress resilience is thought to be mediated by the oxytocin system's impact on the hypothalamic-pituitary-adrenal axis. \"Resilience, conceptualized as a positive bio-psychological adaptation, has proven to be a useful theoretical context for understanding variables for predicting long-term health and well-being\".\n\nThere is some limited research that, like trauma, resilience is epigenetic—that is, it may be inherited—but the science behind this finding is preliminary.\n\nStudies show that there are several factors which develop and sustain a person's resilience:\n\n\nResilience is negatively correlated with personality traits of neuroticism and negative emotionality, which represents tendencies to see and react to the world as threatening, problematic, and distressing, and to view oneself as vulnerable. Positive correlations stands with personality traits of openness and positive emotionality, that represents tendencies to engage and confront the world with confidence in success and a fair value to self-directedness.\n\nThere is significant research found in scientific literature on the relationship between positive emotions and resilience. Studies show that maintaining positive emotions whilst facing adversity promote flexibility in thinking and problem solving. Positive emotions serve an important function in their ability to help an individual recover from stressful experiences and encounters. That being said, maintaining a positive emotionality aids in counteracting the physiological effects of negative emotions. It also facilitates adaptive coping, builds enduring social resources, and increases personal well-being.\n\nFormation of conscious perception and monitoring one's own socioemotional factors is considered as a stability aspect of positive emotions. This is not to say that positive emotions are merely a by-product of resilience, but rather that feeling positive emotions during stressful experiences may have adaptive benefits in the coping process of the individual. Empirical evidence for this prediction arises from research on resilient individuals who have a propensity for coping strategies that concretely elicit positive emotions, such as benefit-finding and cognitive reappraisal, humor, optimism, and goal-directed problem-focused coping. Individuals who tend to approach problems with these methods of coping may strengthen their resistance to stress by allocating more access to these positive emotional resources. Social support from caring adults encouraged resilience among participants by providing them with access to conventional activities.\n\nPositive emotions not only have physical outcomes but also physiological ones. Some physiological outcomes caused by humor include improvements in immune system functioning and increases in levels of salivary immunoglobulin A, a vital system antibody, which serves as the body's first line of defense in respiratory illnesses. Moreover, other health outcomes include faster injury recovery rate and lower readmission rates to hospitals for the elderly, and reductions in a patient's stay in the hospital, among many other benefits. A study was done on positive emotions in trait-resilient individuals and the cardiovascular recovery rate following negative emotions felt by those individuals. The results of the study showed that trait-resilient individuals experiencing positive emotions had an acceleration in the speed in rebounding from cardiovascular activation initially generated by negative emotional arousal, i.e. heart rate and the like.\n\nGrit refers to the perseverance and passion for long-term goals. This is characterized as working persistently towards challenges, maintained effort and interest over years despite negative feedback, adversity, plateaus in progress, or failure. High grit people view accomplishments as a marathon rather than an immediate goal. High grit individuals display a sustained and focused application of self in problematic situations than less gritty individuals.\n\nGrit affects the effort a person contributes by acting on the importance pathway. When people value a goal as more valuable, meaningful, or relevant to their self-concept they are willing to expend more effort on it when necessary. The influence of individual differences in grit results in different levels of effort-related cardiac activity when gritty and less gritty individuals performed the same task. Grit is associated with differences in potential motivation, one pathway in motivational intensity theory. Grit may also influence an individual's perception of task difficulty.\n\nGrit was highly correlated with the Big Five conscientiousness trait. Although grit and conscientiousness highly overlap in their achievement aspects, they differ in their emphasis. Grit emphasizes long-term stamina, whereas conscientiousness focuses on short-term intensity.\n\nGrit varies with level of education and age. More educated adults tend to be higher in grit than less educated individuals of the same age. Post college graduates report higher grit levels than most other education level groups. Grit increases with age when education level is controlled for.\n\nIn life achievements, grit may be as important as talent. College students at an elite university who scored high in grit also earned higher GPAs than their classmates, despite having lower SAT scores. In a study at the West Point military academy it was found that grit was a more reliable predictor of first summer retention than self-control or a summary measure of cadet quality. Gritty competitors at the Scripps National Spelling Bee outranked other competitors who scored lower in grit, at least partially due to accumulated practice.\n\nGrit may also serve as a protective factor against suicide. A study at Stanford University found that grit was predictive of psychological health and well-being in medical residents. Gritty individuals possess self-control and regular commitment to goals that allows them to resist impulses, such as to engage in self-harm. Individuals high in grit also focus on future goals, which may stop them from attempting suicide. It is believed that because grit encourages individuals to create and sustain life goals, these goals provide meaning and purpose in life. Grit alone does not seem to be sufficient, however. Only individuals with high gratitude and grit have decreased suicidal ideation over long periods of time. Gratitude and grit work together to enhance meaning in life, offering protection against death and suicidal thoughts or plans.\n\nA study was conducted among high achieving professionals who seek challenging situations that require resilience. Research has examined 13 high achievers from various professions, all of whom had experienced challenges in the workplace and negative life events over the course of their careers but who had also been recognized for their great achievements in their respective fields. Participants were interviewed about everyday life in the workplace as well as their experiences with resilience and thriving. The study found six main predictors of resilience: positive and proactive personality, experience and learning, sense of control, flexibility and adaptability, balance and perspective, and perceived social support. High achievers were also found to engage in many activities unrelated to their work such as engaging in hobbies, exercising, and organizing meetups with friends and loved ones.\n\nSeveral factors are found to modify the negative effects of adverse life situations. Many studies show that the primary factor for the development of resilience is social support. While many competing definitions of social support exist, most can be thought of as the degree of access to, and use of, strong ties to other individuals who are similar to one’s self. Social support requires not only that you have relationships with others, but that these relationships involve the presence of solidarity and trust, intimate communication, and mutual obligation both within and outside the family. Additional factors are also associated with resilience, like the capacity to make realistic plans, having self-confidence and a positive self image, developing communications skills, and the capacity to manage strong feelings and impulses.\n\nTemperamental and constitutional disposition is considered as a major factor in resilience. It is one of the necessary precursors of resilience along with warmth in family cohesion and accessibility of prosocial support systems. There are three kinds of temperamental systems that play part in resilience, they are the appetitive system, defensive system and attentional system.\n\nAnother protective factor is related to moderating the negative effects of environmental hazards or a stressful situation in order to direct vulnerable individuals to optimistic paths, such as external social support. More specifically a 1995 study distinguished three contexts for protective factors:\n\nFurthermore, a study of the elderly in Zurich, Switzerland, illuminated the role humor plays as a coping mechanism to maintain a state of happiness in the face of age-related adversity.\n\nBesides the above distinction on resilience, research has also been devoted to discovering the individual differences in resilience. Self-esteem, ego-control, and ego-resiliency are related to behavioral adaptation. For example, maltreated children who feel good about themselves may process risk situations differently by attributing different reasons to the environments they experience and, thereby, avoid producing negative internalized self-perceptions. Ego-control is \"the threshold or operating characteristics of an individual with regard to the expression or containment\" of their impulses, feelings, and desires. Ego-resilience refers to \"dynamic capacity, to modify his or her model level of ego-control, in either direction, as a function of the demand characteristics of the environmental context\"\n\nMaltreated children who experienced some risk factors (e.g., single parenting, limited maternal education, or family unemployment), showed lower ego-resilience and intelligence than nonmaltreated children. Furthermore, maltreated children are more likely than nonmaltreated children to demonstrate disruptive-aggressive, withdraw, and internalized behavior problems. Finally, ego-resiliency, and positive self-esteem were predictors of competent adaptation in the maltreated children.\n\nDemographic information (e.g., gender) and resources (e.g., social support) are also used to predict resilience. Examining people's adaptation after disaster showed women were associated with less likelihood of resilience than men. Also, individuals who were less involved in affinity groups and organisations showed less resilience.\n\nCertain aspects of religions and spirituality may, hypothetically, promote or hinder certain psychological virtues that increase resilience. Research has not established connection between spirituality and resilience. According to the 4th edition of \"Psychology of Religion\" by Hood, et al., the \"study of positive psychology is a relatively new development...there has not yet been much direct empirical research looking specifically at the association of religion and ordinary strengths and virtues\". In a review of the literature on the relationship between religiosity/spirituality and PTSD, amongst the significant findings, about half of the studies showed a positive relationship and half showed a negative relationship between measures of religiosity/spirituality and resilience. The United States Army has received criticism for promoting spirituality in its new [Comprehensive Soldier Fitness] program as a way to prevent PTSD, due to the lack of conclusive supporting data.\n\nIn military studies it has been found that resilience is also dependent on group support: unit cohesion and morale is the best predictor of combat resiliency within a unit or organization. Resilience is highly correlated to peer support and group cohesion. Units with high cohesion tend to experience a lower rate of psychological breakdowns than units with low cohesion and morale. High cohesion and morale enhance adaptive stress reactions.\n\nIn cognitive behavioral therapy, building resilience is a matter of mindfully changing basic behaviors and thought patterns. The first step is to change the nature of self-talk. Self-talk is the internal monologue people have that reinforce beliefs about the person's self-efficacy and self-value. To build resilience, the person needs to eliminate negative self-talk, such as \"I can't do this\" and \"I can't handle this\", and to replace it with positive self-talk, such as \"I can do this\" and \"I can handle this\". This small change in thought patterns helps to reduce psychological stress when a person is faced with a difficult challenge. The second step a person can take to build resilience is to be prepared for challenges, crises, and emergencies. In business, preparedness is created by creating emergency response plans, business continuity plans, and contingency plans. For personal preparedness, the individual can create a financial cushion to help with economic crises, he/she can develop social networks to help him/her through trying personal crises, and he/she can develop emergency response plans for his/her household.\n\nResilience is also enhanced by developing effective coping skills for stress. Coping skills help the individual to reduce stress levels, so they remain functional. Coping skills include using meditation, exercise, socialization, and self-care practices to maintain a healthy level of stress, but there are many other lists associated with psychological resilience.\n\nThe American Psychological Association suggests \"10 Ways to Build Resilience\", which are:\n\n\nThe Besht model of natural resilience building in an ideal family with positive access and support from family and friends, through parenting illustrates four key markers. They are:\n\n\nIn this model, self-efficacy is the belief in one's ability to organize and execute the courses of action required to achieve necessary and desired goals and hardiness is a composite of interrelated attitudes of commitment, control, and challenge.\n\nA number of self-help approaches to resilience-building have been developed, drawing mainly on the theory and practice of cognitive behavioral therapy (CBT) and rational emotive behavior therapy (REBT). For example, a group cognitive-behavioral intervention, called the Penn Resiliency Program (PRP), has been shown to foster various aspects of resilience. A meta-analysis of 17 PRP studies showed that the intervention significantly reduces depressive symptoms over time.\n\nThe idea of 'resilience building' is debatably at odds with the concept of resilience as a process, since it is used to imply that it is a developable characteristic of oneself. Those who view resilience as a description of doing well despite adversity, view efforts of 'resilience building' as method to encourage resilience. Bibliotherapy, positive tracking of events, and enhancing psychosocial protective factors with positive psychological resources are other methods for resilience building. In this way, increasing an individual's resources to cope with or otherwise address the negative aspects of risk or adversity is promoted, or builds, resilience.\n\nContrasting research finds that strategies to regulate and control emotions, in order to enhance resilience, allows for better outcomes in the event of mental illness. While initial studies of resilience originated with developmental scientists studying children in high-risk environments, a study on 230 adults diagnosed with depression and anxiety that emphasized emotional regulation, showed that it contributed to resilience in patients. These strategies focused on planning, positively reappraising events, and reducing rumination helped in maintaining a healthy continuity. Patients with improved resilience were found to yield better treatment outcomes than patients with non-resilience focused treatment plans, providing potential information for supporting evidence based psychotherapeutic interventions that may better handle mental disorders by focusing on the aspect of psychological resilience.\n\nThe Head Start program was shown to promote resilience. So was the Big Brothers Big Sisters Programme, the Abecedarian Early Intervention Project, and social programs for youth with emotional or behavioral difficulties.\n\nTuesday's Children, a family service organization that made a long-term commitment to the individuals that have lost loved ones to 9/11 and terrorism around the world, works to build psychological resilience through programs such as Mentoring and Project COMMON BOND, an 8-day peace-building and leadership initiative for teens, ages 15–20, from around the world who have been directly impacted by terrorism.\n\nMilitary organizations test personnel for the ability to function under stressful circumstances by deliberately subjecting them to stress during training. Those students who do not exhibit the necessary resilience can be screened out of the training. Those who remain can be given stress inoculation training. The process is repeated as personnel apply for increasingly demanding positions, such as special forces.\n\nResilience in children refers to individuals who are doing better than expected, given a history that includes risk or adverse experience. Once again, it is not a trait or something that some children simply possess. There is no such thing as an 'invulnerable child' that can overcome any obstacle or adversity that he or she encounters in life—and in fact, the trait is quite common. Resilience is the product of a number of developmental processes over time, that has allowed children experience small exposures to adversity or some sort of age appropriate challenges to develop mastery and continue to develop competently. This gives children a sense of personal pride and self-worth.\n\nResearch on 'protective factors', which are characteristics of children or situations that particularly help children in the context of risk has helped developmental scientists to understand what matters most for resilient children. Two of these that have emerged repeatedly in studies of resilient children are good cognitive functioning (like cognitive self-regulation and IQ) and positive relationships (especially with competent adults, like parents). Children who have protective factors in their lives tend to do better in some risky contexts when compared to children without protective factors in the same contexts. However, this is not a justification to expose any child to risk. Children do better when not exposed to high levels of risk or adversity.\n\nResilient children within classroom environments have been described as working and playing well and holding high expectations, have often been characterized using constructs such as locus of control, self-esteem, self-efficacy, and autonomy. All of these things work together to prevent the debilitating behaviors that are associated with learned helplessness.\n\nCommunities play a huge role in fostering resilience. The clearest sign of a cohesive and supportive community is the presence of social organizations that provide healthy human development. Services are unlikely to be used unless there is good communication concerning them. Children who are repeatedly relocated do not benefit from these resources, as their opportunities for resilience-building, meaningful community participation are removed with every relocation.\n\nFostering resilience in children requires family environments that are caring and stable, hold high expectations for children's behavior and encourage participation in the life of the family. Most resilient children have a strong relationship with at least one adult, not always a parent, and this relationship helps to diminish risk associated with family discord. The definition of parental resilience, as the capacity of parents to deliver a competent and quality level of parenting to children, despite the presence of risk factors, has proven to be a very important role in children's resilience. Understanding the characteristics of quality parenting is critical to the idea of parental resilience. Even if divorce produces stress, the availability of social support from family and community can reduce this stress and yield positive outcomes. Any family that emphasizes the value of assigned chores, caring for brothers or sisters, and the contribution of part-time work in supporting the family helps to foster resilience. Resilience research has traditionally focused on the well being of children, with limited academic attention paid to factors that may contribute to the resilience of parents.\n\nNumerous studies have shown that some practices that poor parents utilize help promote resilience within families. These include frequent displays of warmth, affection, emotional support; reasonable expectations for children combined with straightforward, not overly harsh discipline; family routines and celebrations; and the maintenance of common values regarding money and leisure. According to sociologist Christopher B. Doob, \"Poor children growing up in resilient families have received significant support for doing well as they enter the social world—starting in daycare programs and then in schooling.\"\n\nBeyond preventing bullying, it is also important to consider how interventions based on emotional intelligence (EI) are important in the case that bullying does occur. Increasing EI may be an important step in trying to foster resilience among victims. When a person faces stress and adversity, especially of a repetitive nature, their ability to adapt is an important factor in whether they have a more positive or negative outcome.\n\nA 2013 study examined adolescents who illustrated resilience to bullying and found some interesting gendered differences, with higher behavioral resilience found among girls and higher emotional resilience found among boys. Despite these differences, they still implicated internal resources and negative emotionality in either encouraging or being negatively associated with resilience to bullying respectively and urged for the targeting of psychosocial skills as a form of intervention. Emotional intelligence has been illustrated to promote resilience to stress and as mentioned previously, the ability to manage stress and other negative emotions can be preventative of a victim going on to perpetuate aggression. One factor that is important in resilience is the regulation of one's own emotions. Schneider et al. (2013) found that emotional perception was significant in facilitating lower negative emotionality during stress and Emotional Understanding facilitated resilience and has a positive correlation with positive affect.\n\nTransgender youth experience a wide range of abuse and lack of understanding from the people in their environment and are better off with a high resilience to deal with their lives. A study was done looking at 55 transgender youths studying their sense of personal mastery, perceived social support, emotion-oriented coping and self-esteem. It was seen that around 50% of the variation in the resilience aspects accounted for the problematic issues of the teens. This means that transgender youths with lower resilience were more prone to mental health issues, including depression and trauma symptoms. Emotion-oriented coping was a strong aspect of resilience in determining how depressed the individuals were.\n\nPregnancies among adolescents are considered as a complication, as they favour education interruption, poor present and future health, higher rates of poverty, problems for present and future children, among other negative outcomes.\n\nInvestigators from the Ecuadorian Catholic University (Universidad Católica de Santiago de Guayaquil) (Guayaquil) and the Spanish University of Zaragoza (Zaragoza), performed a comparative study at the Enrique C. Sotomayor Obstetric and Gynecology Hospital (Guayaquil) assessing resilience differences between pregnant adolescents and adults.\n\nA 56.6% of gravids presented total CESD-10 scores 10 or more indicating depressed mood. Despite this, total CESD-10 scores and depressed mood rate did not differ among studied groups. Adolescents did, however, display lower resilience reflected by lower total resilience scores and a higher rate of scores below the calculated median (P < 0.05). Logistic regression analysis could not establish any risk factor for depressed mood among studied subjects; however, having an adolescent partner and a preterm delivery related to a higher risk for lower resilience.\n\nOftentimes divorce is viewed as detrimental to one's emotional health, but studies have shown that cultivating resilience may be beneficial to all parties involved. The level of resilience a child will experience after their parents have split is dependent on both internal and external variables. Some of these variables include their psychological and physical state and the level of support they receive from their schools, friends, and family friends. The ability to deal with these situations also stems from the child's age, gender, and temperament. Children will experience divorce differently and thus their ability to cope with divorce will differ too. About 20–25% of children will \"demonstrate severe emotional and behavioral problems\" when going through a divorce. This percentage is notably higher than the 10% of children exhibiting similar problems in married families. Despite having divorces parents of approximately 75–80% of these children will \"develop into well-adjusted adults with no lasting psychological or behavioral problems\". This comes to show that most children have the tools necessary to allow them to exhibit the resilience needed to overcome their parents' divorce.\n\nThe effects of the divorce extend past the separation of both parents. The remaining conflict between parents, financial problems, and the re-partnering or remarriage of parents can cause lasting stress. Studies conducted by Booth and Amato (2001) have shown that there is no correlation between post-divorce conflict and the child's ability to adjust to their life circumstance. On the other hand, Hetherington (1999) completed research on this same topic and did find adverse effects in children. In regards to the financial standing of a family, divorce does have the potential to reduce the children's style of living. Child support is often given to help cover basic needs such as schooling. If the parents' finances are already scarce then their children may not be able to participate in extracurricular activities such as sports and music lessons, which can be detrimental to their social lives.\n\nRepartnering or remarrying can bring in additional levels of conflict and anger into their home environment. One of the reasons that re-partnering causes additional stress is because of the lack of clarity in roles and relationships; the child may not know how to react and behave with this new \"parent\" figure in their life. In most cases, bringing in a new partner/spouse will be the most stressful when done shortly after the divorce. In the past, divorce had been viewed as a \"single event\", but now research shows that divorce encompasses multiple changes and challenges. It is not only internal factors that allow for resiliency, but the external factors in the environment are critical for responding to the situation and adapting. Certain programs such as the 14-week Children's Support Group and the Children of Divorce Intervention Program may help a child cope with the changes that occur from a divorce.\n\nResilience after a natural disaster can be gauged in a number of different ways. It can be gauged on an individual level, a community level, and on a physical level. The first level, the individual level, can be defined as each independent person in the community. The second level, the community level, can be defined as all those inhabiting the locality affected. Lastly, the physical level can be defined as the infrastructure of the locality affected.\n\nUNESCAP funded research on how communities show resiliency in the wake of natural disasters. They found that, physically, communities were more resilient if they banded together and made resiliency an effort of the whole community. Social support is key in resilient behavior, and especially the ability to pool resources. In pooling social, natural, and economic resources, they found that communities were more resilient and able to over come disasters much faster than communities with an individualistic mindset.\n\nThe World Economic Forum met in 2014 to discuss resiliency after natural disasters. They conclude that countries that are more economically sound, and have more individuals with the ability to diversify their livelihoods, will show higher levels of resiliency. This has not been studied in depth yet, but the ideas brought about through this forum appear to be fairly consistent with already existing research.\n\nLittle research has been done on the topic of family resilience in the wake of the death of a family member. Traditionally, clinical attention to bereavement has focused on the individual mourning process rather than on those of the family unit as a whole. Resiliency is distinguished from recovery as the \"ability to maintain a stable equilibrium\" which is conducive to balance, harmony, and recovery. Families must learn to manage familial distortions caused by the death of the family member, which can be done by reorganizing relationships and changing patterns of functioning to adapt to their new situation. Exhibiting resilience in the wake of trauma can successfully traverse the bereavement process without long-term negative consequences.\n\nOne of the healthiest behaviors displayed by resilient families in the wake of a death is honest and open communication. This facilitates an understanding of the crisis. Sharing the experience of the death can promote immediate and long-term adaptation to the recent loss of a loved one. Empathy is a crucial component in resilience because it allows mourners to understand other positions, tolerate conflict, and be ready to grapple with differences that may arise. Another crucial component to resilience is the maintenance of a routine that helps to bind the family together through regular contact and order. The continuation of education and a connection with peers and teachers at school is an important support for children struggling with the death of a family member.\n\nBrad Evans and Julian Reid criticize resilience discourse and its rising popularity in their book, \"Resilient Life\". The authors assert that policies of resilience can put the onus of disaster response on individuals rather than publicly coordinated efforts. Tied to the emergence of neoliberalism, climate change theory, third-world development, and other discourses, Evans and Reid argue that promoting resilience draws attention away from governmental responsibility and towards self-responsibility and healthy psychological affects such as \"posttraumatic growth\".\n\nAnother criticism regarding resilience is its definition. Like other psychological phenomena, by defining specific psychological and affective states in certain ways, controversy over meaning will always ensue. How the term resilience is defined affects research focuses; different or insufficient definitions of resilience will lead to inconsistent research about the same concepts. Research on resilience has become more heterogeneous in its outcomes and measures, convincing some researchers to abandon the term altogether due to it being attributed to all outcomes of research where results were more positive than expected.\n\nThere is also controversy about the indicators of good psychological and social development when resilience is studied across different cultures and contexts. The American Psychological Association's Task Force on Resilience and Strength in Black Children and Adolescents, for example, notes that there may be special skills that these young people and families have that help them cope, including the ability to resist racial prejudice. Researchers of indigenous health have shown the impact of culture, history, community values, and geographical settings on resilience in indigenous communities. People who cope may also show \"hidden resilience\" when they don't conform with society's expectations for how someone is supposed to behave (in some contexts, aggression may be required to cope, or less emotional engagement may be protective in situations of abuse). Recently there has also been evidence that resilience can indicate a capacity to resist a sharp decline in other harm even though a person temporarily appears to get worse.<ref name=\"doi10.1177/0044118X03257030\"></ref>\n\n\n"}
{"id": "5249981", "url": "https://en.wikipedia.org/wiki?curid=5249981", "title": "Qinling panda", "text": "Qinling panda\n\nThe Qinling panda (\"Ailuropoda melanoleuca qinlingensis\") is a subspecies of the giant panda, discovered in the 1960s but not recognized as a subspecies until 2005. Disregarding the nominate subspecies, it is the first giant panda subspecies to be recognized. It differs from the more familiar nominate subspecies by its smaller skull and dark brown and light brown (rather than black and white) fur, and its smaller overall size. There are an estimated 200–300 Qinling pandas living in the wild.\nOn August 30, 1989, a female of this species was captured and brought to the Xi'an Zoo to be mated with a regular giant panda. Her offspring was black-and-white, but reportedly started becoming brownish as it aged. According to other reports she gave birth to three cubs but all of whom died shortly after being born. The mother, named Dan-Dan, died in 2000.\nThis subspecies is restricted to the Qinling Mountains, at elevations of . Its coloration is possibly a consequence of inbreeding: as the population is closed off from genetic variation and this might have led to the preservation of the mutation responsible.\n\nDue to the Qinling subspecies being captive it has been exposed to toxicants in their bamboo diet. Even though it is not fully known what toxicants it has been exposed it is determined that it was heavy metal from the atmospheric deposition. Thus, the conservation of the Qinling pandas may be compromised in the future due to the issues of air pollution of China.\n"}
{"id": "37291430", "url": "https://en.wikipedia.org/wiki?curid=37291430", "title": "R v McManus and Harvey", "text": "R v McManus and Harvey\n\nR v McManus and Harvey, is a landmark Australian court case for freedom of the press, whistleblowers and reporters privilege that resulted in journalists gaining greater safeguards to protect their sources.\n\nGerard McManus and Michael Harvey were journalists at the Herald Sun newspaper in Melbourne in the state of Victoria, Australia. In June 2007 the pair were convicted of contempt of court for refusing to name the source of an exposé the pair wrote on war veterans’ entitlements.\n\nThe pair’s conviction prompted a widespread debate on journalistic freedom in Australia and resulted in federal and state governments introducing “shield laws” to give judges scope to exempt journalists from revealing their sources during trials.\n\nA February 2004 Herald Sun report written by Harvey and McManus revealed an Australian Federal Government decision to rebuff 60 recommendations for improved financial benefits for returned war servicemen and women, including subsidized funeral costs, while still planning to portray the scaled-down support as a boon for war veterans and widows.\n\nThe story prompted a large-scale investigation into the possible source inside the Australian Public Service including Australian Federal Police probing 3,000 telephone extensions and hundreds of mobile phones. However, McManus and Harvey refused to disclose to the Australian Federal Police their source, citing adherence to journalists “Code of Ethics”.\n\nA civil servant, Desmond Patrick Kelly, was subsequently charged with a breach of the Commonwealth Crimes Act, but at a pre-trial hearing in August 2005 McManus and Harvey refused to give evidence. Nevertheless, Kelly was still found guilty of leaking confidential material, but in October 2006 the Victorian Court of Appeal overturned the conviction finding there was insufficient evidence to prove that he was the person who leaked the information.\n\nMeanwhile, the McManus-Harvey contempt matter continued and on 25 June 2007 McManus was found guilty of five counts of contempt of court and Harvey was found guilty of four counts. Victorian County Court Chief Judge Michael Rozenes said the offence was a serious one and that he had considered incarcerating the pair. McManus and Harvey were each fined $7,000, and a conviction recorded against them. News Limited supported the pair throughout the hearings as well as other media groups, the Media, Entertainment and Arts Alliance, and the Australian Press Council.\n\nThe ensuing outcry from the McManus and Harvey convictions prompted both the John Howard and Kevin Rudd Governments to introduce laws to provide some level of protection for journalists regarding their sources in 2007 and 2009. However, both administrations fell before the proposed laws were passed.\n\nTasmanian independent member of the House of Representatives, Andrew Wilkie, and Queensland Liberal Senator George Brandis respectively introduced Private Member’s Bills in 2009 to provide greater protection for journalists and their sources. Both Bills were referred to the Legal and Constitutional Affairs Legislation Committee, which recommended the Wilkie amendments, based largely on New Zealand “shield laws”, were preferable.\n\nProminent politicians who supported the Wilkie Bill included former Attorney-General Robert McClelland and South Australian independent Senator Nick Xenophon.\n\nDuring the Parliamentary debate Australian Greens argued the definition of a journalist in the legislation was too narrow and should include “bloggers”, but its proposed amendments were defeated.\n\nThe Australian Senate passed Andrew Wilkie’s amendments to the Commonwealth Evidence Act on March 3, 2011.\n\nOther Australian states, including New South Wales, Victoria and Western Australia and the Australian Capital Territory have also introduced their own “shield laws” but have also elected not to extend protection for bloggers.\n\n"}
{"id": "54252064", "url": "https://en.wikipedia.org/wiki?curid=54252064", "title": "Resource selection function", "text": "Resource selection function\n\nResource selection functions (RSFs) are a class of functions that are used in spatial ecology to assess which habitat characteristics are important to a specific population or species of animal, by assessing the a probability of that animal using a certain resource proportional to the availability of that resource in the environment.\n\nResource Selection Functions require two types of data: location information for the wildlife in question, and data on the resources available across the study area. Resources can include a broad range of environmental and geographical variables, including categorical variables such as land cover type, or continuous variables such as average rainfall over a given time period. A variety of methods are used for modeling RSFs, with logistic regression being commonly used.\n\nRSFs can be fit to data where animal presence is known, but absence is not, such as for species where several individuals within a study area are fitted with a GPS collar, but some individuals may be present without collars.\nWhen this is the case, buffers of various distances are generated around known presence points, with a number of available points generated within each buffer, which represent areas where the animal could have been, but it is unknown whether they actually were. These models can be fit using binomial generalized linear models or binomial generalized linear mixed models, with the resources, or environmental and geographic data, as explanatory variables.\n\nResource selection functions can be modeled at a variety of spatial scales, depending on the species and the scientific question being studied. (insert one more sentence on scale)\n\nMost RSFs address one of the following scales, which were defined by Douglas Johnson in 1980 and are still used today:\n"}
{"id": "35684112", "url": "https://en.wikipedia.org/wiki?curid=35684112", "title": "Right to clothing", "text": "Right to clothing\n\nThe right to adequate clothing, or the right to clothing, is recognized as a human right in various international human rights instruments; this, together with the right to food and the right to housing, are parts of the right to an adequate standard of living as recognized under Article 11 of the International Covenant on Economic, Social and Cultural Rights (ICESCR). The right to clothing is similarly recognized under Article 25 of the Universal Declaration of Human Rights (UDHR).\n\nThe right to clothing forms an aspect of the right to an adequate standard of living, and as such, is regarded as something that needs to be ensured so as to prevent people from living below the poverty line. Indeed, being ill-clothed is emblematic of acute poverty:\n\nTo illustrate how far-reaching the right to clothing potentially is, Dr Stephen James has provided a non-exhaustive list of beneficiaries of the right to minimum clothing. Included on this list are those sections of society that suffer the greatest from a lack of clothing, such as:\n\n\nThe lack of discussion about the right to clothing has led to uncertainty as to the ambit of the right and how much clothing is required. Matthew Craven notes that a minimum level of clothing is what is required to be provided; it is of \"paramount importance not least because at minimum levels it represents a question of survival.\" This requirement of a \"minimum\" or \"adequate\" standard is mirrored in reports from the UN Committee on the Rights of the Child (CRC) and a report from the Consortium For Street Children, as well as a number of General Comments from the Committee on Economic, Social and Cultural Rights (CESCR) as regards the elderly, disabled, and workers. There is, however, no indication as to what such a \"minimum\" or \"adequate\" standard entails: indeed, only rarely has the CESCR questioned a party state of the ICESCR on its performance with respect to the right to clothing.\n\nThere has been limited academic commentary on the ambit of the right to clothing with regards to refugees. James Hathaway has argued that refugees should have access to clothing that suitable for the climate and is sufficient for any work or other roles they may wish to undertake. Furthermore, they should not be compelled to wear any sort of clothing that would lead to social stigma or discrimination as foreigners. On the other hand, however, should refugees choose to wear clothing that is representative of their culture, country of origin or society, they are protected under Article 27 of the International Covenant on Civil and Political Rights to do so. The Committee on Economic, Social and Cultural Rights has tended towards applying context-specific interpretations on what is an adequate standard of clothing; so far, the right in its general sense has not yet been considered in a general comment.\n\nThe right to clothing has been recognised domestically for millennia  – at least partially  – but has received very little recognition on the international scene. It is not clear why there is a lack of recognition; one author has suggested that the lack of elaboration is due to the variations in cultural needs and wants. However, this explanation has been regarded as \"not plausible\": Dr James notes that \"[c]ultural, environmental and economic variations in 'needs and wants' are surely as marked with regard to housing, health as they are in relation to clothing, but this has not prevented detailed elaboration of those rights in international law.\" Matthew Craven concluded in 1995 that:\n\nHowever, Dr James has remarked: \"...none of us can be complacent that we will not find ourselves [...] in need of adequate clothing. The right is of great practical importance. It is an essential subsistence right, not an embellishment or a legal absurdity\". He also called for further discussion and academic commentary, arguing:\n\nAs the right to clothing concerns such a fundamental aspect of humanity, it naturally interacts with other human rights that are contained within various human rights instruments.\n\nEveryone has the essential right to life, as confirmed under Article 3 of the UDHR. However, if people are not adequately clothed, they are far more exposed to the elements. Without warm clothing, a person may well die from hypothermia during a cold winter; clothing that is inappropriately warm, on the other hand, could contribute to heat stroke, dehydration and exhaustion during summer or in tropical climates. Furthermore, inadequate clothing could increase exposure to ultraviolet rays; aggravate allergies and skin conditions; and worsen pre-existing medical conditions.\n\nAdditionally, access to medical care – similarly confirmed under Article 25 of the UDHR as well as Article 12 of the ICESCR  – can be impeded by inadequate access to clothing, particularly if the access to therapeutic clothing or orthopedic footwear is unavailable or prohibitively expensive.\n\nWearing clothes – or more accurately, choosing which clothes to wear – is, for many people, an important part of expression as confirmed under Article 19 of the UDHR. Persons with serious disabilities may be dress inappropriately, denying their desired expression. Furthermore, being forced to wear dirty, ripped, ill-fitting and even extremely outdated clothing may invite ridicule and contempt and inflict shame. This can be particularly true with school children  – parents may be reluctant to consider sending a child to school as a result of ridicule and shame brought through the clothing the child wears. A distinction should be drawn, however, between those who are forced to wear ripped, ill-fitting or extremely outdated clothing and those who consciously choose to wear such clothes as a 'fashion statement'.\n\nThe clothes that people choose to wear can identify a great number of things about a person: religious affiliations, ethnicity, national or political identity, culture, or race. Arguably, the clothes that an impoverished person wears can indicate their poverty. This sign of poverty or poorer economic status can be a cause of discrimination and of vilification. Additionally, clothing which is culturally distinctive or denotes religious affiliation could provoke discrimination and lead to a denial of social, economic, or political opportunities.\n\nThere is a large potential for \"abuses of trust, for humiliations and various physical abuses in medical and institutional settings, especially in relation to women and children, the disabled and elderly.\" If a person is denied access to adequate clothing  – especially essential clothes, such as undergarments  – it is possible that they may be rendered vulnerable to cruel, inhuman, or degrading treatment or punishment under the ambit of Article 5 of the UDHR. Such denial would include forcibly taking clothes, and is of particular importance in the context of detention and prisons: \"[o]ne can literally be left naked in the midst of power, a tragic condition [...] seen too often in prisons, in war and in concentration camps.\" Examples of such abuses in Abu Ghraib prison in Iraq and Guantanamo Bay have found to have caused serious mental illnesses, including post-traumatic stress disorder, resulting from forcing prisoners to strip naked and parade in front of female guards, as well as male detainees being forced to wear female underwear.\n\n\n\n"}
{"id": "44199725", "url": "https://en.wikipedia.org/wiki?curid=44199725", "title": "Sit-ups (punishment)", "text": "Sit-ups (punishment)\n\nThe sit-ups punishment (Uthak Baithak) is a form of punishment given in schools of Indian subcontinent, specially in India, Bangladesh, Sri Lanka and Pakistan. In this punishment, one has to sit down and stand up continuously and count the number of sit-ups. It is a common punishment in schools. Generally students are asked to perform this with holding their ears with hands. For humiliation, it is done in front of other students. Normally given in the range of 10 to 100 sit up for boys and 10-50 sit up for girls.\nThere are many forms of this punishment.\nThe student will do situps holding his ears. The hands should be ninety degrees with the body. Also he/she has to count loudly with every step. If the count is not hearable the term will repeat. Every term he/she has to sit fully so that his back touch the heals and to stand straight.\nThe punished will hold his/her left year with right hand and right ear with left hand. This may cause hardness and humilation to the punishment. It is famous mostly.\nThere will be more than one student and they will hold ears of one another and do the situps. This makes the punishment more hard as the punished have to take weight of anothers. Also criss-cross earholding can be applied here. Then it will be extra hard to complete.\nIt is also varient used with the simple situps. Here the punished's heals should touch one another. This will cause friction between the knees for that he/she can fall. Criss-cross earholding can be applied here..\nDoing situps in the sun, situps with leaving tough are other types of punishment.\n\n"}
{"id": "51525633", "url": "https://en.wikipedia.org/wiki?curid=51525633", "title": "Size-asymmetric competition", "text": "Size-asymmetric competition\n\nSize-asymmetric competition refers to situations in which larger individuals exploit disproportionally greater amounts of resources when competing with smaller individuals. This type of competition is common among plants but also exists among animals. Size-asymmetric competition usually results from large individuals monopolizing the resource by \"pre-emption\". i.e. exploiting the resource before smaller individuals are able to obtain it. Size-asymmetric competition has major effects on population structure and diversity within ecological communities.\n\nResource competition can vary from complete symmetric (all individuals receive the same amount of resources, irrespective of their size, known also as scramble competition) to perfectly size symmetric (all individuals exploit the same amount of resource per unit biomass) to absolutely size asymmetric (the largest individuals exploit all the available resource). The degree of size asymmetry can be described by the parameter θ in the following equation focusing on the partition of the resource r among n individuals of sizes Bj.\n\nri refers to the amount of resource consumed by individual i in the neighbourhood of j. When θ =1, competition is perfectly size symmetric, e.g. if a large individual is twice the size of its smaller competitor, the large individual will acquire twice the amount of that resource (i.e. both individuals will exploit the same amount of resource per biomass unit). When θ >1 competition is size-asymmetric, e.g. if large individual is twice the size of its smaller competitor and θ =2, the large individual will acquire four times the amount of that resource (i.e. the large individual will exploit twice the amount of resource per biomass unit). As θ increases, competition becomes more size-asymmetric and larger plants get larger amounts of resource per unit biomass compared with smaller plants.\n\nCompetition among plants for light is size-asymmetric because of the directionality of its supply. Higher leaves shade lower leaves but not vice versa. Competition for nutrients appears to be relatively size-symmetric, although it has been hypothesized that a patchy distribution of nutrients in the soil may lead to size-asymmetry in competition among roots. Nothing is known about the size-asymmetry of competition for water.\n\nVarious ecological processes and patterns have been shown to be affected by the degree of size-asymmetry e.g. succession, biomass distribution, grazing response, population growth, ecosystem functioning, coexistence and species richness. A large body of evidence shows that species loss following nutrient enrichment (eutrophication) is related to light competition (5, 15, 16). However, there is still a debate whether this phenomenon is related to the size-asymmetry of light competition or to other factors.\n\nContrasting assumptions about size-asymmetry characterise the two leading and competing theories in plant ecology, the R* theory and the CSR theory. The R* theory assumes that competition is size-symmetric and therefore predicts that competitive ability in nature results from the ability to withstand low level of resources (known as the R* rule). In contrast the CSR theory assumes that competition is size-asymmetric and therefore predicts that competitive ability in nature results from the ability to grow fast and attain a large size.\n\nSize-asymmetric competition affects also several evolutionary processes in relation to trait selection. Evolution of plant height is highly affected by asymmetric light competition. Theory predicts that only under asymmetric light competition, plants will grow upward and invest in wood production at the expense of investment in leaves, or in reproductive organs (flowers and fruits). Consistent with this, there is evidence that plant height increases as water availability increases, presumably due to increase in the relative importance of size-asymmetric competition for light. Similarly, investment in the size of seeds at the expense of their number may be more effective undersize-asymmetric resource competition, since larger seeds tend to produce larger seedlings that are better competitors.\nSize-asymmetric competition can be exploited in managing plant communities, such as the suppression of weed in crop fields. Weeds are a greater problem for farmer in dry than in moist environments, in large part because crops can suppress weeds much more effectively undersize-asymmetric competition for light than under more size-symmetric competition below ground.\n\n"}
{"id": "33275549", "url": "https://en.wikipedia.org/wiki?curid=33275549", "title": "Sophia (Gnosticism)", "text": "Sophia (Gnosticism)\n\nSophia ( \"Wisdom\", \"the Sophia\") is a major theme, along with Knowledge ( \"gnosis\", Coptic \"sooun\"), among many of the early Christian knowledge-theologies grouped by the heresiologist Irenaeus as \"gnostikos\", \"learned\". Gnosticism is a 17th-century term expanding the definition of Irenaeus' groups to include other syncretic and mystery religions.\n\nIn gnosticism, Sophia is a feminine figure, analogous to the human soul but also simultaneously one of the feminine aspects of God. Gnostics held that she was the \"syzygy\" (female twin divine Aeon) of Jesus (i.e. the Bride of Christ), and Holy Spirit of the Trinity. She is occasionally referred to by the Hebrew equivalent of \"Achamōth\" (, \"chokhmah\") and as \"Prunikos\" (). In the Nag Hammadi texts, Sophia is the lowest Aeon, or anthropic expression of the emanation of the light of God. She is considered to have fallen from grace in some way, in so doing creating or helping to create the material world.\n\nAlmost all Gnostic systems of the Syrian or Egyptian type taught that the universe began with an original, unknowable God, referred to as the Parent or Bythos, or as the Monad by Monoimus. From this initial unitary beginning, the One spontaneously emanated further Aeons, being pairs of progressively 'lesser' beings in sequence. Together with the source from which they emanate they form the \"Pleroma\", or fullness, of God, and thus should not be seen as distinct from the divine, but symbolic abstractions of the divine nature. The transition from the immaterial to the material, from the noumenal to the sensible, is brought about by a flaw, or a passion, or a sin, in one of the Aeons.\n\nIn most versions of the Gnostic mythos, it is Sophia who brings about this instability in the Pleroma, in turn bringing about the creation of materiality. According to some Gnostic texts, the crisis occurs as a result of Sophia trying to emanate without her syzygy or, in another tradition, because she tries to breach the barrier between herself and the unknowable Bythos. After cataclysmically falling from the Pleroma, Sophia's fear and anguish of losing her life (just as she lost the light of the One) causes confusion and longing to return to it. Because of these longings, matter (Greek: \"hylē\", ὕλη) and soul (Greek: \"psychē\", ψυχή) accidentally come into existence. The creation of the Demiurge (also known as Yaldabaoth, \"Son of Chaos\") is also a mistake made during this exile. The Demiurge proceeds to create the physical world in which we live, ignorant of Sophia, who nevertheless manages to infuse some spiritual spark or \"pneuma\" into his creation.\n\nIn the \"Pistis Sophia\", Christ is sent from the Godhead in order to bring Sophia back into the fullness (Pleroma). Christ enables her to again see the light, bringing her knowledge of the spirit (Greek: \"pneuma\", πνευμα). Christ is then sent to earth in the form of the man Jesus to give men the Gnosis needed to rescue themselves from the physical world and return to the spiritual world. In Gnosticism, the Gospel story of Jesus is itself allegorical: it is the Outer Mystery, used as an introduction to Gnosis, rather than it being literally true in a historical context. For the Gnostics, the drama of the redemption of the Sophia through Christ or the Logos is the central drama of the universe. The Sophia resides in all of us as the Divine Spark.\n\nJewish Alexandrine religious philosophy was much occupied with the concept of the Divine \"Sophia\", as the revelation of God's inward thought, and assigned to her not only the formation and ordering of the natural universe (comp. Clem. \"Hom.\" ) but also the communication of all insight and knowledge to mankind. In Wisdom (the noun is feminine) is described as God's Counsellor and Workmistress (Master-workman, R.V.), who dwelt beside Him before the Creation of the world and sported continually before Him.\n\nIn accordance with the description given in the Book of Proverbs, a dwelling-place was assigned by the Gnostics to the Sophia, and her relation to the upper world defined as well as to the seven planetary powers which were placed under her. The seven planetary spheres or heavens were for the ancients the highest regions of the created universe. They were thought of as seven circles rising one above another, and dominated by the seven Archons. These constituted the (Gnostic) Hebdomad. Above the highest of them, and over-vaulting it, was the Ogdoad, the sphere of immutability, which was nigh to the spiritual world (Clemens Alexandrinus, \"Stromata\", , 161; comp. , 138 sqq.). Now we read in :\n\nThese seven pillars being interpreted as the planetary heavens, the habitation of the Sophia herself was placed above the Hebdomad in the Ogdoad (\"Excerpt. ex Theodot\". 8, 47). It is said further of the same divine wisdom ():\n\nThis meant, according to the Gnostic interpretation, that the Sophia has her dwelling-place \"on the heights\" above the created universe, in the place of the midst, between the upper and lower world, between the Pleroma and the \"ektismena\". She sits at \"the gates of the mighty,\" i.e. at the approaches to the realms of the seven Archons, and at the \"entrances\" to the upper realm of light her praise is sung. The Sophia is therefore the highest ruler over the visible universe, and at the same time the mediatrix between the upper and the lower realms. She shapes this mundane universe after the heavenly prototypes, and forms the seven star-circles with their Archons under whose dominion are placed, according to the astrological conceptions of antiquity, the fates of all earthly things, and more especially of man. She is \"the mother\" or \"the mother of the living.\" (Epiph. \"Haer\". 26, 10). As coming from above, she is herself of pneumatic essence, the \"mētēr phōteinē\" (Epiph. 40, 2) or the \"anō dynamis\" (Epiph. 39, 2) from which all pneumatic souls draw their origin.\n\nIn reconciling the doctrine of the pneumatic nature of the Sophia with the dwelling-place assigned her, according to the Proverbs, in the kingdom of the midst, and so outside the upper realm of light, there was envisioned a descent of Sophia from her heavenly home, the Pleroma, into the void (\"kenōma\") beneath it. The concept was that of a seizure or robbery of light, or of an outburst and diffusion of light-dew into the \"kenōma\", occasioned by a vivifying movement in the upper world. But inasmuch as the light brought down into the darkness of this lower world was thought of and described as involved in suffering, this suffering must be regarded as a punishment. This inference was further aided by the Platonic notion of a spiritual fall.\n\nAlienated through their own fault from their heavenly home, souls have sunk down into this lower world without utterly losing the remembrance of their former state, and filled with longing for their lost inheritance, these fallen souls are still striving upwards. In this way the mythos of the fall of Sophia can be regarded as having a typical significance. The fate of the \"mother\" was regarded as the prototype of what is repeated in the history of all individual souls, which, being of a heavenly pneumatic origin, have fallen from the upper world of light their home, and come under the sway of evil powers, from whom they must endure a long series of sufferings until a return into the upper world be once more vouchsafed them.\n\nBut whereas, according to the Platonic philosophy, fallen souls still retain a remembrance of their lost home, this notion was preserved in another form in Gnostic circles. It was taught that the souls of the Pneumatici, having lost the remembrance of their heavenly derivation, required to become once more partakers of Gnosis, or knowledge of their own pneumatic essence, in order to make a return to the realm of light. In the impartation of this Gnosis consists the redemption brought and vouchsafed by Christ to pneumatic souls. But the various fortunes of such souls were wont to be contemplated in those of Sophia, and so it was taught that the Sophia also needed the redemption wrought by Christ, by whom she is delivered from her \"agnoia\" and her \"pathe\", and will, at the end of the world's development, be again brought back to her long lost home, the Upper Pleroma, into which this mother will find an entrance along with all pneumatic souls her children, and there, in the heavenly bridal chamber, celebrate the marriage feast of eternity.\n\nThe Sophia mythos has in the various Gnostic systems undergone great variety of treatment. The oldest, the Syrian Gnosis, referred to the \"Sophia\" the formation of the lower world and the production of its rulers the Archons; and along with this they also ascribed to her the preservation and propagation of the spiritual seed.\n\nAs described by Irenaeus, the great Mother-principle of the universe appears as the first woman, the Holy Spirit (\"rūha d'qudshā\") moving over the waters, and is also called the mother of all living. Under her are the four material elements—water, darkness, abyss, and chaos. With her, combine themselves the two supreme masculine lights, the first and the second man, the Father and the Son, the latter being also designated as the Father's \"ennoia\". From their union proceeds the third imperishable light, the third man, Christ. But unable to support the abounding fulness of this light, the mother in giving birth to Christ, suffers a portion of this light to overflow on the left side. While, then, Christ as \"dexios\" (He of the right hand) mounts upward with his mother into the imperishable Aeon, that other light which has overflowed on the left hand, sinks down into the lower world, and there produces matter. And this is the Sophia, called also \"Aristera\" (she of the left hand), \"Prouneikos\" and the male-female.\n\nThere is here, as yet, no thought of a fall, properly so called, as in the Valentinian system. The power which has thus overflowed leftwards, makes a voluntary descent into the lower waters, confiding in its possession of the spark of true light. It is, moreover, evident that though mythologically distinguished from the \"humectatio luminis\" (Greek: \"ikmas phōtos\", ἰκμὰς φωτός), the Sophia is yet, really nothing else but the light-spark coming from above, entering this lower material world, and becoming here the source of all formation, and of both the higher and the lower life. She swims over the waters, and sets their hitherto immoveable mass in motion, driving them into the abyss, and taking to herself a bodily form from the \"hylē\". She compasses about, and is laden with material every kind of weight and substance, so that, but for the essential spark of light, she would be sunk and lost in the material. Bound to the body which she has assumed and weighed down thereby, she seeks in vain to make her escape from the lower waters, and hasten upwards to rejoin her heavenly mother. Not succeeding in this endeavour, she seeks to preserve, at least, her light-spark from being injured by the lower elements, raises herself by its power to the realm of the upper region, and there spreading herself out she forms out of her own bodily part, the dividing wall of the visible firmament, but still retains the \"aquatilis corporis typus\". Finally seized with a longing for the higher light, she finds, at length, in herself, the power to raise herself even above the heaven of her own forming, and to fully lay aside her corporeity. The body thus abandoned is called \"Woman from Woman.\"\n\nThe narrative proceeds to tell of the formation of the seven Archons by Sophia herself, of the creation of man, which \"the mother\" (i.e. not the first woman, but the Sophia) uses as a mean to deprive the Archons of their share of light, of the perpetual conflict on his mother's part with the self-exalting efforts of the Archons, and of her continuous striving to recover again and again the light-spark hidden in human nature, till, at length, Christ comes to her assistance and in answer to her prayers, proceeds to draw all the sparks of light to Himself, unites Himself with the Sophia as the bridegroom with the bride, descends on Jesus who has been prepared, as a pure vessel for His reception, by Sophia, and leaves him again before the crucifixion, ascending with Sophia into the world or Aeon which will never pass away (; Epiph. 37, 3, sqq.; Theodoret, h. f. i. 14).\n\nIn this system the original cosmogonic significance of the Sophia still stands in the foreground. The antithesis of Christus and Sophia, as He of the right (\"ho dexios\") and She of the Left (\"hē aristera\"), as male and female, is but a repetition of the first Cosmogonic Antithesis in another form. The Sophia herself is but a reflex of the \"Mother of all living\" and is therefore also called \"Mother.\" She is the formatrix of heaven and earth, for as much as mere matter can only receive form through the light which, coming down from above has interpenetrated the dark waters of the \"hylē;\" but she is also at the same time the spiritual principle of life in creation, or, as the world-soul the representative of all that is truly pneumatic in this lower world: her fates and experiences represent typically those of the pneumatic soul which has sunk down into chaos.\n\nIn the Gnostic system described by Irenaeus (; see Ophites) the name Prunikos several times takes the place of Sophia in the relation of her story. The name Prunikos is also given to Sophia in the account of the kindred Barbeliot system, given in the preceding chapter of Irenaeus. Celsus, who shows that he had met with some Ophite work, exhibits acquaintance with the name Prunikos () a name which Origen recognizes as Valentinian. That this Ophite name had really been adopted by the Valentinians is evidenced by its occurrence in a Valentinian fragment preserved by Epiphanius (Epiph. \"Haer\". xxxi. 5). Epiphanius also introduces Prunikos as a technical word in the system of the Simonians (Epiph. \"Haer\". xxi. 2) of those whom he describes under the head of Nicolaitans (Epiph. \"Haer\". xxv. 3, 4) and of the Ophites (Epiph. \"Haer\". xxxvii. 4, 6).\n\nNeither Irenaeus nor Origen indicates that he knew anything as to the meaning of this word; and we have no better information on this subject than a conjecture of Epiphanius (Epiph. \"Haer\". xxv. 48). He says that the word means \"wanton\" or \"lascivious,\" for that the Greeks had a phrase concerning a man who had debauched a girl, \"Eprounikeuse tautēn\". One feels some hesitation in accepting this explanation. Epiphanius was deeply persuaded of the filthiness of Gnostic morals, and habitually put the worst interpretation on their language. If the phrase reported by Epiphanius had been common, it is strange that instances of its use should not have been quoted from the Greek comic writers. It need not be denied that Epiphanius had heard the phrase employed, but innocent words come to be used in an obscene sense, as well by those who think \"double entendre\" witty, as by those who modestly avoid the use of plainer language. The primary meaning of the word \"prouneikos\" seems to be a porter, or bearer of burdens, the derivation being from \"enenkein\", the only derivation indeed that the word seems to admit of. Then, modifying its meaning like the word \"agoraios\", it came to be used in the sense of a turbulent violent person. The only distinct confirmation of the explanation of Epiphanius is that Hesychius (\"s. v. Skitaloi\") has the words \"aphrodisiōn kai tēs prounikias tēs nykterinēs\". This would be decisive, if we could be sure that these words were earlier in date than Epiphanius.\n\nIn favour of the explanation of Epiphanius is the fact, that in the Gnostic cosmogonical myths, the imagery of sexual passion is constantly introduced. It seems on the whole probable that \"prouneikos\" is to be understood in the sense of \"propherēs\" which has for one of its meanings \"precocious in respect of sexual intercourse.\" According to Ernst Wilhelm Möller (1860) the name is possibly meant to indicate her attempts to entice away again from the lower Cosmic Powers the seed of Divine light. In the account given by Epiphanius (\"Haer\". 37:6) the allusion to enticements to sexual intercourse which is involved in this name, becomes more prominent.\n\nHowever, in the \"Exegesis on the Soul\" text found at Nag Hammadi, the soul is likened to a woman which fell from perfection into prostitution, and that the Father will elevate her again to her original perfect state. In this context, the female personification of the soul resembles the passion of Sophia as Prunikos.\n\nNigh related to this is the notion widely diffused among Gnostic sects of the impure \"mētra\" (womb) from whence the whole world is supposed to have issued. As according to the Italian Valentinians the Soter opens the \"mētra\" of the lower Sophia, (the \"Enthymēsis\"), and so occasions the formation of the universe (, ) so on the other hand the \"mētra\" itself is personified. So Epiphanius reports the following cosmogony as that of a branch of the Nicolaitans:\n\nThe Sethians (Hippolytus. \"Philosophum.\" ) teach in like manner that from the first concurrence (\"syndromē\") of the three primeval principles arose heaven and earth as a \"megalē tis idea sphragidos\". These have the form of a \"mētra\" with the \"omphalos\" in the midst. The pregnant \"mētra\" therefore contains within itself all kinds of animal forms in the reflex of heaven and earth and all substances found in the middle region. This \"mētra\" also encounters us in the great \"Apophasis\" ascribed to Simon where it is also called Paradise and Eden as being the locality of man's formation.\n\nThese cosmogonic theories have their precedent in the Thalatth or Tiamat of Syrian mythology, the life-mother of whom Berossus has so much to relate, or in the world-egg out of which when cloven asunder heaven and earth and all things proceed. The name of this Berossian Thalatth meets us again among the Peratae of the \"Philosophumena\" () and is sometimes mistakenly identified with that of the sea—\"thalassa\".\n\nA similar part to that of the \"mētra\" is played by Edem, consort of Elohim in the Gnostic book \"Baruch\" () who there appears as a two-shaped being formed above as a woman and from the middle downwards as a serpent ().\n\nAmong the four and twenty Angels which she bears to Elohim, and which form the world out of her members, the second female angelic form is called \"Achamōs\" [\"Achamōth\"]. Like to this legend of the \"Philosophumena\" concerning the Baruch-Gnosis is that which is related by Epiphanius of an Ophite Party that they fabled that a Serpent from the Upper World had had sexual intercourse with the Earth as with a woman (Epiphanius, \"Haer\". 45: 1 cf. 2).\n\nVery nigh related to the doctrines of the Gnostics in Irenaeus are the views of the so-called Barbeliotae (). The name Barbelo, which according to one interpretation is a designation of the upper Tetrad, has originally nothing to do with the Sophia. This latter Being called also \"Spiritus Sanctus\" and \"Prunikos\" is the offspring of the first angel who stands at the side of the Monogenes. Sophia seeing that all the rest have each its \"syzygos\" within the Pleroma, desires also to find such a consort for herself; and not finding one in the upper world she looks down into the lower regions and being still unsatisfied there she descends at length against the will of the Father into the deep. Here she forms the Demiurge (the \"Proarchōn\"), a composite of ignorance and self-exaltation. This Being, by virtue of pneumatic powers stolen from his mother, proceeds to form the lower world. The mother, on the other hand, flees away into the upper regions and makes her dwelling there in the Ogdoad.\n\nWe meet this Sophia also among the Ophiana whose \"Diagram\" is described by Celsus and Origen, as well as among various Gnostic (Ophite) parties mentioned by Epiphanius. She is there called Sophia or Prunikos, the upper mother and upper power, and sits enthroned above the Hebdomad (the seven Planetary Heavens) in the Ogdoad (Origen, \"Against Celsus\". , , , ; Epiphan. \"Haer\". 25, 3 sqq. 26, 1,10. 39, 2 ; 40, 2). She is also occasionally called \"Parthenos\" () and again is elsewhere identified with the Barbelo or Barbero (Epiph. \"Haer\". 25, 3 ; 26, 1, 10).\n\nThis mythos of the soul and her descent into this lower world, with her various sufferings and changing fortunes until her final deliverance, recurs in the Simonian system under the form of the All-Mother who issues as its first thought from the \"Hestōs\" or highest power of God. She generally bears the name \"Ennoia\", but is also called Wisdom (Sophia), Ruler, Holy Spirit, Prunikos, Barbelo. Having sunk down from the highest heavens into the lowest regions, she creates angels and archangels, and these again create and rule the material universe. Restrained and held down by the power of this lower world, she is hindered from returning to the kingdom of the Father. According to one representation she suffers all manner of insult from the angels and archangels bound and forced again and again into fresh earthly bodies, and compelled for centuries to wander in ever new corporeal forms. According to another account she is in herself incapable of suffering, but is sent into this lower world and undergoes perpetual transformation in order to excite by her beauty the angels and powers, to impel them to engage in perpetual strife, and so gradually to deprive them of their store of heavenly light. The \"Hestōs\" himself at length comes down from the highest heaven in a phantasmal body in order to deliver the suffering \"Ennoia\", and redeem the souls held in captivity by imparting gnosis to them.\n\nThe most frequent designation of the Simonian \"Ennoia\" is \"the lost\" or \"the wandering sheep.\" The Greek divinities Zeus and Athena were interpreted to signify \"Hestōs\" and his \"Ennoia\", and in like manner the Tyrian sun-god Herakles-Melkart and the moon-goddess Selene-Astarte. So also the Homeric Helena, as the cause of quarrel between Greeks and Trojans, was regarded as a type of the \"Ennoia\". The story which the fathers of the church handed down of the intercourse of Simon Magus with his consort Helena (; ; Epiphanius \"Haer\". 21; ; Philaster, \"Haer\". 29; , ; ; ), had probably its origin in this allegorical interpretation, according to Richard Adelbert Lipsius (1867).\n\nIn the Simonian \"Apophasis\" the great \"dynamis\" (also called \"Nous\") and the great \"epinoia\" which gives birth to all things form a syzygy, from which proceeds the male-female Being, who is called \"Hestōs\" (). Elsewhere \"nous\" and \"epinoia\" are called the upper-most of the three Simonian Syzygies, to which the \"Hestōs\" forms the Hebdomad: but on the other hand, \"nous\" and \"epinoia\" are identified with heaven and earth ().\n\nThe most significant development of this Sophia mythos is found in the Valentinian system. The descent of the Sophia from the Pleroma is ascribed after Plato's manner to a fall, and as the final cause of this fall a state of suffering is indicated which has penetrated into the Pleroma itself. Sophia or \"Mētēr\" is in the doctrine of Valentinus the last, i.e. the thirtieth Aeon in the Pleroma, from which having fallen out, she now in remembrance of the better world which she has thus forsaken, gives birth to the Christus \"with a shadow\" (\"meta skias tinos\"). While Christus returns to the Pleroma, Sophia forms the Demiurge and this whole lower world out of the \"skia\", a right and a left principle (). For her redemption comes down to Sophia either Christus himself () or the \"Soter\" (, cf. \"exc. ex Theod\". 23; 41), as the common product of the Aeons, in order to bring her back to the Pleroma and unite her again with her \"syzygos\".\n\nThe motive for the Sophia's fall was defined according to the Anatolian school to have lain therein, that by her desire to know what lay beyond the limits of the knowable she had brought herself into a state of ignorance and formlessness. Her suffering extends to the whole Pleroma. But whereas this is confirmed thereby in fresh strength, the Sophia is separated from it and gives birth outside it (by means of her \"ennoia\", her recollections of the higher world), to the Christus who at once ascends into the Pleroma, and after this she produces an \"ousia amorphos\", the image of her suffering, out of which the Demiurge and the lower world come into existence; last of all looking upwards in her helpless condition, and imploring light, she finally gives birth to the \"spermata tēs ekklēsias\", the pneumatic souls. In the work of redemption the Soter comes down accompanied by the masculine angels who are to be the future \"syzygoi\" of the (feminine) souls of the Pneumatici, and introduces the Sophia along with these Pneumatici into the heavenly bridal chamber (\"Exc. ex Theod.\" 29-42; ). The same view, essentially meets us in the accounts of Marcus, (; cf. ; ; ) and in the Epitomators of the Syntagma of Hippolytus (; Philaster, \"Haer\". 38).\n\nThe Italic school distinguished on the other hand a two-fold Sophia, the \"ano Sophia\" and the \"katō Sophia\" or Achamoth.\n\nAccording to the doctrine of Ptolemaeus and that of his disciples, the former of these separates herself from her \"syzygos\", the \"thelētos\" through her audacious longing after immediate Communion with the Father of all, falls into a condition of suffering, and would completely melt away in this inordinate desire, unless the \"Horos\" had purified her from her suffering and established her again in the Pleroma. Her \"enthymēsis\", on the other hand, the desire which has obtained the mastery over her and the consequent suffering becomes an \"amorphos kai aneideos ousia\", which is also called an \"ektrōma\", is separated from her and is assigned a place beyond the limits of the Pleroma.\n\nFrom her dwelling-place above the Hebdomad, in the place of the Midst, she is also called Ogdoad (Ὀγδοάς), and further entitled \"Mētēr\", \"Sophia\" also, and \"he Hierousalēm, Pneuma hagion\", and (\"arsenikōs\") \"Kyrios\". In these names some partial reminiscences of the old Ophitic Gnosis are retained.\n\nThe Achamoth first receives (by means of Christus and \"Pneuma hagion\" the Pair of Aeons within the Pleroma whose emanation is most recent), the \"morphōsis kat' ousian\". Left alone in her suffering she has become endued with penitent mind (\"epistrophē\"). Now descends the son as the common fruit of the Pleroma, gives her the \"morphōsis kata gnōsin\", and forms out of her various affections the Demiurge and the various constituents of this lower world. By his appointment the Achamoth produces the pneumatic seed (the \"ekklēsia\").\n\nThe end of the world's history is here also (as above) the introduction of the lower Sophia with all her pneumatic offspring into the Pleroma, and this intimately connected with the second descent of the Soter and his transient union with the psychical Christus; then follows the marriage-union of the Achamoth with the Soter and of the pneumatic souls with the angels (; \"exc. ex Theod\". 43-65).\n\nThe same form of doctrine meets us also in Secundus, who is said to have been the first to have made the distinction of an upper and a lower Sophia (), and in the account which the \"Philosophumena\" give us of a system which most probably referred to the school of Heracleon, and which also speaks of a double Sophia (). The name Jerusalem also for the \"exō Sophia\" meets us here (). It finds its interpretation in the fragments of Heracleon (ap. Origen. \"in Joann.\" tom. x. 19). The name Achamoth, on the other hand, is wanting both in Hippolytus and in Heracleon. One school among the Marcosians seems also to have taught a two-fold Sophia (; cf. ).\n\nAugust Hahn (1819) debated whether the name \"Achamōth\" (Ἀχαμώθ) is originally derived from the Hebrew \"Chokhmah\" (חָכְמָ֑ה), in Aramaic \"Ḥachmūth\" or whether it signifies 'She that brings forth'—'Mother.' The Syriac form \"Ḥachmūth\" is testified for us as used by Bardesanes (Ephraim, \"Hymn\" 55), the Greek form \"Hachamōth\" is found only among the Valentinians: the name however probably belongs to the oldest Syrian Gnosis.\n\nCosmogonic myths play their part also in the doctrine of Bardesanes. The \"locus foedus\" whereon the gods (or Aeons) measured and founded Paradise (Ephraim, \"Hymn\" 55) is the same as the impure \"mētra\", which Ephraim is ashamed even to name (cf. also Ephraim, \"Hymn\" 14). The creation of the world is brought to pass through the son of the living one and the Rūha d' Qudshā, the Holy Spirit, with whom Ḥachmūth is identical, but in combination with \"creatures,\" i.e. subordinate beings which co-operate with them (Ephraim, \"Hymn\" 3). It is not expressly so said, and yet at the same time is the most probable assumption, that as was the case with the father and mother so also their offspring the son of the Living One, and the Rūha d' Qudshā or Ḥachmūth, are to be regarded as a Syzygy. This last (the Ḥachmūth) brings forth the two daughters, the \"Shame of the Dry Land\" i.e. the \"mētra\", and the \"Image of the Waters\" i.e. the \"Aquatilis Corporis typus\", which is mentioned in connection with the Ophitic Sophia (Ephraim, \"Hymn\" 55). Beside which, in a passage evidently referring to Bardesanes, air, fire, water, and darkness are mentioned as aeons (Īthyē: \"Hymn\" 41) These are probably the \"Creatures\" to which in association with the Son and the Rūha d' Qudshā, Bardesanes is said to have assigned the creation of the world.\n\nThough much still remains dark as to the doctrine of Bardesanes we cannot nevertheless have any right to set simply aside the statements of Ephraim, who remains the oldest Syrian source for our knowledge of the doctrine of this Syrian Gnostic, and deserves therefore our chief attentions. Bardesanes, according to Ephraim, is able also to tell of the wife or maiden who having sunk down from the Upper Paradise offers up prayers in her dereliction for help from above, and on being heard returns to the joys of the Upper Paradise (Ephraim, \"Hymn\" 55).\n\nThese statements of Ephraim are further supplemented by the \"Acts of Thomas\" in which various hymns have been preserved which are either compositions of Bardesanes himself, or at any rate are productions of his school.\n\nIn the Syriac text of the Acts, we find the \"Hymn of the Pearl\", where the soul which has been sent down from her heavenly home to fetch the pearl guarded by the serpent, but has forgotten here below her heavenly mission until she is reminded of it by a letter from \"the father, the mother, and the brother,\" performs her task, receives back again her glorious dress, and returns to her old home.\n\nOf the other hymns which are preserved in the Greek version more faithfully than in the Syriac text which has undergone Catholic revision, the first deserving of notice is the \"Ode to the Sophia\" which describes the marriage of the \"maiden\" with her heavenly bridegroom and her introduction into the Upper Realm of Light. This \"maiden,\" called \"daughter of light,\" is not as the Catholic reviser supposes the Church, but Ḥachmūth (Sophia) over whose head the \"king,\" i.e. the father of the living ones, sits enthroned; her bridegroom is, according to the most probable interpretation, the son of the living one, i.e. Christ. With her the living Ones i.e. pneumatic souls enter into the Pleroma and receive the glorious light of the living Father and praise along with \"the living spirit\" the \"father of truth\" and the \"mother of wisdom.\"\n\nThe Sophia is also invoked in the first prayer of consecration. She is there called the \"merciful mother,\" the \"consort of the masculine one,\" \"revealant of the perfect mysteries,\" \"Mother of the Seven Houses,\" \"who finds rest in the eighth house,\" i.e. in the Ogdoad. In the second Prayer of Consecration she is also designated, the \"perfect Mercy\" and \"Consort of the Masculine One,\" but is also called \"Holy Spirit\" (Syriac \"Rūha d' Qudshā\") \"Revealant of the Mysteries of the whole Magnitude,\" \"hidden Mother,\" \"She who knows the Mysteries of the Elect,\" and \"she who partakes in the conflicts of the noble Agonistes\" (i.e. of Christ, cf. \"exc. ex Theod\". 58 \"ho megas agōnistēs Iēsous\").\n\nThere is further a direct reminiscence of the doctrine of Bardesanes when she is invoked as the Holy Dove which has given birth to the two twins, i.e. the two daughters of the Rūha d' Qudshā (ap. Ephraim, \"Hymn\" 55).\n\nA special and richly coloured development is given to the mythical form of the Sophia of the Gnostic Book \"Pistis Sophia\". The two first books of this writing to which the name \"Pistis Sophia\" properly belongs, treat for the greater part (pp. 42–181) of the fall, the Repentance, and the Redemption of the Sophia.\n\nShe has by the ordinance of higher powers obtained an insight into the dwelling-place appropriated to her in the spiritual world, namely, the \"thēsauros lucis\" which lies beyond the XIIIth Aeon. By her endeavours to direct thither her upward flight, she draws upon herself the enmity of the \"Authadēs\", Archon of the XIIIth Aeon, and of the Archons of the XII Aeons under him; by these she is enticed down into the depths of chaos, and is there tormented in the greatest possible variety of ways, in order that she may thus incur the loss of her light-nature.\n\nIn her utmost need she addresses thirteen penitent prayers (\"metanoiai\") to the Upper Light. Step by step she is led upwards by Christus into the higher regions, though she still remains obnoxious to the assaults of the Archons, and is, after offering her XIIIth \"Metanoia\", more vehemently attacked than ever, until at length Christus leads her down into an intermediate place below the XIIIth Aeon, where she remains until the consummation of the world, and sends up grateful hymns of praise and thanksgiving.\n\nThe earthly work of redemption having been at length accomplished, the Sophia returns to her original celestial home. The peculiar feature in this representation consists in the further development of the philosophical ideas which find general expression in the Sophia mythos. According to Karl Reinhold von Köstlin (1854), Sophia is here not merely, as with Valentinus, the representative of the longing which the finite spirit feels for the knowledge of the infinite, but at the same time a type or pattern of faith, of repentance, and of hope. After her restoration she announces to her companions the twofold truth that, while every attempt to overstep the divinely ordained limits, has for its consequence suffering and punishment, so, on the other hand, the divine compassion is ever ready to vouchsafe pardon to the penitent.\n\nWe have a further reminiscence of the Sophia of the older Gnostic systems in what is said in the book \"Pistis Sophia\" of the Light-Maiden (\"parthenos lucis\"), who is there clearly distinguished from the Sophia herself, and appears as the archetype of Astraea, the Constellation Virgo. The station which she holds is in the place of the midst, above the habitation assigned to the Sophia in the XIIIth Aeon. She is the judge of (departed) souls, either opening for them or closing against them the portals of the light-realm (pp. 194–295). Under her stand yet seven other light-maidens with similar functions, who impart to pious souls their final consecrations (p. 291 sq. 327 sq. 334). From the place of the \"parthenos lucis\" comes the sun-dragon, which is daily borne along by four light-powers in the shape of white horses, and so makes his circuit round the earth (p. 183, cf. p. 18, 309).\n\nThis light-maiden (\"parthenos tou phōtos\") encounters us also among the Manichaeans as exciting the impure desires of the Daemons, and thereby setting free the light which has hitherto been held down by the power of darkness (\"Dispuiat. Archelai et Manetis\", c. 8, n. 11; Theodoret., h. f. I. 26). On the other hand, the place of the Gnostic Sophia is among Manichaeans taken by the \"Mother of Life\" (\"mētēr tēs zōēs\"), and by the World-Soul (\"psychē hapantōn\"), which on occasions is distinguished from the Life-Mother, and is regarded as diffused through all living creatures, whose deliverance from the realm of darkness constitutes the whole of the world's history (Titus of Bostra, \"adv. Manich\". I., 29, 36, ed. Lagarde, p. 17 sqq. 23; Alexander Lycopolitus c. 3; Epiphan. \"Haer\". 66, 24; \"Acta dispatat. Archelai et Manetis\", c. 7 sq. et passim). Their return to the world of light is described in the famous \"Canticum Amatorium\" (ap. Augustin. c. Faust, iv. 5 sqq).\n\nIn \"On the Origin of the World\", Sophia is depicted as the ultimate destroyer of this material universe, Yaldabaoth and all his Heavens:\n\nCarl Jung linked the figure of Sophia to the highest archetype of the anima in depth psychology. The archetypal fall and recovery of Sophia is additionally linked (to a varying degree) to many different myths and stories (see damsel in distress). Among these are:\n\nNote that many of these myths have alternative psychological interpretations. For example, Jungian psychologist Marie-Louise von Franz interpreted fairy tales like Sleeping Beauty as symbolizing the 'rescue' or reintegration of the anima, the more 'feminine' part of a man's unconscious, but not wisdom or \"sophia\" per se.\n\n"}
{"id": "49895722", "url": "https://en.wikipedia.org/wiki?curid=49895722", "title": "The Australian Canon", "text": "The Australian Canon\n\nThe Australian refers to those texts by Australian authors that espouse the values of canonicity. These values are dynamic and contentious but may generally be said to include: timelessness, universal concerns, a unique Australian identity, an authentic representation of what it means to be 'Australian'.\n\nContentious concerns include the emergence of a distinctive Australian literature and arguments about contending literary canons, including the relation between ‘generational canons’, ‘middlebrow’ and ‘academic’ canons, the ‘syllabus’ and the ‘canonical imaginary’.\n\nBaynton, Barbara, \"Bush Studies\", (1902)\n\nFranklin, Miles, \"My Brilliant Career\", (1901)\n\nCambridge, Ada, \"A Woman's Friendship\", (1988)\n\nRichardson, Henry Handel, \"Maurice Guest\" (1908)\n\nWhite, Patrick, \"Riders in the Chariot\", (1961)\n\nLindsay, Norman, The Magic Pudding, (1918)\n\nDavid Carter writes:\n\nAs a value-laden rather than neutral descriptive term, \"Australian literature\" is more likely to b e contentious than consensual; ironically it has often been contentious precisely because it has functioned to represent one version of consensus against another. Not only do definitions of Australian literature shift over time, at any one time different and potentially conflicting definitions will be operating across the various sites and layers of the culture. The idea of Australian literature is better understood, then, in one of the telling, if excessively self-dramatising phrases of recent criticism, as a \"site of struggle\" where these different definitions or institutional effects are given social and material form. It is political, therefore, because much more are at stake than competing individual literary tastes, or at least there may be under certain social and institutional conditions. The precise relationship between the institutional politics within literature and politics beyond the institution has more often been mystified than clarified.\n"}
{"id": "49834236", "url": "https://en.wikipedia.org/wiki?curid=49834236", "title": "The Shape of Water", "text": "The Shape of Water\n\nThe Shape of Water is a 2017 American romantic dark fantasy drama film directed by Guillermo del Toro and written by del Toro and Vanessa Taylor. It stars Sally Hawkins, Michael Shannon, Richard Jenkins, Doug Jones, Michael Stuhlbarg, and Octavia Spencer. Set in Baltimore, Maryland, in 1962, the story follows a mute custodian at a high-security government laboratory who falls in love with a captured humanoid amphibian creature. Filming took place in Ontario, Canada, between August and November 2016.\n\nThe film was screened in the main competition section of the 74th Venice International Film Festival, where it premiered on August 31, 2017, and was awarded the Golden Lion for best film. It was also screened at the 2017 Toronto International Film Festival. It began a limited release in two theaters in New York City on December 1, 2017, before expanding wide on December 23, 2017, and grossed $195 million worldwide.\n\n\"The Shape of Water\" received critical acclaim for its acting, screenplay, direction, visuals, production design, and musical score, with many critics calling the film del Toro's best work since \"Pan's Labyrinth\"; the American Film Institute selected it as one of the top 10 films of the year. \"The Shape of Water\" received a number of awards and nominations, including thirteen nominations at the 90th Academy Awards, where it won for Best Picture, Best Director, Best Production Design, and Best Original Score. It was nominated for seven awards at the 75th Golden Globe Awards, winning for Best Director and Best Original Score, and 12 at the 71st British Academy Film Awards, winning three awards, including Best Director. A novelization by del Toro and Daniel Kraus was published on March 6, 2018.\n\nElisa Esposito (Sally Hawkins) who was found abandoned as a child by the side of a river with wounds on her neck, is mute and communicates through sign language. She lives alone in an apartment above a cinema and works as a cleaner at a secret government laboratory in Baltimore, Maryland, at the height of the Cold War. Her only friends are her closeted next-door neighbor Giles (Richard Jenkins) a middle-aged struggling advertising illustrator and Zelda (Octavia Spencer) a co-worker who serves as her interpreter.\n\nThe facility receives a mysterious creature captured from the Amazon River by Colonel Richard Strickland (Michael Shannon) who is in charge of the project to study it. Believing it is just a wild beast, Strickland treats it brutally, repeatedly shocking it with his electric cattle prod. Curious about the creature, Elisa discovers it is a male humanoid amphibian (Doug Jones). She begins visiting him in secret and the two form a close bond as she teaches him sign language.\n\nSeeking to exploit the Amphibian Man for an American advantage in the space race, General Frank Hoyt (Nick Searcy) is eventually persuaded by Strickland to vivisect it. One scientist, Robert Hoffstetler (Michael Stuhlbarg) - who is really a Soviet spy named Dimitri Mosenkov - pleads unsuccessfully to keep the Amphibian Man alive for further study and, at the same time, is ordered by his Soviet handlers to euthanize the creature. When Elisa overhears the American plans for the Amphibian Man, she persuades Giles to help her free him. Hoffstetler stumbles upon Elisa's plot in progress and chooses to help her. Though initially reluctant, Zelda also becomes involved in the successful escape.\n\nElisa keeps the Amphibian Man in her bathtub. She plans to release him into a nearby canal when it rains to give access to the ocean in several days' time. Strickland interrogates Elisa and Zelda, among others, but learns nothing. Back at the apartment, Giles discovers the Amphibian Man devouring one of his cats. Startled, the Amphibian Man slashes Giles's arm and rushes out of the apartment. He gets as far as the cinema downstairs, luckily empty of patrons, before Elisa finds him and returns him to her apartment. He touches Giles on his balding head and his wounded arm, and the next morning Giles discovers his hair has begun growing back and the wounds on his arm have healed. After initially walking away, Elisa has sex with the Amphibian Man in her shower. For a later encounter, she fills the bathroom completely with water, which eventually begins to cause water to drip into the theater below. The upset theater owner alerts Giles, who then enters the apartment and opens the bathroom door, interrupting the tryst.\n\nHoyt unexpectedly arrives and asks for the status of the case. When Strickland questions how much experimentation is enough, he is told he has 36 hours to recover the Amphibian Man or his career and life will be over. Meanwhile, Hoffstetler is told he will be extracted in two days. As the planned release date approaches, the Amphibian Man's health starts deteriorating. Hoffstetler goes to meet his handlers with Strickland tailing him. At the rendezvous, Hoffstetler is shot by a handler before Strickland kills him and then continuously tortures Hoffstetler for information (knowing he is a spy by virtue of having heard him speak Russian to his handler) while asking for the name and rank of who has the creature. Before dying, Hoffstetler implicates Elisa and Zelda. Strickland savagely threatens Zelda in her home until her husband Brewster (Martin Roach) reveals that Elisa has the Amphibian Man (having overheard her conversations). After Strickland's departure, Zelda immediately calls Elisa, warning her to get the creature out immediately. An enraged Strickland arrives and searches Elisa's empty apartment until he finds a calendar note revealing where she plans to release the Amphibian Man.\n\nAt the canal, Elisa and Giles are bidding farewell to the creature when Strickland arrives, knocks Giles down, and shoots the Amphibian Man and Elisa. The Amphibian Man quickly heals himself and slashes Strickland's throat, killing him. As police arrive on the scene with Zelda, the Amphibian Man takes Elisa and jumps into the canal, where he heals her. When he applies his healing touch to the scars on Elisa's neck, they open to reveal gills like his; she jolts back to life, and the two embrace. In a closing voice-over narration, Giles conveys his belief that Elisa lived \"happily ever after in love\" with the Amphibian Man.\n\nThe idea for \"The Shape of Water\" formed during del Toro's breakfast with Daniel Kraus in 2011, with whom he later co-wrote the novel \"Trollhunters\". It shows similarities to the 2015 short film \"The Space Between Us\". It was also primarily inspired by del Toro's childhood memories of seeing \"Creature from the Black Lagoon\" and wanting to see the Gill-man and Kay Lawrence (played by Julie Adams) succeed in their romance. When del Toro was in talks with Universal to direct a remake of \"Creature from the Black Lagoon\", he tried pitching a version focused more on the creature's perspective, where the Creature ended up together with the female lead, but the studio executives rejected the concept.\n\nDel Toro set the film during the 1960s Cold War era to counteract today's heightened tensions, specifying, \"if I say once upon a time in 1962, it becomes a fairy tale for troubled times. People can lower their guard a little bit more and listen to the story and listen to the characters and talk about the issues, rather than the circumstances of the issues.\"\n\nA fan of her performances in \"Happy-Go-Lucky\" and \"Fingersmith\", Del Toro wrote the script with Sally Hawkins in mind for the part and pitched the idea to her while intoxicated at the 2014 Golden Globes. Hawkins prepared for the role by watching films of silent comedians Charlie Chaplin, Buster Keaton, Harold Lloyd and was told by Del Toro to watch Stan Laurel from Laurel and Hardy, whom Del Toro thought was capable of doing a \"state of grace without conveying it verbally\".\n\nThe part of Giles was originally written with Ian McKellen in mind and Del Toro was inspired to do so by his performance as the real-life closeted gay filmmaker James Whale who directed \"Frankenstein\", \"The Invisible Man\" and \"Bride of Frankenstein\", who found himself unemployable in his later years. When McKellen proved unavailable, Del Toro sent an e-mail to Richard Jenkins, who accepted the part.\n\nMichael Shannon was cast as Richard Strickland, the villain of the film. According to an interview with Vanity Fair, Shannon and Del Toro had early conversations about the notion that Strickland would have been the hero of the film if it had been made in the 1950s, something that fascinated the actor. Octavia Spencer, who played the role of Elisa's co-worker, friend and interpreter Zelda found it funny that the people Del Toro used to speak for the mute main character were people who represent very disenfranchised groups.\n\nFilming began on August 15, 2016, in Toronto and Hamilton, Ontario, and wrapped on November 6, 2016. In an interview with IndieWire about the film, del Toro said, \"This movie is a healing movie for me. ... For nine movies I rephrased the fears of my childhood, the dreams of my childhood, and this is the first time I speak as an adult, about something that worries me as an adult. I speak about trust, otherness, sex, love, where we're going. These are not concerns that I had when I was nine or seven.\"\n\nAccording to an interview with \"The Wrap\", Guillermo Del Toro was torn between making the film in color or in black and white, and was at one point leaning toward the latter. Fox Searchlight Pictures offered Del Toro either a $20 million budget to make the film in color or a $17 million budget to shoot it in black and white. \"That was honestly a battle I was expecting to lose,\" Del Toro said to \"The Wrap\". \"I was of two minds. On one hand I thought black and white would look luscious, but on the other hand I thought it would look postmodern, like I was being reflective rather than immersed. It's good, because it got me three million more.\"\n\nAlexandre Desplat is the composer of the film's score. The score won the Academy Award for Best Original Score at the 90th Academy Awards.\n\n\"The Shape of Water\" premiered on August 31, 2017 at the 74th Venice International Film Festival. It also screened at Telluride Film Festival, the 2017 Toronto International Film Festival and BFI London Film Festival, among others. The film was released in two theaters in New York City on December 1, 2017 and then expanded to several other cities the following week. It had its official wide release in the United States on December 22, 2017.\n\nOn March 13, 2018, the film was released on Blu-ray, DVD and digital download.\n\n\"The Shape of Water\" grossed $63.9 million in the United States and Canada, and $131.4 million in other countries, for a total of $195.2 million.\n\nAfter grossing $4.6 million over three weeks of limited release, the film began its wide release on December 22, 2017, alongside the openings of \"Downsizing\", \"Pitch Perfect 3\" and \"Father Figures\", and the wide expansion of \"Darkest Hour\", and grossed $3 million from 726 theaters over the weekend, and $4.4 million over the four-day Christmas frame. The following weekend, the film made $3.5 million. The weekend of January 27, 2018, following the announcement of the film's 13 Oscar nominations, the film was added to over 1,000 theaters (for a total of 1,854) and made $5.9 million (an increase of 171% over the previous week's $2.2 million), finishing 8th. The weekend of March 9–11, following its four Oscar wins, the film made $2.4 million. It marked a 64% increase from the previous week's $1.5 million and was similar to the $2.5 million made by the previous year's Best Picture winner, \"Moonlight\".\n\nOn the review aggregator website Rotten Tomatoes, the film has an approval rating of 92% based on 388 reviews, with an average rating of 8.4/10. The website's critical consensus reads: \"\"The Shape of Water\" finds Guillermo del Toro at his visually distinctive best—and matched by an emotionally absorbing story brought to life by a stellar Sally Hawkins performance.\" On Metacritic, the film has a weighted average score of 87 out of 100, based on 53 critics, indicating \"universal acclaim\". According to CinemaScore, audience members under the age of 40 gave the film an average grade of either \"A+\" or \"A\", while those over 40 gave it an \"A\" or \"A−\", on an A+ to F scale; PostTrak reported filmgoers gave the film an overall positive score of 80%.\n\nBen Croll of IndieWire gave the film an 'A' rating and called it \"one of del Toro's most stunningly successful works... also a powerful vision of a creative master feeling totally, joyously free.\" Writing for \"Rolling Stone\", Peter Travers gave the film 3.5 out of 4 stars, praising Hawkins's performance, the cinematography and del Toro's direction, and saying: \"Even as the film plunges into torment and tragedy, the core relationship between these two unlikely lovers holds us in thrall. Del Toro is a world-class film artist. There's no sense trying to analyze how he does it.\" For the \"Minnesota Daily\", Haley Bennett reacted positively, writing, \"\"The Shape of Water\" has tenderness uncommon to del Toro films. ... While \"The Shape of Water\" isn't groundbreaking, it is elegant and mesmerizing.\"\n\nConversely, Rex Reed of the \"New York Observer\" gave the film 1 out of 4 stars and calling it \"a loopy, lunkheaded load of drivel\" and, referring to Hawkins's role in \"Maudie\", described people with disabilities as \"defective creatures.\" Reed's review was criticized for referring to Sally Hawkins' mute character as \"mentally handicapped\" and for erroneously crediting actor Benicio del Toro as the film's director.\n\n\"The Shape of Water\" appeared on many critics' year-end top-ten lists, among them:\n\n\"The Shape of Water\" received 13 nominations at the 90th Academy Awards, the most of any film in the 2018 race. It won in four categories: Best Production Design, Best Original Score, Best Director, and Best Picture.\n\nThe film also spawned some debate about whether the fact that it was filmed in Canada, with a predominantly Canadian crew and many Canadian actors in the supporting roles, should have made it eligible to be nominated for the Canadian Screen Awards. Under Academy of Canadian Cinema and Television rules, to qualify for CSA nominations under the rules for international coproductions at least 15 per cent of a film's funding must come from a Canadian film studio. Even the film's Canadian co-producer, J. Miles Dale, stated that he supports the rules and does not believe the film should have been eligible.\n\nIn February 2018, the estate of Paul Zindel initiated a lawsuit in United States District Court for the Central District of California against director Guillermo del Toro and associate producer Daniel Kraus, alleging that \"The Shape of Water\" \"brazenly copies the story, elements, characters, and themes\" of Zindel's 1969 work \"Let Me Hear You Whisper\", which depicts a cleaning lady bonding with a dolphin and attempting to rescue it from a secret research laboratory's nefarious uses. The complaint spends more than a dozen pages detailing alleged \"overwhelming similarities\" between the works.\n\nDel Toro denied the claim of the Zindel estate, saying that \"I have never read nor seen the play. I'd never heard of this play before making \"The Shape of Water,\" and none of my collaborators ever mentioned the play.\" Distributor Fox Searchlight also denied the claim and said that it would \"vigorously defend\" itself in court.\n\nIn July 2018, Judge Percy Anderson dismissed the suit and stated that del Toro and Fox Searchlight were entitled to recover their legal costs.\n\nThe Shape of Water was also accused by a French director of copying scenes from the romantic comedy Amelie and the cult classic Delicatessen.\nJean-Pierre Jeunet’s black comedy Delicatessen was released in 1991 and quickly became a cult classic, but it was his whimsical take on life and love in the picturesque Paris district of Montmartre, Amelie, that gave him his first global hit.\n\nHe claims fellow director Guillermo del Toro was clearly plagiarising one of his scenes in The Shape of Water when he has the characters played by British actor Sally Hawkins and Richard Jenkins perform a two-step dance while sitting on a sofa watching an old Hollywood movie.\n\nThere are some very strong similarities in the saturation of the colours, overall art direction and the use of anthropomorphic objects. The music is reminiscent of Yann Tiersen's soundtrack for Amelie.\n\n\"When [Guillermo del Toro] steals the scene of the couple sitting on the edge of the bed dancing with their feet, with the musical on telly in the background, it is so cut and pasted from Delicatessen that there’s a moment when I say to myself that he lacks self-respect,\" said Mr Jeunet.\n“It is obvious that he had ‘Delicatessen’ in mind,” he said.\n\n“I’m not going to sue him for plagiarism, that’s not my style. But Guillermo has enough talent not to be doing things like that,” he told Ouest-France newspaper.\n\nMr Jeunet said he emailed the Mexican director to ask him why he was stealing his ideas.\n“He replied.. that he did not steal from others, that it was [American director] Terry Gilliam who influenced all of us,” he said\n\n"}
{"id": "3224522", "url": "https://en.wikipedia.org/wiki?curid=3224522", "title": "Theory of reasoned action", "text": "Theory of reasoned action\n\nThe Theory of Reasoned Action (ToRA or TRA) aims to explain the relationship between attitudes and behaviors within human action. It is mainly used to predict how individuals will behave based on their pre-existing attitudes and behavioral intentions. An individual's decision to engage in a particular behavior is based on the outcomes the individual expects will come as a result of performing the behavior. Developed by Martin Fishbein and Icek Ajzen in 1967, the theory derived from previous research in social psychology, persuasion models, and attitude theories. Fisbein's theories suggested a relationship between attitude and behaviors (the A-B relationship). However, critics estimated that attitude theories were not proving to be good indicators of human behavior. The ToRA was later revised and expanded by the two theorists in the following decades to overcome any discrepancies in the A-B relationship with the Theory of Planned Behavior (TPB) and Reasoned Action Approach (RAA). The theory is also used in communication discourse as a theory of understanding. \n\nThe primary purpose of the TRA is to understand an individual's voluntary behavior by examining the underlying basic motivation to perform an action . ToRA states that a person's intention to perform a behavior is the main predictor of whether or not they actually perform that behavior. Additionally, the normative component (i.e. social norms surrounding the act) also contributes to whether or not the person will actually perform the behavior. According to the theory, intention to perform a certain behavior precedes the actual behavior. This intention is known as behavioral intention and comes as a result of a belief that performing the behavior will lead to a specific outcome. Behavioral intention is important to the theory because these intentions \"are determined by attitudes to behaviors and subjective norms\". The theory of reasoned action suggests that stronger intentions lead to increased effort to perform the behavior, which also increases the likelihood for the behavior to be performed. \n\nA positivistic approach to behavior research, ToRA attempts to predict and explain one's intention of performing a certain behavior. The theory requires that behavior be clearly defined in terms of the four following concepts: Action (e.g. to go get), Target (e.g. a mammogram), Context (e.g. at the breast screening center), and Time (e.g. in the 12 months). According to TRA, behavioral intention is the main motivator of behavior, while the two key determinants on behavioral intention are people's \"attitudes\" and \"norms\". By examining attitudes and subjective norms, researchers can gain an understanding as to whether or not one will perform the intended action. \n\nAccording to TRA, \"attitudes\" are one of the key determinants of behavioral intention and refer to the way people feel towards a particular behavior. These attitudes are influenced by two factors: the \"strength of behavioral beliefs\" regarding the outcomes of the performed behavior (i.e. whether or not the outcome is probable) and the \"evaluation\" of the potential outcomes (i.e. whether or not the outcome is positive). Attitudes regarding a certain behavior can either be positive, negative or neutral. The theory stipulates that there exists a direct correlation between attitudes and outcomes, such that if one believes that a certain behavior will lead to a desirable or \"favorable\" outcome, then one is more likely to have a positive attitude towards the behavior. Alternatively, if one believes that a certain behavior will lead to an undesirable or \"unfavorable\" outcome, then one is more likely to have a negative attitude towards the behavior. \n\n\"Behavioral belief\" allows us to understand people's motivations for their behavior in terms of the behavior's consequences. This concept stipulates that people tend to associate the performance of a certain behavior with a certain set of outcomes or features. For example, a person believes that if he or she studies for a month for his or her driver's license test, that one will pass the test after failing it the first time without studying at all. Here, the behavioral belief is that studying for a month is equated with success, whereas not studying at all is associated with failure. \n\nThe evaluation of the outcome refers to the way people perceive and evaluate the potential outcomes of a performed behavior. Such evaluations are conceived in a binary \"good-bad\" fashion-like manner. For example, a person may evaluate the outcome of quitting smoking cigarettes as positive if the behavioral belief is improved breathing and clean lungs. Conversely, a person may evaluate the outcome of quitting smoking cigarettes as negative if the behavioral belief is weight gain after smoking cessation. \n\n\"Subjective norms\" are also one of the key determinants of behavioral intention and refer to the way perceptions of relevant groups or individuals such as family members, friends, and peers may affect one's performance of the behavior. Ajzen defines subjective norms as the \"perceived social pressure to perform or not perform the behavior\". According to TRA, people develop certain beliefs or \"normative beliefs\" as to whether or not certain behaviors are acceptable. These beliefs shape one's perception of the behavior and determine one's intention to perform or not perform the behavior. For example, if one believes that recreational drug use (the behavior) is acceptable within one's social group, one will more likely be willing to engage in the activity. Alternatively, if one's friends groups perceive that the behavior is bad, one will be less likely to engage in recreational drug use. However, subjective norms also take into account people's \"motivation to comply\" with their social circle's views and perceptions, which vary depending on the situation and the individual's motivations. \n\nNormative beliefs touch on whether or not referent relevant groups approve of the action. There exists a direct correlation between normative beliefs and performance of the behavior. Usually, the more likely the referent groups will approve of the action, the more likely the individual perform the act. Conversely, the less likely the referent groups will approve of the action, the less likely the individual will perform the act. \n\nMotivation to comply addresses the fact that individuals may or may not comply with social norms of the referent groups surrounding the act. Depending on the individuals motivations in terms of adhering to social pressures, the individual will either succumb to the social pressures of performing the act if it is deemed acceptable, or alternatively will resist to the social pressures of performing the act if it is deemed unacceptable. \n\nBehavioral intention is a function of both attitudes and subjective norms toward that behavior (also known as the normative component). Attitudes being how strongly one holds the attitude toward the act and subjective norms being the social norms associated with the act. The stronger the attitude and the more positive the subjective norm, the higher the A-B relationship should be. However, the attitudes and subjective norms are unlikely to be weighted equally in predicting behavior. Depending on the individual and situation, these factors might have different impacts on behavioral intention, thus a weight is associated with each of these factors. A few studies have shown that direct prior experience with a certain activity results in an increased weight on the attitude component of the behavior intention function.\n\nIn its simplest form, the TRA can be expressed as the following equation:\n\nwhere:\n\nThe TORA theorists note that there are three conditions that can affect the relationship between behavioral intention and behavior. The first condition is that \"the measure of intention must correspond with respect to their levels of specificity\". This means that to predict a specific behavior, the behavioral intention must be equally specific. The second condition is that there must be \"stability of intentions between time of measurement and performance of behavior\". The intention must remain the same between the time that it is given and the time that the behavior is performed. The third condition is \"the degree to which carrying out the intention is under the volitional control of the individual\". The individual always has the control of whether or not to perform the behavior. These conditions have to do with the transition from verbal responses to actual behavior.\n\nWhile Fishbein and Ajzen developed the ToRA within the field of health to understand health behaviors, the theorists asserted that TRA could be applied in any given context to understand and even predict any human behavior. According to Sheppard et al., behavioral intention can predict the performance of \"any voluntary act, unless intent changes prior to the performance or unless the intention measure does not correspond to the behavioral criterion in terms of action, target, context, time-frame and/or specificity\". Their statement asserts that according to TRA, the measure of behavioral intention can predict whether or not an individual will preform a certain act, as long as the behavioral intention remains the same and the behavior is clearly and properly defined. Broadening the scope of TRA, Sheppard conducted a study in which they applied TRA in situations that did not completely comply or conform with Fishbein and Ajzen's framework. Surveying 87 previous empirical studies, they applied the theory in contexts where the individual did not have full volitional control over the behavior and/or where individuals did not have all the information to develop the intention. To their surprise, they found that the theory of reasoned action could successfully be applied in situations that did not fully adhere to the three formal terms and conditions specified by the theory. \n\nAlthough the scope of TRA is wide, the theory still has its limitations and like any other theory, needs constant refinement and revision particularly when extending to choice and goals. The distinction between a goal intention and a behavioral intention concerns the capability to achieve one's intention, which involves multiple variables thus creating great uncertainty. Ajzen acknowledged that \"some behaviors are more likely to present problems of controls than others, but we can never be absolutely certain that we will be in a position to carry out our intentions. Viewed in this light it becomes clear that strictly speaking every intention is a goal whose attainment is subject to some degree of uncertainty.\" \n\nAccording to Eagly and Chaiken, TRA does not take into account that certain conditions that enable the performance of a behavior are not available to individuals. Since the ToRA focuses on behaviors that people decisively enact, the theory is limited in terms of being able to predict behaviors that require access to certain opportunities, skills, conditions, and/or resources. Additionally, certain intentions do not necessarily play a role in terms of connecting attitudes and behavior. According to a study conducted by Bagozzi and Yi, the performance of a behavior is not always preceded by a strong intent. In fact, attitudes and behaviors may not always be linked by intentions, particularly when the behavior does not require much cognitive effort. \n\nIn 1979, H. C. Triandis proposed expanding TORA to include more components. These factors were habit, facilitating conditions, and affect. When a person performs a behavior in a routine manner they form a habit. Facilitating conditions are conditions that make completion of an action more or less difficult. Both of these conditions affect their behavior directly. On the other hand, affect is the emotional response a person has towards a behavior and this emotional response only affects behavioral intention rather than directly affecting behavior. This expanded version of TORA has been used to study behaviors like women's participation in mammography procedures.\n\nIn 1985, Ajzen extended TORA to what he refers as the theory of planned behavior (TPB). This involves the addition of one major predictor—perceived behavioral control. This addition was introduced to account for times when people have the intention to conduct the behavior, but the actual behavior is thwarted because of subjective and objective reasons. In the theory of planned behavior, the attitude, subjective norms, and behavioral control have \"important although differently weighted effects on a person's intention to behave\".\n\nIn spite of the improvement, it is suggested that TORA and TPB only provides an account of the determinants of behavior when both motivation and opportunity to process information are high. Further research demonstrating the casual relationships among the variables in TPB and any expansions of it is clearly necessary. The model also mentions little about the memory process.\n\nThe theory of reasoned action has been used in many studies as a framework for examining specific kinds of behavior such as communication behavior, consumer behavior and health behavior. Many researchers use the theory to study behaviors that are associated with high risks and danger, as well as deviant behavior. In contrast, some research has applied the theory to more normative and rational types of action. Researchers Davies, Foxall, and Pallister suggest that the theory of reasoned action can be tested if \"behavior is measured objectively without drawing a connection to prior intention\". Most studies, however, look at intention because of its central role in the theory.\n\nTheory of Reasoned Action has been applied to the study of whistle-blowing intentions and hazing in college organizations, specifically fraternities and sororities. Hazing is understood to be \"any activity expected of someone that joins a group, which humiliates, degrades, abuses or endangers its victims\". In the United States, there have been a variety of hazing incidents that have resulted in death and harm of students on several college campuses. Whistle-blowing \"involves an individual with some level of unique or inside knowledge using public communication to bring attention to some perceived wrongdoing or problem. Whistle-blowing is significant to this issue because individuals who are aware of hazing incidents can come forward to university officials and make the occurrence of hazing known. In their study, Richardson et al. set out to study whistle-blowing by using the theory of reasoned action as a framework to predict whether or not individuals will come forward about report hazing incidents. Their study served to examine whether the relationships suggested by the TRA model remain true in predicting whistle blowing intentions, and if these relationships would change depending on the severity of the hazing incident.\n\nRichardson et al. surveyed a sample of 259 students from Greek organizations at university in the Southwestern United States. The survey questions measured the different aspects of the TRA model: behavioral beliefs, outcome evaluations, attitude toward the behavior, normative beliefs, motivation to comply, subjective norms, and the consequence endogenous variable. The questions asked respondents to rate their responses on various 7 point scales. \"Participants in the study responded to one of three scenarios, varying in level of severity, describing a hazing situation occurring in their fraternity or sorority\". In line with the theory, the researchers wanted to identify if attitudes held about hazing, dangerous activity, and group affiliation, along with subjective norms about whistle-blowing (reactions by others, consequences of reporting the action, isolation from the group) would influence whether or not an individual would go through with reporting a hazing incident. The results of the study found that individuals were more likely to report, or whistle-blow, on hazing incidents that were more severe or harmful to individuals. Simultaneously, individuals were also concerned about the perceptions of others' attitudes towards them and the consequences they may face if they reported hazing incidents.\n\nTRA is used to examine the communication behavior in corporations. One of the behaviors TRA helped characterize is knowledge sharing (KS) in companies. In the study conducted by Ho, Hsu, and Oh, they proposed two models to construct KS process by introducing TRA and game theory (GT). One model captures personal psychological feelings (attitudes and subjective norms), the other model not only captures personal feelings but also takes other people's decisions into consideration. By comparing the two models, researchers found that the model based on TRA has a higher predictive accuracy than the model based on TRA and GT. They concluded that employees \"have a high probability of not analyzing the decisions of others\", and whether taking other colleague's decision into account has a great impact on people's KS behavioral intention. It is indicated that \"the more indirect decision-makers there are in organizations, the less effective is KS\". To encourage KS, company managers should avoid including indirect decision-makers in the projects.\n\nCoupon usage has also been studied through the theory of reasoned action framework by researchers interested in consumer and marketer behavior. In 1984, Terence Shimp and Alican Kavas applied this theory to coupon usage behavior, with the research premise that \"coupon usage is rational, systematic, and thoughtful behavior\" in contrast with other applications of the theory to more dangerous types of behavior.\n\nTheory of Reasoned Action serves as a useful model because it can help examine whether \"consumers' intentions to use coupons are determined by their attitudes and perceptions of whether important others think one should or should not expend the effort to clip, save, and use coupons\". The consumer's behavior intentions are influenced by their personal beliefs about coupon usage, meaning whether or not they think saving money is important and are willing to spend the time clipping coupons. These potential beliefs also influenced the coupon user's thoughts about what others think about their usage of coupons. Together, the coupon user will use their own beliefs and the opinions of others to form an overall attitude towards coupon usage. To approach this study, Shimp and Alican surveyed 770 households and measured the aspects of the TRA model in terms of the participant's responses. The received responses indicated that consumers' norms are \"partially determined by their personal beliefs toward coupon usage, and to an even greater extend, that attitudes are influenced by internalizations of others' beliefs\". Positive attitudes towards this behavior are influenced by an individual's perceptions that their partners will be satisfied by their time spent and efforts made to save money.\nTRA has been applied to redefine brand loyalty. According to the theory of reasoned action, the antecedents of purchase behaviour are attitudes towards the purchase and subjective norm. In 1998, Ha conducted a study to investigate the relationships among several antecedents of unit brand loyalty (UBL) by introducing TRA. Consumers are brand loyal when both attitude and behavior are favorable. In his study, Ha developed a table indicating 8 combinations of customers' brand loyalty based on their loyalty on 3 variables – attitude towards the behavior, subjective norm, and purchase behavior is loyal. According to Ha, marketing managers should not be discouraged by a temporary disloyalty and need to strive for grabbing brand loyalty when customers are showing loyalty to two of the three variables, but they need to rediagnose their customers' brand loyalty when customers are showing loyalty to only one of them. The main focus should be pointed at either enhancing the consumer's attitude toward their brand or adjusting their brand to the social norms.\n\nTRA has also been used to study consumer attitudes towards renewable energy. In 2000, Bang, et. al found that people who cared about environmental issues like pollution were more willing to spend more for renewable energy. Similarly, a 2008 study of Swedish consumers by Hansla et. al showed that those who with a positive view of renewable energy were more willing to spend money on sustainable energy for their homes. These studies are evidence that the emotional response people have towards a topic affects their attitude, which in turn affects their behavioral intent. These studies also provide examples for how the TRA is used to market goods that might not make the most sense from a strictly economic perspective.\n\nIn addition, Mishara et.al 's research proved there is a positively relation between behavioral intention and actual behavior in Green Information Technology (GIT) acceptance. Those professionals with positive intentions towards GIT tend to exploit GIT into practice. \n\nTheory of Reasoned Action has been frequently used as a framework and predictive mechanism of applied research on sexual behavior, especially in prevention of sexually transmitted disease such as HIV. In 2001, Albarracín, Johnson, Fishbein, and Muellerleile applied theory of reasoned action (TRA) and theory of planned behavior (TPB) into studying how well the theories predict condom use. To be consistent with TRA, the authors synthesized 96 data sets (N = 22,594), and associate every component in condom use with certain weight. Their study indicates that the theories of reasoned action and planned behavior are highly successful predictors of condom use. According to their discussion, \"people are more likely to use condoms if they have previously formed the corresponding intentions. These intentions to use condoms appear to derive from attitudes, subjective norms, and perceived behavioral control. These attitudes and norms, in turn, appear to derive from outcome and normative beliefs. Nevertheless, whether behavior was assessed retrospectively or prospectively was an important moderator that influenced the magnitude of the associations between theoretically important variables.\"\n\nIn 2011, W.M. Doswell, Braxter, Cha, and Kim examined sexual behavior in African American teenage girls and applied the theory as a framework for understanding this behavior. The theory of reasoned action can explain these behaviors in that teens' behavioral intentions to engage in early sexual behavior are influenced by their pre-existing attitudes and subjective norms of their peers. Attitudes in this context are favorable or unfavorable dispositions towards teenage sexual behavior. Subjective norms are the perceived social pressure teenagers feel from their friends, classmates, and other peer groups to engage in sexual behavior. As a framework, the TRA suggests that adolescents will participate in early behavior because of their own attitudes towards the behavior and the subjective norms of their peers. In this case, intention is the willful plan to perform early sexual behavior. Findings from the student showed that the TRA was supportive in predicting early sexual behavior among African American teenage girls. Attitudes towards sex and subjective norms both correlated with intentions to participate in early sexual behavior in the study's sample.\n\nA 2011 study examining pediatricians' behaviors surrounding the Human Papillomavirus (HPV) vaccine found that TRA predicted the pediatricians would encourage parents to get their daughters vaccinated. Roberto, Krieger, Katz, Goei, and Jain discovered that the norms surrounding this topic were more important in predicting behavior than perceived behavioral control.\n\nThe public health community, interested in reducing rising obesity rates, has used TRA to study people's exercise behavior. A 1981 study by Bentler and Speckart revealed that intent to exercise was determined by a person's attitude toward exercise, as predicted by TRA. In a broader literature review on the study of exercise using TRA and TPB, it was determined that behavioral intent to exercise is better framed by TRA than TPB because perceived behavioral control did not have a significant effect on the intent to exercise.\n\nChallenges\n\nAccording to Fishbein's and Ajzen's original (1967) formulation of TRA, a behavioral intention measure will predict the performance of any voluntary act, unless intent changes prior to performance or unless the intention measure does not correspond to the behavioral criterion in terms of action, target, context, time-frame and/or specificity. The model of TRA has been challenged by studies determined to examine its limitation and inadequacy.\n\nThe major problem of TRA is pointed out to be the ignorance of the connections between individuals, both the interpersonal and social relations in which they act, and the broader social structures which govern social practice. Although TRA recognizes the importance of social norms, strategies are limited to a consideration of individual perceptions of these social phenomena. Individual's belief, attitudes and understandings are constituted activity, therefore the distinction of the two factors is ambiguous. In 1972, Shwartz and Tessler noted that there are other major and subjective determinants of intentions at play that go beyond attitudes toward the behavior and subjective norms. Namely, they propose that one's sense of right and wrongs, as well as one's beliefs surrounding moral obligation may also impact one's intention. This value system is internalized independently from Fishbein and Ajzen's subjective norms. Furthermore, social change may be generational rather than the sum of individual change. TRA fails to capture and oversimplifies the social processes of change and the social nature of the change itself: a model in which people collectively appropriate and construct new meanings and practice.\n\nAdditionally, the habituation of past behavior also tends to reduce the impact that intention has on behavior as the habit increases. Gradually, the performance of the behavior become less of a rational, initiative behavior and more of a learned response. In addition, intention appears to have a direct effect on behavior in the short term only. Besides, the analysis of the conceptual basis also raises concerns. It is criticized that the model does not enable the generation of hypothesis because of their ambiguity. The model focuses on analytic truth rather than synthetic one, therefore the conclusions resulting from those applications are often true by definition rather than by observation which makes the model unfalsifiable. The strengths of attitudes toward a behavior (social/personal) and subjective norms also vary cross-culturally while the process by which the behavior engaged remains the same. An example of this is shown in a cross-cultural study on fast food choices, where people from Western cultures were found to be more influenced by their prior choice of restaurant than people from Eastern cultures. This would suggest that people from different cultures weight subjective norms and existing attitudes differently. A closer examination of the cross-cultural communication process will benefit and complete the understanding of theory of reasoned action.\n\nFuture Directions\n\nAccording to Jaccard James, three directions are waiting for further research in TRA. The first one is per-individual level. The second area is a split-second situations, namely, instant decision-making. The third one is multioption contexts. In other words, how people perform when facing multiple alternatives should be stressed in the future study.\n\n"}
{"id": "743546", "url": "https://en.wikipedia.org/wiki?curid=743546", "title": "Time server", "text": "Time server\n\nA time server is a server computer that reads the actual time from a reference clock and distributes this information to its clients using a computer network. The time server may be a local network time server or an internet time server.\n\nThe most important and widely used protocol for distributing and synchronising time over the Internet is the Network Time Protocol (NTP), though other less-popular or outdated time protocols continue in use. A variety of protocols are in common use for sending time signals over radio links and serial connections.\n\nThe time reference used by a time server could be another time server on the network or the Internet, a connected radio clock or an atomic clock. The most common true time source is a GPS or GPS master clock. Time servers are sometimes multi-purpose network servers, dedicated network servers, or dedicated devices. All a dedicated time server does is provide accurate time.\n\nAn existing network server (e.g. a file server) can become a time server with additional software. The NTP homepage provides a free and widely used reference implementation of the NTP server and client for many popular operating systems. The other choice is a dedicated time server device.\n\nThe term \"stratum\" is used to label the closeness to a central or high quality time server. The stratum indicates the place of a particular time server in a hierarchy of servers. The scale is 1 to 15 where 1 is the most accurate and likely a highly specialized physical hardware device. Some time clients will reject a time update from a server whose stratum is too high, and most will prefer low strata time sources to higher ones. This can be a pitfall for administrators setting up an in-house time server with no true time source.\n\n\n\n"}
{"id": "19738253", "url": "https://en.wikipedia.org/wiki?curid=19738253", "title": "Tradigital art", "text": "Tradigital art\n\nTradigital art is art (including animation) that combines both traditional and computer-based techniques to create an image.\n\nArtist and teacher Judith Moncrieff first coined the term. In the early 1990s, while an instructor at the Pacific Northwest College of Art, Moncrieff invented and taught a new digital medium called \"Tradigital\". The school held a competition between Moncrieff's students, who used the medium to electronically combine everything from photographs of costumes to stills from videotapes of performing dancers. Moncrieff also referred to her business entity (formerly \"Moncrieff Studios\") as \"Tradigital Imaging\" around the same period.\n\nMoncrieff was one of five founding members of the digital art collective called \"Unique Editions\". These five artists—Helen Golden, Bonny Lhotka, Dorothy Krause, Judith Moncrieff, and Karin Schminke—combined their expertise in traditional studio media and techniques with digital imaging to produce original fine art and editions. The artists met in June, 1994, at \"Beyond the Digital Print\", a workshop organized by Krause at Massachusetts College of Art and Design in Boston. The artists' varied backgrounds are evident in their mixed media approach to using the computer as an art-making tool. Although every image is conceived and executed at least in part on the computer, the range of work includes one of a kind paintings, collages, Polaroid and image transfers, monotypes and prints on such varied substrates as canvas, handmade paper, and embossed metal. Moncrieff used the term \"Tradigital media\" to describe this merging of traditional and digital tools and \"tradigitalism\" as a name for this emerging movement. Unique Editions also served as a research and public relations entity for exploring technologies and promoting digital art. The group forged links with hardware and software developers in an effort to provide feedback on their products from the artist's perspective. It served as a demonstration to the rest of the art world of the role of digital technologies in the artist's studio. Unique Editions became inactive in 1997; however, Golden and Moncrieff continued to work together under the name, \"Tradigital Fine Art\".\n\nIndependently in the early 1990s, artist Lisa Wray was developing the fine art style she calls \"Renaissance of Metaphysical Imagery\". Prototypes were made for each work from color copies, color photos or film negatives made in her graphic arts darkroom. In 1990, she visited the only two places in the country with proprietary computer systems capable of assembling her prototypes: Raphael Digital Transparencies in Houston Texas, and Dodge Color Laboratories in Washington D.C. The first two prototypes, Brew of Life and Fantasy, were assembled by Dodge Color Laboratories on a Superset machine that was first developed by the Department of Defense. The final art was archived on 1\" magnetic tape, and then output as an 11x14” color film transparency. Lisa discovered Judith Montcrieff and her pioneering efforts with Unique Editions and Tradigital Fine Art, in the early 1990s, found the term, \"Tradigital\", and also used the term to describe her own work.\n\nSince then, use of the term has greatly expanded to include other art forms.\n\nIn 2002, \"tradigital\" went mainstream when Jeffrey Katzenberg used the term tradigital animation to refer to the blending of computer animation with classical cell animation techniques, \"a seamless blend of two-dimensional and three-dimensional animation techniques\". He mentioned as examples such animation films as \"Toy Story\", \"Antz\", \"Shrek\", \"Ice Age\", and \"\". He believed that Walt Disney (a traditional art animator) would approve of the changes in the way cartoons are made today. Animation World Magazine describes tradigital television, and the impact of tradigital animation on pre- and post-production processes for television shows.\n\nTradigital printing is an experimental approach to printmaking with contemporary technology. In one form of tradigital printing, printmakers use computers to generate positives for UV photo transfer to plates and screens. In another form, digital print output incorporating silkscreen, relief or intaglio techniques is the focus. For example, the Josephine Press uses a process that combines the use of archival digital prints with traditional techniques such as intaglio, woodcuts, lithographs, and all of the other traditional printmaking methods. The process allows the artist to create a multi-color image without using a four-plate process. In addition to more efficient registration, the artist can work with collage and other mixed media works that can be scanned and reproduced in an archival manner. Tradigital printing greatly expands the possibilities of image-making while still producing an original hand pulled, limited edition, fine art print.\n\nA recent Wall Street Journal article hailed tradigital creatives as the \"voice of tomorrow\", contrasting them with both \"traditionalists\" and \"digitalists\", and identifying several distinguishing characteristics of the new art/marketing medium: voices not eyeballs; experience not messages; community not communication; utility and solutions not cleverness; collaborative not silo thinkers.\n\n"}
{"id": "1155141", "url": "https://en.wikipedia.org/wiki?curid=1155141", "title": "Use of force", "text": "Use of force\n\nThe use of force, in the context of law enforcement, may be defined as the \"amount of effort required by police to compel compliance by an unwilling subject\".\n\nUse of force doctrines can be employed by law enforcement officers and military personnel on guard duty. The aim of such doctrines is to balance the needs of security with ethical concerns for the rights and well-being of intruders or suspects. Injuries to civilians tend to focus attention on self-defense as a justification and, in the event of death, the notion of justifiable homicide.\n\nU.S. military personnel on guard duty are given a \"use of force briefing\" by the sergeant of the guard before being assigned to their post.\n\nFor the English law on the use of force in crime prevention, see Self-defence in English law. The Australian position on the use of troops for civil policing is set out by Michael Hood in \"Calling Out the Troops: Disturbing Trends and Unanswered Questions\"; compare \"Use of Deadly Force by the South African Police Services Re-visited\" by Malebo Keebine-Sibanda and Omphemetse Sibanda.\n\nUse of force dates back to the beginning of established law enforcement, with a fear that officers would abuse their power. In today's society this fear still exists and one of the ways to fix this problem is to require police to wear body cameras and to have them turned on during all interactions with civilians.\n\nThe use of force may be standardized by a use of force continuum, which presents guidelines as to the degree of force appropriate in a given situation. One source identifies five very generalized steps, increasing from least use of force to greatest. It is only one side of the model, as it does not give the levels of subject resistance that merit the corresponding increases in force. Each successive level of force is meant to describe an escalating series of actions an officer may take to resolve a situation, and the level of force used rises only when a lower level of force would be ineffective in dealing with the situation.\nTypically any style of a use of force continuum will start with officer presence, and end with the use of deadly force.\n\n\nUse of force continuums can be further broken down.\n\nOn November 12, 1984 Graham, who was a diabetic, felt an insulin reaction coming on and rushed to the store with a friend to get some orange juice. When the store was too crowded, he and his friend proceeded to go to another friend's house. In the midst of all this, he was being watched by Officer Connor, of the Charlotte City Police Department police department. While on their way to the friend's house, the officer stopped the two of them and called for backup. After several other officers arrived, one of them handcuffed Graham. Eventually, when Connor learned that nothing had happened in the convenience store, the officers drove Graham home and released him. Over the course of the encounter, Graham sustained a broken foot, cuts on his wrists, a bruised forehead and an injured shoulder. The Supreme Court held that it was irrelevant whether Connor acted in good faith, because the use of force must be judged based on its objective reasonableness.\n\nOn October 3, 1974, Officers Elton Hymon and Leslie Wright of the Memphis Police Department were called to respond to a possible burglary. When they arrived to the scene, a woman standing on the porch began to tell them that she heard glass breaking and that she believed the house next door was being broken into. Officer Hymon went to check, where he saw Edward Garner, who was fleeing the scene. As Garner was climbing over the gate, Hymon called out \"police, halt\", and when Garner failed to do so, Hymon fatally shot Garner in the back of the head, despite being \"reasonably sure\" that Garner was unarmed. The Supreme Court held deadly force may be used to prevent the escape of a fleeing felon only if the officer has probable cause to believe that the suspect poses a serious risk to the officer or to others.\n\nOn April 16, 2004, what was supposed to be known as the \"biggest party in history\" took place at the annual UC Davis picnic. Due to the large number of participants at this party, people began to illegally park their cars. Sgt. John Wilson demanded that officers start to issue parking tickets to the illegally parked cars. Tickets were also issued to the underage drinkers. Wilson called the owner of the apartment complex because of the disturbances that were being caused; loud music and the sounds of bottles breaking. Wilson was consented by the complex apartment owner to have non-residents to leave the complex. 30 to 40 officers were rounded up with riot gear - including pepper ball guns - to try to disperse the crowd of 1,000 attendees. The officers gathered in front of the complex where 15 to 20 students, including Timothy C. Nelson, were attempting to leave, but no instructions police were given by the police. Officers began to fire pepper-balls, one of which struck Nelson in the eye. He collapsed immediately and was taken to the hospital much later on, where he suffered multiple injuries including temporary blindness and a permanent loss of visual acuity. He endured multiple surgeries to try to repair the injury. Nelson lost his athletic scholarship due to his injury and was forced to withdraw from UC Davis. The officers were unable to find any criminal charges against Nelson. The Ninth Circuit held that the use of force was unreasonable and the officers were not entitled to qualified immunity.\n\nOn July 18, 2014, a West Memphis police officer stopped Donald Rickard for a broken headlight. As the officer talked with Rickard he noticed that there was an indentation in the windshield and that Rickard was acting very erratic. The officer asked Rickard to step out of the vehicle. Rickard at that point fled the scene. A high speed chase ensued, which involved several other officers. Rickard lost control of his vehicle in a parking lot, and officers exited their vehicles to approach Rickard. Rickard again tried to flee, hitting several police cruisers and nearly hitting several officers. At this time officers opened fire on Rickard. The officers fired a total of 15 rounds which resulted in the death of both Rickard and his passenger. The Supreme Court ruled that the use of force was justified, because the objective reasonableness of the use of deadly force must be based on the situation in which it was used, and not on hindsight.\n\nOf the 40 million people in the United States who had face to face contact with the police 1.4%, or 574,000, reported use of force or the threat of use of force being directed at them. About a quarter of the 574,000 incidents involved the police officer pointing the gun at the subject of the incident and 53.5% of the incidents saw the officer using physical force such as kicking, grabbing, and pushing. In addition, 13.7% of those that had force used against them or were threatened with the use of force submitted complaints to the offending officer's department. Of those that received use of force from a police officer or were threatened with use of force almost 75% reported that they believed it was excessive and unwarranted. This statistic was consistent across the Caucasian, African American, and Hispanic races.\n\nA report by the \"Washington Post\" found that 385 Americans were fatally shot by law enforcement officers in the first five months of 2015, an average of more than two fatal shootings a day, which was more than twice the rate reported in official statistics. 221 of those killed were armed with guns, and 68 were armed with knives or other blades..\n\nStudies have shown that law enforcement personnel with some college education (typically two-year degrees) use force much less often than those with little to no higher education. In events that the educated officers do use force, it is usually what is considered \"reasonable\" force. Despite these findings, very little - only 1% - of police forces within the United States have education requirements for those looking to join their forces. Some argue that police work deeply requires experience that can only be gained from actually working in the field.\n\nIt is argued that the skills for performing law enforcement tasks well cannot be produced from a classroom setting. These skills tend to be better gained through repeated exposure to law enforcement situations while in the line of work. The results as to whether or not the amount of experience an officer has contributes to the likelihood that they will use force differ among studies.\n\nIt has not been strongly found that the race, class, gender, age etc. of an officer affects the likelihood that they will use force. Situational factors may come into play.\n\nSplit-second syndrome is an example of how use of force can be situation-based. Well-meaning officers may resort to the use of force too quickly under situations where they must make a rapid decision.\n\nPolicies on use of force can differ between departments. The type of policies established and whether or not they are enforced can affect an officer's likeliness to use force. If policies are established, but not enforced heavily by the department, the policies may not make a difference. For example, the Rodney King case was described as a problem with the departmental supervision not being clear on policies of (excessive) force. Training offered by the department can be a contributing factor, as well, though it has only been a recent addition to include information on when to use force, rather than how to use force.\n\nOne departmental level policy that is currently being studied and called for by many citizens and politicians is the use of body cameras by officers. In one study body cameras were shown to reduce the use of force by as much as 50%.\n\nAt the micro level, violent crime levels in the neighborhood increase the likelihood of law enforcement use of force. In contrast, at the meso level violent neighborhood crime does not have that much effect on use of force.\n\nIn England and Wales the use of (reasonable) force is provided to police and any other person from Section 3 of the Criminal Law Act 1967, which states:\n\n\"A person may use such force as is reasonable in the circumstances in the prevention of crime, or in effecting or assisting in the lawful arrest of offenders or suspected offenders or of persons unlawfully at large\".\n\nUse of force may be considered lawful if it was, on the basis of the facts as the accused honestly believed them, necessary and reasonable.\n\n\n\n"}
{"id": "24513408", "url": "https://en.wikipedia.org/wiki?curid=24513408", "title": "Visha Kanya", "text": "Visha Kanya\n\nThe Visha Kanya (Sanskrit ; ) were young women reportedly used as assassins, often against powerful enemies, during the times of the Ancient India. Their blood and bodily fluids were purportedly poisonous to other humans, as was mentioned in the ancient Indian treatise on statecraft, \"Arthashastra\", written by Chanakya, an adviser and a prime minister to the first Maurya Emperor Chandragupta (c. 340–293 BCE).\n\nA Hindu mythology text, the \"Kalki Purana\", mentions that they can kill a person just by looking at them, and talks about a Visha Kanya named Sulochana, the wife of a Gandharva, Chitragreeva.\n\nHowever, in time, \"poison damsel\" passed into folklore, became an archetype explored by many writers, resulting in a popular literary character that appears in many works, including classical Sanskrit texts such as \"Sukasaptati\".\n\nThe Poison Damsel (Sanskrit Viṣakanyā) is a literary figure that appears in Sanskrit literature as a type of assassin used by kings to destroy enemies. The story goes that young girls were raised on a carefully crafted diet of poison and antidote from a very young age, a practice referred to as mithridatism. Although many would not survive, those that did were immune to other poisons and their body fluids would be poisonous to others; sexual contact would thus be lethal to other humans. There also exists a myth that says a Visha Kanya can cause instant death with just a touch.\n\nAccording to Kaushik Roy, Visha Kanyas would kill their targets by seducing them and giving them poisoned alcohol.\n\nSome Sanskrit sources narrate that a \"Visha Kanya\" was sent by Nanda's minister Amatyarakshasa to kill Chandragupta Maurya and Chanakya diverted them to kill Parvatak.\n\n\"Visha Kanya\" has been a popular theme in Indian literature and folklore, and apart from appearing in classical Sanskrit texts, it has appeared repeatedly in various works like \"Vishkanya\" by Shivani and \"Ek Aur Vish Kanya?\" by Om Prakash Sharma, who use \"Visha Kanya\" as an archetype in their stories—a beautiful girl who kills when she comes too close. More recently, the archetype has taken a new hue in the HIV/AIDS era, for example in \"Vishkanya\", a 2007 novel, based on the AIDS epidemic in society. \"Vishakanya\"s have also been depicted as important characters in the book \"Chanakya's Chant\". In 2009, Vibha Rahi, a lower caste woman has written an autobiography 'Vishkanya: Untold Secrets' in Marathi, in which she portrays how upper caste women make intimate relationships with lower caste people of high profile and destroy their families and social relationships. \nOver the years, many Hindi films have been made on the subject. The first film, \"Visha Kanya\", was made in 1943, starring Leela Mishra, and more recently, \"Vishkanya\" (1991), starring Pooja Bedi in the lead role. Vishkanya Ek Anokhi Prem Kahani is a soap opera TV series which aired on Zee TV; it starred Aishwarya Khare as Aparajita Ghosh, a Visha Kanya, in the main role.\n\nIn \"The Real Life of Sebastian Knight\" the author Nabokov treats his affair with Irina Guadanini through allusions to the myth of the poison damsel. In the 2015 novel, \"The Entropy of Bones\" by Ayize Jama-Everett, the main character fights a group of \"Visha Kanya\". The trope also appears in a set of Nathaniel Hawthorne's stories.\n\n\n"}
{"id": "33426", "url": "https://en.wikipedia.org/wiki?curid=33426", "title": "Wave–particle duality", "text": "Wave–particle duality\n\nWave–particle duality is the concept in quantum mechanics that every particle or quantum entity may be partly described in terms not only of particles, but also of waves. It expresses the inability of the classical concepts \"particle\" or \"wave\" to fully describe the behavior of quantum-scale objects. As Albert Einstein wrote:\n\nThrough the work of Max Planck, Albert Einstein, Louis de Broglie, Arthur Compton, Niels Bohr and many others, current scientific theory holds that all particles exhibit a wave nature and vice versa. This phenomenon has been verified not only for elementary particles, but also for compound particles like atoms and even molecules. For macroscopic particles, because of their extremely short wavelengths, wave properties usually cannot be detected.\n\nAlthough the use of the wave-particle duality has worked well in physics, the meaning or interpretation has not been satisfactorily resolved; see Interpretations of quantum mechanics.\n\nBohr regarded the \"duality paradox\" as a fundamental or metaphysical fact of nature. A given kind of quantum object will exhibit sometimes wave, sometimes particle, character, in respectively different physical settings. He saw such duality as one aspect of the concept of complementarity. Bohr regarded renunciation of the cause-effect relation, or complementarity, of the space-time picture, as essential to the quantum mechanical account.\n\nWerner Heisenberg considered the question further. He saw the duality as present for all quantic entities, but not quite in the usual quantum mechanical account considered by Bohr. He saw it in what is called second quantization, which generates an entirely new concept of fields which exist in ordinary space-time, causality still being visualizable. Classical field values (e.g. the electric and magnetic field strengths of Maxwell) are replaced by an entirely new kind of field value, as considered in quantum field theory. Turning the reasoning around, ordinary quantum mechanics can be deduced as a specialized consequence of quantum field theory.\n\nDemocritus,the original atomist,argued that all things in the universe, including light, are composed of indivisible sub-components (light being some form of solar atom). At the beginning of the 11th Century, the Arabic scientist Ibn al-Haytham wrote the first comprehensive \"Book of optics\" describing refraction, reflection, and the operation of a pinhole lens via rays of light traveling from the point of emission to the eye. He asserted that these rays were composed of particles of light. In 1630, René Descartes popularized and accredited the opposing wave description in his treatise on light, \"The World (Descartes)\", showing that the behavior of light could be re-created by modeling wave-like disturbances in a universal medium i.e. luminiferous aether. Beginning in 1670 and progressing over three decades, Isaac Newton developed and championed his corpuscular theory, arguing that the perfectly straight lines of reflection demonstrated light's particle nature; only particles could travel in such straight lines. He explained refraction by positing that particles of light accelerated laterally upon entering a denser medium. Around the same time, Newton's contemporaries Robert Hooke and Christiaan Huygens—and later Augustin-Jean Fresnel mathematically refined the wave viewpoint, showing that if light traveled at different speeds in different media (such as water and air), refraction could be easily explained as the medium-dependent propagation of light waves. The resulting Huygens–Fresnel principle was extremely successful at reproducing light's behavior and was subsequently supported by Thomas Young's 1801 discovery of interference by his double-slit experiment. The wave view did not immediately displace the ray and particle view, but began to dominate scientific thinking about light in the mid 19th century, since it could explain polarization phenomena that the alternatives could not.\n\nJames Clerk Maxwell discovered that he could apply his previously discovered Maxwell's equations, along with a slight modification to describe self-propagating waves of oscillating electric and magnetic fields. It quickly became apparent that visible light, ultraviolet light, and infrared light were all electromagnetic waves of differing frequency. The wave theory had prevailed or at least it seemed to. While the 19th century had seen the success of the wave theory at describing light, it had also witnessed the rise of the atomic theory at describing matter.\n\nAt the close of the 19th century, the reductionism of atomic theory began to advance into the atom itself determining, through physics, the nature of the atom and the operation of chemical reactions. Electricity, first thought to be a fluid, was now understood to consist of particles called electrons. This was first demonstrated by J. J. Thomson in 1897 when, using a cathode ray tube, he found that an electrical charge would travel across a vacuum. Since the vacuum offered no medium for an electric fluid to travel, this discovery could only be explained via a particle carrying a negative charge and moving through the vacuum. This electron flew in the face of classical electrodynamics, which had successfully treated electricity as a fluid for many years (leading to the invention of batteries, electric motors, dynamos, and arc lamps). More importantly, the intimate relation between electric charge and electromagnetism had been well documented following the discoveries of Michael Faraday and James Clerk Maxwell. Since electromagnetism was known to be a wave generated by a changing electric or magnetic field (a continuous, wave-like entity itself) an atomic/particle description of electricity and charge was a non sequitur. Furthermore, classical electrodynamics was not the only classical theory rendered incomplete.\n\nIn 1901, Max Planck published an analysis that succeeded in reproducing the observed spectrum of light emitted by a glowing object. To accomplish this, Planck had to make an ad hoc mathematical assumption of quantized energy of the oscillators (atoms of the black body) that emit radiation. Einstein later proposed that electromagnetic radiation itself is quantized, not the energy of radiating atoms.\n\nBlack-body radiation, the emission of electromagnetic energy due to an object's heat, could not be explained from classical arguments alone. The equipartition theorem of classical mechanics, the basis of all classical thermodynamic theories, stated that an object's energy is partitioned equally among the object's vibrational modes. But applying the same reasoning to the electromagnetic emission of such a thermal object was not so successful. That thermal objects emit light had been long known. Since light was known to be waves of electromagnetism, physicists hoped to describe this emission via classical laws. This became known as the black body problem. Since the equipartition theorem worked so well in describing the vibrational modes of the thermal object itself, it was natural to assume that it would perform equally well in describing the radiative emission of such objects. But a problem quickly arose if each mode received an equal partition of energy, the short wavelength modes would consume all the energy. This became clear when plotting the Rayleigh–Jeans law which, while correctly predicting the intensity of long wavelength emissions, predicted infinite total energy as the intensity diverges to infinity for short wavelengths. This became known as the ultraviolet catastrophe.\n\nIn 1900, Max Planck hypothesized that the frequency of light emitted by the black body depended on the frequency of the oscillator that emitted it, and the energy of these oscillators increased linearly with frequency (according E = hν where h is Planck's constant). This was not an unsound proposal considering that macroscopic oscillators operate similarly when studying five simple harmonic oscillators of equal amplitude but different frequency, the oscillator with the highest frequency possesses the highest energy (though this relationship is not linear like Planck's). By demanding that high-frequency light must be emitted by an oscillator of equal frequency, and further requiring that this oscillator occupy higher energy than one of a lesser frequency, Planck avoided any catastrophe; giving an equal partition to high-frequency oscillators produced successively fewer oscillators and less emitted light. And as in the Maxwell–Boltzmann distribution, the low-frequency, low-energy oscillators were suppressed by the onslaught of thermal jiggling from higher energy oscillators, which necessarily increased their energy and frequency.\n\nThe most revolutionary aspect of Planck's treatment of the black body is that it inherently relies on an integer number of oscillators in thermal equilibrium with the electromagnetic field. These oscillators \"give\" their entire energy to the electromagnetic field, creating a quantum of light, as often as they are \"excited\" by the electromagnetic field, absorbing a quantum of light and beginning to oscillate at the corresponding frequency. Planck had intentionally created an atomic theory of the black body, but had unintentionally generated an atomic theory of light, where the black body never generates quanta of light at a given frequency with an energy less than hν. However, once realizing that he had quantized the electromagnetic field, he denounced particles of light as a limitation of his approximation, not a property of reality.\n\nWhile Planck had solved the ultraviolet catastrophe by using atoms and a quantized electromagnetic field, most contemporary physicists agreed that Planck's \"light quanta\" represented only flaws in his model. A more-complete derivation of black body radiation would yield a fully continuous and 'wave-like' electromagnetic field with no quantization. However, in 1905 Albert Einstein took Planck's black body model to produce his solution to another outstanding problem of the day: the photoelectric effect, wherein electrons are emitted from atoms when they absorb energy from light. Since their existence was theorized eight years previously, phenomenon had been studied with the electron model in mind in physics laboratories worldwide.\n\nIn 1902 Philipp Lenard discovered that the energy of these ejected electrons did \"not\" depend on the intensity of the incoming light, but instead on its \"frequency\". So if one shines a little low-frequency light upon a metal, a few low energy electrons are ejected. If one now shines a very intense beam of low-frequency light upon the same metal, a whole slew of electrons are ejected; however they possess the same low energy, there are merely \"more of them\". The more light there is, the more electrons are ejected. Whereas in order to get high energy electrons, one must illuminate the metal with high-frequency light. Like blackbody radiation, this was at odds with a theory invoking continuous transfer of energy between radiation and matter. However, it can still be explained using a fully classical description of light, as long as matter is quantum mechanical in nature.\n\nIf one used Planck's energy quanta, and demanded that electromagnetic radiation at a given frequency could only transfer energy to matter in integer multiples of an energy quantum hν, then the photoelectric effect could be explained very simply. Low-frequency light only ejects low-energy electrons because each electron is excited by the absorption of a single photon. Increasing the intensity of the low-frequency light (increasing the number of photons) only increases the number of excited electrons, not their energy, because the energy of each photon remains low. Only by increasing the frequency of the light, and thus increasing the energy of the photons, can one eject electrons with higher energy. Thus, using Planck's constant \"h\" to determine the energy of the photons based upon their frequency, the energy of ejected electrons should also increase linearly with frequency; the gradient of the line being Planck's constant. These results were not confirmed until 1915, when Robert Andrews Millikan, who had previously determined the charge of the electron, produced experimental results in perfect accord with Einstein's predictions. In fact, Millikan experimented for full ten years from 1906-1916 aimed at disproving Einstein's photoelectric equation.However the value of Planck's constant determined from his experiments came out to be very close to the original value. Thus, the he established the validity of Einstein's photoelectric equation instead of disproving it. \n\nWhile energy of ejected electrons reflected Planck's constant, the existence of photons was not explicitly proven until the discovery of the photon antibunching effect, of which a modern experiment can be performed in undergraduate-level labs. This phenomenon could only be explained via photons, and not through any semi-classical theory (which could alternatively explain the photoelectric effect). When Einstein received his Nobel Prize in 1921, it was not for his more difficult and mathematically laborious special and general relativity, but for the simple, yet totally revolutionary, suggestion of quantized light. Einstein's \"light quanta\" would not be called photons until 1925, but even in 1905 they represented the quintessential example of wave-particle duality. Electromagnetic radiation propagates following linear wave equations, but can only be emitted or absorbed as discrete elements, thus acting as a wave and a particle simultaneously.\n\nIn 1905, Albert Einstein provided an explanation of the photoelectric effect, a hitherto troubling experiment that the wave theory of light seemed incapable of explaining. He did so by postulating the existence of photons, quanta of light energy with particulate qualities.\n\nIn the photoelectric effect, it was observed that shining a light on certain metals would lead to an electric current in a circuit. Presumably, the light was knocking electrons out of the metal, causing current to flow. However, using the case of potassium as an example, it was also observed that while a dim blue light was enough to cause a current, even the strongest, brightest red light available with the technology of the time caused no current at all. According to the classical theory of light and matter, the strength or amplitude of a light wave was in proportion to its brightness: a bright light should have been easily strong enough to create a large current. Yet, oddly, this was not so.\n\nEinstein explained this enigma by postulating that the electrons can receive energy from electromagnetic field only in discrete portions (quanta that were called photons): an amount of energy \"E\" that was related to the frequency \"f\" of the light by\n\nwhere \"h\" is Planck's constant (6.626 × 10 J seconds). Only photons of a high enough frequency (above a certain \"threshold\" value) could knock an electron free. For example, photons of blue light had sufficient energy to free an electron from the metal, but photons of red light did not. One photon of light above the threshold frequency could release only one electron; the higher the frequency of a photon, the higher the kinetic energy of the emitted electron, but no amount of light (using technology available at the time) below the threshold frequency could release an electron. To \"violate\" this law would require extremely high-intensity lasers which had not yet been invented. Intensity-dependent phenomena have now been studied in detail with such lasers.\n\nEinstein was awarded the Nobel Prize in Physics in 1921 for his discovery of the law of the photoelectric effect.\n\nIn 1924, Louis-Victor de Broglie formulated the de Broglie hypothesis, claiming that \"all\" matter, not just light, has a wave-like nature; he related wavelength (denoted as \"λ\"), and momentum (denoted as \"p\"):\n\nThis is a generalization of Einstein's equation above, since the momentum of a photon is given by \"p\" = formula_3 and the wavelength (in a vacuum) by \"λ\" = formula_4, where \"c\" is the speed of light in vacuum.\n\nDe Broglie's formula was confirmed three years later for electrons (which differ from photons in having a rest mass) with the observation of electron diffraction in two independent experiments. At the University of Aberdeen, George Paget Thomson passed a beam of electrons through a thin metal film and observed the predicted interference patterns. At Bell Labs, Clinton Joseph Davisson and Lester Halbert Germer guided their beam through a crystalline grid.\n\nDe Broglie was awarded the Nobel Prize for Physics in 1929 for his hypothesis. Thomson and Davisson shared the Nobel Prize for Physics in 1937 for their experimental work.\n\nIn his work on formulating quantum mechanics, Werner Heisenberg postulated his uncertainty principle, which states:\n\nwhere\n\nHeisenberg originally explained this as a consequence of the process of measuring: Measuring position accurately would disturb momentum and vice versa, offering an example (the \"gamma-ray microscope\") that depended crucially on the de Broglie hypothesis. The thought is now, however, that this only partly explains the phenomenon, but that the uncertainty also exists in the particle itself, even before the measurement is made.\n\nIn fact, the modern explanation of the uncertainty principle, extending the Copenhagen interpretation first put forward by Bohr and Heisenberg, depends even more centrally on the wave nature of a particle: Just as it is nonsensical to discuss the precise location of a wave on a string, particles do not have perfectly precise positions; likewise, just as it is nonsensical to discuss the wavelength of a \"pulse\" wave traveling down a string, particles do not have perfectly precise momenta (which corresponds to the inverse of wavelength). Moreover, when position is relatively well defined, the wave is pulse-like and has a very ill-defined wavelength (and thus momentum). And conversely, when momentum (and thus wavelength) is relatively well defined, the wave looks long and sinusoidal, and therefore it has a very ill-defined position.\n\nDe Broglie himself had proposed a pilot wave construct to explain the observed wave-particle duality. In this view, each particle has a well-defined position and momentum, but is guided by a wave function derived from Schrödinger's equation. The pilot wave theory was initially rejected because it generated non-local effects when applied to systems involving more than one particle. Non-locality, however, soon became established as an integral feature of quantum theory (see EPR paradox), and David Bohm extended de Broglie's model to explicitly include it.\n\nIn the resulting representation, also called the de Broglie–Bohm theory or Bohmian mechanics, the wave-particle duality vanishes, and explains the wave behaviour as a scattering with wave appearance, because the particle's motion is subject to a guiding equation or quantum potential. \"This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored\", J.S.Bell.\n\nThe best illustration of the \"pilot-wave model\" was given by Couder's 2010 \"walking droplets\" experiments, demonstrating the pilot-wave behaviour in a macroscopic mechanical analog.\n\nSince the demonstrations of wave-like properties in photons and electrons, similar experiments have been conducted with neutrons and protons. Among the most famous experiments are those of Estermann and Otto Stern in 1929.\nAuthors of similar recent experiments with atoms and molecules, described below, claim that these larger particles also act like waves.\n\nA dramatic series of experiments emphasizing the action of gravity in relation to wave–particle duality was conducted in the 1970s using the neutron interferometer. Neutrons, one of the components of the atomic nucleus, provide much of the mass of a nucleus and thus of ordinary matter. In the neutron interferometer, they act as quantum-mechanical waves directly subject to the force of gravity. While the results were not surprising since gravity was known to act on everything, including light (see tests of general relativity and the Pound–Rebka falling photon experiment), the self-interference of the quantum mechanical wave of a massive fermion in a gravitational field had never been experimentally confirmed before.\n\nIn 1999, the diffraction of C fullerenes by researchers from the University of Vienna was reported. Fullerenes are comparatively large and massive objects, having an atomic mass of about 720 u. The de Broglie wavelength of the incident beam was about 2.5 pm, whereas the diameter of the molecule is about 1 nm, about 400 times larger. In 2012, these far-field diffraction experiments could be extended to phthalocyanine molecules and their heavier derivatives, which are composed of 58 and 114 atoms respectively. In these experiments the build-up of such interference patterns could be recorded in real time and with single molecule sensitivity.\n\nIn 2003, the Vienna group also demonstrated the wave nature of tetraphenylporphyrin—a flat biodye with an extension of about 2 nm and a mass of 614 u. For this demonstration they employed a near-field Talbot Lau interferometer. In the same interferometer they also found interference fringes for CF, a fluorinated buckyball with a mass of about 1600 u, composed of 108 atoms. Large molecules are already so complex that they give experimental access to some aspects of the quantum-classical interface, i.e., to certain decoherence mechanisms. In 2011, the interference of molecules as heavy as 6910 u could be demonstrated in a Kapitza–Dirac–Talbot–Lau interferometer. In 2013, the interference of molecules beyond 10,000 u has been demonstrated.\n\nWhether objects heavier than the Planck mass (about the weight of a large bacterium) have a de Broglie wavelength is theoretically unclear and experimentally unreachable; above the Planck mass a particle's Compton wavelength would be smaller than the Planck length and its own Schwarzschild radius, a scale at which current theories of physics may break down or need to be replaced by more general ones.\n\nRecently Couder, Fort, \"et al.\" showed that we can use macroscopic oil droplets on a vibrating surface as a model of wave–particle duality—localized droplet creates periodical waves around and interaction with them leads to quantum-like phenomena: interference in double-slit experiment, unpredictable tunneling (depending in complicated way on practically hidden state of field), orbit quantization (that particle has to 'find a resonance' with field perturbations it creates—after one orbit, its internal phase has to return to the initial state) and Zeeman effect.\n\nWave–particle duality is deeply embedded into the foundations of quantum mechanics. In the formalism of the theory, all the information about a particle is encoded in its wave function, a complex-valued function roughly analogous to the amplitude of a wave at each point in space. This function evolves according to Schrödinger equation. For particles with mass this equation has solutions that follow the form of the wave equation. Propagation of such waves leads to wave-like phenomena such as interference and diffraction. Particles without mass, like photons, have no solutions of the Schrödinger equation so have another wave.\n\nThe particle-like behavior is most evident due to phenomena associated with measurement in quantum mechanics. Upon measuring the location of the particle, the particle will be forced into a more localized state as given by the uncertainty principle. When viewed through this formalism, the measurement of the wave function will randomly lead to wave function collapse, or rather quantum decoherence, to a sharply peaked function at some location. For particles with mass the likelihood of detecting the particle at any particular location is equal to the squared amplitude of the wave function there. The measurement will return a well-defined position,and subject to Heisenberg's uncertainty principle. It is important to note that a measurement is only a particular type of interaction where some data is recorded and the measured quantity is forced into a particular quantum state. The act of measurement is therefore not fundamentally different from any other interaction.\n\nFollowing the development of quantum field theory the ambiguity disappeared. The field permits solutions that follow the wave equation, which are referred to as the wave functions. The term particle is used to label the irreducible representations of the Lorentz group that are permitted by the field. An interaction as in a Feynman diagram is accepted as a calculationally convenient approximation where the outgoing legs are known to be simplifications of the propagation and the internal lines are for some order in an expansion of the field interaction. Since the field is non-local and quantized, the phenomena which previously were thought of as paradoxes are explained. Within the limits of the wave-particle duality the quantum field theory gives the same results.\n\nThere are two ways to visualize the wave-particle behaviour: by the \"standard model\", described below; and by the Broglie–Bohm model, where no duality is perceived.\n\nBelow is an illustration of wave–particle duality as it relates to De Broglie's hypothesis and Heisenberg's uncertainty principle (above), in terms of the position and momentum space wavefunctions for one spinless particle with mass in one dimension. These wavefunctions are Fourier transforms of each other.\n\nThe more localized the position-space wavefunction, the more likely the particle is to be found with the position coordinates in that region, and correspondingly the momentum-space wavefunction is less localized so the possible momentum components the particle could have are more widespread.\n\nConversely the more localized the momentum-space wavefunction, the more likely the particle is to be found with those values of momentum components in that region, and correspondingly the less localized the position-space wavefunction, so the position coordinates the particle could occupy are more widespread.\n\nWave–particle duality is an ongoing conundrum in modern physics. Most physicists accept wave-particle duality as the best explanation for a broad range of observed phenomena; however, it is not without controversy. Alternative views are also presented here. These views are not generally accepted by mainstream physics, but serve as a basis for valuable discussion within the community.\n\nThe pilot wave model, originally developed by Louis de Broglie and further developed by David Bohm into the hidden variable theory proposes that there is no duality, but rather a system exhibits both particle properties and wave properties simultaneously, and particles are guided, in a deterministic fashion, by the pilot wave (or its \"quantum potential\") which will direct them to areas of constructive interference in preference to areas of destructive interference. This idea is held by a significant minority within the physics community.\n\nAt least one physicist considers the \"wave-duality\" as not being an incomprehensible mystery. L.E. Ballentine, \"Quantum Mechanics, A Modern Development\", p. 4, explains:\n\nWhen first discovered, particle diffraction was a source of great puzzlement. Are \"particles\" really \"waves?\" In the early experiments, the diffraction patterns were detected holistically by means of a photographic plate, which could not detect individual particles. As a result, the notion grew that particle and wave properties were mutually incompatible, or complementary, in the sense that different measurement apparatuses would be required to observe them. That idea, however, was only an unfortunate generalization from a technological limitation. Today it is possible to detect the arrival of individual electrons, and to see the diffraction pattern emerge as a statistical pattern made up of many small spots (Tonomura et al., 1989). Evidently, quantum particles are indeed particles, but whose behaviour is very different from classical physics would have us to expect. \n\nThe Afshar experiment (2007) may suggest that it is possible to simultaneously observe both wave and particle properties of photons. This claim is, however, disputed by other scientists.\n\nCarver Mead, an American scientist and professor at Caltech, proposes that the duality can be replaced by a \"wave-only\" view. In his book \"Collective Electrodynamics: Quantum Foundations of Electromagnetism\" (2000), Mead purports to analyze the behavior of electrons and photons purely in terms of electron wave functions, and attributes the apparent particle-like behavior to quantization effects and eigenstates. According to reviewer David Haddon:\n\nMead has cut the Gordian knot of quantum complementarity. He claims that atoms, with their neutrons, protons, and electrons, are not particles at all but pure waves of matter. Mead cites as the gross evidence of the exclusively wave nature of both light and matter the discovery between 1933 and 1996 of ten examples of pure wave phenomena, including the ubiquitous laser of CD players, the self-propagating electrical currents of superconductors, and the Bose–Einstein condensate of atoms.\n\nAlbert Einstein, who, in his search for a Unified Field Theory, did not accept wave-particle duality, wrote:\n\nThis double nature of radiation (and of material corpuscles) ... has been interpreted by quantum-mechanics in an ingenious and amazingly successful fashion. This interpretation ... appears to me as only a temporary way out...\n\nThe many-worlds interpretation (MWI) is sometimes presented as a waves-only theory, including by its originator, Hugh Everett who referred to MWI as \"the wave interpretation\".\n\nThe \"\" of R. Horodecki relates the particle to wave. The hypothesis implies that a massive particle is an intrinsically spatially, as well as temporally extended, wave phenomenon by a nonlinear law.\n\nThe ' considers collapse and measurement as two independent physical processes. Collapse occurs when two wavepackets spatially overlap and satisfy a mathemetical criterion, which depends on the parameters of both wavepackets. It is a contraction to the overlap volume. In a measurement apparatus one of the two wavepackets is one of the atomic clusters, which constitute the apparatus, and the wavepackets collapse to at most the volume of such a cluster. This mimics the action of a point particle.\n\nStill in the days of the old quantum theory, a pre-quantum-mechanical version of wave–particle duality was pioneered by William Duane, and developed by others including Alfred Landé. Duane explained diffraction of x-rays by a crystal in terms solely of their particle aspect. The deflection of the trajectory of each diffracted photon was explained as due to quantized momentum transfer from the spatially regular structure of the diffracting crystal.\n\nIt has been argued that there are never exact particles or waves, but only some compromise or intermediate between them. For this reason, in 1928 Arthur Eddington coined the name \"wavicle\" to describe the objects although it is not regularly used today. One consideration\nis that zero-dimensional mathematical points cannot be observed. Another is that the formal representation of such points, the Dirac delta function is unphysical, because it cannot be normalized. Parallel arguments apply to pure wave states. Roger Penrose states:\n\n\"Such 'position states' are idealized wavefunctions in the opposite sense from the momentum states. Whereas the momentum states are infinitely spread out, the position states are infinitely concentrated. Neither is normalizable [...].\"\nRelational quantum mechanics has been developed as a point of view that regards the event of particle detection as having established a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg’s uncertainty principle is consequently avoided; hence there is no wave-particle duality.\n\nAlthough it is difficult to draw a line separating wave–particle duality from the rest of quantum mechanics, it is nevertheless possible to list some applications of this basic idea.\n\n\n"}
{"id": "894706", "url": "https://en.wikipedia.org/wiki?curid=894706", "title": "White Paper of 1939", "text": "White Paper of 1939\n\nThe White Paper of 1939 was a policy paper issued by the British government under Neville Chamberlain in response to the 1936–39 Arab Revolt. Following its formal approval in the House of Commons on 23 May 1939, it acted as the governing policy for Mandatory Palestine from 1939 until the British departure in 1948, the matter of the Mandate meanwhile having been referred to the United Nations.\n\nThe policy, first drafted in March 1939, was prepared by the British government unilaterally as a result of the failure of the Arab-Zionist London Conference. The paper called for the establishment of a Jewish national home in an independent Palestinian state within 10 years, rejecting the idea of partitioning Palestine. It also limited Jewish immigration to 75,000 for 5 years, and ruled that further immigration was to be determined by the Arab majority (section II). Restrictions were put on the rights of Jews to buy land from Arabs (section III).\n\nThe proposal did not meet the political demands proposed by Arab representatives during the London Conference and was officially rejected by the representatives of Palestine Arab parties acting under the influence of Haj Amin Eff el Husseini while more moderate Arab opinion represented in the National Defence Party was prepared to accept the White Paper.\n\nZionist groups in Palestine immediately rejected the White Paper. There was a campaign of attacks on government property, which lasted for several months. On 18 May a Jewish general strike was called.\n\nRegulations governing land transfers and clauses relating to immigration were implemented although at the end of the five-year period in 1944, only 51,000 of the 75,000 immigration certificates provided for had been utilized. In circumstances where Jewish refugees from Europe were fleeing violence and persecution, the White Paper's limits were relaxed and legal immigration was permitted to continue indefinitely at the rate of 18,000 a year. Key provisions were ultimately never to be implemented, initially because of cabinet opposition following the change in government, and later because of preoccupation with World War II.\n\nDuring World War I, the British had made two promises regarding territory in the Middle East. Britain had promised the Hashemite governors of Arabia, through Lawrence of Arabia and the Hussein-McMahon Correspondence, independence for a united Arab country covering Syria in exchange for their supporting the British against the Ottoman Empire. The Ottoman Caliphate had declared a military jihad in support of the Germans and it was hoped that an alliance with the Arabs would quell the chances of a general Muslim uprising in British-held territories in Africa, India, and the Far East. Great Britain had also negotiated the Sykes-Picot Agreement, agreeing to partition the Middle East between Britain and France.\n\nA variety of strategic factors, such as securing Jewish support in Eastern Europe as the Russian front collapsed, culminated in the Balfour Declaration, 1917, with Britain promising to create and foster a Jewish national home in Palestine. These broad delineations of territory and goals for both the creation of a Jewish homeland in Palestine, and Arab self-determination was approved in the San Remo conference.\n\nIn June 1922 the League of Nations approved the Palestine Mandate with effect from September 1923. The Palestine Mandate was an explicit document regarding Britain's responsibilities and powers of administration in Palestine including 'secur[ing] the establishment of the Jewish national home', and 'safeguarding the civil and religious rights of all the inhabitants of Palestine'. In September 1922, the British government presented a memorandum to the League of Nations stating that Transjordan would be excluded from all the provisions dealing with Jewish settlement, in accordance with Article 25 of the Mandate, and this memorandum was approved on 23 September. Due to stiff Arab opposition and pressure against Jewish immigration, Britain redefined Jewish immigration by restricting its flow according to the country's economic capacity to absorb the immigrants. In effect annual quotas were put in place as to how many Jews could immigrate, while Jews possessing a large sum of money (£500) were allowed to enter the country freely.\n\nFollowing Adolf Hitler's rise to power, a growing number of European Jews were prepared to spend the money necessary to enter Palestine. The 1935 Nuremberg Laws stripped the 500,000 German Jews of their citizenship. Jewish migration was impeded by Nazi restrictions on the transfer of finances abroad (departing Jews had to abandon their property), but the Jewish Agency was able to negotiate an agreement allowing Jews resident in Germany to buy German goods for export to Palestine thus circumventing the restrictions.\n\nThe large numbers of Jews entering Palestine led to the 1936–39 Arab revolt in Palestine. Britain responded to the Arab revolt by appointing a Royal Commission, known as the Peel Commission which traveled out to Palestine and undertook a thorough study of the issues. The Peel Commission recommended in 1937 that Palestine be partitioned into two states, one Arab the other Jewish. In January 1938, the Woodhead Commission explored the practicalities of partition. The Woodhead Commission considered three different plans, one of which was based on the Peel plan. Reporting in 1938, the Commission rejected the Peel plan primarily on the grounds that it could not be implemented without a massive forced transfer of Arabs (an option that the British government had already ruled out). With dissent from some of its members, the Commission instead recommended a plan that would leave the Galilee under British mandate, but emphasised serious problems with it that included a lack of financial self-sufficiency of the proposed Arab State. The British Government accompanied the publication of the Woodhead Report by a statement of policy rejecting partition as impracticable due to \"political, administrative and financial difficulties\". It proposed a substantially smaller Jewish state, including the coastal plain only. An international conference (Évian Conference) convened by the United States in July 1938, failed to find any agreement to deal with the rapidly growing number of Jewish refugees.\n\nIn February 1939 the British called the London Conference to negotiate an agreement between Arabs and Jews in Palestine. The Arab delegates attended on condition that they would not meet directly with the Jewish representatives, which would constitute recognition of Jewish claims over Palestine. So the British government held separate meetings with the two sides. The conference ended in failure on March 17.\n\nIn the wake of World War II, the British believed that Jewish support was guaranteed or unimportant. However they feared that the Arab world might turn against them. This geopolitical consideration was, in Raul Hilberg's word, \"decisive\" to British policies. Egypt, Iraq and Saudi Arabia were independent and allied with Britain.\n\nThe main points of the White Paper were:\n\nHis Majesty's Government believe that the framers of the Mandate in which the Balfour Declaration was embodied could not have intended that Palestine should be converted into a Jewish State against the will of the Arab population of the country. [ ... ] His Majesty's Government therefore now declare unequivocally that it is not part of their policy that Palestine should become a Jewish State. They would indeed regard it as contrary to their obligations to the Arabs under the Mandate, as well as to the assurances which have been given to the Arab people in the past, that the Arab population of Palestine should be made the subjects of a Jewish State against their will.\n\nThe objective of His Majesty's Government is the establishment within 10 years of an independent Palestine State in such treaty relations with the United Kingdom as will provide satisfactorily for the commercial and strategic requirements of both countries in the future. [..] The independent State should be one in which Arabs and Jews share government in such a way as to ensure that the essential interests of each community are safeguarded.\n\n\nHis Majesty's Government do not [..] find anything in the Mandate or in subsequent Statements of Policy to support the view that the establishment of a Jewish National Home in Palestine cannot be effected unless immigration is allowed to continue indefinitely. If immigration has an adverse effect on the economic position in the country, it should clearly be restricted; and equally, if it has a seriously damaging effect on the political position in the country, that is a factor that should not be ignored. Although it is not difficult to contend that the large number of Jewish immigrants who have been admitted so far have been absorbed economically, the fear of the Arabs that this influx will continue indefinitely until the Jewish population is in a position to dominate them has produced consequences which are extremely grave for Jews and Arabs alike and for the peace and prosperity of Palestine. The lamentable disturbances of the past three years are only the latest and most sustained manifestation of this intense Arab apprehension [ ... ] it cannot be denied that fear of indefinite Jewish immigration is widespread amongst the Arab population and that this fear has made possible disturbances which have given a serious setback to economic progress, depleted the Palestine exchequer, rendered life and property insecure, and produced a bitterness between the Arab and Jewish populations which is deplorable between citizens of the same country. If in these circumstances immigration is continued up to the economic absorptive capacity of the country, regardless of all other considerations, a fatal enmity between the two peoples will be perpetuated, and the situation in Palestine may become a permanent source of friction amongst all peoples in the Near and Middle East.\n\nJewish immigration during the next five years will be at a rate which, if economic absorptive capacity permits, will bring the Jewish population up to approximately one third of the total population of the country. Taking into account the expected natural increase of the Arab and Jewish populations, and the number of illegal Jewish immigrants now in the country, this would allow of the admission, as from the beginning of April this year, of some 75,000 immigrants over the next four years. These immigrants would, subject to the criterion of economic absorptive capacity, be admitted as follows: For each of the next five years a quota of 10,000 Jewish immigrants will be allowed on the understanding that a shortage one year may be added to the quotas for subsequent years, within the five-year period, if economic absorptive capacity permits. In addition, as a contribution towards the solution of the Jewish refugee problem, 25,000 refugees will be admitted as soon as the High Commissioner is satisfied that adequate provision for their maintenance is ensured, special consideration being given to refugee children and dependents. The existing machinery for ascertaining economic absorptive capacity will be retained, and the High Commissioner will have the ultimate responsibility for deciding the limits of economic capacity. Before each periodic decision is taken, Jewish and Arab representatives will be consulted. After the period of five years, no further Jewish immigration will be permitted unless the Arabs of Palestine are prepared to acquiesce in it.\n\nThe Reports of several expert Commissions have indicated that, owing to the natural growth of the Arab population and the steady sale in recent years of Arab land to Jews, there is now in certain areas no room for further transfers of Arab land, whilst in some other areas such transfers of land must be restricted if Arab cultivators are to maintain their existing standard of life and a considerable landless Arab population is not soon to be created. In these circumstances, the High Commissioner will be given general powers to prohibit and regulate transfers of land.\n\nOn 22 May 1939 the House of Commons debated a motion that the White Paper was inconsistent with the terms of the Mandate. It was defeated by 268 votes to 179. The following day the House of Lords accepted the new policy without a vote.\n\nDuring the debate, Lloyd George called the White Paper an \"act of perfidy\" while Winston Churchill voted against the government of his party. The Liberal MP James Rothschild stated during the parliamentary debate that \"for the majority of the Jews who go to Palestine it is a question of migration or of physical extinction\".\n\nSome supporters of the Conservative Government were opposed to the policy on the grounds that it appeared in their view to contradict the Balfour Declaration. Several government MPs either voted against the proposals or abstained, including Cabinet Ministers such as the illustrious Jewish Secretary of State for War Leslie Hore-Belisha.\n\nThe supervising authority of the League of Nations, the Permanent Mandates Commission abstained unanimously from endorsing the White Paper, though four out of seven members thought the new policy was inconsistent with that mandate. The League of Nations commission rejected the White Paper because it was in conflict with the terms of the Mandate as put forth in the past. The outbreak of the Second World War suspended any further deliberations.\n\nThe Arab Higher Committee initially argued that the independence of a future Palestine Government would prove to be illusory, as the Jews could prevent its functioning by withholding participation, and in any case real authority would still be in the hands of British officials. The limitations on Jewish immigration were also held to be insufficient, as there was no guarantee immigration would not resume after five years. In place of the policy enunciated in the White Paper, the Arab Higher Committee called for \"a complete and final prohibition\" of Jewish immigration and a repudiation of the Jewish national home policy altogether.\n\nIn June 1939, Hajj Amin al-Husayni initially \"astonished\" the other members of the Arab Higher Committee by turning down the White Paper. Al-Husayni, according to Benny Morris, turned the advantageous proposal down for the entirely selfish reason that \"it did not place him at the helm of the future Palestinian state.\"\n\nIn July 1940, after two weeks of meetings with the British representative S. F. Newcombe, the leader of the Palestinian Arab delegates to the London Conference, Jamal al-Husseini and fellow delegate Musa al-Alami, agreed to the terms of the White Paper and both signed a copy of it in the presence of the Prime Minister of Iraq, Nuri as-Said.\n\nZionist groups in Palestine immediately rejected the White Paper and began a campaign of attacks on government property and Arab civilians which lasted for several months. On 18 May a Jewish general strike was called.\n\nOn 27 February 1939, in response to enthusiastic Arab demonstrations following reports that the British were proposing to allow Palestine independence on the same terms as Iraq, a coordinated Irgun bombing campaign across the country killed 38 Arabs and wounded 44.\n\nIn response to the White Paper, the right-wing Zionist militant group Irgun began formulating plans for a rebellion to evict the British and establish an independent Jewish state. Ze'ev Jabotinsky, the founder of Irgun, who had been exiled from Palestine by the British, proposed a plan for a revolt to take place in October 1939, which he sent to the Irgun High Command in six coded letters. Under Jabotinsky's plan, he, together with other \"illegals\", would arrive in Palestine by boat, and the Irgun would help him and other passengers escape. Next, the Irgun would raid and occupy Government House, as well as other British centers of power in Palestine, raise the Jewish national flag, and hold them for at least 24 hours even at a heavy cost. Simultaneously, Zionist leaders in Western Europe and the United States would proclaim an independent Jewish state in Palestine, and would function as a government-in-exile. Irgun seriously considered carrying out the plan, but was concerned over the heavy losses it would doubtless incur. Irgun leader Avraham Stern (who would later break from Irgun to form Lehi), formed a plan for 40,000 armed Jewish fighters recruited in Europe to sail to Palestine and join the rebellion. The Polish government supported his plan, and began training Jews and setting aside weaponry for them. However, the outbreak of World War II in September 1939 quickly put an end to these plans.\n\nAfter the outbreak of war in September 1939, the head of the Jewish Agency for Palestine David Ben-Gurion declared: 'We will fight the White Paper as if there is no war, and fight the war as if there is no White Paper.'\n\nOn 13 July the authorities announced the suspension of all Jewish immigration into Palestine until March 1940. The reason given for this decision was the increase in illegal immigrants arriving.\n\nIn March 1940, the British High Commissioner for Palestine issued an edict dividing Palestine into three zones. \n\nIn Zone A, consisting of about 63 percent of the country including the stony hills, land transfers save to a Palestinian Arab were in general forbidden. In Zone B. consisting of about 32 percent of the country, transfers from a Palestinian Arab save to another Palestinian Arab were severely restricted at the discretion of the High Commissioner. In the remainder of Palestine, consisting of about five percent of the country-which, however, includes the most fertile areas - land sales remained unrestricted.\n\nIn December 1942, when extermination of the Jews became public knowledge, there were 34,000 immigration certificates remaining. In February 1943, the British government announced that the remaining certificates could be used as soon as practicable to rescue Jewish children from southeastern Europe, particularly Bulgaria. This plan was partly successful but many people who received certificates were not able to emigrate (but those in Bulgaria survived). In July it was announced that any Jewish refugee who reached a neutral country in transit would be given clearance for Palestine. During 1943 about half the remaining certificates were distributed, and by the end of the war there were 3,000 certificates left.\n\nAt the end of World War II, the British Labour Party conference voted to rescind the White Paper and establish a Jewish state in Palestine, however the Labour Foreign Minister, Ernest Bevin persisted with the policy and it remained in effect until the British departed Palestine in May 1948.\n\nAfter the war, the determination of Holocaust survivors to reach Palestine led to large scale illegal Jewish migration to Palestine. British efforts to block the migration led to violent resistance by the Zionist underground.\n\nIllegal immigrants detained by the British Government were interned in camps on Cyprus. The immigrants had no citizenship and could not be returned to any country. Those interned included a large number of children and orphans.\n\nFrom October 1946, the British Government, under the 'severest pressure' from the USA, relented and allowed 1,500 Jewish migrants a month into Palestine. The gesture was in deference to the recommendations of the Anglo-American Committee of Inquiry. Half of those admitted came from the prison camps for illegal immigrants in Cyprus due to fears that a growing Jewish presence in Cyprus would lead to an uprising there.\n\nThe Provisional Council of Israel's first constitutional act was a Proclamation that \"All legislation resulting from the British Government's White Paper of May, 1939, will at midnight tonight become null and void. This includes the immigration provisions as well as the land transfer regulations of February, 1940.\"\n\n\n\n"}
{"id": "1834131", "url": "https://en.wikipedia.org/wiki?curid=1834131", "title": "Yabo", "text": "Yabo\n\nThe word \"yabo\" was often used by city dwellers, or \"Chōnin\" (especially those of Edo). It often refers to samurai and farmers (\"nomin)\" from outside Edo, but could also be applied to another \"chonin\". The city dwellers of Edo sometimes called themselves \"Edokko\" (similar to \"New Yorker\" or \"Parisian\"). Proud of having been born and raised in Edo, they had a tendency to despise outsiders. However, the origins of many chonin could be traced back to other areas and backgrounds.\n\nThe meaning of the term has expanded and generalized through the modernization of Japan. Today, the word \"yabo\" is used more frequently than \"iki\".\n\n"}
{"id": "37752", "url": "https://en.wikipedia.org/wiki?curid=37752", "title": "Éraic", "text": "Éraic\n\nÉraic (or \"eric\") was the Irish equivalent of the Welsh galanas and the Anglo-Saxon and Scandinavian weregild, a form of tribute paid in reparation for murder or other major crimes. The term survived into the sixteenth century as \"\", by then relating only to compensation for the killing of an Irishman. In the case of homicide, if the attacker fled, the fine had to be paid by the tribe to which he belonged.\n\nIn Irish mythology the eraic takes an important place. In the \"Oidheadh Chloinne Tuireann\", the children of Tuirreann owe an eraic to Lugh. Lug set them a series of seemingly impossible quests as recompense. They achieved them all, but were fatally wounded in completing the last one.\n\n"}
