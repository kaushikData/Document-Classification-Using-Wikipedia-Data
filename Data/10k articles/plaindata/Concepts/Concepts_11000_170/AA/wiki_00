{"id": "34025356", "url": "https://en.wikipedia.org/wiki?curid=34025356", "title": "2011 Crosstown Shootout brawl", "text": "2011 Crosstown Shootout brawl\n\nThe 2011 Crosstown Shootout brawl, nicknamed The Crosstown Punch-Out, was a bench-clearing brawl that took place at the end of the 2011 edition of the Crosstown Shootout college basketball game between the University of Cincinnati Bearcats and the Xavier University Musketeers. The game took place on December 10, 2011 at Xavier's home arena, the Cintas Center in Norwood.\n\nThe Crosstown Shootout has always been one of the more intense rivalries in college basketball, all the more so because Xavier and UC are only two miles () apart. There has always been a fair amount of trash talk between the two schools, and the 2011 game was no different. On December 8, UC guard Sean Kilpatrick told Andy Furman of WQRT that Xavier's All-American guard Tu Holloway probably wouldn't start for the Bearcats \"with the players that we have now.\"\n\nThe series had developed a history of bad blood in the years leading up to the 2011 Crosstown Shootout.<br>\nIn 2008, a total of \"six\" technicals were called in a 76-66 XU win. XU's Derrick Brown was ejected, and two freshman post players were involved in an altercation that included a heavy dose of foreshadowing: XU's Kenny Frease head-butted UC's Yancy Gates (both players received technicals). The altercation between Frease and Gates in 2008 set the tone for bad blood between the two, and may have motivated Gates to seek revenge in 2011.<br>\nIn 2009, multiple verbal exchanges resulting in UC clearing their bench as well as technicals called on XU's Jordan Crawford and UC's Rashad Bishop in XU's double overtime win.<br>\nIn 2010, Xavier's Tu Holloway received a technical foul for throwing an elbow late in UC's 66-46 win.<br>\n\nThe game was close at first, with eight ties and six lead changes in the first half. Xavier led 34-25 at halftime. The first sign of trouble occurred in the closing seconds of the first half, when Bearcat backup Octavius Ellis began exchanging some words with Xavier player Mark Lyons from the bench. Ellis jumped up from the bench to confront Lyons, and the two players had to be separated. Before the start of the second half, according to Xavier coach Chris Mack, both teams were warned that any further incident would result in technical fouls. The Musketeers started the second half on a 9-2 run and were never seriously threatened afterward. Ultimately, the Musketeers outscored the Bearcats 42-28 in the second half.\n\nWith 18.6 seconds remaining, Tu Holloway scored a layup to give the Musketeers the 76-53 lead. As Holloway walked to the other end of the court, he began shouting at the Bearcat bench, bringing UC's Ge'Lawn Guyn over to confront him, leading to an argument between both men. As Guyn reached for Holloway's throat, Xavier's Dezmine Wells intervened by pushing Guyn to the ground, causing UC's Yancy Gates to throw the ball at Holloway's head, and both benches to empty. In the ensuing fight, Gates punched Xavier's Kenny Frease in the face, opening up a large cut under Frease's left eye and knocking him to the ground. As Frease tried to crawl away from the crowd, UC's Cheikh Mbodj stomped on the back of his head. Ellis threw a punch at Lyons, causing him and Wells to throw several back, before coaches got in between them. This brought Gates over, who began shouting and throwing more punches at a Xavier backup, until Xavier assistant Aaron Williams and Lyons calmed him down. Referees Michael Roberts, Jeff Anderson, and Tony Crisp ended the game with 9.4 seconds remaining, giving Xavier the 76-53 win. UC's Gates and Mbodj, and Xavier's Wells were retroactively ejected from the game for fighting. Under NCAA rules, they were suspended for their next game, a sanction that cannot be appealed.\n\nImmediately following the incident, both sides were escorted to the tunnels at either end of the court so that they would enter their respective locker rooms. However, due to the chaos of the brawl, both teams entered through the tunnels opposite from where they were supposed to go. This would have resulted in another confrontation in the hallways beneath the Cintas Center stands as both teams would have had to cross each other in the narrow hallway leading to their locker rooms. To avoid this, the Xavier team was corralled into the south tunnel leading out of the main arena. Once the UC players were off the court and had filed through the north tunnel to their locker room, the Xavier players were brought back out and began celebrating with the remaining fans and students still in the stands. This however resulted in an incident between the Xavier players and the Cincinnati fans who began yelling obscenities at one another. Security again had to get involved to keep the two groups away from each other.\n\nUC coach Mick Cronin, angry at how his players behaved, ordered them to take off their jerseys in the locker room, and in some cases physically took them off. In his postgame comments, a visibly angry Cronin said that he told his players not to put them on again \"until they have a full understanding of where they go to school and what the university stands for and how lucky they are to even be there, let alone have a scholarship.\" He also hinted that several players could face suspensions, and possibly dismissal from the team, saying that he was going to review the tape with school president Gregory H. Williams and athletic director Whit Babcock \"and decide who's going to be on my team going forward.\" He also appeared to blame the referees for not taking control of the situation sooner (threats of technicals after the first-half incident notwithstanding), saying that Musketeer players were yelling at his bench for the whole game and even cursing at his coaches. Cronin said he tried to call a time out before the brawl broke out so the players could go to their respective benches, but couldn't get the officials' attention.\n\nIn the postgame interview, Holloway claimed the Bearcats had \"disrespected\" his teammates in the run-up to the game. He also said, \"We went out there and zipped them up at the end of the game.\" He also added that he and his teammates considered themselves \"gangsters, not thugs, but tough guys on the court.\" Lyons added, \"If somebody puts their hand in your face or tries to do something to you, where we're from you're gonna do something back.\"\n\nLater that night, Cronin spoke with ESPN's Andy Katz and reiterated that there would be additional suspensions coming. \"Nobody is going to walk on our side,\" he said. He also said this was the most embarrassing moment of his coaching career. Mack was somewhat more restrained in his remarks, saying he wasn't \"in a position to be a decision maker,\" but that his players needed to learn \"how to handle themselves and not let that happen again.\" \nEarlier, Mack had tweeted, \"If my players say they've been taught to be tough their whole life, they mean ON THE FLOOR. Nothing else is condoned.\"\n\nBoth schools' presidents issued statements condemning the brawl. UC's Williams strongly supported Cronin's postgame remarks, saying that the brawl was \"not what we expect of representatives of the University of Cincinnati\" and that school officials would \"act swiftly and firmly\" to ensure it never happened again. Xavier's Michael J. Graham called the brawl \"unsportsmanlike\" and apologized to Xavier's fans and \"the entire Cincinnati community.\"\n\nThe Big East Conference and Atlantic 10 Conference, the home conferences for UC and Xavier respectively at the time, also condemned the brawl and promised to hand down additional suspensions if they felt the schools haven't acted harshly enough.\n\nOn December 11, both schools announced suspensions for the players involved. UC suspended Gates, Mbodj and Ellis for six games each, and Guyn for one game. Cronin said that the four players would still have to earn their way back onto the team even after they serve out their suspensions. At the very least, Cronin said, they would have to sincerely apologize on camera and complete several other unspecified tasks. Xavier suspended Wells and Landen Amos for four games, Lyons for two and Holloway for one. Xavier athletic director Mike Bobinski also apologized for Holloway and Lyons' postgame remarks. Mack said that Holloway was suspended specifically for his postgame comments, and admitted that neither he nor Lyons should have been before the media.\n\nOn December 11, Mack, Holloway, Lyons and Xavier Athletic Director Mike Bobinski all apologized for what occurred. Mike Bobinski said, \"It was an embarrassment. It was a disappointment at every level. We take full responsibility for the role that we played in those events and are resolved to whatever is necessary to see to it that it never happens again.\" Tu Holloway also apologized for post game comments saying, \"Myself, I used the wrong choice of words. I represent Xavier University, my family and myself and I really apologize for what took place.\"\n\nOn December 12, an emotional Gates apologized for his actions, saying they were \"not what my family is about.\" Guyn, Ellis and Mbodj also apologized. On the same day, Hamilton County, Ohio prosecutor Joe Deters said he was considering filing criminal charges related to the brawl. Under Ohio law, Deters' office would handle the case if the charges rose to the level of felonies; Cincinnati city solicitor John Curp would prosecute if they are only misdemeanors. There is no known instance of criminal charges being filed as a result of a brawl at an NCAA game. Two days later, Deters announced he would not file charges. He also indicated that Gates and Frease had \"reached out to each other privately\", and that Frease was satisfied with a personal apology from Gates. On that same day XU athletic Director Mike Bobinski used head coach Chris Mack's radio show to again apologize and take questions from fans concerning the incident.\n\nOn December 18, prior to Xavier's game against Oral Roberts, Mack took the floor again to apologize for the incident that had occurred in the previous game. Mack said, \"We were all embarrassed by our behavior last weekend,\" and added, \"It's extremely disappointing and in no way was a representation of what our university and our basketball program is all about.\"\n\nPartly due to the incident, Xavier and UC officials decided to move the game to U.S. Bank Arena for at least the next two seasons and rename it the Crosstown Classic. Player and fan behavior was extensively reevaluated after the 2013 game, and was said to be a major factor in the decision to continue the series beyond the 2013–14 season. On May 12, 2014, the two schools announced a 10-year agreement to continue the series, alternating between campuses, and to revert the name of the series back to the Crosstown Shootout.\n\n"}
{"id": "57952655", "url": "https://en.wikipedia.org/wiki?curid=57952655", "title": "Arab American Association of New York", "text": "Arab American Association of New York\n\nThe Arab American Association of New York is an Arab American and Muslim civil rights organization located in New York.\n\nLinda Sarsour was the former executive director of the organization until stepping down from the position in 2017.\n"}
{"id": "3818211", "url": "https://en.wikipedia.org/wiki?curid=3818211", "title": "Atheist Centre", "text": "Atheist Centre\n\nThe Atheist Centre is an institution founded by Goparaju Ramachandra Rao (aka Gora, 1902–1975) and Saraswathi Gora (1912–2006) to initiate social change in rural Andhra Pradesh based on the ideology of Gandhism and Atheism. Founded in 1940 at Mudnur village in Krishna district in Andhra Pradesh, India the Centre was later shifted to Vijayawada in 1947.\nAs a member organisation of the Federation of Indian Rationalist Associations, the Atheist Centre endorses the Amsterdam Declaration 2002.The institution received the International Humanist and Ethical Union's International Humanist Award in 1986 for pioneering in the field of social work.\nAfter the Indian Ocean tsunami, the Atheist Centre worked with the Institute for Humanist Studies to provide aid to the victim of the disaster.\nAtheist Centre celebrated its platinum jubilee of 75 years on 5 January 2014. Gora's daughter Mythri is currently the Chairperson, and sons Lavanam and Vijyam continuing as Executive Directors of the Centre.\n\nWith the guiding principles of Gora and his positive atheism, Atheist Centre, India brought revolutionary changes in Indian social work. Born for the cause of social change with Gandhian principles, Atheist Centre, India initiated and continuing the social work for social change. Atheist Centre started its work from removal of untouchability in 1940s, Atheist Centre organised the cosmopolitan dinners and inter-caste marriages to annihilate the caste system, exposing superstitious beliefs and promoting scientific temper and humanism. Mahatma Gandhi was impressed with Atheist Centre's work and interested\nand met atheist Gora. Atheist Centre still continuing the legacy of Gora and Saraswathi Gora in carrying the comprehensive programmes of social as well as individual development for the transformation of society. Atheist Centre \ninitiated social organisation Arthik Samata Mandal, Vasavya Mahila Mandali and Samskar. The first two were founded by Gora and Samskar was started in post-Gora period. Arthik Samata Mandali is involved in integrated rural development and specific programmes for children. It continues as one of the leading organisations in the field of disaster management. The organisation has specific emphasis on weavers and tribal population. The eye camps and polio camps run by the organisation have been popular. Vasavya Mahila Mandali is involved in women's programmes like promotion of women groups for self-help, health programmes, working women's hostel, a shelter home for women with problems, an eye bank and AIDS control. The organisation is highly reputed for its women's counselling programmes in Andhra Pradesh.\nSamskar (Reform) is an initiative of Atheist Centre’s work among the criminal tribes who are now known as denotified tribes.Lavanam, Hemalata Lavanam and others worked in the Criminal settlements of Andhra Pradesh and initiated\nsteps for criminal reformation. Samskar also took up the programme for the eradication of the Jogini system, which is a remnant of the Devadasi in Telangana region of AP. It is a heinous practice thrust on the poor untouchable women in the remote villages of the erstwhile Hyderabad State. Chelli Nilayam, Sister's Home, was founded at Varni, in Nizamabad District in 1987 for the Jogins. All three organisations have their headquarters at Atheist Centre, Vijayawada.\n\nOn September 26, 2011, the Atheist Centre announced that it would open a university and research centre founded on the principles of Gora that would serve as India's first atheist university. It will not be affiliated with any conventional institution. Atheist Centre has a library, with a good collection of books by Gora and his articles on atheism, still primary sources research on atheism, humanism and Gandhian philosophy.\n\n\"The Atheist\", an (English-language monthly magazine, was established in 1969. It carries articles on atheism, humanism, spirit of inquiry, and scientific reform through secular social work.\n\n\"Nasthika Margam\", a Telugu-language monthly, has been published since 1977. It deals with atheism, rationalism, and women's issues.\n\n"}
{"id": "8087746", "url": "https://en.wikipedia.org/wiki?curid=8087746", "title": "Attribution (psychology)", "text": "Attribution (psychology)\n\nHumans are motivated to assign causes to their actions and behaviors. In social psychology, attribution is the process by which individuals explain the causes of behavior and events. Models to explain this process are called attribution theory. Psychological research into attribution began with the work of Fritz Heider in the early 20th century, and the theory was further advanced by Harold Kelley and Bernard Weiner.\n\nGestalt psychologist Fritz Heider is often described as the early-20th-century \"father of attribution theory\".\n\nIn his 1920s dissertation, Heider addressed the problem of phenomenology: why do perceivers attribute the properties such as color to perceived objects, when those properties are mental constructs? Heider's answer that perceivers attribute that which they \"directly\" sense – vibrations in the air for instance – to an object they construe as causing those sense data. \"Perceivers faced with sensory data thus see the perceptual object as 'out there', because they attribute the sensory data to their underlying causes in the world.\"\n\nHeider extended this idea to attributions about people: \"motives, intentions, sentiments ... the core processes which manifest themselves in overt behavior\".\n\nExternal attribution, also called situational attribution, refers to interpreting someone's behavior as being caused by the situation that the individual is in. For example, if Jacob's car tire is punctured he may attribute that to a hole in the road; by making attributions to the poor condition of the highway, he can make sense of the event without any discomfort that it may in reality have been the result of his bad driving.\n\nThe process of assigning the cause of behavior to some internal characteristic, rather than to outside forces.\nThis concept has overlap with the Locus of control, in which individuals feel they are personally responsible for everything that happens to them.\n\nFrom the book \"The Psychology of Interpersonal Relations\" (1958), Fritz Heider tried to explore the nature of interpersonal relationship, and espoused the concept of what he called \"common sense\" or \"naïve psychology\". In his theory, he believed that people observe, analyze, and explain behaviors with explanations. Although people have different kinds of explanations for the events of human behaviors, Heider found it is very useful to group explanation into two categories; Internal (personal) and external (situational) attributions. When an internal attribution is made, the cause of the given behavior is assigned to the individual's characteristics such as ability, personality, mood, efforts, attitudes, or disposition. When an external attribution is made, the cause of the given behavior is assigned to the situation in which the behavior was seen such as the task, other people, or luck (that the individual producing the behavior did so because of the surrounding environment or the social situation). These two types lead to very different perceptions of the individual engaging in a behavior.\n\nCorrespondent inferences state that people make inferences about a person when their actions are freely chosen, are unexpected, and result in a small number of desirable effects. According to Edward E. Jones and Keith Davis' correspondent inference theory, people make correspondent inferences by reviewing the context of behavior. It describes how people try to find out individual's personal characteristics from the behavioral evidence. People make inferences on the basis of three factors; degree of choice, expectedness of behavior, and effects of someone's behaviors. For example, we believe we can make stronger assumptions about a man who gives half of his money to charity, than we can about one who gives $5 to charity. An average person would not want to donate as much as the first man because they would lose a lot of money. By donating half of his money, it is easier for someone to figure out what the first man's personality is like. The second factor, that affects correspondence of action and inferred characteristic, is the number of differences between the choices made and the previous alternatives. If there aren't many differences, the assumption made will match the action because it is easy to guess the important aspect between each choice.\n\nThe covariation model states that people attribute behavior to the factors that are present when a behavior occurs and absent when it does not. Thus, the theory assumes that people make causal attributions in a rational, logical fashion, and that they assign the cause of an action to the factor that co-varies most closely with that action. Harold Kelley's covariation model of attribution looks to three main types of information from which to make an attribution decision about an individual's behavior. The first is \"consensus information\", or information on how other people in the same situation and with the same stimulus behave. The second is \"distinctive information\", or how the individual responds to different stimuli. The third is \"consistency information\", or how frequent the individual's behavior can be observed with similar stimulus but varied situations. From these three sources of information observers make attribution decisions on the individual's behavior as either internal or external. There have been claims that people under-utilise consensus information, although there has been some dispute over this.\n\nThere are several levels in the covariation model: high and low. Each of these levels influences the three covariation model criteria. High consensus is when many people can agree on an event or area of interest. Low consensus is when very few people can agree. High distinctiveness is when the event or area of interest is very unusual, whereas low distinctness is when the event or area of interest is fairly common. High consistency is when the event or area of interest continues for a length of time and low consistency is when the event or area of interest goes away quickly.\n\nBernard Weiner proposed that individuals have initial affective responses to the potential consequences of the intrinsic or extrinsic motives of the actor, which in turn influence future behavior. That is, a person's own perceptions or attributions as to why they succeeded or failed at an activity determine the amount of effort the person will engage in activities in the future. Weiner suggests that individuals exert their attribution search and cognitively evaluate casual properties on the behaviors they experience. When attributions lead to positive affect and high expectancy of future success, such attributions should result in greater willingness to approach to similar achievement tasks in the future than those attributions that produce negative affect and low expectancy of future success. Eventually, such affective and cognitive assessment influences future behavior when individuals encounter similar situations.\n\nWeiner's achievement attribution has three categories:\n\nStability influences individuals' expectancy about their future; control is related with individuals' persistence on mission; causality influences emotional responses to the outcome of task.\n\nWhile people strive to find reasons for behaviors, they fall into many traps of biases and errors. As Fritz Heider says, \"our perceptions of causality are often distorted by our needs and certain cognitive biases\". The following are examples of attributional biases.\n\nThe fundamental attribution error describes the habit to misunderstand dispositional or personality-based explanations for behavior, rather than considering external factors. The fundamental attribution error is most visible when people explain and assume the behavior of others. For example, if a person is overweight, a person's first assumption might be that they have a problem with overeating or are lazy and not that they might have a medical reason for being heavier set. When evaluating others' behaviors, the situational context is often ignored in favor of the disposition of the actor to be the cause of an observed behavior. This is because when a behavior occurs attention is most often focused on the person performing the behavior. Thus, the individual is more salient than the environment and dispositional attributions are made more often than situational attributions to explain the behavior of others. However, when evaluating one's own behavior, the situational factors are often exaggerated when there is a negative outcome while dispositional factors are exaggerated when there is a positive outcome.\n\nThe core process assumptions of attitude construction models are mainstays of social cognition research and are not controversial—as long as we talk about \"judgment\". Once the particular judgment made can be thought of as a person's \"attitude\", however, construal assumptions elicit discomfort, presumably because they dispense with the intuitively appealing attitude concept.\n\nCulture bias is when someone makes an assumption about the behavior of a person based on their cultural practices and beliefs. People in individualist cultures, generally Anglo-America and Anglo-Saxon European societies, value individuals, personal goals, and independence. People in collectivist cultures see individuals as members of groups such as families, tribes, work units, and nations, and tend to value conformity and interdependence. In other words, working together and being involved as a group is more common in certain cultures that views each person as a part of the community. This cultural trait is common in Asia, traditional Native American societies, and Africa. Research shows that culture, either individualist or collectivist, affects how people make attributions.\n\nPeople from individualist cultures are more inclined to make fundamental-attribution error than people from collectivist cultures. Individualist cultures tend to attribute a person's behavior due to their internal factors whereas collectivist cultures tend to attribute a person's behavior to his external factors.\n\nResearch suggests that individualist cultures engage in self-serving bias more than do collectivist cultures, i.e. individualist cultures tend to attribute success to internal factors and to attribute failure to external factors. In contrast, collectivist cultures engage in the opposite of self-serving bias i.e. self-effacing bias, which is: attributing success to external factors and blaming failure on internal factors (the individual).\n\nPeople tend to attribute other people's behaviors to their dispositional factors while attributing own actions to situational factors. In the same situation, people's attribution can differ depending on their role as actor or observer. For example, when a person scores a low grade on a test, they find situational factors to justify the negative event such as saying that the teacher asked a question that he/she never went over in class. However, if another person scores poorly on a test, the person will attribute the results to internal factors such as laziness and inattentiveness in classes. The theory of the actor-observer bias was first developed by E. Jones and R. Nisbett in 1971, whose explanation for the effect was that when we observe other people, we tend to focus on the person, whereas when we are actors, our attention is focused towards situational factors. The actor/observer bias is used less frequently with people one knows well such as friends and family since one knows how his/her close friends and family will behave in certain situation, leading him/her to think more about the external factors rather than internal factors.\n\nDispositional attribution is a tendency to attribute people's behaviors to their dispositions; that is, to their personality, character, and ability.\nFor example, when a normally pleasant waiter is being rude to his/her customer, the customer may assume he/she has a bad temper. The customer, just by looking at the attitude that the waiter is giving him/her, instantly decides that the waiter is a bad person. The customer oversimplifies the situation by not taking into account all the unfortunate events that might have happened to the waiter which made him/her become rude at that moment. Therefore, the customer made dispositional attribution by attributing the waiter's behavior directly to his/her personality rather than considering situational factors that might have caused the whole \"rudeness\".\n\nSelf-serving bias is attributing dispositional and internal factors for success, while external and uncontrollable factors are used to explain the reason for failure. For example, if a person gets promoted, it is because of his/her ability and competence whereas if he/she does not get promoted, it is because his/her manager does not like him/her (external, uncontrollable factor). Originally, researchers assumed that self-serving bias is strongly related to the fact that people want to protect their self-esteem. However, an alternative information processing explanation is that when the outcomes match people's expectations, they make attributions to internal factors. For example, if you pass a test you believe it was because of your intelligence; when the outcome does not match their expectations, they make external attributions or excuses. Whereas if you fail a test, you would give an excuse saying that you did not have enough time to study. People also use defensive attribution to avoid feelings of vulnerability and to differentiate themselves from a victim of a tragic accident. An alternative version of the theory of self-serving bias states that the bias does not arise because people wish to protect their private self-esteem, but to protect their self-image (a self-presentational bias). This version of the theory would predict that people attribute their successes to situational factors, for fear that others will disapprove of them looking overly vain if they should attribute successes to themselves.\n\nFor example, it is suggested that coming to believe that \"good things happen to good people and bad things happen to bad people\" will reduce feelings of vulnerability . This belief would have side-effects of blaming the victim even in tragic situations. When a mudslide destroys several houses in a rural neighborhood, a person living in a more urban setting might blame the victims for choosing to live in a certain area or not building a safer, stronger house. Another example of attributional bias is optimism bias in which most people believe positive events happen to them more often than to others and that negative events happen to them less often than to others. For example, smokers on average believe they are less likely to get lung cancer than other smokers.\n\nThe defensive attribution hypothesis is a social psychological term referring to a set of beliefs held by an individual with the function of defending themselves from concern that they will be the cause or victim of a mishap. Commonly, defensive attributions are made when individuals witness or learn of a mishap happening to another person. In these situations, attributions of responsibility to the victim or harm-doer for the mishap will depend upon the severity of the outcomes of the mishap and the level of personal and situational similarity between the individual and victim. More responsibility will be attributed to the harm-doer as the outcome becomes more severe, and as personal or situational similarity decreases.\n\nAn example of defensive attribution is the just-world hypothesis, which is where \"good things happen to good people and bad things happen to bad people\". People believe in this in order to avoid feeling vulnerable to situations that they have no control over. However, this also leads to blaming the victim even in a tragic situation. When people hear someone died from a car accident, they decide that the driver was drunk at the time of the accident, and so they reassure themselves that an accident will never happen to them. Despite the fact there was no other information provided, people will automatically attribute that the accident was the driver's fault due to an internal factor (in this case, deciding to drive while drunk), and thus they would not allow it to happen to themselves.\n\nAnother example of defensive attribution is optimism bias, in which people believe positive events happen to them more often than to others and that negative events happen to them less often than to others. Too much optimism leads people to ignore some warnings and precautions given to them. For example, smokers believe that they are less likely to get lung cancer than other smokers.\n\nAttribution theory can be applied to juror decision making. Jurors use attributions to explain the cause of the defendant's intent and actions related to the criminal behavior. The attribution made (situational or dispositional) might affect a juror's punitiveness towards the defendant. When jurors attribute a defendant's behavior to dispositional attributions they tend to be more punitive and are more likely find a defendant guilty and to recommend a death sentence compared to a life sentence.\n\nAttribution theory has had a big application in clinical psychology. Abramson, Seligman, and Teasdale developed a theory of the depressive attributional style, claiming that individuals who tend to attribute their failures to internal, stable and global factors are more vulnerable to clinical depression. The Attributional Style Questionnaire (ASQ) has been developed to assess whether individuals have the depressogenic attributional style. However, the ASQ has been criticized, with some researchers preferring to use a technique called Content Analysis of Verbatim Explanation (CAVE) in which an individual's ordinary writings are analysed to assess whether s/he is vulnerable to the depressive attributional style.\n\nThe concept of learned helplessness emerged from animal research in which psychologists Martin Seligman and Steven F. Maier discovered that dogs classically conditioned to an electrical shock which they could not escape, subsequently failed to attempt to escape an avoidable shock in a similar situation. They argued that learned helplessness applied to human psychopathology. In particular, individuals who attribute negative outcomes to internal, stable and global factors reflect a view in which they have no control over their situation. It is suggested that this aspect of not attempting to better a situation exacerbates negative mood, and may lead to clinical depression and related mental illnesses.\n\nWhen people try to make attributions about another's behavior, their information focuses on the individual. Their perception of that individual is lacking most of the external factors which might affect the individual. The gaps tend to be skipped over and the attribution is made based on the perception information most salient. The most salient perceptual information dominates a person's perception of the situation.\n\nFor individuals making behavioral attributions about themselves, the situation and external environment are entirely salient, but their own body and behavior are less so. This leads to the tendency to make an external attribution in regard to their own behavior.\n\nAttribution theory has been criticised as being mechanistic and reductionist for assuming that people are rational, logical, and systematic thinkers. The fundamental attribution error, however, demonstrates that they are cognitive misers and motivated tactician. It also fails to address the social, cultural, and historical factors that shape attributions of cause. This has been addressed extensively by discourse analysis, a branch of psychology that prefers to use qualitative methods including the use of language to understand psychological phenomena. The linguistic categorization theory for example demonstrates how language influences our attribution style.\n\n"}
{"id": "8532185", "url": "https://en.wikipedia.org/wiki?curid=8532185", "title": "Automath", "text": "Automath\n\nAutomath (\"automating mathematics\") was a formal language, devised by Nicolaas Govert de Bruijn starting in 1967, for expressing complete mathematical theories in such a way that an included automated proof checker can verify their correctness.\n\nThe Automath system included many novel notions that were later adopted and/or reinvented in areas such as typed lambda calculus and explicit substitution. Dependent types is one outstanding example. Automath was also the first practical system that exploited the Curry–Howard correspondence. Propositions were represented as sets (called \"categories\") of their proofs, and the question of provability became a question of non-emptiness (type inhabitation); de Bruijn was unaware of Howard's work, and stated the correspondence independently. \n\nL. S. van Benthem Jutting, as part of this Ph.D. thesis in 1976, translated Edmund Landau's \"Foundations of Analysis\" into Automath and checked its correctness.\nAutomath was never widely publicized at the time, however, and so never achieved widespread use; nonetheless, it proved very influential in the later development of logical frameworks and proof assistants. The Mizar system, a system of writing and checking formalized mathematics that is still in active use, was influenced by Automath.\n\n\n"}
{"id": "37122597", "url": "https://en.wikipedia.org/wiki?curid=37122597", "title": "Behavioral epigenetics", "text": "Behavioral epigenetics\n\nBehavioral epigenetics is the field of study examining the role of epigenetics in shaping animal (including human) behaviour. It is an experimental science that seeks to explain how nurture shapes nature, where nature refers to biological heredity and nurture refers to virtually everything that occurs during the life-span (e.g., social-experience, diet and nutrition, and exposure to toxins). Behavioral epigenetics attempts to provide a framework for understanding how the expression of genes is influenced by experiences and the environment to produce individual differences in behaviour, cognition, personality, and mental health.\n\nEpigenetic gene regulation involves changes other than to the sequence of DNA and includes changes to histones (proteins around which DNA is wrapped) and DNA methylation. These epigenetic changes can influence the growth of neurons in the developing brain as well as modify activity of the neurons in the adult brain. Together, these epigenetic changes on neuron structure and function can have a marked influence on an organism's behavior.\n\nIn biology, and specifically genetics, epigenetics is the study of heritable changes in gene activity which are \"not\" caused by changes in the DNA sequence; the term can also be used to describe the study of stable, long-term alterations in the transcriptional potential of a cell that are not necessarily heritable.\n\nExamples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. \nDNA methylation turns a gene \"off\" – it results in the inability of genetic information to be read from DNA; removing the methyl tag can turn the gene back \"on\".\n\nEpigenetics has a strong influence on the development of an organism and can alter the expression of individual traits. Epigenetic changes occur not only in the developing fetus, but also in individuals throughout the human life-span. Because some epigenetic modifications can be passed from one generation to the next, subsequent generations may be affected by the epigenetic changes that took place in the parents.\n\nThe first documented example of epigenetics affecting behavior was provided by Michael Meaney and Moshe Szyf. While working at McGill University in Montréal in 2004, they discovered that the type and amount of nurturing a mother rat provides in the early weeks of the rat's infancy determines how that rat responds to stress later in life. This stress sensitivity was linked to a down-regulation in the expression of the glucocorticoid receptor in the brain. In turn, this down-regulation was found to be a consequence of the extent of methylation in the promoter region of the glucocorticoid receptor gene. Immediately after birth, Meaney and Szyf found that methyl groups repress the glucocorticoid receptor gene in all rat pups, making the gene unable to unwind from the histone in order to be transcribed, causing a decreased stress response. Nurturing behaviours from the mother rat were found to stimulate activation of stress signalling pathways that remove methyl groups from DNA. This releases the tightly wound gene, exposing it for transcription. The glucocorticoid gene is activated, resulting in lowered stress response. Rat pups that receive a less nurturing upbringing are more sensitive to stress throughout their life-span.\n\nThis pioneering work in rodents has been difficult to replicate in humans because of a general lack of availability human brain tissue for measurement of epigenetic changes.\n\nIn a small clinical study in humans published in 2008, epigenetic differences were linked to differences in risk-taking and reactions to stress in monozygotic twins. The study identified twins with different life paths, wherein one twin displayed risk-taking behaviours, and the other displayed risk-averse behaviours. Epigenetic differences in DNA methylation of the CpG islands proximal to the DLX1 gene correlated with the differing behavior. The authors of the twin study noted that despite the associations between epigenetic markers and differences personality traits, epigenetics cannot predict complex decision-making processes like career selection.\n\nAnimal and human studies have found correlations between poor care during infancy and epigenetic changes that correlate with long-term impairments that result from neglect.\n\nStudies in rats have shown correlations between maternal care in terms of the parental licking of offspring and epigenetic changes. A high level of licking results in a long-term reduction in stress response as measured behaviorally and biochemically in elements of the hypothalamic-pituitary-adrenal axis (HPA). Further, decreased DNA methylation of the glucocorticoid receptor gene were found in offspring that experienced a high level of licking; the glucorticoid receptor plays a key role in regulating the HPA. The opposite is found in offspring that experienced low levels of licking, and when pups are switched, the epigenetic changes are reversed. This research provides evidence for an underlying epigenetic mechanism. Further support comes from experiments with the same setup, using drugs that can increase or decrease methylation. Finally, epigenetic variations in parental care can be passed down from one generation to the next, from mother to female offspring. Female offspring who received increased parental care (i.e., high licking) became mothers who engaged in high licking and offspring who received less licking became mothers who engaged in less licking.\n\nIn humans, a small clinical research study showed the relationship between prenatal exposure to maternal mood and genetic expression resulting in increased reactivity to stress in offspring. Three groups of infants were examined: those born to mothers medicated for depression with serotonin reuptake inhibitors; those born to depressed mothers not being treated for depression; and those born to non-depressed mothers. Prenatal exposure to depressed/anxious mood was associated with increased DNA methylation at the glucocorticoid receptor gene and to increased HPA axis stress reactivity. The findings were independent of whether the mothers were being pharmaceutically treated for depression.\n\nRecent research has also shown the relationship of methylation of the maternal glucocorticoid receptor and maternal neural activity in response to mother-infant interactions on video. Longitudinal follow-up of those infants will be important to understand the impact of early caregiving in this high-risk population on child epigenetics and behavior.\n\nA 2010 review discusses the role of DNA methylation in memory formation and storage, but the precise mechanisms involving neuronal function, memory, and methylation reversal remain unclear.\n\nStudies in rodents have found that the environment exerts an influence on epigenetic changes related to cognition, in terms of learning and memory; environmental enrichment correlated with increased histone acetylation, and verification by administering histone deacetylase inhibitors induced sprouting of dendrites, an increased number of synapses, and reinstated learning behaviour and access to long-term memories. Research has also linked learning and long-term memory formation to reversible epigenetic changes in the hippocampus and cortex in animals with normal-functioning, non-damaged brains. In human studies, post-mortem brains from Alzheimer's patients show increased histone de-acetylase levels.\n\nEnvironmental and epigenetic influences seem to work together to increase the risk of addiction. For example, environmental stress has been shown to increase the risk of substance abuse. In an attempt to cope with stress, alcohol and drugs can be used as an escape. Once substance abuse commences, however, epigenetic alterations may further exacerbate the biological and behavioural changes associated with addiction.\n\nEven short-term substance abuse can produce long-lasting epigenetic changes in the brain of rodents, via DNA methylation and histone modification. Epigenetic modifications have been observed in studies on rodents involving ethanol, nicotine, cocaine, amphetamine, methamphetamine and opiates. Specifically, these epigenetic changes modify gene expression, which in turn increases the vulnerability of an individual to engage in repeated substance overdose in the future. In turn, increased substance abuse results in even greater epigenetic changes in various components of a rodent's reward system (e.g., in the nucleus accumbens). Hence, a cycle emerges whereby changes in the pleasure-reward areas contribute to the long-lasting neural and behavioural changes associated with the increased likelihood of addiction, the maintenance of addiction and relapse. In humans, alcohol consumption has been shown to produce epigenetic changes that contribute to the increased craving of alcohol. As such, epigenetic modifications may play a part in the progression from the controlled intake to the loss of control of alcohol consumption. These alterations may be long-term, as is evidenced in smokers who still possess nicotine-related epigenetic changes ten years after cessation. Therefore, epigenetic modifications may account for some of the behavioural changes generally associated with addiction. These include: repetitive habits that increase the risk of disease, and personal and social problems; need for immediate gratification; high rates of relapse following treatment; and, the feeling of loss of control.\n\nEvidence for related epigenetic changes has come from human studies involving alcohol, nicotine, and opiate abuse. Evidence for epigenetic changes stemming from amphetamine and cocaine abuse derives from animal studies. In animals, drug-related epigenetic changes in fathers have also been shown to negatively affect offspring in terms of poorer spatial working memory, decreased attention and decreased cerebral volume.\n\nEpigenetic changes may help to facilitate the development and maintenance of eating disorders via influences in the early environment and throughout the life-span. Pre-natal epigenetic changes due to maternal stress, behaviour and diet may later predispose offspring to persistent, increased anxiety and anxiety disorders. These anxiety issues can precipitate the onset of eating disorders and obesity, and persist even after recovery from the eating disorders.\n\nEpigenetic differences accumulating over the life-span may account for the incongruent differences in eating disorders observed in monozygotic twins. At puberty, sex hormones may exert epigenetic changes (via DNA methylation) on gene expression, thus accounting for higher rates of eating disorders in men as compared to women. Overall, epigenetics contribute to persistent, unregulated self-control behaviours related to the urge to binge.\n\nEpigenetic changes including hypomethylation of glutamatergic genes (i.e., NMDA-receptor-subunit gene NR3B and the promoter of the AMPA-receptor-subunit gene GRIA2) in the post-mortem human brains of schizophrenics are associated with increased levels of the neurotransmitter glutamate. Since glutamate is the most prevalent, fast, excitatory neurotransmitter, increased levels may result in the psychotic episodes related to schizophrenia. Epigenetic changes affecting a greater number of genes have been detected in men with schizophrenia as compared to women with the illness.\n\nPopulation studies have established a strong association linking schizophrenia in children born to older fathers. Specifically, children born to fathers over the age of 35 years are up to three times more likely to develop schizophrenia. Epigenetic dysfunction in human male sperm cells, affecting numerous genes, have been shown to increase with age. This provides a possible explanation for increased rates of the disease in men. To this end, toxins (e.g., air pollutants) have been shown to increase epigenetic differentiation. Animals exposed to ambient air from steel mills and highways show drastic epigenetic changes that persist after removal from the exposure. Therefore, similar epigenetic changes in older human fathers are likely. Schizophrenia studies provide evidence that the nature versus nurture debate in the field of psychopathology should be re-evaluated to accommodate the concept that genes and the environment work in tandem. As such, many other environmental factors (e.g., nutritional deficiencies and cannabis use) have been proposed to increase the susceptibility of psychotic disorders like schizophrenia via epigenetics.\n\nEvidence for epigenetic modifications for bipolar disorder is unclear. One study found hypomethylation of a gene promoter of a prefrontal lobe enzyme (i.e., membrane-bound catechol-O-methyl transferase, or COMT) in post-mortem brain samples from individuals with bipolar disorder. COMT is an enzyme that metabolizes dopamine in the synapse. These findings suggest that the hypomethylation of the promoter results in over-expression of the enzyme. In turn, this results in increased degradation of dopamine levels in the brain. These findings provide evidence that epigenetic modification in the prefrontal lobe is a risk factor for bipolar disorder. However, a second study found no epigenetic differences in post-mortem brains from bipolar individuals.\n\nThe causes of major depressive disorder (MDD) are poorly understood from a neuroscience perspective. The epigenetic changes leading to changes in glucocorticoid receptor expression and its effect on the HPA stress system discussed above, have also been applied to attempts to understand MDD.\n\nMuch of the work in animal models has focused on the indirect downregulation of brain derived neurotrophic factor (BDNF) by over-activation of the stress axis. Studies in various rodent models of depression, often involving induction of stress, have found direct epigenetic modulation of BDNF as well.\n\nEpigenetics may be relevant to aspects of psychopathic behaviour through methylation and histone modification. These processes are heritable but can also be influenced by environmental factors such as smoking and abuse. Epigenetics may be one of the mechanisms through which the environment can impact the expression of the genome. Studies have also linked methylation of genes associated with nicotine and alcohol dependence in women, ADHD, and drug abuse. It is probable that epigenetic regulation as well as methylation profiling will play an increasingly important role in the study of the play between the environment and genetics of psychopaths.\n\nA study of the brains of 24 suicide completers, 12 of whom had a history of child abuse and 12 who did not, found decreased levels of glucocorticoid receptor in victims of child abuse and associated epigenetic changes.\n\nSeveral studies have indicated DNA cytosine methylation linked to the social behavior of insects, such as honeybees and ants. In honeybees, when nurse bee switched from her in-hive tasks to out foraging, cytosine methylation marks are changing. When a forager bee was reversed to do nurse duties, the cytosine methylation marks were also reversed. Knocking down the DNMT3 in the larvae changed the worker to queen-like phenotype. Queen and worker are two distinguish castes with different morphology, behavior, and physiology. Studies in DNMT3 silencing also indicated DNA methylation may regulate gene alternative splicing and pre-mRNA maturation.\n\nMany researchers contribute information to the Human Epigenome Consortium. The aim of future research is to reprogram epigenetic changes to help with addiction, mental illness, age related changes, memory decline, and other issues. However, the sheer volume of consortium-based data makes analysis difficult. Most studies also focus on one gene. In actuality, many genes and interactions between them likely contribute to individual differences in personality, behaviour and health. As social scientists often work with many variables, determining the number of affected genes also poses methodological challenges. More collaboration between medical researchers, geneticists and social scientists has been advocated to increase knowledge in this field of study.\nLimited access to human brain tissue poses a challenge to conducting human research. Not yet knowing if epigenetic changes in the blood and (non-brain) tissues parallel modifications in the brain, places even greater reliance on brain research. Although some epigenetic studies have translated findings from animals to humans, some researchers caution about the extrapolation of animal studies to humans. One view notes that when animal studies do not consider how the subcellular and cellular components, organs and the entire individual interact with the influences of the environment, results are too reductive to explain behaviour.\n\nSome researchers note that epigenetic perspectives will likely be incorporated into pharmacological treatments. Others caution that more research is necessary as drugs are known to modify the activity of multiple genes and may, therefore, cause serious side effects. However, the ultimate goal is to find patterns of epigenetic changes that can be targeted to treat mental illness, and reverse the effects of childhood stressors, for example. If such treatable patterns eventually become well-established, the inability to access brains in living humans to identify them poses an obstacle to pharmacological treatment. Future research may also focus on epigenetic changes that mediate the impact of psychotherapy on personality and behaviour.\n\nMost epigenetic research is correlational; it merely establishes associations. More experimental research is necessary to help establish causation. Lack of resources has also limited the number of intergenerational studies. Therefore, advancing longitudinal and multigenerational, experience-dependent studies will be critical to further understanding the role of epigenetics in psychology.\n\n\n\n"}
{"id": "37476", "url": "https://en.wikipedia.org/wiki?curid=37476", "title": "Civil liberties", "text": "Civil liberties\n\nCivil liberties or personal freedoms are personal guarantees and freedoms that the government cannot abridge, either by law or by judicial interpretation, without due process. Though the scope of the term differs between countries, civil liberties may include the freedom of conscience, freedom of press, freedom of religion, freedom of expression, freedom of assembly, the right to security and liberty, freedom of speech, the right to privacy, the right to equal treatment under the law and due process, the right to a fair trial, and the right to life. Other civil liberties include the right to own property, the right to defend oneself, and the right to bodily integrity. Within the distinctions between civil liberties and other types of liberty, distinctions exist between positive liberty/positive rights and negative liberty/negative rights.\n\nMany contemporary states have a constitution, a bill of rights, or similar constitutional documents that enumerate and seek to guarantee civil liberties. Other states have enacted similar laws through a variety of legal means, including signing and ratifying or otherwise giving effect to key conventions such as the European Convention on Human Rights and the International Covenant on Civil and Political Rights. The existence of some claimed civil liberties is a matter of dispute, as are the extent of most civil rights. Controversial examples include property rights, reproductive rights, and civil marriage. The degree that democracies have involved themselves in needs to take into fact the influence of terrorism. Whether the existence of victimless crimes infringes upon civil liberties is a matter of dispute. Another matter of debate is the suspension or alteration of certain civil liberties in times of war or state of emergency, including whether and to what extent this should occur.\n\nThe formal concept of civil liberties is often dated back to Magna Carta, an English legal charter agreed in 1215 which in turn was based on pre-existing documents, namely the Charter of Liberties.\n\nThe Constitution of People's Republic of China (which applies only to mainland China, not to Hong Kong, Macau and Taiwan), especially its Fundamental Rights and Duties of Citizens, claims to protect many civil liberties.\nTaiwan, which is separated from China, has its own Constitution.\n\nThe Fundamental Rights—embodied in Part III of the constitution—guarantee liberties such that all Indians can lead their lives in peace as citizens of India. The six fundamental rights are right to equality, right to freedom, right against exploitation, right to freedom of religion, cultural and educational rights and right to constitutional remedies.\n\nThese include individual rights common to most liberal democracies, incorporated in the fundamental law of the land and are enforceable in a court of law. Violations of these rights result in punishments as prescribed in the Indian Penal Code, subject to discretion of the judiciary. These rights are neither absolute nor immune from constitutional amendments. They have been aimed at overturning the inequalities of pre-independence social practices. Specifically, they resulted in abolishment of un-touchability and prohibit discrimination on the grounds of religion, race, caste, sex, or place of birth. They forbid human trafficking and unfree labour. They protect cultural and educational rights of ethnic and religious minorities by allowing them to preserve their languages and administer their own educational institutions.\n\nAll people, irrespective of race, religion, caste or sex, have the right to approach the High Courts or the Supreme Court for the enforcement of their fundamental rights. It is not necessary that the aggrieved party has to be the one to do so. In public interest, anyone can initiate litigation in the court on their behalf. This is known as \"Public interest litigation\". High Court and Supreme Court judges can also act on their own on the basis of media reports.\n\nThe Fundamental Rights emphasize equality by guaranteeing to all citizens the access and use of public institutions and protections, irrespective of their background. The rights to life and personal liberty apply for persons of any nationality, while others, such as the freedom of speech and expression are applicable only to the citizens of India (including non-resident Indian citizens). The right to equality in matters of public employment cannot be conferred to overseas citizens of India.\n\nFundamental Rights primarily protect individuals from any arbitrary State actions, but some rights are enforceable against private individuals too. For instance, the constitution abolishes untouchability and prohibits \"begar\". These provisions act as a check both on State action and actions of private individuals. Fundamental Rights are not absolute and are subject to reasonable restrictions as necessary for the protection of national interest. In the \"Kesavananda Bharati vs. state of Kerala\" case, the Supreme Court ruled that all provisions of the constitution, including Fundamental Rights can be amended. However, the Parliament cannot alter the basic structure of the constitution like secularism, democracy, federalism, separation of powers. Often called the \"Basic structure doctrine\", this decision is widely regarded as an important part of Indian history. In the 1978 \"Maneka Gandhi v. Union of India\" case, the Supreme Court extended the doctrine's importance as superior to any parliamentary legislation.According to the verdict, no act of parliament can be considered a law if it violated the basic structure of the constitution. This landmark guarantee of Fundamental Rights was regarded as a unique example of judicial independence in preserving the sanctity of Fundamental Rights.\nThe Fundamental Rights can only be altered by a constitutional amendment, hence their inclusion is a check not only on the executive branch, but also on the Parliament and state legislatures. The imposition of a state of emergency may lead to a temporary suspension of the rights conferred by Article 19 (including freedoms of speech, assembly and movement, etc.) to preserve national security and public order. The President can, by order, suspend the constitutional written remedies as well.\n\nSince 1947, Japan, a country with a constitutional monarchy and known for its socially “conservative society where change is gradual,” has a constitution with a seemingly strong bill of rights at its core (). In many ways, it resembles the U.S. Constitution prior to the Civil Rights Act of 1964, and that is because it came into life during the Allied occupation of Japan. This constitution may have felt like a foreign imposition to the governing elites, but not to the ordinary people \"who lacked faith in their discredited leaders and supported meaningful change.\" In the abstract, the constitution strives to secure fundamental individual liberties and rights, which are covered pointedly in articles 10 to 40. Most salient of the human dignity articles is article 25, section 1, which guarantees that all \n\nDespite, the adoption of this liberal constitution, often referred as the \"Postwar Constitution\" (戦後憲法, Sengo-Kenpō) or the \"Peace Constitution\" (平和憲法, Heiwa-Kenpō), the Japanese governing elites have struggled to usher in an inclusive, open and Pluralist society. Even after the end of World War II and the departure of the Allied government of occupation in 1952, Japan has been the target of international criticism for failing to admit to war crimes, institutional religious discrimination and maintaining a weak freedom of the press, the treatment of children, minorities, foreigners, and women, its punitive criminal justice system, and more recently, the systematic bias against LGBT people.\nThe first Japanese attempt to a bill of rights was in the 19th century Meiji constitution (1890), which took both the Prussian (1850) and British constitutions as basic models. However, it had but a meager influence in the practice of the rule of law as well as in people’s daily living. So, the short and deliberately gradual history of struggles for personal rights and protection against government/society's impositions has yet to transform Japan into a champion of universal and individual freedom. According to constitutional scholar, Shigenori Matsui,\n\nDespite the divergences between Japan's social culture and the Liberal Constitutionalism that it purports to have adopted, the country has moved toward closing the gap between the notion and the practice of the law. The trend is more evident in the long term. Among several examples, the Diet (bicameral legislature) ratified the International Bill of Human Rights in 1979 and then it passed the Law for Equal Opportunity in Employment for Men and Women in 1985, measures that were heralded as major steps toward a democratic and participatory society. In 2015, moreover, it reached an agreement with Korea to compensate for abuses related to the so-called “women of comfort” that took place during the Japanese occupation of the peninsula. However, human rights group, and families of the survivors condemned the agreement as patronizing and insulting.\n\nOn its official site, the Japanese government has identified various human rights problems. Among these are child abuses (e.g., bullying, corporal punishment, child sexual abuse, child prostitution, and child pornography), frequent neglect and ill-treatment of elderly persons and individuals with disabilities, Dowa claims (discrimination against the Burakumin), Ainu people (indigenous people in Japan), foreign nationals, HIV/AIDS carriers, Hansen's disease patients, persons released from prison after serving their sentence, crime victims, people whose human rights are violated using the Internet, the homeless, individuals with identity disorders, and women. Also, the government lists systematic problems with gender biases and the standard reference to sexual preferences for jobs and other functions in society.\n\nHuman rights organizations, national and foreign, expand the list to include human rights violations that relate to government policies, as in the case of daiyo kangoku system (substitute prison) and the methods of interrogating crime suspects. The effort of these agencies and ordinary people seem to pay off. In 2016, the U.S. Department of State released a report stating that Japan's human right record is showing signs of improvement.\n\nAlthough Australia does not have an enshrined Bill of Rights or similar binding legal document, civil liberties are assumed as protected through a series of rules and conventions. Australia was a key player and signatory to the UN Universal Declaration on Human Rights (1948)\n\nThe Constitution of Australia (1900) does offer very limited protection of rights:\nCertain High Court interpretations of the Constitution have allowed for implied rights such as freedom of speech and the right to vote to be established, however others such as freedom of assembly and freedom of association are yet to be identified.\n\nRefugee issues\n\nWithin the past decade Australia has experienced increasing contention regarding its treatment of those seeking asylum. Although Australia is a signatory to the UN Refugee Convention (1951), successive governments have demonstrated an increasing tightening of borders; particularly against those who seek passage via small water vessels.\n\nThe Abbott Government (2013) like its predecessors (the Gillard and Howard Governments) has encountered particular difficulty curbing asylum seekers via sea, increasingly identified as \"illegal immigration\". The recent involvement of the Australian Navy in refugee rescue operations has many human rights groups such as Amnesty International concerned over the \"militarisation\" of treatment of refugees. The current \"turn-back\" policy is particularly divisive, as it involves placing refugees in government lifeboats and turning them towards Indonesia. Despite opposition however, the Abbott government's response has so far seen a reduction in the amount of potential refugees undertaking the hazardous cross to Australia, which is argued by the government as an indicator for its policy success.\n\nThe European Convention on Human Rights, to which almost all European countries belong (apart from Belarus), enumerates a number of civil liberties and is of varying constitutional force in different European states.\n\nFollowing the Velvet Revolution, a constitutional overhaul took place in Czechoslovakia. In 1991, the Charter of Fundamental Rights and Basic Freedoms was adopted, having the same legal standing as the Constitution. The Czech Republic has kept the Charter in its entirety following the dissolution of Czechoslovakia as Act No. 2/1993 Coll. (Constitution being No. 1).\n\nFrance's 1789 Declaration of the Rights of Man and of the Citizen listed many civil liberties and is of constitutional force.\n\nThe German constitution, the \"Grundgesetz\" (lit. \"Base Law\"), starts with an elaborate listing of civil liberties and states in sec. 1 \"The dignity of man is inviolable. To respect and protect it shall be the duty of all public authority.\" Following the \"Austrian System\", the people have the right to appeal to the Federal Constitutional Court of Germany (\"Bundesverfassungsgericht\") if they feel their civil rights are being violated. This procedure has shaped German law considerably over the years.\n\nCivil liberties in the United Kingdom date back to Magna Carta in 1215 and 17th century common law and statute law, such as the 1628 Petition of Right, the Habeas Corpus Act 1679 and the Bill of Rights 1689. Parts of these laws remain in statute today and are supplemented by other legislation and conventions that collectively form the uncodified Constitution of the United Kingdom. In addition, the United Kingdom is a signatory to the European Convention on Human Rights which covers both human rights and civil liberties. The Human Rights Act 1998 incorporates the great majority of Convention rights directly into UK law.\n\nIn June 2008 the then Shadow Home Secretary David Davis resigned his parliamentary seat over what he described as the \"erosion of civil liberties\" by the then Labour government, and was re-elected on a civil liberties platform (although he was not opposed by candidates of other major parties). This was in reference to anti-terrorism laws and in particular the extension to pre-trial detention, that is perceived by many to be an infringement of \"habeas corpus\" established in Magna Carta.\n\nThe Constitution of the Russian Federation guarantees in theory many of the same rights and civil liberties as the U.S. except to bear arms, i.e.: freedom of speech, freedom of religion, freedom of association and assembly, freedom to choose language, to due process, to a fair trial, privacy, freedom to vote, right for education, etc. However, human rights groups like Amnesty International have warned that Vladimir Putin has seriously curtailed freedom of expression, freedom of assembly and freedom of association amidst growing authoritarianism.\n\nThe Constitution of Canada includes the Canadian Charter of Rights and Freedoms which guarantees many of the same rights as the U.S. constitution, with the notable exceptions of protection against establishment of religion. However, the Charter does protect freedom of religion. The Charter also omits any mention of, or protection for, property.\n\nThe United States Constitution, especially its Bill of Rights, protects civil liberties. The passage of the Fourteenth Amendment further protected civil liberties by introducing the Privileges or Immunities Clause, Due Process Clause, and Equal Protection Clause. Human rights within the United States are often called civil rights, which are those rights, privileges and immunities held by all people, in distinction to \"political\" rights, which are the rights that inhere to those who are entitled to participate in elections, as candidates or voters. Before universal suffrage, this distinction was important, since many people were ineligible to vote but still were considered to have the fundamental freedoms derived from the rights to life, liberty and the pursuit of happiness. This distinction is less important now that Americans enjoy near universal suffrage, and civil liberties are now taken to include the political rights to vote and participate in elections. Because Indian tribal governments retain sovereignty over tribal members, the U.S. Congress in 1968 enacted a law that essentially applies most of the protections of the Bill of Rights to tribal members, to be enforced mainly by tribal courts.\n\nThe Civil Liberties Act of 1988 was signed into effect by President Ronald Reagan on August 10, 1988. The act was passed by Congress to issue a public apology for those of Japanese ancestry who lost their property and liberty due to discriminatory actions by the United States Government during the internment period.\n\nThis act also provided many other benefits within various sectors of the government. Within the treasury it establishes a civil liberties public education fund. It directs the Attorney General to identify and locate each individual affected by this act and to pay them $20,000 from the civil liberties public education fund. It also established a board of directors who is responsible for making disbursements from this fund. Finally, it requires that all documents and records that are created or received by the commission be kept in the United States archives.\n\n"}
{"id": "1633582", "url": "https://en.wikipedia.org/wiki?curid=1633582", "title": "Civil penalty", "text": "Civil penalty\n\nA civil penalty or civil fine is a financial penalty imposed by a government agency as restitution for wrongdoing. The wrongdoing is typically defined by a codification of legislation, regulations, and decrees. The civil fine is not considered to be a criminal punishment, because it is primarily sought in order to compensate the state for harm done to it, rather than to punish the wrongful conduct. As such, a civil penalty, in itself, will not carry jail time or other legal penalties. For example, if a person were to dump toxic waste in a state park, the state would have the same right to seek to recover the cost of cleaning up the mess as would a private landowner, and to bring the complaint to a court of law, if necessary.\n\nCivil penalties occupy a strange place in some legal systems - because they are not criminal penalties, the state need not meet a burden of proof that is \"balance of probabilities\"; but because the action is brought by the government, and some civil penalties can run into the millions of dollars, it would be uncomfortable to subject citizens to them by a burden of proof that is merely a \"preponderance of the evidence.\" Therefore, the assessment of most civil penalties requires a finding of \"clear and convincing evidence\" before a civil defendant will be held liable. A defendant may well raise excuses, justifications, affirmative defenses, and procedural defenses. An administrative law judge or hearing officer may oversee the proceedings and render a judgment. Judgment is made on the balance of probabilities, meaning if it is more than fifty percent likely that the accused is responsible then the accused shall be found guilty.\n\nIn some cases, a civil penalty may be supplemented by other legal process, including administrative sanctions or even criminal charges, and their respective appeals. For example, failure to pay a fine assessed for a traffic code violation may result in administrative suspension of a driver's license, and further driving after suspension may be a criminal offense. On the other hand, a minimal case may be \"put on file\", or otherwise suspended for a period during which the defendant may be required to avoid further violations, or carry out specific duties (such as making repairs or restitution, or attending supplemental education), after which the matter is dismissed.\n\nIn other cases, such as public safety and consumer protection violations, the local authorities may revoke permits and licenses, and seek injunction to stop or remove non-conforming works or goods, in addition to the civil penalty.\n\nPending or admitted civil violations may also be used as evidence of responsibility in a civil suit. One example is speeding causing in a car accident, resulting in a wrongful death claim. However, the plaintiff may be required to prove causation through a harm encompassed in the regulations.\n\nThe concept of civil penalties in English is in a state of flux. In contract, damages is a remedy to provide monetary compensation for loss; and damages may be unliquidated (general damages), or liquidated (pre-determined). In the absence of an out-of court settlement, unliquidated damages must be ascertained by a court or tribunal, whereas liquidated damages will be determined by reference to the contract or to a mutually agreed arbitrator. The purpose of liquidated damages is to provide certainty and to avoid both the bother and cost of legal proceedings. \n\nIt is well established that liquidated damages for breach of contract are void if they seek to punish rather than to compensate for loss. For a contractual \"penalty\" clause to be valid, one must show that it was drawn up after a bona fide attempt to estimate loss in advance of the breach. For example, a motorway construction contract may have an estimated finish date with a \"penalty clause\" for every day late; but provided that this date is realistic and the \"penalty\" is a reasonable approximation of loss, the clause will be valid. The validity of the clause will be advanced if there is an equivalent bonus for finishing early.\n\nDifficulties can arise with fines for wrongful parking (parking in the wrong area, or overstaying). There are three scenarios: (i) parking on public streets; & parking in a private car park either (ii) with permission, or (iii) without permission. If a parking fine is imposed for type (i), since the powers exercised by the local authority have been delegated by Parliament, there is little that one can do, except to seek judicial review and allege disproportionality. If it is type (ii), such as in a supermarket car-park, then contract rules apply. If, say, the cost was £1 for an hour, and you got a £60 ticket for overstaying a further hour, you can legally send them an extra £1, plus (say) £5 as a contribution to their administration expenses. If they are not satisfied, they would have to issue a county court summons, which might not be cost effective.\n\nIn type (iii) where one has parked on private land without permission, a typical notice might read: \"In parking on this land, you hereby accept that your vehicle will be clamped and a £100 release charged\". Although this may seem a simple matter of trespass with an unavoidable fine, it may amount to a case of implied contract (i.e. \"if you park here, you agree to pay a penalty\"); and such a \"penalty\" (read \"damages\") must be proportionate or else the fine will be void. Also, since the penalty notice could have been attached to the windscreen, the clamping of the vehicle may itself be unlawful trespass. It should be noted that since the introduction of the Protection of Freedoms Act 2012 (also known as POFA), that wheel clamping is illegal unless by an Authority (e.g. Police, Local Authority or DVLA).\n\n"}
{"id": "46556301", "url": "https://en.wikipedia.org/wiki?curid=46556301", "title": "Dark media", "text": "Dark media\n\nDark media are a type of media outlined by American philosopher Eugene Thacker.\n\nDiscussed at length in the essay of the same name, Eugene Thacker writes that dark media are media that function too well. Thacker writes that, \"dark media have, as their aim, the mediation of that which is unavailable or inaccessible to the senses, and thus that which we are normally \"in the dark\" about.\" Typically in works of Horror, dark media are relatively commonplace media that show more of the world than is expected, with the dark medium showing what lies beyond the possibility of human sense. Dark media are significant in their ability to breach the, typically unbridgeable, gap between objects being mediated. With dark media, as shown in the J-Horror film \"Ring\", dark media can create a point between the natural and the supernatural. In \"Ring\", the dark medium of the VHS cassette makes it possible for antagonist Sadako Yamamura to cross the threshold of a tv, and subsequently kill those that have viewed the videotape.\n\n"}
{"id": "26167139", "url": "https://en.wikipedia.org/wiki?curid=26167139", "title": "Definitionism", "text": "Definitionism\n\nDefinitionism (also called the classical theory of concepts) is the school of thought in which it is believed that a proper explanation of a theory consists of all the concepts used by that theory being well-defined. This approach has been criticized for its dismissal of the importance of ostensive definitions.\n"}
{"id": "1474763", "url": "https://en.wikipedia.org/wiki?curid=1474763", "title": "Distrust", "text": "Distrust\n\nDistrust is a formal way of not trusting any one party too much in a situation of grave risk or deep doubt. It is commonly expressed in civics as a division or balance of powers, or in politics as means of validating treaty terms. Systems based on distrust simply divide the responsibility so that checks and balances can operate. The phrase \"Trust, but verify\" refers specifically to distrust.\n\nAn electoral system or adversarial process inevitably is based on distrust, but not on mistrust. Parties compete in the system, but they do not compete to subvert the system itself, or gain bad faith advantage through it - if they do they are easily caught by the others. Much mistrust does exist between parties, and it is exactly this which motivates putting in place a formal system of distrust. Diplomatic protocol for instance, which applies between states, relies on such means as formal disapproval which in effect say \"we do not trust that person\". It also tends to rely on a strict etiquette - distrusting each person's habits to signal their intent, and instead relying on a global standard for behaviour in sensitive social settings.\n\nA protocol as defined in computer science uses a more formal idea of distrust itself. Different parts of a system are not supposed to \"trust\" each other but rather perform specific assertions, requests and validations. Once these are passed, the responsibility for errors lies strictly with the receiving part of the system, not that which sent the original information. Applying this principle inside one program is called contract-based design.\n\nCorporate governance relies on distrust insofar as the board is not to trust the reports it receives from management, but is empowered to investigate them, challenge them, and otherwise act on behalf of shareholders vs. managers. The fact that they rarely or never do so in most American companies is a sign that the distrust relationship has broken down - accounting scandals and calls for accounting reform are the inevitable result. It is precisely to avoid such larger crises of trust in \"the system\" that formal distrust measures are put in place to begin with.\n\nNeuroeconomics explain how economists are attempting to understand why humans trust or distrust others by recording physiological measurements during trust experiments. Economists conducted an experiment observing distrust through a trust game. Subjects were asked to anonymously donate various amounts of money to other anonymous subjects with no guarantee of receiving money in return. Various conditions were run of the experiment and after each decision, subjects' levels of DHT were measured. The results of this experiment suggest men and women respond to distrust physiologically differently; a heightened level of the hormone Dihydrotestosterone (DHT) in men is associated with distrust. However, more experiments need to be conducted and more results need to be obtained to accurately state the relationship between the amount of DHT present in males and responses to distrust.\n\nDistrust has also been shown to increase the speed and performance of individuals and groups at certain tasks. One way to classify tasks is to split them into routine (normal, usual) and nonroutine (creative, unusual, undefined). In experiments distrust has been shown to increase performance in nonroutine tasks while decreasing performance in routine tasks.\n\nResearch on high risk settings such as oil platforms, investment banking, medical surgery, aircraft piloting and nuclear powerplants has related distrust to failure avoidance. When non-routine strategies are needed, distrusting persons perform better, while when routine strategies are needed trusting persons perform better. This research was extended to entrepreneurial firms by Gudmundsson and Lechner. They argued that in entrepreneurial firms the threat of failure is ever present resembling non-routine situations in high risk settings. They found that the firms of distrusting entrepreneurs were more likely to survive than the firms of optimistic or overconfident entrepreneurs. The reasons were that distrusting entrepreneurs would emphasize failure avoidance through sensible task selection, and more analysis. Kets de Vries has pointed out that distrusting entrepreneurs are more alert about their external environment. Thus, distrusting entrepreneurs are less likely to discount negative events, and are more likely to engage control mechanisms. Thus, according to Gudmundsson and Lechner distrust leads to higher precaution and therefore increases chances of entrepreneurial firm survival.\n\nThe video game Distrust is a co-op survival game based on John Carpenter's 'The Thing'.\n"}
{"id": "40359", "url": "https://en.wikipedia.org/wiki?curid=40359", "title": "Due process", "text": "Due process\n\nDue process is the legal requirement that the state must respect all legal rights that are owed to a person. Due process balances the power of law of the land and protects the individual person from it. When a government harms a person without following the exact course of the law, this constitutes a due process violation, which offends the rule of law.\n\nDue process has also been frequently interpreted as limiting laws and legal proceedings (see substantive due process) so that judges, instead of legislators, may define and guarantee fundamental fairness, justice, and liberty. That interpretation has proven controversial. Analogous to the concepts of natural justice, and procedural justice used in various other jurisdictions, the interpretation of due process is sometimes expressed as a command that the government must not be unfair to the people or abuse them physically.\nThe term is not used in contemporary English law, but two similar concepts are natural justice, which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions, and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American theory of due process, which, as explained below, presently contains many implied rights not found in either ancient or modern concepts of due process in England.\n\nDue process developed from clause 39 of Magna Carta in England. Reference to due process first appeared in a statutory rendition of clause 39 in 1354 thus: \"No man of what state or condition he be, shall be put out of his lands or tenements nor taken, nor disinherited, nor put to death, without he be brought to answer by due process of law.\" When English and American law gradually diverged, due process was not upheld in England but became incorporated in the US Constitution.\n\nIn clause 39 of Magna Carta, issued in 1215, John of England promised: \"No free man shall be seized or imprisoned, or stripped of his rights or possessions, or outlawed or exiled, or deprived of his standing in any other way, nor will we proceed with force against him, or send others to do so, except by the lawful judgment of his equals or by the law of the land.\" Magna Carta itself immediately became part of the \"law of the land\", and Clause 61 of that charter authorized an elected body of 25 barons to determine by majority vote what redress the King must provide when the King offends \"in any respect against any man.\" Thus, Magna Carta established the rule of law in England by not only requiring the monarchy to obey the law of the land but also limiting how the monarchy could change the law of the land. However, in the 13th century, the provisions may have been referring only to the rights of landowners, and not to ordinary peasantry or villagers.\n\nShorter versions of Magna Carta were subsequently issued by British monarchs, and Clause 39 of Magna Carta was renumbered \"29.\" The phrase \"due process of law\" first appeared in a statutory rendition of Magna Carta in 1354 during the reign of Edward III of England, as follows: \"No man of what state or condition he be, shall be put out of his lands or tenements nor taken, nor disinherited, nor put to death, without he be brought to answer by due process of law.\"\n\nIn 1608, the English jurist Edward Coke wrote a treatise in which he discussed the meaning of Magna Carta. Coke explained that no man shall be deprived but by \"legem terrae\", the law of the land, \"that is, by the common law, statute law, or custom of England... (that is, to speak it once and for all) by the due course, and process of law..\"\n\nBoth the clause in Magna Carta and the later statute of 1354 were again explained in 1704 (during the reign of Queen Anne) by the Queen's Bench, in the case of \"Regina v. Paty\". In that case, the British House of Commons had deprived John Paty and certain other citizens of the right to vote in an election and committed them to Newgate Prison merely for the offense of pursuing a legal action in the courts. The Queen's Bench, in an opinion by Justice Powys, explained the meaning of \"due process of law\" as follows:\nChief Justice Holt dissented in this case because he believed that the commitment had not in fact been by a legal authority. The House of Commons had purported to legislate unilaterally, without approval of the British House of Lords, ostensibly to regulate the election of its members. Although the Queen's Bench held that the House of Commons had not infringed or overturned due process, John Paty was ultimately freed by Queen Anne when she prorogued Parliament.\n\nThroughout centuries of British history, many laws and treatises asserted various requirements as being part of \"due process\" or included in the \"law of the land\". That view usually held in regards to what was required by existing law, rather than what was intrinsically required by due process itself. As the United States Supreme Court has explained, a due process requirement in Britain was not \"essential to the idea of due process of law in the prosecution and punishment of crimes, but was only mentioned as an example and illustration of due process of law as it actually existed in cases in which it was customarily used.\"\n\nUltimately, the scattered references to \"due process of law\" in English law did not limit the power of the government; in the words of American law professor John V. Orth, \"the great phrases failed to retain their vitality.\" Orth points out that this is generally attributed to the rise of the doctrine of parliamentary supremacy in the United Kingdom, which was accompanied by hostility towards judicial review as an undemocratic foreign invention.\n\nScholars have occasionally interpreted Lord Coke's ruling in \"Dr. Bonham's Case\" as implying the possibility of judicial review, but by the 1870s, Lord Campbell was dismissing judicial review as \"a foolish doctrine alleged to have been laid down extra-judicially in Dr. Bonham's Case..., a conundrum [that] ought to have been laughed at.\"\nLacking the power of judicial review, English courts possessed no means by which to declare government statutes or actions invalid as a violation of due process. In contrast, American legislators and executive branch officers possessed virtually no means by which to overrule judicial invalidation of statutes or actions as due process violations, with the sole exception of proposing a constitutional amendment, which are rarely successful. As a consequence, English law and American law diverged. Unlike their English counterparts, American judges became increasingly assertive about enforcing due process of law. In turn, the legislative and executive branches learned how to avoid such confrontations in the first place, by tailoring statutes and executive actions to the constitutional requirements of due process as elaborated upon by the judiciary.\n\nIn 1977, an English political science professor explained the present situation in England for the benefit of American lawyers:\nAn American constitutional lawyer might well be surprised by the elusiveness of references to the term 'due process of law' in the general body of English legal writing... Today one finds no space devoted to due process in Halsbury's \"Laws of England\", in Stephen's \"Commentaries\", or Anson's \"Law and Custom of the Constitution.\" The phrase rates no entry in such works as Stroud's \"Judicial Dictionary\" or Wharton's \"Law Lexicon.\"\n\nTwo similar concepts in contemporary English law are natural justice, which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions, and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American conception of due process, which presently contains many implied rights not found in the ancient or modern concepts of due process in England.\n\nThe Fifth and Fourteenth Amendments to the United States Constitution each contain a Due Process Clause. Due process deals with the administration of justice and thus the Due Process Clause acts as a safeguard from arbitrary denial of life, liberty, or property by the government outside the sanction of law. The Supreme Court of the United States interprets the clauses as providing four protections: procedural due process (in civil and criminal proceedings), substantive due process, a prohibition against vague laws, and as the vehicle for the incorporation of the Bill of Rights.\n\nVarious countries recognize some form of due process under customary international law. Although the specifics are often unclear, most nations agree that they should guarantee foreign visitors a basic minimum level of justice and fairness. Some nations have argued that they are bound to grant no more rights to aliens than they do to their own citizens, the doctrine of national treatment, which also means that both would be vulnerable to the same deprivations by the government. With the growth of international human rights law and the frequent use of treaties to govern treatment of foreign nationals abroad, the distinction, in practice, between these two perspectives may be disappearing.\n\n\n"}
{"id": "544919", "url": "https://en.wikipedia.org/wiki?curid=544919", "title": "Eadie–Hofstee diagram", "text": "Eadie–Hofstee diagram\n\nIn biochemistry, an Eadie–Hofstee diagram (also Woolf–Eadie–Augustinsson–Hofstee or Eadie–Augustinsson plot) is a graphical representation of enzyme kinetics in which reaction rate is plotted as a function of the ratio between rate and substrate concentration:\n\nwhere \"v\" represents reaction rate, \"K\" is the Michaelis–Menten constant, [\"S\"] is the substrate concentration, and \"V\" is the maximum reaction rate.\n\nIt can be derived from the Michaelis–Menten equation as follows:\n\ninvert and multiply with formula_3:\n\nRearrange:\n\nIsolate v:\n\nA plot of v against \"v\"/[S] will hence yield \"V\" as the y-intercept, \"V\"/K as the x-intercept, and \"K\" as the negative slope. Like other techniques that linearize the Michaelis–Menten equation, the Eadie-Hofstee plot was used historically for rapid identification of important kinetic terms like \"K\" and \"V\", but has been superseded by nonlinear regression methods that are significantly more accurate and no longer computationally inaccessible. It is also more robust against error-prone data than the Lineweaver–Burk plot, particularly because it gives equal weight to data points in any range of substrate concentration or reaction rate. (The Lineweaver–Burk plot unevenly weights such points.) Both plots remain useful as a means to present data graphically.\n\nOne drawback from the Eadie–Hofstee approach is that neither ordinate nor abscissa represent independent variables: both are dependent on reaction rate. Thus any experimental error will be present in both axes. Also, experimental error or uncertainty will propagate unevenly and become larger over the abscissa thereby giving more weight to smaller values of \"v\"/[S]. Therefore, the typical measure of goodness of fit for linear regression, the correlation coefficient \"R\", is not applicable.\n\n\n"}
{"id": "24471389", "url": "https://en.wikipedia.org/wiki?curid=24471389", "title": "Elisionism", "text": "Elisionism\n\nElisionism is a philosophical standpoint encompassing various social theories. Elisionist theories are diverse; however, they are unified in their adherence to process philosophy as well as their assumption that the social and the individual cannot be separated. The term \"elisionism\" was coined by Margaret Archer in 1995 in the book \"Realist Social Theory: The Morphogenetic Approach\". Elisionism is often contrasted with holism, atomism, and emergentism.\n"}
{"id": "45322397", "url": "https://en.wikipedia.org/wiki?curid=45322397", "title": "Foil (architecture)", "text": "Foil (architecture)\n\nA foil is an architectural device based on a symmetrical rendering of leaf shapes, defined by overlapping circles that produce a series of cusps to make a lobe. Typically, the number of cusps can be three (\"trefoil\"), four (\"quatrefoil\") or five (\"cinquefoil\"), or can be any number (\"multifoil\"). Foil motifs may be used as part of the heads and tracery of window lights, complete windows themselves, the underside of arches, in heraldry, within panelling, and as part of any decorative or ornament device. Foil types are commonly found in Gothic and Islamic architecture.\n"}
{"id": "4174573", "url": "https://en.wikipedia.org/wiki?curid=4174573", "title": "Four discourses", "text": "Four discourses\n\nFour discourses is a concept developed by French psychoanalyst Jacques Lacan. He argued that there were four fundamental types of discourse. He defined four discourses, which he called Master, University, Hysteric and Analyst, and suggested that these relate dynamically to one another. \n\nLacan's theory of the four discourses was initially developed in 1969, perhaps in response to the events of social unrest during May 1968 in France, but also through his discovery of what he believed were deficiencies in the orthodox reading of the Oedipus Complex. The Four Discourses theory is presented in his seminar \"L'envers de la psychanalyse\" and in \"Radiophonie\", where he starts using \"discourse\" as a social bond founded in intersubjectivity. He uses the term discourse to stress the transindividual nature of language: speech always implies another subject.\n\nPrior to the development of the Four Discourses, the primary guideline for clinical psychoanalysis was Freud's Oedipus Complex. In Lacan's Seminar of 1969-70, Lacan argues that the terrifying Oedipal father that Freud invoked was already castrated at the point of intervention. The castration was symbolic rather than physical. In an effort to stem analysts' tendency to project their ownimaginary readings and neurotic fantasies onto psychoanalysis, Lacan worked to formalise psychoanalytic theory with mathematical functions with renewed focus on the semiology of Ferdinand de Saussure. This would ensure only a minimum of teaching is lost when communicated and also provide the conceptual architecture to limit the associations of the analyst.\n\nDiscourse, in the first place, refers to a point where speech and language intersect. The four discourses represent the four possible formulations of the symbolic network which social bonds can take and can be expressed as the permutations of a four-term configuration showing the relative positions — the agent, the other, the product and the truth — of four terms, the subject, the master signifier, knowledge and objet petit a.\n\nThe four positions in each discourse are :\n\nAgent = Upper left. This is the speaker of the discourse\n\nOther = Upper right. This is what the discourse is addressed to\n\nProduct = Lower right. This is what the discourse has created\n\nTruth = Lower left. This is what the discourse attempted to express\n\nThe four variables which occupy these positions are :\n\nS1 = the master signifier\n\nS2 = knowledge (\"le savoir\")\n\n$ = the subject (barred)\n\na = the objet petit a or surplus-\"jouissance\"\n\nS1 refers to \"the marked circle of the field of the Other,\" it is the Master-Signifier. S2 is the \"battery of signifiers, already there\" at the place where \"one wants to determine the status of a discourse as status of statement,\" that is knowledge (\"savoir\"). S1 comes into play in a signifying battery conforming the network of knowledge. $ is the subject, marked by the unbroken line (\"trait unaire\") which represents it and is different from the living individual who is not the locus of this subject. Add the \"objet petit a\", the object-waste or the loss of the object that occurred when the originary division of the subject took place — the object that is the cause of desire: the \"plus-de-jouir\".\n\nDiscourse of the Master:\n\nIt is the basic discourse from which the other three derive. The dominant position is occupied by the master signifier, S1, which represents the subject, S, for all other signifiers: S2. In this signifying operation there is a surplus: \"objet a\". All attempts at totalisation are doomed to fail. This discourse masks the division of the subject, it illustrates the structure of the dialectic of the master and the slave. The master, S1, is the agent who puts the slave, S2, to work: the result is a surplus, \"objet a\", that the master struggles to appropriate.\n\nDiscourse of the University:\n\nIt is caused by an anticlockwise quarter turn of the previous discourse. The dominant position is occupied by knowledge (\"savoir\"). An attempt to mastery can be traced behind the endeavors to impart neutral knowledge: domination of the other to whom knowledge is transmitted. This hegemony is visible in modernity with science.\n\nDiscourse of the Hysteric:\n\nIt is effected by a clockwise quarter turn of the discourse of the master. It is not simply \"that which is uttered by the hysteric,\" but a certain kind of articulation in which any subject may be inscribed. The divided subject, $, the symptom, is in the pole position. This discourse points toward knowledge. \"The cure involves the structural introduction of the discourse of the hysteric by way of artificial conditions\": the analyst hystericizes the analysand's discourse.\n\nDiscourse of the Analyst:\n\nIt is produced by a quarter turn of the discourse of the hysteric in the same way as Freud develops psychoanalysis by giving an interpretative turn to the discourse of his hysterical patients. The position of the agent — the analyst — is occupied by \"objet a\": the analyst becomes the cause of the analysand's desire. This discourse being the reverse of the discourse of the master, does it make psychoanalysis an essentially subversive practice which undermines attempts at domination and mastery?\n\nSlavoj Žižek uses the theory to explain various cultural artefacts, including \"Don Giovanni\" and \"Parsifal\".\n\n\n\n"}
{"id": "42931220", "url": "https://en.wikipedia.org/wiki?curid=42931220", "title": "GapChart", "text": "GapChart\n\nIn information visualization and computing, GapChart is a chart for displaying time series data by using non overlapped thick curves. It was invented in 2013 by Fred. Vernier (LIMSI labs at Univ. Paris Sud) and Charles Perin (LIMSI labs at Univ. Paris Sud and AVIZ-INRIA). Jeremy Boy (AVIZ-INRIA) helped them to improve the original design to its current form.\n\nGapChart display one thick curve by time series data. At each time step curves display a flat part and a transition part. Flat parts are separated by gaps proportional to data difference. When two or more time series have same value at the same time step they are distinguished by an external method and represented side by side. Transition steps are represented by straight or S shaped link. They can partially overlap but never totally since star\nLabels can fit inside thick curves where i't not too steep and different colors ca be used to better distinguish elements.\n\n\n\n\n\n"}
{"id": "20389175", "url": "https://en.wikipedia.org/wiki?curid=20389175", "title": "Generativity", "text": "Generativity\n\nThe term generativity was coined by the psychoanalyst Erik Erikson in 1950 to denote \"a concern for establishing and guiding the next generation.\" He first used the term while defining the Care stage in his theory of the stages of psychosocial development.\n\nJonathan Zittrain adopted the term in 2006 to refer to “the ability of a technology platform or technology ecosystem to create, generate or produce new output, structure or behavior without input from the originator of the system.”\n\nIn 1950 Erik Erikson created the term generativity to explain the Care stage in his theory of the stages of psychosocial development. The Care stage encompasses the middle ages of one’s life, from 40 through 64. Generativity was defined as the “ability to transcend personal interests to provide care and concern for younger and older generations.” It took over 30 years for generativity to become a subject of empirical research. Modern psychoanalysts, starting in the early 1990s, have included a concern for one’s legacy, referred to as an “inner desire for immortality”, in the definition of generativity.\n\nMore recently, the term has been adopted by people who deal with technology, first used by Johnathan Zittrain in 2006. Generativity in technology is defined as “the ability of a technology platform or technology ecosystem to create, generate or produce new output, structure or behavior without input from the originator of the system.” An example of this could be the iOS and Android mobile operating systems, for which developers have created millions of unique applications. It has been argued that the open Internet is both an inspiration of generativity and a means to spread the products of generativity. However, some people including Johnathan Zittrain fear that society and technology are moving away from a generative internet, claiming “A shift in consumer priorities from generativity to stability will compel undesirable responses from regulators and markets and, if unaddressed, could prove decisive in closing today’s open computing environments.”\n\nPsychologically, generativity is concern for the future, a need to nurture and guide younger people and contribute to the next generation. Erikson argued that this usually develops during middle age (which spans ages 40 through 64) in keeping with his stage-model of psychosocial development. After having experienced old age himself, Erikson believed that generativity maintains a more important role in later life than he initially had thought.\n\nIn Erikson’s theory Generativity is contrasted with Stagnation. During this stage, people contribute to the next generation through caring, teaching, engaging in creative work which contributes to society. Generativity involves answering the question \"Can I Make My Life Count?\", and in this process, finding your life’s work and contributing to the development of others through activities such as volunteering, mentoring, and contributing to future generations. It has also been described as a concern for one's legacy, accepting the independence lives of family and increasing philanthropic pursuits. Generative concern leads to concrete goals and actions such as \"providing a narrative schematic of the generative self to the next generation\".\n\nMcAdams developed a 20-item scale to assess generativity, and to help discover who it is that is nurturing and leading the next generation. McAdam's model is not restricted to stages, with generativity able to be a concern throughout adulthood, not just in middle adulthood, as Erikson suggested. Example items include \"I try to pass along the knowledge that I have gained through my experiences\", \"I have a responsibility to improve the neighborhood in which I live\", and (reversed) \"In general, my actions do not have a positive effect on other people.\"\n\nGenerativity in technology refers to cases where a technology supports the creation of novel products. Such technologies are referred to as generative systems. Canonical examples are the PC and the Internet. From its inception, the internet has acted as a generative force allowing users to create and communicate in ways unimagined but foreseen by its creators who for this purpose built-in an openness and hardware and software agnosticism.\n\nZittrain was first to apply this term outside psychology, in cases where a generative technology leads to \"unanticipated change through unfiltered contributions from broad and varied audiences.\" Zittrain has also highlighted that precarious nature of generative technology: arguing that features which, for instance, may enhance security and stability may, even unintentionally reduce or destroy a generativity in a system. He highlighted cases in which apparently innocuous producer, consumer, and government actions from a move away from PCs to one-way systems such as \"smart\" appliances cause a decline in generativity. As a result, he emphasised the need to be clear about treating generativity, rather than apparent means supporting this as the key valued characteristic of the system. In the case of the internet/PC complex this is its capacity as a generative networked grid, rather than traits associated with this, such as \"open internet\" or “network neutrality”. He termed a focus on these mere-means to the end of generativity a \"myopic\" \"end-to-end theory\" which confused means with ends. Zittrain argued:focusing on “network” without regard to a particular network policy’s influence on the design of net- work endpoints such as PCs. As a result of this focus, political advo- cates of end-to-end are too often myopic; many writers seem to pre- sume current PC and OS architecture to be fixed in an “open” position. If they can be persuaded to see a larger picture, they may agree to some compromises at the network level. If complete fidelity to end-to- end network neutrality persists, our PCs may be replaced by informa- tion appliances or may undergo a transformation from open platforms to gated communities to prisons, creating a consumer information en- vironment that betrays the very principles that animate end-to-end theory. ( p. 1978)\n\n"}
{"id": "45225971", "url": "https://en.wikipedia.org/wiki?curid=45225971", "title": "Green information system", "text": "Green information system\n\nGreen information system (green IS) or Green IT as per International Federation of Global & Green ICT \"IFGICT\" is a combination of environment and information technology (IT). The relationship between information technology and the environment is complex, because of the negative environmental impact of IT production, use, and disposal then, making this effect greener has been termed Green Information Technology, which considers IT's environmental impact primarily as a problem to be mitigated. Another effect involves the positive impact of using information systems (IS) to improve the eco-sustainability of businesses and society; this is termed green IS. This green IS viewpoint sees IS as a partial solution to many environmental problems. So green IS is a solution for the negative environmental effect of information technology. IS facilitates the reuse of waste and energy and can serve as a tool for industrial symbiosis, which involves \"the mutualistic interaction of different industries for beneficial reuse of waste flows or energy cascading that results in a more resource-efficient production system and fewer adverse environmental impacts\".\n\n"}
{"id": "250320", "url": "https://en.wikipedia.org/wiki?curid=250320", "title": "Healthy city", "text": "Healthy city\n\nHealthy city is a term used in public health and urban design to stress the impact of policy on human health. Its modern form derives from a World Health Organization (WHO) initiative on Healthy Cities and Villages in 1986, but has a history dating back to the mid 19th century. The term was developed in conjunction with the European Union, but rapidly became international as a way of establishing healthy public policy at the local level through health promotion. It emphasises the multi-dimensionality of health as laid out in WHO's constitution and, more recently, the Ottawa Charter for Health Promotion. An alternative term is Healthy Communities, or \"Municipios saludables\" in parts of Latin America.\n\nMany jurisdictions which have healthy community programmes and cities can apply to become a WHO-designated \"Healthy City\". WHO defines the Healthy City as:\n\"one that is continually creating and improving those physical and social environments and expanding those community resources which enable\npeople to mutually support each other in performing all the functions of life and in developing to their maximum potential.\"\n\nMeasuring the indices required, establishing standards and determining the impact of each component on health is difficult. In some regions such as Europe, a health impact assessment is a required piece of public policy development.\n\nThere are many networks of healthy cities, including in Europe and internationally, such as the Alliance for Healthy Cities. A key feature is ensuring that the social determinants of health are taken into consideration in urban design and urban governance. For example, \"urbanization and health\" was the theme of the 2010 World Health Day. One tool in developing healthy cities is social entrepreneurship.\n\n\n\n"}
{"id": "713946", "url": "https://en.wikipedia.org/wiki?curid=713946", "title": "Hyperplane at infinity", "text": "Hyperplane at infinity\n\nIn geometry, any hyperplane \"H\" of a projective space \"P\" may be taken as a hyperplane at infinity. Then the set complement is called an affine space. For instance, if are homogeneous coordinates for \"n\"-dimensional projective space, then the equation defines a hyperplane at infinity for the \"n\"-dimensional affine space with coordinates . \"H\" is also called the ideal hyperplane.\n\nSimilarly, starting from an affine space \"A\", every class of parallel lines can be associated with a point at infinity. The union over all classes of parallels constitute the points of the hyperplane at infinity. Adjoining the points of this hyperplane (called ideal points) to \"A\" converts it into an \"n\"-dimensional projective space, such as the real projective space .\n\nBy adding these ideal points, the entire affine space \"A\" is completed to a projective space \"P\", which may be called the projective completion of \"A\". Each affine subspace \"S\" of \"A\" is completed to a projective subspace of \"P\" by adding to \"S\" all the ideal points corresponding to the directions of the lines contained in \"S\". The resulting projective subspaces are often called \"affine subspaces\" of the projective space \"P\", as opposed to the infinite or ideal subspaces, which are the subspaces of the hyperplane at infinity (however, they are projective spaces, not affine spaces).\n\nIn the projective space, each projective subspace of dimension \"k\" intersects the ideal hyperplane in a projective subspace \"at infinity\" whose dimension is .\n\nA pair of non-parallel affine hyperplanes intersect at an affine subspace of dimension , but a parallel pair of affine hyperplanes intersect at a projective subspace of the ideal hyperplane (the intersection \"lies on\" the ideal hyperplane). Thus, parallel hyperplanes, which did not meet in the affine space, intersect in the projective completion due to the addition of the hyperplane at infinity.\n\n\n"}
{"id": "50662306", "url": "https://en.wikipedia.org/wiki?curid=50662306", "title": "Ion channel hypothesis of Alzheimer's disease", "text": "Ion channel hypothesis of Alzheimer's disease\n\nThe ion channel hypothesis of Alzheimer’s disease (AD), also known as the channel hypothesis or the amyloid beta ion channel hypothesis, is a more recent variant of the amyloid hypothesis of AD, which identifies amyloid beta (Aβ) as the underlying cause of neurotoxicity seen in AD. While the traditional formulation of the amyloid hypothesis pinpoints insoluble, fibrillar aggregates of Aβ as the basis of disruption of calcium ion homeostasis and subsequent apoptosis in AD, the ion channel hypothesis in 1993 introduced the possibility of an ion-channel-forming oligomer of soluble, non-fibrillar Aβ as the cytotoxic species allowing unregulated calcium influx into neurons in AD.\n\nThe ion channel hypothesis is broadly supported as an explanation for the calcium ion influx that disrupts calcium ion homeostasis and induces apoptosis in neurons. Because the extracellular deposition of Aβ fibrils in senile plaques is not sufficient to predict risk or onset of AD, and clinical trials of drugs that target the Aβ fibrillization process have largely failed, the ion channel hypothesis provides novel molecular targets for continued development of AD therapies and for better understanding of the mechanism underlying onset and progression of AD.\n\nThe ion channel hypothesis was first proposed by Arispe and colleagues in 1993 upon discovery that Aβ could form unregulated cation-selective ion channels when incorporated into planar lipid bilayers. Further research showed that a particular fragment of Aβ, Aβ (25-35), spontaneously inserts into planar lipid bilayers to form weakly selective ion channels and that membrane insertion occurs non-specifically, irreversibly, and with a broad range of oligomer conformations. Though more recent studies have found that Aβ channels can be blocked by small molecules, the broad variety of Aβ ion channel conformations and chemistries make it difficult to design a channel blocker specific to Aβ without compromising other ion channels in the cell membrane.\n\nThe Aβ monomer generally assumes an α-helical formation in aqueous solution, but can reversibly transition between α-helix and β-sheet structures at varying polarities. Atomic force microscopy captured images of Aβ channel structures that facilitated calcium uptake and subsequent neuritic degeneration. Molecular dynamics simulations of Aβ in lipid bilayers suggest that Aβ adopts a β-sheet-rich structure within lipid bilayers that gradually evolves to result in a wide variety of relaxed channel conformations. In particular, data support the organization of Aβ channels in β-barrels, structural formations commonly seen in transmembrane pore-forming toxins including anthrax.\n\nAβ channels are selective for cations over anions, voltage-independent, and display a long channel lifetime, from minutes to hours. They can be extremely large, up to 5 nS in size, and can insert into the cell membrane from aqueous solution. Aβ channels are heterogeneous and allow flow of physiologcially relevant ions such as Ca, Na, K, Cs, and Li across the cell membrane.\n\nCytotoxicity caused by ion channel formation is commonly seen in the world of bacteria. While eukaryotic cells are generally less vulnerable to channel-forming toxins because of their larger volume and stiffer, sterol-containing membranes, several eukaryotic channel-forming toxins have been seen to sidestep these obstacles by forming especially large, stable ion channels or anchoring to sterols in the cell membrane. Neurons are particularly vulnerable to channel-forming toxins because of their reliance on maintenance of strict Na, K, and Ca concentration gradients and membrane potential for proper functioning and action potential propagation. Leakage caused by insertion of an ion channel such as Aβ rapidly alters intracellular ionic concentrations, resulting in energetic stress, failure of signaling, and cell death.\n\nThe large, poorly selective, and long-lived nature of Aβ channels allows rapid degradation of membrane potential in neurons. A single Aβ channel 4 nS in size can cause Na concentration to change as much as 10 μM/s. Degradation of membrane potential in this manner also generates additional Ca influx through voltage-sensitive Ca channels in the plasma membrane. Ionic leakage alone has been demonstrated to be sufficient to rapidly disrupt cellular homeostasis and induce cell necrosis.\n\nAβ channels may also trigger apoptosis through insertion in mitochondrial membranes. Aβ injection in rats has been shown to damage mitochondrial structure in neurons, decrease mitochondrial membrane potential, and increase intracellular Ca concentration. Additionally, Aβ accumulation increases expression of genes associated with the mitochondrial permeability transition pore (MPTP), a non-selective, high conductance channel spanning the inner and outer mitochondrial membrane. Ca influx into mitochondria can collapse mitochondrial membrane potential, causing MPTP opening, which then induces mitochondrial swelling, further dissipation of membrane potential, generation of mitochondrial reactive oxygen species (ROS), rupture of the outer mitochondrial membrane, and release of apoptogenic factors such as cytochrome c.\n\nThe only treatments currently approved for AD are either cholinesterase inhibitors (such as donepezil) or glutamate receptor antagonists (such as memantine), which show limited efficacy in treating symptoms or halting progression of AD. The slight improvement in cognitive function brought about by these drugs is only seen in patients with mild to moderate AD, and is confined to the first year of treatment, as efficacy progressively declines, completely disappearing by 2 or 3 years of treatment. Extensive research has gone into the design of potential AD treatments to reduce Aβ production or aggregation, but these therapeutics have historically failed in Phase III clinical trials. The ion channel hypothesis of AD provides a novel avenue for development of AD therapies that may more directly target the underlying pathophysiology of AD.\n\nNonspecific Aβ channel blockers including tromethamine (Tris) and Zn have successfully inhibited Aβ cytotoxicity. Least-energy molecular models of the Aβ channel have been used to create polypeptide segments to target the mouth of the Aβ pore, and these selective Aβ channel blockers have also been shown to inhibit Aβ cytotoxicity. Structural modeling of Aβ channels, however, suggests that the channels are highly polymorphic, with the ability to move and change size and shape within the lipid membrane. The broad range of conformations adopted by the Aβ channel makes design of a specific, highly effective Aβ channel blocker difficult.\n\nIndirect methods such as membrane hyperpolarization may help limit the cytotoxic depolarizing effects of Aβ channels. Potassium ATP channel activation has been demonstrated to attenuate Ca influx and reduce oxidative stress in neurons, as well as to improve memory and reduce Aβ and tau pathology in a transgenic AD mouse model. Similarly, drugs that block voltage-gated Ca channels have also been shown to protect neurons from Aβ toxicity.\n\nSeveral other classes of amyloid proteins also form ion channels, including proteins implicated in type II diabetes mellitus, prion diseases, Parkinson's disease, and Huntington's disease. Consistent with Aβ channels, other amyloid channels have also been reported to be large, non-selective, voltage-independent, heterogeneous, and irreversible. These distinct properties set amyloid channels apart from other ion channels in neurons and facilitate unregulated ionic leakage resulting in cell depolarization, disruption of ion homeostasis, and cell death. Further investigation of amyloid proteins and the cytotoxic effects of amyloid channel formation is necessary for development of drug candidates that are able to selectively block amyloid channels or bind them prior to membrane insertion, an area of research that may prove highly relevant to not just AD but a wide variety of other diseases.\n"}
{"id": "31765232", "url": "https://en.wikipedia.org/wiki?curid=31765232", "title": "List of mathematical concepts named after places", "text": "List of mathematical concepts named after places\n\nThis list contains mathematical concepts named after geographic locations.\n"}
{"id": "6886172", "url": "https://en.wikipedia.org/wiki?curid=6886172", "title": "Love lock", "text": "Love lock\n\nA love lock or love padlock is a padlock which sweethearts lock to a bridge, fence, gate, monument, or similar public fixture to symbolize their love. Typically the sweethearts' names or initials, and perhaps the date, are inscribed on the padlock, and its key is thrown away (often into the nearby river) to symbolize unbreakable love.\n\nSince the 2000s, love locks have proliferated at an increasing number of locations worldwide. They are now mostly treated by municipal authorities as litter or vandalism, and there is some cost to their removal. However, there are authorities who embrace them, and who use them as fundraising projects or tourist attractions.\n\nThe history of love padlocks dates back at least 100 years to a melancholic Serbian tale of World War I, with an attribution for the bridge Most Ljubavi (lit. the \"Bridge of Love\") in the spa town of Vrnjačka Banja. A local schoolmistress named \"Nada\", who was from Vrnjačka Banja, fell in love with a Serbian officer named \"Relja\". After they committed to each other Relja went to war in Greece where he fell in love with a local woman from Corfu. As a consequence, Relja and Nada broke off their engagement. Nada never recovered from that devastating blow, and after some time she died due to heartbreak from her unfortunate love.\n\nAs young women from Vrnjačka Banja wanted to protect their own loves, they started writing down their names, with the names of their loved ones, on padlocks and affixing them to the railings of the bridge where Nada and Relja used to meet.\n\nIn the rest of Europe, love padlocks started appearing in the early 2000s as a ritual or epidemy. The reasons love padlocks started to appear vary between locations and in many instances are unclear. However, in Rome, the ritual of affixing love padlocks to the bridge Ponte Milvio can be attributed to the 2006 book by Italian author Federico Moccia, who made a film adaptation in 2007.\n\nIn many cities, love locking has been classified an act of vandalism.\nIn several countries, the local authorities and owners of various landmarks have expressed concern often have the padlocks removed:\n\n\n\nOn some locations the padlocks have been given almost legendary or superstitious character:\n"}
{"id": "510370", "url": "https://en.wikipedia.org/wiki?curid=510370", "title": "Neurofeedback", "text": "Neurofeedback\n\nNeurofeedback (NFB), also called neurotherapy or neurobiofeedback, is a type of biofeedback that uses real-time displays of brain activity—most commonly electroencephalography (EEG), to teach self-regulation of brain function. Typically, sensors are placed on the scalp to measure activity, with measurements displayed using video displays or sound.\n\nNeurofeedback is a type of biofeedback that measures brain waves to produce a signal that can be used as feedback to teach self-regulation of brain function. Neurofeedback is commonly provided using video or sound, with positive feedback for desired brain activity and negative feedback for brain activity that is undesirable. Related technologies include hemoencephalography biofeedback (HEG) and functional magnetic resonance imaging (fMRI) biofeedback.\n\nClinical guidelines on neurofeedback as a treatment for ADHD are mixed. Biofeedback is graded by the American Academy of Pediatrics with their Level 2 evidence-based treatment for ADHD. The NICE guideline for ADHD leaves the efficacy of biofeedback an open question (p. 412). In page 202 states \"Biofeedback has been employed as a non-invasive treatment for children with ADHD since the 1970s but is probably not used as a significant intervention in UK clinical practice\". However this is unsurprising since in the UK, NICE evaluates whether treatments should be recommended on the basis of the cost of a quality-adjusted life year. SIGN guideline no 112 in page 24 mentions \"Neurofeedback is presently considered to be an experimental intervention in children and young people with ADHD/HKD. There are no standardised interventions\". Institute for Clinical Systems Improvement guideline on Diagnosis and Management of Attention Deficit Hyperactivity Disorder in Primary Care for School-Age Children and Adolescents in page 41 mentions neurofeedback lacks enough research evidence for efficacy in ADHD.\n\nOverall research into neurofeedback is considered to have been limited and of low quality, although others have disagreed.\n\nIt has been argued there is some indication on the effectiveness of biofeedback for ADHD but that it is not conclusive: several studies have yielded positive results, however the best designed ones have either shown absent or reduced effects. Other experts have proposed that standard neurofeedback protocols for ADHD, such as theta/beta, SMR and slow cortical potentials neurofeedback are well investigated and have demonstrated specificity. No serious adverse side effects from neurofeedback have been reported.\n\nQEEG has been used to develop EEG models of ADHD. According to this model, persons with ADHD often have too many slow theta brain waves (associated with relaxation) and not enough fast beta wave activity (associated with mental focus). Neurofeedback therapies for ADHD generally attempt to increase the production of betawaves and decrease the number of slower brain waves. This can be accomplished by allowing the patient to view their levels of brain waves on a screen and attempt to alter them, or by integrating brain waves into a video game.\n\nResearch shows neurofeedback may be a potentially useful intervention for a range of brain-related conditions. It has been used for pain, addiction, aggression, anxiety, autism, depression, Schizophrenia, epilepsy, headaches, insomnia, Tourette syndrome, and brain damage from stroke, trauma, and other causes.\n\nIt is also used to treat other less well known disorders, such as Auditory Processing Disorder and working memory deficit.\n\nThe applications of neurofeedback to enhance performance extend to the arts in fields such as music, dance, and acting. A study with conservatoire musicians found that alpha-theta training benefitted the three music domains of musicality, communication, and technique. Historically, alpha-theta training, a form of neurofeedback, was created to assist creativity by inducing hypnagogia, a “borderline waking state associated with creative insights”, through facilitation of neural connectivity. Alpha-theta training has also been shown to improve novice singing in children. Alpha-theta neurofeedback, in conjunction with heart rate variability training, a form of biofeedback, has also produced benefits in dance by enhancing performance in competitive ballroom dancing and increasing cognitive creativity in contemporary dancers. Additionally, neurofeedback has also been shown to instil a superior flow state in actors, possibly due to greater immersion while performing.\n\nHowever, randomized control trials have found that neurofeedback training (using either sensorimotor rhythm or theta/beta ratio training) did not enhance performance on attention-related tasks or creative tasks. It has been suggested that claims made by proponents of alpha wave neurofeedback training techniques have yet to be validated by randomized, double-blind, controlled studies, a view which even some supporters of alpha neurofeedback training have also expressed.\n\nIn 1924, the German psychiatrist Hans Berger connected a couple of electrodes (small round discs of metal) to a patient's scalp and detected a small current by using a ballistic galvanometer. During the years 1929-1938 he published 14 reports about his studies of EEGs, and much of our modern knowledge of the subject, especially in the middle frequencies, is due to his research. Berger analyzed EEGs qualitatively, but in 1932 G. Dietsch applied Fourier analysis to seven records of EEG and became the first researcher of what later is called QEEG (quantitative EEG).\n\nLater, Joe Kamiya popularized neurofeedback in the 1960s when an article about the alpha brain wave experiments he had been conducting was published in \"Psychology Today\" in 1968. Kamiya’s experiment had two parts. In the first part, a subject was asked to keep his eyes closed and when a tone sounded to say whether he thought he was in alpha. He was then told whether he was correct or wrong. Initially the subject would get about fifty percent correct, but some subjects would eventually develop the ability to better distinguish between states. In the second part of the study, subjects were asked to go into alpha when a bell rang once and not go into the state when the bell rang twice. Once again some subjects were able to enter the state on command. Alpha states were connected with relaxation, and alpha training had the possibility to alleviate stress and stress-related conditions.\n\nDespite these claims, the universal correlation of high alpha density to a subjective experience of calm cannot be assumed. Alpha states do not seem to have the universal stress-alleviating power indicated by early observations. At one point, Martin Orne and others challenged the claim that alpha biofeedback actually involved the training of an individual to voluntarily regulate brainwave activity. James Hardt and Joe Kamiya, then at UC San Francisco's Langley Porter Neuropsychiatric Institute published a paper that supported biofeedback.\n\nIn the late sixties and early seventies, Barbara Brown, one of the most effective popularizers of Biofeedback, wrote several books on biofeedback, making the public much more aware of the technology. The books included \"New Mind New Body\", with a foreword from Hugh Downs, and \"Stress and the Art of Biofeedback\". Brown took a creative approach to neurofeedback, linking brainwave self-regulation to a switching relay which turned on an electric train.\n\nThe work of Barry Sterman, Joel F. Lubar and others has been relevant on the study of beta training, involving the role of sensorimotor rhythmic EEG activity. This training has been used in the treatment of epilepsy, attention deficit disorder and hyperactive disorder. The sensorimotor rhythm (SMR) is rhythmic activity between 12 and 16 hertz that can be recorded from an area near the sensorimotor cortex. SMR is found in waking states and is very similar if not identical to the sleep spindles that are recorded in the second stage of sleep.\n\nFor example, Sterman has shown that both monkeys and cats who had undergone SMR training had elevated thresholds for the convulsant chemical monomethylhydrazine. These studies indicate that SMR may be associated with an inhibitory process in the motor system.\n\nWithin the last 5–10 years, neurofeedback has taken a new approach in taking a look at deep states. Alpha-theta training has been tried with patients with alcoholism, other addictions as well as anxiety. This low frequency training differs greatly from the high frequency beta and SMR training that has been practiced for over thirty years and is reminiscent of the original alpha training of Elmer Green and Joe Kamiya. Beta and SMR training can be considered a more directly physiological approach, strengthening sensorimotor inhibition in the cortex and inhibiting alpha patterns, which slow metabolism. Alpha-theta training, however, derives from the psychotherapeutic model and involves accessing of painful or repressed memories through the alpha-theta state. The alpha-theta state is a term that comes from the representation on the EEG.\n\nA recent development in the field is a conceptual approach called the Coordinated Allocation of Resource Model (CAR) of brain functioning which states that specific cognitive abilities are a function of specific electrophysiological variables which can overlap across different cognitive tasks. The activation database guided EEG biofeedback approach initially involves evaluating the subject on a number of academically relevant cognitive tasks and compares the subject's values on the QEEG measures to a normative database, in particular on the variables that are related to success at that task.\n\nThe Association for Applied Psychophysiology and Biofeedback (AAPB) is a non-profit scientific and professional society for biofeedback and neurofeedback. The International Society for Neurofeedback and Research (ISNR) is a non-profit scientific and professional society for neurofeedback. The Biofeedback Federation of Europe (BFE) sponsors international education, training, and research activities in biofeedback and neurofeedback.\n\nThe Biofeedback Certification International Alliance (formerly the Biofeedback Certification Institute of America) is a non-profit organization that is a member of the Institute for Credentialing Excellence (ICE). BCIA certifies individuals who meet education and training standards in biofeedback and neurofeedback and progressively recertifies those who satisfy continuing education requirements. BCIA offers biofeedback certification, neurofeedback (also called EEG biofeedback) certification, and pelvic muscle dysfunction biofeedback certification. BCIA certification has been endorsed by the Mayo Clinic, the Association for Applied Psychophysiology and Biofeedback (AAPB), the International Society for Neurofeedback and Research (ISNR), and the Washington State Legislature.\n\nThe BCIA didactic education requirement includes a 36-hour course from a regionally accredited academic institution or a BCIA-approved training program that covers the complete Neurofeedback Blueprint of Knowledge and study of human anatomy and physiology. The Neurofeedback Blueprint of Knowledge areas include: I. Orientation to Neurofeedback, II. Basic Neurophysiology and Neuroanatomy, III. Instrumentation and Electronics, IV. Research, V. Psychopharmalogical Considerations, VI. Treatment Planning, and VII. Professional Conduct.\n\nApplicants may demonstrate their knowledge of human anatomy and physiology by completing a course in biological psychology, human anatomy, human biology, human physiology, or neuroscience provided by a regionally accredited academic institution or a BCIA-approved training program or by successfully completing an Anatomy and Physiology exam covering the organization of the human body and its systems.\n\nApplicants must also document practical skills training that includes 25 contact hours supervised by a BCIA-approved mentor designed to them teach how to apply clinical biofeedback skills through self-regulation training, 100 patient/client sessions, and case conference presentations. Distance learning allows applicants to complete didactic course work over the internet. Distance mentoring trains candidates from their residence or office. They must recertify every 4 years, complete 55 hours of continuing education (30 hours for Senior Fellows) during each review period or complete the written exam, and attest that their license/credential (or their supervisor’s license/credential) has not been suspended, investigated, or revoked.\n\nIn 2010, a study provided some evidence of neuroplastic changes occurring after brainwave training. Half an hour of voluntary control of brain rhythms led in this study to a lasting shift in cortical excitability and intracortical function. The authors observed that the cortical response to transcranial magnetic stimulation (TMS) was significantly enhanced after neurofeedback, persisted for at least 20 minutes, and was correlated with an EEG time-course indicative of activity-dependent plasticity.\n\n\n\n"}
{"id": "25932537", "url": "https://en.wikipedia.org/wiki?curid=25932537", "title": "Panopticon gaze", "text": "Panopticon gaze\n\nThe panopticon gaze (from panopticon) is an ideological phrase, a metaphor. The panopticon gaze is the idea of a silent, unknown overseer in the society such as the government that subconsciously controlled all aspects of life. It symbolizes extreme transparency within the society where the rulers or leaders can look down and know, being able to see exactly what is going on, influencing the actions of every individual.\nMilan Kundera used this idea in his fictional novel about the Soviet Invasion of Czechoslovakia in 1968, \"The Unbearable Lightness of Being\".\n\n"}
{"id": "29193999", "url": "https://en.wikipedia.org/wiki?curid=29193999", "title": "Patient abuse", "text": "Patient abuse\n\nPatient abuse or neglect is any action or failure to act which causes unreasonable suffering, misery or harm to the patient.\nPatient abuse and neglect may occur in settings such as hospitals, nursing homes, clinics and during home-based care.\n\nBooks\n\nAcademic articles\n\n"}
{"id": "35574165", "url": "https://en.wikipedia.org/wiki?curid=35574165", "title": "Personal data service", "text": "Personal data service\n\nPersonal data services or personal data stores (PDS) are services to let an individual store, manage and deploy their key personal data in a highly secure and structured way. \n\nThey give the user a central point of control for their personal information (e.g. interests, contact information, affiliations, preferences, friends). The user's data attributes being managed by the service may be stored in a co-located repository, or they may be stored multiple external distributed repositories, or a combination of both. Attributes from a PDS may be accessed via an API. Users of the same PDS instance may be allowed to selectively share sets of attributes with other users.\n\nCloud-based PDSes:\n\n\nPC-based PDSes:\n\n\n\n"}
{"id": "179824", "url": "https://en.wikipedia.org/wiki?curid=179824", "title": "Plan", "text": "Plan\n\nA plan is typically any diagram or list of steps with details of timing and resources, used to achieve an objective to do something. See also strategy. It is commonly understood as a temporal set of intended actions through which one expects to achieve a goal.\n\nFor spatial or planar topologic or topographic sets see map.\nPlans can be formal or informal:\n\nThe most popular ways to describe plans are by their breadth, time frame, and specificity; however, these planning classifications are not independent of one another. For instance, there is a close relationship between the short- and long-term categories and the strategic and operational categories.\n\nIt is common for less formal plans to be created as abstract ideas, and remain in that form as they are maintained and put to use. More formal plans as used for business and military purposes, while initially created with and as an abstract thought, are likely to be written down, drawn up or otherwise stored in a form that is accessible to multiple people across time and space. This allows more reliable collaboration in the execution of the plan.\n\nThe term planning implies the working out of sub-components in some degree of elaborate detail. Broader-brush enunciations of objectives may qualify as metaphorical roadmaps. Planning literally just means the creation of a plan; it can be as simple as making a list. It has acquired a technical meaning, however, to cover the area of government legislation and regulations elated to the use of resources.\n\nPlanning can refer to the planned use of any and all resources, as in the succession of Five-Year Plans through which the government of the Soviet Union sought to develop the country. However, the term is most frequently used in relation to planning for the use of land and related resources, for example in urban planning, transportation planning, etc.\n\nión planning, etc.\n\nIn a governmental context, \"planning\" without any qualification is most likely to mean the regulation of land use. See also zoning.\n\nPlanners are the professionals that have the requisite training to take or make decisions that will help or balance the society in order to have a functional, aesthetic, and convenient environment.\n\nConcepts such as top-down planning (as opposed to bottom-up planning) reveal similarities with the systems thinking behind the top-down model.\n\nThe subject touches such broad fields as psychology, game theory, communications and information theory, which inform the planning methods that people seek to use and refine; as well as logic and science (i.e. methodological naturalism) which serve as a means of testing different parts of a plan for reliability or consistency.\n\nThe specific methods used to create and refine plans depend on who is to make it, who is to put it to use, and what resources are available for the task. The methods used by an individual in his or her mind or personal organizer, may be very different from the collection of planning techniques found in a corporate board-room, and the planning done by a project manager has different priorities and uses different tools to the planning done by an engineer or industrial designer.\n\n\n"}
{"id": "16802377", "url": "https://en.wikipedia.org/wiki?curid=16802377", "title": "Rafael Y. Herman", "text": "Rafael Y. Herman\n\nRafael Yossef Herman (born 1974 in Be'er Sheva רפאל י. הרמן) is an artist best known for his breakthrough photography project Bereshit - Genesis. Recalling the Genesis Creation story, this photography research project, of Israeli desert Negev trees, marked for the first time in photography history, a \"nocturnes photos\" that looks like taken in day light, using just the \"moon-light\", with no electronic or digital manipulation. The night, like never seen before, with monochromatic but full colored of its \"colors of the night\". This lighted night, nearly divined, without shadow, creates a new surreal reality, one that our eye can not see and does not know.\nThe Bereshit project was exposed for the first time to public in Dec. 2006, in Milano Italy, in the prestigious \"Sala delle Cariatidi\" of Palazzo Reale's museum of art. Bereshit photos, were one out of four fragments of Israel, that Rafael showed, under the title \"Magà\". The exhibition attracted the attention of the public and media, and above all, of the Milan's assessor of culture, Vittorio Sgarbi, that wrote: \"...His images are mirror of Creation, beyond which there is no more to say, revelation that only conceives the contemplative dimension, mixing spirit, mystery and material. Like this Herman wins the darkness.\" This occasion brought to Herman the invitation present the project to Pompidou Center, Paris. In November 2007, in a personal exhibition titled Bereshit, in Studio Guastalla, Rafael Y. Herman discovered by the prestigious art historian, writer and collector Arturo Schwarz that wrote on Herman's project \"...this black light, is the responsible of the intense poetical aura, of this photos serial...\". Newspaper and Magazine titled the exhibition like; The Voice Of The Moon (Arte), Symbolic Research (Elle), The Trees Of Herman (TG), The Beginning Of All (Bolletino). During this exhibition period, Rafael lost his father Maxim (b. 1937 Bucharest d. 2007 19 Kislev ) just before finishing to write a documentary/romance book on his father's family story during the second world war.\n\nRafael Y. Herman showed his works in South America, Europe, and Asia in collective and personal exhibitions. He was collaborating with Amnesty International and Behia's government at Bahian Carnival 2002. Herman was graduate in Economy and Management from Tel Aviv University, and as a child had musical education for 13 years. Today he lives in Italy and spend his life between Europe, America, and Israel. Currently his multiform photographic projects roam from fashion to reportage, from editorial and movie set photography to experimental photo research.\n\n"}
{"id": "20291889", "url": "https://en.wikipedia.org/wiki?curid=20291889", "title": "Rationalist Association of India", "text": "Rationalist Association of India\n\nRationalist Association of India (RAI) is an Indian rationalist organization that was established in 1930. Dr. D' Avoine was the President of the Rationalist Association of India (RAI) from 1938 to\n1944. It is a full voting member of International Humanist and Ethical Union (IHEU), a global representative body of the humanist movement, uniting a diversity of non-religious organisations and individuals.\n\nIndia’s first blasphemy protection happened in 1933 when Dr. D’ Avoine published an article entitled, “Religion and Morality” in the September 1933 issue of “Reason”, the official journal of RAI at that period. The Bombay Police confiscated all the copies of “Reason” and arrested Dr. D’Avoine and charged him under IPC 295A. On 5th March 1934 Sir H.P. Dastur, the Chief Presidency Magistrate of Bombay, dismissed the case by saying that “The accused may be wrong...But the article merely represents the writer’s view.” \n\n\n\n"}
{"id": "44189", "url": "https://en.wikipedia.org/wiki?curid=44189", "title": "Reciprocal altruism", "text": "Reciprocal altruism\n\nIn evolutionary biology, reciprocal altruism is a behaviour whereby an organism acts in a manner that temporarily reduces its fitness while increasing another organism's fitness, with the expectation that the other organism will act in a similar manner at a later time. The concept was initially developed by Robert Trivers to explain the evolution of cooperation as instances of mutually altruistic acts. The concept is close to the strategy of \"tit for tat\" used in game theory.\n\nThe concept of \"reciprocal altruism\", as introduced by Trivers, suggests that altruism, defined as an act of helping another individual while incurring some cost for this act, could have evolved since it might be beneficial to incur this cost if there is a chance of being in a reverse situation where the individual who was helped before may perform an altruistic act towards the individual who helped them initially. This concept finds its roots in the work of W.D. Hamilton, who developed mathematical models for predicting the likelihood of an altruistic act to be performed on behalf of one's kin.\n\nPutting this into the form of a strategy in a repeated prisoner’s dilemma would mean to cooperate unconditionally in the first period and behave cooperatively (altruistically) as long as the other agent does as well. If chances of meeting another reciprocal altruist are high enough, or if the game is repeated for a long enough amount of time, this form of altruism can evolve within a population.\n\nThis is close to the notion of \"tit for tat\" introduced by Anatol Rapoport, although there still seems a slight distinction in that \"tit for tat\" cooperates in the first period and from thereon always replicates an opponent’s previous action, whereas “reciprocal altruists” stop cooperation in the first instance of non-cooperation by an opponent and stay non-cooperative from thereon. This distinction leads to the fact that in contrast to reciprocal altruism, tit for tat may be able to restore cooperation under certain conditions despite cooperation having broken down.\n\nChristopher Stephens shows a set of necessary and jointly sufficient conditions “… for an instance of reciprocal altruism:\n\n\nThere are two additional conditions necessary \"…for reciprocal altruism to evolve:\"\n\n\nThe first two conditions are necessary for altruism as such, while the third is distinguishing reciprocal altruism from simple mutualism and the fourth makes the interaction reciprocal.\nCondition number five is required as otherwise non-altruists may always exploit altruistic behaviour without any consequences and therefore evolution of reciprocal altruism would not be possible. However, it is pointed out that this “conditioning device” does not need to be conscious. Condition number six is required to avoid cooperation breakdown through backwards induction—a possibility suggested by game theoretical models.\n\nThe following examples could be understood as altruism. \nHowever, showing reciprocal altruism in an unambiguous way requires more evidence as will be shown later.\n\nAn example of reciprocal altruism is cleaning symbiosis, such as between cleaner fish and their hosts, though cleaners include shrimps and birds, and clients include fish, turtles, octopuses and mammals. Aside from the apparent symbiosis of the cleaner and the host during actual cleaning, which cannot be interpreted as altruism, the host displays additional behaviour that meets the criteria for altruism:\n\nThe host fish allows the cleaner fish free entrance and exit and does not eat the cleaner, even after the cleaning is done.\nThe host signals the cleaner it is about to depart the cleaner's locality, even when the cleaner is not in its body. The host sometimes chases off possible dangers to the cleaner.\n\nThe following evidence supports the hypothesis:\n\nThe cleaning by cleaners is essential for the host. In the absence of cleaners the hosts leave the locality or suffer from injuries inflicted by ecto-parasites. There is difficulty and danger in finding a cleaner. Hosts leave their element to get cleaned. Others wait no longer than 30 seconds before searching for cleaners elsewhere.\n\nA key requirement for the establishment of reciprocal altruism is that the same two individuals must interact repeatedly, as otherwise the best strategy for the host would be to eat the cleaner as soon as cleaning was complete. This constraint imposes both a spatial and a temporal condition on the cleaner and on its host. Both individuals must remain in the same physical location, and both must have a long enough lifespan, to enable multiple interactions. There is reliable evidence that individual cleaners and hosts do indeed interact repeatedly.\n\nThis example meets some, but not all, of the criteria described in Trivers’s model. In the cleaner-host system the benefit to the cleaner is always immediate. However, the evolution of reciprocal altruism is contingent on opportunities for future rewards through repeated interactions. In one study, nearby host fish observed \"cheater\" cleaners and subsequently avoided them. In these examples, true reciprocity is difficult to demonstrate since failure means the death of the cleaner. However, if Randall’s claim that hosts sometimes chase off possible dangers to the cleaner is correct, an experiment might be constructed in which reciprocity could be demonstrated.\n\nWarning calls, although exposing a bird and putting it in danger, are frequently given by birds. An explanation in terms of altruistic behaviors given by Trivers:\n\nIt has been shown that predators learn specific localities and specialize individually on prey types and hunting techniques.\nIt is therefore disadvantageous for a bird to have a predator eat a conspecific, because the experienced predator may then be more likely to eat him. Alarming another bird by giving a warning call tends to prevent predators from specializing on the caller’s species and locality. In this way, birds in areas in which warning calls are given will be at a selective advantage relative to birds in areas free from warning calls.\n\nNevertheless, this presentation lacks important elements of reciprocity. It is very hard to detect and ostracize cheaters. There is no evidence that a bird refrains from giving calls when another bird is not reciprocating, nor evidence that individuals interact repeatedly. Given the aforementioned characteristics of bird calling, a continuous bird emigration and immigration environment (true of many avian species) is most likely to be partial to cheaters, since selection against the selfish gene is unlikely.\n\nAnother explanation for warning calls is that these are not warning calls at all:\nA bird, once it has detected a bird of prey, calls to signal to the bird of prey that it was detected, and that there is no use trying to attack the calling bird. Two facts support this hypothesis:\n\n\nRed-winged blackbird males help defend neighbor's nests. There are many theories as to why males behave this way. One is that males only defend other nests which contain their extra-pair offspring. Extra-pair offspring is juveniles which may contain some of the male bird's DNA. Another is the tit-for-tat strategy of reciprocal altruism. A third theory is, males help only other closely related males. A study done by The Department of Fisheries and Wildlife provided evidence that males used a tit-for-tat strategy. The Department of Fisheries and Wildlife tested many different nests by placing stuffed crows by nests, and then observing behavior of neighboring males. The behaviors they looked for included the number of calls, dives, and strikes. After analyzing the results, there was not significance evidence for kin selection; the presence of extra-pair offspring did not affect the probability of help in nest defense. However, males reduced the amount of defense given to neighbors when neighbor males reduced defense for their nests. This demonstrates a tit-for-tat strategy, where animals help those who previously helped them. This strategy is one type of reciprocal altruism.\n\nVampire bats also display reciprocal altruism, as described by Wilkinson.\nThe bats feed each other by regurgitating blood. Since bats only feed on blood and will die after just 70 hours of not eating, this food sharing is a great benefit to the receiver and a great cost to the giver.\nTo qualify for reciprocal altruism, the benefit to the receiver would have to be larger than the cost to the donor. This seems to hold as these bats usually die if they do not find a blood meal two nights in a row. Also, the requirement that individuals who have behaved altruistically in the past are helped by others in the future is confirmed by the data. However, the consistency of the reciprocal behaviour, namely that a previously non-altruistic bat is refused help when it requires it, has not been demonstrated. Therefore, the bats do not seem to qualify yet as an unequivocal example of reciprocal altruism.\n\nGrooming in primates meets the conditions for reciprocal altruism according to some studies. One of the studies in vervet monkeys shows that among unrelated individuals, grooming induce higher chance of attending to each other's calls for aid. However, vervet monkeys also display grooming behaviors within group members, displaying alliances. This would demonstrate vervet monkey's grooming behavior as a part of kin selection since the activity is done between siblings in this study. Moreover, following the criteria by Stephen, if the study is to be an example of reciprocal altruism, it must prove the mechanism for detecting cheaters.\n\nIn comparison to that of other animals, the human altruistic system is a sensitive and unstable one. Therefore, the tendency to give, to cheat, and the response to other’s acts of giving and cheating must be regulated by a complex psychology in each individual, social structures, and cultural traditions. Individuals differ in the degree of these tendencies and responses.\nAccording to Trivers, the following emotional dispositions and their evolution can be understood in terms of regulation of altruism.\n\n\nIt is not known how individuals pick partners as there has been little research on choice. Modeling indicates that altruism about partner choices is unlikely to evolve, as costs and benefits between multiple individuals are variable. Therefore, the time or frequency of reciprocal actions contributes more to an individual's choice of partner than the reciprocal act itself.\n"}
{"id": "53950341", "url": "https://en.wikipedia.org/wiki?curid=53950341", "title": "Reciprocal altruism in humans", "text": "Reciprocal altruism in humans\n\nReciprocal altruism in humans refers to an individual behavior that gives benefit conditionally upon receiving a returned benefit, which draws on the economic concept – ″gains in trade″. Human reciprocal altruism would include the following behaviors (but is not limited to): helping patients, the wounded, and the others when they are in crisis; sharing food, implement, knowledge.\n\nThe concept of ″altruism″ was firstly coined by the French philosopher Auguete Comte in the 19th century, which was derived from the French word ″\"altruisme\"″. Comte believed that ″altruism″ is a moral doctrine, which is the opposite of egoism, emphasizing the noble morality of sacrificing themselves and benefiting others. Human beings have both selfish and altruistic motivations, and altruism is used to restrain the egoistical instinct. Comte’s altruism describes the nature of human and ethical significances, but it’s completely different from the altruism in biological sciences. In evolutionary biology, altruism is an individual behavior that benefits another individual’s fitness but reduces their own fitness in population The concept of ″altruism″ in biology arose from the debate of ″the Problem of Altruism″ in Natural Selection. Charles Darwin suggested that animals behave in the ways that can increase their survival and reproductive chances while competing with others. However, altruistic behavior – the act of helping others even if it accompanies with a personal cost – is common in the animal kingdom, like the Vampire Bat and pariousrprimates. Therefore, Charles Darwin regarded ″the Problem of Altruism″ as a potential fatal challenge to his concept of natural selection. In ″The Descent of Man″, Darwin (1859) wrote:\n\nIn 1964, William Hamilton developed mathematical model and put forward to his theory – ″Kin selection″ theory or ″Inclusive fitness″ theory. ″Kin selection″ theory or ″Inclusive fitness″ theory reveals that an altruistic gene evolved by natural selection. The gene can be only shared with relatives, and reduces the individuals own fitness but boots the fitness of their relatives and off-springs. In this way, this behavior increases the proportion of altruistic gene in population. Hamilton’s rule provides mathematical inequality to state that an altruistic gene spread by natural selection only if the following condition can be satisfied: r B > C, where C is the cost to the individual performing the altruistic act, B is the benefit gained by the recipients of the altruistic act, r is the genetic relatedness between individual and recipients.\n\nHamilton's ″Kin selection″ theory or ″Inclusive fitness″ theory expands the Darwinian definition of ″fitness″ and continues the same Darwinian framework that allows the spread of not only selfish genes but also altruistic genes. Nevertheless, Hamilton’s theory did not support an appropriate explanation with unrelated members of other species. In order to solve this problem, Robert Trivers developed the original theory of reciprocal altruism into an attempt to explain the altruism behaviors among unrelated organisms. The idea of reciprocal altruism is straightforward: an altruistic behavior is probably selected only if a return would be obtained in the future. It is similar to the Tit-for-Tat strategy from the game theory explained later in the paper.\n\nThe theory of reciprocal altruism in humanity, based on the biological characteristics of human beings and the realistic society, explicates the interdependence and cooperation between people, as well as its rationality. It also demonstrates the original motivations and the internal mechanisms of the human cooperation, revealing the inevitability and social significance ranging from kin altruism to un-relative altruism in the human population. As a result, the subjective guess and emotion of human cooperation can be refined to a theory, and is gradually become one of the most popular explanations to a variety of social behaviors. In addition, cooperation is the most deep-seated foundation for the formation and existence of human society. Therefore, the proposition of reciprocal altruism is undoubtedly a great theoretical advance in human cognition history.\n\nHuman reciprocal altruism seems to be a huge magnetic field to interweave different disciplines closely. New exploration has been made by these disciplines at different levels from different points. Generally, the core of Human reciprocal altruism is located in the puzzle: How to overcome short-term self-interest and achieve cooperation. Ultimately, it reveals that altruistic individuals are more competitive than selfish individuals, on either improving individual fitness or resolving conflicts. The compatibility and complementarity of different theoretical perspectives lay the basis of human reciprocal altruism, and help with exploring human different viewpoints of human reciprocal altruism. The debate of human reciprocal altruism has been the focus of biology, sociology, economics and culture.\n\nIn 1902, Peter Kropotkin published his monograph – \"Mutual Aid: A Factor of Evolution\", and demonstrates the survival mechanisms of cooperation, based on various examplesm of animal and human societies. He attempted to reveal that the law of biological evolution is mutual aid rather than survival competition. Mutual aid and cooperation are the principles of all species’ biological evolution including human beings’, and the concepts resulting in a profound influence upon biological evolution. Wilson applied the term of ″sociobiology″ as an attempt to explain social behavior of insect and thus explored the evolutionary mechanism of other animals including human such as the social behavior, altruism. He argued that human altruistic behavior, as one of the human nature characteristics, is the result of the genetic inheritance.\n\nIn 1971, Trivers published one of the most important biological articles of the 20th century - \"The Evolution of Reciprocal Altruism\" and introduced the term of ″reciprocal altruism″ to explain the evolution of cooperation. The idea of reciprocal altruism is almost the same as kin selection, except to put emphasis on reciprocation instead of the need for genetic relatedness. It described that an altruistic trait or behavior may be selected, because the recipient will very likely to return benefits to the altruistic sender. If the reproductive benefit that the altruistic sender receives in return is larger than the cost initially incurred by the altruistic action, individuals who engage in this kind of reciprocal altruism will outbreed those who do not. Therefore, a seemingly altruistic trait can spread in a population. The reciprocation can be delayed as long as the individuals stay in the trade, and, so to speak, have sufficient long-term memory. This explains many features of human social life, for example, we do a favor for someone with the expectation that the favor would be remembered, and lead to a return in the future.\n\nAs Trivers supported the foundation for reciprocal altruism, Axelrod and Hamilton applied the Game Theory to study the mechanism of reciprocal altruism, and attempted to answer the key question: How altruism spreads when cheating is an all-win strategy used by members of the population. In this paper, Axelrod and Hamilton revealed that reciprocating the assistance from another individual is stable in evolution as long as there are enough altruists in the population. They also demonstrate that a population of altruists can initially emerge through inclusive fitness, helping and receiving help among genetically related individuals. Subsequent work indicates that only a few altruists are initially needed for the emergence of reciprocation in a heterogeneous population.\n\nSome evolutionary biologists, like Richard Dawkins, wholly endorse Axelrod and Hamilton’s work in individual selection. In describing genes as being selfish, Dawkins states that the organisms act altruistically against their individual interests in order to help copies of themselves in other bodies to replicate. Essentially, reciprocal altruism is the individual behavior behind selfish motivations. The bird is a prime example in the narrative of Dawkins: those altruistic birds who sacrifice their own interests by reproducing late or less during hard times would not have been able to pass their altruistic genes to the future generations, which will be dominated by the selfish genes from birds who take advantage of the situation by using up others’ food supply to reproduce their own offspring. Nevertheless, some scholars such as K. Lorenz and W. Edwards have been strongly opposed to the individual selection. In contrast, they spearhead the campaign of group selection.\n\nThe debate whether individuals/gene or group/species are the basic level/unit of selection has occurred since the 1960s. The major idea of group selection is that individuals may sacrifice their own reproductive interests for the benefit of the survival of the group to which they belong. W. Edwards builds this argument mainly on birth related behaviors of birds. He points out that many bird species with small clutches have prolonged periods before reaching reproductive maturity, and have long breeding seasons sometimes in excess of one year. Other group selection supporters also argue that these behaviors must be social and altruistic in that, for example, when food supply is abundant, clutches are bigger than when food supply is more scarce. Birds thus can regulate their population densities below starvation levels through social conventions. All of these characteristics run contrary to the idea that fitness is defined by individual organisms attempting to selfishly reproduce. However, it soon became clear that group selection was losing the battle. In 1966, George Williams published the influential \"Adaptation and Natural\" \"Selection: a critique of some current evolutionary thought\". By the end of the 1960s, a Neo-Darwinian interpretation of the modern synthesis had taken hold and it has become almost a gold standard that the unit of evolutionary analysis is at the individual's and the gene’s level. Dawkins, Hamilton, and Trivers represent the mainstream view of individual selection.\n\nSome scholars, such as Michael Taylor, Anatol Rapoport, Robert Keohane, Arthur Stein, Helen Milner and Kennth Oye, point out that reciprocal altruism widely spread in international relations and human society, and international reciprocity is the foundation of the international community. States act in the confidence that their cooperative actions will be repaid in the long term instead of seeking for the immediate benefit, so reciprocal altruism can be seen as generally accepted standards in international relations. On a personal scale, some scholars believe that reciprocal altruism derives from the subjective feeling of individuals and compliance with social rules. Smith put forward an alternative based on the idea of sympathy and indicates that altruistic behavior is the product of the measure of gains and losses, emphasizing that people are easy to compare with others when measuring the gains and losses. Due to this, the subjective sense of fairness exerts an effect on people's altruistic behavior. For humans, social norms can be argued to reduce individual level variation and competition, thus shifting selection to the group level, so human behavior should be consistent with social norms. Altruistic behavior is the result of learning and internalizing these social norms by individuals.\n\nThe economic model of reciprocal altruism includes direct reciprocity and indirect reciprocity. Direct reciprocity is an immediate collaborative exchange that benefits everyone. Direct reciprocity was introduced by Robert Trivers ) as a mechanism for the evolution of cooperation. The direct reciprocal is typically one-for-one: I incur the cost today to benefit you, you incur the cost at some point later on to benefit me. There is little negotiation and the exchange is simple. The strategy of ″the prisoner dilemma″ is the common direct reciprocity.\n\nThe Prisoners Dilemma requires that:\n\nand that:\n\nThere is immediate and obvious benefit from direct reciprocity and relatively little need for trust into the future. Cheating is a critical question and may happen at some time.\n\nIn the context of Indirect reciprocity, two players are randomly selected in a population, one is donor while the other is recipient. Each player can play many times, but never with the same partner twice. Thus it is impossible that a cheat is held to account by the victim. Obviously, trigger strategies can ensure a cooperative Nash equilibrium. If all players use these strategies, no player would have deviated. In many situations, cooperation is favored and it even benefits an individual to forgive a defection but cooperative societies are always unstable because mutants which are inclined to defect may lose any balance. In addition, Indirect reciprocity typifies two forms: ″Upstream reciprocity″ and ″Downstream reciprocity″. In the description of \"Figure1. Direct and indirect reciprocity\", Nowak and Sigmund provided the explicit identifications of ″Upstream reciprocity″ and ″Downstream reciprocity″:\n\nUpstream reciprocity is based on a recent positive experience. A person who has been at the receiving end of a donation may feel motivated to donate in turn. Individual B, who has just received help from A, goes on to help C. ‘Downstream reciprocity’ is built on reputation. Individual A has helped B and therefore receives help from C. Mathematical investigations of indirect reciprocity have shown that natural selection can favor strategies that help others based on their reputation. Upstream reciprocity is harder to understand but is observed in economic experiments. In both cases, the decision to help can be interpreted as a misdirected act of gratitude. In one case recipients are thanked for what another did; in the other case they are thanked by someone who did not profit by what they did\".(p. 1292.)\n\nUtility function is an important concept in economy that measures preferences over a set of goods and services using mathematics. In general, a utility function U (X, Y) will represent a consumer’s preferences for different goods if the following condition holds:\n\nEconomists, such as Gary S. Becker, use the theory of utility function and establish the hypothesis of altruism model. Becker argues that the donor's utility function includes the utility of potential recipients. That is, the donor would donate a resource if the vicarious enjoyment of watching the pleasure of others exceeds at the margin the donor's satisfaction from consuming the resource himself. He indicates that all human behaviors are the maximization of different utility functions, and attempts to establish all human behaviors on the basis of generalized utility theory with resource constraints. He also puts the human irrational behavior into the framework of this analysis, emphasizing that human altruistic behavior can be defined by the generalized utility function appropriately.\n\nGame theory, especially the prisoner’s dilemma, is originally introduced by Trivers to explain the mechanism of reciprocal altruism. Unlike Hamilton’s inclusive fitness where the selection of an altruistic allele is ″secured″ by the extent of genetic relatedness between the donor and recipient, reciprocation is no guarantee and, in fact, cheating or not reciprocating is evolutionarily stable because cheaters are doubly rewarded reproductively. That is, they receive a benefit from helpers and, at the same time, helpers bear a cost for the cheaters, although the cost is smaller than the benefit.\n\nThe relationship between donor and recipient in reciprocal altruism is exactly analogous to the situation of the prisoner’s dilemma. In Triver’s narrative, the prisoner’s dilemma can be characterized by the following payoff matrix:\nWhere A and A are the individuals’ altruistic acts and C, C are the cheating acts, and where T represents the temptation to defect, R represents the reward for mutual cooperation and S stands for being a sucker.\n\nAnd the following inequality must hold for reciprocal altruism:\n\nThe Prisoner's dilemma becomes the classical example to understand the reciprocal altruism. Combining the theory of biological evolution with classical game theory, Maynard Smith and George. R. Price explained how selfish individuals can achieve cooperation and develop the basic equilibrium concept in evolutionary - Evolutionarily Stable Strategy (ESS). Evolutionarily Stable Strategy is a strategy and adopted by a population in a given environment. An ESS is an equilibrium refinement of the Nash equilibrium that once it is fixed in a population, natural selection alone is sufficient to prevent alternative strategies from invading successfully. Simultaneously, the collaboration between Axelrod and Hamilton is also significant for applying game theory to evolutionary issues. Their paper and the book \"Evolution and the theory\" \"of games\", written by John Maynard, illustrate that the process of natural selection can be mathematically modeled using game theory. In essence, natural selection entails differential replication of genes. That is, different traits and attributes are selected for or against because of the different effects they have on their ″own″ genetic reproduction or replication. The differential replication process, independent of its underlying nature being biological, physiological, or psychological, can be approximated by game theory. Different game-theoretical strategies have imbedded probabilistic functions that result in their winning or losing a game, similar to the selecting - for or selecting - against of genes.\n\nThe strategy of game theory discussed in Axelrod and Hamilton's paper is called Tit for Tat. Tit for Tat represents such a strategy that, in a repeated game, the player starts by not telling on the other player or cooperates and subsequently reciprocates the same action undertaken by the other player. That is, the player cooperates if the other player cooperates and defects if the other player defects. As a game theoretical strategy, Tit for Tat essentially entails the same concept as Trivers' reciprocal altruism. Unlike Trivers' original publication which provides conceptual explanations and examples, Axelrod and Hamilton's paper provides more rigorous mathematical proofs of the viability or ESS of reciprocal altruism.\n\nReciprocal altruism has since become one of the major theoretical foundations of evolutionary psychology and game theory, and the repeated prisoner's dilemma game has also become popular tools by which to derive and test evolutionary psychological concepts. Social interactions similar in form to Tit for Tat are prevalent in our daily lives. When someone does you a favor, you feel you owe that person. If you fail to return the favor, you are likely to feel guilty and may try to find ways to overcompensate for your friend. Generally, when you do favors for another person, you are also likely to expect something in return. When that person fails to reciprocate, you may feel cheated and may seek revenge by not being generous to the person in the future. On the other hand, when someone gives you a large gift for which you did not do anything, you may feel uncomfortable and even resentful because you do not want to owe the person a debt. These behaviors and emotions that have clearly been selected for are consistent with the Tit for Tat game theoretical strategy and reciprocal altruism.\n\nD. S. Wilson and E. O. Wilson stated that the speed and function of \"gene evolution\" in human society is far less than that of \"cultural evolution\", but these two elements interact, thus they achieve the evolution of human altruism. To further illustrate this mechanism, Dawkins proposed the concept of ″Meme″ in his book \"The selfish gene.\" ″Meme″ refers to ″an idea, behavior, or style that spreads from person to person within a culture″. It is seen as the basic unit of culture that can be transmitted from one mind to another. Susan Blackmore is one of the scholars that has made contributions to the theory of ″Meme″. Blackmore insisted that memes are the medium of the spread of altruism. The transmission of altruism has been carried out through memes. Memes are true evolutionary replicators like genetics and undergo evolutionary change.\n"}
{"id": "48630374", "url": "https://en.wikipedia.org/wiki?curid=48630374", "title": "Regressive left", "text": "Regressive left\n\n\"Regressive left\" (also formulated as \"regressive liberals\" and \"regressive leftists\") is a neologism and political epithet, used as a pejorative to describe a section of left-wing politics who are accused of holding paradoxical, reactionary views by their tolerance of illiberal principles and ideologies, particularly identity politics (emphasis on group identities like race and gender, rather than on the individual), and opposition to free speech for the sake of multiculturalism and cultural relativism.\n\nBritish political activist Maajid Nawaz, American political talk-show hosts such as Bill Maher and Dave Rubin, as well as New Atheist writers like Sam Harris and Richard Dawkins are among those who have used the term.\n\nIn 2007, Maajid Nawaz renounced his previous association with the radical Islamist group Hizb ut-Tahrir in favor of secular Islam. He is co-founder and chairman of Quilliam, a counter-extremism think tank based in London that seeks to challenge Islamist ideology.\n\nNawaz has used the phrase \"regressive left\" to describe left-leaning people who—in his opinion—pander to Islamism, which he defines as a \"global totalitarian theo-political project\" with a \"desire to impose any given interpretation of Islam over society as law\" and which he opposes on the ground that \"any desire to impose any version of Islam over anyone anywhere, ever, is a fundamental violation of our basic civil liberties\". According to Nawaz, such sympathizers of Islamism include \"atheists who are on the side of the Islamists, defending Islamism in the name of cultural tolerance\".\n\nIn an October 2015 interview with political talk show host Dave Rubin, Nawaz elucidated further the reasoning behind his choice of the word \"regressive\" and hypothesized that a section of people on the left \"genuinely believe\" that they are fighting an \"ideological war\" against neoconservative and neocolonialist foreign policies of Western governments which promote state-organized violence and chaos in the form of wars and military invasions. In contrast, he claims that such leftists forgo their duty to denounce the violent acts of theocratic extremists such as Islamists, at times going so far as to \"make alliances\" with some of the most regressive, theocratic and murderous regimes and organizations. He cited Jeremy Corbyn, leader of the British Labour Party, as an example of someone who \"has been historically very close\" to supporters of Islamist organisations like Hamas and Hezbollah. In Nawaz's opinion, it is possible to denounce both neoconservative foreign policies such as the Iraq War (which he had opposed) and theocratic extremism, but those that he labels \"regressive leftists\" fail to do so he says.\n\nAccording to Nawaz, the notion that Muslims cannot cope with criticism or mockery of Islam and only react violently is \"patronizing, self-pity inspiring mollycoddling\" of the very Muslims it claims to serve and emancipate, because it does not expect them to be civil and control their anger. This \"racism of low expectations\" lowers the moral standards of people within minorities, seeking excuses if they express misogyny, homophobia, chauvinism, bigotry or antisemitism, whilst holding members of the majority to \"universal liberal standards\".\n\nHaras Rafiq, managing director of Quilliam, expressed the view that there is a tendency of some on the left to excuse Islamism: \"We have not got to grips with the symbiotic relationship between Islamism and far-right hatred, and the regressive left that is prepared to excuse Islamism\".\n\nIn September 2015, Sam Harris and Maajid Nawaz participated in a public forum hosted by Harvard University's Institute of Politics, which was later published in a short book, titled \"Islam and the Future of Tolerance\" (2015). In a review of the book in the magazine \"National Review Online\", political writer Brian Stewart noted that according to both Nawaz and Harris \"regressive leftists\" in the West are \"willfully blind\" to the fact that jihadists and Islamists make up a significant portion (20% in Harris's estimate) of the global Muslim community and the minority Muslim communities within the West, even though these factions are opposed to liberal values such as individual autonomy, freedom of expression, democracy, women's rights, gay rights, etc. Nawaz and Harris have denounced the paradoxically illiberal, isolationist and censuring attitude towards any criticism of this phenomenon, which they contend betrays universal liberal values and also abandons supporting and defending the most vulnerable liberal members living within the Muslim community such as women, homosexuals and apostates.\n\nIn October 2015, \"The Washington Times\" reported that American comedian and show host Bill Maher and British biologist and New Atheist author Richard Dawkins \"lamented regressive leftists who fail to understand they are anything but liberal when it comes to Islam\". Maher noted a willingness to criticise anything except Islam, excusing it as \"their culture\", to which Dawkins responded: \"Well, to hell with their culture\". Making reference to student initiatives to disinvite ex-Muslim speakers on campus, Dawkins saw this as \"a betrayal of the Free Speech Movement of the 1960s\".\n\nIn October and November 2015, Sam Harris frequently used the term in his exchanges with the media, saying the greatest danger is that the \"regressive left\" is willing to give up freedom of speech \"out of fear of offending minorities\", which will lead to censorship imposed by those minorities, citing American journalist Glenn Greenwald's comments on the Charlie Hebdo shooting as an example. Harris considers Reza Aslan and Noam Chomsky to be of the regressive left.\n\nIn November 2015 in an appearance on the talk radio show \"The Humanist Hour\", author and philosopher Peter Boghossian defined the term as a pejorative used to describe those on the left that have made the \"strangest bedfellows\" with the Islamists. According to him, the word \"regressive\" is used to contrast with the word \"progressive\" – the latter being the group that is egalitarian and wants to create systems of justice and racial equality, while the former being a group that \"[looks] for the worst in people... and [does] not extend hermeneutics of charity, or a charitable interpretation of anything anyone says, but uses it as a hammer to beat people down\". In addition, Boghossian believes that \"regressive leftists\" have become \"hyper-moralists\" and champions of their perceived victims. He cites the historical wrongdoings such as slavery in the United States and colonialism as a legitimate concern that has caused mistrust of anything Western and capitalistic. He also added that \"there are people who have suffered and still suffer legitimate instances of racism, homophobia etc. The problem is that every time the word racist is just thrown around like that, that word loses its meaning. And it should have quite a sting. That should be a horrible word\".\n\nIn late 2015, talk show host Dave Rubin hosted discussions about the \"regressive left\" in several \"The Rubin Report\" segments. Rubin describes the regressive left as \"the left's version of the Tea Party\", saying that the regressive left will damage the Democratic Party in a similar way the Tea Party damages the Republican Party.\n\nPolitical commentator David Pakman supported the concept in his talk show, saying \"there are liberals who do use cultural relativism and distaste for US foreign policy as an excuse to defend or at least minimize violence and injustice that they would certainly otherwise oppose\". Pakman has distanced himself from the term, saying that it is misused by conservatives to insult all liberals. Pakman suggests that the actual regressive leftists are leftists who use authoritarianism to enforce progressivism.\n\nIn November 2015, psychiatrist Khwaja Khusro Tariq from \"The Huffington Post\" classified the term as an unsubstantiated \"ad hominem\" attack, stating that the harshest critics of Islam are courted by both liberal and conservative media in the United States. Khusro also stated the term has been directed towards Glenn Greenwald and Noam Chomsky, both of whom he said have never condoned violence or opined on the doctrine of Islam. He argued that there was no genuine inhibition on speaking against the religion.\n\nIn March 2016, Joseph Bernstein, a \"BuzzFeed\" reporter on web culture, wrote that according to Google Trends interest in the term \"shot up\" in late 2015. According to Bernstein, instead of criticising \"cultural tolerance gone too far\", the phrase has \"become a catch-all for any element of the dominant new media culture that the anti-SJW internet doesn't like\". He also suggests that even though the term can be sourced back to self-described liberal commentators like Nawaz, Maher and Dawkins, it is currently heavily used by the alt-right and other Anti-SJW Groups on Internet forums and social media as part of their rhetorical warfare.\n"}
{"id": "3083164", "url": "https://en.wikipedia.org/wiki?curid=3083164", "title": "Second-class citizen", "text": "Second-class citizen\n\nA second-class citizen is a person who is systematically discriminated against within a state or other political jurisdiction, despite their nominal status as a citizen or legal resident there. While not necessarily slaves, outlaws or criminals, second-class citizens have limited legal rights, civil rights and socioeconomic opportunities, and are often subject to mistreatment or neglect at the hands of their putative superiors. However, they are different from \"less-than-whole citizens\", as second-class citizens are often disregarded by the law or have it used to harass them (see police misconduct and racial profiling). Systems with \"de facto\" second-class citizenry are generally regarded as violating human rights.\n\nTypical conditions facing second-class citizens include but are not limited to: \n\nThe category is normally unofficial and mostly academic, and the term itself is generally used as a pejorative and governments will typically deny the existence of a second class within the polity. As an informal category, second-class citizenship is not objectively measured; however, cases such as the American South under segregation, aborigines in Australia prior to 1967, apartheid in South Africa, women in Saudi Arabia under Saudi law, Dalits in India and Nepal, and Roman Catholics in Northern Ireland during the parliamentary era are all examples of groups that have been historically described as having second-class citizenry. Historically, before the mid-20th century, this policy was applied by some European Colonial Empires on colonial residents of overseas holdings. \n\nA resident alien or foreign national, and children in general, fit most definitions of second-class citizen. This does not mean that they do not have any legal protections, nor do they lack acceptance by the local population. A naturalized citizen carries essentially the same rights and responsibilities as any other citizen (a possible exception being ineligibility for certain public offices), and is also legally protected.\n\n\n"}
{"id": "13804392", "url": "https://en.wikipedia.org/wiki?curid=13804392", "title": "Self-executing right", "text": "Self-executing right\n\nSelf-executing rights in international human rights law are rights that are formulated in such a way that one can deduce that it was the purpose to create international laws that citizens can invoke directly in their national courts. Self-executing rights, or directly applicable rights, are rights which, from the viewpoint of international law, do not require transformation into national law. They are binding as such and national judges can apply them as such, as if they were national rules. From the viewpoint of national law, it may be required that all international law be incorporated into national law before becoming valid. This depends on the national legal tradition.\n\nIn order to decide whether a rule is self-executing or not, one must only look at the rule in question. National traditions do not count. A rule that says that states should guarantee freedom of expression to its citizens is self-executing. A rule that says that states should take all the necessary measures to create enough employment is not. Non-self-executing rules of international law only impose the obligation on states to take measures and to create or alter legislation. Citizens or national judges cannot invoke these rules (and demand employment as in the previous example) in a national court. This means that international law that is not self-executing must be transformed into national law in order to take effect.\n\nThe priority of international law remains a fact whether this law is or is not self-executing. A state cannot invoke its national law as a reason not to respect its international obligations. In case of non-self-executing rules, it is obliged to change its national law or to take certain measures. It violates international law if it does not do so. In this case, a national judge can only decide that their state should modify national law or take certain measures. They cannot invalidate national law that contradicts non-self-executing international law. They can only declare national law null and void if it contradicts self-executing international rights.\n\nMost human rights contained in the main human rights treaties are self-executing and can be invoked by individuals in a national courtroom, although this is the case more for civil rights than for economic and social rights.\n"}
{"id": "34114455", "url": "https://en.wikipedia.org/wiki?curid=34114455", "title": "Self-validating reduction", "text": "Self-validating reduction\n\nA self-validating reduction is kind of self-fulfilling prophecy of which the result is a dramatic reduction in a person, group, or natural being. This term was coined by Anthony Weston and used in his book \"Back to Earth\" in 1994. Following Weston's work, Bob Jickling, et al. in \"Environmental Education, Ethics, and Action\" (United Nations Education Program (UNEP), 2006) wrote: \n\nAccording to Thomas Schelling’s classic (1978) treatment, the term “self-fulfilling prophecy” originally referred to a process in which a negative but quite possibly baseless expectation or prediction generates a feedback loop that ends by producing exactly the expected negative result: the diminution or defeat of something originally stable and seemingly solid. The standard example is a run on an initially solvent bank that can accelerate into a panic and drive the bank into bankruptcy simply because very few banks, however well-managed, can cash out large numbers of depositors upon unexpected demand. The term has since broadened greatly, however, to include any self-augmenting expectation, prediction, or disposition, many of which can also have positive effects. High self-esteem, however ill-founded, may produce more confident and capable acts, hence \"fulfilling\" itself. Other interpersonal expectations such as predictions of student success, initially quite baseless, can produce student success through subtly altering the expectations of their teachers.\n\nA 1996 article by philosopher Anthony Weston refocused on part of what Schelling described as the original denotation of “self-fulfilling prophecy” using the more specific term “self-validating reduction”. The \"reduction\" is an actual change – some kind of deprivation, loss, or diminution – not merely a \"prophecy\" in the sense of an attitude change in another person or society at large; and a change that comes to justify or \"validate\" a changed judgment of the person changed or reduced, specifically by ignoring the actual cause of the reduction and attributing it instead to the essential character of the reduced person(s) instead. \n\nFor example, the abolitionist Frederick Douglass pointed out that\n\nAmong many other possible examples are the Nazi concentration camps, designed to destroy the inmates' very humanity and thus to validate the Nazis’ prejudices and make systematic murder possible; and the ways in which prejudiced persons, convinced of (say) a co-worker's incapacity or inequality, are likely to reduce their co-workers to uncooperative antagonists: missing their own contribution to the process, they will then naturally conclude that their co-workers – not themselves – are incapable of good work.\n\nSelf-validating reduction applies beyond the human sphere as well. For example, wild animals who were not originally hostile or wary become that way upon being hunted. Scientific research on other animals that tends to avoid calling on their social instincts, in the name of objectivity, may in fact drive them away and actually induce an avoidance of humans that then is taken to be an objective and independent fact about them. Highly confined and managed animals under \"factory farming\" can be reduced to a state of social and physical dysfunction that seems to be their \"inherent character\", as Douglass puts it, thus making their treatment less morally troubling, and thereby making them more available for further reduction in turn.\n\nThe ecological critic Paul Shepard noted that land can be reduced in self-validating ways as well, as when Isaiah commands Israel to “tear down the [pagan] altars, break their images, cut down their groves” -– so that it \"becomes\" true, as it may not have been initially, that only the other world, beyond nature, is truly sacred. Shepard called this kind of self-validating reduction “the evangelical desacralizing of place”.\n\nSelf-fulfilling prophecy\n\n"}
{"id": "43711007", "url": "https://en.wikipedia.org/wiki?curid=43711007", "title": "Sterilization of Native American women", "text": "Sterilization of Native American women\n\nForced Sterilization of Indigenous women is an issue affecting both Native American and First Nations women. Indigenous women have been sterilized without informed consent, or coerced into consenting to medical procedures; this has been done when they were unconscious from sedation, or during the stress of childbirth.\n\nIn the late 1960s, the Bureau of Indian Affairs (BIA), through the federally-funded Indian Health Service (IHS), covertly imposed a policy of involuntary surgical sterilization upon Native American women in the United States. This policy was only revealed when members of the American Indian Movement occupied the offices of the BIA in 1972. The U.S. General Accounting Office found that the Indian Health Service sterilized 3,406 Native American women between 1973 and 1976. The study showed that 36 women under age 21 were forcibly sterilized regardless of a court-ordered moratorium on sterilizations of women younger than 21. One out of four Native American women were involuntarily sterilized through tubal ligation or hysterectomy. The procedure was often done under the pretense of a check up or abortion and most individuals were not aware they had been sterilized. In the 1970s, the average birth rate of Native American women was 3.7 however, in 1980 it fell to 1.8.\n\nIn Canada, forced and coerced sterilizations are alleged to have happened as recently as 2017. In November, 2018, it was announced that over 60 Indigenous women are pursuing a class-action lawsuit alleging they underwent forced or coerced sterilizations over the past 20 to 25 years in Saskatchewan, in which they were not allowed contact with their newborns until they signed forms \"consenting\" to tubal ligations.\n\nIn the 1970s, after being forced onto reservations by the United States government, or relocated into urban areas without adequate support, many Native Americans were struggling with poverty. The Indian Health Service (IHS) was their main health provider. A common sterilization procedure was the hysterectomy, a form of permanent sterilization in which the uterus is removed through the patient's abdomen or vagina. Hysterectomies were often performed on Native American women by residents without the patient's knowledge. Another common form of sterilization was tubal ligation, a sterilization procedure in which a woman's fallopian tubes are tied, blocked, or cut.\n\nQuinacrine was also used to sterilize Native American women. Quinacrine is commonly used to treat malaria, but can also be used for non-surgical sterilization. Capsules inserted into the uterus spread and destroy the lining of the Fallopian tubes.\n\nNon-permanent forms of sterilization were also used including Depo-Provera and Norplant. Depo-Provera was used mainly on intellectually disabled Native American women before it gained clearance from the FDA in 1992. Norplant, promoted by the IHS, was marketed by Wyeth Pharmaceuticals (who were sued over insufficient disclosure of side effects including irregular menstrual bleeding, headaches, nausea and depression). Side effects of these two types of sterilization included the cessation of the menstrual cycle and excessive bleeding.\n\nUsing 2002 data from the National Survey of Family Growth, the Urban Indian Health Institute found that among women using contraception, the most common methods used by urban American Indian and Alaskan Native women age 15–44 years were female sterilization (34%), oral contraceptive pills (21%), and male condoms (21%). However among the urban Non-Hispanic Whites, the most common methods were oral contraceptive pills 36%), female sterilization (20%) and male condoms (18%).\n\nRacism and harmful stereotypes of the Native American population, held by members of the dominant culture, were factors that made Native American women targets for sterilization. The media commonly referred to Native American women by misogynist, racist slurs such as \"squaw\", defined as a \"dirty, subservient, abused, alcoholic and ugly woman who loves to torture white men.\" Negative racial stereotypes propagated the belief among the dominant culture that Native American women were unfit to raise or to have children in comparison to white women. First Nations women who have given birth in white hospitals have been coerced into being sterilized against their will; often being told it was mandatory before seeing their babies, or being lied to that it was a requirement of the delivery process. Not agreeing to the sterilization procedure would result in the withdrawal of welfare benefits. Consent forms presented to them failed to indicate that the decision would affect their ability to bear children in the future.\nStudies by the Health Research Group in 1973 and Doctor Bernard Rosenfeld's interviews in 1974 and 1975 show that this action was driven by social and economic factors.\n\nMost of the white physicians performing this procedure viewed sterilization as the best alternative for these women. They claimed it would improve their financial situation and their family's quality of life. The physicians were paid more for performing hysterectomies and tubal ligations than for prescribing other forms of birth control. The influx of surgical procedures was seen as a training for physicians and as a practice for resident physicians. In 1971, Dr. James Ryan stated that he favored hysterectomies over tubal ligations because \"it's more of a challenge...and it's good experience for the junior resident\". With fewer people applying for Medicaid and welfare, the federal government could decrease spending on welfare programs.\n\nIn the 1970s, the negative stereotypes of Native American women held by members of the dominant culture, along with racist beliefs of white superiority, contributed to the belief among white physicians that these women would not be able to limit the number of children or use birth control effectively. These factors all contributed to the sterilization policy.\n\nThe Indian Health Service (IHS) continues to offer sterilization as a method of family planning. Tubal ligation and vasectomy are the only procedures which may be performed for the primary purpose of sterilization. Legally, the IHS requires for the patient to give informed consent to the operation, be 21 years of age or older, and not be institutionalized in a correctional or mental health facility.\n\n"}
{"id": "23396910", "url": "https://en.wikipedia.org/wiki?curid=23396910", "title": "Systemic problem", "text": "Systemic problem\n\nA systemic problem is a problem due to issues inherent in the overall system,\nrather than due to a specific, individual, isolated factor. Contrast with pilot error, user error, or mistake.\n\nA change to the structure, organization or policies in that system could alleviate the systemic problem. On an Ishikawa diagram (fishbone diagram) of cause-and-effect links, the source of the problem can be said to be a common cause, rather than a special cause.\n\n\n"}
{"id": "3707152", "url": "https://en.wikipedia.org/wiki?curid=3707152", "title": "Tarksheel Society", "text": "Tarksheel Society\n\nTarksheel Society (Rationalist Society) is a rationalist group based in Punjab, India.\n\nFounded in 1984 under the leadership of Megh Raj Mitter & Sarjit Talwar. Tarksheel Society aims to disseminate rationalist ideas and scientific temper among the Indian people in order to eradicate religious fanaticism, communalism, caste system, untouchability and superstitions. Affiliated to Federation of Indian Rationalist Associations, Tarksheel Society advocates the separation of religion and education. The society has units in almost all the villages and towns of Punjab.\n\nThough mainly confined to the state of Punjab, the Society now operates in the neighboring states of Haryana, Himachal Pradesh, Rajasthan, Delhi, Chhattisgarh, Uttar Pradesh, Madhya Pradesh and Jammu and Kashmir also. Due to its high South-Asian population, the Society is also active in three major cities in Canada: Vancouver, Calgary and Toronto.\n\nTo accomplish its mission, the society organizes public meetings, conferences, study camps, seminars and publishes rationalist literature. The society undertakes campaigns to expose the so-called miracles and charlatanry of godmen who claim supernatural powers. Towards this the Society has announced a cash award of rupees five lakh (US$8,000) for anybody who demonstrates supernatural powers or miracles under fraud-proof conditions. The Society has so far published about 50 books in Punjabi and Hindi on rationalism and science to inculcate scientific temper among people.\n\nIn 2004 Tarksheel organized the three-day national conference for Federation of Indian Rationalist Associations.\n\nFirst book of the society was 'Te Dev Pursh Har Gaye' original author Dr. T. Kapoor was translated in Punjabi by Sarjit Talwar & Meghraj Mitter. The society publishes its own journals in Punjabi and Hindi. \"Tarak Bodh\" (Logical Cognition) and \"Taraksheel\" (Rationalist) are brought out in Punjabi and \"Tarak Jyoti\" (Logical Enlightenment) in Hindi.\n\n"}
{"id": "37541820", "url": "https://en.wikipedia.org/wiki?curid=37541820", "title": "Tatramajjhattatā", "text": "Tatramajjhattatā\n\nTatramajjhattatā (Pali) is a Buddhist term that is translated as \"equanimity\", \"neutrality of mind\", etc. In the Theravada tradition, it is defined as a mental attitude of balance, detachment, and impartiality. \n\nTatramajjhattatā is identified as:\n\nBhikkhu Bodhi explains:\n\nThe Visuddhimagga (XIV, 153) states about equanimity:\n\nNina van Gorkom explains:\n\n\n\nTheravada tradition:\n"}
{"id": "29131320", "url": "https://en.wikipedia.org/wiki?curid=29131320", "title": "The Moral Landscape", "text": "The Moral Landscape\n\nThe Moral Landscape: How Science Can Determine Human Values is a book by Sam Harris published in 2010. In it, he promotes a science of morality and argues that many thinkers have long confused the relationship between morality, facts, and science. He aims to carve a third path between secularists who say morality is subjective (e.g. moral relativists), and religionists who say that morality is given by God and scripture. Harris contends that the only moral framework worth talking about is one where \"morally good\" things pertain to increases in the \"well-being of conscious creatures\". He then argues that, problems with philosophy of science and reason in general notwithstanding, 'moral questions' will have objectively right and wrong answers which are grounded in empirical facts about what causes people to flourish.\n\nChallenging the traditional philosophical notion that humans can never get an 'ought' from an 'is' (the so called Hume's law), Harris argues that moral questions are best pursued using not just philosophy, but the methods of science. Thus, \"science can determine human values\" translates to \"science can tell us which values lead to human flourishing\". It is in this sense that Harris advocates that scientists begin conversations about a normative science of \"morality\".\n\nSam Harris's case starts with two premises: \"(1) some people have better lives than others, and (2) these differences are related, in some lawful and not entirely arbitrary way, to states of the human brain and to states of the world\". The idea is that a person is simply describing material facts (many about their brain) when they describe possible \"better\" and \"worse\" lives for themselves. Granting this, Harris says we must conclude that there are facts about which courses of action will allow one to pursue a better life.\n\nHarris attests to the importance of admitting that such facts exist, because he says this logic applies to groups of individuals as well. He suggests that there are better and worse ways for whole societies to pursue better lives. Just like at the scale of the individual, there may be multiple different paths and \"peaks\" to flourishing for societies - and many more ways to fail.\n\nHarris then makes a case that science could usefully define \"morality\" according to such facts (about people's wellbeing). Often his arguments point out that problems with this scientific definition of morality seem to be problems shared by all science, or reason and words in general. Harris also spends some time describing how science might engage nuances and challenges of identifying the best ways for individuals, and groups of individuals, to improve their lives. Many of these issues are covered below.\n\nAlthough Harris's book discusses the challenges that a science of morality must face, he also mentions that his scientific argument is indeed philosophical. Furthermore, he says that this is the case for almost all scientific investigation. He mentions that modern science amounts to careful practice of accepted first philosophical principles like empiricism and physicalism. He also suggests that science has already very much settled on \"values\" in answering the question \"what should I believe, and why should I believe it?\". Harris says it should not be surprising that normative ethical sciences are, or would be, similarly founded on bedrock assumptions (Basic norms). Harris says:\n\nThe way he thinks science might engage moral issues draws on various philosophical positions like ethical realism (there are facts worth calling 'moral facts'), and ethical naturalism (these facts relate to the physical world). Harris says a science of morality may resemble Utilitarianism, but that the science is, importantly, more open-ended because it involves an evolving definition of well-being. Rather than committing to Reductive materialism, then, Harris recognizes the arguments of revisionists that psychological definitions themselves are contingent on research and discoveries. Harris adds that any science of morality must consider everything from emotions and thoughts to the actual actions and their consequences.\n\nTo Harris, moral propositions, and explicit values in general, are concerned with the flourishing of conscious creatures in a society. He argues that \"Social morality exists to sustain cooperative social relationships, and morality can be objectively evaluated by that standard.\" Harris sees some philosophers' talk of strictly \"private\" morality as akin to unproductive discussion of some private, personal physics. \"If philosophers want to only talk about some bizarrely unnatural private morality, they are just changing the subject...\"\n\nHarris also discusses how interchangeability of perspective might emerge as an important part of moral reasoning. He alludes to an 'unpleasant surprise principle', where someone realizes they have been supporting an ineffective moral norm (e.g. reported cases of Jew-hunting Nazis discovering that they themselves were of Jewish descent).\n\nHarris identifies three projects for science as it relates to morality: (1) explaining why humans do what they do in the name of \"morality\" (e.g. traditional evolutionary psychology), (2) determining which patterns of thought and behaviour humans actually \"should\" follow (i.e. the science of morality), and (3) generally persuading humans to change their ways. Harris says that the first project is focused only on describing what is, whereas projects (2) and (3) are focused on what should and could be, respectively. Harris's point is that this second, prescriptive project should be the focus of a science of morality. He mentions, however, that we should not fear an \"Orwellian future\" with scientists at every door - vital progress in the science of morality could be shared in much the same way as advances in medicine.\n\nHarris says it is important to delineate project (1) from project (2), or else we risk committing a moralistic fallacy. He also highlights the importance of distinguishing between project (2) (asking what is right) from project (3) (trying to change behaviour). He says we must realize that the nuances of human motivation is a challenge in itself; humans often fail to do what they \"ought\" to do even to be successfully selfish - there is every reason to believe that discovering what is best for society would not change every member's habits overnight.\n\nHarris does not imagine that people, even scientists, have always made the right moral decisions—indeed it is precisely his argument that many of them are wrong about moral facts. This is due to the many real challenges of good science in general, including human cognitive limitations and biases (e.g. loss aversion can sway human decisions on important issues like medicine). He mentions the research of Paul Slovic and others to describe just a few of these established mental heuristics that might keep us from reasoning properly. Although he mentions that training might temper the influence of these biases, Harris worries about research showing that incompetence and ignorance in a domain leads to confidence (the Dunning–Kruger effect).\n\nHarris explains that debates and disagreement are a part of the scientific method, and that one side can certainly be wrong. He also explains that all the debates still available to science illustrate how much work could still be done, and how much conversation must continue.\n\nThe book is full of issues that Harris thinks are far from being empirically, morally grey areas. That is, besides saying that 'reasonable' thinking about moral issues amounts to scientific thinking. For instance, he references one poll that found that 36 percent of British Muslims think apostates should be put to death for their unbelief, and he says that these individuals are \"morally confused\". He also suggests it is obvious that loneliness, helplessness, and poverty are \"bad\", but that these are by no means as far as positive psychology has taken, and will take us.\n\nIn one section, called \"The illusion of free will\", Harris argues that there is a wealth of evidence in psychology (e.g. the illusion of introspection) or specifically related to the neuroscience of free will that suggests that metaphysically free will does not exist. This, he thinks, is intuitive; \"trains of thought...convey the apparent reality of choices, freely made. But from a deeper perspective...thoughts simply arise (what else could they do?)\". He adds \"The illusion of free will is itself an illusion\". The implications of free will's non-existence may be a working determinism, and Harris warns us not to confuse this with fatalism.\n\nOne implication of a determined will, Harris says, is that it becomes unreasonable to punish people out of retribution—only behaviour modification and the deterrence of others still seem to be potentially valid reasons to punish. This, especially because behaviour modification is a sort of cure for the evil behaviours; Harris provides a thought experiment: \n\nHarris acknowledges a hierarchy of moral consideration (e.g. humans are more important than bacteria or mice). He says it follows that there could, in principle, be a species compared to which we are relatively unimportant (although he doubts such a species exists).\n\nHarris supports the development of lie-detection technology and believes it would be, on the whole, beneficial for humanity. He also supports the formation of an explicit global civilization because of the potential for stability under a world government.\n\nConsistent with Harris's definition of morality, he says we must ask whether religion increases human flourishing today (regardless of whether it increased it in the distant past). He argues that religions may largely be practiced because they fit well with human cognitive tendencies (e.g. animism).\nIn Harris's view, religion and religious dogma is an impediment to reason, and he discusses the views of Francis Collins as one example.\n\nHarris criticizes the tactics of secularists like Chris Mooney, who argue that science is not fundamentally (and certainly not superficially) in conflict with religion. Harris sees this as a very serious disagreement, that patronizingly attempts to pacify more devout theists. Harris claims that societies can move away from deep dependence on religion just as it has from witchcraft, which he says was once just as deeply ingrained.\n\nIn advance of publication, four personal and professional acquaintances of the author, biologist and science popularizer Richard Dawkins, novelist Ian McEwan, psycholinguist Steven Pinker, and theoretical physicist Lawrence Krauss, offered their praise for the book. They each serve on the Advisory Board of Harris's Project Reason, and their praise appears as blurbs (released by the book's publisher on Harris's website and reproduced on the book's dust jacket).\nDawkins said,\nMcEwan wrote that \"Harris breathes intellectual fire into an ancient debate. Reading this thrilling, audacious book, you feel the ground shifting beneath your feet. Reason has never had a more passionate advocate.\" Pinker said that Harris offers \"a tremendously appealing vision, and one that no thinking person can afford to ignore.\" Krauss opined that Harris \"has the rare ability to frame arguments that are not only stimulating, they are downright nourishing, even if you don't always agree with him!\" Krauss predicted that \"readers are bound to come away with previously firm convictions about the world challenged, and a vital new awareness about the nature and value of science and reason in our lives.\"\n\n\"The Moral Landscape\" reached 9th in the \"New York Times\" Best Seller list for Hardcover Non-Fiction in October 2010.\n\nECSU Associate Professor of Philosophy James W. Diller and Andrew E. Nuzzolilli wrote a generally favorable review in a journal of the Association for Behavior Analysis International:\n\nIn his review for Barnes & Noble, Cal State Associate Professor of Philosophy Troy Jollimore wrote that the book \"has some good, reasonable, and at times persuasive things to say\" to people who are unfamiliar with moral skepticism, but \"has little to say to those people who actually do know what the arguments are, and it will not help others become much better informed.\" Jollimore also worried that Harris wrongly presents complex issues as having simple solutions.\n\nKwame Anthony Appiah wrote in \"The New York Times\" \"when [Harris] stays closest to neuroscience, he says much that is interesting and important...\". He later criticized Harris for failing to articulate \"his central claim\" and to identify how science has \"revealed\" that human well-being has an objective component. Appiah argued that Harris \"ends up endorsing ... something very like utilitarianism, a philosophical position that is now more than two centuries old, ... that faces a battery of familiar problems,\" which Harris merely \"push[es] ... aside.\" Harris responded to Appiah in the afterword of the paperback version, writing that all of Appiah's criticisms had already been addressed in the chapter \"Good and Evil\".\n\nCognitive scientist and anthropologist Scott Atran criticized Harris for failing to engage with the philosophical literature on ethics and the problems in attempting to scientifically quantify human well being, noting that\nCritiquing the book, Kenan Malik wrote:\n\nAmerican novelist Marilynne Robinson, writing in \"The Wall Street Journal\", asserted that Harris fails to \"articulate a positive morality of his own\" but, had he done so, would have found himself in the company of the \"Unitarians, busily cooperating on schemes to enhance the world's well being, as they have been doing for generations.\"\n\nDavid Sexton of the London Evening Standard described Harris's claim to provide a science of morality as“the most extraordinarily overweening claim and evidently flawed. Science does not generate its own moral values; it can be used for good or ill and has been. Harris cannot stand outside culture, and the 'better future' he prophesies is itself a cultural projection. \"\n\nJohn Horgan, journalist for the \"Scientific American\" blog and author of \"The End of Science\", wrote \"Harris further shows his arrogance when he claims that neuroscience, his own field, is best positioned to help us achieve a universal morality. ... Neuroscience can't even tell me how I can know the big, black, hairy thing on my couch is my dog Merlin. And we're going to trust neuroscience to tell us how we should resolve debates over the morality of abortion, euthanasia and armed intervention in other nations' affairs?\"\n\nRussell Blackford said \"The Moral Landscape is an ambitious work that will gladden the hearts, and strengthen the spines, of many secular thinkers\" but that he had \"serious reservations about a good book\".\n\nThe philosopher Simon Blackburn, reviewing the book, described Harris as \"a knockabout atheist\" who \"joins the prodigious ranks of those whose claim to have transcended philosophy is just an instance of their doing it very badly\", pointing out that \"if Bentham's hedonist is in one brain state and Aristotle's active subject is in another, as no doubt they would be, it is a moral, not an empirical, problem to say which is to be preferred.\" And H. Allen Orr in the \"New York Review of Books\" finds that \"Despite Harris's bravado about 'how science can determine human values,' The Moral Landscape delivers nothing of the kind.\"\n\nSteve Isaacson wrote \"Mining The Moral Landscape: Why Science Does Not (and cannot) Determine Human Values\". Isaacson concludes, \"The largest objection to Harris' argument is still Moore's open-question argument. Harris dismisses the argument as a word game easily avoided, but he never explains the game nor how to avoid it. He just ignores it.\"\n\nAt the Moving Naturalism Forward workshop, Nobel Prize winning physicist Steven Weinberg described how in his youth he had been a utilitarian but had been dissuaded of the notion that \"the fundamental principle that guides our actions should be the greatest happiness for the greatest number\" by reading Aldous Hŭley's \"Brave New World\". Weinberg went on to say: \"Now, Sam Harris is aware of this kind of counter argument [to utilitarianism], and says it's not happiness, it's human welfare. Well, as you make things vaguer and vaguer, of course, it becomes harder and harder to say it doesn't fit your own moral feelings, but it also becomes less and less useful as a means of making moral judgements. You could take that to the extreme and make up some nonsense word and say that's the important thing and no-one could refute it but it wouldn't be very helpful. I regard human welfare and the way Sam Harris refers to it as sort of halfway in that direction to absolute nonsense.\"\n\nA few months after the book's release, Sam Harris wrote a follow-up at \"The Huffington Post\" responding to his critics.\n\nOn August 31, 2013, in response to the negative reviews of his book, Harris issued a public challenge for anyone to write an essay of less than 1,000 words rebutting the \"central argument\" of the book. The submissions were vetted by Russell Blackford, with the author of the essay judged best to receive $2,000, or $20,000 if they succeeded in changing Harris's mind. 424 essays were received by the deadline. On March 11, 2014, Blackford announced the winning essay was written by philosophy instructor Ryan Born.\n"}
{"id": "9040377", "url": "https://en.wikipedia.org/wiki?curid=9040377", "title": "The Toyota Way", "text": "The Toyota Way\n\nThe Toyota Way is a set of principles and behaviors that underlie the Toyota Motor Corporation's managerial approach and production system. Toyota first summed up its philosophy, values and manufacturing ideals in 2001, calling it \"The Toyota Way 2001\". It consists of principles in two key areas: continuous improvement, and respect for people.\n\nThe Toyota Way has been called \"a system designed to provide the tools for people to continually improve their work\" The 14 principles of The Toyota Way are organized in four sections: \nThe two focal points of the principles are continuous improvement and respect for people. The principles for a continuous improvement include establishing a long-term vision, working on challenges, continual innovation, and going to the source of the issue or problem. The principles relating to respect for people include ways of building respect and teamwork.\n\nThe system can be summarized in 14 principles. The principles are set out and briefly described below:\n\nPrinciple 1\nPeople need purpose to find motivation and establish goals.\n\nPrinciple 2\n\nWork processes are redesigned to eliminate waste (muda) through the process of continuous improvement — kaizen. The seven types of muda are:\n\nPrinciple 3\n\nA method where a process signals its predecessor that more material is needed. The pull system produces only the required material after the subsequent operation signals a need for it. This process is necessary to reduce overproduction.\n\nPrinciple 4\n\nThis helps achieve the goal of minimizing waste (muda), not overburdening people or the equipment (muri), and not creating uneven production levels (mura).\n\nPrinciple 5\n\nQuality takes precedence (Jidoka). Any employee in the Toyota Production System has the authority to stop the process to signal a quality issue.\n\nPrinciple 6\n\nAlthough Toyota has a bureaucratic system, the way that it is implemented allows for continuous improvement (kaizen) from the people affected by that system. It empowers the employee to aid in the growth and improvement of the company.\n\nPrinciple 7\n\nIncluded in this principle is the 5S Program - steps that are used to make all work spaces efficient and productive, help people share work stations, reduce time looking for needed tools and improve the work environment.\n\n\nPrinciple 8\n\nTechnology is pulled by manufacturing, not pushed to manufacturing.\n\nPrinciple 9\n\nWithout constant attention, the principles will fade. The principles have to be ingrained, it must be the way one thinks. Employees must be educated and trained: they have to maintain a learning organization.\n\nPrinciple 10\n\nTeams should consist of 4-5 people and numerous management tiers. Success is based on the team, not the individual.\n\nPrinciple 11\n\nToyota treats suppliers much like they treat their employees, challenging them to do better and helping them to achieve it. Toyota provides cross functional teams to help suppliers discover and fix problems so that they can become a stronger, better supplier.\n\nPrinciple 12\n\nToyota managers are expected to \"go-and-see\" operations. Without experiencing the situation firsthand, managers will not have an understanding of how it can be improved. Furthermore, managers use Tadashi Yamashima's (President, Toyota Technical Center (TTC)) ten management principles as a guideline:\n\n\nPrinciple 13\n\nThe following are decision parameters:\n\nPrinciple 14\n\nThe process of becoming a learning organization involves criticizing every aspect of what one does. The general problem solving technique to determine the root cause of a problem includes:\n\nIn 2004, Dr. Jeffrey Liker, a University of Michigan professor of industrial engineering, published \"The Toyota Way\". In his book Liker calls the Toyota Way \"a system designed to provide the tools for people to continually improve their work.\" \nAccording to Liker, the 14 principles of The Toyota Way are organized in four sections: (1) long-term philosophy, (2) the right process will produce the right results, (3) add value to the organization by developing your people, and (4) continuously solving root problems drives organizational learning.\n\nThe first principle involves managing with a long-view rather than for short-term gain. It reflects a belief that people need purpose to find motivation and establish goals.\n\nThe next seven principles are focused on process with an eye towards quality outcome. Following these principles, work processes are redesigned to eliminate waste (muda) through the process of continuous improvement — kaizen. The seven types of muda are (1) overproduction; (2) waiting, time on hand; (3) unnecessary transport or conveyance; (4) overprocessing or incorrect processing; (5) excess inventory; (6) motion; and (7) defects.\n\nThe principles in this section empower employees in spite of the bureaucratic processes of Toyota, as any employee in the Toyota Production System has the authority to stop production to signal a quality issue, emphasizing that quality takes precedence (Jidoka). The way the Toyota bureaucratic system is implemented to allow for continuous improvement (kaizen) from the people affected by that system so that any employee may aid in the growth and improvement of the company.\n\nRecognition of the value of employees is also part of the principle of measured production rate (heijunka), as a level workload helps avoid overburdening people and equipment (muri), but this is also intended to minimize waste (muda) and avoid uneven production levels (mura).\n\nThese principles are also designed to ensure that only essential materials are employed (to avoid overproduction), that the work environment is maintained efficiently (the 5S Program) to help people share work stations and to reduce time looking for needed tools, and that the technology used is reliable and thoroughly tested.\n\nHuman development is the focus of principles 9 through 11. Principle 9 emphasizes the need to ensure that leaders embrace and promote the corporate philosophy. This reflects, according to Liker, a belief that the principles have to be ingrained in employees to survive. The 10th principle emphasizes the need of individuals and work teams to embrace the company's philosophy, with teams of 4-5 people who are judged in success by their team achievements, rather than their individual efforts. Principle 11 looks to business partners, who are treated by Toyota much like they treat their employees. Toyota challenges them to do better and helps them to achieve it, providing cross functional teams to help suppliers discover and fix problems so that they can become a stronger, better supplier.\n\nThe final principles embrace a philosophy of problem solving that emphasizes thorough understanding, consensus-based solutions swiftly implemented and continual reflection (hansei) and improvement (kaizen). The 12th principle (Genchi Genbutsu) sets out the expectation that managers will personally evaluate operations so that they have a firsthand understanding of situations and problems. Principle 13 encourages thorough consideration of possible solutions through a consensus process, with rapid implementation of decisions once reached (nemawashi). The final principle requires that Toyota be a \"learning organization\", continually reflecting on its practices and striving for improvement. According to Liker, the process of becoming a learning organization involves criticizing every aspect of what one does.\n\nThere is a question of uptake of the principles now that Toyota has production operations in many different countries around the world. As a \"New York Times\" article notes, while the corporate culture may have been easily disseminated by word of mouth when Toyota manufacturing was only in Japan, with worldwide production, many different cultures must be taken into account. Concepts such as \"mutual ownership of problems\", or \"genchi genbutsu\", (solving problems at the source instead of behind desks), and the \"kaizen mind\", (an unending sense of crisis behind the company’s constant drive to improve), may be unfamiliar to North Americans and people of other cultures. A recent increase in vehicle recalls may be due, in part, to \"a failure by Toyota to spread its obsession for craftsmanship among its growing ranks of overseas factory workers and managers.\" Toyota is attempting to address these needs by establishing training institutes in the United States and in Thailand.\n\nToyota Way has been driven so deeply into the psyche of employees at all levels that it has morphed from a strategy into an important element of the company's culture. According to Masaki Saruta, author of several books on Toyota, \"the real Toyota Way is a culture of control.\" The Toyota Way rewards intense company loyalty that at the same time invariably reduces the voice of those who challenge authority. \"The Toyota Way of constructive criticism to reach a better way of doing things 'is not always received in good spirit at home.'\" The Toyota Way management approach at the automaker \"worked until it didn't.\"\n\nOne consequence was when Toyota was given reports of sudden acceleration in its vehicles and the company faced a potential recall situation. There were questions if Toyota's crisis was caused by the company losing sight of its own principles. The Toyota Way in this case did not address the problem and provide direction on what the automaker would be doing, but managers instead protected the company and issued flat-out denials and placed the blame at others. The consequence of the automaker's actions led to the 2009–11 Toyota vehicle recalls. Although one of the Toyota Way principles is to \"build a culture of stopping to fix problems to get quality right the first time,\" Akio Toyoda, President and CEO, stated during Congressional hearings that the reason for the problems was that his \"company grew too fast.\" Toyota management had determined its goal was to become the world's largest automotive manufacturer. According to some management consultants, when the pursuit of growth took priority, the automaker \"lost sight of the key values that gave it its reputation in the first place.\"\n\n\n"}
{"id": "914025", "url": "https://en.wikipedia.org/wiki?curid=914025", "title": "Triune brain", "text": "Triune brain\n\nThe triune brain is a model of the evolution of the vertebrate forebrain and behavior, proposed by the American physician and neuroscientist Paul D. MacLean. MacLean originally formulated his model in the 1960s and propounded it at length in his 1990 book \"The Triune Brain in Evolution\". The triune brain consists of the reptilian complex, the paleomammalian complex (limbic system), and the neomammalian complex (neocortex), viewed as structures sequentially added to the forebrain in the course of evolution. However, this hypothesis is no longer espoused by the majority of comparative neuroscientists in the post-2000 era. \nThe triune brain hypothesis became familiar to a broad popular audience through Carl Sagan's Pulitzer prize winning 1977 book \"The Dragons of Eden\". The theory has been embraced by some psychiatrists and at least one leading affective neuroscience researcher.\n\nThe reptilian complex, also known as the R-complex or \"reptilian brain\" was the name MacLean gave to the basal ganglia, structures derived from the floor of the forebrain during development. The term derives from the idea that comparative neuroanatomists once believed that the forebrains of reptiles and birds were dominated by these structures. MacLean proposed that the reptilian complex was responsible for species-typical instinctual behaviours involved in aggression, dominance, territoriality, and ritual displays. \n\nThe paleomammalian brain consists of the septum, amygdalae, hypothalamus, hippocampal complex, and cingulate cortex. MacLean first introduced the term \"limbic system\" to refer to this set of interconnected brain structures in a paper in 1952. MacLean's recognition of the limbic system as a major functional system in the brain was widely accepted among neuroscientists, and is generally regarded as his most important contribution to the field. MacLean maintained that the structures of the limbic system arose early in mammalian evolution (hence \"paleomammalian\") and were responsible for the motivation and emotion involved in feeding, reproductive behaviour, and parental behaviour. \n\nThe neomammalian complex consists of the cerebral neocortex, a structure found uniquely in higher mammals, and especially humans. MacLean regarded its addition as the most recent step in the evolution of the mammalian brain, conferring the ability for language, abstraction, planning, and perception. \n\nMacLean originally formulated the triune brain hypothesis in the 1960s, drawing on comparative neuroanatomical work done by Ludwig Edinger, Elizabeth C. Crosby and Charles Judson Herrick early in the twentieth century. The 1980s saw a rebirth of interest in comparative neuroanatomy, motivated in part by the availability of a variety of new neuroanatomical techniques for charting the circuitry of animal brains. Subsequent findings have refined the traditional neuroanatomical ideas upon which MacLean based his hypothesis.\n\nFor example, the basal ganglia (structures derived from the floor of the forebrain and making up MacLean's reptilian complex) were shown to take up a much smaller portion of the forebrains of reptiles and birds (together called sauropsids) than previously supposed, and to exist in amphibians and fish as well as mammals and sauropsids. Because the basal ganglia are found in the forebrains of all modern vertebrates, they most likely date to the common evolutionary ancestor of the vertebrates, more than 500 million years ago, rather than to the origin of reptiles.\n\nSome recent behavioral studies do not support the traditional view of sauropsid behavior as stereotyped and ritualistic (as in MacLean's reptilian complex). Birds have been shown to possess highly sophisticated cognitive abilities, such as the toolmaking of the New Caledonian crow and the language-like categorization abilities of the grey parrot. Structures of the limbic system, which MacLean proposed arose in early mammals, have now been shown to exist across a range of modern vertebrates. The \"paleomammalian\" trait of parental care of offspring is widespread in birds and occurs in some fishes as well. Thus, like the basal ganglia, the evolution of these systems presumably dates to a common vertebrate ancestor.\n\nFinally, recent studies based on paleontological data or comparative anatomical evidence strongly suggest that the neocortex was already present in the earliest emerging mammals. In addition, although non-mammals do not have a neocortex in the true sense (that is, a structure comprising part of the forebrain roof, or pallium, consisting of six characteristic layers of neurons), they possess pallial regions, and some parts of the pallium are considered homologous to the mammalian neocortex. While these areas lack the characteristic six neocortical layers, birds and reptiles generally possess three layers in the dorsal pallium (the homolog of the mammalian neocortex). The telencephalon of birds and mammals makes neuroanatomical connections with other telecencephalic structures like those made by neocortex. It mediates similar functions such as perception, learning and memory, decision making, motor control, conceptual thinking.\n\nThe triune model of the mammalian brain is seen as an oversimplified organizing theme by some in the field of comparative neuroscience. It continues to hold public interest because of its simplicity. While technically inaccurate in many respects as an explanation for brain activity, it remains one of very few approximations of the truth we have to work with: the \"neocortex\" represents that cluster of brain structures involved in advanced cognition, including planning, modeling and simulation; the \"limbic brain\" refers to those brain structures, wherever located, associated with social and nurturing behaviors, mutual reciprocity, and other behaviors and affects that arose during the age of the mammals; and the \"reptilian brain\" refers to those brain structures related to territoriality, ritual behavior and other \"reptile\" behaviors. The broad explanatory value makes this approximation very engaging and is a useful level of complexity for high school students to begin engaging with brain research.\nHoward Bloom, in his book \"The Lucifer Principle\", references the concept of the triune brain in his explanations of certain aspects of human behavior. Arthur Koestler made MacLean's concept of the triune brain the centerpiece of much of his later work, notably \"The Ghost in the Machine\". English novelist Julian Barnes quotes MacLean on the triune brain in the foreword to his 1982 novel \"Before She Met Me\". Peter A. Levine uses the triune brain concept in his book \"Waking the Tiger\" to explain his somatic experiencing approach to healing trauma.\n\nGlynda-Lee Hoffmann, in her book, \"The Secret Dowry of Eve, Women's Role in the Development of Consciousness,\" references the triune theory explored by MacLean, and she goes one step further. Her theory about human behavior and the problems we create with that behavior, distinguishes the prefrontal cortex as uniquely different from the rest of the neocortex. The prefrontal cortex, with its agenda of integration, is the part of the brain that can get the other parts to work together for the good of the individual. In many humans the reptilian cortex (agenda: territory and reproduction [in humans that translates to power and sex] is out of control and the amygdala stokes the fear that leads to more bad behavior. The prefrontal cortex is the key to our future if we can harness its power.\n\n"}
{"id": "32680", "url": "https://en.wikipedia.org/wiki?curid=32680", "title": "Vilfredo Pareto", "text": "Vilfredo Pareto\n\nVilfredo Federico Damaso Pareto (; ; born Wilfried Fritz Pareto, 15 July 1848 – 19 August 1923) was an Italian engineer, sociologist, economist, political scientist, and philosopher. He made several important contributions to economics, particularly in the study of income distribution and in the analysis of individuals' choices. He was also responsible for popularising the use of the term \"elite\" in social analysis.\n\nHe introduced the concept of Pareto efficiency and helped develop the field of microeconomics. He was also the first to discover that income follows a Pareto distribution, which is a power law probability distribution. The Pareto principle was named after him, and it was built on observations of his such as that 80% of the land in Italy was owned by about 20% of the population. He also contributed to the fields of sociology and mathematics, according to the mathematician Benoit Mandelbrot and Richard L. Hudson:\n\nPareto was born of an exiled noble Genoese family in 1848 in Paris, the centre of the popular revolutions of that year. His father, Raffaele Pareto (1812–1882), was an Italian civil engineer and Ligurian marquis who had left Italy much like Giuseppe Mazzini and other Italian nationalists. His mother, Marie Metenier, was a French woman. Enthusiastic about the 1848 German revolution, his parents named him Fritz Wilfried, which became Vilfredo Federico upon his family's move back to Italy in 1858. In his childhood, Pareto lived in a middle-class environment, receiving a high standard of education, attending the new created \"Istituto Tecnico Leardi\" where Fernando Pio Rosellini was his mathematics professor. In 1869, he earned a doctor's degree in engineering from what is now the Polytechnic University of Turin (then the Technical School for Engineers). His dissertation was entitled \"The Fundamental Principles of Equilibrium in Solid Bodies\". His later interest in equilibrium analysis in economics and sociology can be traced back to this paper.\n\nFor some years after graduation, he worked as a civil engineer, first for the state-owned Italian Railway Company and later in private industry. He was manager of the Iron Works of San Giovanni Valdarno and later general manager of Italian Iron Works.\n\nHe did not begin serious work in economics until his mid-forties. He started his career a fiery advocate of classical liberalism, besting the most ardent British liberals with his attacks on any form of government intervention in the free market. In 1886, he became a lecturer on economics and management at the University of Florence. His stay in Florence was marked by political activity, much of it fueled by his own frustrations with government regulators. In 1889, after the death of his parents, Pareto changed his lifestyle, quitting his job and marrying a Russian, Alessandrina Bakunina. She left him in 1902 for a young servant.\n\nIn 1893, he succeeded Léon Walras to the chair of Political Economy at the University of Lausanne in Switzerland where he remained for the rest of his life. In 1906, he made the famous observation that twenty percent of the population owned eighty percent of the property in Italy, later generalised by Joseph M. Juran into the Pareto principle (also termed the 80–20 rule). In one of his books published in 1909 he showed the Pareto distribution of how wealth is distributed, he believed \"through any human society, in any age, or country\". He maintained cordial personal relationships with individual socialists, but always thought their economic ideas were severely flawed. He later became suspicious of their humanitarian motives and denounced socialist leaders as an 'aristocracy of brigands' who threatened to despoil the country and criticized the government of Giovanni Giolitti for not taking a tougher stance against worker strikes. Growing unrest among labor in Italy led him to the anti-socialist and anti-democratic camp. His attitude toward fascism in his last years is a matter of controversy.\n\nPareto's relationship with scientific sociology in the age of the foundation is grafted in a paradigmatic way in the moment in which he, starting from the political economy, criticizes positivism as a totalizing and metaphysical system devoid of a rigorous logical-experimental method. In this sense we can read the fate of the\nParetian production within a history of the social sciences that continues to show its peculiarity and interest for its contributions in the 21st century (Giovanni Busino, \"Sugli studi paretiani all'alba del XXI secolo\" in \"Omaggio a Vilfredo Pareto\", \"Numero monografico in memoria di Giorgio Sola\" a cura di Stefano Monti Bragadin, \"Storia Politica Società\", Quaderni di Scienze Umane, anno IX, n. 15, giugno-dicembre 2009, p. 1 e sg.)) . The story of Pareto is also part of the multidisciplinary research of a scientific model that privileges sociology as a critique of cumulative models of knowledge as well as a discipline tending to the affirmation of relational models of science (Guglielmo Rinzivillo, \"Vilfredo Pareto e i modelli interdisciplinari nella scienza,\" \"Sociologia\", A. XXIX, n. 1, New Series, 1995, pp. 2017–222 see also in Guglielmo Rinzivillo, \"Una epistemologia senza storia\", Rome, New Culture, 2013, pp. 13–29, ).\n\nIn 1923 Pareto remarried with Jeanne Regis, just before he died in Geneva, Switzerland, 19 August 1923, \"among a menagerie of cats that he and his French lover kept\" in their villa; \"the local divorce laws prevented him from divorcing his wife and remarrying until just a few months prior to his death\".\n\nPareto's later years were spent in collecting the material for his best-known work, \"Trattato di sociologia generale\" (1916) (\"The Mind and Society\", published in 1935). His final work was \"Compendio di sociologia generale\" (1920).\n\nIn his \"Trattato di Sociologia Generale\" (1916, rev. French trans. 1917), published in English by Harcourt, Brace in a four-volume edition edited by Arthur Livingston under the title \"The Mind and Society\" (1935), Pareto developed the notion of the circulation of elites, the first social cycle theory in sociology. He is famous for saying \"history is a graveyard of aristocracies\".\n\nPareto seems to have turned to sociology for an understanding of why his abstract mathematical economic theories did not work out in practice, in the belief that unforeseen or uncontrollable social factors intervened. His sociology holds that much social action is nonlogical and that much personal action is designed to give spurious logicality to non-rational actions. We are driven, he taught, by certain \"residues\" and by \"derivations\" from these residues. The more important of these have to do with conservatism and risk-taking, and human history is the story of the alternate dominance of these sentiments in the ruling elite, which comes into power strong in conservatism but gradually changes over to the philosophy of the \"foxes\" or speculators. A catastrophe results, with a return to conservatism; the \"lion\" mentality follows. This cycle might be broken by the use of force, says Pareto, but the elite becomes weak and humanitarian and shrinks from violence.\n\nPareto's sociology was introduced to the United States by George Homans and Lawrence J. Henderson at Harvard, and had considerable influence, especially on Harvard sociologist Talcott Parsons, who developed a systems approach to society and economics that argues the status quo is usually functional.\n\nPareto was a lifelong opponent of Marxism.\n\nBenoît Mandelbrot wrote:\n\nPareto had argued that democracy was an illusion and that a ruling class always emerged and enriched itself. For him, the key question was how actively the rulers ruled. For this reason he called for a drastic reduction of the state and welcomed Benito Mussolini's rule as a transition to this minimal state so as to liberate the \"pure\" economic forces.\n\nMandelbrot summarized Pareto's notions as follows:\n\nThe future leader of Italian fascism Benito Mussolini, in 1904, when he was a young student, attended some of Pareto's lectures at the University of Lausanne. It has been argued that Mussolini's move away from socialism towards a form of \"elitism\" may be attributed to Pareto's ideas.\n\nTo quote Franz Borkenau, a biographer:\nKarl Popper dubbed Pareto the \"theoretician of totalitarianism\", but, according to Cirillo, there is no evidence in Popper's published work that he read Pareto in any detail before repeating what was then a common but dubious judgment in anti-fascist circles.\n\nSome fascist writers, such as Luigi Amoroso, wrote approvingly of Pareto's ideas:\nAuthor Renato Cirillo argued, on the contrary, that:\n\nPareto Theory Of Maximum Economics\n\nPareto turned his interest to economic matters and he became an advocate of free trade, finding himself in difficulty with the Italian government. His writings reflected the ideas of Léon Walras that economics is essentially a mathematical science. Pareto was a leader of the \"Lausanne School\" and represents the second generation of the Neoclassical Revolution. His \"tastes-and-obstacles\" approach to general equilibrium theory was resurrected during the great \"Paretian Revival\" of the 1930s and has influenced theoretical economics since.\n\nIn his \"Manual of Political Economy\" (1906) the focus is on equilibrium in terms of solutions to individual problems of \"objectives and constraints\". He used the indifference curve of Edgeworth (1881) extensively, for the theory of the consumer and, another great novelty, in his theory of the producer. He gave the first presentation of the trade-off box now known as the \"Edgeworth-Bowley\" box.\n\nPareto was the first to realize that cardinal utility could be dispensed with and economic equilibrium thought of in terms of ordinal utility – that is, it was not necessary to know how much a person valued this or that, only that he preferred X of this to Y of that. Utility was a preference-ordering. With this, Pareto not only inaugurated modern microeconomics, but he also demolished the alliance of economics and utilitarian philosophy (which calls for the greatest good for the greatest number; Pareto said \"good\" cannot be measured). He replaced it with the notion of \"Pareto-optimality\", the idea that a system is enjoying maximum economic satisfaction when no one can be made better off without making someone else worse off. Pareto optimality is widely used in welfare economics and game theory. A standard theorem is that a perfectly competitive market creates distributions of wealth that are Pareto optimal.\n\nSome economic concepts in current use are based on his work:\nHe argued that in all countries and times, the distribution of income and wealth is highly skewed, with a few holding most of the wealth. He argued that all observed societies follow a regular logarithmic pattern:\n\nwhere N is the number of people with wealth higher than x, and A and m are constants. Over the years, Pareto's Law has proved remarkably close to observed data.\n\n\n\n\n\n\n"}
{"id": "2056815", "url": "https://en.wikipedia.org/wiki?curid=2056815", "title": "Void type", "text": "Void type\n\nThe Void type, in several programming languages derived from C and Algol68, is the type for the result of a function that returns normally, but does not provide a result value to its caller. Usually such functions are called for their side effects, such as performing some task or writing to their output parameters. The usage of the void type in such context is comparable to procedures in Pascal and syntactic constructs which define subroutines in Visual Basic. It is also similar to the unit type used in functional programming languages and type theory. See Unit type#In programming languages for a comparison.\n\nC and C++ also support the pointer to void type (specified as codice_1), but this is an unrelated notion. Variables of this type are pointers to data of an \"unspecified\" type, so in this context (but not the others) codice_1 acts roughly like a universal or top type. A program can probably convert a pointer to any type of data (except a function pointer) to a pointer to void and back to the original type without losing information, which makes these pointers useful for polymorphic functions. The C language standard does not guarantee that the different pointer types have the same size.\n\nA function with void result type ends either by reaching the end of the function or by executing a return statement with no returned value. The void type may also appear as the sole argument of a function prototype to indicate that the function takes no arguments. Note that despite the name, in all of these situations, the void type serves as a unit type, not as a zero or bottom type (which \nis sometimes confusingly called the \"void type\"), even though unlike a real unit type which is a singleton, the void type lacks a way to represent its value and the language does not provide any way to declare an object or represent a value with type codice_3.\n\nIn the earliest versions of C, functions with no specific result defaulted to a return type of codice_4 and functions with no arguments simply had empty argument lists. Pointers to untyped data were declared as integers or pointers to codice_5. Some early C compilers had the feature, now seen as an annoyance, of generating a warning on any function call that did not use the function's returned value. Old code sometimes casts such function calls to void to suppress this warning. By the time Bjarne Stroustrup began his work on C++ in 1979–1980, void and void pointers were part of the C language dialect supported by AT&T-derived compilers.\n\nThe explicit use of void vs. giving no arguments in a function prototype has different semantics in C and C++, as detailed in the following table:\n\nA C prototype taking no arguments, e.g. codice_6 above, has been deprecated in C99, however.\n\nQuite contrary to C++, in the functional programming language Haskell the void type denotes the empty type, which has no inhabitants . A function into the void type does not return results, and a side-effectful program with type signature codice_7 does not terminate, or crashes. In particular, there are no total functions into the void type.\n"}
{"id": "2195142", "url": "https://en.wikipedia.org/wiki?curid=2195142", "title": "Wallflower (people)", "text": "Wallflower (people)\n\nA wallflower is someone with an introverted personality type (or in more extreme cases, social anxiety) who will attend parties and social gatherings, but will usually distance themselves from the crowd and actively avoid being in the limelight.\n\nThe name itself derives from eponymous plant's unusual growth pattern; against a wall as a stake or in cracks and gaps in stone walls. \"Wallflowers\" might literally stand against a wall and simply observe others at a social gathering, rather than mingle. This could be due to anxiety, shyness, lack of social skills or self-esteem.\n\nWallflower can also be used on a grander scale. When a company or organization chooses or is forced to remain on the sidelines of any activity, they can be referred to as a \"wallflower\".\n\nStructural functionalism is a sociological theory that sees society as a number of complex parts that form a stable and functional whole. This leads to a strong and coherent family unit made of smaller parts, with the functioning family unit then going on to form the smaller parts of a wider community, society and so on.\n\nSocial conflict theory in sociology claims that society is in a state of perpetual conflict due to competition for limited resources. It holds that social order is maintained by domination and power, rather than consensus and conformity. According to conflict theory, those with wealth and power try to hold on to it by any means possible, chiefly by suppressing the poor and powerless.\n\nThe most relevant sociological theory that the \"wallflower\" relates to, Symbolic Interaction describes specific gestures or social norms that are symbolic in meaning. The theory consists of three core principles: meaning, language and thought. These core principles lead to conclusions about the creation of a person’s self and socialization into a larger community.\n\nBecause the \"wallflower\" will usually exhibit a \"lack of\" interaction with others, it becomes symbolic of their thoughts and feelings towards others. The most specific example would be in the body language. Many times people who are shy have little or no eye contact with others. You may see a man, woman, or child try to avoid eye contact with others while out walking around in public or even in private. For some this may be a condition that becomes consistent over time and become a normal action.\n\nIn the case of parties or social gatherings, the \"wallflower\" will remain at a certain distance from the crowd or most of the people. When a shy person is around others you may see that they do what they can to stay away from the people that they do not know. Even with some friends you may or may not see the shy man or woman even near or in the Bell Bubble or within the intimate distance of friends. In a social setting you may not see a shy person in the center of the room without a friend or group of friends. Shy people tend to stay out of the possibility of even being the center of attention.\n\nSocial anxiety the extreme fear of being scrutinized and judged by others in social or performance situations: Social anxiety disorder can wreak havoc on the lives of those who suffer from it. This disorder is \"not\" simply shyness that has been inappropriately medicated. Symptoms may be so extreme that they disrupt daily life. People with this disorder, also called \"social phobia\", may have few or no social or romantic relationships, making them feel powerless, alone, or even ashamed.\n\nAlthough they recognize that the fear is excessive and unreasonable, people with social anxiety disorder feel powerless against their anxiety. They are terrified they will humiliate or embarrass themselves.The anxiety can interfere significantly with daily routines, occupational performance, or social life, making it difficult to complete school, interview and get a job, and have friendships and romantic relationships.\n\nBeing a wallflower can be considered a less-intense form of social anxiety. A person with social anxiety may feel a sense of hesitation in large crowds, and may even have a sense of panic if forced to become the center of attention. This fear may cause them to do something as minor as stand away from the center of a party, but it may also cause a major or minor anxiety attack.\n\nPeople with social anxiety disorder do not believe that their anxiety is related to a medical or physical illness or disease. This type of anxiety occurs in most social situations, especially when the person feels on display or is the center of attention. Once a person avoids almost all social and public interactions we say the person has an extreme case of social anxiety disorder, more commonly called Avoidant Personality Disorder. As you would expect, people with social anxiety disorder have an elevated rate of relationship difficulties and substance abuse.\n\nAnxiety attacks are a combination of physical and mental symptoms that are intense and overwhelming. The anxiety is, however, more than just regular nervousness. Symptoms of anxiety attacks and panic attacks mimic serious medical issues, such as:\n\nDespite their intensity, anxiety attacks are generally not life-threatening.\n\n"}
{"id": "18953129", "url": "https://en.wikipedia.org/wiki?curid=18953129", "title": "Wood economy", "text": "Wood economy\n\nThe existence of a wood economy, or more broadly, a forest economy (since in many countries a bamboo economy predominates), is a prominent matter in many developing countries as well as in many other nations with temperate climate and especially in those with low temperatures. These are generally the countries with greater forested areas. The uses of wood in furniture, buildings, bridges, and as a source of energy are widely known. Additionally, wood from trees and bushes, can be employed in a wide variety, including those produced from wood pulp, as cellulose in paper, celluloid in early photographic film, cellophane, and rayon (a substitute for silk).\n\nAt the end of their normal usage, wood products can be burnt to obtain thermal energy, or can be used as a fertilizer. The potential environmental damage that a wood economy could occasion include (problems of reduction the biodiversity due to monoculture forestry—the intensive cultivation of very few types of trees); and CO emissions. However, forests can aid in reduction of atmospheric carbon dioxide and therefore decrease global warming.\n\nThe wood economy is historically the starting point of the civilizations worldwide, since eras preceding the Paleolithic and the Neolithic. It necessarily preceded ages of metals by many millenia, as the melting of metals was possible only through the discovery of techniques to light fire (usually obtained by the scraping of two very dry wooden rods) and the building of many simple machines and rudimentary tools, as canes, club handles, bows, arrows, lances. One of the most ancient handmade articles ever found is one smoothed pricked of wood (Clacton Spear) 250,000 years old (third interglacial period), that was buried under sediments in England, at Clacton-on-Sea.\n\nSuccessive civilizations such as the Egyptians and Sumerians built sophisticated objects of furniture. Many types of furniture in ivory and valuable woods have survived to our time practically intact, because secluded in inviolated secret tombs, they were protected from decay also by the dry environment of desert. Many buildings and parts of these (above all roofs) contained elements in wood (often of oak) forming structural supports and covering; means of transport such as boats, ships; and later (with the invention of the wheel) wagons and carriages, winches, flour mills powered by water, etc.\n\nThe main source of the lumber used in the world is forests, which can be classified as virgin, semivirgin and plantations. Much timber is removed for firewood by local populations in many countries, especially in the third world, but this amount can only be estimated, with wide margins of uncertainty.\n\nIn 1998, the worldwide production of \"roundwood\" (officially counted wood not used as firewood), was about , amounting to around 45% of the wood cultivated in the world. Cut logs and branches destined to become elements for building construction accounted for approximately 55% of the world's industrial wood production. 25% became wood pulp (including wood powder and truccioli) mainly destined for the production of paper and paperboard, and approximately 20% became panels in plywood and valuable wood for furnitures and objects of common use (FAO 1998). The World's largest producer and consumer of officially accounted wood is the United States, although the country that possesses the greatest area of forest is Russia.\n\nIn the 1970s, the countries with the largest forest area were: Soviet Union (approximately 8,800,000 km²), Brazil (5,150,000 km²), Canada (4,400,000 km²), United States (3,000,000 km²), Indonesia (1,200,000 km²) and Democratic Republic of Congo (1,000,000 km²). Other countries with important production and consumption of wood usually have a low density of population in relation to their territorial extension, here we can include countries as Argentina, Chile, Finland, Poland, Sweden, Ukraine.\n\nBy 2001 the rainforest areas of Brazil were reduced by a fifth (respect of 1970), to around 4,000,000 km²; the ground cleared was mainly destined for cattle pasture—Brazil is the world's largest exporter of beef with almost 200,000,000 head of cattle. The booming Brazilian ethanol economy based upon sugar cane cultivation, is likewise reducing forests area. Canadian forest was reduced by almost 30% to 3,101,340 km² over the same period.\n\nRegarding the problem of climate change, it is known that burning forests increase CO in atmosphere, while intact virgin forest or plantations act as sinks for CO, for these reasons wood economy fights greenhouse effect. The amount of CO absorbed depends on the type of trees, lands and the climate of the place where trees naturally grow or are planted. Moreover, by night plants do not photosynthesize, and produce CO, eliminated the successive day. Paradoxically in summer oxygen created by photosynthesis in forests near to cities and urban parks, interacts with urban air pollution (from cars, etc.) and is transformed by solar beams in ozone (molecule of three oxygen atoms), that while in high atmosphere constitutes a filter against ultraviolet beams, in the low atmosphere is a pollutant, able to provoke respiratory disturbances.\n\nIn a low-carbon economy, forestry operations will be focused on low-impact practices and regrowth. Forest managers will make sure that they do not disturb soil based carbon reserves too much. Specialized tree farms will be the main source of material for many products. Quick maturing tree varieties will be grown on short rotations in order to maximize output.\n\n\nBrazil has a long tradition in the harvesting of several types of trees with specific uses. Since the 1960s, imported species of pine tree and eucalyptus have been grown mostly for the plywood and paper pulp industries. Currently high-level research is being conducted, to apply the enzymes of sugar cane fermentation to cellulose in wood, in order to obtain methanol, but the cost is much higher when compared with ethanol derived from corn costs.\n\nThere is a close relation in the forestry economy between these countries; they have many tree genera in common, and Canada is the main producer of wood and wooden items destined to the US, the biggest consumer of wood and its byproducts in the world. The water systems of the Great Lakes, Erie Canal, Hudson River and Saint Lawrence Seaway to the east coast and the Mississippi River to the central plains and Louisiana allows transportation of logs at very low costs. On the west coast, the basin of the Columbia River has plenty of forests with excellent timber.\n\nThe agency Canada Wood Council calculates that in the year 2005 in Canada, the forest sector employed 930,000 workers (1 job in every 17), making around $108 billion of value in goods and services. For many years products derived from trees in Canadian forests had been the most important export items of the country. In 2011, exports around the world totaled some $64.3 billion – the single largest contributor to Canadian trade balance.\n\nCanada is the world leader in sustainable forest management practices. Only (28% of Canadian forests) are currently managed for timber production while an estimated are protected from harvesting by the current legislation.\n\n\n\nThe species that are ideal for the many uses in this type of economy are those employed by arboriculture, that are very well known for their features and the need for certain types of ground and climates.\n\n\nIn Sweden, Finland and to an extent Norway, much of the land area is forested, the pulp and paper industry is one of the most significant industrial sectors. Chemical pulping produces an excess of energy, since the organic matter in black liquor, mostly lignin and hemicellulose breakdown products, is burned in the recovery boiler. Thus, these countries have high proportions of renewable energy use (25% in Finland, for instance). Considerable effort is directed towards increasing the value and usage of forest products by companies and by government projects.\n\nThe burning of wood is currently the largest use of energy derived from a solid fuel biomass. Wood fuel may be available as firewood (e.g. logs, bolts, blocks), charcoal, chips, sheets, pellets and sawdust. Wood fuel can be used for cooking and heating through stoves and fireplaces, and occasionally for fueling steam engines and steam turbines that generate electricity. For many centuries many types of traditional ovens were used in order to benefit from the heat generated by wood combustion. Now, more efficient and clean solutions have been developed: advanced fireplaces (with heat exchangers), wood-fired ovens, wood-burning stoves and pellet stoves, that are able to filter and separate pollutants (centrifuging ashes with rotative filters), thus eliminating many emissions, also allowing to recover a higher quantity of heat that escaped with the chimney fumes.\n\nMean energy density of Wood, was calculated at around 6–17 Megajoule/Kilogram, depending on species and moisture content.\n\nCombustion of wood is, however, linked to the production of micro-environmental pollutants, as carbon dioxide (CO), carbon monoxide (CO) (an invisible gas able to provoke irreversible saturation of blood's hemoglobine), as well as nanoparticles.\n\nIn Italy poplar has been proposed as a tree cultivated to be transformed into biofuels, because of the excellent ratio of energy extracted from its wood because of poplar's fast growing and capture of atmospheric carbon dioxide to the small amount of energy needed to cultivate, cut and transport the trees. \"Populus x canadensis\" 'I-214', grows so fast that is able to reach in diameter and heights of in ten years.\n\nCharcoal is the dark grey residue consisting of impure carbon obtained by removing water and other volatile constituents from animal and vegetation substances. Charcoal is usually produced by slow pyrolysis, the heating of wood or other substances in the absence of oxygen. Charcoal can then be used as a fuel with a higher combustion temperature.\n\nWood gas generator (gasogen): is a bulky and heavy device (but technically simple) that transforms burning wood in a mix of molecular hydrogen (H), carbon monoxide (CO), carbon dioxide (CO), molecular nitrogen (N) and water vapor (HO). This gas mixture, known as \"wood gas\", \"poor gas\" or \"syngas\" is obtained after the combustion of dry wood in a reductive environment (low in oxygen) with a limited amount of atmospheric air, at temperatures of 900° Celsius, and can fuel an internal combustion engine.\nIn the time between World War I and World War II included, because of the lack of oil, in many countries, like Italy, France, Great Britain and Sweden, several gasoline-powered cars were modified, with the addition of a wood gas generator (a \"gasogen\"), a device powered by wood, coal, or burnable waste, able to produce (and purify) gas that immediately, in the same vehicle, could power a slightly modified ICE engine of a standard car (low-compression engine). Carburetor had to be changed with an air-gas mixer). There were several setbacks, as the great reduction of maximum speed and the need to drive using low gears and wisely dosing the amount of air. In modern cars, modified with a wood gas generator, gas emissions (CO, CO and NOx) are lower to those of the same vehicle running with gasoline (keeping the same catalytic converter).\n\nMethanol (the simplest alcohol) behaves as a liquid at 25 °C, is toxic and corrosive, and in organic chemistry basic books is often called \"the spirit of wood\", since it can be obtained from wood fermentation. Rarely, when unwise wine-makers mix small chunks of wood and leaves with grapes, methanol can be found as a pollutant of the blend of water, ethanol and other substances derived from grape's fermentation.\n\nThe best way to obtain methanol from wood is through syngas (CO, CO, H) produced by the anhydrous pyrolysis of wood, a method discovered by ancient Egyptians.\n\nMethanol can be used as an oxygen-rich additive for gasoline. However, it is usually much cheaper to produce methanol from methane or from syngas. Methanol is the most important base material for industrial chemistry, where it is often used to make more complex molecules through reactions of halogenation and chemical addition reaction.\n\nThe American M1 Abrams main battle tank is powered by a gas turbine of , that it is able to function also with a mix at 50% of wood powder and biodiesel, diesel fuel or kerosene. Its advantages over turbo-diesel engine, are the small size and light weight, the lack of a radiator (which gives an advantage against the effect of gun and cannon shots and missile strikes suffered in battle). A setback is the high fuel consumption, since the turbine engine has not the ability to work at a low revolutions per minute rate, much lower than ideal, and during the march this engine consumes twice as much fuel as a modern turbo-diesel engine with intercooler and direct injection.\n\nWood is relatively light in weight, because its specific weight is less than 500 kg/m³, this is an advantage, when compared against 2,000-2,500 kg/m³ for armed concrete or 7,800 kg/m³ for steel.\n\nWood is strong, because the efficiency of wood for structural purposes has qualities that are similar to steel.\n\nWood is used to build bridges (as the Magere bridge in Amsterdam), as well as water and air mills, and microhydro generators for electricity.\n\nHardwood is used as a material in wooden houses, and other structures with a broad range of dimensions. In traditional homes wood is preferred for ceilings, doors, floorings and windows. Wooden frames were traditionally used for home ceilings, but they risk collapse during fires.\n\nThe development of energy efficient houses including the \"passive house\" has revamped the importance of wood in construction, because wood provides acoustic and thermal insulation, with much better results than concrete.\n\nIn Japan, ancient buildings, of relatively high elevation, like pagodas, historically had shown to be able to resist earthquakes of high intensity, thanks to the traditional building techniques, employing elastic joints, and to the excellent ability of wooden frames to elastically deform and absorb severe accelerations and compressive shocks.\n\nIn 2006, Italian scientists from CNR patented a building system that they called \"SOFIE\", a seven-storey wooden building, 24 meters high, built by the \"Istituto per la valorizzazione del legno e delle specie arboree\" (Ivalsa) of San Michele all'Adige. In 2007 it was tested with the hardest Japanese antiseismic test for civil structures: the simulation of Kobe's earthquake (7.2 Richter scale), with the building placed over an enormous oscillating platform belonging to the NIED-Institute, located in Tsukuba science park, near the city of Miki in Japan. This Italian project, employed very thin and flexible panels in glued laminated timber, and according to CNR researchers could brought to the construction of much more safe houses in seismic areas.\n\nOne of the most enduring materials is the lumber from virginian southern live oak and white oak, specially live oak is 60% stronger than white oak and more resistant to moisture. As an example, the main component in the structure of battle ship USS Constitution, the world's oldest commissioned naval vessel afloat (launched in 1797) is white oak.\n\nOne of the most famous crisis of a wood-based economy is what happened in Classical Greece, where trees began to disappear specially in the areas of Attica, Boeotia and Peloponnesus where indiscriminate cutting of trees for several uses, associated to drought and wildfires led to a severe lack of timber in order to build lances, shields, ships, etc. and to a slow but progressive weakening in military and naval power of the peninsular kingdoms in Greece, that were overwhelmed by Epirus and by the Kingdom of Macedon, much more fertile lands because of their rainy winters. This process arrived to the apex with the conquest of Greece by Phillip II of Macedon.\n\nThe secret weapon of the Sarissaphoros soldiers (supported by peltast javelineers), commanded by Philipp II in the Battle of Chaeronea (338 BC) and in those that followed fought by Alexander the Great (which brought to the conquest of Lesser Asia, Babylon, Persia and Egypt), was the sarissa, a type of pike, longer and stronger (5–7 m.) than the other Greek lances, obtained from the heavy and strong cornel wood.\n\nRapa Nui, best known as \"Easter Island\", is a typical example of malthusianism, specifically how the exponential growth of a populace leads to the end of a renewable resource. At a certain point, the compelling societal need forces exploitation of the resource above and beyond the resource's natural rate of renewal.\n\nIt has been calculated that after the year 1000, around 10 million palmtrees were cut in Rapa Nui, resulting in the erosion of the fertile land, and eventually to a desertification around the 15th century. (This deforestation may have also been aggravated and/or caused by a rat infestation). This provoked a population reduction from 15,000 to 2,500 individuals. Without palmtree wood, no boats, or lances could be constructed. Without palm fibers, construction of ropes and fishing nets halted. This led to a decrease in the local fish harvest, which in turn led to a decrease in the quantity of dietary protein available to the island's inhabitants. At the end, the society became an easy prey for hunger and civil war. From 1600 to 1700, the people became superstitious in a fanatical way. In the last moments, there was a disintegration of society and total chaos. The destruction of the traditional symbols followed, leading to the eventual extinction of the Moais civilization and culture, even if there was not any external human enemy.\n\n\n\n\n\n"}
