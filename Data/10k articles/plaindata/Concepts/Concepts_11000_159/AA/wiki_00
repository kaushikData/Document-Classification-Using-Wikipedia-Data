{"id": "7258068", "url": "https://en.wikipedia.org/wiki?curid=7258068", "title": "2006–08 Juba talks", "text": "2006–08 Juba talks\n\nThe Juba talks were a series of negotiations between the government of Uganda and the Lord's Resistance Army rebel group over the terms of a ceasefire and possible peace agreement. The talks, held in Juba, the capital of autonomous Southern Sudan, began in July 2006 and were mediated by Riek Machar, the Vice President of Southern Sudan. The talks, which had resulted in a ceasefire by September 2006, were described as the best chance ever for a negotiated settlement to the 20-year-old war. However, LRA leader Joseph Kony refused to sign the peace agreement in April 2008. Two months later, the LRA carried out an attack on a Southern Sudanese town, prompting the Government of Southern Sudan to officially withdraw from their mediation role.\n\nA delegation from the LRA arrived in Juba, Sudan on 8 June 2006 to prepare for talks with the Ugandan government, to be mediated by the Government of Southern Sudan and by the Community of Sant'Egidio. These talks were agreed to after Kony released a video in May in which he denied committing atrocities and seemed to call for an end to hostilities, in response to an announcement by Museveni that he would guarantee the safety of Kony if peace was agreed to by July. Museveni had pledged to grant Kony total amnesty if he gave up \"terrorism\". Uganda's security minister Amama Mbabazi urged the International Criminal Court to drop the indictments issued in 2005 against leaders of the LRA, but LRA legal adviser Krispus Ayena Odongo rejected the offer, saying that accepting amnesty \"presupposes surrender\" and would mean the LRA was no longer available for discussions. Several organizations, including the ICC and the International Bar Association's Human Rights Institute insisted that LRA leaders must be arrested in accordance with the Rome Statute. \n\nJoseph Kony gave his first interview to the press after 20 years of carrying out the conflict in late June 2006. He denied that the LRA had carried out any atrocities and blamed President Museveni for oppressing the Acholi. Regardless, in late June 2006, the Government of Southern Sudan formally invited Uganda to attend peace talks.\n\nOn 14 July 2006 talks began in Juba between delegations from the LRA and Uganda, with the Vice-president of Southern Sudan Riek Machar as the chief mediator. The leader of the Ugandan delegation, Internal Affairs Minister Ruhakana Rugunda stated that his priority was to obtain a quick ceasefire. The LRA delegation, led by Martin Ojul, said that LRA's acceptance of the peace talks should not be interpreted that LRA can no longer fight, but stressed that a negotiated settlement is the best way to end the conflict.\n\nThe initial delegation was criticized as largely consisting of expatriate Acholi, rather than members of the fighting force. However, after many delays Vincent Otti arrived for meetings on the 29th, followed the next day by Kony's 14-year-old son Salim Saleh Kony (sharing a name with brother of President Museveni, Salim Saleh). Kony himself met with local religious and political leaders from northern Uganda and southern Sudan on the following day. On 2 August, Kony held his first ever press conference in which he demanded a ceasefire before LRA-government negotiations resumed on the 7th and denied ever abducting children. Some media sources noted that, of the approximately 80 LRA fighters surrounding the press venue, several appeared to be in their early teens.\n\nThe broader context of the talks remained confused. The government of Southern Sudan views the talks as a means of ridding itself of a foreign army that is complicating their delicate relationship with the Khartoum government. The request by the Ugandan Government for ICC to suspend war crimes indictments against leaders of the LRA, condemned by international human rights groups but largely supported by leaders and civilians within northern Uganda, led some political analysts to see Ugandan Government's request as a ploy to gain local support. The comment of a George Olara, an IDP living in a camp in Pader, was fairly typical: \"He [Kony] should not be taken to The Hague. Let him come back and live with the community because this is how reconciliation will be achieved. ... Peace will come if the talks succeed, but there is the potential that they may also fail like they have done before\".\n\nOn 4 August 2006, Vincent Otti declared a unilateral ceasefire and asked the Ugandan government to reciprocate. Ugandan Internal Affairs Minister Ruhakana Rugunda stated that they were waiting to see the effect on the ground. ICC indictee Raska Lukwiya was killed in battle on 12 August 2006; the LRA asked for three days of mourning though a spokesman said that talks would continue. Ugandan President Museveni set a 12 September 2006 deadline to finalize a peace deal. The government and LRA signed a truce on 26 August 2006. Under the terms of the agreement, LRA forces were required to leave Uganda and gather in two assembly areas, where the Ugandan government promised they would not attack and the government of Southern Sudan guaranteed their safety. Once this is accomplished, talks on a comprehensive peace agreement would begin. Although a final agreement was not reached by the 12 August deadline, LRA rebels began gathering in the assembly areas and the government delegation stated that they would not hold to the deadline. Machar stated that several hundred rebels, including Otti, had gathered either at Ri-Kwangba in West Equatoria or Owiny Ki-Bul in East Equatoria.\n\nThe government also began a process of creating \"satellite camps\" to decongest the main IDP camps. In Pader, 28 satellite sites were occupied out of 48 identified as of late September 2006, while the numbers in Kitgum were 21 of 36. IDPs farther south in Teso and Lango were being encouraged to return home directly. However, talks continued to be delayed. On 23 September, the LRA delegation threatened to walk out of the negotiations, claiming that the UPDF had attacked their forces at Owiny Ki-Bul and demanding that composition of the government delegation be changed and that the ICC warrants be voided before any agreement. Uganda denied the accusation of attacks. Both delegations met with mediator Riek Machar on 25 September 2006, but not with each other.\n\nThe negotiations were paused in early October while a Cessation of Hostilities monitoring team was sent to Owiny Ki-Bul. The team found that no attack had taken place, but that the LRA had simply moved away from the designated site. The team recommended that the LRA rebels regroup at Owiny Ki-Bul, while stating that the LRA had not honored the agreement and was using hostile propaganda, that the UPDF was located close to the assembly points and that the mediators had failed to provide armed guards for the assembled rebels. On 11 October 2006, the LRA proposed that Uganda adopt a federalist structure, prompting criticism from the government spokesperson.\n\nOn 20 October 2006, Ugandan President Yoweri Museveni traveled to Juba to meet the LRA negotiators face-to-face for the first time in an attempt to revive the talks, described as \"stalled\" by \"BBC News\" and \"faltering\" by \"The Monitor\" newspaper. A Uganda government source reported that the president spoke angrily and rebuked the LRA team several times, before later referring to the LRA as \"unserious\" in a subsequent address to South Sudan government officials. A pall had been thrown over the talks by the murder of several dozen civilians, including the shooting of women and children in the head, near Juba during the two previous days. The attacks were carried out by an as-yet unnamed group, but some suspected that the LRA was responsible for the mayhem.\n\nAfter a week-long impasse, the LRA and government signed a second truce on 1 November 2006 that mandated the monitoring team until 1 December. The previous agreement had technically expired in September. As part of the agreement, the army was to withdraw from Owiny Ki-Bul, past a 30-km (18-mi) buffer zone. The LRA was given a week to regroup at Owiny Ki-Bul, and four weeks to gather at Ri-Kwangba. Both Kony and Otti refused to enter the camps, citing fear of arrest on the ICC warrants. The agreement further stated that food would not be provided to LRA units outside the assembly points except in \"exceptional circumstances\".\n\nIn one of the most significant moments for the LRA during the talks, United Nations Undersecretary-General for Humanitarian Affairs and Emergency Relief Coordinator Jan Egeland met with Kony and Otti in the hopes of pushing the talks forward on 12 November 2006. Egeland had previously stated that he would meet with Kony only if the LRA released abducted children and wounded members, but Kony denied that anyone in the LRA was either wounded or held against their will.\n\nThe LRA declared that it was withdrawing from the talks on November 30, stating that UPDF had killed three of its fighters. The deadline for the LRA fighters to finish gathering at the assembly points was 1 December. Uganda denied the charge. Also, outgoing United Nations Secretary-General Kofi Annan appointed Joaquim Chissano, former president of Mozambique, to be the UN envoy to the conflict. The truce was further extended for two more months on 18 December.\n\nOn 12 January 2007 Ojul stated that recent comments made by al-Bashir and Kiir clearly signified that the LRA was not welcome any longer in Sudan, and that further talks should occur in Kenya instead. On 14 March 2007 the LRA stated it would once again return to the Juba talks. After South Africa, Kenya and Mozambique agreed to join the peace talks (a demand the LRA had made before it would return to Juba), the next round of talks was held from 13 April to 14 April 2007. In this round, the ceasefire was extended until 30 June 2007, and Ri Kwangba was the agreed upon assembly point. The next round of talks was set for 25 April 2007.\n\nFollowing this suspension in the peace talks, the Juba Initiative Project enabled the resumption of the talks in May 2007, thanks to the efforts of the Special Representative of the Secretary-General for LRA-affected areas Joaquim Chissano. The talks were again mediated by the Government of Southern Sudan, but with the support of the United Nations and logistic facilitation from the Office for the Coordination of Humanitarian Affairs (OCHA), under the leadership of her local head Eliane Duthoit.\n\nOn 29 June 2007, the sides agreed to the principles of how justice and reconciliation will be handled, the third of the five-point agenda. The LRA and government agreed that both formal justice procedures and the traditional Mato Oput ceremony of reconciliation would play a role. Government delegation spokesperson Barigye Ba-Hoku stated that they would attempt to convince the ICC that this would address their concerns about impunity and that arrests under ICC auspices would not be necessary. In November 2007, an LRA delegation led by Martin Ojul journeyed to Kampala to restate their commitment to a peaceful resolution of the conflict. Ojul later led the delegation on a tour of northern Uganda to meet victims of the insurgency and ask their forgiveness. However, reports surfaced that LRA deputy commander Otti had been executed on or around 8 October 2007 over an internal power struggle with Kony.\n\nOn 20 December 2007 the government set an ultimatum for the peace talks to conclude by 31 January 2008, threatening that a new military offensive otherwise. The death of Vincent Otti, confirmed in mid-January 2008, was reported to threaten the success of the talks. Talks resumed on 30 January 2008, and the ceasefire was extended until 29 February 2008. The European Union and the United States joined the negotiations, increasing the number of observers to eight.\n\nA breakthrough in negotiations was reached on 3 February 2008 regarding accountability and reconciliation. A deal was signed on 19 February 2008 which decided that the war crimes would be tried in a special section of the High Court of Uganda, thus bypassing the International Criminal Court and also removing one of the last obstacles to a final peace deal. On 22 February 2008, the rebels walked out of the talks again after being denied senior government posts. However, shortly thereafter they signed another breakthrough agreement according to which they \"would be considered for government and army posts\", but not automatically appointed. A permanent ceasefire to come into effect 24 hours after the signing a comprehensive peace treaty (expected by 29 February 2008) was agreed upon on 23 February 2008.\n\nMore problems appeared on 28 February 2008: The rebels demand a retraction of the ICC indictments, but the Ugandan government only wants to ask the UN to do that after the rebels have demobilised. An accord on Disarmament, Demobilisation and Reintegration was signed late on 29 February 2008, leaving the signing of the peace treaty itself as the last missing action.\n\nThe truce was extended until 28 March 2008, and the final peace talks will continue on 12 March 2008. The ICC prosecutor-general Luis Moreno-Ocampo on 5 March 2008 rejected demands by the rebels for a meeting, stating that \"arrest warrants issued by the court... remain in effect and have to be executed\". it was reported that rebel leader Kony would nonetheless come out of the bush to sign the peace agreement on 28 March 2008, with the implicit agreement that he will not be apprehended and transferred to the ICC while out in the open; such an action was also thought to likely see a remobilisation of his rebel army. Furthermore, it was suggested that Uganda should lobby at the United Nations Security Council to suspend the ICC indictments for a year.\n\nOn 12 March 2008, as final talks were set to continue, the ICC inquired as to the precise definition of the powers of the proposed intra-Ugandan war crimes court section, in a move seen as softening the indictments on the LRA rebels. The final signing of the peace deal was delayed on 26 March 2008 from 28 March 2008 to 3 April 2008; while the ceasefire was not formally extended with this deadline, both parties were expected to continue adhering to it. The signing was then further delayed to 5 April 2008. It was later announced that Kony would sign the deal in the bush two days before that. However, this was postponed to 10 April 2008; reportedly, Kony was suffering from diarrhoea. \n\nThe ICC inquired as to the precise nature of the special courts in Uganda. Kony delayed the signing of the final treaty further on 10 April 2008, reportedly asking for more information about what kind of punishments he could face. He later clarified that he wanted to know further details about how \"mato-oput\", the Acholi traditional justice, would be used, and how exactly the special division of the High Court would work; he then suspended the peace talks and appointed a new negotiating team, claiming to \"have been misled\". Specifically, Kony fired chief LRA negotiator David Nyakorach Matsanga and replaced him with James Obita. Kony subsequently failed to show up at Nabanga to sign the treaty.\n\nThe government subsequently stated that they would return to Juba and Kampala, as the LRA had broken the agreement, and that the ceasefire agreement would not be extended. The next steps of both sides are unclear. Diplomats unsuccessfully tried to restart the talks on 26 April 2008. On 26 May 2008 the government set up a special war crimes court with the mandate to try the LRA in an attempt to convince the ICC to withdraw its indictments against LRA leaders.\n\nSince April 2008, the LRA had begun rearming and abducting recruits, with the \"BBC\" stating that 1000 new abductees had been added to the 600 old LRA fighters by June. Lord's Resistance Army negotiator James Obita stated that on 4 June 2008, the Southern Sudan army attacked an LRA encampment killing two, though this is unconfirmed. On 5 June 2008, the LRA attacked the Sudan People's Liberation Army/Movement (SPLA) camp at Nabanga, killing 21, seven soldiers and 14 civilians, before killing a local chief in the nearby village of Yamba. The LRA fighters burnt the SPLA camp before returning to Ri-Kwangba. This occurred as Kony reappointed Matsanga as chief negotiator. Matsanga claimed on 6 June to have contacted UN Envoy Joaquim Chissano to revive the talks; Chissano subsequently arrived in Kampala for talks with President Museveni on 7 June. However, the Government of Southern Sudan announced on 8 June that they would no longer mediate, with Information Minister Gabriel Changson Cheng noting that there were multiple reasons for the decision, including the recent attack and the apparent lack of interest in the peace process on the part of the Ugandan government. The governments of the nations in which the LRA is active met earlier in the week and all suggested military action.\n\nIn December 2008 the United Nations Security Council agreed with a Joaquim Chissano's recommendation that the peace efforts should continue will continuing to support the ICC arrest warrants.\n\n"}
{"id": "1674987", "url": "https://en.wikipedia.org/wiki?curid=1674987", "title": "Abstract index notation", "text": "Abstract index notation\n\nAbstract index notation is a mathematical notation for tensors and spinors that uses indices to indicate their types, rather than their components in a particular basis. The indices are mere placeholders, not related to any basis and, in particular, are non-numerical. Thus it should not be confused with the Ricci calculus. The notation was introduced by Roger Penrose as a way to use the formal aspects of the Einstein summation convention to compensate for the difficulty in describing contractions and covariant differentiation in modern abstract tensor notation, while preserving the explicit covariance of the expressions involved.\n\nLet \"V\" be a vector space, and \"V\" its dual. Consider, for example, an order-2 covariant tensor \nformula_1. Then \"h\" can be identified with a bilinear form on \"V\". In other words, it is a function of two arguments in \"V\" which can be represented as a pair of \"slots\":\n\nAbstract index notation is merely a \"labelling\" of the slots with Latin letters, which have no significance apart from their designation as labels of the slots (i.e., they are non-numerical):\n\nA tensor contraction (or trace) between two tensors is represented by the repetition of an index label, where one label is contravariant (an \"upper index\" corresponding to a tensor in \"V\") and one label is covariant (a \"lower index\" corresponding to a tensor in \"V\"). Thus, for instance,\n\nis the trace of a tensor \"t\" = \"t\" over its last two slots. This manner of representing tensor contractions by repeated indices is formally similar to the Einstein summation convention. However, as the indices are non-numerical, it does not imply summation: rather it corresponds to the abstract basis-independent trace operation (or duality pairing) between tensor factors of type \"V\" and those of type \"V\".\n\nA general homogeneous tensor is an element of a tensor product of copies of \"V\" and \"V\", such as\n\nLabel each factor in this tensor product with a Latin letter in a raised position for each contravariant \"V\" factor, and in a lowered position for each covariant \"V\" position. In this way, write the product as\n\nor, simply\n\nThe last two expressions denote the same object as the first. Tensors of this type are denoted using similar notation, for example:\n\nIn general, whenever one contravariant and one covariant factor occur in a tensor product of spaces, there is an associated \"contraction\" (or \"trace\") map. For instance,\n\nis the trace on the first two spaces of the tensor product.\n\nis the trace on the first and last space.\n\nThese trace operations are signified on tensors by the repetition of an index. Thus the first trace map is given by\n\nand the second by\n\nTo any tensor product on a single vector space, there are associated braiding maps. For example, the braiding map\ninterchanges the two tensor factors (so that its action on simple tensors is given by formula_14). In general, the braiding maps are in one-to-one correspondence with elements of the symmetric group, acting by permuting the tensor factors. Here, we use formula_15 to denote the braiding map associated to the permutation formula_16 (represented as a product of disjoint cyclic permutations).\n\nBraiding maps are important in differential geometry, for instance, in order to express the Bianchi identity. Here let formula_17 denote the Riemann tensor, regarded as a tensor in formula_18. The first Bianchi identity then asserts that\n\nAbstract index notation handles braiding as follows. On a particular tensor product, an ordering of the abstract indices is fixed (usually this is a lexicographic ordering). The braid is then represented in notation by permuting the labels of the indices. Thus, for instance, with the Riemann tensor\nthe Bianchi identity becomes\n\nA general tensor may be antisymmetrized or symmetrized, and there is according notation.\n\nWe demonstrate the notation by example. Let's antisymmetrize the type-(0,3) tensor formula_22, where formula_23 is the symmetric group on three elements.\n\nformula_24\n\nSimilarly, we may symmetrize:\n\nformula_25\n\n\n"}
{"id": "36937107", "url": "https://en.wikipedia.org/wiki?curid=36937107", "title": "Ageneros", "text": "Ageneros\n\nAgeneros, Aristotle's \"On the Heavens\", used when referring to a universe that has been created in his work. It means \"never existed\" or \"never created\". It is a Greek concept.\n\n"}
{"id": "8836159", "url": "https://en.wikipedia.org/wiki?curid=8836159", "title": "Alexander Braun", "text": "Alexander Braun\n\nHe studied botany in Heidelberg, Paris and Munich. In 1833 he began teaching botany at the Polytechnic School of Karlsruhe, staying there until 1846. Afterwards he was a professor of botany in Freiburg (from 1846), Giessen (from 1850) and at the University of Berlin (1851), where he remained until 1877. While in Berlin, he was also director of the botanical garden. In 1852, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nBraun is largely known for his research involving plant morphology. He accepted evolution but was a critic of Darwinism. He was a proponent of vitalism, a popular 19th century speculative theory that claimed that a regulative force existed within living matter in order to maintain functionality. Despite these beliefs, Braun made important contributions in the field of cell theory.\n\nFrom his 1830s analysis of the arrangement of scales on a pine cone he was a pioneer of phyllotaxis.\n\nIn 1877 Wilhelm Philippe Schimper and Philipp Bruch named the plant genus \"Braunia\" in his honor. Also, a decorative plant known as \"Braun's holly fern\" (\"Polystichum braunii\") commemorates his name.\n\n\n\n\n"}
{"id": "48615302", "url": "https://en.wikipedia.org/wiki?curid=48615302", "title": "BF-algebra", "text": "BF-algebra\n\nIn mathematics, BF algebras are a class of algebraic structures arising out of a symmetric \"Yin Yang\" concept for Bipolar Fuzzy logic, the name was introduced by Andrzej Walendziak in 2007. The name covers discrete versions, but a canonical example arises in the BF space [-1,0]x[0,1] of pairs of (false-ness, truth-ness).\n\nA BF-algebra is a non-empty subset formula_1 with a constant formula_2 and a binary operation formula_3 satisfying the following:\n\nLet formula_7 be the set of integers and 'formula_8' be the binary operation 'subtraction'. Then the algebraic structure formula_9 obeys the following properties:\n"}
{"id": "39193418", "url": "https://en.wikipedia.org/wiki?curid=39193418", "title": "Carnism", "text": "Carnism\n\nCarnism is a concept used in discussions of humanity's relation to other animals, defined as a prevailing ideology in which people support the use and consumption of animal products, especially meat. Carnism is presented as a dominant belief system supported by a variety of defense mechanisms and mostly unchallenged assumptions. The term \"carnism\" was coined by social psychologist and author Melanie Joy in 2001 and popularized by her book \"Why We Love Dogs, Eat Pigs, and Wear Cows\" (2009).\n\nCentral to the ideology, according to the theory, is the acceptance of meat-eating as \"natural\", \"normal\", \"necessary\", and (sometimes) \"nice\". An important feature of carnism is the classification of only particular species of animal as food, and the acceptance of practices toward those animals that would be rejected as unacceptable cruelty if applied to other species. This classification is culturally relative, so that, for example, dogs are eaten by some people in Korea but may be pets in the West, while cows are eaten in the West but protected in much of India.\n\nAnalyzing the history of vegetarianism and opposition to it from ancient Greece to the present day, literary scholar Renan Larue found certain commonalities in what he described as carnist arguments. According to him, carnists typically held that vegetarianism is a ludicrous idea unworthy of attention, that mankind is invested with dominion over animals by divine authority, and that abstaining from violence against animals would pose a threat to humans. He found that the views that farmed animals do not suffer, and that slaughter is preferable to death by disease or predation, gained currency in the nineteenth century, but that the former had precedent in the writings of Porphyry, a vegetarian who advocated the humane production of animal products which do not require animals to be slaughtered, such as wool.\n\nIn the 1970s traditional views on the moral standing of animals were challenged by animal rights advocates, including psychologist Richard Ryder, who in 1971 introduced the notion of speciesism. This is defined as the assignment of value and rights to individuals solely on the basis of their species membership. In 2001 psychologist and animal rights advocate Melanie Joy coined the term \"carnism\" for a form of speciesism that she argues underpins using animals for food, and particularly killing them for meat. Joy compares carnism to patriarchy, arguing that both are dominant normative ideologies that go unrecognized because of their ubiquity:\n\nSandra Mahlke argues that carnism is the \"central crux of speciesism\" because the eating of meat motivates ideological justification for other forms of animal exploitation.\n\nA central aspect of carnism is that animals are categorized as edible, inedible, pets, vermin, predators, or entertainment animals, according to people's schemata – mental classifications that determine, and are determined by, our beliefs and desires. There is cultural variability regarding which animals count as food. Dogs are eaten in China, and South Korea, but elsewhere are not viewed as food, either because they are loved or, as in the Middle East and parts of India, regarded as unclean. Cows are eaten in the West, but revered in much of India. Pigs are rejected by Muslims and Jews but widely regarded by other groups as edible. Joy and other psychologists argue that these taxonomies determine how the animals within them are treated, influence subjective perceptions of their sentience and intelligence, and reduce or increase empathy and moral concern for them.\n\nJeff Mannes writes that carnism is rooted in a paradox between most people's values and actions: they oppose harming animals, and yet eat them. He argues that this conflict leads to cognitive dissonance, which people attempt to attenuate through psychic numbing. The apparent conflict between caring about animals and embracing diets which require them to be harmed has been termed the \"meat paradox\".\n\nThere is experimental evidence supporting the idea that the meat paradox induces cognitive dissonance in Westerners. Westerners are more willing to eat animals which they regard as having lesser mental capacities and moral standing, and conversely, to attribute lesser mental faculties and moral standing to animals which are eaten. Furthermore, the relationship is causative: the categorization of animals as food or not affects people's perception of their mental characteristics, and the act of eating meat itself causes people to attribute diminished mental capacity to animals. For example, in one study people rated an unfamiliar exotic animal as less intelligent if they were told native people hunted it, and in another they regarded cows as less intelligent after eating beef jerky.\n\nAvoiding consideration of the provenance of animal products is another strategy. Joy argues that this is why meat is rarely served with the animal's head or other intact body parts.\n\nJoy introduced the idea of the \"Three Ns of Justification\", writing that meat-eaters regard meat consumption as \"normal, natural, and necessary\". She argues that the \"Three Ns\" have been invoked to justify other ideologies, including slavery and denying women the right to vote, and are widely recognized as problematic only after the ideology they support has been dismantled.\n\nThe argument holds that people are conditioned to believe that humans evolved to eat meat, that it is expected of them, and that they need it to survive or be strong. These beliefs are said to be reinforced by various institutions, including religion, family and the media. Although scientists have shown that humans get more than enough protein in their diets without eating meat, the belief that meat is required persists.\n\nBuilding on Joy's work, psychologists conducted a series of studies in the United States and Australia, published in 2015, that found the great majority of meat-eaters' stated justifications for consuming meat were based on the \"Four Ns\" – \"natural, normal, necessary, and nice\". The arguments were that humans are omnivores (\"natural\"), that most people eat meat (\"normal\"), that vegetarian diets are lacking in nutrients (\"necessary\"), and that meat tastes good (\"nice\").\n\nMeat-eaters who endorsed these arguments more strongly reported less guilt about their dietary habits. They tended to objectify animals, have less moral concern for them and attribute less consciousness to them. They were also more supportive of social inequality and hierarchical ideologies, and less proud of their consumer choices.\n\nAn illustration of dissonance reduction is the prominence given to \"saved from slaughter\" stories, in which the media focus on one animal that evaded slaughter, while ignoring the millions that did not. Joy wrote that this dichotomy is characteristic of carnism.\n\nAnimals at the center of these narratives include Wilbur in \"Charlotte's Web\" (1952); the eponymous and fictional star of \"Babe\" (1995); Christopher Hogwood in Sy Montgomery's \"The Good, Good Pig\" (2006); the Tamworth Two; and Cincinnati Freedom. The American National Thanksgiving Turkey Presentation is cited as another example. A 2012 study found that most media reporting on it celebrated the poultry industry while marginalizing the link between living animals and meat.\n\nOpinion pieces in \"The Huffington Post\", \"The Statesman\", and \"The Drum\" praised the idea, saying the term made it easier to discuss, and challenge, the practices of animal exploitation.\nAn article in the beef industry publication Drovers Cattle Network criticized the use of the term, saying it implied that eating animal foods was a \"psychological sickness\".\n\n\n"}
{"id": "28267626", "url": "https://en.wikipedia.org/wiki?curid=28267626", "title": "Categorical quantum mechanics", "text": "Categorical quantum mechanics\n\nCategorical quantum mechanics is the study of quantum foundations and quantum information using paradigms from mathematics and computer science, notably monoidal category theory. The primitive objects of study are physical processes, and the different ways that these can be composed. It was pioneered in 2004 by Abramsky and Coecke.\n\nMathematically, the basic setup is captured by a dagger symmetric monoidal category: composition of morphisms models sequential composition of processes, and the tensor product describes parallel composition of processes. The role of the dagger is to assign to each state a corresponding test. These can then be adorned with more structure to study various aspects. For instance:\n\n\nA substantial portion of the mathematical backbone to this approach is drawn from Australian category theory, most notably from work by Kelly and Laplaza, Joyal and Street, Carboni and Walters, and Lack.\n\nOne of the most notable features of categorical quantum mechanics is that the compositional structure can be faithfully captured by a purely diagrammatic calculus.\n\nThese diagrammatic languages can be traced back to Penrose graphical notation, developed in the early 1970s. Diagrammatic reasoning has been used before in quantum information science in the quantum circuit model, however, in categorical quantum mechanics primitive gates like the CNOT-gate arise as composites of more basic algebras, resulting in a much more compact calculus. In particular, the ZX-calculus has sprang forth from categorical quantum mechanics as a diagrammatic counterpart to conventional linear algebraic reasoning about quantum gates. The ZX-calculus consists of a set of generators representing the common Pauli quantum gates and the Hadamard gate equipped with a set of graphical rewrite rules governing their interaction. Although a standard set of rewrite rules has not yet been established some versions have been proven to be \"complete\", meaning that any equation that holds between two quantum circuits represented as diagrams can be proven using the rewrite rules. The ZX-calculus has been used to study for instance Measurement Based Quantum Computing.\n\nOne of the main successes of the categorical quantum mechanics research program is that from seemingly weak abstract constraints on the compositional structure, it turned out to be possible to derive many quantum mechanical phenomena. In contrast to earlier axiomatic approaches which aimed to reconstruct Hilbert space quantum theory from reasonable assumptions, this attitude of not aiming for a complete axiomatization may lead to new interesting models that describe quantum phenomena, which could be of use when crafting future theories.\n\nThere are several theorems relating the abstract setting of categorical quantum mechanics to traditional settings for quantum mechanics.\n\n\nCategorical quantum mechanics can also be seen as a type theoretic form of quantum logic that, in contrast to traditional quantum logic, supports formal deductive reasoning. There exists software that supports and automates this reasoning.\n\nThere is another connection between categorical quantum mechanics and quantum logic as subobjects in dagger kernel categories and dagger complemented biproduct categories form orthomodular lattices. In fact, the former setting allows logical quantifiers, the existence of which was never satisfactorily addressed in traditional quantum logic.\n\nCategorical quantum mechanics allows a description of more general theories than quantum theory. This enables one to study which features single out quantum theory in contrast to other non-physical theories, hopefully providing some insight in the nature of quantum theory. For example, the framework allows a succinct compositional description of Spekkens' Toy Theory that allows one to pinpoint which structural ingredient causes it to be different from quantum theory.\n"}
{"id": "9117779", "url": "https://en.wikipedia.org/wiki?curid=9117779", "title": "Codework", "text": "Codework\n\nCodework is the mixing of Alan Sondheim's writing with the code of various computer languages. The word 'codework' was originally created and given its name by Sondheim, but also popularized by other Internet artists such as Mez Breeze. It is also sometimes referred to as net.writing. It is said to have been inspired by the eclectic poetry of e.e.cummings.\n\nCodework has been used for many forms of writing, mostly poetry and fiction. Duc Thuan's Days of the Java Moon is an example of fiction in the codework style. For example:\n"}
{"id": "27477352", "url": "https://en.wikipedia.org/wiki?curid=27477352", "title": "Common Weakness Enumeration", "text": "Common Weakness Enumeration\n\nThe Common Weakness Enumeration (CWE) is a category system for software weaknesses and vulnerabilities. It is sustained by a community project with the goals of understanding flaws in software and creating automated tools that can be used to identify, fix, and prevent those flaws. The project is sponsored by the National Cybersecurity FFRDC, which is owned by The MITRE Corporation, with support from US-CERT and the National Cyber Security Division of the U.S. Department of Homeland Security.\n\nVersion 3.0 of the CWE standard was released in November 2017.\n\nCWE has over 600 categories, including classes for buffer overflows, path/directory tree traversal errors, race conditions, cross-site scripting, hard-coded passwords, and insecure random numbers.\n\n\nCommon Weakness Enumeration (CWE) Compatibility program allows a service or a product to be reviewed and registered as officially \"CWE-Compatible\" and \"CWE-Effective\". The program assists organizations in selecting the right software tools and learning about possible weaknesses and their possible impact.\n\nIn order to obtain CWE Compatible status a product or a service must meet 4 out of 6 requirements, shown below:\n\nThere are 48 organizations as of January 2018 that develop and maintain products and services that achieved CWE Compatible status.\n\nSome researchers think that ambiguities in CWE can be avoided or reduced.\n\n\n"}
{"id": "14575050", "url": "https://en.wikipedia.org/wiki?curid=14575050", "title": "Diango Hernández", "text": "Diango Hernández\n\nDiango Hernández is a Cuban artist.\n\nHernandez was born in 1970 in Sancti Spíritus, Cuba. He earned a degree in industrial design at the Havana Superior Institute of Design (ISDI). After graduation in 1994 he began a collaboration experience together with Ernesto Oroza, Juan Bernal, Francis Acea and Manuel Piña under the name of Ordo Amoris Cabinet after the Latin words for \"order\" and \"love\". From 1996 until 2003 Ordo Amoris Cabinet was only formed by Hernández and Francis Acea. The duo's sculptural installations were exhibited throughout Cuba, Europe, and North America. Hernández’s works has been included at the 51st Venice Art Biennale (Arsenale, 2005), São Paulo Art Biennial (2006), Biennale of Sydney (2006), Kunsthalle Basel (2006), Munich’s Haus der Kunst (2010), London’s Hayward Gallery (2010) and more recently with a survey at the MART in Rovereto, Italy, (2011). Hernández currently lives and works in Düsseldorf, Germany.\n"}
{"id": "570152", "url": "https://en.wikipedia.org/wiki?curid=570152", "title": "Egalitarian community", "text": "Egalitarian community\n\nEgalitarian communities are groups of people who have chosen to live together, with egalitarianism as one of their core values. A broad definition of egalitarianism is \"equal access to resources and to decision-making power.\" For example, decision-making is done by consensus or another system in which each person has a voice; it is not done hierarchically with only one or a few people making choices that will affect the whole group. If the group shares assets (income, vehicles, etc.), they are distributed equitably throughout the group, and each member has access to more-or-less the same resources as any other member. Egalitarian communities are a type of commune (some communal groups are not egalitarian in nature).\n\nAn \"egalitarian decision\" is a decision made by a group as opposed to a single individual. The decision may be made by committee or elected members but still is an egalitarian decision.\n\nThe Federation of Egalitarian Communities is a network of communal groups in North America with values including egalitarianism, non-violence, income-sharing and cooperation.\n\n\n"}
{"id": "9236", "url": "https://en.wikipedia.org/wiki?curid=9236", "title": "Evolution", "text": "Evolution\n\nEvolution is change in the heritable characteristics of biological populations over successive generations. These characteristics are the expressions of genes that are passed on from parent to offspring during reproduction. Different characteristics tend to exist within any given population as a result of mutation, genetic recombination and other sources of genetic variation. Evolution occurs when evolutionary processes such as natural selection (including sexual selection) and genetic drift act on this variation, resulting in certain characteristics becoming more common or rare within a population. It is this process of evolution that has given rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms and molecules.\n\nThe scientific theory of evolution by natural selection was proposed by Charles Darwin and Alfred Russel Wallace in the mid-19th century and was set out in detail in Darwin's book \"On the Origin of Species\" (1859). Evolution by natural selection was first demonstrated by the observation that more offspring are often produced than can possibly survive. This is followed by three observable facts about living organisms: 1) traits vary among individuals with respect to their morphology, physiology and behaviour (phenotypic variation), 2) different traits confer different rates of survival and reproduction (differential fitness) and 3) traits can be passed from generation to generation (heritability of fitness). Thus, in successive generations members of a population are more likely to be replaced by the progenies of parents with favourable characteristics that have enabled them to survive and reproduce in their respective environments. In the early 20th century, other competing ideas of evolution such as mutationism and orthogenesis were refuted as the modern synthesis reconciled Darwinian evolution with classical genetics, which established adaptive evolution as being caused by natural selection acting on Mendelian genetic variation.\n\nAll life on Earth shares a last universal common ancestor (LUCA) that lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite, to microbial mat fossils, to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis) and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct phylogenetic trees.\n\nEvolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but numerous other scientific and industrial fields, including agriculture, medicine and computer science.\n\nThe proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork \"De rerum natura\" (\"On the Nature of Things\").\n\nIn contrast to these materialistic views, Aristotelianism considered all natural things as actualisations of fixed natural possibilities, known as forms. This was part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.\n\nIn the 17th century, the new method of modern science rejected the Aristotelian approach. It sought explanations of natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences, the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species,\" to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.\n\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's \"transmutation\" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the \"Natural Theology or Evidences of the Existence and Attributes of the Deity\" (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.\n\nThe crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin in terms of variable populations. Partly influenced by \"An Essay on the Principle of Population\" (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his \"abstract\" as \"On the Origin of Species\" explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.\n\nThe mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.\n\nIn the 1920s and 1930s the so-called modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that applied generally to any branch of biology. The modern synthesis explained patterns observed across species in populations, through fossil transitions in palaeontology, and complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution,\" because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.\n\nSince then, the modern synthesis has been further extended to explain biological phenomena across the full and integrative scale of the biological hierarchy, from genes to species. One extension, known as evolutionary developmental biology and informally called \"evo-devo,\" emphasises how changes between generations (evolution) acts on patterns of change within individual organisms (development). Since the beginning of the 21st century and in light of discoveries made in recent decades, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability.\n\nEvolution in organisms occurs through changes in heritable traits—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.\n\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. These traits come from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\n\nHeritable traits are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specify the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by quantitative trait loci (multiple interacting genes).\n\nRecent findings have confirmed important examples of heritable changes that cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.\n\nAn individual organism's phenotype results from both its genotype and the influence from the environment it has lived in. A substantial part of the phenotypic variation in a population is caused by genotypic variation. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.\n\nNatural selection will only cause evolution if there is enough genetic variation in a population. Before the discovery of Mendelian genetics, one common hypothesis was blending inheritance. But with blending inheritance, genetic variance would be rapidly lost, making evolution by natural selection implausible. The Hardy–Weinberg principle provides the solution to how variation is maintained in a population with Mendelian inheritance. The frequencies of alleles (variations in a gene) will remain constant in the absence of selection, mutation, migration and genetic drift.\n\nVariation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is identical in all individuals of that species. However, even relatively small differences in genotype can lead to dramatic differences in phenotype: for example, chimpanzees and humans differ in only about 5% of their genomes.\n\nMutations are changes in the DNA sequence of a cell's genome. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. Based on studies in the fly \"Drosophila melanogaster\", it has been suggested that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70% of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial.\n\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.\n\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA.\n\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions. When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to one hundred independent domains that each catalyse one step in the overall process, like a step in an assembly line.\n\nIn asexual organisms, genes are inherited together, or \"linked\", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.\n\nThe two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. (This cost does not apply to hermaphroditic species, like most plants and many invertebrates.) The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment.\n\nGene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.\n\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast \"Saccharomyces cerevisiae\" and the adzuki bean weevil \"Callosobruchus chinensis\" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.\n\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.\n\nFrom a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms, for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, genetic hitchhiking, mutation and gene flow.\n\nEvolution by means of natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It has often been called a \"self-evident\" mechanism because it necessarily follows from three simple facts:\n\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Consequences of selection include nonrandom mating and genetic hitchhiking.\n\nThe central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.\n\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele will become more common within the population. These traits are said to be \"selected \"for\".\" Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele becoming rarer—they are \"selected \"against\".\" Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form (see Dollo's law). However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.\n\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.\n\nA special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.\n\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system...\" Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\n\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation, as discussed below.\n\nIn addition to being a major source of variation, mutation may also function as a mechanism of evolution when there are different probabilities at the molecular level for different mutations to occur, a process known as mutation bias. If two genotypes, for example one with the nucleotide G and another with the nucleotide A in the same position, have the same fitness, but mutation from G to A happens more often than mutation from A to G, then genotypes with A will tend to evolve. Different insertion vs. deletion mutation biases in different taxa can lead to the evolution of different genome sizes. Developmental or mutational biases have also been observed in morphological evolution. For example, according to the phenotype-first theory of evolution, mutations can eventually cause the genetic assimilation of traits that were previously induced by the environment.\n\nMutation bias effects are superimposed on other processes. If selection would favour either one out of two mutations, but there is no extra advantage to having both, then the mutation that occurs the most frequently is the one that is most likely to become fixed in a population. Mutations leading to the loss of function of a gene are much more common than mutations that produce a new, fully functional gene. Most loss of function mutations are selected against. But when selection is weak, mutation bias towards loss of function can affect evolution. For example, pigments are no longer useful when animals live in the darkness of caves, and tend to be lost. This kind of loss of function can occur because of mutation bias, and/or because the function had a cost, and once the benefit of the function disappeared, natural selection leads to the loss. Loss of sporulation ability in \"Bacillus subtilis\" during laboratory evolution appears to have been caused by mutation bias, rather than natural selection against the cost of maintaining sporulation ability. When there is no selection for loss of function, the speed at which loss evolves depends more on the mutation rate than it does on the effective population size, indicating that it is driven more by mutation bias than by genetic drift. In parasitic organisms, mutation bias leads to selection pressures as seen in \"Ehrlichia\". Mutations are biased towards antigenic variants in outer-membrane proteins.\n\nGenetic drift is the random fluctuations of allele frequencies within a population from one generation to the next. When selective forces are absent or relatively weak, allele frequencies are equally likely to \"drift\" upward or downward at each successive generation because the alleles are subject to sampling error. This drift halts when an allele eventually becomes fixed, either by disappearing from the population or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that began with the same genetic structure to drift apart into two divergent populations with different sets of alleles.\n\nThe neutral theory of molecular evolution proposed that most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. Hence, in this model, most genetic changes in a population are the result of constant mutation pressure and genetic drift. This form of the neutral theory is now largely abandoned, since it does not seem to fit the genetic variation seen in nature. However, a more recent and better-supported version of this model is the nearly neutral theory, where a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other alternative theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.\n\nThe time for a neutral allele to become fixed by genetic drift depends on population size, with fixation occurring more rapidly in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.\n\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.\n\nRecombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.\n\nGene flow involves the exchange of genes between populations and between species. The presence or absence of gene flow fundamentally changes the course of evolution. Due to the complexity of organisms, any two completely isolated populations will eventually evolve genetic incompatibilities through neutral processes, as in the Bateson-Dobzhansky-Muller model, even if both populations remain essentially identical in terms of their adaptation to the environment.\n\nIf genetic differentiation between populations develops, gene flow between populations can introduce traits or alleles which are disadvantageous in the local population and this may lead to organisms within these populations evolving mechanisms that prevent mating with genetically distant populations, eventually resulting in the appearance of new species. Thus, exchange of genetic information between individuals is fundamentally important for the development of the \"Biological Species Concept\" (BSC).\n\nDuring the development of the modern synthesis, Sewall Wright developed his shifting balance theory, which regarded gene flow between partially isolated populations as an important aspect of adaptive evolution. However, recently there has been substantial criticism of the importance of the shifting balance theory.\n\nEvolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed.\n\nThese outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction; whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation. In general, macroevolution is regarded as the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.\n\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to modern evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.\n\nAdaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term \"adaptation\" for the evolutionary process and \"adaptive trait\" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:\n\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria \"Escherichia coli\" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, \"Flavobacterium\" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium \"Sphingobium\" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).\n\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.\n\nDuring evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.\n\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard \"Holaspis guentheri\", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.\n\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.\n\nInteractions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.\n\nNot all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.\n\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.\n\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the \"helping\" individual contains alleles which promote the helping activity, it is likely that its kin will \"also\" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.\n\nSpeciation is the process where a species diverges into two or more descendant species.\n\nThere are multiple ways to define the concept of \"species.\" The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The \"Biological Species Concept\" (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\" Despite its wide and long-term use, the BSC like others is not without controversy, for example because these concepts cannot be applied to prokaryotes, and this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.\n\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.\n\nSpeciation has been observed multiple times under both controlled laboratory conditions (see laboratory experiments of speciation) and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.\n\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.\n\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass \"Anthoxanthum odoratum\", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.\n\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.\n\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species \"Arabidopsis thaliana\" and \"Arabidopsis arenosa\" crossbred to give the new species \"Arabidopsis suecica\". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.\n\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.\n\nExtinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future. Despite the estimated extinction of more than 99 percent of all species that ever lived on Earth, about 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.\n\nThe Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. Commenting on the Australian findings, Stephen Blair Hedges wrote, \"If life arose relatively quickly on Earth, then it could be common in the universe.\" In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nMore than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80 percent not yet described.\n\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.\n\nAll organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits and finally, that organisms can be classified using these similarities into a hierarchy of nested groups—similar to a family tree. However, modern research has suggested that, due to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree since some genes have spread independently between distantly related species.\n\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, paleontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\n\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.\n\nProkaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6–2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.\n\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.\n\nSoon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.\n\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.\n\nConcepts and models used in evolutionary biology, such as natural selection, have many applications.\n\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.\n\nUnderstanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.\n\nEvolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.\n\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.\n\nIn the 19th century, particularly after the publication of \"On the Origin of Species\" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.\n\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of \"Vestiges of the Natural History of Creation\" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\n\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 \"Epperson v. Arkansas\" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 \"Kitzmiller v. Dover Area School District\" case.\n\nIntroductory reading\n\nAdvanced reading\n\n"}
{"id": "981631", "url": "https://en.wikipedia.org/wiki?curid=981631", "title": "Failure mode and effects analysis", "text": "Failure mode and effects analysis\n\nFailure mode and effects analysis (FMEA)—also \"failure modes\", plural, in many publications—was one of the first \"highly structured, systematic\" techniques for failure analysis. It was developed by reliability engineers in the late 1950s to study problems that might arise from malfunctions of military systems. An FMEA is often the first step of a system reliability study. It involves reviewing as many components, assemblies, and subsystems as possible to identify failure modes, and their causes and effects. For each component, the failure modes and their resulting effects on the rest of the system are recorded in a specific FMEA worksheet. There are numerous variations of such worksheets. An FMEA can be a qualitative analysis, but may be put on a quantitative basis when mathematical failure rate models are combined with a statistical failure mode ratio database.\n\nA few different types of FMEA analyses exist, such as:\n\nSometimes FMEA is extended to FMECA (failure mode, effects, and criticality analysis) to indicate that criticality analysis is performed too.\n\nFMEA is an inductive reasoning (forward logic) single point of failure analysis and is a core task in reliability engineering, safety engineering and quality engineering.\n\nA successful FMEA activity helps identify potential failure modes based on experience with similar products and processes—or based on common physics of failure logic. It is widely used in development and manufacturing industries in various phases of the product life cycle. \"Effects analysis\" refers to studying the consequences of those failures on different system levels.\n\nFunctional analyses are needed as an input to determine correct failure modes, at all system levels, both for functional FMEA or Piece-Part (hardware) FMEA. An FMEA is used to structure Mitigation for Risk reduction based on either failure (mode) effect severity reduction or based on lowering the probability of failure or both. The FMEA is in principle a full inductive (forward logic) analysis, however the failure probability can only be estimated or reduced by understanding the \"failure mechanism\". Hence, FMEA may include information on causes of failure (deductive analysis) to reduce the possibility of occurrence by eliminating identified \"(root) causes\".\n\nThe FME(C)A is a design tool used to systematically analyze postulated component failures and identify the resultant effects on system operations. The analysis is sometimes characterized as consisting of two sub-analyses, the first being the failure modes and effects analysis (FMEA), and the second, the criticality analysis (CA). Successful development of an FMEA requires that the analyst include all significant failure modes for each contributing element or part in the system. FMEAs can be performed at the system, subsystem, assembly, subassembly or part level. The FMECA should be a living document during development of a hardware design. It should be scheduled and completed concurrently with the design. If completed in a timely manner, the FMECA can help guide design decisions. The usefulness of the FMECA as a design tool and in the decision-making process is dependent on the effectiveness and timeliness with which design problems are identified. Timeliness is probably the most important consideration. In the extreme case, the FMECA would be of little value to the design decision process if the analysis is performed after the hardware is built. While the FMECA identifies all part failure modes, its primary benefit is the early identification of all critical and catastrophic subsystem or system failure modes so they can be eliminated or minimized through design modification at the earliest point in the development effort; therefore, the FMECA should be performed at the system level as soon as preliminary design information is available and extended to the lower levels as the detail design progresses.\n\nRemark: For more complete scenario modelling another type of Reliability analysis may be considered, for example fault tree analysis (FTA); a \"deductive\" (backward logic) failure analysis that may handle multiple failures within the item and/or external to the item including maintenance and logistics. It starts at higher functional / system level. A FTA may use the basic failure mode FMEA records or an effect summary as one of its inputs (the basic events). Interface hazard analysis, Human error analysis and others may be added for completion in scenario modelling.\n\nThe analysis may be performed at the functional level until the design has matured sufficiently to identify specific hardware that will perform the functions; then the analysis should be extended to the hardware level. When performing the hardware level FMECA, interfacing hardware is considered to be operating within specification. In addition, each part failure postulated is considered to be the only failure in the system (i.e., it is a single failure analysis). In addition to the FMEAs done on systems to evaluate the impact lower level failures have on system operation, several other FMEAs are done. Special attention is paid to interfaces between systems and in fact at all functional interfaces. The purpose of these FMEAs is to assure that irreversible physical and/or functional damage is not propagated across the interface as a result of failures in one of the interfacing units. These analyses are done to the piece part level for the circuits that directly interface with the other units. The FMEA can be accomplished without a CA, but a CA requires that the FMEA has previously identified system level critical failures. When both steps are done, the total process is called a FMECA.\n\nThe ground rules of each FMEA include a set of project selected procedures; the assumptions on which the analysis is based; the hardware that has been included and excluded from the analysis and the rationale for the exclusions. The ground rules also describe the indenture level of the analysis, the basic hardware status, and the criteria for system and mission success. Every effort should be made to define all ground rules before the FMEA begins; however, the ground rules may be expanded and clarified as the analysis proceeds. A typical set of ground rules (assumptions) follows:\n\nMajor benefits derived from a properly implemented FMECA effort are as follows:\n\n\nFrom the above list, early identifications of SFPS, input to the troubleshooting procedure and locating of performance monitoring / fault detection devices are probably the most important benefits of the FMECA. In addition, the FMECA procedures are straightforward and allow orderly evaluation of the design.\n\nProcedures for conducting FMECA were described in US Armed Forces Military Procedures document MIL-P-1629\n(1949); revised in 1980 as MIL-STD-1629A. By the early 1960s, contractors for the U.S. National Aeronautics and Space Administration (NASA) were using variations of FMECA or FMEA under a variety of names. NASA programs using FMEA variants included Apollo, Viking, Voyager, Magellan, Galileo, and Skylab. The civil aviation industry was an early adopter of FMEA, with the Society for Automotive Engineers (SAE) publishing ARP926 in 1967. After two revisions, ARP926 has been replaced by ARP4761, which is now broadly used in civil aviation.\n\nDuring the 1970s, use of FMEA and related techniques spread to other industries. In 1971 NASA prepared a report for the U.S. Geological Survey recommending the use of FMEA in assessment of offshore petroleum exploration. A 1973 U.S. Environmental Protection Agency report described the application of FMEA to wastewater treatment plants. FMEA as application for HACCP on the Apollo Space Program moved into the food industry in general.\n\nThe automotive industry began to use FMEA by the mid 1970s. The Ford Motor Company introduced FMEA to the automotive industry for safety and regulatory consideration after the Pinto affair. Ford applied the same approach to processes (PFMEA) to consider potential process induced failures prior to launching production. In 1993 the Automotive Industry Action Group (AIAG) first published an FMEA standard for the automotive industry. It is now in its fourth edition. The SAE first published related standard J1739 in 1994. This standard is also now in its fourth edition.\n\nAlthough initially developed by the military, FMEA methodology is now extensively used in a variety of industries including semiconductor processing, food service, plastics, software, and healthcare. Toyota has taken this one step further with its Design Review Based on Failure Mode (DRBFM) approach. The method is now supported by the American Society for Quality which provides detailed guides on applying the method. The standard Failure Modes and Effects Analysis (FMEA) and Failure Modes, Effects and Criticality Analysis (FMECA)\nprocedures identify the product failure mechanisms, but may not model them without specialized software. This limits their applicability to provide a meaningful input to critical procedures such as virtual qualification, root cause analysis, accelerated test programs, and to remaining life assessment. To overcome the shortcomings of FMEA and FMECA a Failure Modes, Mechanisms and Effect Analysis (FMMEA) has often been used.\n\nThe following covers some basic FMEA terminology.\n\n\nIt is necessary to look at the cause of a failure mode and the likelihood of occurrence. This can be done by analysis, calculations / FEM, looking at similar items or processes and the failure modes that have been documented for them in the past. A failure cause is looked upon as a design weakness. All the potential causes for a failure mode should be identified and documented. This should be in technical terms. Examples of causes are: Human errors in handling, Manufacturing induced faults, Fatigue, Creep, Abrasive wear, erroneous algorithms, excessive voltage or improper operating conditions or use (depending on the used ground rules).\nA failure mode is given a \"Probability Ranking\".\n\nDetermine the Severity for the worst-case scenario adverse end effect (state). It is convenient to write these effects down in terms of what the user might see or experience in terms of functional failures. Examples of these end effects are: full loss of function x, degraded performance, functions in reversed mode, too late functioning, erratic functioning, etc. Each end effect is given a Severity number (S) from, say, I (no effect) to V (catastrophic), based on cost and/or loss of life or quality of life. These numbers prioritize the failure modes (together with probability and detectability). Below a typical classification is given. Other classifications are possible. See also hazard analysis.\n\nThe means or method by which a failure is detected, isolated by operator and/or maintainer and the time it may take. This is important for maintainability control (availability of the system) and it is especially important for multiple failure scenarios. This may involve dormant failure \"modes\" (e.g. No direct system effect, while a redundant system / item automatically takes over or when the failure only is problematic during specific mission or system states) or latent failures (e.g. deterioration failure \"mechanisms\", like a metal growing crack, but not a critical length). It should be made clear how the failure mode or cause can be discovered by an operator under normal system operation or if it can be discovered by the maintenance crew by some diagnostic action or automatic built in system test. A dormancy and/or latency period may be entered.\n\nThe average time that a failure mode may be undetected may be entered if known. For example:\n\n\nIf the undetected failure allows the system to remain in a \"safe\" / working state, a second failure situation should be explored to determine whether or not an indication will be evident to all \"operators\" and what corrective action they may or should take.\n\nIndications to the operator should be described as follows:\n\n\nPERFORM DETECTION COVERAGE ANALYSIS FOR TEST PROCESSES AND MONITORING (From ARP4761 Standard):\n\nThis type of analysis is useful to determine how effective various test processes are at the detection of latent and dormant faults. The method used to accomplish this involves an examination of the applicable failure modes to determine whether or not their effects are detected, and to determine the percentage of failure rate applicable to the failure modes which are detected. The possibility that the detection means may itself fail latently should be accounted for in the coverage analysis as a limiting factor (i.e., coverage cannot be more reliable than the detection means availability). Inclusion of the detection coverage in the FMEA can lead to each individual failure that would have been one effect category now being a separate effect category due to the detection coverage possibilities. Another way to include detection coverage is for the FTA to conservatively assume that no holes in coverage due to latent failure in the detection method affect detection of all failures assigned to the failure effect category of concern. The FMEA can be revised if\nnecessary for those cases where this conservative assumption does not allow the top event probability requirements to be met.\n\nAfter these three basic steps the Risk level may be provided.\n\nRisk is the combination of End Effect Probability And Severity where probability and severity includes the effect on non-detectability (dormancy time). This may influence the end effect probability of failure or the worst case effect Severity. The exact calculation may not be easy in all cases, such as those where multiple scenarios (with multiple events) are possible and detectability / dormancy plays a crucial role (as for redundant systems). In that case Fault Tree Analysis and/or Event Trees may be needed to determine exact probability and risk levels.\n\nPreliminary Risk levels can be selected based on a Risk Matrix like shown below, based on Mil. Std. 882. The higher the Risk level, the more justification and mitigation is needed to provide evidence and lower the risk to an acceptable level. High risk should be indicated to higher level management, who are responsible for final decision-making.\n\nThe FMEA should be updated whenever:\n\n\n\nWhile FMEA identifies important hazards in a system, its results may not be comprehensive and the approach has limitations. In the healthcare context, FMEA and other risk assessment methods, including SWIFT (Structured What If Technique) and retrospective approaches, have been found to have limited validity when used in isolation. Challenges around scoping and organisational boundaries appear to be a major factor in this lack of validity.\n\nIf used as a top-down tool, FMEA may only identify major failure modes in a system. Fault tree analysis (FTA) is better suited for \"top-down\" analysis. When used as a \"bottom-up\" tool FMEA can augment or complement FTA and identify many more causes and failure modes resulting in top-level symptoms. It is not able to discover complex failure modes involving multiple failures within a subsystem, or to report expected failure intervals of particular failure modes up to the upper level subsystem or system.\n\nAdditionally, the multiplication of the severity, occurrence and detection rankings may result in rank reversals, where a less serious failure mode receives a higher RPN than a more serious failure mode. The reason for this is that the rankings are ordinal scale numbers, and multiplication is not defined for ordinal numbers. The ordinal rankings only say that one ranking is better or worse than another, but not by how much. For instance, a ranking of \"2\" may not be twice as severe as a ranking of \"1,\" or an \"8\" may not be twice as severe as a \"4,\" but multiplication treats them as though they are. See Level of measurement for further discussion. Various solutions to this problems have been proposed, e.g., the use of fuzzy logic as an alternative to classic RPN model.\n\nProcess FMEA can be challenging for participants who have not completed many PFMEAS, often confusing FAILURE MODES with EFFECTS and CAUSES. To clarify, a Process FMEA shows how the process can go wrong. Using a detailed Process Map will aid the person filling in the worksheet to correctly list the steps of the process being reviewed. The FAILURE MODE is then simply how that step can go wrong. Example, Process Step 1. Pick Up right handed part. Can they pick up the wrong part? (some manufacturing centers have left and right handed parts etc.) FAILURE MODE put left hand part in, EFFECT could be wrecked CNC machine and scrapped part, or hole drilled in wrong location. The cause, keeping inventory of similar parts at the job. Why is it important to do a PFMEA with regard to the process? When a process is examined or if we ask what can go wrong with the process unknown issues are uncovered, solving problems before they occur and tackling root cause issues or at least 2 Y's deep on a 5 Y. Here the manufacturing engineer could possibly poke yoke the tooling to prevent a left handed part in the fixture when running the right handed parts or program a touch off probe in the CNC programming - all before ever making the mistake the first time. If a PFMEA is set up where the FAILURE MODE relates to the feature on the print, example FAILURE MODE drilled hole too big - no further understanding of what caused the problem is gained. Numerous PFMEA's have been examined and show that little to no value is gained when reviewing features off of a print as FAILURE MODES - little understanding of the cause is gained. New PFMEA practitioners often try to relate the PFMEA FAILURE MODE to the FEATURE, numerous authors list this as trying to inspect in quality rather than listing the process step determining how it can go wrong and building in quality through root cause evaluation.\n\nBesides, two shortcomings are\n\nThe FMEA worksheet is hard to produce, hard to understand and read, as well as hard to maintain. The use of neural network techniques to cluster and visualise failure modes were suggested, recently.\n\n\n"}
{"id": "33513133", "url": "https://en.wikipedia.org/wiki?curid=33513133", "title": "Field dependence", "text": "Field dependence\n\nField dependence is a concept in the field of cognitive styles. It is a one-dimensional model of variation in cognitive style. The concept was first proposed by American psychologist Herman Witkin in 1962. Field dependence/independence was the earliest studied area in the study of cognitive styles.\n\nIn general, people who exhibit field dependence tend to rely on information provided by the outer world, the field or frame of a situation and their cognition (toward other things) is based on this overall field.\n\nField Dependence or Independence is indicated by the Tilting Rod and Frame Test and the Tilting Room, Tilting Chair Test. The tilt of the frame or room provided a field and the degree of independence from it was shown by the accuracy with which the subject had the experimenter adjust the rod or chair. Their interpretation and the concept of field dependence/independence was initially presented by Witkin and his group in his book \"Personality Through Perception\". As its title suggests, other tests were studied and reported on. A field dependence/independence factor was reported in his later book, \"Psychological Differentiation\".\n\nLater, Witkin and his group developed an Embedded Figure Test and did more work with human development. The EFT figured prominently in \"Psychological Differentiation\". Differentiation was shown by an ability to trace a figure \"embedded\" within a more complex figure. Field-independent people were generally more differentiated, but differentiation was a broader concept than field dependency.\n\nWitkin's group participated in developing their concepts and were given coauthor credit in their two books. The group included the psychoanalyst Helen Block Lewis, the psychologist Hanna Fatterson, and the experimental psychologist Don Goodenough.\n\nMore independently, Witkin wrote a \"Journal of Abnormal Psychology\" review of relationships between Field Dependency/Psychological Differentiation and types of psychopathology. For example, field-independent people appeared to be more likely to develop delusions while field-dependent people were more likely to have scattered hallucinations.\n\nEducational Testing Service (ETS) produced a Group Embedded Figures Test, suited for testing a classroom of students. It correlated with the EFT to an extent, but was not exactly the same in some correlations. After his work on the books and review, Witkin went to ETS and published some ETS reviews of the thousands of studies related to Field Dependency/Psychological Differentiation.\n\nA rating scale was developed for the Draw a Person Test by Witkin's clinicians, described in \"Psychological Differentiation\", and was widely used as another measure of psychological differentiation. In it, greater psychological differentiation was shown by more sophisticated detail, demonstrating more specific identity.\n\nLess differentiated people were usually also more field-dependent and were more inclined to use what Witkin's clinicians in \"Psychological Differentiation\" called \"massive global repression.\" Since his experienced clinicians could also recognize field dependence and lack of psychological differentiation upon meeting subjects, however, their conclusion about subjects' use of repression was open to possible contamination.\n\nTheir repression hypothesis was more objectively supported for group extremes by studies of \"perceptual defense\"—the tendency to recognize emotionally significant, disturbing words less easily than more neutral words. Such \"defense\" had been shown by numerous studies to be correlated with a tendency to repress as shown by measures such as the MMPI.\n\nThose average in Differentiation/Field Independence had been given little specific attention since it was assumed they would simply be intermediate in repression.\n\nIn their perceptual defense, however, they were not intermediate. People who were average (statistically \"normal\") were more able to recognize words if they were emotionally significant. That was defended as making more sense in evolutionary/survival terms. (Minard, et. al. in various publications). It seems reasonable to suppose that average (\"normal\") subjects can confuse the results of a study that includes them without recognizing their particular contribution.\n\nTo be more specific, in the RFT, examinees are required to sit in a completely dark room without reflecting surfaces where a 3.5 foot luminous square frame with one inch wide sides contains a rod much like one of the sides. The test is properly given after brief (120-second) dark adaptation and the large glowing RFT is seven feet from the subject so it is a dramatic visual display filling much of the visual field. If the test is given with insufficient dark adaptation, many subjects will see the luminous stimulus disappear or fragment when they fixate it visually and a large proportion of them will not happen to report that. The test's validity then is not known. Hebb and others have written about the fragmentation phenomenon.\n\nThe RFT is presented with the tilt of rod and tilt of frame in every possible order (RR, LL, RL, LR). The examinee is then asked to tell the examiner how to adjust the rod to \"straight up and down like the flagpole outside.\" Scores are based the number of degrees away from 90.\n\nA less commonly used task was the tilting-room-tilting-chair test, which was similar to the RFT but involved rotating the participant in a chair within a room that also rotated.\n\nExaminees are shown a simple geometric figure and then asked to find the simple figure embedded or hidden in a relatively complex figure. Examinees may circle the figure and are scored based on identifying the full, correct figure. The underlying learning style theory is that field-independent people can better ignore the influence of the background image and find the hidden figure, while field-dependent people will find it harder to ignore the irrelevant field and will be slower to locate the figure. The Witkin EFT was first developed by Herman Witkin as suggested by the figures of Kurt Gotschaldt and described in his book \"Psychological Differentiation\" by Witkin's group. Thurston had an earlier Hidden Figures Test which correlated with the EFT but had a different pattern of other correlations.\n\nResearchers since Witkin's group have extended the use of this measure to other concepts, including components of memory and logic. These measures may also be sensitive to traits of Autism Spectrum Disorder, where individuals with ASD are quicker to solve EFT tasks.\n\n\n"}
{"id": "2258103", "url": "https://en.wikipedia.org/wiki?curid=2258103", "title": "Francis Polkinghorne Pascoe", "text": "Francis Polkinghorne Pascoe\n\nFrancis Polkinghorne Pascoe (1 September 1813 – 20 June 1893) was an English entomologist mainly interested in Coleoptera, commonly known as beetles.\n\nHe was born in Penzance, Cornwall and trained at St. Bartholomew's Hospital, London. Appointed surgeon in the Navy he served on Australian, West Indian and Mediterranean stations. He married a Miss Mary Glasson of Cornwall and settled at Trewhiddle near St Austell where his wife's property produced china clay. Widowed in 1851 he settled in London devoting himself to natural history and entomology in particular. The results of collecting trips to Europe, North Africa and the Lower Amazons were poor and Pascoe worked mainly on insects collected by others. His entomological papers listed and described species collected by Alfred Russel Wallace (in \"Longicornia Malayana\"), Robert Templeton and other assiduous collectors but not prolific writers on systematic entomology. He became a Fellow of the Entomological Society in 1854, was president from 1864–1865, a Member of the Société Entomologique de France and belonged to the Belgian and Stettin Societies. He was also a Fellow of the Linnean Society (elected 1852) and was on the Council of the Ray Society. His 2,500 types are in the Natural History Museum, London.\n\nPascoe accepted the fact of evolution but was an opponent to natural selection.\n\n\n"}
{"id": "12581684", "url": "https://en.wikipedia.org/wiki?curid=12581684", "title": "Harvey Molotch", "text": "Harvey Molotch\n\nHarvey Luskin Molotch (born January 3, 1940) is an American sociologist known for studies that have reconceptualized power relations in interaction, the mass media, and the city. He helped create the field of environmental sociology and has advanced qualitative methods in the social sciences. In recent years, Molotch helped develop a new field—the sociology of objects. He is currently a professor of Sociology and of Metropolitan Studies at New York University. His \"Introduction to Sociology\" is featured as one of NYU Open Education's courses available to stream freely. Other courses that he teaches include \"Approaches to Metropolitan Studies\" and \"Urban Objects\". He is also affiliated with the graduate program in Humanities and Social Thought.\n\nMolotch was born Harvey Luskin in Baltimore, Maryland, where his family was in the retail car business on one side and the Luskin's home appliance business on the other. His father, Paul Luskin, died in the Battle of the Bulge in 1944 during World War II. His mother remarried to Nathan Molotch. He received a B.A. in Philosophy from the University of Michigan (1963), with a thesis on John Dewey. He received an M.A. (1966) and Ph.D. (1968) in Sociology from the University of Chicago. He served in the U.S. Army, stationed in Maryland and Virginia, 1961-62.\n\nHe taught at the University of California, Santa Barbara from 1967 to 2003. He has also been a visiting professor at Stony Brook University, the University of Essex, and Northwestern University. In 1998-99 he was Centennial Professor at the London School of Economics.\n\nHis 1964 marriage to Linda Molotch ended with her death, by car accident, in 1976. The couple had two children, Shana (born 1969), now with two children and living in Northern California; and Noah (born 1972), now with two children and living in Boulder Colorado where he is on faculty at University of Colorado as hydrological scientist, as well as research scientist at Jet Propulsion Laboratory. Molotch has lived with his domestic partner, Glenn Wharton, a conservator at the Museum of Modern Art and faculty member at New York University, since 1979.\n\nMolotch's early work on \"white flight\" overturned conventional wisdom on neighborhood change, showing that normal mobility makes neighborhood racial change possible. When blacks constitute the bulk of those who move into the vacancies that result, racial change is made inevitable. The implication of this finding, based on Molotch's systematic studies of matched neighborhoods (and since replicated by others on large data sets), was that it is the reluctance of whites to move into a changing neighborhood that makes racial integration so difficult to achieve. From a policy perspective, Molotch concluded that while stabilizing neighborhoods would not be easy, the focus needs to be on getting white people to replace the whites who are leaving, rather than talking people who are leaving into staying.\n\nOn January 28, 1969, there was a massive eruption of crude oil from Union Oil's Platform A in the Santa Barbara Channel--an eruption which was to cover much of the coast line of two counties with oil. Molotch saw in this disaster a research opportunity. His article \"Oil in Santa Barbara and Power in America\" became a founding document of the new field of environmental sociology, and a key contribution to political sociology.\n\nMolotch argued that accident research at the local level might be capable of revealing what political scientists called the \"second face of power.\" This is a dimension of power ordinarily ignored by traditional community studies which fail to concern themselves with the processes by which bias is mobilized and thus how issues rise and fall.\n\nMolotch's findings highlighted the extraordinary intransigence of national institutions in the face of local dissent, but more importantly, pointed out the processes and tactics which undermine that dissent and frustrate and radicalize the dissenters. Molotch called for comparable studies of the agriculture industry, the banking industry, and for more accident research at the local level, which might bring to light the larger social arrangements which structure the parameters of such local debate. In this way, research at the local level might serve as an avenue to knowledge about \"national\" power. Molotch ended, \"Sociologists should be ready when an accident hits in their neighborhood, and then go to work.\"\n\nMolotch helped introduce the social construction framework to the study of news media. Whereas news accounts had been treated, however critically, as \"failed\" representations of a presumed reality, Molotch and Marilyn Lester held that every account is a product of the social organization that goes into its production. In founding papers in the sociology of the mass media, Molotch and Lester applied the insights of ethnomethodology to the Santa Barbara oil spill and the way it was covered. They argued for an approach to the mass media which does not look for reality, but for practices of those having the power to determine the experience of others.\n\nIn addition, Molotch and Lester recognized that this social construction of the news had a crucial political component, a perspective later endorsed by such media sociologists as W. Lance Bennett. In normal times, Molotch and Lester said, the news is merely the ritualized presentation of the stories of powerful corporate and governmental organizations. Only in certain contexts does the veil of this ruling elite consensus get pushed aside to reveal other possible constructions of the facts. Molotch and Lester pointed to such disruptive contexts as scandals and accidents like the Santa Barbara Oil Spill, while Bennett pointed to significant social issues that break through the normally ritualized conflicts of the two political parties.\n\nMolotch's work has inspired studies of the social construction of news, of the particular ways that the content of presentation is contingent on the social setting of its production, including the occupational workplace of news professionals as well as the larger societal setting. His more recent work on mass media has included studies of war protest and the stock market.\n\nMolotch is probably best known for his book \"Urban Fortunes\" (1987, with John Logan), which won sociology's most prestigious prize for scholarship in 1990. Urban Fortunes builds on Molotch's 1976 classic paper, \"The City as a Growth Machine.\" In this body of work, Molotch took the dominant convention of studying urban land use and turned it on its head. The field of urban sociology (as well as urban geography, planning, and economics) was dominated by the idea that cities were basically containers for human action, in which actors competed among themselves for the most strategic parcels of land, and the real estate market reflected the state of that competition. Out of this competition were thought to come the shape of the city and the distribution of social types within it (e.g. banks in the center, affluent residents in the suburbs). Long established notions such as central place theory and the sectoral hypothesis were claims that are more or less \"natural\" spatial geography evolved from competitive market activity.\n\nMolotch helped reverse the course of urban theory by pointing out that land parcels were not empty fields awaiting human action, but were associated with specific interests—commercial, sentimental, and psychological. Especially important in shaping cities were the real estate interests of those whose properties gain value when growth takes place. These actors make up what Molotch termed \"the local growth machine\" -- a term now standard in the urban studies lexicon. From this perspective, cities need to be studied (and compared) in terms of the organization, lobbying, manipulating, and structuring carried out by these actors. The outcome—the shape of cities and the distribution of their peoples—is thus not due to an interpersonal market or geographic necessities, but to social actions, including opportunistic dealing. \"Urban Fortunes\" has influenced hundreds of national and international studies. A twentieth anniversary edition was issued by the University of California Press in 2007 with a new preface.\n\nMolotch has also conducted a series of studies in conversation analysis on mechanisms such as gaps and silences in human conversation that reveal the way power operates at the micro-interactional level. This work includes a notable collaboration with Mitchell Duneier on talk between men on the street and women passersby. His research builds on writings of Don Zimmerman, Harvey Sacks, Gail Jefferson, and Emanuel Schegloff. Molotch was among the first to utilize ethnomethodology and conversation analysis in the study of traditional sociological topics, bridging what had been regarded as a highly esoteric and specialized approach to micro-sociology with mainstream, macro-level sociological issues such as hegemony and power.\n\nMore recently in \"Where Stuff Comes From\", Molotch builds on the work of Howard S. Becker and Bruno Latour, to show how objects and physical artifacts are joint result of various types of actors, most particularly product designers operating within frameworks of technology, regulation, mass tastes, and corporate profits. While neo-Marxists and others have treated \"commodity fetishism\" as a signal of oppression, repression, and delusion, he uses goods to understand, in a more comprehensive way, just what makes production happen and how artifacts reveal larger social and cultural forces.\n\n\n\n"}
{"id": "53888687", "url": "https://en.wikipedia.org/wiki?curid=53888687", "title": "Henrik Palmgren", "text": "Henrik Palmgren\n\nHenrik Palmgren is a Swedish alt-right political podcaster, vlogger, and YouTube personality. He founded the alt-right website \"Red Ice\" in 2003.\n\nPalmgren launched the company Red Ice in 2002 in Gothenburg, Sweden as an outlet for conspiracy theories about U.F.O.s, Freemasonry, 9/11, and The Illuminati. In 2008 he began collaborating with Lana Lokteff, an Oregon-based musician who was a listener to his internet radio show. They married in 2011. Beginning in 2012, Red Ice started shifting towards more racist conspiracy theories, such as the white genocide conspiracy theory and Holocaust denial.\n\nPalmgren is the host of the podcast and video program \"Red Ice Radio\", while Lokteff hosts \"Radio 3Fourteen\". Palmgren's program includes content promoting white nationalism, anti-Semitism, paranormal topics, holocaust revisionism, and far-right politics. Palmgren's show has included guests such as Ingrid Carlqvist, Richard B. Spencer, Kevin B. MacDonald, David Duke, Michael A. Hoffman II, Gilad Atzmon, David Icke, Faith Goldy, Mark Collett, Jim Fetzer, Paul Nehlen, Andrew Anglin, UKIP prospective Member of Parliament, Jack Sen, and Millennial Woes, among many others.\n\nIn May 2017, \"The Forward\" reported that Palmgren is collaborating with white nationalist Richard B. Spencer to form a new media company.\n\nPalmgren took part in the Unite the Right rally in Charlottesville.\n\nIn July 2018, Palmgren and Lotkeff, alongside several other alt-right figures including Goldy, were banned from the online money transfer service PayPal.\n\nOriginally from Kungälv, Palmgren has lived in Oregon for many years. He is married to Lana Lokteff who is associated with the alt-right.\n\n"}
{"id": "30857507", "url": "https://en.wikipedia.org/wiki?curid=30857507", "title": "High Adventure Role Playing", "text": "High Adventure Role Playing\n\nHigh Adventure Role Playing (HARP) is a fantasy role-playing game, designed by Tim Dugger & Heike A. Kubasch, and published by Iron Crown Enterprises (ICE).\n\nHARP is produced by Iron Crown Enterprises, the same company that produces \"Rolemaster\", but the mechanics of the system are very much simplified in comparison. The system also takes cues from the \"d20 system\".\n\nThe \"HARP\" book is 15 chapters long, with the first nine devoted to character generation.\n\n\"HARP\" has Professions that determine which sets of skills are favoured or not, many also provide level bonuses to skills, spell spheres and/or talents. The Professions in \"HARP Revised\" are: Cleric, Fighter, Harper, Mage, Monk, Ranger, Rogue, Thief, and Warrior Mage. Additional professions are found in other support books and include: Paladin, Beastmaster, Elementalist, Thaumaturge, Necromancer, Vivimancer, Adventurer, Mystic, Shadowblade, & Druid. \"HARP\"'s Professions are designed to be flexible, with a single level progression chart allowing the player to build the character he or she wants from the base profession.\n\nA system of eight statistics are used for characters featuring Strength, Constitution, Agility, Quickness, Self Discipline, Reasoning, Insight and Presence. Statistics range from 1 to 105.\n\nStatistics are generated during character creation one of three ways: -\n\n\nMuch of the game play revolves around the ability scores, (the Development Points that are used throughout the rest of character creation are based on these scores,) so a character generated with method one runs a risk of not being playable, though the chance of a highly exceptional score is possible.\n\nPlayer characters in \"HARP\" belong to one of a set of fantasy races: Human, Dwarf, Elf, Gnome and Halfling; \"Harp\" adds the \"Gryx\", a race who are physically similar to Orcs, with a more peaceful mentality.\n\n\"HARP\" does not directly offer mixed races. Instead characters may purchase Greater and Lesser Blood Talents to customize their character. Greater Blood Talents reflect the more traditional half-race, (meaning that the character is half the Base Race and half the Race chosen with the Blood Talent,) while Lesser Blood Talents represent traits from inter-racial ancestors.\n\nCultures represent the other side to the nature versus nurture equation. These traits represent the character's upbringing, from nomadic and rural, to urban and underhill. These directly influence the character's background, starting location, clothing, demeanour and language. They provide a relatively standard set of cultural skill ranks called adolescent ranks. These skills are typical skills that most people in a culture will learn in their formative years.\n\nThere is also a new campaign setting offered by ICE which is called \"Gryphon World\" and has set its main focus on the continent of \"Cyradon\". It offers a variation on the standard fantasy races, with the Dwarves becoming the Mablung, the Arali and Sithi replacing the normal elves, the Rhona a wise gnomish race, Nagazi a civilized lizard race, and Gryphons as a flying quadruped race.\n\nNotably it excludes Halflings, orcs, goblins, kobolds and many other standard fantasy races and creatures by default.\n\nThere are 10 skill categories with 3-9 skills each, for a total of 60 skills. Some skills have sub-skills (that are a little more difficult to perform). Each profession receives 20 starting ranks, divided among their favored skill categories, and can build from there with development points.\n\nSkills are used by rolling an open ended d100 (96-100 re-roll and add), adding your skill bonus and the related stat bonus, a modifier for the difficulty of the task, and trying to get over 101. Difficulty modifiers range from Mundane (+60) to Absurd (-100).\n\nSpells are purchased in this stage as well, each spell is purchased as a separate skill. Several professions have a Professional Sphere of spells, and any character may purchase spells from the Universal Sphere. Spells are powered by Power Points, and in order to be able to cast a spell, a character must have a rank in that spell equal to the power points needed. Spells can be scaled upwardly, thereby increasing the required power points and skill ranks needed to cast the scaled version of the spell.\n\nTalents are both inherent to the character's race and available as a form of character customization. In order to obtain a talent the character expends DP while creating or levelling up a character. GM's are encouraged to customize the available talents and even require in game explanations should they wish. Forty-one talents make up the Master List in \"HARP Revised Edition\" (though more are available in support material) and include Dark Vision, Blazing speed, Ambidexterity, Familiar, Athletic, Shapechanger and Outdoorsman.\n\nSpecial Starting Items such as an Item of Quality or a Loyal Domesticated or Unusual Animal or Creature are also available, and while not talents, they are purchased in the same way.\n\nFate Points are a way to aid in dice rolls in critical circumstances or in reducing critical wounds, a single Fate Point is worth +50 on the dice or a -25 to the critical received and can be purchased at a cost of 5 development points to a maximum of 5 Fate Points. All new player characters begin with 3 fate points. These points are a method of allowing the heroic protagonists to achieve the heroic, they are not normally given to non player characters.\n\nTraining packages are sets of linked skills that come at a discount and hail from a common background. Characters may take one package per level. Examples include Bounty Hunter, Astothian Archer, Jade Dragon, and Tyrian Sage.\n\nThis allows quick customizations of professions within settings without the need to create a new profession. For example, in a military order within a campaign several training packages can be created to mimic troop specialization and or rank. Only one training package can be acquired by a character at each level.\n\nOn August 21, 2004, HARP won the Silver ENnie for Best Non-d20 Game at Gen Con.\n\n\"HARP\" is available from the OneBookShelf network It retails at $20.00 for an Adobe PDF version.\n\n"}
{"id": "316976", "url": "https://en.wikipedia.org/wiki?curid=316976", "title": "Hobson's choice", "text": "Hobson's choice\n\nA Hobson's choice is a free choice in which only one thing is offered. Because a person may refuse to accept what is offered, the two options are taking it or taking nothing. In other words, one may \"take it or leave it\".\nThe phrase is said to have originated with Thomas Hobson (1544–1631), a livery stable owner in Cambridge, England, who offered customers the choice of either taking the horse in his stall nearest to the door or taking none at all.\n\nAccording to a plaque underneath a painting of Hobson donated to Cambridge Guildhall, Hobson had an extensive stable of some 40 horses. This gave the appearance to his customers that, upon entry, they would have their choice of mounts, when in fact there was only one: Hobson required his customers to choose the horse in the stall closest to the door. This was to prevent the best horses from always being chosen, which would have caused those horses to become overused. Hobson's stable was located on land that is now owned by St Catharine's College, Cambridge.\n\nHenry Ford is said to have offered the Ford Model T as a Hobson's choice with the famous dictum: \"Any customer can have a car in any color as long as it is black.\" In fact, the car was offered in a variety of colors in the earliest days.\n\nThe ultimatum game is a form of Hobson's choice.\n\nAccording to the \"Oxford English Dictionary\", the first known written usage of this phrase is in \"The rustick's alarm to the Rabbies\", written by Samuel Fisher in 1660:\n\nIt also appears in Joseph Addison's paper \"The Spectator\" (No. 509 of 14 October 1712); and in Thomas Ward's 1688 poem \"England's Reformation\", not published until after Ward's death. Ward wrote:\n\nThe term \"Hobson's choice\" is often used to mean an illusion of choice, but it is not a choice between two equivalent options, which is a Morton's fork, nor is it a choice between two undesirable options, which is a dilemma. Hobson's choice is one between something or nothing.\n\nJohn Stuart Mill, in his book \"Considerations on Representative Government\", refers to Hobson's choice:\n\nIn another of his books, \"The Subjection of Women\", Mill discusses marriage:\n\nIn the 1951 Robert Heinlein book \"Between Planets\", the main character Don Harvey incorrectly mentions he has a Hobson's choice. While on a space station orbiting Earth, Don needs to get to Mars, where his parents are. The only rockets available are back to Earth (where he is not welcome) or on to Venus.\n\nA Hobson's choice is different from:\n\nA common error is to use the phrase \"Hobbesian choice\" instead of \"Hobson's choice\", confusing the philosopher Thomas Hobbes with the relatively obscure Thomas Hobson (It's possible they may be confusing \"Hobson's choice\" with \"Hobbesian trap\", which refers to the trap into which a state falls when it attacks another out of fear). Notwithstanding that confused usage, the phrase \"Hobbesian choice\" is historically incorrect.\n\n\"Hobson's Choice\" is a full-length stage comedy written by Harold Brighouse in 1915. At the end of the play, the central character, Henry Horatio Hobson, formerly a wealthy, self-made businessman but now a sick and broken man, faces the unpalatable prospect of being looked after by his daughter Maggie and her husband Will Mossop, who used to be one of Hobson's underlings. His other daughters have refused to take him in, so he has no choice but to accept Maggie's offer which comes with the condition that he must surrender control of his entire business to her and her husband, Will.\n\nThe play was adapted for filming several times:\n\nIn \"Immigration and Naturalization Service v. Chadha\" (1983), Justice Byron White dissented and classified the majority's decision to strike down the \"one-house veto\" as unconstitutional as leaving Congress with a Hobson's choice. Congress may choose between \"refrain[ing] from delegating the necessary authority, leaving itself with a hopeless task of writing laws with the requisite specificity to cover endless special circumstances across the entire policy landscape, or in the alternative, to abdicate its lawmaking function to the executive branch and independent agency\".\n\nIn \"Monell v. Department of Social Services of the City of New York\", 436 U.S. 658 (1978) the judgement of the court was that\nIn the South African Constitutional Case MEC for Education, Kwa-Zulu Natal and Others v Pillay, 2008 (1) SA 474 (CC) Chief Justice Langa for the majority of the Court (in Paragraph 62 of the judgement) writes that:\nIn Epic Systems Corp. v. Lewis (2018), Justice Ruth Bader Ginsburg dissented and added in one of the footnotes that the petitioners \"faced a Hobson’s choice: accept arbitration on their employer’s terms or\ngive up their jobs\".\n\nAlfred Bester's 1952 short story \"Hobson's Choice\" describes a world in which time travel is possible, and the option is to travel or to stay in one's native time. \n\nIn \"The Grim Grotto\" by Lemony Snicket, the Baudelaire orphans and Fiona are said to be faced with a Hobson's Choice when they are trapped by the Medusoid Mycelium Mushrooms in the Gorgonian Grotto: \"We can wait until the mushrooms disappear, or we can find ourselves poisoned\".\n\nIn Bram Stoker's short story \"The Burial of Rats\", the narrator advises he has a case of Hobson's Choice while being chased by villains. The story was written around 1874. \n\nA well known line of the 1835 novel Le Père Goriot by French novelist Honoré de Balzac is when Vautrin tells Eugene, \"In that case I will make you an offer that no one would decline.\" This has been reworked by Mario Puzo in the novel \"The Godfather\" (1969) and its film adaptation (1972); \"I'm gonna make him an offer he can't refuse\", a line which was ranked as the second most significant cinematic quote in AFI's 100 Years...100 Movie Quotes (2005) by the American Film Institute.\n\n\"The Terminal Experiment\", a 1995 science fiction novel by Robert J. Sawyer, was originally serialised under the title Hobson's Choice.\n\n\"Half-Life\", a video game created in 1998 by Valve Corporation includes a Hobson's Choice in the final chapter. A human-like entity, known only as the 'G-Man', offers the protagonist Gordon Freeman a job, working under his control. If Gordon were to refuse this offer, he would be killed, thus creating the 'illusion of free choice'.\n\nIn \"Early Edition\", the lead character Gary Hobson is named after the choices he regularly makes during his adventures. \n\nIn an episode of \"Inspector George Gently\", a character claims her resignation was a Hobson's choice, prompting a debate among other police officers as to who Hobson is.\n\nIn \"Cape May\" (\"The Blacklist\" season 3, episode 19), Raymond Reddington describes having faced a Hobson's choice in the previous episode where he was faced with the choice of saving Elizabeth Keen's baby and losing Elizabeth Keen or losing them both.\n\nIn his 1984 novel Job: A Divine Comedy, Robert A. Heinlein’s protagonist is said to have Hobson’s Choice when he has the options of boarding the wrong cruise ship or staying on the island.\n"}
{"id": "15398744", "url": "https://en.wikipedia.org/wiki?curid=15398744", "title": "Homo homini lupus", "text": "Homo homini lupus\n\nHomo homini lupus, or in its unabridged form Homo homini lupus est, is a Latin proverb meaning \"A man is a wolf to another man,\" or more tersely \"Man is wolf to man.\" It has meaning in reference to situations where people are known to have behaved in a way comparably in nature to a wolf. The wolf as a creature is thought, in this example, to have qualities of being predatory, cruel, inhuman i.e. more like an animal than civilized.\n\nA variation of the proverb appeared as line 495 in the play \"Asinaria\" by Plautus: \"Lupus est homo homini, non homo, quom qualis sit non novit\", which as \"Man is no man, but a wolf, to a stranger,\" or more precisely \"A man is a wolf, not a man, to another man which he hasn't met yet.\"\n\nAs a counterpoint, Seneca the Younger wrote, in his \"Epistulae morales ad Lucilium\" (specifically, Epistula XCV, paragraph 33), \"homo, sacra res homini\", which as \"man, an object of reverence in the eyes of man\".\n\nErasmus included the proverb in his \"Adagia\", writing of the variation by Plautus, \"Here we are warned not to trust ourselves to an unknown person, but to beware of him as of a wolf.\"\n\nThe philosopher, theologian, and jurist Francisco de Vitoria (in Latin, Franciscus de Victoria) that the poet Ovid disagreed with the proverb: \"'Man,' says Ovid, 'is\nnot a wolf to his fellow man, but a man.'\"\n\nThomas Hobbes drew upon the proverb in his \"De Cive\", \"To speak impartially, both sayings are very true; That Man to Man is a kind of God; and that Man to Man is an arrant Wolfe. The first is true, if we compare Citizens amongst themselves; and the second, if we compare Cities.\" Hobbes was describing the tendency of people to act fairly and generously toward other people in the same society and the tendency of societies to act deceptively and violently toward other societies, or , \"In the one, there's some analogie of similitude with the Deity, to wit, Justice and Charity, the twin-sisters of peace: But in the other, Good men must defend themselves by taking to them for a Sanctuary the two daughters of War, Deceipt and Violence.\"\n\nSigmund Freud agreed with the proverb, writing in his \"Civilization and Its Discontents\", \"Men are not gentle creatures, who want to be loved, who at the most can defend themselves if they are attacked; they are, on the contrary, creatures among whose instinctual endowments is to be reckoned a powerful share of aggressiveness. As a result, their neighbor is for them not only a potential helper or sexual object, but also someone who tempts them to satisfy their aggressiveness on him, to exploit his capacity for work without compensation, to use him sexually without his consent, to seize his possessions, to humiliate him, to cause him pain, to torture and to kill him. \"Homo homini lupus.\" Who in the face of all his experience of life and of history, will have the courage to dispute this assertion?\"\n\nThe primatologist and ethologist Frans de Waal disagreed with the proverb, writing that it \"contains two major flaws. First, it fails to do justice to canids, which are among the most gregarious and cooperative animals on the planet (Schleidt and Shalter 2003). But even worse, the saying denies the inherently social nature of our own species.\"\n\nBartolomeo Vanzetti, after being convicted of murder, along with Nicolo Sacco, in 1927, said that their pending execution would become an emblem \"of a cursed past in which man was wolf to the man\".\n\n"}
{"id": "45145", "url": "https://en.wikipedia.org/wiki?curid=45145", "title": "Hubble sequence", "text": "Hubble sequence\n\nThe Hubble sequence is a morphological classification scheme for galaxies invented by Edwin Hubble in 1926. It is often known colloquially as the Hubble tuning fork diagram because of the shape in which it is traditionally represented.\n\nHubble’s scheme divides regular galaxies into three broad classes – ellipticals, lenticulars and spirals – based on their visual appearance (originally on photographic plates). A fourth class contains galaxies with an irregular appearance. To this day, the Hubble sequence is the most commonly used system for classifying galaxies, both in professional astronomical research and in amateur astronomy.\n\nOn the left (in the sense that the sequence is usually drawn) lie the ellipticals. Elliptical galaxies have relatively smooth, featureless light distributions and appear as ellipses in photographic images. They are denoted by the letter E, followed by an integer \"n\" representing their degree of ellipticity in the sky. By convention, \"n\" is ten times the ellipticity of the galaxy, rounded to the nearest integer, where the ellipticity is defined as for an ellipse with semi-major and semi-minor axes of lengths \"a\" and \"b\" respectively. The ellipticity increases from left to right on the Hubble diagram, with near-circular (E0) galaxies situated on the very left of the diagram. It is important to note that the ellipticity of a galaxy on the sky is only indirectly related to the true 3-dimensional shape (for example, a flattened, discus-shaped galaxy can appear almost round if viewed face-on or highly elliptical if viewed edge-on). Observationally, the most flattened \"elliptical\" galaxies have ellipticities \"e\" = 0.7 (denoted E7). However, from studying the light profiles and the ellipticity profiles, rather than just looking at the images, it was realised in the 1960s that the E5–E7 galaxies are probably misclassified lenticular galaxies with large-scale disks seen at various inclinations to our line-of-sight. Observations of the kinematics of early-type galaxies further confirmed this.\n\nExamples of elliptical galaxies: M49, M59, M60, M87, NGC 4125.\n\nAt the centre of the Hubble tuning fork, where the two spiral-galaxy branches and the elliptical branch join, lies an intermediate class of galaxies known as lenticulars and given the symbol S0. These galaxies consist of a bright central bulge, similar in appearance to an elliptical galaxy, surrounded by an extended, disk-like structure. Unlike spiral galaxies, the disks of lenticular galaxies have no visible spiral structure and are not actively forming stars in any significant quantity. \n\nWhen simply looking at a galaxy's image, lenticular galaxies with relatively face-on disks are difficult to distinguish from ellipticals of type E0–E3, making the classification of many such galaxies uncertain. When viewed edge-on, the disk becomes more apparent and prominent dust-lanes are sometimes visible in absorption at optical wavelengths.\n\nAt the time of the initial publication of Hubble's galaxy classification scheme, the existence of lenticular galaxies was purely hypothetical. Hubble believed that they were necessary as an intermediate stage between the highly flattened \"ellipticals\" and spirals. Later observations (by Hubble himself, among others) showed Hubble's belief to be correct and the S0 class was included in the definitive exposition of the Hubble sequence by Allan Sandage. Missing from the Hubble sequence are the early-type galaxies with intermediate-scale disks, in between the E and S0 type, Martha Liller denoted them ES galaxies in 1966.\n\nLenticular and spiral galaxies, taken together, are often referred to as disk galaxies. The bulge-to-disk flux ratio in lenticular galaxies can take on a range of values, just as it does for each of the spiral galaxy morphological types (Sa, Sb, etc.).\n\nExamples of lenticular galaxies: M85, M86, NGC 1316, NGC 2787, NGC 5866, Centaurus A.\n\nOn the right of the Hubble sequence diagram are two parallel branches encompassing the spiral galaxies. A spiral galaxy consists of a flattened disk, with stars forming a (usually two-armed) spiral structure, and a central concentration of stars known as the bulge. Roughly half of all spirals are also observed to have a bar-like structure, extending from the central bulge, at the ends of which the spiral arms begin. In the tuning-fork diagram, the regular spirals occupy the upper branch and are denoted by the letter S, while the lower branch contains the barred spirals, given the symbol SB. Both type of spirals are further subdivided according to the detailed appearance of their spiral structures. Membership of one of these subdivisions is indicated by adding a lower-case letter to the morphological type, as follows:\n\nHubble originally described three classes of spiral galaxy. This was extended by Gérard de Vaucouleurs to include a fourth class:\n\nAlthough strictly part of the de Vaucouleurs system of classification, the Sd class is often included in the Hubble sequence. The basic spiral types can be extended to enable finer distinctions of appearance. For example, spiral galaxies whose appearance is intermediate between two of the above classes are often identified by appending two lower-case letters to the main galaxy type (for example, Sbc for a galaxy that is intermediate between an Sb and an Sc).\n\nOur own Milky Way is generally classed as SBb or SBc, making it a barred spiral with well-defined arms. \n\nExamples of regular spiral galaxies: M31 (Andromeda Galaxy), M74, M81, M104 (Sombrero Galaxy), M51a (Whirlpool Galaxy), NGC 300, NGC 772.\n\nExamples of barred spiral galaxies: M91, M95, NGC 1097, NGC 1300, NGC1672, NGC 2536, NGC 2903.\n\nGalaxies that do not fit into the Hubble sequence, because they have no regular structure (either disk-like or ellipsoidal), are termed irregular galaxies. Hubble defined two classes of irregular galaxy:\nIn his extension to the Hubble sequence, de Vaucouleurs called the Irr I galaxies 'Magellanic irregulars', after the Magellanic Clouds – two satellites of the Milky Way which Hubble classified as Irr I. The discovery of a faint spiral structure in the Large Magellanic Cloud led de Vaucouleurs to further divide the irregular galaxies into those that, like the LMC, show some evidence for spiral structure (these are given the symbol Sm) and those that have no obvious structure, such as the Small Magellanic Cloud (denoted Im). In the extended Hubble sequence, the Magellanic irregulars are usually placed at the end of the spiral branch of the Hubble tuning fork.\n\nExamples of irregular galaxies: M82, NGC 1427A, Large Magellanic Cloud, Small Magellanic Cloud.\n\nElliptical and lenticular galaxies are commonly referred to together as “early-type” galaxies, while spirals and irregular galaxies are referred to as “late types”. This nomenclature is the source of the common, but erroneous, belief that the Hubble sequence was intended to reflect a supposed evolutionary sequence, from elliptical galaxies through lenticulars to either barred or regular spirals. In fact, Hubble was clear from the beginning that no such interpretation was implied:\n\nThe nomenclature, it is emphasized, refers to position in the sequence, and temporal connotations are made at one's peril. The entire classification is purely empirical and without prejudice to theories of evolution...\n\nThe evolutionary picture appears to be lent weight by the fact that the disks of spiral galaxies are observed to be home to many young stars and regions of active star formation, while elliptical galaxies are composed of predominantly old stellar populations. In fact, current evidence suggests the opposite: the early Universe appears to be dominated by spiral and irregular galaxies. In the currently favored picture of galaxy formation, present-day ellipticals formed as a result of mergers between these earlier building blocks; while some lenticular galaxies may have formed this way, others may have accreted their disks around pre-existing spheroids. Some lenticular galaxies may also be evolved spiral galaxies, whose gas has been stripped away leaving no fuel for continued star formation, although the galaxy LEDA 2108986 opens the debate on this.\n\nA common criticism of the Hubble scheme is that the criteria for assigning galaxies to classes are subjective, leading to different observers assigning galaxies to different classes (although experienced observers usually agree to within less than a single Hubble type). Although not really a short-coming, since the 1961 Hubble Atlas of Galaxies, the primary criteria used to assign the morphological type (a, b, c, etc.) has been the nature of the spiral arms, rather than the bulge-to-disk flux ratio, and thus a range of flux ratios exist for each morphological type, as with the lenticular galaxies.\n\nAnother criticism of the Hubble classification scheme is that, being based on the appearance of a galaxy in a two-dimensional image, the classes are only indirectly related to the true physical properties of galaxies. In particular, problems arise because of orientation effects. The same galaxy would look very different, if viewed edge-on, as opposed to a face-on or ‘broadside’ viewpoint. As such, the early-type sequence is poorly represented: the ES galaxies are missing from the Hubble sequence, and the E5–E7 galaxies are actually S0 galaxies. Furthermore, the barred ES and barred S0 galaxies are also absent.\n\nVisual classifications are also less reliable for faint or distant galaxies, and the appearance of galaxies can change depending on the wavelength of light in which they are observed.\n\nNonetheless, the Hubble sequence is still commonly used in the field of extragalactic astronomy and Hubble types are known to correlate with many physically relevant properties of galaxies, such as luminosities, colours, masses (of stars and gas) and star formation rates.\n\n\n"}
{"id": "25977", "url": "https://en.wikipedia.org/wiki?curid=25977", "title": "Ideal (ring theory)", "text": "Ideal (ring theory)\n\nIn ring theory, a branch of abstract algebra, an ideal is a special subset of a ring. Ideals generalize certain subsets of the integers, such as the even numbers or the multiples of 3. Addition and subtraction of even numbers preserves evenness, and multiplying an even number by any other integer results in another even number; these closure and absorption properties are the defining properties of an ideal. An ideal can be used to construct a quotient ring similarly to the way that, in group theory, a normal subgroup can be used to construct a quotient group.\n\nAmong the integers, the ideals correspond one-for-one with the non-negative integers: in this ring, every ideal is a principal ideal consisting of the multiples of a single non-negative number. However, in other rings, the ideals may be distinct from the ring elements, and certain properties of integers, when generalized to rings, attach more naturally to the ideals than to the elements of the ring. For instance, the prime ideals of a ring are analogous to prime numbers, and the Chinese remainder theorem can be generalized to ideals. There is a version of unique prime factorization for the ideals of a Dedekind domain (a type of ring important in number theory).\n\nThe concept of an order ideal in order theory is derived from the notion of ideal in ring theory. A fractional ideal is a generalization of an ideal, and the usual ideals are sometimes called integral ideals for clarity.\n\nIdeals were first proposed by Richard Dedekind in 1876 in the third edition of his book \"Vorlesungen über Zahlentheorie\" (English: \"Lectures on Number Theory\"). They were a generalization of the concept of ideal numbers developed by Ernst Kummer. Later the concept was expanded by David Hilbert and especially Emmy Noether.\n\nFor an arbitrary ring formula_1, let formula_2 be its additive group. A subset formula_3 is called a two-sided ideal (or simply an ideal) of formula_4 if it is an additive subgroup of formula_4 that \"absorbs multiplication by elements of formula_4.\" Formally we mean that formula_3 is an ideal if it satisfies the following conditions:\n\nEquivalently, an ideal of formula_4 is a sub-formula_4-bimodule of formula_4.\n\nA subset formula_3 of formula_4 is called a right ideal of formula_4 if it is an additive subgroup of formula_4 and absorbs multiplication on the right, that is:\nEquivalently, a right ideal of formula_4 is a right formula_4-submodule of formula_4.\n\nSimilarly a subset formula_3 of formula_4 is called a left ideal of formula_4 if it is an additive subgroup of formula_4 absorbing multiplication on the left:\nEquivalently, a left ideal of formula_4 is a left formula_4-submodule of formula_4.\n\nIn all cases, the first condition can be replaced by the following well-known criterion that ensures a nonempty subset of a group is a subgroup:\n\nThe left ideals in formula_4 are exactly the right ideals in the opposite ring formula_37 and vice versa.\n\nA two-sided ideal is a left ideal that is also a right ideal, and is often called an ideal except to emphasize that there might exist single-sided ideals. When formula_4 is a commutative ring, the definitions of left, right, and two-sided ideal coincide, and the term \"ideal\" is used alone.\n\nJust as normal subgroups of groups are kernels of group homomorphisms, ideals have interpretations as kernels. For a nonempty subset \"A\" of \"R\": \n\nIf \"p\" is in \"R\", then \"pR\" is a right ideal and \"Rp\" is a left ideal of \"R\". These are called, respectively, the principal right and left ideals generated by \"p\". To remember which is which, note that right ideals are stable under right-multiplication (\"IR\" ⊆ \"I\") and left ideals are stable under left-multiplication (\"RI\" ⊆ \"I\").\n\nThe connection between cosets and ideals can be seen by switching the operation from \"multiplication\" to \"addition\".\n\nIdeals formula_3 of a ring formula_4 are in some sense a generalisation of the set of multiples of an integer. That is, formula_41\n\nIntuitively, the definition can be motivated as follows: Suppose we have a subset formula_42 of a ring formula_4 and that we would like to obtain a ring with the same structure as formula_4, except that the elements of formula_42 should be zero (they are in some sense \"negligible\").\n\nBut if formula_46 and formula_47 in our new ring, then surely formula_48 should be zero too, and formula_49 as well as formula_50 should be zero for \"any\" element formula_51 (zero or not).\n\nThe definition of an ideal is such that the ideal formula_3 generated (see below) by formula_42 is exactly the set of elements that are forced to become zero if formula_42 becomes zero, and the quotient ring formula_55 is the desired ring where formula_42 is zero, and \"only\" elements that are forced by formula_42 to be zero are zero. The requirement that formula_4 and formula_55 should have the same structure (except that formula_3 becomes zero) is formalized by the condition that the projection from formula_4 to formula_55 is a (surjective) ring homomorphism.\n\nThe quintessential example is when formula_63 and formula_64 for some nonzero formula_65. By \"considering formula_66 to be zero\", we are essentially reducing elements of formula_67 to their remainder after dividing by n. Thus formula_68 and formula_69 is the set of possible remainders after dividing an integer by n (ie. the cyclic group of order n).\n\n\nLet \"R\" be a (possibly not unital) ring. Any intersection of any nonempty family of left ideals of \"R\" is again a left ideal of \"R\". If \"X\" is any subset of \"R\", then the intersection of all left ideals of \"R\" containing \"X\" is a left ideal \"I\" of \"R\" containing \"X\", and is clearly the smallest left ideal to do so. This ideal \"I\" is said to be the left ideal generated by \"X\". Similar definitions can be created by using right ideals or two-sided ideals in place of left ideals.\n\nIf \"R\" has unity, then the left, right, or two-sided ideal of \"R\" generated by a subset \"X\" of \"R\" can be expressed internally as we will now describe. The following set is a left ideal:\n\nEach element described would have to be in every left ideal containing \"X\", so this left ideal is in fact the left ideal generated by \"X\". The right ideal and ideal generated by \"X\" can also be expressed in the same way:\nThe former is the right ideal generated by \"X\", and the latter is the ideal generated by \"X\".\n\nBy convention, 0 is viewed as the sum of zero such terms, agreeing with the fact that the ideal of \"R\" generated by ∅ is {0} by the previous definition.\n\nIf a left ideal \"I\" of \"R\" has a finite subset \"F\" such that \"I\" is the left ideal generated by \"F\", then the left ideal \"I\" is said to be finitely generated. Similar terms are also applied to right ideals and two-sided ideals generated by finite subsets.\n\nIn the special case where the set \"X\" is just a singleton {\"a\"} for some \"a\" in \"R\", then the above definitions turn into the following:\n\nThese ideals are known as the left/right/two-sided principal ideals generated by \"a\". It is also very common to denote the two-sided ideal generated by \"a\" as (\"a\").\n\nIf \"R\" does not have a unit, then the internal descriptions above must be modified slightly. In addition to the finite sums of products of things in \"X\" with things in \"R\", we must allow the addition of \"n\"-fold sums of the form \"x\"+\"x\"+...+\"x\", and \"n\"-fold sums of the form (\"−x\")+(\"−x\")+...+(\"−x\") for every \"x\" in \"X\" and every \"n\" in the natural numbers. When \"R\" has a unit, this extra requirement becomes superfluous.\n\n\n\n\"To simplify the description all rings are assumed to be commutative. The non-commutative case is discussed in detail in the respective articles.\"\n\nIdeals are important because they appear as kernels of ring homomorphisms and allow one to define factor rings. Different types of ideals are studied because they can be used to construct different types of factor rings.\n\n\nTwo other important terms using \"ideal\" are not always ideals of their ring. See their respective articles for details:\n\n\nThe sum and product of ideals are defined as follows. For formula_92 and formula_93, ideals of a ring R,\n\nand\n\ni.e. the product of two ideals formula_92 and formula_93 is defined to be the ideal formula_98 generated by all products of the form \"ab\" with \"a\" in formula_92 and \"b\" in formula_93. The product formula_98 is contained in the intersection of formula_92 and formula_93.\n\nNote that formula_92 + formula_93 is also the intersection of all ideals containing both formula_92 and formula_93.\n\nThe sum and the intersection of ideals is again an ideal; with these two operations as join and meet, the set of all ideals of a given ring forms a complete modular lattice. Also, the union of two ideals is a subset of the sum of those two ideals, because for any element \"a\" inside an ideal, we can write it as \"a\"+0, or 0+\"a\"; therefore, it is contained in the sum as well. However, the union of two ideals is not necessarily an ideal; rather, the sum is the ideal generated by the union.\n\nThe three operations of intersection, sum (or join), and product make the set of ideals of a commutative ring into a quantale.\n\nIn formula_67 we have\nsince formula_110 is the set of integers which are divisible by both formula_66 and formula_112.\n\nLet formula_113 and let formula_114. Then,\nIn the first computation, we see the general pattern for taking the sum of two finitely generated ideals, it is the ideal generated by the union of their generators. In the last three we observe that products and intersections agree whenever the two ideals intersect in the zero ideal. These computations can be checked using Macaulay2.\n\nThere is a bijective correspondence between ideals and congruence relations (equivalence relations that respect the ring structure) on the ring:\n\nGiven an ideal \"I\" of a ring \"R\", let \"x\" ~ \"y\" if \"x\" − \"y\" ∈ \"I\". Then ~ is a congruence relation on \"R\".\n\nConversely, given a congruence relation ~ on \"R\", let \"I\" = {\"x\" : \"x\" ~ 0}. Then \"I\" is an ideal of \"R\".\n\n\n"}
{"id": "49492438", "url": "https://en.wikipedia.org/wiki?curid=49492438", "title": "International Conference on Green Chemistry", "text": "International Conference on Green Chemistry\n\nThe International IUPAC Conferences on Green Chemistry (ICGCs) gather several hundreds scientists, technologists, and experts from all over the world with the aim to exchange and disseminate new ideas, discoveries, and projects on green chemistry and a sustainable development. After mid twentieth century, an increasingly general consensus acknowledges that these subjects play a unique role in mapping the way ahead for the humankind progress. Typical topics discussed in these IUPAC Conferences are: \n\nIn 2006 the International Union of Pure and Applied Chemistry (IUPAC) promoted the organization of the 1st International IUPAC Conference on Green-Sustainable Chemistry (ICGC-1). This conference, started in collaboration with the German Chemical Society (GDCh), was a major acknowledgement by IUPAC of the relevance of green chemistry. The Special Topic Issue on Green Chemistry in \"Pure and Applied Chemistry\" and the starting of a Subcommittee on Green Chemistry, operating in the IUPAC Division of Organic and Biomolecular Chemistry, were two important landmarks towards that acknowledgement. \nICGC-1 registered the presence of over 450 participants from 42 countries and proceedings were published in \"Pure and Applied Chemistry\". This Conference then became a biannual appointment that continuously attracted several hundreds scientists and technologists from academia, research institutes, and industries.\n\nOn 14 of July 2017, IUPAC established the Interdivisional Committee on Green Chemistry for Sustainable Development ICGCSD that supersedes the former Subcommittee on Green Chemistry and has the aim to assist IUPAC in initiating, promoting, and coordinating the work of the Union in the area of green and sustainable chemistry. \n\nICGCSD will continue to organize the ICGCs Series. The next 8th Conference will take place in Bangkok, Thailand, in September 2018. \n"}
{"id": "37446136", "url": "https://en.wikipedia.org/wiki?curid=37446136", "title": "L. A. Paul", "text": "L. A. Paul\n\nLaurie Ann (L. A.) Paul is a professor of philosophy and cognitive science at Yale University. She previously taught at the University of North Carolina at Chapel Hill and the University of Arizona. She is best known for her research on the counterfactual analysis of causation and the concept of \"transformative experience.\"\n\nPaul graduated from Antioch College in Yellow Springs, Ohio in 1990 with a B.A. in chemistry. Before going to graduate school, Paul corresponded with a number of philosophers about their work, including Nancy Cartwright and Lynne Rudder Baker. In 1999, Paul graduated from Princeton University with a Ph.D in philosophy, where she wrote a dissertation titled \"Essays on Causation\" under the supervision of David Lewis.\n\nPaul taught at Yale University from 1999 to 2001, and at the University of Arizona from 2001 until 2008, before moving to North Carolina. She has also held appointments at the Australian National University and at the University of St. Andrews.\n\nShe is married to sociologist Kieran Healy.\n\nPaul's principal research interests are in metaphysics and the philosophy of mind. Her work focuses on causation, mereology, the philosophy of time, and related topics in phenomenology, the philosophy of science, and philosophy of language. Her work in ontology and mereology develops a distinctive view of objects as fusions of property instances. Her article \"What You Can't Expect When You're Expecting\" develops the notion of transformative experience and explores its consequences for the possibility of rational decision-making.\n\nShe has written more than twenty articles, and is the editor of \"Causation and Counterfactuals\", co-author of \"Causation: A User's Guide\", and author of \"Transformative Experience.\"\n\nPaul has received the following awards:\n\n\n"}
{"id": "29857137", "url": "https://en.wikipedia.org/wiki?curid=29857137", "title": "Marconi's law", "text": "Marconi's law\n\nMarconi's law is the relation between length of antennas and maximum signaling distance of radio transmissions. Guglielmo Marconi enunciated at one time an empirical law that, for simple vertical sending and receiving antennas of equal height, the maximum working telegraphic distance varied as the square of the height of the antenna. It has been stated that the rule was tested in experiments made on Salisbury Plain in 1897, and also by experiments made by Italian naval officers on behalf of the Royal Italian Navy in 1900 and 1901. Captain Quintino Bonomo gave a report of these experiments in an official report.\n\nIf \"H\" is the height (i.e. the length) of the antenna and \"D\" the maximum signalling distance, then we have, according to \"Marconi's law\"\nwhere \"c\" is some constant.\n\n\"Marconi's law\" can be deduced theoretically as follows:\nHertz has shown that distances large compared with its length the magnetic force of a linear oscillator varies inversely as the distance. The maximum value of the current set up in any given receiving antenna varies as its length, also as the magnetic force of the waves incident on it, and as the maximum value of the current in the transmitting antenna. Hence, if formula_2 is the magnetic force of the waves incident on a receiving antenna of height H, and if formula_3 is the distance between the sending and receiving antenna, and if formula_4 and formula_5 are the maximum values of the currents in the sending and receiving antennæ, we have—\n\nformula_6 and formula_7\n\nHence formula_8\n\nAlso, since for a given charging voltage the current formula_4 in the sending antenna varies very nearly as its capacity—that is, as its height—and if the sending antenna has the same height, formula_10, as the receiving aerial, we have—\n\nformula_11\n\nBut formula_12\n\nTherefore formula_13 some constant\n\nFor any given receiving apparatus a certain constant minimum value of the maximum current in the receiving antenna is necessary to cause a signal. Therefore it follows that, with given receiving and sending apparatus, we must have formula_14 a constant, or—\n\nformula_1\n\nThat is, the maximum signalling distance with given apparatus will vary as the square of the height of the antenna.\n\nThe above law is, however, much interfered with by the nature of the surface over which the propagation takes place.\n\n\n"}
{"id": "4153402", "url": "https://en.wikipedia.org/wiki?curid=4153402", "title": "Meaning (semiotics)", "text": "Meaning (semiotics)\n\nIn semiotics, the meaning of a sign is its place in a sign relation, in other words, the set of roles that it occupies within a given sign relation. \n\nThis statement holds whether \"sign\" is taken to mean a \"sign type\" or a \"sign token\". Defined in these global terms, the meaning of a sign is not in general analyzable with full exactness into completely localized terms, but aspects of its meaning can be given approximate analyses, and special cases of sign relations frequently admit of more local analyses.\n\nTwo aspects of meaning that may be given approximate analyses are the \"connotative relation\" and the \"denotative relation\". The connotative relation is the relation between signs and their interpretant signs. The denotative relation is the relation between signs and objects. An arbitrary association exists between the \"signified\" and the \"signifier.\"\nFor example, a US salesperson doing business in Japan might interpret silence following an offer as rejection, while to Japanese negotiators silence means the offer is being considered. This difference in interpretations represents a difference in: semiotics\n\nThe triadic (three part) model of the sign separates the meaning of a sign into three distinct components:\n\n1.The representamen, which is the medium, or ‘sign vehicle’, through which the sign is represented. For example, this could be written/spoken words, a photograph, or a painting.\n\n2.The interpretant, or what is meant by the sign \n\n3. The object, or that to which the sign refers \n\nTogether, these three components generate semiosis. For example, an exclamation mark can be broken down into these components. The representamen is the exclamation mark itself, the interpretant is the idea of excitement or an elevated volume of speech, and the object is the actual excitement or elevated volume of speech to which it refers.\nWhile it might appear that the latter two are the same, the subtle difference lies in the fact that the interpretant refers to the idea of something, and the object is the thing itself.\n\nThe representamen component of the sign can be further broken down into three categories, which are icon, index, and symbol. These denote the degree of abstraction from the object to which they refer. A symbol, which is the most abstract, does not resemble or bear any physical relation to the thing that it represents in any way. For example, a peace sign has no relation to peace aside from its social construction as a symbol that represents it. An icon is slightly less abstract, and resembles to some degree the thing that it represents, and bears some physical likeness to it. A good example of this would be a painted portrait. An index is the least arbitrary category of representamen, and has a definite physical tie to that which it represents. This could be something like a weather vane blowing in the wind indicating that it is windy out, or smoke, which indicates a fire.\n\nThe triadic model of the sign was proposed by Charles Peirce. In contradistinction to Ferdinand de Saussure's dyadic model, which assumed no material referent, Peirce's model assumes that in order for a sign to be meaningful, it must refer to something external and cannot be self-contained, as it is for Saussure. Thus, Peirce's model includes the addition of an 'object'. The ‘representamen’ and ‘interpretant’ components of the triadic model are comparable to Saussure’s dyadic model of the sign, which breaks down into signifier and signified.\n\n\n"}
{"id": "951950", "url": "https://en.wikipedia.org/wiki?curid=951950", "title": "Meyer v. Nebraska", "text": "Meyer v. Nebraska\n\nMeyer v. Nebraska, 262 U.S. 390 (1923), was a U.S. Supreme Court case that held that a 1919 Nebraska law restricting foreign-language education violated the Due Process clause of the Fourteenth Amendment.\n\nWorld War I witnessed an extensive campaign against all things German, such as the performance of German music at symphony concerts and the meetings of German-American civic associations. Language was a principal focus of legislation at the state and local level. It took many forms, from requiring associations to have charters written in English to a ban on the use of German within the town limits. Some states banned foreign language instruction, while a few banned only German. Some extended their bans into private instruction and even to religious education. A bill to create a Department of Education at the federal level was introduced in October 1918, designed to restrict federal funds to states that enforced English-only education. An internal battle over conducting services and religious instruction in German divided the Lutheran churches.\n\nOn April 9, 1919, Nebraska enacted a statute called \"An act relating to the teaching of foreign languages in the state of Nebraska,\" commonly known as the Siman Act. It imposed restrictions on both the use of a foreign language as a medium of instruction and on foreign languages as a subject of study. With respect to the use of a foreign language while teaching, it provided that \"No person, individually or as a teacher, shall, in any private, denominational, parochial or public school, teach any subject to any person in any language other than the English language.\" With respect to foreign-language education, it prohibited instruction of children who had yet to successfully complete the eighth grade.\n\nOn May 25, 1920, Robert T. Meyer, while an instructor in Zion Parochial School, a one-room schoolhouse in Hampton, Nebraska, taught the subject of reading in the German language to 10-year-old Raymond Parpart, a fourth-grader. The Hamilton County Attorney entered the classroom and discovered Parpart reading from the Bible in German. He charged Meyer with violating the Siman Act.\n\nMeyer was tried and convicted in the district court for Hamilton County, was and fined $25 USD (about $ in dollars). The Nebraska Supreme Court affirmed his conviction by a vote of 4 to 2. The majority thought the law a proper response to \"the baneful effects\" of allowing immigrants to educate their children in their mother tongue, with results \"inimical to our own safety.\" The dissent called the Siman Act the work of \"crowd psychology.\"\n\nMeyer appealed to the Supreme Court of the United States. His lead attorney was Arthur Mullen, an Irish Catholic and a prominent Democrat, who had earlier failed in his attempt to obtain an injunction against enforcement of the Siman Act from the Nebraska State Supreme Court. Oral arguments expressed conflicting interpretations of the World War I experience. Mullen attributed the law to \"hatred, national bigotry and racial prejudice engendered by the World War.\" Opposing counsel countered that \"it is the ambition of the State to have its entire population 100 per cent. American.\"\n\nIn his decision, Justice McReynolds stated that the \"liberty\" protected by the Due Process clause \"[w]ithout doubt...denotes not merely freedom from bodily restraint but also the right of the individual to contract, to engage in any of the common occupations of life, to acquire useful knowledge, to marry, establish a home and bring up children, to worship God according to the dictates of his own conscience, and generally to enjoy those privileges long recognized at common law as essential to the orderly pursuit of happiness by free men.\"\n\nAnalyzing in that context the liberty of the teacher and of parents with respect to their children, McReynolds wrote: \"Practically, education of the young is only possible in schools conducted by especially qualified persons who devote themselves thereto. The calling always has been regarded as useful and honorable, essential, indeed, to the public welfare. Mere knowledge of the German language cannot reasonably be regarded as harmful. Heretofore it has been commonly looked upon as helpful and desirable. Plaintiff in error taught this language in school as part of his occupation. His right thus to teach and the right of parents to engage him so to instruct their children, we think, are within the liberty of the amendment.\" And further: \"Evidently the Legislature has attempted materially to interfere with the calling of modern language teachers, with the opportunities of pupils to acquire knowledge, and with the power of parents to control the education of their own.\"\n\nAnd finally: \"That the state may do much, go very far, indeed, in order to improve the quality of its citizens, physically, mentally and morally, is clear; but the individual has certain fundamental rights which must be respected. The protection of the Constitution extends to all, to those who speak other languages as well as to those born with English on the tongue. Perhaps it would be highly advantageous if all had ready understanding of our ordinary speech, but this cannot be coerced by methods which conflict with the Constitution​—​a desirable end cannot be promoted by prohibited means.\"\n\nHe allowed that wartime circumstances might justify a different understanding, but that Nebraska had not demonstrated sufficient need \"in time of peace and domestic tranquility\" to justify \"the consequent infringement of rights long freely enjoyed.\"\n\nJustices Oliver Wendell Holmes and George Sutherland dissented. Their dissenting opinion, written by Holmes, is found in the companion case of \"Bartels v. State of Iowa\". Holmes wrote that he differed with the majority \"with hesitation and unwillingness\" because he thought the law did not impose an undue restriction on the liberty of the teacher since it was not arbitrary, was limited in its application to the teaching of children, and the State had areas where many children might hear only a language other than English spoken at home. \"I think I appreciate the objection to the law, but it appears to me to present a question upon which men reasonably might differ and therefore I am unable to say the Constitution of the United States prevents the experiment being tried.\"\n\n\"Meyer\", along with \"Pierce v. Society of Sisters\" (1925), is often cited as one of the first instances in which the U.S. Supreme Court engaged in substantive due process in the area of civil liberties. Laurence Tribe has called them \"the two sturdiest pillars of the substantive due process temple\". He noted that the decisions in these cases did not describe specific acts as constitutionally protected but a broader area of liberty: \"[they] described what they were protecting from the standardizing hand of the state in language that spoke of the family as a center of value-formation and value-transmission...the authority of parents to make basic choices\" and not just controlling the subjects one's child is taught. Substantive due process has since been used as the basis for many far-reaching decisions of the Court, including \"Roe v. Wade\", \"Planned Parenthood v. Casey\", and \"Lawrence v. Texas\". Justice Kennedy speculated in 2000 that both of those cases might have been written differently nowadays: \"\"Pierce\" and \"Meyer\", had they been decided in recent times, may well have been grounded upon First Amendment principles protecting freedom of speech, belief, and religion.\"\n\n"}
{"id": "11243865", "url": "https://en.wikipedia.org/wiki?curid=11243865", "title": "Miško Šuvaković", "text": "Miško Šuvaković\n\nMiško Šuvaković () is a contemporary aestheticist, art theorist and conceptual artist born in 1954 in Belgrade, Yugoslavia. He taught theory of art and theory of culture in Interdisciplinary Postgraduates Studies at the University of Arts in Belgrade. He teaches theory of art and theory of culture in Transdisciplinar master and doctoral studies at the Faculty of media and communication.\n\nHe was co-founder and member of conceptual artistic Group 143 (1975–1980), and was co-founder and member of informal theoretic and artistic \"Community for Space Investigation\" (1982–1989). He has participated in TkH – theory platform from October 2000. From 1988 he is the member of Slovenian Aesthetic Society. In March 1993 he got PhD degree with theme \"Analytical philosophy and Theory in Art\" (Faculty of Visual Arts, Belgrade, University of Arts in Belgrade ). He teaches aesthetics and theory of art, Faculty of Music, Belgrade (Professor). He was co-editor of magazine Katalog 143 (Belgrade, 1975–78), Mentalni prostor (Belgrade, 1982–1987), Transkatalog (Novi Sad, 1995–1998), Teorija koja hoda (Walking Theory, Belgrade, from 2001), Kultura (Beograd, from 2004), Razlika (Difference, Tuzla, 2002), Anomalija (Novi Sad, 2004), Sarajevske sveske (Sarajevo, Zagreb, Ljubljana, Beograd, Skoplje, 2006).\n\n\n\n\n\n"}
{"id": "164095", "url": "https://en.wikipedia.org/wiki?curid=164095", "title": "Moral panic", "text": "Moral panic\n\nA moral panic is a feeling of fear spread among a large number of people that some evil threatens the well-being of society. \"A Dictionary of Sociology\" defines a moral panic as \"the process of arousing social concern over an issue – usually the work of moral entrepreneurs and the mass media\".\n\nThe media are key players in the dissemination of moral indignation, even when they do not appear to be consciously engaged in crusading or muckraking. Simply reporting the facts can be enough to generate concern, anxiety, or panic. Stanley Cohen states that moral panic happens when \"a condition, episode, person or group of persons emerges to become defined as a threat to societal values and interests.\" Examples of moral panic include the belief in widespread abduction of children by predatory paedophiles, belief in ritual abuse of women and children by satanic cults, the War on Drugs, and other public health issues.\n\nMarshall McLuhan gave the term academic treatment in his book \"Understanding Media\", written in 1964. According to Stanley Cohen, author of a sociological study about youth culture and media called \"Folk Devils and Moral Panics\" (1972), a moral panic occurs when \"...[a] condition, episode, person or group of persons emerges to become defined as a threat to societal values and interests\". Those who start the panic when they fear a threat to prevailing social or cultural values are known by researchers as 'moral entrepreneurs', while people who supposedly threaten the social order have been described as 'folk devils'.\n\nMany sociologists have pointed out the differences between definitions of a \"moral panic\" as described by American versus British sociologists. In addition to pointing out other sociologists who note the distinction, Kenneth Thompson has characterized the difference as American sociologists tending to emphasize psychological factors while the British portray \"moral panics\" as crises of capitalism.\n\nBritish criminologist Jock Young used the term in his participant observation study of drug taking in Porthmadog between 1967 and 1969. In \"Policing the Crisis: Mugging, the State and Law and Order\" (1978), Stuart Hall and his colleagues studied the public reaction to the phenomenon of mugging and the perception that it had recently been imported from American culture into the UK. Employing Cohen's definition of 'moral panic', Hall \"et al.\" theorized that the \"...rising crime rate equation...\" performs an ideological function relating to social control. Crime statistics, in Hall's view, are often manipulated for political and economic purposes; moral panics could thereby be ignited to create public support for the need to \"...police the crisis\".\n\nAccording to Stanley Cohen, who seems to have borrowed the term from Marshall McLuhan (see above), there are five key stages in the construction of a moral panic:\n\nIn 1971 Stanley Cohen investigated a series of \"moral panics\". Cohen used the term \"moral panic\" to characterize the reactions of the media, the public, and agents of social control to youth disturbances. This work, involving the Mods and Rockers, demonstrated how agents of social control amplified deviance. According to Cohen, these groups were labeled as being outside the central core values of consensual society and as posing a threat to both the values of society and society itself, hence the term \"folk devils\".\n\nIn a more recent edition of \"Folk Devils and Moral Panics,\" Cohen outlines some of the criticisms that have arisen in response to moral panic theory. One of these is of the term \"panic\" itself, as it has connotations of irrationality and a lack of control. Cohen maintains that \"panic\" is a suitable term when used as an extended metaphor.\n\nAccording to Stanley Cohen in \"Folk Devils and Moral Panics\", the concept of \"moral panic\" was linked to certain assumptions about the mass media. Stanley Cohen showed that the mass media are the primary source of the public's knowledge about deviance and social problems. He further argued that moral panic gives rise to the folk devil by labeling actions and people.\n\nAccording to Cohen, the media appear in any or all three roles in moral panic dramas:\n\nMoral panics have several distinct features. According to Goode and Ben-Yehuda, moral panic consists of the following characteristics:\n\nThe fear of disease (or fear of threats to public health) and spread of panic dates back many centuries and continues into the 21st century with diseases such as AIDS, Ebola, H1N1, Zika, and SARS. Cohen's idea of the \"folk devil\" and epidemics can be compared because of their role in spreading mass panic and fear. The intense concentration on hygiene emerged, before the 20th century, with a medical belief referred to as miasma theory, which states that disease was the direct result of the polluting emanations of filth: sewer gas, garbage fumes, and stenches that polluted air and water, which results in an epidemic. The Great Stink of 1858 was blamed on miasma, along with reoccurring cholera epidemics during the Victorian era. Although the water was safe to drink in most parts of London, such a panic had arisen that very few people would dare drink the water.\n\nIn the United States, a 1950 article titled \"The Toy That Kills\" in the \"Women's Home Companion\", about automatic knives, or \"switchblades\", sparked a storm of controversy, further fed by highly popular films of the late 1950s including \"Rebel Without a Cause\", \"Crime in the Streets\", \"12 Angry Men\", \"The Delinquents\", \"High School Confidential\", and the 1957 Broadway musical \"West Side Story\". Fixation on the switchblade as the symbol of youth violence, sex, and delinquency resulted in demands from the public and Congress to control the sale and possession of such knives. State laws restricting or criminalizing switchblade possession and use were adopted by an increasing number of state legislatures, and many of the restrictive laws around them worldwide date back to this period.\n\nResearch shows that fear of increasing crime is often the cause of moral panics. Recent studies have shown that despite declining crime rates, this phenomenon, which often taps into a population's \"herd mentality\", continues to occur in various cultures. Japanese jurist explains how the changes in crime recording in Japan since the 1990s caused people to believe that the crime rate was rising and that crimes were getting increasingly severe.\n\nThere have been calls to regulate violence in video games for nearly as long as the video game industry has existed, with \"Death Race\" a notable early example. In the 1990s, however, improvements in video game technology allowed for more lifelike depictions of violence in games like \"Mortal Kombat\" and \"Doom\". The industry attracted controversy over violent content and concerns about effects they might have on players, generating frequent media stories drawing connections between video games and violent behavior as well as a number of academic studies reporting conflicting findings about the strength of correlations. According to Christopher Ferguson, sensationalist media reports and the scientific community unintentionally worked together in \"promoting an unreasonable fear of violent video games\". Concerns from parts of the public about violent games led to cautionary, often exaggerated news stories, warnings from politicians and other public figures, and calls for research to prove the connection, which in turn led to studies \"speaking beyond the available data and allowing the promulgation of extreme claims without the usual scientific caution and skepticism.\"\n\nSince the 1990s, there have been attempts to regulate violent video games in the United States through congressional bills as well as within the industry. Public concern and media coverage of violent video games reached a high point following the Columbine High School massacre in 1999, after which videos were found of the perpetrators talking about violent games like \"Doom\" and making comparisons between the acts they intended to carry out and aspects of games.\n\nFerguson and others have explained the video game moral panic as part of a cycle that all new media go through. In 2011, the U.S. Supreme Court ruled that legally restricting sales of video games to minors would be unconstitutional and called the research presented in favor of regulation \"unpersuasive\".\n\nSome critics have pointed to moral panic as an explanation for the War on Drugs. For example, a Royal Society of Arts commission concluded that \"the Misuse of Drugs Act 1971 ... is driven more by 'moral panic' than by a practical desire to reduce harm.\"\n\nSome have written that one of the many rungs supporting the moral panic behind the war on drugs was a separate but related moral panic, which peaked in the late 1990s, involving media's gross exaggeration of the frequency of the surreptitious use of date rape drugs. News media have been criticized for advocating \"grossly excessive protective measures for women, particularly in coverage between 1996 and 1998,\" for overstating the threat and for excessively dwelling on the topic. For example, a 2009 Australian study found that drug panel tests were unable to detect any drug in any of the 97 instances of patients admitted to the hospital believing their drinks might have been spiked.\n\nAt various times, \"Dungeons & Dragons\" and other tabletop role-playing games have been accused of promoting such practices as Satanism, witchcraft, suicide, pornography and murder. In the 1980s and later, some groups, especially fundamentalist Christian groups, accused the games of encouraging interest in sorcery and the veneration of demons. While many of these criticisms have been aimed specifically at \"Dungeons & Dragons\", they touch on the genre of fantasy roleplaying games as a whole.\n\nAlso known as the \"satanic panic\", this was a series of moral panics regarding Satanic ritual abuse that originated in the United States and spread to other English-speaking countries in the 1980s and 1990s, and led to a string of wrongful convictions.\n\nAcquired immune deficiency syndrome (AIDS) may lead to or exacerbate other health conditions such as pneumonia, fungal infections, tuberculosis, toxoplasmosis, and cytomegalovirus. A meeting of the British Sociological Association's South West and Wales Study entitled \"AIDS: The Latest Moral Panic\" was prompted by the growing interest of medical sociologists in AIDS, as well as that of UK health care professionals working in the field of health education. It took place at a time when both groups were beginning to voice an increased concern with the growing media attention and fear-mongering that AIDS was attracting. In the 1980s, a moral panic was created within the media over HIV/AIDS. In Britain the notable iceberg advert by the government clearly hinted that the public was uninformed about HIV/AIDS due to a lack of publicly assessable and accurate information.\n\nThe media outlets nicknamed HIV/AIDS the \"gay plague\", causing further stigmatization and misunderstandings about the disease. However, scientists gained a far better understanding of HIV/AIDS as it grew in the 1980s and moved into the 1990s and beyond. The illness was still negatively viewed by many as either caused by, or passed on through, the gay community. Once it became clear that this wasn't the case, the moral panic created by the media changed to blaming the overall negligence of ethical standards of the younger generation (both male and female), resulting in another moral panic. It is prevalent in the media and the way HIV/AIDS is depicted taken from this extract, \"British TV and press coverage is locked into an agenda which blocks out any approach to the subject which does not conform in advance to the values and language of a profoundly homophobic culture-a culture that is which does not regard gay men as fully or properly human. No distinction obtains for the agenda between 'quality' and 'tabloid' newspapers, or between 'popular' and 'serious' television.\"\n\nIn the 1990s, blame shifted to \"uncivilized Africans\" as the new \"folk devils\", with a popular theory alleging that HIV originated from humans having sex with simians. This theory was debunked by numerous experts.\n\nThe media narrative of a sex offender highlighting egregious offenses as typical behavior of any sex offender, and media distorting the facts of some cases, has led legislators to attack judicial discretion, making sex offender registration mandatory based on certain listed offenses rather than individual risk or the actual severity of the crime, thus practically catching less serious offenders under the domain of harsh sex offender laws.\n\nIn the 1990s and 2000s, there have been instances of moral panics in the UK and the US related to colloquial uses of the term pedophilia to refer to such unusual crimes as high-profile cases of child abduction.\n\nMany critics of contemporary anti-prostitution activism argue that much of the current concern about human trafficking and its more general conflation with prostitution and other forms of sex work have all the hallmarks of a moral panic. They further argue that this moral panic shares much in common with the 'white slavery' panic of a century earlier as prompted passage of the 1910 Mann Act.\n\nThe so-called \"Blue Whale\" hypothetical communities encouraging minors to commit suicide in alternative reality game form. As with many other moral panics, this one may have been started with an article in a newspaper. Some people consider it as a hoax with the objective to increase approval to Internet censorship in societies. Some governments have taken legislative actions strengthening internet censorship and introducing new felonies in the wake of the panic.\n\nPaul Joosse (2017) has argued that while classic moral panic theory styled itself as being part of the 'sceptical revolution' that sought to critique structural functionalism, it is actually very similar to Durkheim's depiction of how the collective conscience is strengthened through its reactions to deviance (in Cohen's case, for example, 'right-thinkers' use folk devils to strengthen societal orthodoxies). In his analysis of Donald Trump's 2016 electoral victory, Joosse reimagines moral panic in Weberian terms, showing how charismatic moral entrepreneurs can at once deride folk devils in the traditional sense while avoiding the conservative moral recapitulation that classic moral panic theory predicts.\n\nAnother criticism is that of disproportionality. The problem with this argument is that there is no way to measure what a proportionate reaction should be to a specific action. Jarrett Thibodeaux (2014) further argues that the criteria of disproportionality erroneously assumes that a social problem \"should\" correspond with some objective criteria of harm. The idea that a social problem should correspond with some objective criteria of harm, but is a moral panic when it does not, is a 'constructionism of the gaps' line of explanation.\n\nWriting in 1995 about the moral panic that arose in the UK after a series of murders by juveniles, chiefly that of two-year-old James Bulger by two ten-year-old boys but also including that of 70-year-old Edna Phillips by two 17-year-old girls, the sociologist Colin Hay pointed out that the folk devil was ambiguous in such cases; the child perpetrators would normally be thought of as innocent.\n\nIn \"Rethinking 'moral panic' for multi-mediated social worlds\", Angela McRobbie and Sarah Thornton argue \"that it is now time that every stage in the process of constructing a moral panic, as well as the social relations which support it, should be revised.\" Their argument is that mass media has changed since the concept of moral panic emerged so \"that 'folk devils' are less marginalized than they once were\", and that 'folk devils' are not only castigated by mass media but supported and defended by it as well. They also suggest that the \"points of social control\" that moral panics used to rest on \"have undergone some degree of shift, if not transformation.\"\n\nThe British criminologist Yvonne Jewkes has also raised issue with the term 'morality', how it is accepted unproblematically in the concept of 'moral panic' and how most research into moral panics fails to approach the term critically but instead accepts it at face value. Jewkes goes on to argue that the thesis and the way it has been used fails to distinguish between crimes that quite rightly offend human morality, and thus elicit a justifiable reaction, and those that demonise minorities. The public are not sufficiently gullible to keep accepting the latter and allowing themselves to be manipulated by the media and the government.\n\nAnother British criminologist, Steve Hall, goes a step further to suggest that the term 'moral panic' is a fundamental category error. Hall argues that although some crimes are sensationalized by the media, in the general structure of the crime/control narrative the ability of the existing state and criminal justice system to protect the public is also overstated. Public concern is whipped up only for the purpose of being soothed, which produces not panic but the opposite, comfort and complacency.\n\nEchoing another point Hall makes, the sociologists Thompson and Williams argue that the concept of 'moral panic' is not a rational response to the phenomenon of social reaction, but itself a product of the irrational middle-class fear of the imagined working-class 'mob'. Using as an example a peaceful and lawful protest staged by local mothers against the re-housing of sex-offenders on their estate, Thompson and Williams show how the sensationalist demonization of the protesters by moral panic theorists and the liberal press was just as irrational as the demonization of the sex offenders by the protesters and the tabloid press.\n\nMany sociologists and criminologists (Ungar, Hier, Rohloff) have revised Cohen's original framework. The revisions are compatible with the way in which Cohen theorizes panics in the third \"Introduction to Folk Devils and Moral Panics\".\n\nThe term was used in 1830, in a way that completely differs from its modern social science application, by a religious magazine regarding a sermon. The phrase was used again in 1831, with an intent that is possibly closer to its modern use.\n\n\n"}
{"id": "16634886", "url": "https://en.wikipedia.org/wiki?curid=16634886", "title": "Moral perception", "text": "Moral perception\n\nMoral perception is a term used in ethics to denote the discernment of the morally salient qualities in particular situations. Moral perceptions are argued to be necessary to moral reasoning (see practical reason), the deliberation of what is the right thing to do. Moral perception is variously conceptualized by Aristotle, Hannah Arendt, and Martha C. Nussbaum. Lawrence Blum (1994) distinguishes moral perception from moral judgment. Whereas a person's judgment about what the moral course of action would be is the result of a conscious deliberation, the basis for that process is the perception of aspects of one's situation, which is different for each person. Moral perceptions are also particular in nature.\n"}
{"id": "12052182", "url": "https://en.wikipedia.org/wiki?curid=12052182", "title": "Murder of pregnant women", "text": "Murder of pregnant women\n\nMurder of pregnant women is a type of homicide often resulting from domestic violence. Domestic violence—or intimate partner violence (IPV)—is suffered by many, and when analyzing cases in which victims came forward, majority of them are women. Many of these women fear harm not just to themselves but also to their unborn children. Recently, more focus has been placed on pregnancy-associated deaths due to violence. IPV may begin when the victim becomes pregnant. Research has shown that abuse while pregnant is a red flag for pregnancy-associated homicide.\n\nThe murder of pregnant women represents a relatively recently studied class of murder. Limited statistics are available as there is no reliable system in place yet to track such cases. Whether pregnancy is a causal factor is hard to determine.\n\nABC News have reported that studies in Maryland, New York, and Chicago found that approximately 20 percent of women who die during pregnancy are victims of murder. In 2004, \"The Washington Post\" examined death-record data across the US which documented the killings of 1,367 pregnant women and new mothers since 1990. The newspaper reports:\n\n[T]he killings span racial and ethnic groups. In cases whose details were known, 67 percent of women were killed with firearms. Many women were slain at home — in bedrooms, living rooms, kitchens — usually by men they knew. Husbands. Boyfriends. Lovers.Isabelle Horon and Diana Cheng of the Maryland Department of Health and Mental Hygiene conducted a study of pregnancy-associated deaths in Maryland between 1993 and 1998, defining pregnancy-associated death as death during pregnancy or within a year of pregnancy ending. They found that homicide was the leading cause of pregnancy-associated death, accounting for 20% of such deaths. In contrast, homicide was only the fifth leading cause of death for women of reproductive age who had not been pregnant in the year previous to their death, and accounted for 6.4% of deaths in this group. The second leading cause of pregnancy-associated death, heart disease, accounted for 19%. A study using data from the District of Columbia also found that homicide was the leading cause of pregnancy-associated death.\n\nA CDC study reported that \"the overall pregnancy-associated homicide ratio was 1.7 deaths per 100 000 live births\", and that homicide was not the leading cause of pregnancy-associated death, though it was still highlighted as among the leading causes. However, Horon and Cheng argued that the CDC's study underestimated the problem because it used incomplete information. Using more sources of information, Horon and Cheng found that the rate of pregnancy-associated homicide in Maryland was 10.5 per 100,000 live births. They suggested a need for US states to improve their collection of data on pregnancy-associated deaths. \"The Washington Post\" reported in 2004 that \"13 states said they had no way of telling how many pregnant and postpartum women had been killed in recent years\". In 2003, California remedied this by changing its death certificate documentation to include a female victim's maternity status.\n\nHomicide was the second-leading cause of death among women ages 20 to 24 and fifth among women ages 25–34 in 1999. The top cause of death in both age groups is accidents.\n\nThe Unborn Victims of Violence Act, passed in 2004, defines a fetus as a \"child in uterus\" and a person as being a legal crime victim \"if a fetal injury or death occurs during the commission of a federal violent crime.\" In the U.S., 38 states have laws with more harsh penalties if the victim is murdered while pregnant. Some of these laws defining the fetus as being a person, \"for the purpose of criminal prosecution of the offender\" (National Conference of State Legislatures, 2008). Laci Peterson, murdered in 2002, is one of the more high-profile homicides.\n\nCurrently in the North Carolina Senate, a bill called the SB 353 Unborn Victims of Violence Act is being considered for legislation that would create a separate criminal offense for the death of a fetus when the mother is murdered. The North Carolina Coalition Against Domestic Violence does not support this law for numerous reasons including failure to see violence against the mother as the cause of the fetal death. The Coalition does, however, support the position of the National Network to End Domestic Violence regarding the Unborn Victims of Violence Act.\n\nWhile it is almost impossible to determine an exact intervention point to prevent pregnancy-associated homicides, some possible opportunities can be pinpointed. The medical community is one of those points. Women may feel safe speaking to their health care providers about the abuse, especially after discovering they are pregnant. Some medical office and hospital policies specify that doctors will examine the patient privately without allowing the partner access. In a 2001 editorial in the Journal of the American Medical Association, Victoria Frye writes, \"Homicide by intimate partners may offer a focal point for effective pregnancy-associated mortality prevention efforts because many of these women come into contact with the health care system before their deaths.\" Reviews of Intimate Partner Homicide policy and research has identified several needs: System-wide training in healthcare on signs of domestic violence and system-wide screening for domestic violence by health care providers, as well as the knowledge of where to refer women to that need services when abuse is disclosed.\n\nStatistics for pregnancy as being a motivating factor in the murder of a pregnant woman are usually unavailable. Motives may vary, with a woman's pregnancy at the time of death sometimes being coincidental.\n\n"}
{"id": "54002666", "url": "https://en.wikipedia.org/wiki?curid=54002666", "title": "Norwegian red deer", "text": "Norwegian red deer\n\nThe Norwegian red deer (\"Cervus elaphus atlanticus\") is a small subspecies of red deer native to Norway. Today it is farmed on a commercial basis since the 1980s.\n"}
{"id": "228026", "url": "https://en.wikipedia.org/wiki?curid=228026", "title": "Om", "text": "Om\n\nOm (, IAST: \"Oṃ\", Devanagari: ) is a sacred sound and a spiritual symbol in Hinduism, that signifies the essence of the ultimate reality, consciousness or Atman. It is a syllable that is chanted either independently or before a mantra in Hinduism, Buddhism, and Jainism.\n\nOm is part of the iconography found in ancient and medieval era manuscripts, temples, monasteries and spiritual retreats in Hinduism, Buddhism, and Jainism. The symbol has a spiritual meaning in all Indian dharmas, but the meaning and connotations of \"Om\" vary between the diverse schools within and across the various traditions.\n\nIn Hinduism, Om is one of the most important spiritual symbols. It refers to Atman (soul, self within) and Brahman (ultimate reality, entirety of the universe, truth, divine, supreme spirit, cosmic principles, knowledge). The syllable is often found at the beginning and the end of chapters in the Vedas, the Upanishads, and other Hindu texts. It is a sacred spiritual incantation made before and during the recitation of spiritual texts, during puja and private prayers, in ceremonies of rites of passages (sanskara) such as weddings, and sometimes during meditative and spiritual activities such as Yoga.\n\nThe syllable \"Om\" is also referred to as onkara (ओङ्कार, '), omkara (ओंकार, '), and pranava (प्रणव, \"\").\n\nThe syllable \"Om\" is referred to as \"praṇava\". Other used terms are ' (literally, letter of the alphabet, imperishable, immutable) or ' (one letter of the alphabet), and \"\" (literally, beginning, female divine energy).\n\"Udgitha\", a word found in Sama Veda and \"bhasya\" (commentaries) based on it, is also used as a name of the syllable. The word has three phonemes: \"a-u-m\", though it is often described as trisyllabic despite this being either archaic or the result of translation.\n\nThe syllable \"Om\" is first mentioned in the Upanishads, the mystical texts associated with the Vedanta philosophy.\nIt has variously been associated with concepts of \"cosmic sound\" or \"mystical syllable\" or \"affirmation to something divine\", or as symbolism for abstract spiritual concepts in the Upanishads.\nIn the Aranyaka and the Brahmana layers of Vedic texts, the syllable is so widespread and linked to knowledge, that it stands for the \"whole of Veda\".\nThe etymological foundations of \"Om\" are repeatedly discussed in the oldest layers of the Vedantic texts (the early Upanishads). The Aitareya Brahmana of Rig Veda, in section 5.32, for example suggests that the three phonetic components of \"Om\" (pronounced \"AUM\") correspond to the three stages of cosmic creation, and when it is read or said, it celebrates the creative powers of the universe. The Brahmana layer of Vedic texts equate \"Om\" with \"Bhur-bhuvah-Svah\", the latter symbolizing \"the whole Veda\". They offer various shades of meaning to \"Om\", such as it being \"the universe beyond the sun\", or that which is \"mysterious and inexhaustible\", or \"the infinite language, the infinite knowledge\", or \"essence of breath, life, everything that exists\", or that \"with which one is liberated\". The Sama Veda, the poetical Veda, orthographically maps \"Om\" to the audible, the musical truths in its numerous variations (\"Oum\", \"Aum\", \"Ovā Ovā Ovā Um\", etc.) and then attempts to extract musical meters from it.\n\nThe syllable \"Om\" evolves to mean many abstract ideas in the earliest Upanishads. Max Müller and other scholars state that these philosophical texts recommend \"Om\" as a \"tool for meditation\", explain various meanings that the syllable may be in the mind of one meditating, ranging from \"artificial and senseless\" to \"highest concepts such as the cause of the Universe, essence of life, Brahman, Atman, and Self-knowledge\".\n\nPhonologically, the syllable ओम् represents , which is regularly monophthongised to in Sanskrit phonology. When occurring within spoken Sanskrit, the syllable is subject to the normal rules of sandhi in Sanskrit grammar, however with the additional peculiarity that after preceding \"a\" or \"ā\", the \"au\" of \"aum\" does not form vriddhi (\"au\") but guna (\"o\") per Pāṇini 6.1.95 (i.e. 'om').\nIt is sometimes also written ओ३म् (\"ō̄m\" ), notably by Arya Samaj, where ३ (i.e., the digit \"3\") is \"pluta\" (\"three times as long\"), indicating a length of three morae (that is, the time it takes to say three syllables) — an overlong nasalised close-mid back rounded vowel.\n\nThe \"Om\" symbol is a ligature in Devanagari, combining ओ (') and chandrabindu (ँ, '). In Unicode, the symbol is encoded at and at (\"generic symbol independent of Devanagari font\").\n\nThe \"Om\" or \"Aum\" symbol is found on ancient coins, in regional scripts. In Sri Lanka, Anuradhapura era coins (dated from the 1st to 4th centuries) are embossed with \"Aum\" along with other symbols.\nNagari or Devanagari representations are found epigraphically on medieval sculpture, such as the dancing Shiva (ca. 10th to 12th century); Joseph Campbell (1949) even argued that the dance posture itself can be taken to represent \"AUM\" as a symbol of the entirety of \"consciousness, universe\" and \"the message that God is within a person and without\".\n\nThe \"Om\" symbol, with epigraphical variations, is also found in many southeast Asian countries. For example, it is called \"Unalom\" or \"Aum\" in Thailand and has been a part of various flags and official emblems such as in the Thong Chom Klao of King Rama IV (r. 1851–1868).\nThe Cambodian official seal has similarly incorporated the \"Aum\" symbol.\nIn traditional Chinese characters, it is written as 唵 (pinyin – ǎn), and as 嗡 (pinyin – wēng) in simplified Chinese characters.\n\nThere have been proposals that the \"Om\" syllable may already have had written representations in Brahmi script, dating to before the Common Era. A proposal by Deb (1848) held that the \"swastika\" is \"a monogrammatic representation of the syllable Om, wherein two Brahmi /o/ characters () were superposed crosswise and the 'm' was represented by dot\". A commentary in \"Nature\" considers this theory questionable and unproven.\nRoy (2011) proposed that \"Om\" was represented using the Brahmi symbols for \"A\", \"U\" and \"M\" (𑀅 𑀉 𑀫), and that this may have influenced the unusual epigraphical features of the symbol for \"Om\".\n\"Om\" came to be used as a standard utterance at the beginning of mantras, chants or citations taken from the Vedas. For example, the Gayatri mantra, which consists of a verse from the Rigveda Samhita (RV 3.62.10), is prefixed not just by \"Om\" but by \"Om\" followed by the formula \" bhūr bhuvaḥ svaḥ\". Such recitations continue to be in use in Hinduism, with many major incantations and ceremonial functions beginning and ending with \"Om\".\nMaheshwarananda (2002) suggests that the Om reflects the cosmological beliefs in Hinduism, as the primordial sound associated with the creation of universe from nothing.\n\nThe syllable \"Om\" is described with various meanings in the Upanishads. Descriptions include \"the sacred sound, the Yes!, the Vedas, the \"Udgitha\" (song of the universe), the infinite, the all encompassing, the whole world, the truth, the ultimate reality, the finest essence, the cause of the Universe, the essence of life, the Brahman, the Atman, the vehicle of deepest knowledge, and Self-knowledge\".\n\nThe Chandogya Upanishad is one of the oldest Upanishads of Hinduism. It opens with the recommendation that \"let a man meditate on Om\". It calls the syllable Om as \"udgitha\" (उद्गीथ, song, chant), and asserts that the significance of the syllable is thus: the essence of all beings is earth, the essence of earth is water, the essence of water are the plants, the essence of plants is man, the essence of man is speech, the essence of speech is the Rig Veda, the essence of the Rig Veda is the Sama Veda, and the essence of Sama Veda is the \"udgitha\" (song, \"Om\").\n\n\"Rik\" (ऋच्, Ṛc) is speech, states the text, and \"Sāman\" (सामन्) is breath; they are pairs, and because they have love for each other, speech and breath find themselves together and mate to produce a song. The highest song is Om, asserts section 1.1 of Chandogya Upanishad. It is the symbol of awe, of reverence, of threefold knowledge because \"Adhvaryu\" invokes it, the \"Hotr\" recites it, and \"Udgatr\" sings it.\n\nThe second volume of the first chapter continues its discussion of syllable \"Om\", explaining its use as a struggle between \"Devas\" (gods) and \"Asuras\" (demons). Max Muller states that this struggle between gods and demons is considered allegorical by ancient Indian scholars, as good and evil inclinations within man, respectively. The legend in section 1.2 of Chandogya Upanishad states that gods took the \"Udgitha\" (song of \"Om\") unto themselves, thinking, \"with this \"song\" we shall overcome the demons\". The syllable \"Om\" is thus implied as that which inspires the good inclinations within each person.\n\nChandogya Upanishad's exposition of syllable \"Om\" in its opening chapter combines etymological speculations, symbolism, metric structure and philosophical themes. In the second chapter of the Chandogya Upanishad, the meaning and significance of \"Om\" evolves into a philosophical discourse, such as in section 2.10 where \"Om\" is linked to the Highest Self, and section 2.23 where the text asserts \"Om\" is the essence of three forms of knowledge, \"Om\" is Brahman and \"Om is all this [observed world]\".\n\nThe Katho Upanishad is the legendary story of a little boy, Nachiketa – the son of sage Vajasravasa – who meets Yama, the Indian deity of death. Their conversation evolves to a discussion of the nature of man, knowledge, Atman (Soul, Self) and moksha (liberation). In section 1.2, Katho Upanishad characterizes Knowledge/Wisdom as the pursuit of good, and Ignorance/Delusion as the pursuit of pleasant, that the essence of Veda is to make man liberated and free, look past what has happened and what has not happened, free from the past and the future, beyond good and evil, and one word for this essence is the word \"Om\".\n\nThe Maitrayaniya Upanishad in sixth \"Prapathakas\" (lesson) discusses the meaning and significance of \"Om\". The text asserts that \"Om\" represents Brahman-Atman. The three roots of the syllable, states the Maitri Upanishad, are \"A\" + \"U\" + \"M\". The sound is the body of Soul, and it repeatedly manifests in three: as gender-endowed body – feminine, masculine, neuter; as light-endowed body – Agni, Vayu and Aditya; as deity-endowed body – Brahma, Rudra and Vishnu; as mouth-endowed body – Garhapatya, Dakshinagni and Ahavaniya; as knowledge-endowed body – Rig, Saman and Yajur; as world-endowed body – Bhūr, Bhuvaḥ and Svaḥ; as time-endowed body – Past, Present and Future; as heat-endowed body – Breath, Fire and Sun; as growth-endowed body – Food, Water and Moon; as thought-endowed body – intellect, mind and psyche. Brahman exists in two forms – the material form, and the immaterial formless. The material form is changing, unreal. The immaterial formless isn't changing, real. The immortal formless is truth, the truth is the Brahman, the Brahman is the light, the light is the Sun which is the syllable \"Om\" as the Self.\n\nThe world is Om, its light is Sun, and the Sun is also the light of the syllable \"Om\", asserts the Upanishad. Meditating on Om, is acknowledging and meditating on the Brahman-Atman (Soul, Self).\n\nThe Mundaka Upanishad in the second \"Mundakam\" (part), suggests the means to knowing the Self and the Brahman to be meditation, self-reflection and introspection, that can be aided by the symbol \"Om\".\n\nAdi Shankara, in his review of the Mundaka Upanishad, states \"Om\" as a symbolism for Atman (soul, self).\n\nThe Mandukya Upanishad opens by declaring, \"Om!, this syllable is this whole world\". Thereafter it presents various explanations and theories on what it means and signifies. This discussion is built on a structure of \"four fourths\" or \"fourfold\", derived from \"A\" + \"U\" + \"M\" + \"silence\" (or without an element).\n\nThe Shvetashvatara Upanishad, in verses 1.14 to 1.16, suggests meditating with the help of syllable \"Om\", where one's perishable body is like one fuel-stick and the syllable \"Om\" is the second fuel-stick, which with discipline and diligent rubbing of the sticks unleashes the concealed fire of thought and awareness within. Such knowledge, asserts the Upanishad, is the goal of Upanishads. The text asserts that \"Om\" is a tool of meditation empowering one to know the God within oneself, to realize one's Atman (Soul, Self).\n\nAitareya Aranyaka in verse 23.6, explains \"Om\" as \"an acknowledgment, melodic confirmation, something that gives momentum and energy to a hymn\".\nThe Bhagavad Gita, in the Epic Mahabharata, mentions the meaning and significance of \"Om\" in several verses. For example, Fowler notes that verse 9.17 of the Bhagavad Gita synthesizes the competing dualistic and monist streams of thought in Hinduism, by using \"\"Om\" which is the symbol for the indescribable, impersonal Brahman\".\nThe significance of the sacred syllable in the Hindu traditions, is similarly highlighted in various of its verses, such as verse 17.24 where the importance of \"Om\" during prayers, charity and meditative practices is explained as follows,\nThe aphoristic verse 1.27 of Pantanjali's Yogasutra links \"Om\" to Yoga practice, as follows,\n\nJohnston states this verse highlights the importance of \"Om\" in the meditative practice of Yoga, where it symbolizes three worlds in the Soul; the three times – past, present and future eternity, the three divine powers – creation, preservation and transformation in one Being; and three essences in one Spirit – immortality, omniscience and joy. It is, asserts Johnston, a symbol for the perfected Spiritual Man (his emphasis).\n\nThe medieval era texts of Hinduism, such as the Puranas adopt and expand the concept of \"Om\" in their own ways, and to their own theistic sects. According to the Vayu Purana, \"Om\" is the representation of the Hindu Trimurti, and represents the union of the three gods, viz. \"A\" for Brahma, \"U\" for Vishnu and \"M\" for Shiva. The three sounds also symbolise the three Vedas, namely (Rigveda, Samaveda, Yajurveda).\n\nThe Shiva Purana highlights the relation between deity Shiva and the \"Pranava\" or \"Om\". Shiva is declared to be \"Om\", and that \"Om\" is Shiva.\n\nIn Jainism, om is considered a condensed form of reference to the Pañca-Parameṣṭhi, by their initials \"A+A+A+U+M\" (\"\"). The Dravyasamgraha quotes a Prakrit line:\n\n\"Om\" is often used in some later schools of Buddhism, for example Tibetan Buddhism, which was influenced by Indian Hinduism and Tantra.\n\nIn Chinese Buddhism, \"Om\" is often transliterated as the Chinese character (pinyin ') or (pinyin ').\n\nIn Tibetan Buddhism, \"Om\" is often placed at the beginning of mantras and dharanis. Probably the most well known mantra is \"Om mani padme hum\", the six syllable mantra of the Bodhisattva of compassion, Avalokiteśvara. This mantra is particularly associated with the four-armed Shadakshari form of Avalokiteśvara. Moreover, as a seed syllable (\"bija mantra\"), \"Aum\" is considered sacred and holy in Esoteric Buddhism.\n\nSome scholars interpret the first word of the mantra \"oṃ maṇipadme hūṃ\" to be \"auṃ\", with a meaning similar to Hinduism – the totality of sound, existence and consciousness.\n\n\"Oṃ\" has been described by the 14th Dalai Lama as \"composed of three pure letters, A, U, and M. These symbolize the impure body, speech, and mind of everyday unenlightened life of a practitioner; they also symbolize the pure exalted body, speech and mind of an enlightened Buddha.\" According to Simpkins, Om is a part of many mantras in Tibetan Buddhism and is a symbolism for \"wholeness, perfection and the infinite\".\n\n\"Aum\" is symbolically represented by Niō (仁王) statues in Japan, and their equivalent in East Asia. \"Niō\" appear in pairs in front of Buddhist temple gates and stupas, in the form of two fierce looking guardian kings (\"Vajra\"). One has an open mouth, regarded by Buddhists as symbolically speaking the \"A\" syllable; the other has a closed mouth, symbolically speaking the \"Um\" syllable. The two together are regarded as saying \"Aum\", the \"vajra-breath\", or the Absolute in Sanskrit.\n\nKomainu (狛犬), also called lion-dogs, found in Japan, Korea and China, also occur in pairs before Buddhist temples and public spaces, and again, one has an open mouth (\"Agyō\"), the other closed (\"Ungyō\"). Like \"Nio\" statues, they are traditionally interpreted to be saying the start and end of \"Aun\" – a transliteration of the Sanskrit sacred syllable \"Aum\" (or \"Om\"), signifying the start and end of everything.\n\n\"Ik Oankar\", iconically represented as in Sikhism are the opening words of the Guru Granth Sahib, the Sikh scripture. It is the statement that 'there is one God', and that there is 'singularity despite seeming plurality'. The \"Oankar\" of Sikhism is related to \"Om\" in Hinduism, states the Indologist Wendy Doniger. Some Sikhs disagree that \"Ik Oankar\" is same as \"Om\". The phrase is a compound of the numeral one (\"ik\") and \"onkar\", states Doniger, canonically understood in Sikhism to refer to \"absolute monotheistic unity of God\".\n\nOnkar is, states Wazir Singh, a \"variation of Om (Aum) of the ancient Indian scriptures (with a slight change in its orthography), implying the seed-force that evolves as the universe\". \"Ik Onkar\" is part of the \"Mul Mantra\" in Sikh teachings and represents \"One God\", explains Gulati, where \"Ik\" means One, and Onkar is \"equivalent of the Hindu \"Om\" (Aum)\". Guru Nanak wrote a poem entitled Oankar in which, states Doniger, he \"attributed the origin and sense of speech to the Divinity, who is thus the Om-maker\".\n\"Ik Aumkara\" appears at the start of \"Mul Mantra\", states Kohli, and it occurs as \"Aum\" in the \"Upanishads\" and in \"Gurbani\". However, the meaning of \"Oankar\" in the Sikh tradition, states Pashaura Singh, is quite different in certain respects than those in other Indian philosophical traditions.\n\nThe Brahmic script \"om\"-ligature has become widely recognised in Western counterculture since the 1960s, mostly in its standard Devanagari form (ॐ), but the Tibetan alphabet \"om\" (ༀ) has also gained limited currency in popular culture.\n\n"}
{"id": "11992699", "url": "https://en.wikipedia.org/wiki?curid=11992699", "title": "Ostrich effect", "text": "Ostrich effect\n\nIn behavioral finance, the ostrich effect is the attempt made by investors to avoid negative financial information. The name comes from the common (but false) legend that ostriches bury their heads in the sand to avoid danger.\n\nOriginally the term was coined by , and was defined as \"the avoidance of apparently risky financial situations by pretending they do not exist\", but since it took the slightly broader meaning of \"avoiding to expose oneself to [financial] information that one fear may cause psychological discomfort\". For example, in the event of a market downturn, people may choose to avoid monitoring their investments or seeking out further financial news.\n\n explain differences in returns in the fixed income market by using a psychological explanation, which they name the \"ostrich effect,\" attributing this anomalous behavior to an aversion to receiving information on potential interim losses. They also provide evidence that the entrance to a leading financial portal in Israel is positively related to the equity market. Later, research by determined that people in Scandinavia looked up the value of their investments 50% to 80% less often during bad markets.\n\nA study by showed no perceivable attempt by investors to ignore or avoid negative information, but instead found that \"investors increase their portfolio monitoring following both positive and daily negative market returns, behaving more like hyper-vigilant meerkats than head-in-the-sand ostriches\". They dubbed this phenomenon \"meerkat effect\".\n\n\n"}
{"id": "7495406", "url": "https://en.wikipedia.org/wiki?curid=7495406", "title": "Overall pressure ratio", "text": "Overall pressure ratio\n\nIn aeronautical engineering, overall pressure ratio, or overall compression ratio, is the ratio of the stagnation pressure as measured at the front and rear of the compressor of a gas turbine engine. The terms \"compression ratio\" and \"pressure ratio\" are used interchangeably. Overall compression ratio also means the \"overall cycle pressure ratio\" which includes intake ram.\n\nEarly jet engines had limited pressure ratios due to construction inaccuracies of the compressors and various material limits. For instance, the Junkers Jumo 004 from World War II had an overall pressure ratio 3.14:1. The immediate post-war Snecma Atar improved this marginally to 5.2:1. Improvements in materials, compressor blades, and especially the introduction of multi-spool engines with several different rotational speeds, led to the much higher pressure ratios common today.\n\nModern civilian engines generally operate between 40 and 55:1. The highest in-service is the General Electric GEnx-1B/75 with an OPR of 58 at the end of the climb to cruise altitude (Top of Climb) and 47 for takeoff at sea level.\n\nGenerally speaking, a higher overall pressure ratio implies higher efficiency, but the engine will usually weigh more, so there is a compromise.\nA high overall pressure ratio permits a larger area ratio nozzle to be fitted on the jet engine. This means that more of the heat energy is converted to jet speed, and energetic efficiency improves. This is reflected in improvements in the engine's specific fuel consumption.\n\nOne of the primary limiting factors on pressure ratio in modern designs is that the air heats up as it is compressed. As the air travels through the compressor stages it can reach temperatures that pose a material failure risk for the compressor blades. This is especially true for the last compressor stage, and the outlet temperature from this stage is a common figure of merit for engine designs.\n\nMilitary engines are often forced to work under conditions that maximize the heating load. For instance, the General Dynamics F-111 Aardvark was required to operate at speeds of Mach 1.1 at sea level. As a side-effect of these wide operating conditions, and generally older technology in most cases, military engines typically have lower overall pressure ratios. The Pratt & Whitney TF30 used on the F-111 had a pressure ratio of about 20:1, while newer engines like the General Electric F110 and Pratt & Whitney F135 have improved this to about 30:1.\n\nAn additional concern is weight. A higher compression ratio implies a heavier engine, which in turn costs fuel to carry around. Thus, for a particular construction technology and set of flight plans an optimal overall pressure ratio can be determined.\n\nThe contribution of intake to the overall pressure ratio, defined as the ratio of the stagnation pressure as measured at the front and rear of the intake is negative in supersonic flight, and approximately non-existent in subsonic flight. Therefore with the definition given in the first sentence in this article, the overall effective pressure ratio of Concorde taking intake into account is less than without it. In other words less than 15.5:1 as in the table.\nInformation from the source given above obviously uses a different definition for overall pressure ratio, comparing static pressures instead of stagnation pressures, and is misleading given the definition in this article as it is.\nThe term should not be confused with the more familiar term compression ratio applied to reciprocating engines. Compression ratio is a ratio of volumes. In the case of the Otto cycle reciprocating engine, the maximum expansion of the charge is limited by the mechanical movement of the pistons (or rotor), and so the compression can be measured by simply comparing the volume of the cylinder with the piston at the top and bottom of its motion. The same is not true of the \"open ended\" gas turbine, where operational and structural considerations are the limiting factors. Nevertheless, the two terms are similar in that they both offer a quick way of determining overall efficiency relative to other engines of the same class.\n\nThe broadly equivalent measure of rocket engine efficiency is chamber pressure/exit pressure, and this ratio can be over 2000 for the Space Shuttle Main Engine.\n\nCompression ratio and overall pressure ratio would be interrelated as follows, if the first sentence in this article would be incorrect, given as it is, what follows is valid only for static pressures, but not overall pressure ratios, as compresson while slowing down flow without doing work is not changing stagnation pressure at all, and in supersonic flow with shock waves stagnation pressure will be less than 1 with > 1 compression ratio:\nThe reason for this difference is that compression ratio is defined via the volume reduction:\nwhile pressure ratio is defined as the pressure increase:\nIn calculating the pressure ratio, we assume that an adiabatic compression is carried out (i.e. that no heat energy is supplied to the gas being compressed, and that any temperature rise is solely due to the compression). We also assume that air is a perfect gas. With these two assumptions, we can define the relationship between change of volume and change of pressure as follows:\nwhere formula_4 is the ratio of specific heats (air: approximately 1.4).\nThe values in the table above are derived using this formula. Note that in reality the ratio of specific heats changes with temperature and that significant deviations from adiabatic behavior will occur.\n\n"}
{"id": "715838", "url": "https://en.wikipedia.org/wiki?curid=715838", "title": "Persecution and the Art of Writing", "text": "Persecution and the Art of Writing\n\nPersecution and the Art of Writing, published in 1952 by the Free Press, is a book of collected articles written by Leo Strauss. The book contains five previously published essays, many of which were significantly altered by Strauss from their original publication. The general theme of the book is the relationship between politics and philosophy. The thesis of the book is that many ancient and early modern political philosophers, in order to avoid persecution, hid their most heterodox ideas within their texts.\n\nStrauss's general argument—rearticulated throughout his subsequent writings (most notably in \"The City and Man\" – 1964)—is that prior to the 19th century, Western scholars commonly understood that philosophical writing is not at home in any polity, no matter how liberal. Insofar as it questions conventional wisdom at its roots, philosophy must guard itself especially against those readers who believe themselves authoritative, wise, and liberal defenders of the status quo. In questioning established opinions, or in investigating the principles of morality, philosophers of old found it necessary to convey their messages in an oblique manner. Their \"art of writing\" was the art of \"exoteric\" communication. This is all the more apparent in medieval times, when heterodox political thinkers wrote under the threat of the Inquisition or comparably intransigent tribunals.\n\nStrauss's argument is not that the medieval writers he studies reserved one exoteric meaning for the many (\"hoi polloi\") and an esoteric or hidden one for the few (\"hoi aristoi\", literally \"the best\") but rather that their writings' respective core meanings extended beyond and were irreducible to their texts' literal and/or historical dimension.\n\nExplicitly following Gotthold Ephraim Lessing's lead, Strauss indicates that medieval political philosophers, no less than their ancient counterparts, in writing, carefully adapted their wording to the dominant moral views of their time, lest their writings be condemned as heretical or unjust, not by \"the many\" (who did not read), but by those \"few\" whom the many regarded as the most righteous guardians of morality: precisely those few righteous personalities would be most inclined to persecute or ostracize anyone who is in the business of exposing the \"noble\" or \"great lie\" upon which stands or falls the authority of the few over the many.\n\n\n"}
{"id": "9040892", "url": "https://en.wikipedia.org/wiki?curid=9040892", "title": "Racial and Religious Tolerance Act 2001", "text": "Racial and Religious Tolerance Act 2001\n\nThe Racial and Religious Tolerance Act 2001, is a statute passed by the Victorian Parliament during the Premiership of Steve Bracks. The Act makes behavior that incites or encourages hatred, serious contempt, revulsion or severe ridicule against another person or group of people because of their race and/or religion unlawful in Victoria\n\nThe Act criminalises racist graffiti, racist posters, racist stickers, racist comments made in a publication, including the Internet and email, statements at a meeting or at a public rally. The Act explicitly criminalises public behavior – not personal beliefs.\n\nIn a landmark ruling on 17 December 2004, a VCAT tribunal ruled that Danny Nalliah, Daniel Scot and Catch the Fire Ministries had breached the law. The pair stood accused of making fun of Muslim beliefs and conduct that was \"hostile, demeaning and derogatory of all Muslim people, their god, Allah, the prophet Muhammad and in general Muslim religious beliefs and practices\". Nalliah publicly condemned the verdict and declared his intention to continue fighting the case, potentially as far as the High Court of Australia. \"The Age\" newspaper quoted him as stating, \"We may have lost the battle, but the war is not over. The law has to be removed, there is no question.\"\n\nThe matter was eventually resolved without a VCAT hearing after mediation between the two parties. On 22 June 2007 VCAT published a statement agreed to by both parties which affirmed everyone's rights to \"robustly debate religion including the right to criticise the religious belief of another, in a free, open and democratic society\".\n\nIn September 2017 three prominent members of the far-right United Patriots Front; Blair Cottrell, Neil Erikson and Chris Shortis, were charged with serious religious vilification, among other offenses. In September that same year they were convicted for inciting serious contempt of Muslims, each were fined $2,000.\n\nFormer state shadow Attorney general Andrew McIntosh announced prior to the 2006 election that the Liberal Party would repeal the religious section of the \"fundamentally flawed\" Act. \n\nIn January 2006, nineteen Christian leaders from Melbourne's largest churches wrote to the Premier requesting the removal of the civil provisions in the act, claiming that aspects of the religious vilification law undermines multiculturalism. \n\nIndependent MP Russell Savage, whose support helped Bracks form Government in 1999, described the act as \"the worst legislation he had ever seen passed\".\n\n\n"}
{"id": "50607028", "url": "https://en.wikipedia.org/wiki?curid=50607028", "title": "Rationalist (magazine)", "text": "Rationalist (magazine)\n\nRationalist () was a Polish magazine published in Warsaw from October 1930 to December 1935 by the Warsaw Circle of Intellectuals, Polish Association of Free Thought.\n\nEditor and publisher of \"rationalist\" was Józef Landau. The leading publicists were: Tadeusz Kotarbiński, Henryk Ułaszyn, and Józef Landau.\n\n"}
{"id": "11944829", "url": "https://en.wikipedia.org/wiki?curid=11944829", "title": "Red Sea rig", "text": "Red Sea rig\n\nRed Sea rig, sometimes known as gulf rig or schooner rig, is a dress code for semi-formal evening events, which in general consists of black tie attire with the jacket removed, a red bow tie and red cummerbund, although there are local variations.\n\nRed Sea rig was originally a Royal Navy concept appearing during the nineteenth century. Historically, it was felt that Royal Navy officers, like their British Army counterparts, should wear the full appropriate uniform for all formal events, whatever the temperature. The sole exception was in the Red Sea, where the heat and humidity often made this physically impossible. Here, officers were permitted to remove their jackets in the wardroom, provided they added a cummerbund to temper the somewhat informal look. Royal Air Force officers serving on Navy ships follow the naval tradition wearing a Red Sea rig version of their own mess dress.\n\nIn his reminiscences \"For King and Country\", Nelson Albert Tomalin describes a rather home-made version of Red Sea rig worn on board the whaler Southern Sea in 1943 as \"...white shirt with epaulettes and long blue trousers with a black scarf as a cummerbund...\".\n\nBecause of its obvious practicality, Red Sea rig was adopted into civilian life, first by British diplomats in the Red Sea town of Jeddah, and later by the local British Business Group. It is now widely worn by many military and civilian organisations and is often the dress code of choice for dinner parties in British expatriate communities in the Middle East and Far East.\n\nRed Sea rig originated prior to air conditioning as a purely practical measure, but has now become a dress-style in its own right, even if the party or function is held in an air conditioned venue.\n\nDecorations, even in miniature, are not normally worn with Red Sea rig, although medal miniature ribbons are.\n\nThere are many military and civilian variations of Red Sea rig:\n\n\n"}
{"id": "1806952", "url": "https://en.wikipedia.org/wiki?curid=1806952", "title": "Reverse vending machine", "text": "Reverse vending machine\n\nA reverse vending machine is a device that accepts used (empty) beverage containers and returns money to the user. The machines are popular in places that have mandatory recycling laws or container deposit legislation. In some places, bottlers paid funds into a centralized pool to be disbursed to people who recycled the containers. Any excess funds were to be used for general environmental cleanup. In other places, such as Norway, the state mandated that a vendor pay for recycled bottles, but left the system in the hands of private industry.\n\nThe recycler places the empty bottle or can into the receiving aperture; the horizontal in-feed system allows the user to insert containers one at a time. (An alternative system, found in many older machines, is one in which the user opens a door by hand and places the empty container in a pan. When the door is released and closed, the process continues.) The bottle or can is then automatically rotated; the bottle/can is then scanned by an omnidirectional UPC scanner, which scans the beverage container's UPC. \nOnce a container is scanned, identified (matched to database) and determined to be a participating container, it is processed and typically crushed (for one-time-use containers) to reduce its size, to avoid spillages of liquid and to increase storage capacity. Refillable containers are collected and sorted by hand to be brought back to the bottling company. The machines can use material recognition instead of/as well as a bar code scanner when needed.\n\nThe first patent for a \"Bottle Return and Handling Machine\" was filed in the United States in 1920. The first working bottle return machine was invented and manufactured by Wicanders from Sweden used in the late 1950s. In 1962 an advanced automatic bottle return machine was designed by Aage Tveitan and manufactured in Norway by his company Arthur Tveitan AS. The first 3 in 1 machine was invented in 1994 by Kansmacker and is still in operation today in Detroit, Michigan.\n"}
{"id": "8313678", "url": "https://en.wikipedia.org/wiki?curid=8313678", "title": "Right to petition", "text": "Right to petition\n\nThe right to petition government for redress of grievances is the right to make a complaint to, or seek the assistance of, one's government, without fear of punishment or reprisals, ensured by the First Amendment to the United States Constitution (1791). Article 44 of the Charter of Fundamental Rights of the European Union ensures the right to petition to the European Parliament. \nThe right can be traced back to the Basic Law for the Federal Republic of Germany, the Bill of Rights 1689, the Petition of Right (1628), and Magna Carta (1215).\n\nThe prohibition of abridgment of the \"right to petition\" originally referred only to the Congress and the US federal courts. The incorporation doctrine later expanded the protection of the right to its current scope, over all state and federal courts and legislatures, and the executive branches of the state and federal governments.\n\nThe right to petition includes, under its umbrella, the petition. For example, in January 2006, the US Senate considered S. 2180, an omnibus \"ethics reform\" bill. This bill contained a provision (Section 204) to establish federal regulation, for the first time, of certain efforts to encourage \"grassroots lobbying\". The bill said that \"'grassroots lobbying' means the voluntary efforts of members of the general public to communicate their own views on an issue to Federal officials or to encourage other members of the general public to do the same\".\n\nThis provision was opposed by a broad array of organizations, including the American Civil Liberties Union, the National Right to Life Committee, and the National Rifle Association. On January 18, 2007, the US Senate voted 55-43 to strike Section 221 from the bill.\n\nThere are ongoing conflicts between organizations that wish to impose greater restrictions on citizen's attempts to influence or \"lobby\" policymakers. and the right of individuals, groups, and corporations (via corporate personhood ), to lobby the government.\n\nAnother controversial bill, the Executive Branch Reform Act, H.R. 984, would require over 8,000 Executive Branch officials to report into a public database nearly any \"significant contact\" from any \"private party\", a term that the bill defines to include almost all persons other than government officials. The bill defines \"significant contact\" to be any \"oral or written communication (including electronic communication) . . . in which the private party seeks to influence official action by any officer or employee of the executive branch of the United States.\" This covers all forms of communication, one way or two ways, including letters, faxes, e-mails, phone messages, and petitions. The bill is supported by some organizations as an expansion of \"government in the sunshine\", but other groups oppose it as an infringing on the right to petition by making it impossible for citizens to communicate their views on controversial issues to government officials without those communications becoming a matter of public record.\n\n"}
{"id": "56224201", "url": "https://en.wikipedia.org/wiki?curid=56224201", "title": "Row2Recovery", "text": "Row2Recovery\n\nRow2Recovery is a British unincorporated Association of volunteers which assists military adaptive-rowing. The Association has completed four Atlantic adaptive-rowing crossings and, when a charity, supported a national adaptive-rowing programme for the British military wounded, injured and sick in partnership with British Rowing and Help for Heroes. Row2Recovery was founded in 2010 by former Army-Captains Edward Janvrin and Alexander Mackenzie.\n\nOn 5 December 2011, Row2Recovery's first Atlantic rowing crew departed the Port of San Sebastian on La Gomera, one of Spain's Canary Islands. The crew was made of up of four servicemen who had been wounded in action and two able-bodied servicemen. 50 days, 23 hours and 12 minutes later, Team Sealegs arrived in Port St Charles, Barbados on 25 January 2012.\nThe 2011/12 Crew consisted of:\n\nIn April 2013, Row2Recovery formally merged with Rowing For Our Wounded. Rowing For Our Wounded had been founded by Paddy Nicoll in April 2012 to assist \"serving and former members of the armed forces who have been wounded or disabled whilst on active service to recover mentally and/ or physically and cope with their disabilities through the provision of specially adaptive equipment, services and funding to enable them to take part in adaptive-rowing or other adaptive sports.\". Rowing For Our Wounded had been specifically helping with a national adaptive-rowing programme for the British military wounded, injured and sick in partnership with British Rowing and Help for Heroes, a role Row2Recovery then took on under the new Chairmanship of Paddy Nicoll.\n\nOn 4 December 2013, Row2Recovery's second Atlantic rowing crew departed the Port of San Sebastian on La Gomera, one of Spain's Canary Islands. The crew was made of up of two servicemen who had been wounded in action and two able-bodied servicemen. 48 days; 9 hours; 13 minutes and 30 seconds later, Team Endeavour arrived in English Harbour, Antigua on 21 January 2014.\nThe 2013/14 Crew consisted of:\n\nOn 11 March 2014, Prince Harry formally launched Row2Recovery’s inland programme in the UK at the Henley River and Rowing Museum, helped by a grant from the Endeavour Fund, part of The Royal Foundation of The Duke and Duchess of Cambridge and Prince Harry. In addition to British Rowing and Help for Heroes, the programme is specifically supported by the Sports Recovery programme at Tedworth House; the GB Rowing Team; British Rowing's Rowability Programme; the Defence Medical Rehabilitation Centre Headley Court; Hasler Naval Service Recovery Centre; Guildford Rowing Club; Marlow Rowing Club and Gateshead Community Rowing Club. It was at this event that Prince Harry and Sir Keith Mills were given a demonstration of an indoor adaptive-rowing race which helped convinced them that it was an appropriate sport for the Invictus Games.\n\nOn 20 December 2015, Row2Recovery's third Atlantic rowing crew departed the Port of San Sebastian on La Gomera, one of Spain's Canary Islands. The crew was made of up of four injured servicemen; two who had been wounded in action and two in accidents in the UK. 46 days; 6 hours; 49 minutes later, Team Endeavour arrived in English Harbour, Antigua on 4 February 2016.\nThe 2015/16 Crew consisted of:\n\nRow2Recovery's fourth Atlantic rowing crew, having departed the Port of San Sebastián on La Gomera, one of Spain's Canary Islands on 14 December 2017, arrived in English Harbour, Antigua, on the 20 January 2018 after 37 days, 8 hours and 8 minutes. They beat the previous fastest time for a Pair to row the Atlantic by 3 days.\n\nThe crew was made of up of one injured and one able-bodied serviceman.\nThe 2017/18 Crew consists of:\n\nThe fifth crew to row the Atlantic under the Row2Recovery “banner” is Team Second Chance who plan to row from Port of San Sebastian on La Gomera, one of Spain's Canary Islands in December 2018 to English Harbour, Antigua.\n\nThe crew is made up of one injured veteran and one able-bodied civilian.\n\nThe 2018/19 Crew consists of:\n\nTo date, through these extreme adaptive rowing endeavours, Row2Recovery crews have raised over a million pounds for the large military charities including BLESMA; Prince Harry's Endeavour Fund; Help for Heroes; The Soldiers' Charity and SSAFA.\n"}
{"id": "32457161", "url": "https://en.wikipedia.org/wiki?curid=32457161", "title": "Shelling (topology)", "text": "Shelling (topology)\n\nIn mathematics, a shelling of a simplicial complex is a way of gluing it together from its maximal simplices (simplices that are not a face of another simplex) in a well-behaved way. A complex admitting a shelling is called shellable.\n\nA \"d\"-dimensional simplicial complex is called pure if its maximal simplices all have dimension \"d\". Let formula_1 be a finite or countably infinite simplicial complex. An ordering formula_2 of the maximal simplices of formula_1 is a shelling if the complex formula_4 is pure and formula_5-dimensional for all formula_6. That is, the \"new\" simplex formula_7 meets the previous simplices along some union formula_8 of top-dimensional simplices of the boundary of formula_7. If formula_8 is the entire boundary of formula_7 then formula_7 is called spanning.\n\nFor formula_1 not necessarily countable, one can define a shelling as a well-ordering of the maximal simplices of formula_1 having analogous properties.\n\n\n\n"}
{"id": "217792", "url": "https://en.wikipedia.org/wiki?curid=217792", "title": "Tricolour (flag)", "text": "Tricolour (flag)\n\nA tricolour or tricolor is a type of flag or banner design with a triband design which originated in the 16th century as a symbol of republicanism, liberty or indeed revolution. The flags of France, Italy, Romania, Mexico, and Ireland were all first adopted with the formation of an independent republic in the period of the French Revolution to the Revolutions of 1848, with the exception of the Irish tricolour, which dates from 1848 but was not popularised until the Easter Rising in 1916 and adopted in 1919.\n\nThe first association of the tricolour with republicanism is the orange-white-blue design of the Prince's Flag (\"Prinsenvlag\", predecessor of the flags of the Netherlands and Luxembourg), used from 1579 by William I of Orange-Nassau in the Eighty Years' War, establishing the independence of the Dutch Republic from the Spanish Empire.\nThough not the first tricolour flag, one of the most famous, known as \"Le Tricolore\", is the blue, white and red (whence also called \"Le Bleu-Blanc-Rouge\") flag of France adopted in the French Revolution.\n\nWith the formation of French client republics after 1795, the revolutionary tricolour was exported and adopted more widely in Europe, by \nthe Republic of Alba 1796 (red-blue-yellow),\nthe Cisalpine Republic 1797 (Transpadane Republic, green-white-red),\nthe Cisrhenian Republic 1797 (green-white-red),\nthe Anconine Republic 1797 (blue-yellow-red),\nthe Roman Republic 1798 (black-white-red),\nthe Helvetic Republic 1798 (green-yellow-red; canton of Neuchatel 1848), \nthe Parthenopean Republic 1799 (blue-yellow-red), \nthe Principality of Lucca and Piombino 1805 (blue-white-red). Thus providing the format for most of modern Europe's national flags, from the flag of Italy, to the flag of Germany, flag of Ireland, flag of Belgium, flag of Romania, flag of Bulgaria, flag of Moldova, and others around the World such as the flag of Cameroon, flag of Chad, flag of Ivory Coast, flag of Gabon, flag of Guinea, flag of Mali, and flag of Nigeria.\n\nThe green-white-red tricolour remained a symbol of republicanism throughout the 19th century and was adopted as national flag by a number of states following the Revolutions of 1848. It was also adopted by the Kingdom of Sardinia (inherited by the Kingdom of Italy 1861).\n\nThe flag of Germany (black-red-gold) originated as the flag of the revolutionary, anti-monarchist Freikorps of the 1830s and was later adopted by the republicanist bourgeoisie, at the time known as \"Dreifarb\", a German calque of \"Tricolore\". This flag was a symbol of opposition against the German \"Kleinstaaterei\" and the desire for German Unification. It was at first illegal in the German Confederation, but was adopted as the national flag at the Frankfurt Parliament of 1848/9. The flag of Belgium was introduced in a similar context, in 1831, its colours taken from the flag used in the Brabant Revolution of 1789.\nThe first national flag of the New World inspired by this symbolism was the flag of Mexico, adopted when the First Mexican Empire gained independence from Spain in 1821.\n\nAfter 1848, the young republican nation states continued to pick triband designs, but now more prevalently expressing the sentiment of nationalism or ethnic identity than anti-monarchism, the flag of Hungary (1848),\nthe flag of Romania (1848), \nthe Flag of Ireland (1848),\nthe flag of Estonia (1880s),\nthe flag of Lithuania (1905),\nand the flag of Armenia (1918). \nBy contrast, the flag of Russia was adopted by the Tsardom of Russia in the late 17th century and while it may or may not have been inspired by the Dutch tricolour, it never had any republican implications.\n\nThe political ideology of the unification of an ethnic nation state associated with tricolour flags since the 19th century has resulted in the design of new \"tricolours\" expressing specific nationalisms in the 20th century, \nthe Pan-African colours adopted in the 1920s for Pan-Africanism, chosen in numerous African flags during decolonisation (green-yellow-red, taken from the triband design used by the Solomonic dynasty for the Ethiopian Empire since 1897). The Pan-Arab colours adopted in Arab nationalism 1916 are a comparable concept, even though they combine four, not three, colours.\nAlso in the 20th century, Pan-Iranian colours for Iranian nationalism and Pan-Slavic colours for Slavic nationalism were adopted based on the triband design of the flags used during the 19th century by the Qajar dynasty and the Russian Empire, respectively.\n\nThe Indian independence movement in 1931 also adopted a tricolour (loan-translated as Hindi, तिरंगा \"Tiraṅgā\") in the traditional symbolism of \"national unification\" and republican \"self-rule\" (Purna Swaraj), adopted as the flag of the Indian Republic in 1947.\n\nIn 1999, a red, green, and blue tricolour was proposed as the Flag of Mars. The design symbolizes liberty, and also the terraforming of Mars by humanity from a red planet to a green one, and eventually an Earth-like blue one.\n\n"}
{"id": "22757436", "url": "https://en.wikipedia.org/wiki?curid=22757436", "title": "Trivial semigroup", "text": "Trivial semigroup\n\nIn mathematics, a trivial semigroup (a semigroup with one element) is a semigroup for which the cardinality of the underlying set is one. The number of distinct nonisomorphic semigroups with one element is one. If \"S\" = { \"a\" } is a semigroup with one element then the Cayley table of \"S\" is as given below:\n\nThe only element in \"S\" is the zero element 0 of \"S\" and is also the identity element 1 of \"S\". However not all semigroup theorists consider the unique element in a semigroup with one element as the zero element of the semigroup. They define zero elements only in semigroups having at least two elements.\n\nIn spite of its extreme triviality, the semigroup with one element is important in many situations. It is the starting point for understanding the structure of semigroups. It serves as a counterexample in illuminating many situations. For example, the semigroup with one element is the only semigroup in which 0 = 1, that is, the zero element and the identity element are equal. \nFurther, if \"S\" is a semigroup with one element, the semigroup obtained by adjoining an identity element to \"S\" is isomorphic to the semigroup obtained by adjoining a zero element to \"S\".\n\nThe semigroup with one element is also a group.\n\nIn the language of category theory, any semigroup with one element is a terminal object in the category of semigroups.\n\n"}
{"id": "54117020", "url": "https://en.wikipedia.org/wiki?curid=54117020", "title": "Unrestricted algorithm", "text": "Unrestricted algorithm\n\nAn unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.\n\nIn the problem of developing algorithms for computing the values of a real-valued function of a real variable, say \"g\"(\"x\"), in \"restricted\" algorithms, the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values where in the values of function are to be evaluated. Different algorithms may have to applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of \"x\" and also the precision required in \"g\"(\"x\"), quite arbitrarily. The algorithm should then produce an acceptable result without failure.\n"}
{"id": "15755429", "url": "https://en.wikipedia.org/wiki?curid=15755429", "title": "Virtual volunteering", "text": "Virtual volunteering\n\nVirtual volunteering refers to volunteer activities completed, in whole or in part, using the Internet and a home, school, telecenter, or work computer or other Internet-connected device, such as a smart-phone (a cell phone with Internet functions) or personal digital assistant (PDA). Virtual volunteering is also known as online volunteering, remote volunteering or e-volunteering.\n\nIn one study, over 70 per cent of online volunteers chose assignments requiring one to five hours a week and nearly half chose assignments lasting 12 weeks or less. Some organizations offer online volunteering opportunities which last from ten minutes to an hour. A unique feature of online volunteering is that it can be done from a distance. People with restricted mobility or other special needs participate in ways that might not be possible in traditional face-to-face volunteering. Likewise, online volunteering may allow people to overcome social inhibitions and social anxiety, particularly if they would normally experience disability-related labeling or stereotyping. This empowers people who might not otherwise volunteer. It can build self-confidence and self-esteem while enhancing skills and extending networks and social ties. Online volunteering also allows participants to adapt their program of volunteer work to their unique skills and passions.\n\nPeople engaged in virtual volunteering undertake a variety of activities from locations remote to the organization or people they are assisting, via a computer or other Internet-connected device, such as:\n\n\nIn the developing world, innovative synergies between volunteerism and technology typically focus on mobile communication technologies rather than the Internet. Around 26 per cent of people worldwide had Internet access in 2009. However, Internet penetration in low-income countries was only 18 per cent, compared to over 64 per cent in developed countries. While the costs of fixed broadband Internet are falling, access still remains unaffordable to many. Despite this, online volunteering is developing rapidly. Online volunteers are \"people who commit their time and skills over the Internet, freely and without financial considerations, for the benefit of society.\" Online volunteering has eliminated the need for volunteerism to be tied to specific times and locations. Thus, it greatly increases the freedom and flexibility of volunteer engagement and complements the outreach and impact of volunteers serving in situ. Most online volunteers engage in operational and managerial activities such as fundraising, technological support, communications, marketing and consulting. Increasingly, they also engage in activities such as research and\nwriting and leading e-mail discussion groups.\n\nOnline \"micro-volunteering\" is also an example of virtual volunteering and crowdsourcing, where volunteers undertake assignments via their PDAs or smartphones. These volunteers either aren't required to undergo any screening or training by the nonprofit for such tasks, and do not have to make any other commitment when a micro-task is completed, or, have already undergone screening or training by the nonprofit, and are therefore approved to take on micro-tasks as their availability and interests allow. Online micro-volunteering was originally called \"byte-sized volunteering\" by the Virtual Volunteering Project, and has always been a part of the more than 30-year-old practice of online volunteering. An early example of both micro-volunteering and crowdsourcing is ClickWorkers, a small NASA project begun in 2001 that engaged online volunteers in scientific-related tasks that required just a person's perception and common sense, but not scientific training, such as identifying craters on Mars in photos the project posted online; volunteers were not trained or screened before participating. The phrase \"micro-volunteering\" is usually credited to a San Francisco-based nonprofit called The Extraordinaries.\n\nThe practice of virtual volunteering to benefit nonprofit initiatives dates back to at least the early 1970s, when Project Gutenberg began involving online volunteers to provide electronic versions of works in the public domain.\n\nIn 1995, a new nonprofit organization called Impact Online (now called VolunteerMatch), based in Palo Alto, California, began promoting the idea of \"virtual volunteers\". In 1996, Impact Online received a grant from the James Irvine Foundation to launch an initiative to research the practice of virtual volunteering and to promote the practice to nonprofit organizations in the US. This new initiative was dubbed the Virtual Volunteering Project, and the web site was launched in early 1997. After one year of operations, the Virtual Volunteering Project moved to the Charles A. Dana Center at The University of Texas at Austin. In 2002, the Virtual Volunteering Project moved within the university to the Lyndon B. Johnson School of Public Affairs. The first two years of the Virtual Volunteer Project were spent reviewing and adapting telecommuting manuals and existing volunteer management guidelines with regarding to virtual volunteering, as well as identifying organizations that were involving online volunteers. By April 1999, almost 100 organizations had been identified by the Virtual Volunteering Project as involving online volunteers and were listed on the web site. Due to the growing numbers of nonprofit organizations, schools, government programs and other not-for-profit entities involving online volunteers, the Virtual Volunteering Project stopped listing every such organization involving online volunteers on its web site in 2000, and focused its efforts on promoting the practice, profiling organizations with large or unique online volunteering programs, and creating guidelines for the involvement of online volunteers. Until January 2001, the Virtual Volunteering Project listed all telementoring and teletutoring programs in the USA (programs where online volunteers mentor or tutor others, through a nonprofit organization or school). At that time, 40 were identified.\n\nIn August 1999, the NetAid.org initiative was launched. The initiative included an online volunteering component, today known as the UN Online Volunteering service. It went live in 2000 and has been managed by United Nations Volunteers since its inception. It quickly attracted a high number of people ready to support organizations working for development. In 2003, several thousand people already contributed to the UN's Online Volunteering service – volunteers with very diverse backgrounds, including university graduates, private sector employees, and retirees. While the UN's Online Volunteering service became independent, NetAid continued as a joint project of UNDP and Cisco Systems. It aimed \"to utilize the unique networking capabilities of the Internet to promote development and alleviate extreme poverty across the world\".\n\nOnline volunteering has been adopted by thousands of nonprofit organizations and other initiatives. There is no organization currently tracking best practices in online volunteering in the USA or worldwide, how many people are engaged in online volunteering, or how many organizations utilize online volunteers, and studies regarding volunteering, such as reports on volunteering trends in the USA, rarely include information about online volunteering (for example, a search of the term \"virtual volunteering\" on the Corporation for National Service's \"Volunteering in America\" yields no results. On IVCO's Forum Discussion Paper 2015 it is recommended that a collective measurement tool developed as part of a global measurement framework should also capture online volunteering.\n\nThe UN's Online Volunteering service connects organizations working in or for the developing world with online volunteers. It does have statistics available regarding numbers of online volunteers and involving organizations (i.e. NGOs, other civil society organizations, a government or other public institutions, United Nations agencies or other intergovernmental institutions) that collaborate online via their platform. In 2013, all 17,370 online volunteering assignments offered by development organizations through the Online Volunteering service attracted applications from numerous qualified volunteers. About 58 percent of the 11,037 online volunteers were women, and 60 percent came from developing countries; on average, they were 30 years of age. More than 94 percent of organizations and online volunteers rated their collaboration as good or excellent in 2013. For civil society organizations with limited resources in particular, the impact of online volunteer engagement is significant: 41% involve UN Online Volunteers for technical expertise that is not available internally. According to the same impact evaluation carried out in 2014, in many instances, organizations without access to online volunteers would have difficulties achieving their own peace and development outcomes.\n\nIn July 2016, UNV unveiled a redesigned website and launched two additional services: The 1-click query to allow organizations to reach out to half a million people to provide real-time data for their projects, and its new employee online volunteering solution for global companies. Inclusive multi-stakeholder partnerships emerged as a necessity to achieve the Sustainable Development Goals (SDGs), and the first private sector partner of the Online Volunteering service is based in Brazil (Samsung Electronics Latin American Office).\n\nSeveral other matching services, such as VolunteerMatch and Idealist, also offer virtual volunteering positions with nonprofit organizations in addition to traditional, on-site volunteering opportunities. VolunteerMatch currently reports that about 5 percent of its active volunteer listings are virtual in nature. As of June 2010, its directory included more than 2,770 such listings including roles in interactive marketing, fundraising, accounting, social media, and business mentoring. The percentage of virtual listings has dropped since 2006, when it peaked at close to 8 percent of overall volunteer opportunities in the VolunteerMatch system.\n\nWikipedia and other Wikimedia Foundation endeavors are examples of online volunteering, in the form of crowdsourcing or micro-volunteering; the majority of Wikipedia contributing volunteers aren't required to undergo any screening or training by the nonprofit for their role as researchers, writers or editors, and do not have to make a specific time commitment to the organization in order to contribute service.\n\nMany organizations involved in virtual volunteering might never mention the term, or the words \"online volunteer,\" on their web sites or in organizational literature. For example, the nonprofit organization Business Council for Peace (Bpeace) recruits business professionals to donate their time mentoring entrepreneurs in conflict-affected countries, including Afghanistan and Rwanda, but the majority of these volunteers interact with Bpeace staff and entrepreneurs online rather than face-to-face; yet, the term virtual volunteering is not mentioned on the web site. Bpeace also engages in online micro-volunteering, asking for information leads from its supporters, such as where to find online communities of particular professionals in the USA, but the organization never mentions the term micro-volunteering on its web site. Another example is the Electronic Emissary, one of the first K-12 online mentoring programs, launched in 1992; the web site does not use the phrase virtual volunteering and prefers to call online volunteers online subject matter experts.\n\nEvolving forms of volunteerism will enhance opportunities for people to volunteer. The spread of technology connects ever more rural and isolated areas. NGOs and governments are beginning to realise the value of South-to-South international volunteerism, as well as diaspora volunteering, and are dedicating resources to these schemes. Corporations are responding to the \"social marketplace\" by supporting CSR initiatives that include volunteerism. New opportunities for engaging in volunteerism are opening up with the result that more people are becoming involved and those already participating can expand their commitment. A phenomenon that is still quite new, but growing rapidly, is the formal integration of online employee volunteering programmes into the infrastructure and business plan of companies.\n"}
