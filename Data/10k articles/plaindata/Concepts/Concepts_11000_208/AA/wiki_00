{"id": "49482835", "url": "https://en.wikipedia.org/wiki?curid=49482835", "title": "Aerobic fermentation", "text": "Aerobic fermentation\n\nAerobic fermentation is a metabolic process by which cells metabolize sugars via fermentation in the presence of oxygen and occurs through the repression of normal respiratory metabolism (also referred to as the crabtree effect in yeast). This phenomenon is fairly rare and is primarily observed in yeasts. Aerobic fermentation evolved independently in at least three yeast lineages (\"Saccharomyces\", \"Dekkera\", \"Schizosaccharomyces\"). It has also been observed in plant pollen, trypanosomatids, mutated \"E. coli\", and tumor cells. Crabtree-positive yeasts will respire when grown with very low concentrations of glucose or when grown on most other carbohydrate sources. The Crabtree effect is a regulatory system whereby respiration is repressed by fermentation, except in low sugar conditions. When \"Saccharomyces cerevisiae\" is grown below the sugar threshold and undergoes a respiration metabolism, the fermentation pathway is still fully expressed, while the respiration pathway is only expressed relative to the sugar availability. This contrasts with the pasteur effect, which is the inhibition of fermentation in the presence of oxygen, and observed in most organisms.\n\nThe evolution of aerobic fermentation likely involved multiple successive molecular steps, which included the expansion of hexose transporter genes, copy number variation (CNV) and differential expression in metabolic genes, and regulatory reprogramming. Research is still needed to fully understand the genomic basis of this complex phenomenon. Many crabtree-positive yeast species are used for their fermentation ability in industrial processes in the production of wine, beer, sake, bread, and bioethanol. Through domestication, these yeast species have evolved, often through artificial selection, to better fit their environment. Strains evolved through mechanisms that include interspecific hybridization, horizontal gene transfer (HGT), gene duplication, pseudogenization, and gene loss.\n\nApproximately 100 million years ago (mya), within the yeast lineage there was a whole genome duplication (WGD). A majority of Crabtree-positive yeasts are post-WGD yeasts. It was believed that the WGD was a mechanism for the development of Crabtree effect in these species due to the duplication of alcohol dehydrogenase (ADH) encoding genes and hexose transporters. However, recent evidence has shown that aerobic fermentation originated before the WGD and evolved as a multi-step process, potentially aided by the WGD. The origin of aerobic fermentation, or the first step, in \"Saccharomyces\" crabtree-positive yeasts likely occurred in the interval between the ability to grow under anaerobic conditions, horizontal transfer of anaerobic DHODase (encoded by URA1 with bacteria), and the loss of respiratory chain Complex I. A more pronounced Crabtree effect, the second step, likely occurred near the time of the WGD event. Later evolutionary events that aided in the evolution of aerobic fermentation are better understood and outlined in the Genomic basis of the crabtree effect section.\n\nIt is believed that a major driving force in the origin of aerobic fermentation was its simultaneous origin with modern fruit (~125 mya). These fruit provided an abundance of simple sugar food source for microbial communities, including both yeast and bacteria. Bacteria, at that time, were able to produce biomass at a faster rate than the yeast. Producing a toxic compound, like ethanol, can slow the growth of bacteria, allowing the yeast to be more competitive. However, the yeast still had to use a portion of the sugar it consumes to produce ethanol. Crabtree-positive yeasts also have increased glycolytic flow, or increased uptake of glucose and conversion to pyruvate, which compensates for using a portion of the glucose to produce ethanol rather than biomass. Therefore, it is believed that the original driving force was to kill competitors. This is supported by research that determined the kinetic behavior of the ancestral ADH protein, which was found to be optimized to make ethanol, rather than consume it.\n\nFurther evolutionary events in the development of aerobic fermentation likely increased the efficiency of this lifestyle, including increased tolerance to ethanol and the repression of the respiratory pathway. In high sugar environments, \"S. cerevisiae\" outcompetes and dominants all other yeast species, except its closest relative \"Saccharomyces paradoxus\". The ability of \"S. cerevisiae\" to dominate in high sugar environments evolved more recently than aerobic fermentation and is dependent on the type of high-sugar environment. Other yeasts' growth is dependent on the pH and nutrients of the high-sugar environment.\n\nThe genomic basis of the crabtree effect is still being investigated, and its evolution likely involved multiple successive molecular steps that increased the efficiency of the lifestyle.\n\nHexose transporters (HXT) are a group of proteins that are largely responsible for the uptake of glucose in yeast. In \"S. cerevisiae\", 20 \"HXT\" genes have been identified and 17 encode for glucose transporters (\"HXT1-HXT17\"), \"GAL2\" encodes for a galactose transporter, and \"SNF3\" and \"RGT2\" encode for glucose sensors. The number of glucose sensor genes have remained mostly consistent through the budding yeast lineage, however glucose sensors are absent from \"Schizosaccharomyces pombe\". \"Sch. pombe\" is a Crabtree-positive yeast, which developed aerobic fermentation independently from \"Saccharomyces\" lineage, and detects glucose via the cAMP-signaling pathway. The number of transporter genes vary significantly between yeast species and has continually increased during the evolution of the \"S. cerevisiae\" lineage. Most of the transporter genes have been generated by tandem duplication, rather than from the WGD. \"Sch. pombe\" also has a high number of transporter genes compared to its close relatives. Glucose uptake is believed to be a major rate-limiting step in glycolysis and replacing \"S. cerevisiae\"'s \"HXT1-17\" genes with a single chimera \"HXT\" gene results in decreased ethanol production or fully respiratory metabolism. Thus, having an efficient glucose uptake system appears to be essential to ability of aerobic fermentation. There is a significant positive correlation between the number of hexose transporter genes and the efficiency of ethanol production.\n\nAfter a WGD, one of the duplicated gene pair is often lost through fractionation; less than 10% of WGD gene pairs have remained in \"S. cerevisiae\" genome. A little over half of WGD gene pairs in the glycolysis reaction pathway were retained in post-WGD species, significantly higher than the overall retention rate. This has been associated with an increased ability to metabolize glucose into pyruvate, or higher rate of glycolysis. After glycolysis, pyruvate can either be further broken down by pyruvate decarboxylase (Pdc) or pyruvate dehydrogenase (Pdh). The kinetics of the enzymes are such that when pyruvate concentrations are high, due to a high rate of glycolysis, there is increased flux through Pdc and thus the fermentation pathway. The WGD is believed to have played a beneficial role in the evolution of the Crabtree effect in post-WGD species partially due to this increase in copy number of glycolysis genes.\n\nThe fermentation reaction only involves two steps. Pyruvate is converted to acetaldehyde by Pdc and then acetaldehyde is converted to ethanol by alcohol dehydrogenase (Adh). There is no significant increase in the number of \"Pdc\" genes in Crabtree-positive compared to Crabtree-negative species and no correlation between number of \"Pdc\" genes and efficiency of fermentation. There are five \"Adh\" genes in \"S. cerevisiae\". Adh1 is the major enzyme responsible for catalyzing the fermentation step from acetaldehyde to ethanol. Adh2 catalyzes the reverse reaction, consuming ethanol and converting it to acetaldehyde. The ancestral, or original, Adh had a similar function as Adh1 and after a duplication in this gene, Adh2 evolved a lower K for ethanol. Adh2 is believed to have increased yeast species' tolerance for ethanol and allowed Crabtree-positive species to consume the ethanol they produced after depleting sugars. However, Adh2 and consumption of ethanol is not essential for aerobic fermentation. \"Sch. pombe\" and other Crabtree positive species do not have the \"ADH2\" gene and consumes ethanol very poorly.\n\nIn Crabtree-negative species, respiration related genes are highly expressed in the presence of oxygen. However, when \"S. cerevisiae\" is grown on glucose in aerobic conditions, respiration-related gene expression is repressed. Mitochondrial ribosomal proteins expression is only induced under environmental stress conditions, specifically low glucose availability. Genes involving mitochondrial energy generation and phosphorylation oxidation, which are involved in respiration, have the largest expression difference between aerobic fermentative yeast species and respiratory species. In a comparative analysis between \"Sch. pombe\" and \"S. cerevisiae\", both of which evolved aerobic fermentation independently, the expression pattern of these two fermentative yeasts were more similar to each other than a respiratory yeast, \"C. albicans\". However, \"S. cerevisiae\" is evolutionarily closer to \"C. albicans\". Regulatory rewiring was likely important in the evolution of aerobic fermentation in both lineages.\n\nAerobic fermentation is also essential for multiple industries, resulting in human domestication of several yeast strains. Beer and other alcoholic beverages, throughout human history, have played a significant role in society through drinking rituals, providing nutrition, medicine, and uncontaminated water. During the domestication process, organisms shift from natural environments that are more variable and complex to simple and stable environments with a constant substrate. This often favors specialization adaptations in domesticated microbes, associated with relaxed selection for non-useful genes in alternative metabolic strategies or pathogenicity. Domestication might be partially responsible for the traits that promote aerobic fermentation in industrial species. Introgression and HGT is common in \"Saccharomyces\" domesticated strains. Many commercial wine strains have significant portions of their DNA derived from HGT of non-\"Saccharomyces\" species. HGT and introgression are less common in nature than is seen during domestication pressures. For example, the important industrial yeast strain \"Saccharomyces pastorianus\", is an interspecies hybrid of \"S. cerevisiae\" and the cold tolerant \"S. eubayanus.\" This hybrid is commonly used in lager-brewing, which requires slow, low temperature fermentation.\n\nAlcoholic fermentation is often used by plants in anaerobic conditions to produce ATP and regenerate NAD to allow for glycolysis to continue. For most plant tissues, fermentation only occurs in anaerobic conditions, but there are a few exceptions. In the pollen of maize (\"Zea mays\") and tobacco (\"Nicotiana tabacum\" & \"Nicotiana plumbaginifolia\"), the fermentation enzyme ADH is abundant, regardless of the oxygen level. In tobacco pollen, PDC is also highly expressed in this tissue and transcript levels are not influenced by oxygen concentration. Tobacco pollen, similar to Crabtree-positive yeast, perform high levels of fermentation dependent on the sugar supply, and not oxygen availability. In these tissues, respiration and alcoholic fermentation occur simultaneously with high sugar availability. Fermentation produces the toxic acetaldehyde and ethanol, that can build up in large quantities during pollen development. It has been hypothesized that acetaldehyde is a pollen factor that causes cytoplasmic male sterility. Cytoplasmic male sterility is a trait observed in maize, tobacco and other plants in which there is an inability to produce viable pollen. It is believed that this trait might be due to the expression of the fermentation genes, ADH and PDC, a lot earlier on in pollen development than normal and the accumulation of toxic aldehyde.\n\nWhen grown in glucose-rich media, trypanosomatid parasites degrade glucose via aerobic fermentation. In this group, this phenomenon is not a pre-adaptation to/or remnant of anaerobic life, shown through their inability to survive in anaerobic conditions. It is believed that this phenomenon developed due to the capacity for a high glycolytic flux and the high glucose concentrations of their natural environment. The mechanism for repression of respiration in these conditions is not yet known.\n\nA couple \"Escherichia coli\" mutant strains have been bioengineered to ferment glucose under aerobic conditions. One group developed the ECOM3 (\"E. coli\" cytochrome oxidase mutant) strain by removing three terminal cytochrome oxidases (cydAB, cyoABCD, and cbdAB) to reduce oxygen uptake. After 60 days of adaptive evolution on glucose media, the strain displayed a mixed phenotype. In aerobic conditions, some populations' fermentation solely produced lactate, while others did mixed-acid fermentation.\n\nOne of the hallmarks of cancer is altered metabolism or deregulating cellular energetics. Cancers cells often have reprogrammed their glucose metabolism to perform lactic acid fermentation, in the presence of oxygen, rather than send the pyruvate made through glycolysis to the mitochondria. This is referred to as the Warburg effect, and is associated with high consumption of glucose and a high rate of glycolysis. ATP production in these cancer cells is often only through the process of glycolysis and pyruvate is broken down by the fermentation process in the cell's cytoplasm. This phenomenon is often seen as counterintuitive, since cancer cells have higher energy demands due to the continued proliferation and respiration produces significantly more ATP than glycolysis alone (fermentation produces no additional ATP). Typically, there is an up-regulation in glucose transporters and enzymes in the glycolysis pathway (also seen in yeast). There are many parallel aspects of aerobic fermentation in tumor cells that are also seen in Crabtree-positive yeasts. Further research into the evolution of aerobic fermentation in yeast such as \"S. cerevisiae\" can be a useful model for understanding aerobic fermentation in tumor cells. This has a potential for better understanding cancer and cancer treatments.\n"}
{"id": "3788586", "url": "https://en.wikipedia.org/wiki?curid=3788586", "title": "Bioconversion", "text": "Bioconversion\n\nBioconversion, also known as \"biotransformation\", is the conversion of organic materials, such as plant or animal waste, into usable products or energy sources by biological processes or agents, such as certain microorganisms. One example is the industrial production of cortisone, which one step is the bioconversion of progesterone to 11-alpha-Hydroxyprogesterone by \"Rhizopus nigricans.\" Another example is the bioconversion of glycerol to 1,3-propanediol, which is part of scientific research for many decades.\n\nAnother example of bioconversion is the conversion of organic materials, such as plant or animal waste, into usable products or energy sources by biological processes or agents, such as certain microorganisms, some detritivores or enzymes.\n\nIn the USA, the Bioconversion Science and Technology group performs multidisciplinary R&D for the Department of Energy's (DOE) relevant applications of bioprocessing, especially with biomass. Bioprocessing combines the disciplines of chemical engineering, microbiology and biochemistry. The Group 's primary role is investigation of the use of microorganism, microbial consortia and microbial enzymes in bioenergy research. New cellulosic ethanol conversion processes have enabled the variety and volume of feedstock that can be bioconverted to expand rapidly. Feedstock now includes materials derived from plant or animal waste such as paper, auto-fluff, tires, fabric, construction materials, municipal solid waste (MSW), sludge, sewage, etc.\n\n1 - Enzymatic hydrolysis - a single source of feedstock, switchgrass for example, is mixed with strong enzymes which convert a portion of cellulosic material into sugars which can then be fermented into ethanol. Genencor and Novozymes are two companies that have received United States government Department of Energy funding for research into reducing the cost of cellulase, a key enzyme in the production cellulosic ethanol by this process.\n\n2 - Synthesis gas fermentation - a blend of feedstock, not exceeding 30% water, is gasified in a closed environment into a syngas containing mostly carbon monoxide and hydrogen. The cooled syngas is then converted into usable products through exposure to bacteria or other catalysts. BRI Energy, LLC is a company whose pilot plant in Fayetteville, Arkansas is currently using synthesis gas fermentation to convert a variety of waste into ethanol. After gasification, anaerobic bacteria (\"Clostridium ljungdahlii\") are used to convert the syngas (CO, CO, and H) into ethanol. The heat generated by gasification is also used to co-generate excess electricity.\n\n3 - C.O.R.S. and Grub Composting are sustainable technologies that employ organisms that feed on organic matter to reduce and convert organic waste in to high quality feedstuff and oil rich material for the biodiesel industry.\nOrganizations pioneering this novel approach to waste management are EAWAG, ESR International, Prota Culture and BIOCONVERSION that created the \"e\"-CORS® system to meet large scale organic waste management needs and environmental sustainability in both urban and livestock farming reality. This type of engineered system introduces a substantial innovation represented by the automatic modulation of the treatment, able to adapt conditions of the system to the biology of the scavenger used, improving their performances and the power of this technology.\n"}
{"id": "9774491", "url": "https://en.wikipedia.org/wiki?curid=9774491", "title": "Biomass heating system", "text": "Biomass heating system\n\nBiomass heating systems generate heat from biomass.<br>\nThe systems fall under the categories of:\n\nThe use of biomass in heating systems is beneficial because it uses agricultural, forest, urban and industrial residues and waste to produce heat and electricity with less effect on the environment than fossil fuels. This type of energy production has a limited long-term effect on the environment because the carbon in biomass is part of the natural carbon cycle; while the carbon in fossil fuels is not, and permanently adds carbon to the environment when burned for fuel (carbon footprint). Historically, before the use of fossil fuels in significant quantities, biomass in the form of wood fuel provided most of humanity's heating.\n\nOn a large scale, the use of biomass removes agricultural land from food production, reduces the carbon sequestration capacity of forests, and extracts nutrients from the soil. Combustion of biomass creates air pollutants and adds significant quantities of carbon to the atmosphere that may not be returned to the soil for many decades.\n\nUsing biomass as a fuel produces air pollution in the form of carbon monoxide, NOx (nitrogen oxides), VOCs (volatile organic compounds), particulates and other pollutants, in some cases at levels above those from traditional fuel sources such as coal or natural gas. Black carbon – a pollutant created by incomplete combustion of fossil fuels, biofuels, and biomass – is possibly the second largest contributor to global warming. In 2009 a Swedish study of the giant brown haze that periodically covers large areas in South Asia determined that it had been principally produced by biomass burning, and to a lesser extent by fossil-fuel burning. Researchers measured a significant concentration of C, which is associated with recent plant life rather than with fossil fuels.\n\nOn combustion, the carbon from biomass is released into the atmosphere as carbon dioxide (CO). The amount of carbon stored in dry wood is approximately 50% by weight. When from agricultural sources, plant matter used as a fuel can be replaced by planting for new growth. When the biomass is from forests, the time to recapture the carbon stored is generally longer, and the carbon storage capacity of the forest may be reduced overall if destructive forestry techniques are employed.\n\nThe biomass-is-carbon-neutral proposal put forward in the early 1990s has been superseded by more recent science that recognizes that mature, intact forests sequester carbon more effectively than cut-over areas. When a tree’s carbon is released into the atmosphere in a single pulse, it contributes to climate change much more than woodland timber rotting slowly over decades. Current studies indicate that \"even after 50 years the forest has not recovered to its initial carbon storage\" and \"the optimal strategy is likely to be protection of the standing forest\".\n\nThe oil price increases since 2003 and consequent price increases for natural gas and coal have increased the value of biomass for heat generation. Forest renderings, agricultural waste, and crops grown specifically for energy production become competitive as the prices of energy dense fossil fuels rise. Efforts to develop this potential may have the effect of regenerating mismanaged croplands and be a cog in the wheel of a decentralized, multi-dimensional renewable energy industry. Efforts to promote and advance these methods became common throughout the European Union through the 2000s. In other areas of the world, inefficient and polluting means to generate heat from biomass coupled with poor forest practices have significantly added to environmental degradation.\n\nBuffer tanks store the hot water the biomass appliance generates and circulates it around the heating system. Sometimes referred to as 'thermal stores', they are crucial for the efficient operation of all biomass boilers where the system loading fluctuates rapidly, or the volume of water in the complete hydraulic system is relatively small. Using a suitably sized buffer vessel prevents rapid cycling of the boiler when the loading is below the minimum boiler output. Rapid cycling of the boiler causes a large increase in harmful emissions such as Carbon monoxide, dust, and NOx, greatly reduces boiler efficiency and increases electrical consumption of the unit. In addition, service and maintenance requirements will be increased as parts are stressed by rapid heating and cooling cycles. Although most boilers claim to be able to turn down to 30% of nominal output, in the real world this is often not achievable due to differences in the fuel from the 'ideal' or test fuel. A suitably sized buffer tank should therefore be considered where the loading of the boiler drops below 50% of the nominal output – in other words unless the biomass component is purely base load, the system should include a buffer tank. In any case where the secondary system does not contain sufficient water for safe removal of residual heat from the biomass boiler irrespective of the loading conditions, the system must include a suitably sized buffer tank. The residual heat from a biomass unit varies greatly depending on the boiler design and the thermal mass of the combustion chamber. Light weight, fast response boilers require only 10L/kW, while industrial wet wood units with very high thermal mass require 40L/kW.\n\nThe use of Biomass in heating systems has a use in many different types of buildings, and all have different uses. There are four main types of heating systems that use biomass to heat a boiler. The types are Fully Automated, Semi-Automated, Pellet-Fired, and Combined Heat and Power.\n\nIn fully automated systems chipped or ground up waste wood is brought to the site by delivery trucks and dropped into a holding tank. A system of conveyors then transports the wood from the holding tank to the boiler at a certain managed rate. This rate is managed by computer controls and a laser that measures the load of fuel the conveyor is bringing in. The system automatically goes on and off to maintain the pressure and temperature within the boiler. Fully automated systems offer a great deal of ease in their operation because they only require the operator of the system to control the computer, and not the transport of wood while offering comprehensive and cost effective solutions to complex industrial challenges.\n\nSemi-automated or \"Surge Bin\" systems are very similar to fully automated systems except they require more manpower to keep operational. They have smaller holding tanks, and a much simpler conveyor systems which will require personnel to maintain the systems operation. The reasoning for the changes from the fully automated system is the efficiency of the system. The heat created by the combustor can be used to directly heat the air or it can be used to heat water in a boiler system which acts as the medium by which the heat is delivered. Wood fire fueled boilers are most efficient when they are running at their highest capacity, and the heat required most days of the year will not be the peak heat requirement for the year. Considering that the system will only need to run at a high capacity a few days of the year, it is made to meet the requirements for the majority of the year to maintain its high efficiency.\n\nThe third main type of biomass heating systems are pellet-fired systems. Pellets are a processed form of wood, which make them more expensive. Although they are more expensive, they are much more condensed and uniform, and therefore are more efficient. Further, it is relatively easy to automatically feed pellets to boilers. In these systems, the pellets are stored in a grain-type storage silo, and gravity is used to move them to the boiler. The storage requirements are much smaller for pellet-fired systems because of their condensed nature, which also helps cut down costs. these systems are used for a wide variety of facilities, but they are most efficient and cost effective for places where space for storage and conveyor systems is limited, and where the pellets are made fairly close to the facility.\n\nOne subcategory of pellet systems are boilers or burners capable of burning pellet with higher ash rate (paper pellets, hay pellets, straw pellets). One of this kind is PETROJET pellet burner with rotating cylindrical burning chamber.\nIn terms of efficiencies advanced pellet boilers can exceed other forms of biomass because of the more stable fuel charataristics. Advanced pellet boilers can even work in condensing mode and cool down combustion gases to 30-40 °C, instead of 120 °C before sent into the flue.\n\nCombined heat and power systems are very useful systems in which wood waste, such as wood chips, is used to generate power, and heat is created as a byproduct of the power generation system. They have a very high cost because of the high pressure operation. Because of this high pressure operation, the need for a highly trained operator is mandatory, and will raise the cost of operation. Another drawback is that while they produce electricity they will produce heat, and if producing heat is not desirable for certain parts of the year, the addition of a cooling tower is necessary, and will also raise the cost.\n\nThere are certain situations where CHP is a good option. Wood product manufacturers would use a combined heat and power system because they have a large supply of waste wood, and a need for both heat and power. Other places where these systems would be optimal are hospitals and prisons, which need energy, and heat for hot water. These systems are sized so that they will produce enough heat to match the average heat load so that no additional heat is needed, and a cooling tower is not needed.\n\n\n"}
{"id": "49304088", "url": "https://en.wikipedia.org/wiki?curid=49304088", "title": "Book desert", "text": "Book desert\n\nA book desert is a geographic area (country, state, county, city, neighborhood, home) where printed books and other reading material are allegedly hard to obtain, particularly without access to an automobile or other form of transportation. Some researchers have defined book deserts by linking them to poverty and low income, while others use a combination of factors that include census data, income, ethnicity, geography, language, and the number of books in a home.\n\nInitiatives that increase the availability of books by such measures as bookmobiles and librarians on bicycles have been offered as possible solutions to book deserts, as have Little Free Libraries and offering children's literature available online, free of charge.\n\nIn the past, researchers have studied how the absence or scarcity of books impact how a child's early literacy and language skills develop. The term \"book desert\" came into regular use in the mid-2010s and the social enterprise Unite for Literacy is credited as having coined the term. Unite for Literacy created an operational definition of a book desert when they published the U.S. Book Desert Map: A geographic area (country, state, county, census tract) where it is predicted that a majority of homes have less than 100 printed books. In March 2014, James LaRue, director of the American Library Association's Office for Intellectual Freedom and the Freedom to Read Foundation, used the term in an issue of \"American Libraries\", where he described the term as applying to houses with 25 or fewer books in them and discussed ways to lessen or eradicate the problem.\n\nIn July 2016, professors Susan B. Neuman and Naomi Moland published a study in \"Urban Education\", where they examined how the lack of printed reading material among low-income and poverty stricken neighborhoods impacts early childhood development and used the term to describe areas and homes with little access to written materials. This study built upon other research Neuman had conducted in 2001 on the same topic and the researchers found few stores in Detroit, Los Angeles, and Washington, D.C.; the focus areas of their research had print resources for children ages 0 through 18. Of those stores, many were dollar stores. \"The Atlantic\" reported that in 2015 Neuman and JetBlue Airways held an experiment to foster literacy by providing book vending machines in a low-income Washington D.C. neighborhood. Over 20,000 books were given away, prompting Neuman to conclude that the neighborhood's parents did care about their children's education but lacked \"the resources to enable their children to be successful.\"\n\nMultiple factors are credited as contributing to the formation of book deserts, the most frequently highlighted of which tends to be poverty and low income. Other factors tend to include language and geography, as some areas lack access to bookstores or public or community school libraries that would provide books. Book store closures due to bankruptcy or other financial difficulties are also occasionally cited as a contributing factor, when the closure leaves the area without a bookseller.\n\nUnite for Literacy has developed a book desert map of the United States powered by Esri's ArcGIS platform, which provides a visual presentation of the lack of books in the nation, states, counties and census tracts. To create the map, Unite for Literacy performed a statistical analyses of data from the National Assessment of Educational Progress and the American Community Survey. Data used in the map includes the number of books in 4th graders' homes, average community income, ethnic diversity, geographic location and home language. Unite for Literacy unveiled the map during the Clinton Global Initiative America (CGI America) meeting held in Denver, Colorado, in June 2014.\n\n"}
{"id": "404181", "url": "https://en.wikipedia.org/wiki?curid=404181", "title": "Closed and exact differential forms", "text": "Closed and exact differential forms\n\nIn mathematics, especially vector calculus and differential topology, a closed form is a differential form \"α\" whose exterior derivative is zero (), and an exact form is a differential form, \"α\", that is the exterior derivative of another differential form \"β\". Thus, an \"exact\" form is in the \"image\" of \"d\", and a \"closed\" form is in the \"kernel\" of \"d\".\n\nFor an exact form \"α\", for some differential form \"β\" of degree one less than that of \"α\". The form \"β\" is called a \"potential form\" or \"primitive\" for \"α\". Since the exterior derivative of a closed form is zero, \"β\" is not unique, but can be modified by the addition of any closed form of degree one less than that of \"α\".\n\nBecause , any exact form is necessarily closed. The question of whether \"every\" closed form is exact depends on the topology of the domain of interest. On a contractible domain, every closed form is exact by the Poincaré lemma. More general questions of this kind on an arbitrary differentiable manifold are the subject of de Rham cohomology, which allows one to obtain purely topological information using differential methods.\n\nA simple example of a form which is closed but not exact is the 1-form formula_1 given by the derivative of argument on the punctured plane formula_2. Since formula_3 is not actually a function (see the next paragraph) formula_1 is not an exact form. Still, formula_1 has vanishing derivative and is therefore closed.\n\nNote that the argument formula_3 is only defined up to an integer multiple of formula_7 since a single point formula_8 can be assigned different arguments formula_9, formula_10, etc. We can assign arguments in a locally consistent manner around formula_8, but not in a globally consistent manner. This is because if we trace a loop from formula_8 counterclockwise around the origin and back to formula_8, the argument increases by formula_7. Generally, the argument formula_3 changes by\nover a counter-clockwise oriented loop formula_17.\n\nEven though the argument formula_3 is not technically a function, the different \"local\" definitions of formula_3 at a point formula_8 differ from one another by constants. Since the derivative at formula_8 only uses local data, and since functions that differ by a constant have the same derivative, the argument has a globally well-defined derivative \"formula_1\".\n\nThe upshot is that formula_1 is a one-form on formula_2 that is not actually the derivative of any well-defined function formula_3. We say that formula_1 is not \"exact\". Explicitly, formula_1 is given as:\nwhich by inspection has derivative zero. Because formula_1 has vanishing derivative, we say that it is \"closed\".\n\nThis form generates the de Rham cohomology group formula_30 meaning that any closed form formula_31 is the sum of an exact form formula_32 and a multiple of formula_33 formula_34 where formula_35 accounts for a non-trivial contour integral around the origin, which is the only obstruction to a closed form on the punctured plane (locally the derivative of a potential function) being the derivative of a globally defined function.\n\nDifferential forms in R and R were well known in the mathematical physics of the nineteenth century. In the plane, 0-forms are just functions, and 2-forms are functions times the basic area element , so that it is the 1-forms\n\nthat are of real interest. The formula for the exterior derivative \"d\" here is\n\nwhere the subscripts denote partial derivatives. Therefore the condition for formula_38 to be \"closed\" is\n\nIn this case if is a function then\n\nThe implication from 'exact' to 'closed' is then a consequence of the symmetry of second derivatives, with respect to \"x\" and \"y\".\n\nThe gradient theorem asserts that a 1-form is exact if and only if the line integral of the form depends only on the endpoints of the curve, or equivalently,\nif the integral around any smooth closed curve is zero.\n\nOn a Riemannian manifold, or more generally a pseudo-Riemannian manifold, \"k\"-forms correspond to \"k\"-vector fields (by duality via the metric), so there is a notion of a vector field corresponding to a closed or exact form.\n\nIn 3 dimensions, an exact vector field (thought of as a 1-form) is called a conservative vector field, meaning that it is the derivative (gradient) of a 0-form (smooth scalar field), called the scalar potential. A closed vector field (thought of as a 1-form) is one whose derivative (curl) vanishes, and is called an irrotational vector field.\n\nThinking of a vector field as a 2-form instead, a closed vector field is one whose derivative (divergence) vanishes, and is called an incompressible flow (sometimes solenoidal vector field).\n\nThe concepts of conservative and incompressible vector fields generalize to \"n\" dimensions, because gradient and divergence generalize to \"n\" dimensions; curl is defined only in three dimensions, thus the concept of irrotational vector field does not generalize in this way.\n\nThe Poincaré lemma states that if \"B\" is an open ball in R, any smooth closed \"p\"-form \"ω\" defined on \"B\" is exact, for any integer \"p\" with .\n\nTranslating if necessary, it can be assumed that the ball \"B\" has centre 0. Let \"α\" be the flow on R defined by . For it carries \"B\" into itself and induces an action on functions and differential forms. \nThe derivative of the flow is the vector field \"X\" defined on functions \"f\" by : it is the \"radial vector field\" . The derivative of the flow on forms defines the Lie derivative with respect to \"X\" given by . In particular \n"}
{"id": "18800923", "url": "https://en.wikipedia.org/wiki?curid=18800923", "title": "Conflict early warning", "text": "Conflict early warning\n\nThe field of conflict early warning seeks to forecast the outbreak of armed conflict, or, at minimum, to detect the early escalation of violence, with the objective of preventing the outbreak or the further escalation of violence in order to save lives. \n\nInitial conceptions of conflict early warning materialized in the 1970s and 1980s but the field really emerged on the international policy agenda after the end of the Cold War. Both qualitative and quantitative approaches have been developed for conflict forecasting and conflict monitoring. Qualitative methodologies typically draw on local area experts with extensive knowledge on one country or region. This is the approach taken by the International Crisis Group, for example. In contrast, quantitative methodologies quantify conflict trends and use mathematical techniques to forecast future trends or \"events of interest\" (EOIs) such as the onset of conflicts. For example, the Integrated Conflict Early Warning System (ICEWS) project at the Defense Advanced Research Projects Agency (DARPA) takes this approach. Some approaches to conflict early warning combine both qualitative and quantitative methodologies, such as Swisspeace's formerly operational project called FAST.\n\nThe unanticipated events of the Yom Kippur War in 1973 and that of the Falklands War in 1982 provoked a series of debates over the lack of early warning. The incident over the Falklands had taken the United Nations completely by surprise and it is said \"no map of the islands was available in the Secretariat when the invasion began\". The initial drivers, however, were humanitarian agencies \"driven by the need for accurate and timely predictions of refugee flows to enable effective contingency planning\". After the end of the Cold War, political scientists at leading academic institutions began modifying old Cold War models of conflict to understand the onset of new wars. The horrors of the 1994 Rwandan genocide also spurred increased interest in operational conflict early warning systems. The FAST project of Swisspeace and the Forum on Early Warning and Early Response (FEWER) were responses to the genocide.\n\n\n"}
{"id": "21650435", "url": "https://en.wikipedia.org/wiki?curid=21650435", "title": "Conservation-reliant species", "text": "Conservation-reliant species\n\nConservation-reliant species are animal or plant species that require continuing species-specific wildlife management intervention such as predator control, habitat management and parasite control to survive, even when a self-sustainable recovery in population is achieved.\n\nThe term \"conservation-reliant species\" grew out of the conservation biology undertaken by \"The Endangered Species Act at Thirty Project\" (launched 2001) and its popularization by project leader J. Michael Scott. Its first use in a formal publication was in \"Frontiers in Ecology and the Environment\" in 2005. Worldwide use of the term has not yet developed and it has not yet appeared in a publication compiled outside North America.\n\nPassages of the 1973 Endangered Species Act (ESA) carried with it the assumption that endangered species would be delisted as their populations recovered. It assumed they would then thrive under existing regulations and the protections afforded under the ESA would no longer be needed. However, eighty percent of species currently listed under the ESA fail to meet that assumption. To survive, they require species-specific conservation interventions (e.g. control of predators, competitors, nest parasites, prescribed burns, altered hydrological processes, etc.) and thus they are conservation-reliant.\n\nThe criteria for assessing whether a species is conservation-reliant are:\n\nThere are five major areas of management action for conservation of vulnerable species:\n\nA prominent example is in India, where tigers, an apex predator and the national animal, are considered a conservation-reliant species. This keystone species can maintain self-sustaining wild populations; however, they require ongoing management actions because threats are pervasive, recurrent and put them at risk of extinction. The origin of these threats are rooted in the changing socio-economic, political and spatial organization of society in India. Tigers have become extinct in some areas because of extrinsic factors such as habitat destruction, poaching, disease, floods, fires and drought, decline of prey species for the same reasons, as well as intrinsic factors such as demographic stochasticity and genetic deterioration.\n\nRecognizing the conservation reliance of tigers, Project Tiger is establishing a national science-based framework for monitoring tiger population trends in order to manage the species more effectively. India now has 28 tiger reserves, located in 17 states. These reserves cover including 1.14% of the total land area of the country. These reserves are kept free of biotic disturbances, forestry operations, collection of minor forest products, grazing and human disturbance. The populations of tigers in these reserves now constitute some of the most important tiger source populations in the country.\n\nThe magnitude and pace of human impacts on the environment make it unlikely that substantial progress will be made in delisting many species unless the definition of \"recovery\" includes some form of active management. Preventing delisted species from again being at risk of extinction may require continuing, species-specific management actions. Viewing \"recovery\" of \"conservation-reliant species\" as a continuum of phases rather a simple \"recovered/not recovered\" status may enhance the ability to manage such species within the framework of the Endangered Species Act. With ongoing loss of habitat, disruption of natural cycles, increasing impacts of non-native invasive species, it is probable that the number of conservation-reliant species will increase.\n\nIt has been proposed that development of \"recovery management agreements\", with legally and biologically defensible contracts would provide for continuing conservation management following delisting. The use of such formalized agreements will facilitate shared management responsibilities between federal wildlife agencies and other federal agencies, and with state, local, and tribal governments, as well as with private entities that have demonstrated the capability to meet the needs of conservation-reliant species.\n"}
{"id": "67088", "url": "https://en.wikipedia.org/wiki?curid=67088", "title": "Conservation of energy", "text": "Conservation of energy\n\nIn physics and chemistry, the law of conservation of energy states that the total energy of an isolated system remains constant, it is said to be \"conserved\" over time. This law means that energy can neither be created nor destroyed; rather, it can only be transformed or transferred from one form to another. For instance, chemical energy is converted to kinetic energy when a stick of dynamite explodes. If one adds up all the forms of energy that were released in the explosion, such as the kinetic energy of the pieces, as well as heat and sound, one will get the exact decrease of chemical energy in the combustion of the dynamite. Classically, conservation of energy was distinct from conservation of mass; however, special relativity showed that mass is related to energy and vice versa by \"E = mc\", and science now takes the view that mass–energy is conserved.\n\nConservation of energy can be rigorously proven by Noether's theorem as a consequence of continuous time translation symmetry; that is, from the fact that the laws of physics do not change over time.\n\nA consequence of the law of conservation of energy is that a perpetual motion machine of the first kind cannot exist, that is to say, no system without an external energy supply can deliver an unlimited amount of energy to its surroundings. For systems which do not have time translation symmetry, it may not be possible to define conservation of energy. Examples include curved spacetimes in general relativity or time crystals in condensed matter physics.\n\nAncient philosophers as far back as Thales of Miletus  550 BCE had inklings of the conservation of some underlying substance of which everything is made. However, there is no particular reason to identify this with what we know today as \"mass-energy\" (for example, Thales thought it was water). Empedocles (490–430 BCE) wrote that in this universal system, composed of four roots (earth, air, water, fire), \"nothing comes to be or perishes\"; instead, these elements suffer continual rearrangement.\n\nIn 1605, Simon Stevinus was able to solve a number of problems in statics based on the principle that perpetual motion was impossible.\n\nIn 1638, Galileo published his analysis of several situations—including the celebrated \"interrupted pendulum\"—which can be described (in modern language) as conservatively converting potential energy to kinetic energy and back again. Essentially, he pointed out that the height a moving body rises is equal to the height from which it falls, and used this observation to infer the idea of inertia. The remarkable aspect of this observation is that the height to which a moving body ascends on a frictionless surface does not depend on the shape of the surface.\n\nIn 1669, Christian Huygens published his laws of collision. Among the quantities he listed as being invariant before and after the collision of bodies were both the sum of their linear momentums as well as the sum of their kinetic energies. However, the difference between elastic and inelastic collision was not understood at the time. This led to the dispute among later researchers as to which of these conserved quantities was the more fundamental. In his Horologium Oscillatorium, he gave a much clearer statement regarding the height of ascent of a moving body, and connected this idea with the impossibility of a perpetual motion. Huygens' study of the dynamics of pendulum motion was based on a single principle: that the center of gravity of a heavy object cannot lift itself.\n\nThe fact that kinetic energy is scalar, unlike linear momentum which is a vector, and hence easier to work with did not escape the attention of Gottfried Wilhelm Leibniz. It was Leibniz during 1676–1689 who first attempted a mathematical formulation of the kind of energy which is connected with \"motion\" (kinetic energy). Using Huygens' work on collision, Leibniz noticed that in many mechanical systems (of several masses, \"m\" each with velocity \"v\"),\n\nwas conserved so long as the masses did not interact. He called this quantity the \"vis viva\" or \"living force\" of the system. The principle represents an accurate statement of the approximate conservation of kinetic energy in situations where there is no friction. Many physicists at that time, such as Newton, held that the conservation of momentum, which holds even in systems with friction, as defined by the momentum:\n\nwas the conserved \"vis viva\". It was later shown that both quantities are conserved simultaneously, given the proper conditions such as an elastic collision.\n\nIn 1687, Isaac Newton published his Principia, which was organized around the concept of force and momentum. However, the researchers were quick to recognize that the principles set out in the book, while fine for point masses, were not sufficient to tackle the motions of rigid and fluid bodies. Some other principles were also required.\n\nThe law of conservation of vis viva was championed by the father and son duo, Johann and Daniel Bernoulli. The former enunciated the principle of virtual work as used in statics in its full generality in 1715, while the latter based his Hydrodynamica, published in 1738, on this single conservation principle. Daniel's study of loss of vis viva of flowing water led him to formulate the Bernoulli's principle, which relates the loss to be proportional to the change in hydrodynamic pressure. Daniel also formulated the notion of work and efficiency for hydraulic machines; and he gave a kinetic theory of gases, and linked the kinetic energy of gas molecules with the temperature of the gas.\n\nThis focus on the vis viva by the continental physicists eventually led to the discovery of stationarity principles governing mechanics, such as the D'Alembert's principle, Lagrangian,\nand Hamiltonian formulations of mechanics.\n\nÉmilie du Châtelet (1706 – 1749) proposed and tested the hypothesis of the conservation of total energy, as distinct from momentum. Inspired by the theories of Gottfried Leibniz, she repeated and publicized an experiment originally devised by Willem 's Gravesande in 1722 in which balls were dropped from different heights into a sheet of soft clay. Each ball's kinetic energy - as indicated by the quantity of material displaced - was shown to be proportional to the square of the velocity. The deformation of the clay was found to be directly proportional to the height the balls were dropped from, equal to the initial potential energy. Earlier workers, including Newton and Voltaire, had all believed that \"energy\" (so far as they understood the concept at all) was not distinct from momentum and therefore proportional to velocity. According to this understanding, the deformation of the clay should have been proportional to the square root of the height from which the balls were dropped from. In classical physics the correct formula is formula_3, where formula_4 is the kinetic energy of an object, formula_5 its mass and formula_6 its speed. On this basis, Châtelet proposed that energy must always have the same dimensions in any form, which is necessary to be able to relate it in different forms (kinetic, potential, heat…).\n\nEngineers such as John Smeaton, Peter Ewart, , Gustave-Adolphe Hirn and Marc Seguin recognized that conservation of momentum alone was not adequate for practical calculation and made use of Leibniz's principle. The principle was also championed by some chemists such as William Hyde Wollaston. Academics such as John Playfair were quick to point out that kinetic energy is clearly not conserved. This is obvious to a modern analysis based on the second law of thermodynamics, but in the 18th and 19th centuries the fate of the lost energy was still unknown.\n\nGradually it came to be suspected that the heat inevitably generated by motion under friction was another form of \"vis viva\". In 1783, Antoine Lavoisier and Pierre-Simon Laplace reviewed the two competing theories of \"vis viva\" and caloric theory. Count Rumford's 1798 observations of heat generation during the boring of cannons added more weight to the view that mechanical motion could be converted into heat, and (as importantly) that the conversion was quantitative and could be predicted (allowing for a universal conversion constant between kinetic energy and heat). \"Vis viva\" then started to be known as \"energy\", after the term was first used in that sense by Thomas Young in 1807.\n\nThe recalibration of \"vis viva\" to\n\nwhich can be understood as converting kinetic energy to work, was largely the result of Gaspard-Gustave Coriolis and Jean-Victor Poncelet over the period 1819–1839. The former called the quantity \"quantité de travail\" (quantity of work) and the latter, \"travail mécanique\" (mechanical work), and both championed its use in engineering calculation.\n\nIn a paper \"Über die Natur der Wärme\"(German \"On the Nature of Heat/Warmth\"), published in the \"Zeitschrift für Physik\" in 1837, Karl Friedrich Mohr gave one of the earliest general statements of the doctrine of the conservation of energy in the words: \"besides the 54 known chemical elements there is in the physical world one agent only, and this is called \"Kraft\" [energy or work]. It may appear, according to circumstances, as motion, chemical affinity, cohesion, electricity, light and magnetism; and from any one of these forms it can be transformed into any of the others.\"\n\nA key stage in the development of the modern conservation principle was the demonstration of the \"mechanical equivalent of heat\". The caloric theory maintained that heat could neither be created nor destroyed, whereas conservation of energy entails the contrary principle that heat and mechanical work are interchangeable.\n\nIn the middle of the eighteenth century, Mikhail Lomonosov, a Russian scientist, postulated his corpusculo-kinetic theory of heat, which rejected the idea of a caloric. Through the results of empirical studies, Lomonosov came to the conclusion that heat was not transferred through the particles of the caloric fluid.\n\nIn 1798, Count Rumford (Benjamin Thompson) performed measurements of the frictional heat generated in boring cannons, and developed the idea that heat is a form of kinetic energy; his measurements refuted caloric theory, but were imprecise enough to leave room for doubt.\n\nThe mechanical equivalence principle was first stated in its modern form by the German surgeon Julius Robert von Mayer in 1842. Mayer reached his conclusion on a voyage to the Dutch East Indies, where he found that his patients' blood was a deeper red because they were consuming less oxygen, and therefore less energy, to maintain their body temperature in the hotter climate. He discovered that heat and mechanical work were both forms of energy and in 1845, after improving his knowledge of physics, he published a monograph that stated a quantitative relationship between them.\n\nMeanwhile, in 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. In the most famous, now called the \"Joule apparatus\", a descending weight attached to a string caused a paddle immersed in water to rotate. He showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.\n\nOver the period 1840–1843, similar work was carried out by engineer Ludwig A. Colding, although it was little known outside his native Denmark.\n\nBoth Joule's and Mayer's work suffered from resistance and neglect but it was Joule's that eventually drew the wider recognition.\n\nIn 1844, William Robert Grove postulated a relationship between mechanics, heat, light, electricity and magnetism by treating them all as manifestations of a single \"force\" (\"energy\" in modern terms). In 1846, Grove published his theories in his book \"The Correlation of Physical Forces\". In 1847, drawing on the earlier work of Joule, Sadi Carnot and Émile Clapeyron, Hermann von Helmholtz arrived at conclusions similar to Grove's and published his theories in his book \"Über die Erhaltung der Kraft\" (\"On the Conservation of Force\", 1847). The general modern acceptance of the principle stems from this publication.\n\nIn 1850, William Rankine first used the phrase \"the law of the conservation of energy\" for the principle.\n\nIn 1877, Peter Guthrie Tait claimed that the principle originated with Sir Isaac Newton, based on a creative reading of propositions 40 and 41 of the \"Philosophiae Naturalis Principia Mathematica\". This is now regarded as an example of Whig history.\n\nMatter is composed of such things as atoms, electrons, neutrons, and protons. It has \"intrinsic\" or \"rest\" mass. In the limited range of recognized experience of the nineteenth century it was found that such rest mass is conserved. Einstein's 1905 theory of special relativity showed that it corresponds to an equivalent amount of \"rest energy\". This means that it can be converted to or from equivalent amounts of \"other\" (non-material) forms of energy, for example kinetic energy, potential energy, and electromagnetic radiant energy. When this happens, as recognized in twentieth century experience, rest mass is not conserved, unlike the \"total\" mass or \"total\" energy. All forms of energy contribute to the total mass and total energy.\n\nFor example, an electron and a positron each have rest mass. They can perish together, converting their combined rest energy into photons having electromagnetic radiant energy, but no rest mass. If this occurs within an isolated system that does not release the photons or their energy into the external surroundings, then neither the total \"mass\" nor the total \"energy\" of the system will change. The produced electromagnetic radiant energy contributes just as much to the inertia (and to any weight) of the system as did the rest mass of the electron and positron before their demise. Likewise, non-material forms of energy can perish into matter, which has rest mass.\n\nThus, conservation of energy (\"total\", including material or \"rest\" energy), and conservation of mass (\"total\", not just \"rest\"), each still holds as an (equivalent) law. In the 18th century these had appeared as two seemingly-distinct laws.\n\nThe discovery in 1911 that electrons emitted in beta decay have a continuous rather than a discrete spectrum appeared to contradict conservation of energy, under the then-current assumption that beta decay is the simple emission of an electron from a nucleus. This problem was eventually resolved in 1933 by Enrico Fermi who proposed the correct description of beta-decay as the emission of both an electron and an antineutrino, which carries away the apparently missing energy.\n\nFor a closed thermodynamic system, the first law of thermodynamics may be stated as:\n\nwhere formula_10 is the quantity of energy added to the system by a heating process, formula_11 is the quantity of energy lost by the system due to work done by the system on its surroundings and formula_12 is the change in the internal energy of the system.\n\nThe l's before the heat and work terms are used to indicate that they describe an increment of energy which is to be interpreted somewhat differently than the formula_12 increment of internal energy (see Inexact differential). Work and heat refer to kinds of process which add or subtract energy to or from a system, while the internal energy formula_14 is a property of a particular state of the system when it is in unchanging thermodynamic equilibrium. Thus the term \"heat energy\" for formula_10 means \"that amount of energy added as the result of heating\" rather than referring to a particular form of energy. Likewise, the term \"work energy\" for formula_11 means \"that amount of energy lost as the result of work\". Thus one can state the amount of internal energy possessed by a thermodynamic system that one knows is presently in a given state, but one cannot tell, just from knowledge of the given present state, how much energy has in the past flowed into or out of the system as a result of its being heated or cooled, nor as the result of work being performed on or by the system.\n\nEntropy is a function of the state of a system which tells of limitations of the possibility of conversion of heat into work.\n\nFor a simple compressible system, the work performed by the system may be written:\n\nwhere formula_18 is the pressure and formula_19 is a small change in the volume of the system, each of which are system variables. In the fictive case in which the process is idealized and infinitely slow, so as to be called \"quasi-static\", and regarded as reversible, the heat being transferred from a source with temperature infinitesimally above the system temperature, then the heat energy may be written\n\nwhere formula_21 is the temperature and formula_22 is a small change in the entropy of the system. Temperature and entropy are variables of state of a system.\n\nIf an open system (in which mass may be exchanged with the environment) has several walls such that the mass transfer is through rigid walls separate from the heat and work transfers, then the first law may be written:\n\nwhere formula_24 is the added mass and formula_25 is the internal energy per unit mass of the added mass, measured in the surroundings before the process.\n\nThe conservation of energy is a common feature in many physical theories. From a mathematical point of view it is understood as a consequence of Noether's theorem, developed by Emmy Noether in 1915 and first published in 1918. The theorem states every continuous symmetry of a physical theory has an associated conserved quantity; if the theory's symmetry is time invariance then the conserved quantity is called \"energy\". The energy conservation law is a consequence of the shift symmetry of time; energy conservation is implied by the empirical fact that the laws of physics do not change with time itself. Philosophically this can be stated as \"nothing depends on time per se\".\nIn other words, if the physical system is invariant under the continuous symmetry of time translation then its energy (which is canonical conjugate quantity to time) is conserved. Conversely, systems which are not invariant under shifts in time (an example, systems with time dependent potential energy) do not exhibit conservation of energy – unless we consider them to exchange energy with another, external system so that the theory of the enlarged system becomes time invariant again. Conservation of energy for finite systems is valid in such physical theories as special relativity and quantum theory (including QED) in the flat space-time.\n\nWith the discovery of special relativity by Henri Poincaré and Albert Einstein, energy was proposed to be one component of an energy-momentum 4-vector. Each of the four components (one of energy and three of momentum) of this vector is separately conserved across time, in any closed system, as seen from any given inertial reference frame. Also conserved is the vector length (Minkowski norm), which is the rest mass for single particles, and the invariant mass for systems of particles (where momenta and energy are separately summed before the length is calculated—see the article on invariant mass).\n\nThe relativistic energy of a single massive particle contains a term related to its rest mass in addition to its kinetic energy of motion. In the limit of zero kinetic energy (or equivalently in the rest frame) of a massive particle, or else in the center of momentum frame for objects or systems which retain kinetic energy, the total energy of particle or object (including internal kinetic energy in systems) is related to its rest mass or its invariant mass via the famous equation formula_26.\n\nThus, the rule of \"conservation of energy\" over time in special relativity continues to hold, so long as the reference frame of the observer is unchanged. This applies to the total energy of systems, although different observers disagree as to the energy value. Also conserved, and invariant to all observers, is the invariant mass, which is the minimal system mass and energy that can be seen by any observer, and which is defined by the energy–momentum relation.\n\nIn general relativity, energy–momentum conservation is not well-defined except in certain special cases. Energy-momentum is typically expressed with the aid of a stress–energy–momentum pseudotensor. However, since pseudotensors are not tensors, they do not transform cleanly between reference frames. If the metric under consideration is static (that is, does not change with time) or asymptotically flat (that is, at an infinite distance away spacetime looks empty), then energy conservation holds without major pitfalls. In practice, some metrics such as the Friedmann–Lemaître–Robertson–Walker metric do not satisfy these constraints and energy conservation is not well defined. The theory of general relativity leaves open the question of whether there is a conservation of energy for the entire universe.\n\nIn quantum mechanics, energy of a quantum system is described by a self-adjoint (or Hermitian) operator called the Hamiltonian, which acts on the Hilbert space (or a space of wave functions) of the system. If the Hamiltonian is a time-independent operator, emergence probability of the measurement result does not change in time over the evolution of the system. Thus the expectation value of energy is also time independent. The local energy conservation in quantum field theory is ensured by the quantum Noether's theorem for energy-momentum tensor operator. Note that due to the lack of the (universal) time operator in quantum theory, the uncertainty relations for time and energy are not fundamental in contrast to the position-momentum uncertainty principle, and merely holds in specific cases (see Uncertainty principle). Energy at each fixed time can in principle be exactly measured without any trade-off in precision forced by the time-energy uncertainty relations. Thus the conservation of energy in time is a well defined concept even in quantum mechanics.\n\n\n\n\n"}
{"id": "482678", "url": "https://en.wikipedia.org/wiki?curid=482678", "title": "Dance notation", "text": "Dance notation\n\nDance notation is the symbolic representation of human dance movement and form, using methods such as graphic symbols and figures, path mapping, numerical systems, and letter and word notations. Several dance notation systems have been invented, many of which are designed to document specific types of dance. A \"dance score\" is recorded dance notation that describes a particular dance.\n\nThe primary uses of dance notation are historical dance preservation through documentation, and analysis (e.g., in ethnochoreology) or reconstruction of choreography, dance forms, and technical exercises. Also, dance notation allows a dance work to be documented and therefore potentially copyrighted. \n\nTwo popular dance notation systems used in Western culture are Labanotation (also known as Kinetography Laban) and Benesh Movement Notation. Others include Eshkol-Wachman Movement Notation and DanceWriting.\n\nMany dance notation systems are designed for a specific type of dance. Examples of such systems include Shorthand Dance Notation for dances from Israel, Morris Dance Notation for Morris dance, and Beauchamp-Feuillet notation for Baroque dance. As a result, these systems usually cannot effectively describe other types of dance.\n\nIn the 1680s, Pierre Beauchamp invented a dance notation system for Baroque dance. His system, known as \"Beauchamp-Feuillet notation\", was published in 1700 by Raoul-Auger Feuillet and used to record dances throughout the eighteenth century.\n\nA well-known collection of dance scores is the Sergeyev Collection, recorded using Vladimir Ivanovich Stepanov's notation method (1892). This collection documents the Imperial Ballet's (today the Kirov/Mariinsky Ballet) repertoire from the turn of the 20th century, including Marius Petipa's original choreographic designs for \"The Sleeping Beauty\", \"Giselle\", \"Le Corsaire\", and \"Swan Lake\", as well as \"Coppélia\" and the original version of \"The Nutcracker\". It was with this collection that many of these works were first staged outside Russia.\n\nIn 1948, Hanya Holm became the first Broadway choreographer to have her dance scores copyrighted, for her work on \"Kiss Me Kate\".\n\nIn 1951, Stanley D. Kahn published Kahnotation, a dance notation system specific to tap dance.\n\nIn 1975, Ann Hutchinson Guest reconstructed choreographer Arthur Saint-Léon's \"Pas de Six\" from his 1844 ballet \"La Vivandière\", along with its original music by composer Cesare Pugni, for the Joffrey Ballet. The piece was reconstructed from Saint-Léon's work, which was documented using his own method of dance notation, known as \"La Sténochorégraphie\".\n\nIn 1982, the first computerized notation system—the \"DOM\" dance notation system—was created by Eddie Dombrower for the Apple II personal computer. The system displayed an animated figure on the screen that performed dance moves specified by the choreographer.\n\n\n"}
{"id": "2067746", "url": "https://en.wikipedia.org/wiki?curid=2067746", "title": "Dhāraṇī", "text": "Dhāraṇī\n\nA dharani (Devanagari: धारणी, IAST: ) is a Buddhist chant, mnemonic code, incantation, or recitation, usually a mantra consisting of Sanskrit or Pali phrases. Believed to be protective and with powers to generate merit for the Buddhist devotee, they constitute a major part of historic Buddhist literature. These chants have roots in Vedic Sanskrit literature, and many are written in Sanskrit scripts such as the Siddham as well as transliterated into Chinese, Korean, Japanese and other regional scripts.\n\nDharani are found in the ancient texts of all major traditions of Buddhism. They are a major part of the Pali canon preserved by the Theravada tradition. Mahayana sutras – such as the Lotus Sutra and the Heart Sutra – include or conclude with dharani. Some Buddhist texts, such as \"Pancaraksa\" found in the homes of many Buddhist tantra tradition followers, are entirely dedicated to dharani. They are a part of their ritual prayers as well as considered to be an amulet and charm in themselves, whose recitation believed to allay bad luck, diseases or other calamity. They were an essential part of the monastic training in Buddhism's history in East Asia. In some Buddhist regions, they served as texts upon which the Buddhist witness would swear to tell the truth. \n\nThe dharani-genre of literature became popular in East Asia in the 1st-millennium CE, with Chinese records suggesting their profusion by the early centuries of the common era. These migrated from China to Korea and Japan. The demand for printed dharani among the Buddhist lay devotees may have led to the development of textual printing innovations. The dharani records of East Asia are the oldest known \"authenticated printed texts in the world\", state Robert Sewell and other scholars. The early 8th-century dharani texts discovered in the Pulguksa temple of Gyeongju, Korea are considered as the oldest known printed texts in the world.\n\nDharani recitation for the purposes of healing and protection is referred to as Paritta in some Buddhist regions, particularly in Theravada communities. The dharani-genre ideas also inspired the Japanese Koshiki texts, and chanting practices called \"Daimoku\", \"Nenbutsu\" (Japan), \"Nianfo\" (China) or \"Yombul\" (Korea). They are a significant part of the historic Chinese \"dazangjing\" (scriptures of the great repository) and the Korean \"daejanggyeong\" – the East Asian compilations of the Buddhist canon between the 5th- and 10th-centuries.\n\nThe word \"dhāraṇī\" derives from a Sanskrit root √\"dhṛ\" meaning \"to hold or maintain\". This root is likely derived from the Vedic religion of ancient India where chants and melodious sounds were believed to have innate spiritual and healing powers even if the sound cannot be translated and has no meaning (as in a music). The same root gives \"dharma\" or \"dhamma\". According to the East Asian Buddhism studies scholar Paul Copp, some Buddhist communities outside India sometimes refer to \"dharanis\" with alternate terms such as \"mantra, hṛdaya (hridiya), paritrana (paritta), raksha (Pali: rakkha), gutti, or vidyā\" though these terms also have other contextual meanings in Buddhism.\n\nAccording to the traditional belief in Tibetan texts, states Jose Cabezon – the Dalai Lama professor of Tibetan Buddhism studies, there were three councils and the term \"dharani\" was recorded and became the norm after the third council. The first council, according to this belief, compiled the \"Sutranta\", the \"Vinaya\" and the \"Abhidhamma\" in Vimalabhada to the south of Rajagriha in India. The first council was held in the year Buddha died, but the compiled dhamma consisted of spoken words that were not written down. The second council occurred about 200 years after the death of the Buddha in a grove provided by Ashoka, where the knowledge was compiled again, but it too did not write anything down. The third council gathered in Kashmir a century later, according to the Tibetan tradition, and the teachings were put down in writing for those \"who had not obtained the power (\"dharani\") of not-forgetting\" because people were reciting corrupted forms of the teachings of the Buddha. In this context, \"dharani\" were acknowledged in the Buddhist tradition by about 2nd-century BCE, and they were a memory aid to ground and remember the \"dhamma\" teachings.\n\nThe term dharani as used in the history of Mahayana and tantric Buddhism, and its interpretation has been problematic since the mid-19th-century, states Ronald Davidson. It was initially understood as \"magical formula or phrase\", but later studies such as by Lamotte and Berhard interpreted them to be \"memory\", while Davidson proposes that some dharani are \"codes\". Eugène Burnouf, the 19th-century French Indologist and a scholar of Buddhism, dharanis are magical formulas that to Buddhist devotees are the most important parts of their books. Burnouf, states Davidson, was the first scholar to realise how important and widespread dharani had been in Buddhism sutras and Mahayana texts. The Indologist Moriz Winternitz concurred in early 20th-century that dharanis constituted a \"large and important\" part of Mahayana Buddhism, and that they were magic formulae and \"protective spells\" as well as amulets. \nAccording to Winternitz, a dharani resembles the incantations found in the \"Atharvaveda\" and \"Yajurveda\" of Hinduism. The dharani-genre of Buddhist literature includes mantra, states Étienne Lamotte, but they were also a \"memory aid\" to memorize and chant Buddha's teachings. This practice was linked to concentration (\"samadhi\") and believed to have magical virtues and a means to both spiritual and material \"karma\"-related merit making. According to Braarvig, the dharanis are \"seemingly meaningless strings of syllables\". While they may once have been \"memory aids\", the dharanis that have survived into the modern era do not match with any text. In later practice, the dharanis were \"hardly employed as summaries of doctrine, but were employed as aids to concentration and magical protection benefits\".\n\nAccording to Jan Nattier, Vedic mantras are more ancient than Buddhist dharani, but over time they both were forms of incantations that are quite similar. In the early texts of Buddhism, proposes Nattier, \"it would appear that the word dharani was first employed in reference to mnemonic devices used to retain (Skt. \"hold\") certain elements of Buddhist doctrine in one's memory\". In Nattier's view, the term dharani is \"peculiar to Buddhism\". A dhāraṇī can be a mnemonic to encapsulate the meaning of a section or chapter of a sutra. According to the Buddhism-related writer Red Pine, \"mantra\" and \"dharani\" were originally interchangeable, but at some point \"dhāraṇī\" came to be used for meaningful, intelligible phrases, and mantra for syllabic formulae which are not meant to be understood. \n\nAccording to Robert Buswell and Ronald Davidson, \"dharani\" were codes in some Buddhist texts. They appeared at the end of the text, and they may be seen as a coded, distilled summary of Buddhist teachings in the chapters that preceded it. For example, the \"Vajrasamadhi-sutra\" – a Korean Buddhist text likely composed in the 7th-century by an unknown monk, one important to the Ch'an (Zen Buddhism) tradition in East Asia, the \"Dharani\" chapter is the eighth (second last), with a brief conversational epilogue between the Tathagata Buddha and Ananda being the last chapter. This \"dharani\" chapter, states Buswell, \"encodes (\"dharayati\") the important meanings, without forgetting them, and it reminds and codes the points to remember.\n\nThe Indologist Frits Staal who is known for his scholarship on mantras and chants in Indian religions, states the Dharani mantras reflect a continuity of the Vedic mantras. He quotes Wayman to be similarly stressing the view that the Buddhist chants have a \"profound debt to the Vedic religion\". The Yogacara scholars, states Staal, followed the same classification as one found in the Vedas – \"arthadharani\", \"dharmadharani\" and \"mantradharani\", along with express acknowledgment like the Vedas that some \"dharani are meaningful and others are meaningless\" yet all effective for ritual purposes.\n\nThe early Buddhism literature includes the dharani spells and incantations. It demonstrates that dharanis were valued and in use within Buddhist communities before the 1st-century CE, state Charles Prebish and Damien Keown.\n\nThe role of dharanis in Buddhist practice of mid-1st-millennium CE is illustrated by numerous texts including the systematic treatises that emerged. According to Paul Copp, one of the earliest attestable literary mandate about writing dharanis as an effective spell in itself is found in a Chinese text dated between 317 and 420 CE. This text is the \"Qifo bapusa suoshuo da tuoluoni shenzhou jing\" (or, Great Dharani Spirit-Spell Scripture Spoken by the Seven Buddhas and Eight Bodhisattvas). \"The Collected Dhāraṇī Sūtras\", for example, were compiled in the mid-seventh century. Some of the oldest Buddhist religious inscriptions in Stupas (Dagoba, Chörten) are extracts from dharani-genre compositions such as the \"Bodhigarbhalankaralaksa-dharani\". Manuscript fragments of \"Sumukha-dharani\" discovered in Central Asia and now held at the Leningrad Branch of Russian Academy of Sciences are in the Sanskrit language and the Brahmi script, a script that was prevalent before the early centuries of the common era.\n\nThe Chinese text \"Wugou jing guangda tuoluoni jing\" of the influential Empress Wu's era – 683 to 705 CE – is about the Buddha reciting six \"dharanis\". The first part states its significance as follows (Japanese version of the Chinese text):\nEarly mentions of dharani in the European literature are from the records left by John of Plano Carpini (1245–7) and William of Rubruck (1254) where they wrote in their respective memoirs that Uighurs and Mongols chanted \"Om man baccam\", later identified with \"Om mani padme hum\". They also mention that these Asians write \"short sorcery sentences on paper and hang them up\". Other than such scant remarks, little was known about the Dharani-genre of literature or its value in Buddhism till the mid-19th-century colonial era, when Brian Hodgson began buying Sanskrit and related manuscripts in Nepal, Tibet and India for a more thorough scholarship, often at his personal expense. According to Hodgson, as quoted by Ronald Davidson, dharani were esoteric short prayers \"derived from [Buddhist tantric] Upadesa\" that are believed to be amulet to be constantly repeated or worn inside little lockets, something that leads to \"a charmed life\".\n\nThe colonial era scholarship initially proposed that the dharanis and related rituals may have been an influence on Buddhism of other Indian religions such as from the esoteric tantra traditions of Hinduism around the mid-1st-millennium CE. This assumption, along with the view that early Buddhism was an \"abstract philosophy or even a broad-based social movement\" is now a part of a scholarly debate. With increased access to the primary texts of Buddhism and the discoveries of historical manuscripts in China, Korea and Japan, such as those about early Silla Buddhism, McBride and others state that dharani incantations and ritualism had widespread significance in East Asia from the early years. Coupled with Waddell's scholarship on the \"dharani cult in Buddhism\" in early 20th-century, the post-colonial era scholarship proposed that dharanis did not develop with or after tantric Buddhism emerged, but preceded it and were a form of proto-tantrism. \n\nAccording to Richard McBride, as well as Richard Payne, the \"proto-tantra\" proposal too is problematic because it is a meaningless anachronistic teleological category that \"misleads\" and implies that the dharanis somehow anticipated and nurtured Buddhist tantra tradition. There is no evidence for such a sequential development. Instead, the evidence points to an overlap but that the significance of the dharanis in mainstream Buddhist traditions and the esoteric Buddhist tantra tradition co-existed independent of each other. Phonic mysticism and musical chanting based on dharanis – \"parittas\" or \"raksas\" in the Theravada Pali literature – along with related mantras were important in early Buddhism. They continue to be an essential part of actual Buddhist practice in Asia, both for its laypersons and the monks. The emerging evidence and later scholarship increasingly states that \"dharani and ritual procedures were mainstream Mahayana practices\" many centuries before the emergence of tantric and esoteric Buddhism and Vajrayana, states McBride. The Buddhist tantra traditions added another layer of sophistication and complexity to the rituals with deities and mandalas.\n\nDharanis are not limited to an esoteric cult within Buddhism, states Paul Copp, rather the \"dharani incantations and related mystic phrases and practices have been integral parts of nearly all Buddhist traditions since at least the early centuries of the common era\".\n\nDhāraṇīs are a form of amulet and believed in the various Buddhist traditions to deliver protection from malign influences and calamities. Mantra and dharani are synonymous in some Buddhist traditions, but in others such as the Tibetan tantric traditions a dharani is a type of mantra. According to Jose Cabezon, in the tantric traditions, mantra (\"sngags\") is all knowledge and the mind of all the Buddhas, that which possesses the \"dharma-dhatu\" (essence of dhamma). The mantra exist in three forms – \"guhya\" (secret), \"vidya\" (knowledge) \"dharani\" (memory aid). The \"guhya\" mantra are about male deity and female deity relationships and union. The \"vidya\" mantra represent the mind of male Buddhist deities, while \"dharani\" mantras of the female Buddhist deities. Theologically, the \"vidya\" mantras constitute that knowledge in tantric Buddhism, according to Cabezon, which \"pacifies the suffering experienced in the existential world (\"samsara\") and the heaps of faults such as desire\". The \"dharani\" mantras, in contrast, constitute that knowledge in tantric Buddhism which \"causes one to hold onto the \"dhamma\", to remember the \"dhamma\", to remember virtue\". There is very little prescriptive or practical difference between dharani and mantras except that dharani are much longer, states Eugene Burnouf.\n\nAccording to Winternitz, a Buddhist dharani resembles the incantations and mantras found in Hinduism. A dharani may contain simple magical syllables and words without any literal meaning (\"mantra-padani\"), or its power is believed to result from it containing words or wisdom \"in nunce\" from a Buddhist Sutta. The Japanese Horiuzi manuscript of \"Prajna paramita hrdaya sutra\" and \"Usnisha Vijaya dharani\" dated to 609 CE illustrate both, with the latter being only invocations consisting of meaningless series of syllables. In Buddhism, a dharani has been believed to have magical virtues and a means to earn merit to offset the past \"karma\", allay fear, diseases and disasters in this life, and for a better rebirth. To the lay Buddhist communities, states Davidson, the material benefits encouraged the popularity and use of dharanis for devotionalism, rituals and rites in Buddhism. According to Janet Gyatso, there is a difference between mantras and dharanis. The mantras are more than melodious sounds and have meaning, and these were found sporadically in pre-Mahayana Buddhism. With the emergence of the Mahayana Buddhism tradition, the dharanis became closely related to mantras. Later, as the Vajrayana Buddhism tradition grew, they proliferated. The dharanis and mantras overlap because in the Vajrayana tradition. There exist \"single seed-syllable \"bija\" like dharanis, treated as having special powers to protect chanters from dangers such as \"snakes, enemies, demons and robbers\". The \"bija\" (seed) mantra condenses the protective powers of a Buddhist deity or a Buddhist text into a single syllable. For example, the single letter \"a\" (अ) condenses the 100,000 verses of the \"Prajna-paramita sutras\" into a single syllable.\nThe Japanese Buddhist monk Kūkai drew a distinction between \"dhāraṇī\" and \"mantra\" and used it as the basis of his theory of language. According to Kūkai, a Buddhist \"mantra\" is restricted to esoteric Buddhist practice whereas \"dhāraṇī\" is found in both esoteric and exoteric rituals. In the Nara and early Heian period of Japanese history, a monk or nun was tested for their fluency and knowledge of dharanis to confirm whether they are well trained and competent in Buddhist knowledge. Their appointment letters listed the sutras and dharanis that he or she could recite from memory. In an appointment recommendation letter dated 732 CE, as an example, a Japanese priest named Chishu supports the ordination of his student Hata no kimi Toyotari by listing that he can recite following dharanis: \"the \"Greater Prajna-paramita\", Amoghapasa Avalokiteshvara, Eleven-faced Avalokiteshvara, the \"Golden Light\", Akashagarbha, Bhaisajyaguru, consecrating water, concealing ritual space\" with the dharani rituals of prostration after eight years of training. A study of numerous such \"ubasoku koshinge\" recommendation letters from the 1st-millennium Japan confirm that dharanis were an essential and core part of monastic training, though the specific group of dharanis memorized by a monk or nun varied.\n\nKūkai classified mantras as a special class of dhāraṇīs and argued that every syllable of a dhāraṇī was a manifestation of the true nature of reality – in Buddhist terms, that all sound is a manifestation of \"śūnyatā\" or emptiness of self-nature. Thus, rather than being devoid of meaning, Kūkai suggests that dhāraṇīs are in fact saturated with meaning – every syllable is symbolic on multiple levels.\n\nThe dharanis have been a large and important part of Mahayana Buddhist literature. They are particularly abundant in the esoteric tradition of Buddhism (Vajrayana, Tibetan). However, the dharanis were not unique to esoteric Mahayana texts. The most significant and popular Mahayana sutras such as the \"Lotus sutra\", \"Heart sutra\" and others prominently include dharani chapters. The dharanis are prominent in the \"Prajna-paramita sutras\" wherein the Buddha \"praises dharani incantation, along with the cultivation of samadhi, as virtuous activity of a bodhisattva\", states Ryûichi Abé.\n\nThe \"Megha-Sutra\" is an example of an ancient Mahayana magico-religious text. In it, the snake deities appear before the Buddha and offer him adoration, then ask how the suffering of snakes, as well as people, can be alleviated. The text suggests friendliness (\"maitri\") and lists numerous invocations such as those to female deities, exorcisms, means to induce rains, along with a series of magical formulae such as \"sara sire sire suru suru naganam java java jivi jivi juvu juvu etc\", states Moriz Winternitz. The historic Mahayana dharanis have survived as single manuscripts as well as large collections. The versions found in Nepal and China include spells to end sickness, lengthen life, recovery from poison, magic for luck in war, drive away demons and snakes, protection from the effects of ill-omened constellations, release from a confessed sin, birth of a son or daughter to a woman wanting a baby, rebirth into \"sukhavati\" heaven or avoiding a bad rebirth. The snake-charm dharani is found in the Bower Manuscript found in Western China. While a 443 CE Chinese translation of Lankavatara Sutra does not contain some of the dharani chapters, other Chinese translations dated to the 2nd-century and 4th-century CE of Mahayana texts do contain dharanis. The Dunhuang manuscript collections include extensive talismanic dharani sections. The dharanis as conceptualized by medieval era Buddhist intellectuals and eminent Chinese monks were an \"integral component of mainstream Sinitic Buddhism\", states Richard McBride. The popularity of Buddhist spells in China was probably because older native Chinese religions valued spells already.\n\nAccording to Robert Buswell and Donald Lopez, it is \"almost certain\" that some of the East Asian Buddhist literature on dharani were indigenous Chinese texts and syncretic with the Daoist practices. For example, the \"Guanding jing\" composed in mid-5th century in China is largely a collection of magical spells in the dharani-genre in twelve semi-independent chapters. It includes spells such as those of the 72,000 spirit kings to protect Bhikshus (Buddhist monks), spells of the 120,000 spirit kings to protect the Bhikshunis (Buddhist nuns), incantations of spirit kings to protect one's surroundings, seals and spells to subdue devils, chants to summon dragon kings to treat infections and remove pests, and seeking rebirth in pure lands of one's desire, state Buswell and Lopez.\n\nThe significance of dharanis was such that both the government and monastic organization had stipulated, by the 7th-century, how and when dharanis may or may not be used. A \"ritsuryo\" code for Buddhist clerics dated 718 CE, promulgated by the Nara government in Japan, forbid the use of dharani for any unauthorized medical, military and political rebellion. The code explicitly exempted their use for \"healing of the sick by chanting dharanis in accordance with the Buddha dharma\". Another document dated 797 CE mentions \"healer-meditation masters\" (\"kanbyo zenji\") in dharanis to protect the family of the ruler. Others evidence the use of dharani chanting by monks and nuns as \"one of the common methods of healing during the Nara period\", states Ryûichi Abé.\n\nThe dharanis were an essential part of the \"rokujikyoho\" (six-syllable sutra) liturgy ritual in Japan, states Benedetta Lomi. The \"ritsuryo\" code They were greatly popular between the 11th- and 15th-centuries and a part of comprehensive solution to various ailments, a ritual performed by Buddhist monks and practitioners of \"onmyōdō\".\n\nThe Theravada tradition is found in Sri Lanka and Southeast Asian Buddhist countries. The three historic ritual practices of the Theravada community include the Buddha puja (\"triratna\" rite), the Five Precepts ceremony (\"pancasil\"), and the protective chanting of Paritrana (\"paritta\"). The Theravada Paritrana texts are equivalent to the Dharani texts in the Mahayana tradition, both providing protective charm through chanting of hymns. According to Buddhist studies scholars Sarah LeVine and David Gellner, Theravada lay devotees traditionally invite the monks into their homes for rites of \"protection from evil\" and the monk(s) chant the paritrana hymns. These rituals are particularly common during rites-of-passage ceremonies such as baby naming, first rice-eating and others. According to Buddhologist Karel Werner, some Mahayana and Vajrayana dharani texts influenced the paritta texts of Theravada tradition, such as the Gini (fire) Paritta, as the hymns are identical in parts and the Theravada text uses the same terms, for example, \"dharani dharaniti\".\n\nThe Pali canon makes many references to protective (\"raksha\", \"paritta\") incantations and magical spells. These invocations provide protection from \"malignant spirits, disease and calamity\". For example, in \"Digha Nikaya\" (DN I.116.14), Sonadanda remarks that wherever the Buddha stays, \"non-humans do not harm the people of that town or village\", states the Buddhism scholar Peter Skilling. This and similar statements are also found in the early Chinese translations of Indian Buddhist texts. According to Skilling, these \"protective Buddhist literature\" are used by both the monks and the laypeople of Theravada countries. These texts are a part of any \"meagre library of Buddhist Sri Lankan households\" and they are called \"Pirit Pota\". In Myanmar, all classes of the Theravada community more widely know about the \"paritta\" incantation literature than any other Pali Buddhist work. The average Theravada monk in other southeast Asian countries who may not know much about a \"Tipitaka\", states Skilling, is likely to \"be able to recite numerous chants [paritta, dharani] from memory\".\n\nIn northern Thailand, the \"Suat Boek Phranet\" (lit. Eye-Opening Sutta) is a Pali chant text used during rites such as the consecration of a Buddha image. The text, states Donald Swearer, includes a \"unique dharani in praise of the Buddha\" and his victory over the evil Mara. Though the dharani appears at the end of the text and the associated chant in Thai Buddhist practice occurs at the close of the ceremony, they highlight their key role in \"the \"buddhabhiseka\" ritual\".\n\nThe Buddhist dharani invocations are the earliest mass printed texts that have survived. Till the mid-20th-century, the \"Hyakumantō Darani\" found as charms in wooden pagodas of Japan were broadly accepted as having been printed between 764 and 770 CE, and the oldest extant printed texts. In 1966, similarly printed dharani were discovered in stone pagoda of Pulguksa temple in Gyeongju, Korea. These are dated to the first half of the 8th-century, and now considered as the oldest known printed texts in the world. According to Tsien Tsuen-Hsuin, the Korean dharani scrolls were printed after the era of Empress Wu in China, and these date \"no earlier than 704 CE, when the translation of the sutra was finished, and no later than 751, when the building of the temple and stupa was completed\". The printed Korean text consists of \"Chinese characters transliterated from the [Indian] Sanskrit\". While the Korean dharani were likely printed in China, the evidence confirms that the Japanese dharani were printed in Japan from Buddhist chants that arrived through China. The tradition of printing and distributing the Buddhist dharanis, as well as transliterated Sanskrit sutras, continued in East Asia over the centuries that followed. By the 9th-century, the era of mass printing and the sale of books had begun covering additional subjects such as \"astrology, divination of dreams, alchemy, and geomancy\".\n\nAccording to languages and ancient manuscripts scholar Ernst Wolff, \"it was Buddhism, above all, that eminently stimulated and sustained printing activities\". Its chants and ideas were in demand in East Asia, and this led to the development of wood-block based mass printing technology. The oldest known dharanis were mass produced by the 8th-century, and later in the 10th-century the canonical \"Tripitaka\" in addition to 84,000 copies of dharanis were mass printed. \n\nThe 8th-century dharanis are the \"oldest authenticated printed texts in the world\", states Robert Sewell. These were mass produced as a set consisting of miniature hollow wooden pagodas each containing a printed dharani prayer or charm in Sanskrit on thick paper strips. The Japanese records state a million dharanis were so produced and distributed through Buddhist temples by the order of Empress Shotoku – previously a Buddhist nun – after an attempted coup against her court. According to Ross Bender, these events and Empress Shotoku's initiatives led to the founding of major new Buddhist temples, a \"great acceleration\" and the \"active propagation of Buddhism\" in Japan. Empress Shotoku's million dharanis are among the oldest known printed literature in the world.\n\nWhile dharanis are found inside major texts of Buddhism, some texts are predominantly or exclusively of the dharani-genre. Some illustrations include,\n\nThe Theravada compilations of paritta (dharani) are ancient and extensive. Some are a part of various \"suttas\", while others are dedicated texts. Illustrations include:\n\n\n\n"}
{"id": "939694", "url": "https://en.wikipedia.org/wiki?curid=939694", "title": "Double-elimination tournament", "text": "Double-elimination tournament\n\nA double-elimination tournament is a type of elimination tournament competition in which a participant ceases to be eligible to win the tournament's championship upon having lost \"two\" games or matches. It stands in contrast to a single-elimination tournament, in which only \"one\" defeat results in elimination.\n\nOne method of arranging a double-elimination tournament is to break the competitors into two sets of brackets, the \"winners bracket\" and \"losers bracket\" (\"W\" and \"L\" brackets for short; also sometimes \"upper bracket\" and \"lower bracket\", respectively) after the first round. The first-round winners proceed into the W bracket and the losers proceed into the L bracket. The W bracket is conducted in the same manner as a single-elimination tournament, except that the losers of each round \"drop down\" into the L bracket. Another method of double-elimination tournament management is the \"Draw and Process\".\n\nAs with single-elimination tournaments, most often the number of competitors is equal to a power of two (8, 16, 32, etc.) so that in each round there is an even number of competitors and never any byes. The maximum number of games in a double-elimination tournament is one less than twice the number of teams participating (e.g., 8 teams – 15 games). The minimum number is two less than twice the number of teams (e.g., 8 teams – 14 games).\n\nIf the standard double-elimination bracket arrangement is being used, then each round of the L Bracket is conducted in two stages; a minor stage followed by a major stage. Both contain the same number of matches (assuming there are no byes) which is the same again as the number of matches in the corresponding round of the W Bracket. If the minor stage of an L Bracket round contains \"N\" matches, it will produce \"N\" winners. Meanwhile, the \"N\" matches in the corresponding round of the W Bracket will produce \"N\" losers. These 2\"N\" competitors will then pair off in the \"N\" matches of the corresponding major stage of the L Bracket.\n\nFor example, in an eight-competitor double-elimination tournament, the four losers of the first round, W Bracket quarter finals, pair off in the first stage of the L Bracket, the L Bracket minor semifinals. The two losers are eliminated, while the two winners proceed to the L Bracket major semifinals. Here, those two players/teams will each compete against a loser of the W Bracket semifinal in the L Bracket major semifinals. The winners of the L Bracket major semifinals compete against each other in the L Bracket minor-final, with the winner playing the loser of the W Bracket final in the L Bracket major final.\n\nThe championship finals of a double elimination tournament is usually set up to be a possible two games. The rationale is that since the tournament is indeed double elimination, it is unfair to have the W Bracket champion eliminated with its first loss. Therefore, while the W Bracket champion needs to beat the L Bracket champion only once to win the tournament, the L Bracket champion must beat the Winners' Bracket champion twice.\n\nA Draw and Process tournament requires less intervention by the manager. The competitors are allocated their first round positions on the competition grid and this is played as if it were a single elimination event. This grid is called the \"Draw\". A second competition grid called the \"Process\" is then produced and again played as a single elimination event. The fixed arrangement of the Process ensures that players who met in the first round of the Draw cannot meet until the final of the Process. Similarly, players who meet in the second round of the Draw cannot meet until the semi finals of the Process. If the same person wins both the Draw and Process then they are the overall winner and the losing finalists will play each other for second and third place. Otherwise the winners of the Draw and Process will play off to determine the winner.\n\nThe double-elimination format has some advantages over the single-elimination format, most notably the fact that third and fourth places can be determined without the use of a consolation or \"classification\" match involving two contestants who have already been eliminated from winning the championship.\n\nSome tournaments, such as in tennis, will use \"seeding\" to prevent the strongest contestants from meeting until the later round. However, in tournaments where contestants are placed randomly in the draw, or in situations where seeding is not available, it is possible for 2 of the strongest teams to meet in the early rounds rather than a final or semifinal as would be expected in a seeded draw. Double elimination overcomes this shortfall by allowing a strong team which loses early to work their way through the L Bracket and progress to the later rounds, despite meeting the strongest team in the early rounds of competition.\n\nAnother advantage of the double-elimination format is the fact that all competitors will play at least twice and three quarters will play three games or more. In a single-elimination tournament with no byes, half of the competitors will be eliminated after their first game. This can be disappointing to those who had to travel to the tournament and were only able to play once.\n\nA disadvantage compared to the single-elimination format is that at least twice the number of matches have to be conducted. Since each player has to lose twice and since the tournament ends when only one player remains, in a tournament for \"n\" competitors there will be either 2\"n\" − 2 or 2\"n\" − 1 games depending on whether or not the winner was undefeated during the tournament. This may result in a scheduling hardship for venues where only one facility for play is available.\n\nIt is possible for the Championship finals to be determined by just a single match if the W Bracket winner defeats the L Bracket winner. It is therefore unknown, until this match has been concluded, whether the final scheduled match will in fact be required. This can be seen as a disadvantage of the system, particularly if broadcasting and ticket sales companies have an interest in the tournament.\n\nOne such athletic event that employs a double-elimination format is the NCAA baseball tournament, including the College World Series, where a team is not eliminated until it loses twice in each of the four rounds (regional, super regional, College World Series, and CWS championship, with the super regional and CWS championship series featuring two teams in a best-of-3 format). The NCAA softball tournament (including the Women's College World Series) uses the same format. The Little League World Series switched from round-robin to double-elimination formats for each of its pools starting in 2010 in an effort to eliminate meaningless games.\n\nIt is also used in video game tournaments and table football tournaments. Double-elimination brackets are also popular in amateur wrestling of all levels, pool, surfing, windsurfing and kiteboarding freestyle competitions, as well as Curling bonspiels (where triple-elimination is also used), Hardcourt Bike Polo. The World Baseball Classic used a double-elimination format for its second rounds of the tournament in 2009 and 2013, as well as in its first round in 2009. In contract bridge, the English Bridge Union Spring Foursomes, first contested in 1962, uses a double elimination format.\n\nIt is also used, in modified form, in the All-Ireland Senior Gaelic Football Championship and All-Ireland Senior Hurling Championship.\n\nWorld Championship Wrestling was the only professional wrestling promotion to date to use the double-elimination format. They used the format for a tournament for the vacant WCW World Tag Team Championship in 1999.\n\nIn judo, players that end up in the L bracket can finish in third place at best. The winner of the W bracket will win the tournament, with the losing finalist finishing second. The other losers of the W bracket will end up in the L bracket, which will only be played to the minor stage of the final, resulting in two 3rd placed players. Thus, compared to double elimination, there is no major stage of the L Bracket final played, and there is no game between the winners of the W and L Brackets.\n\nAnother aspect of the system used in judo is that losers of the first round (of the W bracket) only advance to the L bracket if the player they lost to wins his or her second round match. If a player loses to a second round loser, they are eliminated from the tournament.\n\nAnother variant, called the \"(third-place) challenge\", is used, particularly in scholastic wrestling. The winner of the L bracket may challenge the loser of the finals in the W bracket, if and only if the two contestants had not faced each other previously; if the challenger (the winner of the L bracket) wins, he is awarded second place, and the loser of the W final is dropped to third place. This system is used particularly where the top two places advance to a higher level of competition (example: advancement from a regional tournament to a state tournament).\n\nAnother is the \"balanced\" variant which is a bracket arrangement that is \"not\" strictly divided into two brackets based on number of losses. Players with different numbers of losses can play each other in any round. A goal of the variant is that no player sits idle for more than one round consecutively. The added complexity of the brackets is handled by using \"if necessary\" matches. The flexible approach allows practical bracket designs to be made for any number of competitors including odd numbers (9, 10, 11, 12, 13, etc.).\n\nA possible alternative is a single-elimination format where each match is a best-of-5 or best-of-7 series. This format still allows a competitor to lose (perhaps multiple times) while still remaining eligible to win the tournament. Of course, having multiple games in each series \"also\" requires considerably more games to be conducted.\n\nAnother is the modified single elimination tournament which guarantees at least two games per competitor, but not necessarily two losses for elimination. The brackets are similar to the double elimination format, except the two finalists from the L bracket (each with one loss) face the two finalists from the W bracket (neither with a loss) in a single elimination semi-final and final.\n\nThe College World Series (a baseball tournament) has frequently tried to modify the double-elimination format to set up, if possible, a single championship game. Until 1988, the College World Series did this by adding an extra round to the L Bracket. What would be the L Bracket major semifinals (i.e. the round where the W Bracket semifinal losers dropped down) became the L Bracket quarterfinals. The winners would then progress to the L Bracket semifinals against the two participants in the W Bracket final (i.e. the WINNERS of the W Bracket semifinals drop down). This thus left open the possibility that the W Bracket champion would pick up a loss, albeit in the L Bracket semifinal. If, however, the W Bracket champion prevailed in the L Bracket semifinal, the same two-game final setup existed in effect, albeit not in practice ... for under the CWS pre-1988 version, the unbeaten W Bracket champion would be playing a once-beaten L Bracket opponent in the L Bracket final, with the winner to advance to play the unbeaten W Bracket champion in the finals (if necessary). The CWS subsequently broke up its eight-team field into two four-team double elimination tournaments, with the winners meeting in either a sudden-death or, currently, a best-of-three final.\n\nA way to reduce the number of rounds is to do cross-bracket elimination in the last rounds. For instance, in a double-elimination tournament of eight teams, you could have both the winner and the loser of the W Bracket final join the third round of the L Bracket, the winner facing the lowest-seeded L Bracket team or crossing inversely how W Bracket semifinal losers are placed in L Bracket. If the W Bracket team wins, there will be two teams left and they will go straight to the finals (with the W Bracket team having a one-game advantage as usual). However, if the W Bracket team loses then three teams will still be in the tournament, all with one loss. Usually in the subsequent fifth round either the last W Bracket team that just lost has a bye round or the top seed remaining will have a bye, while the other two teams square off. This leaves two teams for a one-game final in the sixth and last round. Whether the W Bracket team wins or loses in round four, this cross-bracket procedure shortens an eight team double elimination tournament from 6–7 rounds to 5–6 rounds. This system also gives more odds to a single game final (75% of situations, instead the ordinary 50%)\n\nThe Little League World Series began using a modified double elimination bracket in 2011. Eight U.S. teams and eight international teams compete in respective double elimination formats until their respective championship games, which are single elimination. That is, irrespective of whether a team has one loss, or no losses, that team would be eliminated with a loss in either the U.S. or international championship game. The two respective champions then play a single elimination game for the World Series championship.\n\nVariations of the double-elimination tournament include:\nOther common tournament types are\n"}
{"id": "13977590", "url": "https://en.wikipedia.org/wiki?curid=13977590", "title": "Eco-municipality", "text": "Eco-municipality\n\nAn eco-municipality or eco-town is a local government area that has adopted ecological and social justice values in its charter. The development of eco-municipalities stems from changing systems in Sweden, where more than seventy municipal governments have accepted varying principles of sustainability in their operations as well as community-wide decision making processes. The purpose of these policies is to increase the overall sustainability of the community.\n\nThe distinction between an eco-municipality and other sustainable development projects (such as green building and alternative energy) is the focus on community involvement and social transformation in a public agency as well as the use of a holistic systems approach. An eco-municipality is one that recognizes that issues of sustainability are key to all decisions made by government. \n\nIn 1983 the Övertorneå community of Sweden first adopted an Eco-municipality framework followed by a formal organization in 1995 (SEKOM).\n\nIn becoming an eco-municipality, cities or towns typically adopt a resolution, based on the Natural Step framework (or Framework for Strategic Sustainable Development (FSSD)), which sets the following objectives:\n\nCommunities in North America, Europe and Africa ranging in size from villages of 300 to cities of 700,000 have become eco-municipalities. In Sweden, over one hundred municipalities have officially become eco-municipalities. They have formed a national association of eco-municipalities to assist one another and work to influence national policy. Whistler, BC, was awarded first place in a United Nations-endorsed international competition for sustainable communities. Its long-term sustainability plan, Whistler 2020, is based on the Natural Step framework.\n\nIn Wisconsin, there is a growing eco-municipality movement which began in the Chequamegon Bay region. As of November 2007, twelve local communities had formally adopted eco-municipality resolutions. The resolutions state the community's intention to become an eco-municipality, endorsing the Natural Step sustainability principles and framework as a guide.\n\n\n\n"}
{"id": "41865411", "url": "https://en.wikipedia.org/wiki?curid=41865411", "title": "Entention", "text": "Entention\n\nEntention is a neologism coined by biological anthropologist Terrence Deacon in his 2011 book Incomplete Nature. The term is deliberately similar to the term intention, which has a long history of use in philosophy of mind, but was designed to have a broader scope. \"Ententional\" is an adjective that applies to the class of objects and phenomena that refer to or are in some other way \"about\" something not present. This wikipedia page is ententional because it refers to and is explicitly about an abstract concept which is not physically present in the page itself. Other paradigm examples of ententional objects are books, DNA strands, and tools. In contrast, rocks, stars, and electromagnetic radiation are not ententional.\n\nJeremy Sherman writes on ententionality, \"Deacon coins the term \"ententional,\" to encompass the entire range of phenomena that must be explained, everything from the first evolvable function, to human social processes, everything traditionally called intentional but also everything merely functional, fitting and therefore representing its environment with normative (good or bad fit) consequences.\" \n\n"}
{"id": "857235", "url": "https://en.wikipedia.org/wiki?curid=857235", "title": "Equivalence principle", "text": "Equivalence principle\n\nIn the theory of general relativity, the equivalence principle is the equivalence of gravitational and inertial mass, and Albert Einstein's observation that the gravitational \"force\" as experienced locally while standing on a massive body (such as the Earth) is the same as the \"pseudo-force\" experienced by an observer in a non-inertial (accelerated) frame of reference.\n\nSomething like the equivalence principle emerged in the early 17th century, when Galileo expressed experimentally that the acceleration of a test mass due to gravitation is independent of the amount of mass being accelerated.\n\nKepler, using Galileo's discoveries, showed knowledge of the equivalence principle by accurately describing what would occur if the moon were stopped in its orbit and dropped towards Earth. This can be deduced without knowing if or in what manner gravity decreases with distance, but requires assuming the equivalency between gravity and inertia.\n\nThe 1/54 ratio is Kepler's estimate of the Moon–Earth mass ratio, based on their diameters. The accuracy of his statement can be deduced by using Newton's inertia law F=ma and Galileo's gravitational observation that distance formula_1. Setting these accelerations equal for a mass is the equivalence principle. Noting the time to collision for each mass is the same gives Kepler's statement that D/D=M/M, without knowing the time to collision or how or if the acceleration force from gravity is a function of distance.\n\nNewton's gravitational theory simplified and formalized Galileo's and Kepler's ideas by recognizing Kepler's \"animal force or some other equivalent\" beyond gravity and inertia were not needed, deducing from Kepler's planetary laws how gravity reduces with distance.\n\nThe equivalence principle was properly introduced by Albert Einstein in 1907, when he observed that the acceleration of bodies towards the center of the Earth at a rate of 1\"\"g\"\" (\"g\" = 9.81 m/s being a standard reference of gravitational acceleration at the Earth's surface) is equivalent to the acceleration of an inertially moving body that would be observed on a rocket in free space being accelerated at a rate of 1\"g\". Einstein stated it thus:\n\nThat is, being on the surface of the Earth is equivalent to being inside a spaceship (far from any sources of gravity) that is being accelerated by its engines. The direction or vector of acceleration equivalence on the surface of the earth is \"up\" or directly opposite the center of the planet while the vector of acceleration in a spaceship is directly opposite from the mass ejected by its thrusters. From this principle, Einstein deduced that free-fall is inertial motion. Objects in free-fall do not experience being accelerated downward (e.g. toward the earth or other massive body) but rather weightlessness and no acceleration. In an inertial frame of reference bodies (and photons, or light) obey Newton's first law, moving at constant velocity in straight lines. Analogously, in a curved spacetime the world line of an inertial particle or pulse of light is \"as straight as possible\" (in space \"and\" time). Such a world line is called a geodesic and from the point of view of the inertial frame is a straight line. This is why an accelerometer in free-fall doesn't register any acceleration; there isn't any.\n\nAs an example: an inertial body moving along a geodesic through space can be trapped into an orbit around a large gravitational mass without ever experiencing acceleration. This is possible because spacetime is radically curved in close vicinity to a large gravitational mass. In such a situation the geodesic lines bend inward around the center of the mass and a free-floating (weightless) inertial body will simply follow those curved geodesics into an elliptical orbit. An accelerometer on-board would never record any acceleration.\n\nBy contrast, in Newtonian mechanics, gravity is assumed to be a force. This force draws objects having mass towards the center of any massive body. At the Earth's surface, the force of gravity is counteracted by the mechanical (physical) resistance of the Earth's surface. So in Newtonian physics, a person at rest on the surface of a (non-rotating) massive object is in an inertial frame of reference. These considerations suggest the following corollary to the equivalence principle, which Einstein formulated precisely in 1911:\n\nEinstein also referred to two reference frames, K and K'. K is a uniform gravitational field, whereas K' has no gravitational field but is uniformly accelerated such that objects in the two frames experience identical forces:\n\nThis observation was the start of a process that culminated in general relativity. Einstein suggested that it should be elevated to the status of a general principle, which he called the \"principle of equivalence\" when constructing his theory of relativity:\n\nEinstein combined (postulated) the equivalence principle with special relativity to predict that clocks run at different rates in a gravitational potential, and light rays bend in a gravitational field, even before he developed the concept of curved spacetime.\n\nSo the original equivalence principle, as described by Einstein, concluded that free-fall and inertial motion were physically equivalent. This form of the equivalence principle can be stated as follows. An observer in a windowless room cannot distinguish between being on the surface of the Earth, and being in a spaceship in deep space accelerating at 1g. This is not strictly true, because massive bodies give rise to tidal effects (caused by variations in the strength and direction of the gravitational field) which are absent from an accelerating spaceship in deep space. The room, therefore, should be small enough that tidal effects can be neglected.\n\nAlthough the equivalence principle guided the development of general relativity, it is not a founding principle of relativity but rather a simple consequence of the \"geometrical\" nature of the theory. In general relativity, objects in free-fall follow geodesics of spacetime, and what we perceive as the force of gravity is instead a result of our being unable to follow those geodesics of spacetime, because the mechanical resistance of matter prevents us from doing so.\n\nSince Einstein developed general relativity, there was a need to develop a framework to test the theory against other possible theories of gravity compatible with special relativity. This was developed by Robert Dicke as part of his program to test general relativity. Two new principles were suggested, the so-called Einstein equivalence principle and the strong equivalence principle, each of which assumes the weak equivalence principle as a starting point. They only differ in whether or not they apply to gravitational experiments.\n\nAnother clarification needed is that the equivalence principle assumes a constant acceleration of 1g without considering the mechanics of generating 1g. If we do consider the mechanics of it, then we must assume the aforementioned windowless room has a fixed mass. Accelerating it at 1g means there is a constant force being applied, which = m*g where m is the mass of the windowless room along with its contents (including the observer). Now, if the observer jumps inside the room, an object lying freely on the floor will decrease in weight momentarily because the acceleration is going to decrease momentarily due to the observer pushing back against the floor in order to jump. The object will then gain weight while the observer is in the air and the resulting decreased mass of the windowless room allows greater acceleration; it will lose weight again when the observer lands and pushes once more against the floor; and it will finally return to its initial weight afterwards. To make all these effects equal those we would measure on a planet producing 1g, the windowless room must be assumed to have the same mass as that planet. Additionally, the windowless room must not cause its own gravity, otherwise the scenario changes even further. These are technicalities, clearly, but practical ones if we wish the experiment to demonstrate more or less precisely the equivalence of 1g gravity and 1g acceleration.\n\nThree forms of the equivalence principle are in current use: weak (Galilean), Einsteinian, and strong.\n\nThe weak equivalence principle, also known as the universality of free fall or the Galilean equivalence principle can be stated in many ways. The strong EP includes (astronomic) bodies with gravitational binding energy (e.g., 1.74 solar-mass pulsar PSR J1903+0327, 15.3% of whose separated mass is absent as gravitational binding energy). The weak EP assumes falling bodies are bound by non-gravitational forces only. Either way:\n\nLocality eliminates measurable tidal forces originating from a radial divergent gravitational field (e.g., the Earth) upon finite sized physical bodies. The \"falling\" equivalence principle embraces Galileo's, Newton's, and Einstein's conceptualization. The equivalence principle does not deny the existence of measurable effects caused by a \"rotating\" gravitating mass (frame dragging), or bear on the measurements of light deflection and gravitational time delay made by non-local observers.\n\nBy definition of active and passive gravitational mass, the force on formula_2 due to the gravitational field of formula_3 is:\n\nLikewise the force on a second object of arbitrary mass due to the gravitational field of mass is:\n\nBy definition of inertial mass:\n\nIf formula_7 and formula_8 are the same distance formula_9 from formula_10 then, by the weak equivalence principle, they fall at the same rate (i.e. their accelerations are the same)\n\nHence:\n\nTherefore:\n\nIn other words, passive gravitational mass must be proportional to inertial mass for all objects.\n\nFurthermore, by Newton's third law of motion:\n\nmust be equal and opposite to\n\nIt follows that:\n\nIn other words, passive gravitational mass must be proportional to active gravitational mass for all objects.\n\nThe dimensionless Eötvös-parameter formula_17 is the difference of the ratios of gravitational and inertial masses divided by their average for the two sets of test masses \"A\" and \"B.\"\n\nTests of the weak equivalence principle are those that verify the equivalence of gravitational mass and inertial mass. An obvious test is dropping different objects, ideally in a vacuum environment, e.g., inside the Fallturm Bremen drop tower.\n\nSee:\n\nExperiments are still being performed at the University of Washington which have placed limits on the differential acceleration of objects towards the Earth, the Sun and towards dark matter in the galactic center. Future satellite experiments – STEP (Satellite Test of the Equivalence Principle), Galileo Galilei, and MICROSCOPE (MICROSatellite à traînée Compensée pour l'Observation du Principe d'Équivalence) – will test the weak equivalence principle in space, to much higher accuracy.\n\nWith the first successful production of antimatter, in particular anti-hydrogen, a new approach to test the weak equivalence principle has been proposed. Experiments to compare the gravitational behavior of matter and antimatter are currently being developed.\n\nProposals that may lead to a quantum theory of gravity such as string theory and loop quantum gravity predict violations of the weak equivalence principle because they contain many light scalar fields with long Compton wavelengths, which should generate fifth forces and variation of the fundamental constants. Heuristic arguments suggest that the magnitude of these equivalence principle violations could be in the 10 to 10 range. Currently envisioned tests of the weak equivalence principle are approaching a degree of sensitivity such that \"non-discovery\" of a violation would be just as profound a result as discovery of a violation. Non-discovery of equivalence principle violation in this range would suggest that gravity is so fundamentally different from other forces as to require a major reevaluation of current attempts to unify gravity with the other forces of nature. A positive detection, on the other hand, would provide a major guidepost towards unification.\n\nWhat is now called the \"Einstein equivalence principle\" states that the weak equivalence principle holds, and that:\nHere \"local\" has a very special meaning: not only must the experiment not look outside the laboratory, but it must also be small compared to variations in the gravitational field, tidal forces, so that the entire laboratory is freely falling. It also implies the absence of interactions with \"external\" fields \"other than the gravitational field\".\n\nThe principle of relativity implies that the outcome of local experiments must be independent of the velocity of the apparatus, so the most important consequence of this principle is the Copernican idea that dimensionless physical values such as the fine-structure constant and electron-to-proton mass ratio must not depend on where in space or time we measure them. Many physicists believe that any Lorentz invariant theory that satisfies the weak equivalence principle also satisfies the Einstein equivalence principle.\n\n\"Schiff's conjecture\" suggests that the weak equivalence principle implies the Einstein equivalence principle, but it has not been proven. Nonetheless, the two principles are tested with very different kinds of experiments. The Einstein equivalence principle has been criticized as imprecise, because there is no universally accepted way to distinguish gravitational from non-gravitational experiments (see for instance Hadley and Durand).\n\nIn addition to the tests of the weak equivalence principle, the Einstein equivalence principle can be tested by searching for variation of dimensionless constants and mass ratios. The present best limits on the variation of the fundamental constants have mainly been set by studying the naturally occurring Oklo natural nuclear fission reactor, where nuclear reactions similar to ones we observe today have been shown to have occurred underground approximately two billion years ago. These reactions are extremely sensitive to the values of the fundamental constants.\n\nThere have been a number of controversial attempts to constrain the variation of the strong interaction constant. There have been several suggestions that \"constants\" do vary on cosmological scales. The best known is the reported detection of variation (at the 10 level) of the fine-structure constant from measurements of distant quasars, see Webb et al. Other researchers dispute these findings. Other tests of the Einstein equivalence principle are gravitational redshift experiments, such as the Pound–Rebka experiment which test the position independence of experiments.\n\nThe strong equivalence principle suggests the laws of gravitation are independent of velocity and location. In particular,\nand\nThe first part is a version of the weak equivalence principle that applies to objects that exert a gravitational force on themselves, such as stars, planets, black holes or Cavendish experiments. The second part is the Einstein equivalence principle (with the same definition of \"local\"), restated to allow gravitational experiments and self-gravitating bodies. The freely-falling object or laboratory, however, must still be small, so that tidal forces may be neglected (hence \"local experiment\").\n\nThis is the only form of the equivalence principle that applies to self-gravitating objects (such as stars), which have substantial internal gravitational interactions. It requires that the gravitational constant be the same everywhere in the universe and is incompatible with a fifth force. It is much more restrictive than the Einstein equivalence principle.\n\nThe strong equivalence principle suggests that gravity is entirely geometrical by nature (that is, the metric alone determines the effect of gravity) and does not have any extra fields associated with it. If an observer measures a patch of space to be flat, then the strong equivalence principle suggests that it is absolutely equivalent to any other patch of flat space elsewhere in the universe. Einstein's theory of general relativity (including the cosmological constant) is thought to be the only theory of gravity that satisfies the strong equivalence principle. A number of alternative theories, such as Brans–Dicke theory, satisfy only the Einstein equivalence principle.\n\nThe strong equivalence principle can be tested by searching for a variation of Newton's gravitational constant \"G\" over the life of the universe, or equivalently, variation in the masses of the fundamental particles. A number of independent constraints, from orbits in the solar system and studies of big bang nucleosynthesis have shown that \"G\" cannot have varied by more than 10%.\n\nThus, the strong equivalence principle can be tested by searching for fifth forces (deviations from the gravitational force-law predicted by general relativity). These experiments typically look for failures of the inverse-square law (specifically Yukawa forces or failures of Birkhoff's theorem) behavior of gravity in the laboratory. The most accurate tests over short distances have been performed by the Eöt–Wash group. A future satellite experiment, SEE (Satellite Energy Exchange), will search for fifth forces in space and should be able to further constrain violations of the strong equivalence principle. Other limits, looking for much longer-range forces, have been placed by searching for the Nordtvedt effect, a \"polarization\" of solar system orbits that would be caused by gravitational self-energy accelerating at a different rate from normal matter. This effect has been sensitively tested by the Lunar Laser Ranging Experiment. Other tests include studying the deflection of radiation from distant radio sources by the sun, which can be accurately measured by very long baseline interferometry. Another sensitive test comes from measurements of the frequency shift of signals to and from the Cassini spacecraft. Together, these measurements have put tight limits on Brans–Dicke theory and other alternative theories of gravity.\n\nIn 2014, astronomers discovered a stellar triple system including a millisecond pulsar PSR J0337+1715 and two white dwarfs orbiting it. The system provided them a chance to test the strong equivalence principle in a strong gravitational field with high accuracy.\n\nOne challenge to the equivalence principle is the Brans–Dicke theory. Self-creation cosmology is a modification of the Brans–Dicke theory. The Fredkin Finite Nature Hypothesis is an even more radical challenge to the equivalence principle and has even fewer supporters.\n\nIn August 2010, researchers from the University of New South Wales, Swinburne University of Technology, and Cambridge University published a paper titled \"Evidence for spatial variation of the fine structure constant\", whose tentative conclusion is that, \"qualitatively, [the] results suggest a violation of the Einstein Equivalence Principle, and could infer a very large or infinite universe, within which our 'local' Hubble volume represents a tiny fraction.\"\n\nIn his book \"Einstein's Mistakes\", pages 226–227, Hans C. Ohanian describes several situations which falsify Einstein's Equivalence Principle. Inertial accelerative effects are analogous to, but not equivalent to, gravitational effects. Ohanian cites Ehrenfest for this same opinion.\n\nDutch physicist and string theorist Erik Verlinde has generated a self-contained, logical derivation of the equivalence principle based on the starting assumption of a holographic universe. Given this situation, gravity would not be a true fundamental force as is currently thought but instead an \"emergent property\" related to entropy. Verlinde's entropic gravity theory apparently leads naturally to the correct observed strength of dark energy; previous failures to explain its incredibly small magnitude have been called by such people as cosmologist Michael Turner (who is credited as having coined the term \"dark energy\") as \"the greatest embarrassment in the history of theoretical physics\". However, it should be noted that these ideas are far from settled and still very controversial.\n\n\n\n"}
{"id": "14946795", "url": "https://en.wikipedia.org/wiki?curid=14946795", "title": "Extrajudicial killings and forced disappearances in the Philippines", "text": "Extrajudicial killings and forced disappearances in the Philippines\n\nExtrajudicial killings and forced disappearances in the Philippines are illegal executionsunlawful or felonious killingsand forced disappearances in the Philippines. These are forms of extrajudicial punishment, and include extrajudicial executions, summary executions, arbitrary arrest and detentions, and failed prosecutions due to political activities of leading political, trade union members, dissident and/or social figures, left-wing political parties, non-governmental organizations, political journalists, outspoken clergy, anti-mining activists, agricultural reform activists, members of organizations that are allied or legal fronts of the communist movement like \"Bayan group\" or suspected supporters of the NPA and its political wing, the Communist Party of the Philippines (CPP).\n\nExtrajudicial killings are most commonly referred to as \"salvaging\" in Philippine English. The word is believed to be a direct Anglicization of Tagalog \"salbahe\" (\"cruel\", \"barbaric\"), from Spanish \"salvaje\" (\"wild\", \"savage\").\n\nExtrajudicial killings (EJKs) is also synonymous with the term \"extralegal killings\" (ELKs). Extrajudicial/ extralegal killings (EJKs/ ELKs) and enforced disappearances (EDs) are unique in the Philippines in as much as it is publicly and commonly known to be committed also by non-state armed groups (NAGs) such as the New Peoples Army (NPA) and the Moro Islamic Liberation Front (MILF). Although cases have been well documented with conservative estimates of EJKs/ ELKs and EDs committed by the NPAs numbering to about 900-1,000 victims based on the discovery of numerous mass grave sites all over country, legal mechanisms for accountability of non-state actors have been weak if not wholly non-existent.\n\nPhilippine extrajudicial killings are politically motivated murders committed by government officers, punished by local and international law or convention. They include assassinations; deaths due to strafing or indiscriminate firing; massacre; summary execution is done if the victim becomes passive before the moment of death (i.e., abduction leading to death); assassination means forthwith or instant killing while is akin to genocide or mass extermination; thus, killings occurred in many regions or places throughout the Philippines in different times - 136 killings in Southern Tagalog region were recorded by human rights group Karapatan from 2001 to May 19, 2006.\n\nA forced disappearance (desaparecidos), on the other hand, as form of extrajudicial punishment is perpetrated by government officers, when any of its public officers abducts an individual, to vanish from public view, resulting to murder or plain sequestration. The victim is first kidnapped, then illegally detained in concentration camps, often tortured, and finally executed and the corpse hidden. In Spanish and Portuguese, \"disappeared people\" are called \"desaparecidos\", a term which specifically refers to the mostly South American victims of state terrorism during the 1970s and the 1980s, in particular concerning Operation Condor. In the International Convention for the Protection of All Persons from Enforced Disappearance, \"Enforced disappearance\" is defined in Article 2 of the United Nations Convention Against Torture as \"the arrest, detention, abduction or any other form of deprivation of liberty by agents of the State or by persons or groups of persons acting with the authorization, support or acquiescence of the State, followed by a refusal to acknowledge the deprivation of liberty or by concealment of the fate or whereabouts of the disappeared person, which place such a person outside the protection of the law.\"\n\nEven if Philippine Republic Act No. 7438 provides for the rights of persons arrested, detained, it does not punish acts of enforced disappearances. Thus, on August 27, Bayan Muna (People First), Gabriela Women's Party (GWP), and Anakpawis (Toiling Masses) filed House Bill 2263\"An act defining and penalizing the crime of enforced or involuntary disappearance.\" Sen. Jinggoy Estrada also filed last June 30, 2007, Senate Bill No. 7\"An Act Penalizing the Commission of Acts of Torture and Involuntary Disappearance of Persons Arrested, Detained or Under Custodial Investigation, and Granting Jurisdiction to the Commission on Human Rights to Conduct Preliminary Investigation for Violation of the Custodial Rights of the Accused, Amending for this Purpose Sections 2, 3 and 4 of RA 7438, and for Other Purposes.\"\n\nIn 1995, 10,000 Filipinos won a U.S. class-suit against the Ferdinand Marcos estate. The charges were filed by victims or their surviving relatives for torture, execution and disappearances. Human rights groups placed the number of victims of extrajudicial killings under martial law at 1500 and over 800 abductions; Karapatan (a local human rights group's) records show 759 involuntarily disappeared (their bodies never found). Military historian Alfred McCoy in his book \"Closer than Brothers: Manhood at the Philippine Military Academy\" and in his speech \"Dark Legacy\" cites 3,257 extrajudicial killings, 35,000 torture victims, and 70,000 incarcerated during the Marcos years. The newspaper \"Bulatlat\" places the number of victims of arbitrary arrest and detention at 120,000.\n\nThe New People's Army (NPA) groups known as \"Sparrow Units\" were active in the mid-1980s, killing government officials, police personnel, military members, and anyone else they targeted for elimination. They were also part of an NPA operation called \"Agaw Armas\" (Filipino for \"Stealing Weapons\"), where they raided government armories as well as stealing weapons from slain military and police personnel. A low level civil war with south Muslims, Al-Qaeda sympathizers and communist insurgents has led to a general break down of law and order. The Philippines government has promised to curb the killings, but is itself implicated in many of the killings.\n\nSince 1975, the Armed Forces of the Philippines (AFP) was deeply concerned in politics. Because of the armed conflict, the military continued its campaign versus the New People's Army of the Communist Party of the Philippines (CPP). Since 1969 it aimed to establish a Marxist regime with armed rebellion against the government. On top of all these chaos, left-wing non-governmental organizations (NGOs) were/are critical of the Gloria Macapagal-Arroyo administration. The members who associated with the CPP and NPA had been targeted as victims in the spate of political killings. Human Rights Watch investigated extrajudicial murders in the Philippines in September 2007.\n\nThree major investigation groups were commissioned and their final reports had been submitted and published: the Gloria Macapagal-Arroyo government-appointed bodies: a) Task Force Usig created by her on August; as a special police body, it was assigned to solve 10 cases of killings; it claimed having solved 21 cases, by initiating court cases, but only 12 suspects were arrested; b) the Melo Commission (chaired by Supreme Court Associate Justice Jose Melo) with members National Bureau of Investigation Director Nestor Mantaring, Chief State Prosecutor Jovencito Zuño, Bishop Juan de dios Pueblos, and Nelia Torres Gonzales; its final report states: \"There is no official or sanctioned policy on the part of the military or its civilian superiors to resort to what other countries euphemistically call \"alternative procedures\"meaning illegal executions. However, there is certainly evidence pointing the finger of suspicion at some elements and personalities in the armed forces, in particular General Jovito Palparan, as responsible for an undetermined number of killings, by allowing, tolerating, and even encouraging the killings.\" (Melo Commission report, p. 53), and c) Philip Alston, the United Nations Special Rapporteur on Extrajudicial Executions (February 12 to 21, 2007)\n\n\nBecause of the inefficacy and insufficiency of the Philippines Writ of Habeas Corpus, on September 25, 2007, Chief Justice Reynato Puno signed and released the Writ of Amparo: \"This rule will provide the victims of extralegal killings and enforced disappearances the protection they need and the promise of vindication for their rights. This rule empowers our courts to issue reliefs that may be granted through judicial orders of protection, production, inspection and other relief to safeguard one's life and liberty The writ of amparo shall hold public authorities, those who took their oath to defend the constitution and enforce our laws, to a high standard of official conduct and hold them accountable to our people. The sovereign Filipino people should be assured that if their right to life and liberty is threatened or violated, they will find vindication in our courts of justice'.\" Puno explained the interim reliefs under amparo: temporary protection order (TPO), inspection order (IO), production order (PO), and witness protection order (WPO, RA 6981). As supplement to Amparo, on August 30, 2007, Puno (at Silliman University in Dumaguete City, Negros Oriental) promised to release also the \"writ of habeas data\" (“you should have the idea” or “you should have the data”) another new legal remedy to solve the extrajudicial killings and enforced disappearances. Puno explained that the \"writ of amparo\" denies to authorities defense of simple denial, and \"habeas data\" can find out what information is held by the officer, rectify or even the destroy erroneous data gathered. Brazil used the writ, followed by Colombia, Paraguay, Peru, Argentina and Ecuador.\n\n\nIn 2006, the Dutch Lawyers for Lawyers Foundation and Lawyers without Borders with the support of the Netherlands Bar Association, the Amsterdam Bar Association and the International Association of Democratic Lawyers created a fact-finding mission in different parts of the Philippines. The international groups conducted interviews of various legal sectors from June 15 to June 20, 2006.\n\nFrom November 4–12, 2008, the Dutch Lawyers for Lawyers Foundation will conduct a follow-up verification and fact finding mission (IVFFM) in Manila and Mindanao, with the National Host Committee, National Union of Peoples' Lawyers (NUPL) and the Counsels for the Defense of Liberties (CODAL). This team is composed of 8 judges and lawyers from Belgium and Netherlands, who had dialogue with Reynato Puno on the probe of killings.\nOn September 28, 2007, the Asian Human Rights Commission (AHRC) criticized the Writ of Amparo and Habeas Data (Philippines) for being insufficient: \"Though it responds to practical areas it is still necessary that further action must be taken in addition to this. The legislative bodies, House of Representatives and Senate, should also initiate its own actions promptly and without delay. They must enact laws which ensure protection of rights—laws against torture and enforced disappearance and laws to afford adequate legal remedies to victims.\" AHRC objected since the writ failed to protect non-witnesses, even if they too face threats or risk to their lives.\n\n\n\n\n\nOn March 11, 2008, the US Department of State reported that \"arbitrary, unlawful arrests and extrajudicial and political killings continued to be a major problem in the Philippines in 2007. Washington stated that \"many of these killings went unsolved and unpunished despite intensified efforts of the government to investigate and prosecute these cases.\"\n\nOn January 25, 2005, and on December 10, 2006, Philippines Social Weather Stations released the results of its two surveys on corruption in the judiciary; it published that: a) like 1995, 1/4 of lawyers said many/very many judges are corrupt. But (49%) stated that a judges received bribes, just 8% of lawyers admitted they reported the bribery, because they could not prove it. [Tables 8-9]; judges, however, said, just 7% call many/very many judges as corrupt[Tables 10-11];b) \"Judges see some corruption; proportions who said - many/very many corrupt judges or justices: 17% in reference to RTC judges, 14% to MTC judges, 12% to Court of Appeals justices, 4% i to Shari'a Court judges, 4% to Sandiganbayan justices and 2% in reference to Supreme Court justices [Table 15].\n\nIn the Maguindanao massacre in the Philippines on November 23, 2009, 57 people were killed while en route to file an electoral certificate of candidacy for Esmael Mangudadatu, vice mayor of Buluan town, in upcoming gubernatorial elections for Maguindanao province. The dead included Mangudadatu's wife, his two sisters, journalists, lawyers, aides, and motorists who were witnesses. At least 198 suspects were charged with murder, including incumbent governor Andal Ampatuan Sr., and his son, Andal Ampatuan Jr. who was to be a candidate to succeed him. On November 16, 2010, the international non-governmental organization Human Rights Watch issued a 96 page report titled \"They Own the People,\" charting the Ampatuans’ rise to power, including their use of violence to expand their control and eliminate threats to the family's rule.\n\nOn July 2, 2016, the Communist Party of the Philippines stated that it \"reiterates its standing order for the NPA to carry out operations to disarm and arrest the chieftains of the biggest drug syndicates, as well as other criminal syndicates involved in human rights violations and destruction of the environment\" after its political wing Bagong Alyansang Makabayan accepted Cabinet posts in the new government. On July 3, the Philippine National Police said they had killed 30 alleged drug dealers since Duterte was sworn in as president on June 30. They later stated they had killed 103 suspects between May 10 and July 7.\n\nOn August 26, 2016, the official death total reached 2,000.\n\n\n\nGeneral:\n\n\n"}
{"id": "42523304", "url": "https://en.wikipedia.org/wiki?curid=42523304", "title": "Feed the Deed", "text": "Feed the Deed\n\nFeed The Deed, also known as #FeedtheDeed, is a social media pay-it-forward initiative that started in February 2014. Participants film themselves performing a creative random act of kindness, then nominate friends and family to continue on the chain. The participant usually uploads a video or pictures of the kind act to Facebook, then will tag four or five friends in the post. The nominated person is generally told to complete the task within 24 hours.\n\nSince this initiative was started, over 10,000 #FeedtheDeed posts have been uploaded to various forms of social media. The trend is most popular in Canada, United States, United Kingdom, Australia, and Mexico, but has since spread to over 30 countries. The act of kindness can be anything – from donating food and clothes to the homeless, to giving blood.\n\nFeed The Deed originated as a response to the Neknominate drinking game, in which participants film themselves drinking alcohol then nominate friends to do the same.\n\nJosh Stern, a medical student at the University of Ottawa, posted his original #FeedtheDeed video to his Facebook page. He was then contacted by his friend Russell Citron, who is president and founder of the non-profit organization Kindness Counts. The two decided to collaborate on the project by taking Stern's #FeedtheDeed moniker and running it through Citron's organization Kindness Counts.\n\nStern says that the inspiration for #FeedtheDeed came from a YouTube video in which Brent Lindeque decided to use his neknomination to give food to a homeless man instead of drinking alcohol. Lindeque is currently running a similar campaign in South Africa called #ChangeOneThing.\n\nFeed the Deed has been run through the \"Kindness Counts\" Facebook page.\n\n\n"}
{"id": "44308703", "url": "https://en.wikipedia.org/wiki?curid=44308703", "title": "Flajolet–Martin algorithm", "text": "Flajolet–Martin algorithm\n\nThe Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption logarithmic in the maximal number of possible distinct elements in the stream (the count-distinct problem). The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 article \"Probabilistic Counting Algorithms for Data Base Applications\". Later it has been refined in \"LogLog counting of large cardinalities\" by Marianne Durand and Philippe Flajolet, and \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\" by Philippe Flajolet et al.\n\nIn their 2010 article \"An optimal algorithm for the distinct elements problem\", Daniel M. Kane, Jelani Nelson and David P. Woodruff give an improved algorithm, which uses nearly optimal space and has optimal \"O\"(1) update and reporting times.\n\nAssume that we are given a hash function formula_1 that maps input formula_2 to integers in the range formula_3, and where the outputs are sufficiently uniformly distributed. Note that the set of integers from 0 to formula_4 corresponds to the set of binary strings of length formula_5. For any non-negative integer formula_6, define formula_7 to be the formula_8-th bit in the binary representation of formula_6, such that:\n\nWe then define a function formula_11 that outputs the position of the least-significant set bit in the binary representation of formula_6:\n\nwhere formula_14. Note that with the above definition we are using 0-indexing for the positions. For example, formula_15, since the least significant bit is a 1 (0th position), and formula_16, since the least significant bit is at the 3rd position. At this point, note that under the assumption that the output of our hash function is uniformly distributed, then the probability of observing a hash output ending with formula_17 (a one, followed by formula_8 zeroes) is formula_19, since this corresponds to flipping formula_8 heads and then a tail with a fair coin.\n\nNow the Flajolet–Martin algorithm for estimating the cardinality of a multiset formula_21 is as follows:\n\nThe idea is that if formula_33 is the number of distinct elements in the multiset formula_21, then formula_35 is accessed approximately formula_36 times, formula_37 is accessed approximately formula_38 times and so on. Consequently, if formula_39, then formula_40 is almost certainly 0, and if formula_41, then formula_40 is almost certainly 1. If formula_43, then formula_40 can be expected to be either 1 or 0.\n\nThe correction factor formula_32 is found by calculations, which can be found in the original article.\n\nA problem with the Flajolet–Martin algorithm in the above form is that the results vary significantly. A common solution has been to run the algorithm multiple times with formula_8 different hash functions and combine the results from the different runs. One idea is to take the mean of the formula_8 results together from each hash function, obtaining a single estimate of the cardinality. The problem with this is that averaging is very susceptible to outliers (which are likely here). A different idea is to use the median, which is less prone to be influences by outliers. The problem with this is that the results can only take form formula_31, where formula_27 is integer. A common solution is to combine both the mean and the median: Create formula_50 hash functions and split them into formula_8 distinct groups (each of size formula_52). Within each group use the median for aggregating together the formula_52 results, and finally take the mean of the formula_8 group estimates as the final estimate.\n\nThe 2007 HyperLogLog algorithm splits the multiset into subsets and estimates their cardinalities, then it uses the harmonic mean to combine them into an estimate for the original cardinality.\n\n"}
{"id": "246176", "url": "https://en.wikipedia.org/wiki?curid=246176", "title": "Gettier problem", "text": "Gettier problem\n\nThe Gettier problem, in the field of epistemology, is a landmark philosophical problem concerning our understanding of descriptive knowledge. Attributed to American philosopher Edmund Gettier, Gettier-type counterexamples (called \"Gettier-cases\") challenge the long-held justified true belief (JTB) account of knowledge. The JTB account holds that knowledge is equivalent to justified true belief; if all three conditions (justification, truth, and belief) are met of a given claim, then we have knowledge of that claim. In his 1963 three-page paper titled \"Is Justified True Belief Knowledge?\", Gettier attempts to illustrate by means of two counterexamples that there are cases where individuals can have a justified, true belief regarding a claim but still fail to know it because the reasons for the belief, while justified, turn out to be false. Thus, Gettier claims to have shown that the JTB account is inadequate; that it does not account for all of the necessary and sufficient conditions for knowledge.\n\nThe term \"Gettier problem\", \"Gettier case\", or even the adjective \"Gettiered\", is sometimes used to describe any case in the field of epistemology that purports to repudiate the JTB account of knowledge.\n\nResponses to Gettier's paper have been numerous; some reject Gettier's examples, while others seek to adjust the JTB account of knowledge and blunt the force of these counterexamples. Gettier problems have even found their way into sociological experiments, where the intuitive responses from people of varying demographics to Gettier cases have been studied.\n\nThe question of what constitutes \"knowledge\" is as old as philosophy itself. Early instances are found in Plato's dialogues, notably \"Meno\" (97a–98b) and \"Theaetetus\". Gettier himself was not actually the first to raise the problem named after him; its existence was acknowledged by both Alexius Meinong and Bertrand Russell, the latter of which discussed the problem in his book \"Human knowledge: Its scope and limits\".\n\nRussell's case, called the stopped clock case, goes as follows: Alice sees a clock that reads two o'clock and believes that the time is two o'clock. It is, in fact, two o'clock. There's a problem, however: unknown to Alice, the clock she's looking at stopped twelve hours ago. Alice thus has an accidentally true, justified belief. Russell provides an answer of his own to the problem. Edmund Gettier's formulation of the problem was important as it coincided with the rise of the sort of philosophical naturalism promoted by W. V. O. Quine and others, and was used as a justification for a shift towards externalist theories of justification. John L. Pollock and Joseph Cruz have stated that the Gettier problem has \"fundamentally altered the character of contemporary epistemology\" and has become \"a central problem of epistemology since it poses a clear barrier to analyzing knowledge\".\n\nAlvin Plantinga rejects the historical analysis:\nDespite this, Plantinga \"does\" accept that some philosophers before Gettier have advanced a JTB account of knowledge, specifically C. I. Lewis and A. J. Ayer.\n\nThe JTB account of knowledge is the claim that knowledge can be conceptually analyzed as justified true belief, which is to say that the \"meaning\" of sentences such as \"Smith knows that it rained today\" can be given with the following set of conditions, which are necessary and sufficient for knowledge to obtain:\n\nThe JTB account was first credited to Plato, though Plato argued against this very account of knowledge in the \"Theaetetus\" (210a). This account of knowledge is what Gettier subjected to criticism.\n\nGettier's paper used counterexamples (see also thought experiment) to argue that there are cases of beliefs that are both true and justified—therefore satisfying all three conditions for knowledge on the JTB account—but that do not appear to be genuine cases of knowledge. Therefore, Gettier argued, his counterexamples show that the JTB account of knowledge is false, and thus that a different conceptual analysis is needed to correctly track what we mean by \"knowledge\".\n\nGettier's case is based on two counterexamples to the JTB analysis. Each relies on two claims. Firstly, that justification is preserved by entailment, and secondly that this applies coherently to Smith's putative \"belief\". That is, that if Smith is justified in believing P, and Smith realizes that the truth of P entails the truth of Q, then Smith would \"also\" be justified in believing Q. Gettier calls these counterexamples \"Case I\" and \"Case II\":\n\nIn both of Gettier's actual examples (see also counterfactual conditional), the justified true belief came about, if Smith's purported claims are disputable, as the result of entailment (but see also material conditional) from justified false beliefs that \"Jones will get the job\" (in case I), and that \"Jones owns a Ford\" (in case II). This led some early responses to Gettier to conclude that the definition of knowledge could be easily adjusted, so that knowledge was justified true belief that does not depend on false premises.\n\nIn a 1966 scenario known as \"The sheep in the field\", Roderick Chisholm asks us to imagine that someone is standing outside a field looking at something that looks like a sheep (although in fact, it is a dog disguised as a sheep). They believe there is a sheep in the field, and in fact, they are right because there is a sheep behind the hill in the middle of the field. Hence, they have a justified true belief that there is a sheep in the field. But is that belief knowledge? A similar problem which seeks to be more plausible called the \"Cow in the Field\" appears in Martin Cohen's book \"101 Philosophy Problems\", where it is supposed that a farmer checking up on his favourite cow confuses a piece of black and white paper caught up in a distant bush for his cow. However, since the animal actually is in the field, but hidden in a hollow, again, the farmer has a justified, true belief which seems nonetheless not to qualify as \"knowledge\". \n\nAnother scenario by Brian Skyrms is \"The Pyromaniac\", in which a struck match lights not for the reasons the pyromaniac imagines but because of some unknown \"Q radiation\". \n\nA different perspective on the issue is given by Alvin Goldman in the \"fake barns\" scenario (crediting Carl Ginet with the example). In this one, a man is driving in the countryside, and sees what looks exactly like a barn. Accordingly, he thinks that he is seeing a barn. In fact, that is what he is doing. But what he does not know is that the neighborhood generally consists of many fake barns — barn facades designed to look exactly like real barns when viewed from the road, as in the case of a visit in the countryside by Catherine II of Russia, just to please her. Since if he had been looking at one of them, he would have been unable to tell the difference, his \"knowledge\" that he was looking at a barn would seem to be poorly founded. A similar process appears in Robert A. Heinlein's \"Stranger in a Strange Land\" as an example of Fair Witness behavior.\n\nThe \"no false premises\" (or \"no false lemmas\") solution which was proposed early in the discussion proved to be somewhat problematic, as more general Gettier-style problems were then constructed or contrived in which the justified true belief does not seem to be the result of a chain of reasoning from a justified false belief.\n\nFor example:\n\nAgain, it seems as though Luke does not \"know\" that Mark is in the room, even though it is claimed he has a justified true belief that Mark is in the room, but it is not nearly so clear that the \"perceptual belief\" that \"Mark is in the room\" was inferred from any premises at all, let alone any false ones, nor led to significant conclusions on its own; Luke did not seem to be reasoning about anything; \"Mark is in the room\" seems to have been part of what he \"seemed to see\".\n\nTo save the \"no false lemmas\" solution, one must logically say that Luke's inference from sensory data does not count as a justified belief unless he consciously or unconsciously considers the possibilities of deception and self-deception. A justified version of Luke's thought process, by that logic, might go like this:\n\nThe second line counts as a false premise. However, by the previous argument, this suggests we have fewer justified beliefs than we think we do.\n\nThe main idea behind Gettier's examples is that the justification for the belief is flawed or incorrect, but the belief turns out to be true by sheer luck. Thus, a general scenario can be constructed as such:\n\nBob believes A is true because of B. Argument B is flawed, but A turns out to be true by a different argument C. Since A is true, Bob believes A is true, and Bob has justification B, all of the conditions (JTB) are satisfied. However, Bob had no knowledge of A.\n\nThe Gettier problem is formally a problem in first-order logic, but the introduction by Gettier of terms such as \"believes\" and \"knows\" moves the discussion into the field of epistemology. Here, the sound (true) arguments ascribed to Smith then need also to be valid (believed) and convincing (justified) if they are to issue in the real-world discussion about \"justified true belief\".\nResponses to Gettier problems have fallen into one of three categories:\n\n\nOne response, therefore, is that in none of the above cases was the belief justified because it is impossible to justify anything that is not true. Conversely, the fact that a proposition turns out to be untrue is proof that it was not sufficiently justified in the first place. Under this interpretation, the JTB definition of knowledge survives. This shifts the problem to a definition of justification, rather than knowledge. Another view is that justification and non-justification are not in binary opposition. Instead, justification is a matter of degree, with an idea being more or less justified. This account of justification is supported by mainstream philosophers such as Paul Boghossian and Stephen Hicks. In common sense usage, an idea can not only be more justified or less justified, but it can also partially justified (Smith's boss told him X) and partially unjustified (Smith's boss is a liar). Gettier's cases involve propositions that were true, believed, but which had weak justification. In case 1, the premise that the testimony of Smith's boss is \"strong evidence\" is rejected. The case itself depends on the boss being either wrong or deceitful (Jones did not get the job) and therefore unreliable. In case 2, Smith again has accepted a questionable idea (Jones owns a Ford) with unspecified justification. Without justification, both cases do not undermine the JTB account of knowledge.\n\nOther epistemologists accept Gettier's conclusion. Their responses to the Gettier problem, therefore, consist of trying to find alternative analyses of knowledge. They have struggled to discover and agree upon as a beginning any single notion of truth, or belief, or justifying which is wholly and obviously accepted. Truth, belief, and justifying have not yet been satisfactorily defined, so that JTB (justified true belief) may be defined satisfactorily is still problematical, on account or otherwise of Gettier's examples. Gettier, for many years a professor at University of Massachusetts Amherst later also was interested in the epistemic logic of Hintikka, a Finnish philosopher at Boston University, who published \"Knowledge and Belief\" in 1962.\nThe most common direction for this sort of response to take is what might be called a \"JTB+G\" analysis: that is, an analysis based on finding some \"fourth\" condition—a \"no-Gettier-problem\" condition—which, when added to the conditions of justification, truth, and belief, will yield a set of necessary and jointly sufficient conditions.\n\nOne such response is that of Alvin Goldman (1967), who suggested the addition of a \"causal\" condition: a subject's belief is justified, for Goldman, only if the truth of a belief has \"caused\" the subject to have that belief (in the appropriate way); and for a justified true belief to count as knowledge, the subject must \"also\" be able to \"correctly reconstruct\" (mentally) that causal chain. Goldman's analysis would rule out Gettier cases in that Smith's beliefs are not caused by the truths of those beliefs; it is merely \"accidental\" that Smith's beliefs in the Gettier cases happen to be true, or that the prediction made by Smith: \" The winner of the job will have 10 coins\", on the basis of his putative belief, (see also bundling) came true in this one case. This theory is challenged by the difficulty of giving a principled explanation of how an appropriate causal relationship differs from an inappropriate one (without the circular response of saying that the appropriate sort of causal relationship is the knowledge-producing one); or retreating to a position in which justified true belief is weakly defined as the consensus of learned opinion. The latter would be useful, but not as useful nor desirable as the unchanging definitions of scientific concepts such as momentum. Thus, adopting a causal response to the Gettier problem usually requires one to adopt (as Goldman gladly does) some form of reliabilism about justification. See \"Goldman\"s Theory of justification.\n\nKeith Lehrer and Thomas Paxson (1969) proposed another response, by adding a \"defeasibility condition\" to the JTB analysis. On their account, knowledge is \"undefeated justified true belief\" — which is to say that a justified true belief counts as knowledge if and only if it is also the case that there is no further truth that, had the subject known it, would have defeated her present justification for the belief. (Thus, for example, Smith's justification for believing that the person who will get the job has ten coins in his pocket is his justified belief that Jones will get the job, combined with his justified belief that Jones has ten coins in his pocket. But if Smith had known the truth that Jones will \"not\" get the job, that would have defeated the justification for his belief.) However, many critics (such as Marshall Swain [1974]) have argued that the notion of a \"defeater\" fact cannot be made precise enough to rule out the Gettier cases without also ruling out \"a priori\" cases of knowledge .\n\nPragmatism was developed as a philosophical doctrine by C.S.Peirce and William James (1842–1910). In Peirce's view, the truth is nominally defined as a sign's correspondence to its object and pragmatically defined as the ideal final opinion to which sufficient investigation \"would\" lead sooner or later. James' epistemological model of truth was that which \"works\" in the way of belief, and a belief was true if in the long run it \"worked\" for all of us, and guided us expeditiously through our semihospitable world.\nPeirce argued that metaphysics could be cleaned up by a pragmatic approach.\nConsider what effects that might \"conceivably\" have practical bearings you \"conceive\" the objects of your \"conception\" to have. Then, your \"conception\" of those effects is the whole of your \"conception\" of the object.\nFrom a pragmatic viewpoint of the kind often ascribed to James, defining on a particular occasion whether a particular belief can rightly be said to be both true and justified is seen as no more than an exercise in pedantry, but being able to discern whether that belief led to fruitful outcomes is a fruitful enterprise. Peirce emphasized fallibilism, considered the assertion of absolute certainty a barrier to inquiry, and in 1901 defined truth as follows: \"Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth.\" In other words, any unqualified assertion is likely to be at least a little wrong or, if right, still right for not entirely the right reasons. Therefore one is more veracious by being Socratic, including a recognition of one's own ignorance and knowing one may be proved wrong. This is the case, even though in practical matters one sometimes must act, if one is to act at all, with decision and complete confidence.\n\nThe difficulties involved in producing a viable fourth condition have led to claims that attempting to repair the JTB account is a deficient strategy. For example, one might argue that what the Gettier problem shows is not the need for a fourth independent condition in addition to the original three, but rather that the attempt to build up an account of knowledging by conjoining a set of independent conditions was misguided from the outset. Those who have adopted this approach generally argue that epistemological terms like justification, evidence, certainty, etc. should be analyzed in terms of a primitive notion of \"knowledge,\" rather than vice versa. Knowledge is understood as \"factive,\" that is, as embodying a sort of epistemological \"tie\" between a truth and a belief. The JTB account is then criticized for trying to get and encapsulate the factivity of knowledge \"on the cheap,\" as it were, or via a circular argument, by replacing an irreducible notion of factivity with the conjunction of some of the properties that accompany it (in particular, truth and justification). Of course, the introduction of irreducible primitives into a philosophical theory is always problematical (some would say a sign of desperation), and such anti-reductionist accounts are unlikely to please those who have other reasons to hold fast to the method behind JTB+G accounts.\n\nFred Dretske (1971) developed an account of knowledge which he called \"conclusive reasons\", revived by Robert Nozick as what he called the \"subjunctive\" or truth-tracking account (1981). Nozick's formulation posits that proposition P is an instance of knowledge when:\n\n\nNozick's definition is intended to preserve Goldman's intuition that Gettier cases should be ruled out by disacknowledging \"accidentally\" true justified beliefs, but without risking the potentially onerous consequences of building a causal requirement into the analysis. This tactic though, invites the riposte that Nozick's account merely hides the problem and does not solve it, for it leaves open the question of \"why\" Smith would not have had his belief if it had been false. The most promising answer seems to be that it is because Smith's belief was \"caused\" by the truth of what he believes; but that puts us back in the causalist camp.\n\nCriticisms and counter examples (notably the \"Grandma case\") prompted a revision, which resulted in the alteration of (3) and (4) to limit themselves to the same method (i.e. vision):\n\n\nSaul Kripke has pointed out that this view remains problematic and uses a counterexample called the \"Fake Barn Country example\", which describes a certain locality containing a number of fake barns or facades of barns. In the midst of these fake barns is one real barn, which is painted red. There is one more piece of crucial information for this example: the fake barns cannot be painted red.\n\nJones is driving along the highway, looks up and happens to see the real barn, and so forms the belief\n\n\nThough Jones has gotten lucky, he could have just as easily been deceived and not have known it. Therefore it doesn't fulfill premise 4, for if Jones saw a fake barn he wouldn't have any idea it was a fake barn. So this is not knowledge.\n\nAn alternate example is if Jones looks up and forms the belief\n\n\nAccording to Nozick's view this fulfills all four premises. Therefore this is knowledge, since Jones couldn't have been wrong, since the fake barns cannot be painted red. This is a troubling account however, since it seems the first statement \"I see a barn\" can be inferred from \"I see a red barn\"; however by Nozick's view the first belief is \"not\" knowledge and the second is knowledge.\n\nIn the first chapter of his book \"Pyrronian Reflexions on Truth and Justification\", Robert Fogelin gives a diagnosis that leads to a dialogical solution to Gettier's problem. The problem always arises when the given justification has nothing to do with what really makes the proposition true. Now, he notes that in such cases there is always a mismatch between the information disponible to the person who makes the knowledge-claim of some proposition p and the information disponible to the evaluator of this knowledge-claim (even if the evaluator is the same person in a later time). A Gettierian counterexample arises when the justification given by the person who makes the knowledge-claim cannot be accepted by the knowledge evaluator because it does not fit with his wider informational setting. For instance, in the case of the fake barn the evaluator knows that a superficial inspection from someone who does not know the peculiar circumstances involved isn't a justification aceptable as making the proposition p (that it is a real barn) true..\nRichard Kirkham has proposed that it is best to start with a definition of knowledge so strong that giving a counterexample to it is logically impossible. Whether it can be weakened without becoming subject to a counterexample should then be checked. He concludes that there will always be a counterexample to any definition of knowledge in which the believer's evidence does not logically necessitate the belief. Since in most cases the believer's evidence does not necessitate a belief, Kirkham embraces skepticism about knowledge. He notes that a belief can still be rational even if it is not an item of knowledge. (see also: fallibilism)\n\nOne might respond to Gettier by finding a way to avoid his conclusion(s) in the first place. However, it can hardly be argued that knowledge is justified true belief if there are cases that are justified true belief without being knowledge; thus, those who want to avoid Gettier's conclusions have to find some way to defuse Gettier's counterexamples. In order to do so, within the parameters of the particular counter-example or exemplar, they must then either accept that\n\n\nor, demonstrate a case in which it is possible to circumvent surrender to the exemplar by eliminating any necessity for it to be considered that JTB apply in just those areas that Gettier has rendered obscure, without thereby lessening the force of JTB to apply in those cases where it actually is crucial.\nThen, though Gettier's cases \"stipulate\" that Smith has a certain belief and that his belief is true, it seems that in order to propose (1), one must argue that Gettier, (or, that is, the writer responsible for the particular form of words on this present occasion known as case (1), and who makes assertion's about Smith's \"putative\" beliefs), goes wrong because he has the wrong notion of \"justification.\" Such an argument often depends on an externalist account on which \"justification\" is understood in such a way that whether or not a belief is \"justified\" depends not just on the internal state of the believer, but also on how that internal state is related to the outside world. Externalist accounts typically are constructed such that Smith's putative beliefs in Case I and Case II are not really justified (even though it seems to Smith that they are), because his beliefs are not lined up with the world in the right way, or that it is possible to show that it is invalid to assert that \"Smith\" has any significant \"particular\" belief at all, in terms of JTB or otherwise. Such accounts, of course, face the same burden as causalist responses to Gettier: they have to explain what sort of relationship between the world and the believer counts as a justificatory relationship.\n\nThose who accept (2) are by far in the minority in analytic philosophy; generally those who are willing to accept it are those who have independent reasons to say that more things count as knowledge than the intuitions that led to the JTB account would acknowledge. Chief among these are epistemic minimalists such as Crispin Sartwell, who hold that all true belief, including both Gettier's cases and lucky guesses, counts as knowledge.\n\nSome early work in the field of experimental philosophy suggested that traditional intuitions about Gettier cases might vary cross-culturally. However, subsequent studies have consistently failed to replicate these results, instead finding that participants from different cultures do share the traditional intuition. Indeed, more recent studies have actually been providing evidence for the opposite hypothesis, that people from a variety of different cultures have surprisingly similar intuitions in these cases.\n\n"}
{"id": "45152804", "url": "https://en.wikipedia.org/wiki?curid=45152804", "title": "Growth and underinvestment", "text": "Growth and underinvestment\n\nThe Growth and Underinvestment Archetype is one of the common system archetype patterns defined as part of the system dynamics discipline.\n\nSystem dynamics is an approach which strives to understand, describe and optimize nonlinear behaviors of complex systems over time, using tools such as feedback loops in order to find a leverage point of the system. As part of this discipline, several commonly found patterns of system behavior were found, named and described in detail. The Growth and Underinvestment Archetype is one of such patterns.\n\nThe system described in the Growth and Underinvestment Archetype consists of three feedback loops. Each feedback loop can be one of two types:\n\nThe reinforcing loop consists of a growing action, such as units of a specific product shipped to a customer, and a current state, such as current demand for a specific product. The growing action causes a positive increase in the current state. The increase of the current state then in turn causes a positive increase of the growing action, thereby creating the reinforcing characteristic of the loop.\n\nAs discussed above, this reinforcing loop would have exponential behavior in time, if its growth wouldn’t be bound by the combination of the two balancing loops present in the system.\n\nThe first balancing loop is directly connected to the reinforcing loop via the current state variable. The first balancing loop consists of a current state and a slowing action (for example exceeding capacity limits). The growth of the current state causes the growth of the slowing action. The growth of the slowing action in turn reduces the current state, thereby creating a balancing loop.\n\nOne example of this balancing loop is a situation where a number of units manufactured is increasing (current state), which causes the manufacturing utilization to increase (end eventually exceed capacity). This will make each additional unit of manufacturing more expensive, reducing the growth in units manufacture. One can note that a rubber-banding effect occurs, since the more units are manufactured, the more expensive the manufacturing is. This loop taken in isolation would eventually find a stable state, independently of its beginning state.\n\nThe second balancing loop is what differentiates the Growth and Underinvestment Archetype from other archetypes. It is directly connected to the first balancing loop via the slowing action variable. The balancing loop consists of several elements:\n\n\nFirst, the growth of the slowing action causes growth of the perceived need for investment (e.g. building additional manufacturing capacity). Another factor that can positively contribute to the perceived need to invest is the failure to uphold the performance standard (for example manufacturing error rate). The perceived need to invest positively translates into actually making the investment. The investment made then negatively influences the slowing action (e.g. removal of capacity limits).\n\nThe last element of the second balancing loop is the delay in investment, which happens for a variety of reasons, for example hesitation of management to invest in additional capacity.\n\nThe key to understanding the Growth and Underinvestment Archetype is in the delay in investment. This delay causes the second balancing loop to have longer cycle times than the first balancing loop. That in turn has the following effect:\n\nSince the second balancing loop has a shorter loop cycle, it causes the current state to be reduced, which in turn decreases the slowing action. This happens before an investment is made, in effect reducing the perceived need for investment.\n\nIn effect, the first and second reinforcing loop act together as a reinforcing loop to restrict growth.\n\nIf it were not for the delay, the whole system would work optimally thanks to timely investments.\n\nAt least two factors can contribute to the difficulty of identifying the Growth and Underinvestment Archetype in real-world complex systems.\n\nFirst, the archetype can be temporarily covered up by shifting the burden, that is, by trying to solve the underlying problem by a symptomatic solution, instead of a fundamental one. This leads to further delaying the investment decision, narrowing the window for effective and timely investment or missing it entirely.\n\nSecond, in order to recognize the archetype, a holistic view of the system is required. This can be difficult, since the Growth and Underinvestment Archetype can create many issues that management must attend to, in effect preventing them from stepping back and seeing the bigger picture.\n\nWhen discussing how to optimize the system, it can be beneficial to discuss what a leverage point is.\n\nThe leverage point in the system is a place where structural changes can lead to significant and lasting improvements to the system. There are two kinds of leverage points:\n\nWhen dealing with this archetype, several generic strategies can be considered in order to solve the problem the archetype presents.\n\nThe first strategy to consider is whether it is possible to shorten the delay between the perceived need to invest and actually making an investment.\n\nOne tool one can utilize in order to shorten the delay is Business Process Management, a field of study focusing on improving the efficiency of business processes. With its help, we might be able to identify the excessive delays in the investment process and shorten the delays or eliminate the parts of process that cause it entirely.\n\nWhen the reduction of investment delay is not possible, consider having a plan in advance. This includes monitoring the right key performance indicators (some KPIs such as utilization rate might act as an inhibitor for investment, since they frown upon unused capacity) and have an investment plan prepared in advance.\n\nSuch plan can also include a stop-gap solution that can temporarily weaken the growth inhibitor, such as hiring outside help in the form of contractors or lending additional capacity. But beware to not let the stop-gap solution become a permanent one, which could become a Shifting the Burden archetype.\n\nA new home delivery-focused pizzeria opens up in the neighborhood. At first, the demand is low, but the pizza’s quality is excellent, as well as the delivery times. After a while, the pizzeria gets noticed and is featured in a local online food blog. As a result, the demand for the pizza rises sharply. But the pizzeria owners are reluctant to purchase more delivery capacity (pizza delivery vehicles and personnel) along with higher pizza production capacity (additional pizza ovens). That results in higher delivery times and a larger percentage of undercooked pizzas, in turn lowering the number of returning customers. As a result, the pressure for additional investment in both delivery and production capacity is eliminated. The pizzeria owners are happy that they held off on the additional investment.\n\nSuch an example clearly represents a missed opportunity for further growth. It could have been avoided in two ways:\n\nThe application of the Growth and Underinvestment Archetype can be especially crucial for startup businesses, which need to grow fast or might have to face failure to raise additional funds. For them, the growing concern is a going concern.\n\nA new startup company focused on developing mobile gaming experiences has recently released its first game after successfully completing the first round of raising capital from investors. The game is initially priced at $5.99 in the Store. After the initial release, the game starts gaining a little bit of traction, but not enough to be considered a success. The company operates as usual, adding more content into the game and fixing bugs. Also, the game has an online component that is sized well for the current audience.\n\nAfter several weeks, the company comes to a major decision. It will re-release the game as free, instead focusing on selling additional content on the form in in-app purchases. The strategy works and many new users start playing the game. This has two effects:\n\nShortly after the free version of the game comes out, the influx of players starts to affect the online component, which occasionally crashes and disconnects users, causing them to save progress they have made in the game. The company redeploys its resources and tries to mitigate the situation by incrementally improving the online component. It is clear, however, that a complete rewrite of the online component is needed on order to eliminate the problem entirely. Therefore the company contacts its investors in order to raise additional funds to rebuild the online component.\n\nMeanwhile, the number of active players dwindles. In response to this fact, as well as the weak cash flow generated by the game, the investor decides to take time to make the investment decision. Unfortunately, the cash flow from the game is not improving, since the remaining user base purchased the content they were interested in and new content is delayed, since most of the developers have been reassigned to solving the online component woes. In response to this, the investor sees the ever-flattening sales and dwindling user base and decides not to invest further resources into the company. A few weeks later, the company runs out of funds and declares bankruptcy.\n\nHow such a situation could be prevented:\n\nThe Growth and Underinvestment with a Drifting Standard is a special case of the archetype.\n\nIt adds an additional relationship between the slowing action and the performance standard. When the slowing action is growing (e.g. the backlog of order is increasing in size), it has a negative effect on the performance standard (e.g. raising the maximum permitted time it takes to deliver an order). The rest of the system behaves in the same way as the original archetype.\n\nThis additional relationship can have severe consequences, since in some cases the performance standard can have major contribution to pressure exerted on individuals deciding whether to make the investment. With the slowing action actively undermining the performance standard, it can be harder to find the incentive to invest into additional resources.\n\nThe Growth and Underinvestment Archetype can be considered to be an elaboration o the Limits to Success archetype. It adds another feedback loop which effectively elaborates the Limiting State part of the Limits to Success archetype.\n"}
{"id": "52731546", "url": "https://en.wikipedia.org/wiki?curid=52731546", "title": "Guilt-Shame-Fear spectrum of cultures", "text": "Guilt-Shame-Fear spectrum of cultures\n\nIn cultural anthropology, the distinction between a guilt society (or guilt culture), shame society (also shame culture or honor-shame culture), and a fear society (or culture of fear) has been used to categorize different cultures. The differences can apply to how behavior is governed with respect to government laws, business rules, or social etiquette. This classification has been applied especially to apollonian societies, sorting them according to the emotions they use to control individuals (especially children) and maintaining social order, swaying them into norm obedience and conformity. \n\n\nThe terminology were popularized by Ruth Benedict in \"The Chrysanthemum and the Sword\", who described American culture as a \"guilt culture\" and Japanese culture as a \"shame culture\".\n\nThough the same person may emphasize different considerations depending on the situation, government and business projects that bring together people from different types of cultures may experience problems.\n\nIn a guilt society, the primary method of social control is the inculcation of feelings of guilt for behaviors that the individual believes to be undesirable. A prominent feature of guilt societies is the provision of sanctioned releases from guilt for certain behaviors, whether before or after the fact. There is opportunity in such cases for authority figures to derive power, monetary and/or other advantages, etc. by manipulating the conditions of guilt and the forgiveness of guilt.\n\nPaul Hiebert characterizes the guilt society as follows:\n\n\nAnglo-Saxon England is particularly notable as a shame culture, and this trait survived even after its conversion to Christianity, which is typically a guilt culture. Other examples of shame culture under Christianity are the cultures of Mexico, Andalusia and generally Christian Mediterranean societies.\n\nIn China, the concept of shame is widely accepted due to Confucian teachings. In \"Analects\", Confucius is quoted as saying:\n\nLead the people with administrative injunctions and put them in their place with penal law, and they will avoid punishments but will be without a sense of shame. Lead them with excellence and put them in their place through roles and ritual practices, and in addition to developing a sense of shame, they will order themselves harmoniously.\n\nThe society of traditional Japan was long held to be a good example of one in which shame is the primary agent of social control. The first book to cogently explain the workings of the Japanese society for the Western reader was \"The Chrysanthemum and the Sword\" by Ruth Benedict. This book was produced under less than ideal circumstances since it was written during the early years of World War II in an attempt to understand the people who had become such a powerful enemy of the West. Under the conditions of war, it was impossible to do field research in Japan.\n\nWithout being able to study in Japan, Benedict relied on newspaper clippings, histories, literature, films, and interviews of Japanese-Americans. Her studies came to conclusions about Japanese culture and society that are still widely criticized today, both in America and Japan.\n\nTo the Roma, though living as local minorities in mostly Christian or Islamic societies, the concept of \"lajav\" (\"shame\") is important, while the concept of \"bezax\" (\"sin\") does not have such significance.\n\n\n\n\n"}
{"id": "505526", "url": "https://en.wikipedia.org/wiki?curid=505526", "title": "HAKMEM", "text": "HAKMEM\n\nHAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\".\nContributors included about two dozen members and associates of the AI Lab. The title of the report is short for \"hacks memo\", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).\n\nHAKMEM is notable as an early compendium of algorithmic technique, particularly for its practical bent, and as an illustration of the wide-ranging interests of AI Lab people of the time, which included almost anything other than AI research.\n\nHAKMEM contains original work in some fields, notably continued fractions.\n\n\n"}
{"id": "20744016", "url": "https://en.wikipedia.org/wiki?curid=20744016", "title": "Health management system", "text": "Health management system\n\nThe health management system (HMS) is an evolutionary medicine regulative process proposed by Nicholas Humphrey in which actuarial assessment of fitness and economic-type cost–benefit analysis determines the body’s regulation of its physiology and health. This incorporation of cost–benefit calculations into body regulation provides a science grounded approach to mind–body phenomena such as placebos that are otherwise not explainable by low level, noneconomic, and purely feedback based homeostatic or allostatic theories.\n\n\nPlacebos are explained as the result of false information about the availability of external treatment and support that mislead the health management system into not deploying evolved self-treatments. This results in the placebo suppression of medical symptoms.\n\nSince Hippocrates, it has been recognized that the body has self-healing powers (vis medicatrix naturae). Modern evolutionary medicine identifies them with physiologically based self-treatments that provide the body with prophylactic, healing, or restorative capabilities against injuries, infections and physiological disruption. Examples include:\n\n\nThese evolved self-treatments deployed by the body are experienced by humans as unpleasant and unwanted illness symptoms.\n\nSuch self-treatments according to evolutionary medicine are deployed to increase an individual’s biological fitness.\n\nTwo factors affect their deployment.\n\nFirst, it is usually advantageous to deploy them on a precautionary basis. As a result, it will often turn out that they have been deployed apparently unnecessarily, though this has in fact been advantageous since in probabilistic terms they have provided an insurance against a potentially costly outcome. As Nesse notes: \"Vomiting, for example, may cost only a few hundred calories and a few minutes, whereas not vomiting may result in a 5% chance of death\" page 77.\n\nSecond, self-treatments are costly both in using energy, and also in their risk of damaging the body.\n\n\nOne factor in deployment is low level physiological control by proinflammatory cytokines such as IL-1 triggered by bacterial lipopolysaccharides (LPS).\n\nAnother is higher level control in which the brain takes into account what it learns about circumstances and how that makes it well and ill. Conditioning shows the existence of such learnt control: give saccharin paired in a drink with a drug that creates immunosuppression, and later on, giving saccharin alone will produce immunosuppression. Such conditioning happens both in experimental rodents and humans.\n\nEvolution, according to Nicholas Humphrey, has selected an internal health management system that uses cost benefit analysis upon whether the deployment of a self-treatment aids biological fitness, and so should be activated. \na specially designed procedure for “economic resource management” that is, I believe, one of the key features of the “natural health-care service” which has evolved in ourselves and other animals to help us deal throughout our lives with repeated bouts of sickness, injury, and other threats to our well-being.\n\nAn analogy is explicitly made with the health economics consideration used in management decisions involving external medical treatment.\n\nNow, if you wonder about this choice of managerial terminology for talking about biological healing systems, I should say that it is quite deliberate (and so is the pun on NHS.) With the phrase “natural health-care service” I do intend to evoke, at a biological level, all the economic connotations that are so much a part of modern health-care in society.\n\nExternal medications will affect the cost benefits advantages of deploying an evolved self-treatment. Some animals use external ones. Wild animals, including apes, do so in the form of ingested detoxifying clays, rough leaves that clear gut parasites, and pharmacologically active plants Complementary to this, research finds that animals have the ability to select and prefer substances that aid their recuperation from illness.\n\nThe welfare of social animals (including humans) depends upon other individuals (social buffering). The actuarial assessments of the costs and benefits of deploying a self-treatment therefore will depend upon the presence, or not, of other individuals. The presence of helpful others will affect, for example, the risk of predators when incapacitated, and—in those case in which animals do this (such as humans)—the provision of food, and care during sickness.\n\nThe health management system factors in the presence of such external treatment and social support as one aspect of the circumstances needed to determine whether it is advantageous to deploy or not an evolved self-treatment.\n\nAll humans societies use external medications, and some individuals exist that are considered to have special healing knowledge about illnesses and their treatments. Humans are also usually supportive to those in their group. The availability of these things will affect the cost benefits of the body deploying its own biological ones. This could, in turn, lead to the health management system (given its beliefs (information) about treatments and support) to deploy or not, or doing so differently, the body’s own treatments.\n\nNicholas Humphrey describes how the health management system explains placebos – an external treatment without direct physiological effects – as follows:\nSuppose, for example, a doctor gives someone who is suffering an infection a pill that she rightly believes to contain an antibiotic: because her hopes will be raised she will no doubt make appropriate adjustments to her health-management strategy – lowering her precautionary defences in anticipation of the sickness not lasting long.\n\nThe health management system, in other words, when faced with an infection is tricked into making a mistaken cost benefit analysis using false information. The effect of that false information is that the benefits of the self-treatment cease to outweigh its costs. As a result, it is not deployed, and an individual does not experience unwanted medical symptoms.\n\nFailure to deploy an evolved self-treatment need not put an individual at risk since evolution has advantaged their deployment on a precautionary basis. As Nicholas Humphrey notes:\n\nTherefore, not deploying an evolved self-treatment, and so not having a medical symptom due to placebo false information might be without consequence.\n\nThe health management system’s idea of a top down neural control of the body is also found in the idea that a central governor regulates muscle fatigue to protect the body from the harmful effects (such as anoxia and hyperglycemia) of over prolonged exercise.\n\nThe idea of a fatigue governor was first proposed in 1924 by the 1922 Nobel Prize winner Archibald Hill, and more recently, on the basis of modern research, by Tim Noakes.\n\nLike with the health management system, the central governor shares the idea that much of what is attributed to low level feedback homeostatic regulation is, in fact, due to top down control by the brain. The advantage of this top down management is that the brain can enhance such regulation by allowing it to be modified by information. For example, in endurance running, a cost benefit trade exists off between the advantages of continuing to run, and the risk if this is too prolonged that it might harm the body. Being able to regulate fatigue in terms of information about the benefits and costs of continued exercise would enhance biological fitness.\n\nLow level theories exist that suggest that fatigue is due mechanical failure of the exercising muscles (\"peripheral fatigue\"). However, such low level theories do not explain why running muscle fatigue is affected by information relevant to cost benefit trade offs. For example, marathon runners can carry on running longer if told they are near the finishing line, than far away. The existence of a central governor can explain this effect.\n\n\n"}
{"id": "318742", "url": "https://en.wikipedia.org/wiki?curid=318742", "title": "Hyperbolic space", "text": "Hyperbolic space\n\nIn mathematics, hyperbolic space is a homogeneous space that has a constant negative curvature, where in this case the curvature is the sectional curvature.\nIt is hyperbolic geometry in more than 2 dimensions, and is distinguished from Euclidean spaces with zero curvature that define the Euclidean geometry, and elliptic geometry that have a constant positive curvature.\n\nWhen embedded to a Euclidean space (of a higher dimension), every point of a hyperbolic space is a saddle point. Another distinctive property is the amount of space covered by the \"n\"-ball in hyperbolic \"n\"-space: it increases exponentially with respect to the radius of the ball for large radii, rather than polynomially.\n\nHyperbolic \"n\"-space, denoted H, is the maximally symmetric, simply connected, \"n\"-dimensional Riemannian manifold with a constant negative sectional curvature. \nHyperbolic space is a space exhibiting hyperbolic geometry. \nIt is the negative-curvature analogue of the \"n\"-sphere. Although hyperbolic space H is diffeomorphic to R, its negative-curvature metric gives it very different geometric properties.\n\nHyperbolic 2-space, H, is also called the hyperbolic plane.\n\nHyperbolic space, developed independently by Nikolai Lobachevsky and János Bolyai, is a geometrical space analogous to Euclidean space, but such that Euclid's parallel postulate is no longer assumed to hold. Instead, the parallel postulate is replaced by the following alternative (in two dimensions):\nIt is then a theorem that there are infinitely many such lines through \"P\". This axiom still does not uniquely characterize the hyperbolic plane up to isometry; there is an extra constant, the curvature , which must be specified. However, it does uniquely characterize it up to homothety, meaning up to bijections which only change the notion of distance by an overall constant. By choosing an appropriate length scale, one can thus assume, without loss of generality, that .\n\nModels of hyperbolic spaces that can be embedded in a flat (e.g. Euclidean) spaces may be constructed. In particular, the existence of model spaces implies that the parallel postulate is logically independent of the other axioms of Euclidean geometry.\n\nThere are several important models of hyperbolic space: the Klein model, the hyperboloid model, the Poincaré ball model and the Poincaré half space model. These all model the same geometry in the sense that any two of them can be related by a transformation that preserves all the geometrical properties of the space, including isometry (though not with respect to the metric of a Euclidean embedding).\n\nThe hyperboloid model realizes hyperbolic space as a hyperboloid in R = {(\"x\"...,\"x\")|\"x\"∈R, \"i\"=0,1...,\"n\"}. The hyperboloid is the locus H of points whose coordinates satisfy\nIn this model a \"line\" (or geodesic) is the curve formed by the intersection of H with a plane through the origin in R.\n\nThe hyperboloid model is closely related to the geometry of Minkowski space. The quadratic form\nwhich defines the hyperboloid, polarizes to give the bilinear form\nThe space R, equipped with the bilinear form \"B\", is an (\"n\"+1)-dimensional Minkowski space R.\n\nOne can associate a \"distance\" on the hyperboloid model by defining the distance between two points \"x\" and \"y\" on H to be\nThis function satisfies the axioms of a metric space. It is preserved by the action of the Lorentz group on R. Hence the Lorentz group acts as a transformation group preserving isometry on H.\n\nAn alternative model of hyperbolic geometry is on a certain domain in projective space. The Minkowski quadratic form \"Q\" defines a subset given as the locus of points for which in the homogeneous coordinates \"x\". The domain \"U\" is the Klein model of hyperbolic space.\n\nThe lines of this model are the open line segments of the ambient projective space which lie in \"U\". The distance between two points \"x\" and \"y\" in \"U\" is defined by\nThis is well-defined on projective space, since the ratio under the inverse hyperbolic cosine is homogeneous of degree 0.\n\nThis model is related to the hyperboloid model as follows. Each point corresponds to a line \"L\" through the origin in R, by the definition of projective space. This line intersects the hyperboloid H in a unique point. Conversely, through any point on H, there passes a unique line through the origin (which is a point in the projective space). This correspondence defines a bijection between \"U\" and H. It is an isometry, since evaluating along reproduces the definition of the distance given for the hyperboloid model.\n\nA closely related pair of models of hyperbolic geometry are the Poincaré ball and Poincaré half-space models.\n\nThe ball model comes from a stereographic projection of the hyperboloid in R onto the hyperplane {\"x\" = 0}. In detail, let \"S\" be the point in R with coordinates (−1,0,0...,0): the \"South pole\" for the stereographic projection. For each point \"P\" on the hyperboloid H, let \"P\" be the unique point of intersection of the line \"SP\" with the plane {\"x\" = 0}.\n\nThis establishes a bijective mapping of H into the unit ball\nin the plane {\"x\" = 0}.\n\nThe geodesics in this model are semicircles that are perpendicular to the boundary sphere of \"B\". Isometries of the ball are generated by spherical inversion in hyperspheres perpendicular to the boundary.\n\nThe half-space model results from applying inversion in a circle with centre a boundary point of the Poincaré ball model \"B\" above and a radius of twice the radius.\n\nThis sends circles to circles and lines, and is moreover a conformal transformation. Consequently, the geodesics of the half-space model are lines and circles perpendicular to the boundary hyperplane.\n\nEvery complete, connected, simply connected manifold of constant negative curvature −1 is isometric to the real hyperbolic space H. As a result, the universal cover of any closed manifold \"M\" of constant negative curvature −1, which is to say, a hyperbolic manifold, is H. Thus, every such \"M\" can be written as H/Γ where Γ is a torsion-free discrete group of isometries on H. That is, Γ is a lattice in SO(\"n\",1).\n\nTwo-dimensional hyperbolic surfaces can also be understood according to the language of Riemann surfaces. According to the uniformization theorem, every Riemann surface is either elliptic, parabolic or hyperbolic. Most hyperbolic surfaces have a non-trivial fundamental group π=Γ; the groups that arise this way are known as Fuchsian groups. The quotient space H²/Γ of the upper half-plane modulo the fundamental group is known as the Fuchsian model of the hyperbolic surface. The Poincaré half plane is also hyperbolic, but is simply connected and noncompact. It is the universal cover of the other hyperbolic surfaces.\n\nThe analogous construction for three-dimensional hyperbolic surfaces is the Kleinian model.\n\n\n"}
{"id": "55627940", "url": "https://en.wikipedia.org/wiki?curid=55627940", "title": "L. Gordon Rylands", "text": "L. Gordon Rylands\n\nLouis Gordon Rylands (July 10, 1862 - December 20, 1942), best known as L. Gordon Rylands was a British criminologist and writer.\n\nRylands was born in Warrington, Lancashire. His father was the politician Peter Rylands. He was educated at Charterhouse and University College London (B.A., B.Sc). After completing his studies, he was secretary of the family business, Rylands Brothers wire manufacturers from 1884 to 1887, then from 1897 to 1898 was science master at the Royal Grammar School, Newcastle upon Tyne. He worked as a private tutor from 1898 to 1924. At the time of his death he was assistant registrar of Grimes's Manchester Tutorial College. In 1887, he married Katherine Edwards; they had a daughter. Rylands was an uncle of the academic Dadie Rylands.\n\nRylands authored the book \"Crime: Its Causes and Remedy\" (1889). Rylands was best known as an advocate of the Christ myth theory. He denied the historicity of Jesus.\n\n"}
{"id": "4665628", "url": "https://en.wikipedia.org/wiki?curid=4665628", "title": "Latent inhibition", "text": "Latent inhibition\n\nLatent inhibition is a technical term used in classical conditioning to refer to the observation that a familiar stimulus takes longer to acquire meaning (as a signal or conditioned stimulus) than a new stimulus. The term \"latent inhibition\" dates back to Lubow and Moore. The LI effect is \"latent\" in that it is not exhibited in the stimulus pre-exposure phase, but rather in the subsequent test phase. \"Inhibition\", here, simply connotes that the effect is expressed in terms of relatively poor learning. The LI effect is extremely robust, appearing in all mammalian species that have been tested and across many different learning paradigms, thereby suggesting some adaptive advantages, such as protecting the organism from associating irrelevant stimuli with other, more important, events. \n\nThe LI effect has received a number of theoretical interpretations. One class of theory holds that inconsequential stimulus pre-exposure results in reduced associability for that stimulus. The loss of associability has been attributed to a variety of mechanisms that reduce attention, which then must be reacquired in order for learning to proceed normally. Alternatively, it has been proposed that LI is a result of retrieval failure rather than acquisition failure. Such a position advocates that, following stimulus pre-exposure, the acquisition of the new association to the old stimulus proceeds normally. However, in the test stage, two associations (the stimulus-no consequence association from the pre-exposure stage and the stimulus-consequence stimulus association of the acquisition stage) are retrieved and compete for expression. The group not pre-exposed to the stimulus performs better than the pre-exposed group because for the first group there is only the second association to be retrieved.\n\nLI is affected by many factors, one of the most important of which is context. In virtually all LI studies, the context remains the same in the stimulus pre-exposure and test phases. However, if context is changed from the pre-exposure to the test phase, then LI is severely attenuated. The context-dependency of LI plays major roles in all current theories of LI, and in particular to their applications to schizophrenia, where it has been proposed that relationship between the pre-exposed stimulus and the context breaks down; context no longer sets the occasion for the expression of the stimulus-no consequence association. Consequently, working-memory is inundated with experimentally familiar but phenomenally novel stimuli, each competing for the limited resources required for efficient information processing. This description fits well with the positive symptoms of schizophrenia, particularly high distractibility, as well as with research findings.\n\nThe assumption that the attentional process that produces LI in normal subjects is dysfunctional in schizophrenia patients has stimulated considerable research, with humans, as well as with rats and mice. There is much data that indicate that dopamine agonists and antagonists modulate LI in rats and in normal humans. Dopamine agonists, such as amphetamine, abolish LI while dopamine antagonists, such as haloperidol and other anti-psychotic drugs, produce a super-LI effect. In addition, manipulations of putative dopamine pathways in the brain also have the expected effects on LI. Thus, hippocampal and septal lesions interfere with the development of LI, as do lesions in selective portions of the nucleus accumbens. With human subjects, there is evidence that acute, non-medicated schizophrenics show reduced LI compared to chronic, medicated schizophrenics and to healthy subjects, while there is no difference in the amount of LI in the latter two groups. Finally, symptomatically normal subjects who score high on self-report questionnaires that measure psychotic-proneness or schizotypality also exhibit reduced LI compared to those who score low on the scales.\n\nIn addition to LI illustrating a fundamental strategy for information processing and providing a useful tool for examining attentional dysfunctions in pathological groups, the LI procedure has been used to screen for drugs that can ameliorate schizophrenia symptoms LI. LI has also been used to explain why certain therapies, such as alcohol aversion treatments, are not as effective as might be expected. On the other hand, LI procedures may be useful in counteracting some of the undesirable side-effects that frequently accompany radiation and chemo-therapies for cancer, as for example food aversion. LI research also has suggested techniques that may be efficacious in the prophylactic treatment of certain fears and phobias. Of popular interest, several studies have attempted to relate LI to creativity.\n\nIn summary, the basic LI phenomenon represents some output of a selective attention process that results in learning to ignore irrelevant stimuli. It has become an important tool for understanding information processing in general, as well as attentional dysfunctions in schizophrenia, and it has implications for a variety of practical problems.\n\nMost people are able to ignore the constant stream of incoming stimuli, but this capability is reduced in those with low latent inhibition. Low latent inhibition (that may resemble hyper-activity or attention deficit hyperactivity disorder (ADHD) in early decades of the individual life) seems to often correlate with distracted behaviors. This distractedness can manifest itself as general inattentiveness, a tendency to switch subjects without warning in conversation, and other absentminded habits. This is not to say that all distractedness can be explained by low latent inhibition, nor does it necessarily follow that people with low LI will have a hard time paying attention. It does mean, however, that the higher quantity of incoming information requires a mind capable of handling it. \nThose of above average intelligence are thought to be capable of processing this stream effectively, enabling their creativity and increasing their awareness of their surroundings. Those with average and, less than average intelligence, on the other hand, are less able to cope and as a result are more likely to suffer from mental illness and sensory overload. It is hypothesized that a low level of latent inhibition can cause either psychosis or a high level of creative achievement or both, which is usually dependent on the individual's intelligence. When they cannot develop the creative ideas, they become frustrated and/or depressive.\n\nHigh levels of the neurotransmitter dopamine (or its agonists) in the ventral tegmental area of the brain have been shown to decrease latent inhibition. Certain dysfunctions of the neurotransmitters glutamate, serotonin and acetylcholine have also been implicated.\n\nLow latent inhibition is not a mental disorder but an observed personality trait , and a description of how an individual absorbs and assimilates data or stimuli. Furthermore, it does not necessarily lead to mental disorder or creative achievement—this is, like many other factors of life, a case of environmental and predispositional influences, whether these be positive (e.g., education) or negative (e.g., abuse) in nature.\n\n\n\n"}
{"id": "2296282", "url": "https://en.wikipedia.org/wiki?curid=2296282", "title": "Lawlessness", "text": "Lawlessness\n\nLawlessness is a lack of law, in any of the various senses of that word. Lawlessness may describe various conditions.\nAnomie is a breakdown of social bonds between an individual and their community, in which individuals do not feel bound by the moral strictures of society. The term was popularized by French sociologist Émile Durkheim in his influential 1897 book \"Suicide\".\n\nAnarchy (meaning \"without leadership\") is a condition in which a person or group of people reject societal hierarchies, laws, and other institutions. It often entails the dissolution of government.\n\nAnarchism is a political philosophy that advocates self-governed societies based on voluntary institutions.\n\nCivil disorder, or civil unrest, refers to public disturbances generally involving groups of people, and resulting in danger or damage to persons or property. Civil disorder is a breakdown of civil society, and may be a form of protest. It may take various forms, such as illegal parades, sit-ins, riots, sabotage, and other forms of crime.\n\nRandomness is the lack of pattern or predictability in events.\n\nAntinomianism, in Christianity, is a theological position which takes the principle of salvation by faith and divine grace to the point of asserting that the saved are not bound to follow the Law of Moses.\n\n"}
{"id": "477815", "url": "https://en.wikipedia.org/wiki?curid=477815", "title": "Lemuria (continent)", "text": "Lemuria (continent)\n\nLemuria or Limuria is a hypothetical \"lost land\" located either in the Indian or the Pacific Ocean, as postulated by a now-discredited 19th-century scientific theory. The idea was then adopted by the occultists of the time and consequently has been incorporated into pop culture.\n\nOriginally, Lemuria was hypothesized as a land bridge, now sunken, which would account for certain discontinuities in biogeography. This idea has been rendered obsolete by modern theories of plate tectonics. Sunken continents such as Zealandia in the Pacific, Mauritia and the Kerguelen Plateau in the Indian Ocean do exist, but no geological formation under the Indian or Pacific Oceans is known that could have served as a land bridge between continents.\n\nThe idea of Lemuria was subsequently incorporated into the proto-New Age philosophy of Theosophy and subsequently into general fringe belief. Accounts of Lemuria here differ. All share a common belief that a continent existed in ancient times and sank beneath the ocean as a result of a geological, often cataclysmic, change, such as pole shift, which such theorists anticipate will destroy and transform the modern world.\n\nIn 1864, \"The Mammals of Madagascar\" by zoologist and biogeographer Philip Sclater appeared in \"The Quarterly Journal of Science\". Using a classification he referred to as lemurs, but which included related primate groups, and puzzled by the presence of their fossils in both Madagascar and India, but not in Africa or the Middle East, Sclater proposed that Madagascar and India had once been part of a larger continent (he was correct in this; though in reality this was the supercontinent Pangaea).\n\nThe anomalies of the mammal fauna of Madagascar can best be explained by supposing that ... a large continent occupied parts of the Atlantic and Indian Oceans ... that this continent was broken up into islands, of which some have become amalgamated with ... Africa, some ... with what is now Asia; and that in Madagascar and the Mascarene Islands we have existing relics of this great continent, for which ... I should propose the name Lemuria!\n\nSclater's theory was hardly unusual for his time; \"land bridges\", real and imagined, fascinated several of Sclater's contemporaries. Étienne Geoffroy Saint-Hilaire, also looking at the relationship between animals in India and Madagascar, had suggested a southern continent about two decades before Sclater, but did not give it a name. The acceptance of Darwinism led scientists to seek to trace the diffusion of species from their points of evolutionary origin. Prior to the acceptance of continental drift, biologists frequently postulated submerged land masses to account for populations of land-based species now separated by barriers of water. Similarly, geologists tried to account for striking resemblances of rock formations on different continents. The first systematic attempt was made by Melchior Neumayr in his book \"Erdgeschichte\" in 1887. Many hypothetical submerged land bridges and continents were proposed during the 19th century to account for the present distribution of species.\n\nAfter gaining some acceptance within the scientific community, the concept of Lemuria began to appear in the works of other scholars. Ernst Haeckel, a Darwinian taxonomist, proposed Lemuria as an explanation for the absence of \"missing link\" fossil records. According to another source, Haeckel put forward this thesis prior to Sclater (but without using the name \"Lemuria\"). Locating the origins of the human species on this lost continent, he claimed the fossil record could not be found because it sank beneath the sea.\n\nOther scientists hypothesized that Lemuria had extended across parts of the Pacific Ocean, seeking to explain the distribution of various species across Asia and the Americas.\n\nThe Lemuria theory disappeared completely from conventional scientific consideration after the theories of plate tectonics and continental drift were accepted by the larger scientific community. According to the theory of plate tectonics, Madagascar and India were indeed once part of the same landmass (thus accounting for geological resemblances), but plate movement caused India to break away millions of years ago, and move to its present location. The original landmass, the supercontinent Gondwana, broke apart; it did not sink beneath sea level.\n\nSome Tamil writers such as Devaneya Pavanar have associated Lemuria with Kumari Kandam, a legendary sunken landmass mentioned in the Tamil literature, claiming that it was the cradle of civilization.\n\nSince the 1880s, the hypothesis of Lemuria has inspired many novels, television shows, films, and music.\n\n"}
{"id": "25372042", "url": "https://en.wikipedia.org/wiki?curid=25372042", "title": "Linked timestamping", "text": "Linked timestamping\n\nLinked timestamping is a type of trusted timestamping where issued time-stamps are related to each other.\n\nLinked timestamping creates time-stamp tokens which are dependent on each other, entangled in some authenticated data structure. Later modification of the issued time-stamps would invalidate this structure. The temporal order of issued time-stamps is also protected by this data structure, making backdating of the issued time-stamps impossible, even by the issuing server itself.\n\nThe top of the authenticated data structure is generally \"published\" in some hard-to-modify and widely witnessed media, like printed newspaper or public blockchain. There are no (long-term) private keys in use, avoiding PKI-related risks.\n\nSuitable candidates for the authenticated data structure include:\n\nThe simplest linear hash chain-based time-stamping scheme is illustrated in the following diagram:\nThe linking-based time-stamping authority (TSA) usually performs the following distinct functions:\n\n\n\n\nLinked timestamping is inherently more secure than the usual, public-key signature based time-stamping. All consequential time-stamps \"seal\" previously issued ones - hash chain (or other authenticated dictionary in use) could be built only in one way; modifying issued time-stamps is nearly as hard as finding a preimage for the used cryptographic hash function. Continuity of operation is observable by users; periodic publications in widely witnessed media provide extra transparency.\n\nTampering with absolute time values could be detected by users, whose time-stamps are relatively comparable by system design.\n\nAbsence of secret keys increases system trustworthiness. There are no keys to leak and hash algorithms are considered more future-proof than modular arithmetic based algorithms, e.g. RSA.\n\nLinked timestamping scales well - hashing is much faster than public key cryptography. There is no need for specific cryptographic hardware with its limitations.\n\nThe common technology for guaranteeing long-term attestation value of the issued time-stamps (and digitally signed data) is periodic over-time-stamping of the time-stamp token. Because of missing key-related risks and of the plausible safety margin of the reasonably chosen hash function this over-time-stamping period of hash-linked token could be an order of magnitude longer than of public-key signed token.\n\nHaber and Stornetta proposed in 1990 to link issued time-stamps together into linear hash-chain, using a collision-resistant hash function. The main rationale was to diminish TSA trust requirements.\n\nTree-like schemes and operating in rounds were proposed by Benaloh and de Mare in 1991 and by Bayer, Haber and Stornetta in 1992.\n\nBenaloh and de Mare constructed a one-way accumulator in 1994 and proposed its use in time-stamping. When used for aggregation, one-way accumulator requires only one constant-time computation for round membership verification.\n\nSurety started the first commercial linked timestamping service in January 1995. Linking scheme is described and its security is analyzed in the following article by Haber and Sornetta.\n\nBuldas et al. continued with further optimization and formal analysis of binary tree and threaded tree based schemes.\n\nSkip-list based time-stamping system was implemented in 2005; related algorithms are quite efficient.\n\nSecurity proof for hash-function based time-stamping schemes was presented by Buldas, Saarepera in 2004. There is an explicit upper bound formula_1 for the number of time stamps issued during the aggregation period; it is suggested that it is probably impossible to prove the security without this explicit bound - the so-called black-box reductions will fail in this task. Considering that all known practically relevant and efficient security proofs are black-box, this negative result is quite strong.\n\nNext, in 2005 it was shown that bounded time-stamping schemes with a trusted audit party (who periodically reviews the list of all time-stamps issued during an aggregation period) can be made \"universally composable\" - they remain secure in arbitrary environments (compositions with other protocols and other instances of the time-stamping protocol itself).\n\nBuldas, Laur showed in 2007 that bounded time-stamping schemes are secure in a very strong sense - they satisfy the so-called \"knowledge-binding\" condition. The security guarantee offered by Buldas, Saarepera in 2004 is improved by diminishing the security loss coefficient from formula_1 to formula_3.\n\nThe hash functions used in the secure time-stamping schemes do not necessarily have to be collision-resistant or even one-way; secure time-stamping schemes are probably possible even in the presence of a universal collision-finding algorithm (i.e. universal and attacking program that is able to find collisions for any hash function). This suggests that it is possible to find even stronger proofs based on some other properties of the hash functions.\nAt the illustration above hash tree based time-stamping system works in rounds (formula_4, formula_5, formula_6, ...), with one aggregation tree per round. Capacity of the system (formula_1) is determined by the tree size (formula_8, where formula_9 denotes binary tree depth). Current security proofs work on the assumption that there is a hard limit of the aggregation tree size, possibly enforced by the subtree length restriction.\n\nISO 18014 part 3 covers 'Mechanisms producing linked tokens'.\n\nAmerican National Standard for Financial Services, \"Trusted Timestamp Management and Security\" (ANSI ASC X9.95 Standard) from June 2005 covers linking-based and hybrid time-stamping schemes.\n\nThere is no IETF RFC or standard draft about linking based time-stamping. RFC 4998 (Evidence Record Syntax) encompasses hash tree and time-stamp as an integrity guarantee for long-term archiving.\n\n"}
{"id": "54300448", "url": "https://en.wikipedia.org/wiki?curid=54300448", "title": "Mercy (given name)", "text": "Mercy (given name)\n\nMercy is a feminine given name or nickname which may refer to:\n\n\n\n"}
{"id": "38966256", "url": "https://en.wikipedia.org/wiki?curid=38966256", "title": "Moneyval", "text": "Moneyval\n\nMoneyval is the common and official name of the Committee of Experts on the Evaluation of Anti-Money Laundering Measures and the Financing of Terrorism. Moneyval is a monitoring body of the Council of Europe, with 47 member states, reporting directly to its principal organ, the Committee of Ministers of the Council of Europe.\n\nThe task of Moneyval is assessing compliance with the principal international standards to counter money laundering and the financing of terrorism and the effectiveness of their implementation, as well as making recommendations to national authorities in respect of necessary improvements to their systems.\n\nIn 1997, the Council of Europe established the \"Select Committee of Experts on the Evaluation\nof Anti-Money Laundering Measures\", carrying the official abbreviation of PC-R-EV, as a sub-committee of the European Committee on Crime Problems (CDPC, for \"Comité Européen pour les Problèmes Criminels\"). The functions of PC-R-EV were to be regulated by the general provisions of Resolution Res(2005)47 on committees and subordinate bodies.\n\nIn 2002, the name of the committee was changed to \"Committee of Experts on the Evaluation of Anti-Money Laundering Measures and the Financing of Terrorism\", with the abbreviation Moneyval, on the grounds that \"the abbreviation 'PC-R-EV' does not express the aim of the committee’s activities sufficiently clearly.\" \n\nAt their meeting of 13 October 2010, the Committee of Ministers adopted a resolution elevating Moneyval, from 1 January 2011, to an \"independent monitoring mechanism within the Council of Europe, answerable directly to the Committee of Ministers\".\n\n\n"}
{"id": "53046631", "url": "https://en.wikipedia.org/wiki?curid=53046631", "title": "Multilinear multiplication", "text": "Multilinear multiplication\n\nIn multilinear algebra, applying a map that is the tensor product of linear maps to a tensor is called a multilinear multiplication.\n\nLet formula_1 be a field of characteristic zero, such as formula_2 or formula_3.\nLet formula_4 be a finite-dimensional vector space over formula_5, and let formula_6 be an order-d simple tensor, i.e., there exist some vectors formula_7 such that formula_8. If we are given a collection of linear maps formula_9, then the multilinear multiplication of formula_10 with formula_11 is defined as the action on formula_10 of the tensor product of these linear maps, namely\n\nformula_13\n\nSince the tensor product of linear maps is itself a linear map, and because every tensor admits a tensor rank decomposition, the above expression extends linearly to all tensors. That is, for a general tensor formula_6, the multilinear multiplication is\n\nformula_15\n\nwhere formula_16 with formula_17 is one of formula_10's tensor rank decompositions. The validity of the above expression is not limited to a tensor rank decomposition; in fact, it is valid for any expression of formula_10 as a linear combination of pure tensors, which follows from the universal property of the tensor product.\n\nIt is standard to use the following shorthand notations in the literature for multilinear multiplications:formula_20andformula_21where formula_22 is the identity operator.\n\nIn computational multilinear algebra it is conventional to work in coordinates. Assume that an inner product is fixed on formula_4 and let formula_24 denote the dual vector space of formula_25. Let formula_26 be a basis for formula_25, let formula_28 be the dual basis, and let formula_29 be a basis for formula_30. The linear map formula_31 is then represented by the matrix formula_32. Likewise, with respect to the standard tensor product basis formula_33, the abstract tensorformula_34is represented by the multidimensional array formula_35 . Observe that formula_36\n\nwhere formula_37 is the \"j\"th standard basis vector of formula_38 and the tensor product of vectors is the affine Segre map formula_39. It follows from the above choices of bases that the multilinear multiplication formula_40 becomes\n\nformula_41\n\nThe resulting tensor formula_42 lives in formula_43.\n\nFrom the above expression, an element-wise definition of the multilinear multiplication is obtained. Indeed, since formula_42 is a multidimensional array, it may be expressed as formula_45where formula_46 are the coefficients. Then it follows from the above formulae that\n\nformula_47\n\nwhere formula_48 is the Kronecker delta. Hence, if formula_49, then\n\nformula_50\n\nwhere the formula_51 are the elements of formula_52 as defined above.\n\nLet formula_53 be an order-d tensor over the tensor product of formula_54-vector spaces. \n\nSince a multilinear multiplication is the tensor product of linear maps, we have the following multilinearity property (in the construction of the map):\n\nformula_55 \n\nMultilinear multiplication is a linear map: formula_56 \n\nIt follows from the definition that the composition of two multilinear multiplications is also a multilinear multiplication:\n\nformula_57\n\nwhere formula_58 and formula_59 are linear maps.\n\nObserve specifically that multilinear multiplications in different factors commute,\n\nformula_60\n\nif formula_61\n\nThe factor-k multilinear multiplication formula_62 can be computed in coordinates as follows. Observe first that\n\nformula_63\n\nNext, since\n\nformula_64\n\nthere is a bijective map, called the factor-\"k\" standard flattening, denoted by formula_65, that identifies formula_66 with an element from the latter space, namely\n\nformula_67\n\nwhere formula_68is the \"j\"th standard basis vector of formula_69, formula_70, and formula_71 is the factor-\"k\" flattening matrix of formula_10 whose columns are the factor-\"k\" vectors formula_73 in some order, determined by the particular choice of the bijective map\n\nformula_74\n\nIn other words, the multilinear multiplication formula_75 can be computed as a sequence of \"d\" factor-\"k\" multilinear multiplications, which themselves can be implemented efficiently as classic matrix multiplications.\n\nThe higher-order singular value decomposition (HOSVD) factorizes a tensor given in coordinates formula_76 as the multilinear multiplication formula_77, where formula_78 are orthogonal matrices and formula_79.\n"}
{"id": "9458094", "url": "https://en.wikipedia.org/wiki?curid=9458094", "title": "Mutual liberty", "text": "Mutual liberty\n\nMutual liberty is an idea first coined by Alexis de Tocqueville in his 1835 work \"Democracy in America\". In effect, Tocqueville was referring to the general nature of American society during the 19th century. It appeared to him, at least on the surface, that every citizen in the United States had the opportunity to participate in the civic activities of the country. Another way to look at mutual liberty is by accounting for the collective free wills of every rational being in a community. Even though the notion of mutual liberty was introduced by Tocqueville, it was John Stuart Mill who greatly expanded it. Mill believed that the most proper occasion for mutual liberty was in a community governed by the consent of the governed, i.e., a republic. And according to Mill, it is only in a republic where members of all political factions can participate. It has been said that a republic is the form of government that divides people least. This statement pertains greatly to mutual liberty. Unlike positive and negative liberty, mutual liberty encompasses all citizens. It makes no distinction between political preference and social status. Mutual liberty pervades all sectors of society, from the homeless man on the street to the premier of the state. It is the process through which a general sense of morality gets exerted on the widest range of people in any given communal setting.\n\n"}
{"id": "23083748", "url": "https://en.wikipedia.org/wiki?curid=23083748", "title": "New Atheism", "text": "New Atheism\n\nNew Atheism is a term coined in 2006 by the agnostic journalist Gary Wolf to describe the positions promoted by some atheists of the twenty-first century. This modern-day atheism is advanced by a group of thinkers and writers who advocate the view that superstition, religion and irrationalism should not simply be tolerated but should be countered, criticized, and exposed by rational argument wherever their influence arises in government, education, and politics. According to Richard Ostling, Bertrand Russell, in his 1927 essay \"Why I Am Not a Christian\", put forward similar positions as those espoused by the New Atheists, suggesting that there are no substantive differences between traditional atheism and New Atheism.\n\nNew Atheism lends itself to and often overlaps with secular humanism and antitheism, particularly in its criticism of what many New Atheists regard as the indoctrination of children and the perpetuation of ideologies founded on belief in the supernatural. Some critics of the movement characterise it pejoratively as \"militant atheism\" or \"fundamentalist atheism\".\n\nThe Harvard botanist Asa Gray, a believing Christian and one of the first supporters of Charles Darwin's theory of evolution, commented in 1868 that the more worldly Darwinists in England had \"the English-materialistic-positivistic line of thought\". Darwin's supporter Thomas Huxley was openly skeptical, as the biographer Janet Browne describes:\nHuxley was rampaging on miracles and the existence of the soul. A few months later, he was to coin the word \"agnostic\" to describe his own position as neither a believer nor a disbeliever, but one who considered himself free to inquire rationally into the basis of knowledge, a philosopher of pure reason [...] The term fitted him well [...] and it caught the attention of the other free thinking, rational doubters in Huxley's ambit, and came to signify a particularly active form of scientific rationalism during the final decades of the 19th century. [...] In his hands, agnosticism became as doctrinaire as anything else--a religion of skepticism. Huxley used it as a creed that would place him on a higher moral plane than even bishops and archbishops. All the evidence would nevertheless suggest that Huxley was sincere in his rejection of the charge of outright atheism against himself. He refused to be \"a liar\". To inquire rigorously into the spiritual domain, he asserted, was a more elevated undertaking than slavishly to believe or disbelieve. \"A deep sense of religion is compatible with the entire absence of theology,\" he had told [Anglican clergyman] Charles Kingsley back in 1860. \"Pope Huxley\", the [magazine] \"Spectator\" dubbed him. The label stuck.\" —Janet Browne\n\nThe 2004 publication of \"The End of Faith: Religion, Terror, and the Future of Reason\" by Sam Harris, a bestseller in the United States, was joined over the next couple years by a series of popular best-sellers by atheist authors. Harris was motivated by the events of 11 September 2001, which he laid directly at the feet of Islam, while also directly criticizing Christianity and Judaism. Two years later Harris followed up with \"Letter to a Christian Nation\", which was also a severe criticism of Christianity. Also in 2006, following his television documentary \"The Root of All Evil?\", Richard Dawkins published \"The God Delusion\", which was on the \"New York Times\" best-seller list for 51 weeks.\n\nIn a 2010 column entitled \"Why I Don't Believe in the New Atheism\", Tom Flynn contends that what has been called \"New Atheism\" is neither a movement nor new, and that what was new was the publication of atheist material by big-name publishers, read by millions, and appearing on bestseller lists.\n\nOn 6 November 2015, the \"New Republic\" published an article entitled, \"Is the New Atheism dead?\" The atheist and evolutionary biologist David Sloan Wilson wrote, \"The world appears to be tiring of the New Atheism movement..\" In 2017, PZ Myers who formerly considered himself a new atheist, publicly renounced the New Atheism movement.\n\nOn 30 September 2007, four prominent atheists (Richard Dawkins, Sam Harris, Christopher Hitchens and Daniel Dennett) met at Hitchens' residence in Washington, D.C., for a private two-hour unmoderated discussion. The event was videotaped and titled \"The Four Horsemen\". During \"The God Debate\" in 2010 featuring Christopher Hitchens versus Dinesh D'Souza, the men were collectively referred to as the \"Four Horsemen of the Non-Apocalypse\", an allusion to the biblical Four Horsemen of the Apocalypse from the Book of Revelation. The four have been described disparagingly as \"evangelical atheists\".\n\nSam Harris is the author of the bestselling non-fiction books \"The End of Faith\", \"Letter to a Christian Nation\", \"The Moral Landscape\", and \"\", as well as two shorter works, initially published as e-books, \"Free Will\" and \"Lying\". Harris is a co-founder of the Reason Project.\n\nRichard Dawkins is the author of \"The God Delusion\", which was preceded by a Channel 4 television documentary titled \"The Root of All Evil?\". He is the founder of the Richard Dawkins Foundation for Reason and Science. He wrote: \"I don't object to the horseman label, by the way. I'm less keen on 'new atheist': it isn't clear to me how we differ from old atheists.\"\n\nChristopher Hitchens was the author of \"God Is Not Great\" and was named among the \"Top 100 Public Intellectuals\" by \"Foreign Policy\" and \"Prospect\" magazines. In addition, Hitchens served on the advisory board of the Secular Coalition for America. In 2010 Hitchens published his memoir \"Hitch-22\" (a nickname provided by close personal friend Salman Rushdie, whom Hitchens always supported during and following \"The Satanic Verses\" controversy). Shortly after its publication, Hitchens was diagnosed with esophageal cancer, which led to his death in December 2011. Before his death, Hitchens published a collection of essays and articles in his book \"Arguably\"; a short edition \"Mortality\" was published posthumously in 2012. These publications and numerous public appearances provided Hitchens with a platform to remain an astute atheist during his illness, even speaking specifically on the culture of deathbed conversions and condemning attempts to convert the terminally ill, which he opposed as \"bad taste\".\n\nDaniel Dennett, author of \"Darwin's Dangerous Idea\", \"Breaking the Spell\" and many others, has also been a vocal supporter of The Clergy Project, an organization that provides support for clergy in the US who no longer believe in God and cannot fully participate in their communities any longer.\n\nAfter the death of Hitchens, Ayaan Hirsi Ali (who attended the 2012 Global Atheist Convention, which Hitchens was scheduled to attend) was referred to as the \"plus one horse-woman\", since she was originally invited to the 2007 meeting of the \"Horsemen\" atheists but had to cancel at the last minute. Hirsi Ali was born in Mogadishu, Somalia, fleeing in 1992 to the Netherlands in order to escape an arranged marriage. She became involved in Dutch politics, rejected faith, and became vocal in opposing Islamic ideology, especially concerning women, as exemplified by her books \"Infidel\" and \"The Caged Virgin\". Hirsi Ali was later involved in the production of the film \"Submission\", for which her friend Theo Van Gogh was murdered with a death threat to Hirsi Ali pinned to his chest. This event resulted in Hirsi Ali's hiding and later immigration to the United States, where she now resides and remains a prolific critic of Islam. She regularly speaks out against the treatment of women in Islamic doctrine and society and is a proponent of free speech and the freedom to offend.\n\n\nMany contemporary atheists write from a scientific perspective. Unlike previous writers, many of whom thought that science was indifferent or even incapable of dealing with the \"God\" concept, Dawkins argues to the contrary, claiming the \"God Hypothesis\" is a valid scientific hypothesis, having effects in the physical universe, and like any other hypothesis can be tested and falsified. The late Victor Stenger proposed that the personal Abrahamic God is a scientific hypothesis that can be tested by standard methods of science. Both Dawkins and Stenger conclude that the hypothesis fails any such tests, and argue that naturalism is sufficient to explain everything we observe in the universe, from the most distant galaxies to the origin of life, the existence of different species, and the inner workings of the brain and consciousness. Nowhere, they argue, is it necessary to introduce God or the supernatural to understand reality. New Atheists reject Jesus' divinity.\n\nNon-believers assert that many religious or supernatural claims (such as the virgin birth of Jesus and the afterlife) are scientific claims in nature. They argue, as do deists and Progressive Christians, for instance, that the issue of Jesus' supposed parentage is not a question of \"values\" or \"morals\" but a question of scientific inquiry. Rational thinkers believe science is capable of investigating at least some, if not all, supernatural claims. Institutions such as the Mayo Clinic and Duke University are attempting to find empirical support for the healing power of intercessory prayer. According to Stenger, these experiments have thus far found no evidence that intercessory prayer works.\n\nStenger also argues in his book, \"\", that a God having omniscient, omnibenevolent and omnipotent attributes, which he termed a \"3O God\", cannot logically exist. A similar series of logical disproofs of the existence of a God with various attributes can be found in Michael Martin and Ricki Monnier's \"The Impossibility of God\", or Theodore M. Drange's article, \"Incompatible-Properties Arguments\".\n\nRichard Dawkins has been particularly critical of the conciliatory view that science and religion are not in conflict, noting, for example, that the Abrahamic religions constantly deal in scientific matters. In a 1998 article published in \"Free Inquiry\" magazine and later in his 2006 book \"The God Delusion\", Dawkins expresses disagreement with the view advocated by Stephen Jay Gould that science and religion are two non-overlapping magisteria (NOMA), each existing in a \"domain where one form of teaching holds the appropriate tools for meaningful discourse and resolution\". In Gould's proposal, science and religion should be confined to distinct non-overlapping domains: science would be limited to the empirical realm, including theories developed to describe observations, while religion would deal with questions of ultimate meaning and moral value. Dawkins contends that NOMA does not describe empirical facts about the intersection of science and religion: \"It is completely unrealistic to claim, as Gould and many others do, that religion keeps itself away from science's turf, restricting itself to morals and values. A universe with a supernatural presence would be a fundamentally and qualitatively different kind of universe from one without. The difference is, inescapably, a scientific difference. Religions make existence claims, and this means scientific claims.\"\n\nPopularized by Sam Harris is the view that science and thereby currently unknown objective facts may instruct human morality in a globally comparable way. Harris' book \"The Moral Landscape\" and accompanying TED Talk \"How Science can Determine Moral Values\" propose that human well-being and conversely suffering may be thought of as a landscape with peaks and valleys representing numerous ways to achieve extremes in human experience, and that there are objective states of well-being.\n\nNew Atheism is politically engaged in a variety of ways. These include campaigns to draw attention to the biased privileged position religion has and to reduce the influence of religion in the public sphere, attempts to promote cultural change (centering, in the United States, on the mainstream acceptance of atheism), and efforts to promote the idea of an \"atheist identity\". Internal strategic divisions over these issues have also been notable, as are questions about the diversity of the movement in terms of its gender and racial balance.\n\nThe theologians Jeffrey Robbins and Christopher Rodkey take issue with what they regard as \"the evangelical nature of the New Atheism, which assumes that it has a Good News to share, at all cost, for the ultimate future of humanity by the conversion of as many people as possible.\" They believe they have found similarities between New Atheism and evangelical Christianity and conclude that the all-consuming nature of both \"encourages endless conflict without progress\" between both extremities.\n\nSociologist William Stahl said, \"What is striking about the current debate is the frequency with which the New Atheists are portrayed as mirror images of religious fundamentalists.\"\n\nThe atheist philosopher of science Michael Ruse has made the claim that Richard Dawkins would fail \"introductory\" courses on the study of \"philosophy or religion\" (such as courses on the philosophy of religion), courses which are offered, for example, at many educational institutions such as colleges and universities around the world. Ruse also claims that the movement of New Atheism—which is perceived, by him, to be a \"bloody disaster\"—makes him ashamed, as a professional philosopher of science, to be among those holding to an atheist position, particularly as New Atheism does science a \"grave disservice\" and does a \"disservice to scholarship\" at more general level.\n\nPaul Kurtz, editor in chief of Free Inquiry, founder of Prometheus Books, was critical of many of the new atheists. He said, \"I consider them atheist fundamentalists... They're anti-religious, and they're mean-spirited, unfortunately. Now, they're very good atheists and very dedicated people who do not believe in God. But you have this aggressive and militant phase of atheism, and that does more damage than good\".\n\nJonathan Sacks, author of \"The Great Partnership: Science, Religion, and the Search for Meaning\", feels the new atheists miss the target by believing the \"cure for bad religion is no religion, as opposed to good religion\". He wrote: \nThe philosopher Massimo Pigliucci contends that the new atheist movement overlaps with scientism, which he finds to be philosophically unsound. He writes: \"What I do object to is the tendency, found among many New Atheists, to expand the definition of science to pretty much encompassing anything that deals with 'facts', loosely conceived..., it seems clear to me that most of the New Atheists (except for the professional philosophers among them) pontificate about philosophy very likely without having read a single professional paper in that field... I would actually go so far as to charge many of the leaders of the New Atheism movement (and, by implication, a good number of their followers) with anti-intellectualism, one mark of which is a lack of respect for the proper significance, value, and methods of another field of intellectual endeavor.\"\n\nAtheist professor Jacques Berlinerblau has criticised the New Atheists' mocking of religion as being inimical to their goals and claims that they haven't achieved anything politically.\n\n"}
{"id": "11132183", "url": "https://en.wikipedia.org/wiki?curid=11132183", "title": "Normative social influence", "text": "Normative social influence\n\nNormative social influence is a type of social influence that leads to conformity. It is defined in social psychology as \"...the influence of other people that leads us to conform in order to be liked and accepted by them.\" The power of normative social influence stems from the human identity as a social being, with a need for companionship and association. Normative social influence involves a change in behaviour that is deemed necessary in order to fit in a particular group. The need for a positive relationship with the people around leads us to conformity. This fact often leads to people exhibiting public compliance—but not necessarily private acceptance—of the group's social norms in order to be accepted by the group. Social norms refers to the unwritten rules that govern social behavior. These are customary standards for behavior that are widely shared by members of a culture.\n\nIn 1955, Solomon Asch conducted his classic conformity experiments in an attempt to discover if people still conform when the right answer is obvious. Specifically, he asked participants in his experiment to judge the similarity of lines, an easy task by objective standards. Using accomplices to the plot, also known as confederates, Asch created the illusion that an entire group of participants believed something that was clearly false (i.e., that dissimilar lines were actually similar). When in this situation, participants conformed over a third of the time on trials where the confederates gave blatantly false answers. When asked to make the judgments in private, participants gave the right answer more than 98% of the time. Asch's results cannot be explained by informational social influence, because in this case, the task was easy and the correct answer was obvious. Thus, participants were not necessarily looking to others to figure out the right answer, as informational social influence predicts. Instead, they were seeking acceptance and avoiding disapproval. Follow-up interviews with participants of the original Asch studies confirmed this. When participants were asked why they conformed, many provided reasons other than a need for accuracy.\n\nIn more current research, Schultz (1999) found that households that received more normative messages describing the frequency and amount of weekly recycling, began to have a direct impact on both the households frequency and amount of their curbside recycling. The sudden change was due to the fact that \"the other neighbors'\" recycling habits had a direct normative effect on the household to change their recycling behaviors. Similar results were apparent in another study in which researchers were able to increase household energy conservation through the use of normative messages. Participants in this conservation study did not believe that such normative messages could influence their behavior; they attributed their conservation efforts to environmental concerns or social responsibility needs. Thus, normative social influence can be a very powerful, yet unconscious, motivator of behavior.\n\nLastly, different studies have illustrated the consequences of deviation from a group's influence. In a study by Schachter (1951), participants were placed in groups and asked to discuss what to do with a juvenile delinquent they had read about. A \"deviant\" was instructed by the experimenter to take a stand strongly opposing that of the rest of the group and to hold this position in the midst of any arguments from other members. After the conclusion of the discussions, participants chose to reject this deviant the most, considering him the least desirable of the members, and relegating him to the least important tasks. Recent work by Berns et al. (2005) examined the physiological effects of deviation by using fMRI to scan participants' brains as they completed an object rotation task with other \"participants\", who were really confederates. The researchers were interested in examining participants' brain activity when they were under the pressure to conform to an incorrect group majority. The amygdala region (which is associated with negative emotions) was activated when participants sought to break off from the influence of the majority; providing evidence for the fact that resisting normative social influence can often lead to negative emotional consequences for individuals.\n\nLatane's social impact theory posits that three factors influence the extent to which we conform to group norms: personal importance, immediacy, and size. As the group becomes more important to a person, physically closer to him/her, and larger in number, Social Impact Theory predicts that conformity to group norms will increase. However, the size of the group only affects conformity to an extent—as a group expands past 3–5 members, the effect levels off.\n\nWhen a group is unanimous in its support of a norm, an individual feels greater pressure to follow suit. However, even a small break in unanimity can lead to decrease in the power of such normative influence. In Asch's study, when even one other confederate dissented from the majority and provided the correct answer, the participant answered incorrectly on fewer trials (about a fourth less). In addition, participants experienced positive emotions towards such dissenters. A similar reduction in conformity even occurred when the dissenting confederate provided an answer that was false (but still different from that of the majority).\n\nIn some versions of the experiment, Asch had dissenting confederates eventually rejoin the majority opinion after several trials; when this occurred, participants experienced greater pressure from normative influence and conformed as if they had never had the dissenter on their side. However, when the conditions were altered and the dissenting confederate left the room after several trials, the participants did not experience a similar pressure to conform as they had when the confederate rejoined the majority—they made less mistakes than they had in the condition where the confederate rejoined the others.\n\nThe pressure to bend to normative influence increases for actions performed in public, whereas this pressure decreases for actions done in private. In another variation of the Asch study, the researchers allowed the participant to privately write down his answer after all of the confederates had publicly stated their answers; this variation reduced the level of conformity among participants. In addition, the control condition of the Asch study revealed that participants were almost perfectly accurate when answering independently.\n\nIt is possible for a vocal minority to stem the normative influence of a larger majority. In the versions of the Asch study where a dissenter was inserted into the group (see Unanimity section), his presence as a minority member gave the participant the confidence to exert his independence to a greater extent. However, as soon as the dissenter waffled on his opinions and rejoined the majority, participant conformity increased. Thus, a minority must consistently stand by its beliefs to be effective.\n\nIn addition, there are other factors that increase the power of the minority: when the majority is forced to think about the beliefs and perspective of the minority, when the majority and minority are similar to one another, and when the minority exhibits some willingness to compromise and be flexible, although there is debate over the degree to which consistency and compromise should be balanced.\n\nIt is often the case that whereas a majority influences public compliance with a norm, a minority can engender private acceptance of a new norm, with the end result often being conversion (public and private acceptance of a norm).\n\nThere is a distinction between individualistic (e.g., United States) and collectivistic (e.g., Japan) cultures. While some predict that collectivistic cultures would exhibit stronger conformity under normative social influence, this is not necessarily the case—the identity of the group acts as a potential moderator. Because collectivists emphasize the importance of in-group members (e.g., family and friends), normative pressure from in-groups can lead to higher conformity than pressures from strangers.\n\nMany have long wondered whether there is a gender gap in conformity under normative influence, with women possibly conforming more than men. A meta-analysis by Eagly and Carli (1981) shows that this gap is small, and driven by public vs. private situations. Women do conform (slightly) more under normative influence than do men when in public situations as opposed to private ones. Eagly and Carli found that male researchers reported higher levels of conformity among female participants than did female researchers; the authors speculate that each gender could be implicitly biased towards portraying itself in a positive light, thus leading to actions (e.g., setting up experimental conditions under which males or females may be more comfortable) that might favor one gender over the other.\n\nIn many cases, normative social influence serves to promote social cohesion. When a majority of group members conform to social norms, the group generally becomes more stable. This stability translates into social cohesion, which allows group members to work together toward a common understanding, or \"good,\" but also has the unintended impact of making the group members less individualistic.\n\nFashion choices are often impacted by normative social influence. To feel accepted by a particular crowd, men and women often dress similarly to individuals in that group. Fashion conformity promotes social cohesion within the group, and can be a result of both conscious and unconscious motivations. Similar to fashion conformity, both the male and female view of the ideal body image is often affected by normative social influence. Social media and marketing helps to portray what is commonly considered the current view of physical attractiveness by the masses. As each generation defines the ideal female figure, women feel the pressure to conform to avoid disapproval of others. Likewise, as society continues to define the ideal male body type as muscular and fit, men also come under pressure to conform, as well, often leading to changes in eating habits to reach that ideal.\n\n"}
{"id": "2000128", "url": "https://en.wikipedia.org/wiki?curid=2000128", "title": "Online identity", "text": "Online identity\n\nInternet identity (IID), also online identity or internet persona, is a social identity that an Internet user establishes in online communities and websites. It can also be considered as an actively constructed presentation of oneself. Although some people choose to use their real names online, some Internet users prefer to be anonymous, identifying themselves by means of pseudonyms, which reveal varying amounts of personally identifiable information. An online identity may even be determined by a user's relationship to a certain social group they are a part of online. Some can even be deceptive about their identity.\n\nIn some online contexts, including Internet forums, online chats, and massively multiplayer online role-playing games (MMORPGs), users can represent themselves visually by choosing an avatar, an icon-sized graphic image. Avatars are one way users express their online identity. Through interaction with other users, an established online identity acquires a reputation, which enables other users to decide whether the identity is worthy of trust. Online identities are associated with users through authentication, which typically requires registration and logging in. Some websites also use the user's IP address or tracking cookies to identify users.\n\nThe concept of the self, and how this is influenced by emerging technologies, are a subject of research in fields such as education, psychology and sociology. The online disinhibition effect is a notable example, referring to a concept of unwise and uninhibited behavior on the Internet, arising as a result of anonymity and audience gratification.\n\nThe social web, i.e. the usage of the web to support the social process, represents a space in which people have the possibility to express and expose their identity in a social context.\nFor example, people define their identity explicitly by creating user profiles in social network services such as Facebook or LinkedIn and online dating services. By expressing opinions on blogs and other social media, they define more tacit identities.\n\nThe disclosure of a person's identity may present certain issues related to privacy. Many people adopt strategies that help them control the disclosure of their personal information online. Some strategies require users to invest considerable effort.\n\nThe emergence of the concept of online identity has raised many questions among academics. Social networking services and online avatars have complicated the concept of identity. Academia has responded to these emerging trends by establishing domains of scholarly research such as technoself studies, which focuses on all aspects of human identity in technological societies.\n\nOnline activities may affect our offline personal identity, as well.\n\nDorian Wiszniewski and Richard Coyne in their contribution to the book \"Building Virtual Communities\" explore online identity, with emphasis on the concept of \"masking\" identity. They point out that whenever an individual interacts in a social sphere they portray a mask of their identity. This is no different online and in fact becomes even more pronounced due to the decisions an online contributor must make concerning his or her online profile. He or she must answer specific questions about age, gender, address, username and so forth. Furthermore, with the accrual of one's online activity, his or her mask is increasingly defined by his or her style of writing, vocabulary and topics.\n\nThe kind of mask one chooses reveals at least something of the subject behind the mask. One might call this the \"metaphor\" of the mask. The online mask does not reveal the actual identity of a person. It, however, does reveal an example of what lies behind the mask. For instance, if a person chooses to act like a rock star online, this metaphor reveals an interest in rock music. Even if a person chooses to hide behind a totally false identity, this says something about the fear and lack of self-esteem behind the false mask.\n\nBecause of many emotional and psychological dynamics, people can be reluctant to interact online. By evoking a mask of identity a person can create a safety net. One of the great fears of online identity is having one's identity stolen or abused. This fear keeps people from sharing who they are. Some are so fearful of identity theft or abuse that they will not even reveal information already known about them in public listings. By making the mask available, people can interact with some degree of confidence without fear.\n\nWiszniewski and Coyne state \"Education can be seen as the change process by which identity is realized, how one finds one's place. Education implicates the transformation of identity. Education, among other things, is a process of building up a sense of identity, generalized as a process of edification.\" Students interacting in an online community must reveal something about themselves and have others respond to this contribution. In this manner, the mask is constantly being formulated in dialogue with others and thereby students will gain a richer and deeper sense of who they are. There will be a process of edification that will help students come to understand their strengths and weaknesses.\n\nThis mask perspective is likened to the concept of 'blended identity', whereby the offline-self informs the creation of a new online-self, which in turn informs the offline-self through further interaction with those the individual first met online.\n\nAs blogs allow an individual to express his or her views in individual essays or as part of a wider discussion, it creates a public forum for expressing ideas. Bloggers often choose to use pseudonyms, whether in platforms such as WordPress or in interest-centered sites like Blogster, to protect personal information and allow them more editorial freedom to express ideas that might be unpopular with their family, employers, etc. Use of a pseudonym (and a judicious approach to revealing personal information) can allow a person to protect their \"real\" identities, but still build a reputation online using the assumed name.\n\nDigital identity management is becoming something that individuals need to consider when applying for jobs of while working for a company. Social media has been a tool for human resources for years. A KPMG report on social media in human resources say that 76 percent of American companies used LinkedIn for recruiting. The ease of search means that reputation management will become more vital especially in professional services such as lawyers, doctors and accountants.\n\nOnline social networks like Facebook and MySpace allow people to maintain an online identity with some overlap between online and real world context. These identities are often created to reflect a specific aspect or ideal version of themselves. Representations include pictures, communications with other 'friends' and membership in network groups. Privacy control settings on social networks are also part of social networking identity.\n\nSome users may use their online identity as an extension of their physical selves, and center their profiles around realistic details. These users value continuity in their identity, and would prefer being honest with the portrayal of themselves. However, there is also a group of social network users that would argue against using a real identity online. These users have experimented with online identity, and ultimately what they have found is that it is possible to create an alternate identity through the usage of such social networks. For example, a popular blogger on medium.com writes under the name of Kel Campbell – a name that was chosen by her, not given to her. She states that when she was verbally attacked online by another user, she was able to protect herself from the sting of the insult by taking it as Kel, rather than her true self. Kel became a shield of sorts, and acted as a mask that freed the real user beneath it.\n\nSuch are the benefits of forming an alternate identity in online spaces. The \"I\" becomes subject to the \"Me\", and individuals are given the power to shape themselves into whoever (or whatever) they desire to become. Stories of people learning about their \"hidden extroversion\" or \"unknown creativity\" or becoming \"someone else\" are still at large. Research from scientists such as Danah Boyd and Knut Lundby has even found that in some cultures, the ability to form an identity online is considered a sacred privilege. This is because having an online identity allows the user to accomplish things that otherwise are impossible to do in real life. These cultures believe that the self has become a subjective concept on the online spaces; by logging onto their profiles, users are essentially freed from the prison of a physical body and can \"create a narrative of the self in virtual space that may be entirely new\".\n\nThe possibilities are endless – which is a feasible explanation for the explosive popularity of social networking sites in the past decade. Users are captivated by the opportunity to express themselves in an endless number of ways. Perhaps social networks will always remain popular, as long as the incentive to join is always present.\n\nAs the development of social networks, there appears a new economic phenomenon: doing business via social networks. For example, there are many users of WeChat called wei-businessmen sell products on WeChat. Doing business via social networks is not that easy. The identities of users in social networks are not the same as that in real world. For the sake of security, people do not tend to trust someone in social networks, in particular when it is related with money. So for wei-businessmen, reputations are very important for wei-business. They need invest enormous efforts to gain reputations among the users of WeChat, then these wei-businessmen could probably sell something to users.\n\nOnline identity in classrooms forces people to reevaluate their concepts of classroom environments. With the invention of online classes, classrooms have changed and no longer have the traditional face-to-face communications. These communications have been replaced by computer screen. Students are no longer defined by visual characteristics unless they make them known. There are pros and cons to each side. In a traditional classroom, students are able to visually connect with a teacher who was standing in the same room. During the class, if questions arise, clarification can be provided immediately. Students can create face-to-face connections with other students, and these connections can easily be extended beyond the classroom. For timid or socially awkward students, this ability to form and extend relationships through personal contact may hold little appeal. For these students, the appeal may reside in online courses, where computer communications allow them a greater degree of separation and anonymity.\n\nWith the prevalence of remote Internet communications, students do not form preconceptions of their classmates based on the classmate's appearance or speech characteristics. Rather, impressions are formed based only on the information presented by the classmate. Some students are more comfortable with this paradigm as it avoids the discomfort of public speaking. Students who do not feel comfortable stating their ideas in class can take time to sit down and think through exactly what they wish to say.\n\nCommunication via written media may lead students to take more time to think through their ideas since their words are in a more permanent setting (online) than most conversations carried on during class (Smith).\n\nOnline learning situations also cause a shift in perception of the professor. Whereas anonymity may help some students achieve a greater level of comfort, professors must maintain an active identity with which students may interact. The students should feel that their professor is ready to help whenever they may need it. Although students and professors may not be able to meet in person, emails and correspondence between them should occur in a timely manner. Without this students tend to drop online classes since it seems that they are wandering through a course without anyone to guide them.\n\nIn Virtual world, the users create a personal avatar and communicate with others through the virtual identity. The virtual personal figure and voice may draw from the real figure or fantasy worlds. The virtual figure to some degree reflects the personal expectation, and users may play a totally different personality in virtual world than in reality.\n\nAn Internet forum, or message board, is an online discussion site where people can hold conversations in the form of posted messages.\nThere are many types of Internet forums based on certain themes or groups. The properties of online identities also differ from different type of forums. For example, the users in a university BBS usually know some of the others in reality since the users can only be the students or professors in this university. However the freedom of expression is limited since some university BBSs are under control of the school administration and the identities are related to student IDs. On another hand, in some question-and-answer website like \"ZhiHu\" in China, is open to the public and users can create accounts only with e-mail address. But they can describe their specialties or personal experiences to show reliability in certain questions, and other users can also invite them to answer questions based on their profiles. The answers and profiles can be either real-name or anonymous.\n\nA discussed positive aspect of virtual communities is that people can now present themselves without fear of persecution, whether it is personality traits, behaviors that they are curious about, or the announcement of a real world identity component that has never before been announced.\n\nThis freedom results in new opportunities for society as a whole, especially the ability for people to explore the roles of gender and sexuality in a manner that can be harmless, yet interesting and helpful to those undertaking the change. Online identity has given people the opportunity to feel comfortable in wide-ranging roles, some of which may be underlying aspects of the user's life that the user is unable to portray in the real world.\n\nOnline identity has a beneficial effect for minority groups, including ethnic minority populations, people with disabilities, etc. Online identities may help remove prejudices created by stereotypes found in real life, and thus provide a greater sense of inclusion.\n\nA prime example of these opportunities is the establishment of many communities welcoming gay and lesbian teens who are dealing with their sexuality. These communities allow teens to share their experiences with one another and older gay and lesbian people, and may they provide a community that is both non-threatening and non-judgmental. In a review of such a community, Silberman quotes an information technology worker, Tom Reilly, as stating: \"The wonderful thing about online services is that they are an intrinsically decentralized resource. Kids can challenge what adults have to say and make the news\". If teen organizers are successful anywhere, news of it is readily available. The Internet is arguably the most powerful tool that young people with alternative sexualities have ever had.\n\nThe online world provides users with a choice to determine which sex, sexuality preference and sexual characteristics they would like to embody. In each online encounter, a user essentially has the opportunity to interchange which identity they would like to portray. As McRae argues in Surkan (2000), \"The lack of physical presence and the infinite malleability of bodies complicates sexual interaction in a singular way: because the choice of gender is an \"option\" rather than a strictly defined biological characteristic, the entire concept of gender as a primary marker of identity becomes partially subverted.\"\n\nOnline identity can offer potential social benefits to those with physical and sensory disabilities. The flexibility of online media provides control over their disclosure of impairment, an opportunity not typically available in real world social interactions. Researchers highlight its value in improving inclusion. However, it is notable that the affordance of \"normalization\" offers the possibility of experiencing non-stigmatized identities while also offering the capacity to create harmful and dangerous outcomes, which may jeopardize participants' safety.\n\nPrimarily, concerns regarding virtual identity revolve around the areas of misrepresentation and the contrasting effects of on and offline existence. Sexuality and sexual behavior online provide some of the most controversial debate with many concerned about the predatory nature of some users. This is particularly in reference to concerns about child pornography and the ability of pedophiles to obscure their identity.\n\nFinally, the concerns regarding the connection between on and offline lives are challenging the notions of what constitutes real experience. In reference to gender, sexuality and sexual behavior, the ability to play with these ideas has resulted in a questioning of how virtual experience may affect one's offline emotions. As McRae states, at its best, virtual sex not only complicates but drastically unsettles the division between mind, body, and self that has become a comfortable truism in Western metaphysics. When projected into virtuality, mind, body and self all become consciously-manufactured constructs through which individuals interact with each other.\n\nThe identities that people define in the social web are not necessarily facets of their offline self. Studies have shown that people lie in online dating services. In the case of social network services such as Facebook, companies are even proposing to sell 'friends' as a way to increase a user's visibility, calling into question even more the reliability of a person's 'social identity'.\n\nVan Gelder reported a famous incident occurring on a computer conferencing system during the early 80s where a male psychiatrist posed as Julie, a female psychologist with multiple disabilities including deafness, blindness, and serious facial disfigurement. Julie endeared herself to the computer conferencing community, finding psychological and emotional support from many members. The psychiatrist's choice to present differently was sustained by drawing upon the unbearable stigma attached to Julie's multiple disabilities as justification for not meeting face-to-face. Lack of visual cues allowed the identity transformation to continue, with the psychiatrist also assuming the identity of Julie's husband, who adamantly refused to allow anyone to visit Julie when she claimed to be seriously ill. This example highlights the ease with which identity may be constructed, transformed, and sustained by the textual nature of online interaction and the visual anonymity it affords.\n\nCatfishing is a way for a user to create a fake online profile, sometimes with fake photos and information, in order to enter into a relationship, intimate or platonic, with another user. Catfishing became popular in mainstream culture through the MTV reality show \"\".\n\nA problem facing anyone who hopes to build a positive online reputation is that reputations are site-specific; for example, one's reputation on eBay cannot be transferred to Slashdot.\n\nMultiple proposals have been made to build an identity management infrastructure into the Web protocols. All of them require an effective public key infrastructure so that the identity of two separate manifestations of an online identity (say, one on Wikipedia and another on Twitter) are probably one and the same.\n\nOpenID, an open, decentralized standard for authenticating users is used for access control, allowing users to log on to different services with the same digital identity. These services must allow and implement OpenID.\n\nGiven the malleability of online identities, some economists have expressed surprise that flourishing trading sites (such as eBay) have developed on the Internet. When two pseudonymous identities propose to enter into an online transaction, they are faced with the prisoner's dilemma: the deal can succeed only if the parties are willing to trust each other, but they have no rational basis for doing so. But successful Internet trading sites have developed reputation management systems, such as eBay's feedback system, which record transactions and provide the technical means by which users can rate each other's trustworthiness. However, users with malicious intent can still cause serious problems on such websites.\n\nAn online reputation is the perception that one generates on the Internet based on their digital footprint. Digital footprints accumulate through all of the content shared, feedback provided and information that created online. Due to the fact that if someone has a bad online reputation, he can easily change his pseudonym, new accounts on sites such as eBay or Amazon are usually distrusted. If an individual or company wants to manage their online reputation, they will face many more difficulties. This is why a merchant on the web having a brick and mortar shop is usually more trusted.\n\nUltimately, online identity cannot be completely free from the social constraints that are imposed in the real world. As Westfall (2000, p. 160) discusses, \"the idea of truly departing from social hierarchy and restriction does not occur on the Internet (as perhaps suggested by earlier research into the possibilities presented by the Internet) with identity construction still shaped by others. Westfall raises the important, yet rarely discussed, issue of the effects of literacy and communication skills of the online user.\" Indeed, these skills or the lack thereof have the capacity to shape one's online perception as they shape one's perception through a physical body in the \"real world.\"\n\nThis issue of gender and sexual reassignment raises the notion of disembodiment and its associated implications. \"Disembodiment\" is the idea that once the user is online, the need for the body is no longer required, and the user can participate separately from it. This ultimately relates to a sense of detachment from the identity defined by the physical body. In cyberspace, many aspects of sexual identity become blurred and are only defined by the user. Questions of truth will therefore be raised, particularly in reference to online dating and virtual sex. As McRae states, \"Virtual sex allows for a certain freedom of expression, of physical presentation and of experimentation beyond one's own real-life limits\". At its best, it not only complicates but drastically unsettles the division between mind, body and self in a manner only possible through the construction of an online identity.\n\nThe future of online anonymity depends on how an identity management infrastructure is developed. Law enforcement officials often express their opposition to online anonymity and pseudonymity, which they view as an open invitation to criminals who wish to disguise their identities. Therefore, they call for an identity management infrastructure that would irrevocably tie online identity to a person's legal identity]; in most such proposals, the system would be developed in tandem with a secure national identity document. Eric Schmidt, CEO of Google, has stated that the Google+ social network is intended to be exactly such an identity system. The controversy resulting from Google+'s policy of requiring users to sign in using legal names has been dubbed the \"nymwars\".\n\nOnline civil rights advocates, in contrast, argue that there is no need for a privacy-invasive system because technological solutions, such as reputation management systems, are already sufficient and are expected to grow in their sophistication and utility.\n\nAn online predator is an Internet user who exploits other users' vulnerability, often for sexual or financial purposes. It is relatively easy to create an online identity which is attractive to people that would not normally become involved with the predator, but fortunately there are a few means by which you can make sure that a person whom you haven't met is actually who they say they are. Many people will trust things such as the style in which someone writes, or the photographs someone has on their web page as a way to identify that person, but these can easily be forged. Long-term Internet relationships may sometimes be difficult to sufficiently understand knowing what someone's identity is actually like.\n\nThe most vulnerable age group to online predators is often considered to be young teenagers or older children. \"Over time - perhaps weeks or even months - the stranger, having obtained as much personal information as possible, grooms the child, gaining his or her trust through compliments, positive statements, and other forms of flattery to build an emotional bond.\" The victims often do not suspect anything until it is too late, as the other party usually misleads them to believe that they are of similar age.\n\nThe show Dateline on NBC has, overall, conducted three investigations on online predators. They had adults, posing online as teenage juveniles, engage in sexually explicit conversations with other adults (the predators) and arrange to meet them in person. But instead of meeting a teenager, the unsuspecting adult was confronted by Chris Hansen, an NBC News correspondent, arrested, and shown on nationwide television. \"Dateline\" held investigations in five different locations apprehending a total of 129 men in all.\n\nFederal laws have been passed in the U.S. to assist the government when trying to catch online predators. Some of these include wiretapping, so online offenders can be caught in advance, before a child becomes a victim. In California, where one \"Dateline\" investigation took place, it is a misdemeanor for someone to have sexually-tinged conversations with a child online. The men who came to the house were charged with a felony because their intent was obvious.\n\nAn online identity that has acquired an excellent reputation is valuable for two reasons: first, one or more persons invested a great deal of time and effort to build the identity's reputation; and second, other users look to the identity's reputation as they try to decide whether it is sufficiently trustworthy. It is therefore unsurprising that online identities have been put up for sale at online auction sites. However, conflicts arise over the ownership of online identities. Recently, a user of a massively multiplayer online game called EverQuest, which is owned by Sony Online Entertainment, Inc., attempted to sell his EverQuest identity on eBay. Sony objected, asserting that the character is Sony's intellectual property, and demanded the removal of the auction; under the terms of the U.S. Digital Millennium Copyright Act (DMCA), eBay could have become a party to a copyright infringement lawsuit if it failed to comply. Left unresolved is a fundamental question: Who owns an online identity created at a commercial Web site? Does an online identity belong to the person who created it, or to the company that owns the software used to create the identity?\n\n"}
{"id": "3837469", "url": "https://en.wikipedia.org/wiki?curid=3837469", "title": "Patent of Toleration", "text": "Patent of Toleration\n\nThe Patent of Toleration () was an edict of toleration issued on 13 October 1781 by the Habsburg emperor Joseph II. Part of the Josephinist reforms, the Patent extended religious freedom to non-Catholic Christians living in the crown lands of the Habsburg Monarchy, including Lutherans, Calvinists, and the Eastern Orthodox. Specifically, these members of minority faiths were now legally permitted to hold \"private religious exercises\" in clandestine churches. \n\nFor the first time after the Counter-Reformation, the Patent guaranteed the practice of religion by the Evangelical Lutheran and the Reformed Church in Austria. Nevertheless, worship was heavily regulated, wedding ceremonies remained reserved for the Catholic Church, and the Unity of the Brethren was still suppressed. Similar to the articular churches admitted 100 years before, Protestants were only allowed to erect 'houses of prayer' (\"Bethäuser\") which should not in any way resemble church buildings. In many Habsburg areas, especially in the 'hereditary lands' of Upper Austria, Styria and Carinthia, Protestant parishes quickly developed, strongly relying on crypto-protestant traditions.\n\nThe Patent was followed by the Edict of Tolerance for Jews in 1782. The edict extended to Jews the freedom to pursue all branches of commerce, but also imposed new requirements. Jews were required to create German-language primary schools or send their children to Christian schools (Jewish schools had previously taught children to read and write Hebrew in addition to mathematics.) The Patent also permitted Jews to attend state secondary schools. A series of laws issued soon after the Edict of Toleration abolished the autonomy of the Jewish communities, which had previously run their own court, charity, internal taxation and school systems; required Jews to acquire family names; made Jews subject to military conscription; and required candidates for the rabbinate to have secular education.\n\nThe 1781 Patent was originally called the \"Divine Send of Equal Liberties\" but was further put down by the monarch's advisor. Constraints on the construction of churches were abolished after the revolutions of 1848. The Protestant Church did not receive an equivalent legal status until Emperor Franz Joseph I of Austria issued the \"Protestantenpatent\" in 1861.\n"}
{"id": "5767788", "url": "https://en.wikipedia.org/wiki?curid=5767788", "title": "Problem of religious language", "text": "Problem of religious language\n\nThe problem of religious language considers whether it is possible to talk about God meaningfully if the traditional conceptions of God as being incorporeal, infinite, and timeless, are accepted. Because these traditional conceptions of God make it difficult to describe God, religious language has the potential to be meaningless. Theories of religious language either attempt to demonstrate that such language is meaningless, or attempt to show how religious language can still be meaningful.\n\nTraditionally, religious language has been explained as via negativa, analogy, symbolism, or myth, each of which describes a way of talking about God in human terms. The \"via negativa\" is a way of referring to God according to what God is not; analogy uses human qualities as standards against which to compare divine qualities; symbolism is used non-literally to describe otherwise ineffable experiences; and a mythological interpretation of religion attempts to reveal fundamental truths behind religious stories. Alternative explanations of religious language cast it as having political, performative, or imperative functions.\n\nEmpiricist David Hume's requirement that claims about reality must be verified by evidence influenced the logical positivist movement, particularly the philosopher A. J. Ayer. The movement proposed that, for a statement to hold meaning, it must be possible to verify its truthfulness empirically – with evidence from the senses. Consequently, the logical positivists argued that religious language must be meaningless because the propositions it makes are impossible to verify. Austrian philosopher Ludwig Wittgenstein has been regarded as a logical positivist by some academics because he distinguished between things that can and cannot be spoken about; others have argued that he could not have been a logical positivist because he emphasised the importance of mysticism. British philosopher Antony Flew proposed a similar challenge based on the principle that, in so far as assertions of religious belief cannot be empirically falsified, religious statements are rendered meaningless.\n\nThe analogy of games – most commonly associated with Ludwig Wittgenstein – has been proposed as a way of establishing meaning in religious language. The theory asserts that language must be understood in terms of a game: just as each game has its own rules determining what can and cannot be done, so each context of language has its own rules determining what is and is not meaningful. Religion is classified as a possible and legitimate language game which is meaningful within its own context. Various parables have also been proposed to solve the problem of meaning in religious language. R. M. Hare used his parable of a lunatic to introduce the concept of \"bliks\" – unfalsifiable beliefs according to which a worldview is established – which are not necessarily meaningless. Basil Mitchell used a parable to show that faith can be logical, even if it seems unverifiable. John Hick used his parable of the Celestial City to propose his theory of eschatological verification, the view that if there is an afterlife, then religious statements will be verifiable after death.\n\nReligious language is a philosophical problem arising from the difficulties in accurately describing God. Because God is generally conceived as incorporeal, infinite, and timeless, ordinary language cannot always apply to that entity. This makes speaking about or attributing properties to God difficult: a religious believer might simultaneously wish to describe God as good, yet also hold that God's goodness is unique and cannot be articulated by human language of goodness. This raises the problem of how (and whether) God can be meaningfully spoken about at all, which causes problems for religious belief since the ability to describe and talk about God is important in religious life. The French philosopher Simone Weil expressed this problem in her work \"Waiting for God\", in which she outlined her dilemma: she was simultaneously certain of God's love and conscious that she could not adequately describe him.\n\nThe medieval doctrine of divine simplicity also poses problems for religious language. This suggests that God has no accidental properties – these are properties that a being can have which do not contribute to its essence. If God has no accidental properties, he cannot be as he is traditionally conceived, because properties such as goodness are accidental. If divine simplicity is accepted, then to describe God as good would entail that goodness and God have the same definition. Such limits can also be problematic to religious believers; for example, the Bible regularly ascribes different emotions to God, ascriptions which would be implausible according to the doctrine of divine simplicity.\n\nThe theologian Sallie McFague believes that the more recent problem of religious language is based on individual experience, owing to the increased secularisation of society. She notes that human experience is of this world rather than regular encounters with the divine, which makes the experience of God uncommon and potentially unnecessary. Because of this, she argues, religious language is both idolatrous because it fails to express sufficient awe of God, and irrelevant because without adequate words it becomes meaningless.\n\nJewish philosopher Maimonides believed that God can only be ascribed negative attributes, a view based on two fundamental Jewish beliefs: that the existence of God must be accepted, and that it is forbidden to describe God. Maimonides believed that God is simple and so cannot be ascribed any essential attributes. He therefore argued that statements about God must be taken negatively, for example, \"God lives\" should be taken as \"God does not lack vitality\". Maimonides did not believe that God holds all of his attributes perfectly and without impairment; rather, he proposed that God lies outside of any human measures. To say that God is powerful, for example, would mean that God's power is beyond worldly power, and incomparable to any other power. In doing so, Maimonides attempted to illustrate God's indescribable nature and draw attention to the linguistic limits of describing God.\n\nThomas Aquinas argued that statements about God are analogous to human experience. An analogous term is partly univocal (has only one meaning) and partly equivocal (has more than one potential meaning) because an analogy is in some ways the same and in some ways different from the subject. He proposed that those godly qualities which resemble human qualities are described analogously, with reference to human terms; for example, when God is described as good, it does not mean that God is good in human terms, but that human goodness is used as a reference to describe God's goodness.\n\nPhilosopher Taede Smedes argued that religious language is symbolic. Denying any conflict between science and religion, he proposes that 'to believe' means to accept a conviction (that God exists, in the context of Christianity), which is different from 'knowing', which only occurs once something is proven. Thus, according to Smedes, we believe things that we do not know for sure. Smedes argues that, rather than being part of the world, God is so far beyond the world that there can be no common standard to which both God and the world can be compared. He argues that people can still believe in God, even though he cannot be compared to anything in the world, because belief in God is just an alternative way of viewing that world (he likens this to two people viewing a painting differently). Smedes claims that there should be no reason to look for a meaning behind our metaphors and symbols of God because the metaphors are all we have of God. He suggests that we can only talk of God \"pro nobis\" (for us) and not \"in se\" (as such) or \"sine nobis\" (without us). The point, he argues, is not that our concept of God should correspond with reality, but that we can only conceive of God through metaphors.\n\nIn the twentieth century, Ian Ramsey developed the theory of analogy, a development later cited in numerous works by Alister McGrath. He argued that various models of God are provided in religious writings that interact with each other: a range of analogies for salvation and the nature of God. Ramsey proposed that the models used modify and qualify each other, defining the limits of other analogies. As a result, no one analogy on its own is sufficient, but the combination of every analogy presented in Scripture gives a full and consistent depiction of God. The use of other analogies may then be used to determine if any one model of God is abused or improperly applied.\n\nPhilosopher Paul Tillich argued that religious faith is best expressed through symbolism because a symbol points to a meaning beyond itself and best expresses transcendent religious beliefs. He believed that any statement about God is symbolic and participates in the meaning of a concept. Tillich used the example of a national flag to illustrate his point: a flag points to something beyond itself, the country it represents, but also participates in the meaning of the country. He believed that symbols could unite a religious believer with a deeper dimension of himself as well as with a greater reality. Tillich believed that symbols must emerge from an individual collective unconsciousness, and can only function when they are accepted by the unconscious. He believed that symbols cannot be invented, but live and die at the appropriate times.\n\nLouis Dupré differentiates between signs and symbols, proposing that a sign points to something while a symbol represents it. A symbol holds its own meaning: rather than merely pointing someone towards another object, it takes the place of and represents that object. He believes that a symbol has some ambiguity which does not exist with a sign. Dupré believes that a symbol may deserve respect because it contains what is signified within itself. A symbol reveals a reality beyond what is already perceived and transforms the ways the current reality is perceived. Dupré differentiates between religious and aesthetic symbols, suggesting that a religious symbol points towards something which \"remains forever beyond our reach\". He proposed that a religious symbol does not reveal the nature of what it signifies, but conceals it.\n\nLangdon Brown Gilkey explained religious language and experience in terms of symbolism, identifying three characteristic features of religious symbolism which distinguish it from other language use. Firstly, religious symbolism has a double focus, referring both to something empirical and to something transcendent; Gilkey argued that the empirical manifestation points towards the transcendent being. Secondly, he believed that religious symbolism concerns fundamental questions of life, involving issues important to an individual or community. Finally, he argued that religious symbols provide standards by which life should be lived.\n\nIn the Sikh religious text the Guru Granth Sahib, religious language is used symbolically and metaphorically. In the text, Sikh Gurus repeat that the experiences they have while meditating are ineffable, incognizable, incomprehensible, and transensuous – this means that there is no object of their experience that can be conceptualised. To overcome this, the Sikh Gurus used symbolic and metaphorical language, assuming that there is a resemblance between the mystical experience of the divine (the sabad) and those experiencing it. For example, light is used to refer to the spiritual reality.\n\nWilliam Paden argued that religious language uses myth to present truths through stories. He argued that to those who practice a religion, myths are not mere fiction, but provide religious truths. Paden believed that a myth must explain something in the world with reference to a sacred being or force, and dismissed any myths which did not as \"folktales\". Using the example of creation myths, he differentiated myths from scientific hypotheses, the latter of which can be scientifically verified and do not reveal a greater truth; a myth cannot be analysed in the same way as a scientific theory.\n\nLutheran theologian Rudolf Bultmann proposed that the Bible contains existential content which is expressed through mythology; Bultmann sought to find the existential truths behind the veil of mythology, a task known as 'demythologising'. Bultmann distinguished between informative language and language with personal import, the latter of which commands obedience. He believed that God interacts with humans as the divine Word, perceiving a linguistic character inherent in God, which seeks to provide humans with self-understanding. Bultmann believed that the cultural embeddedness of the Bible could be overcome by demythologising the Bible, a process which he believed would allow readers to better encounter the word of God.\n\nChristian philosopher John Hick believed that the language of the Bible should be demythologised to be compatible with naturalism. He offered a demythologised Christology, arguing that Jesus was not God incarnate, but a man with incredible experience of divine reality. To Hick, calling Jesus the Son of God was a metaphor used by Jesus' followers to describe their commitment to what Jesus represented. Hick believed that demythologising the incarnation would make sense of the variety of world religions and give them equal validity as ways to encounter God.\n\nIslamic philosopher Carl Ernst has argued that religious language is often political, especially in the public sphere, and that its purpose is to persuade people and establish authority, as well as convey information. He explains that the modern criticisms of the West made by some sections of Islam are an ideological reaction to colonialism, which intentionally uses the same language as colonialists. Ernst argues that when it is used rhetorically, religious language cannot be taken at face value because of its political implications.\n\nPeter Donovan argues that most religious language is not about making truth-claims; instead, it is used to achieve certain goals. He notes that language can be used in alternative ways beyond making statements of fact, such as expressing feelings or asking questions. Donovan calls many of these uses \"performative\", as they serve to perform a certain function within religious life. For example, the words \"I promise\" perform the action of promising themselves – Donovan argues that most religious language fulfils this function. Ludwig Wittgenstein also proposed that language could be performative and presented a list of the different uses of language. Wittgenstein argued that \"the meaning of the language is in the use\", taking the use of language to be performative. The philosopher J. L. Austin argued that religious language is not just cognitive but can perform social acts, including vows, blessings, and the naming of children. He distinguished performative statements as those that do not simply describe a state of affairs, but bring them about. Historian of religion Benjamin Ray uses the performance of rituals within religions as evidence for a performative interpretation of language. He argues that the language of rituals can perform social tasks: when a priest announces that a spiritual event has occurred, those present believe it because of the spiritual authority of the priest. He believed that the meaning of a ritual is defined by the language used by the speaker, who is defined culturally as a superhuman agent.\n\nBritish philosopher R. B. Braithwaite attempted to approach religious language empirically and adopted Wittgenstein's idea of \"meaning as use\". He likened religious statements to moral statements because they are both non-descriptive yet still have a use and a meaning; they do not describe the world, but the believer's attitudes towards it. Braithwaite believed that the main difference between a religious and a moral statement was that religious statements are part of a linguistic system of stories, metaphors, and parables.\n\nProfessor Nathan Katz writes of the analogy of a burning building, used by the Buddha in the Lotus Sutra, which casts religious language as imperative. In the analogy, a father sees his children at the top of a burning building. He persuades them to leave, but only by promising them toys if they leave. Katz argues that the message of the parable is not that the Buddha has been telling lies; rather, he believes that the Buddha was illustrating the imperative use of language. Katz believes that religious language is an imperative and an invitation, rather than a truth-claim.\n\nIn the conclusion of his \"Enquiry Concerning Human Understanding\", Scottish philosopher David Hume argued that statements that make claims about reality must be verified by experience, and dismissed those that cannot be verified as meaningless. Hume regarded most religious language as unverifiable by experiment and so dismissed it.\n\nHume criticised the view that we cannot speak about God, and proposed that this view is no different from the skeptical view that God cannot be spoken about. He was unconvinced by Aquinas' theory of analogy and argued that God's attributes must be completely different from human attributes, making comparisons between the two impossible. Hume's scepticism influenced the logical positivist movement of the twentieth century.\n\nThe logical positivism movement originated in the Vienna Circle and was continued by British philosopher A. J. Ayer. The Vienna Circle adopted the distinction between analytic and synthetic statements: analytic statements are those whose meaning is contained within the words themselves, such as definitions, tautologies or mathematical statements, while synthetic statements make claims about reality. To determine whether a synthetic statement is meaningful, the Vienna Circle developed a verifiability theory of meaning, which proposed that for a synthetic statement to have cognitive meaning, its truthfulness must be empirically verifiable. Because claims about God cannot be empirically verified, the logical positivists argued that religious propositions are meaningless.\n\nIn 1936, Ayer wrote \"Language, Truth and Logic\", in which he claimed that religious language is meaningless. He put forward a strong empirical position, arguing that all knowledge must either come from observations of the world or be necessarily true, like mathematical statements. In doing so, he rejected metaphysics, which considers the reality of a world beyond the natural world and science. Because it is based on metaphysics and is therefore unverifiable, Ayer denounced religious language, as well as statements about ethics or aesthetics, as meaningless. Ayer challenged the meaningfulness of all statements about God – theistic, atheistic and agnostic – arguing that they are all equally meaningless because they all discuss the existence of a metaphysical, unverifiable being.\n\nAustrian philosopher Ludwig Wittgenstein finished his \"Tractatus Logico-Philosophicus\" with the proposition that \"Whereof one cannot speak, thereof one must be silent.\" Beverly and Brian Clack have suggested that because of this statement, Wittgenstein was taken for a positivist by many of his disciples because he made a distinction between what can and cannot be spoken about. They argue that this interpretation is inaccurate because Wittgenstein held the \"mystical\", which cannot be described, as important. Rather than dismissing the mystical as meaningless, as the logical positivists did, Wittgenstein believed that while the facts of the world remain the same, the perspective from which they are viewed will vary.\n\nThe falsification principle has been developed as an alternative theory by which it may be possible to distinguish between those religious statements that may potentially have meaning, and those that are meaningless. It proposes that most religious language is unfalsifiable because there is no way that it could be empirically proven false. In a landmark paper published in 1945, analytic philosopher Antony Flew argued that a meaningful statement must simultaneously assert and deny a state of affairs; for example, the statement \"God loves us\" both asserts that God loves us and denies that God does not love us. Flew maintained that if a religious believer could not say what circumstances would have to exist for their statements about God to be false, then they are unfalsifiable and meaningless.\n\nUsing John Wisdom's parable of the invisible gardener, Flew attempted to demonstrate that religious language is unfalsifiable. The parable tells the story of two people who discover a garden on a deserted island; one believes it is tended to by a gardener, the other believes that it formed naturally, without the existence of a gardener. The two watch out for the gardener but never find him; the non-believer consequently maintains that there is no gardener, whereas the believer rationalises the non-appearance by suggesting that the gardener is invisible and cannot be detected. Flew contended that if the believer's interpretation is accepted, nothing is left of the original gardener. He argued that religious believers tend to adopt counterpart rationalisations in response to any apparent challenge to their beliefs from empirical evidence; and these beliefs consequently suffer a \"death by a thousand qualifications\" as they are qualified and modified so much that they end up asserting nothing meaningful. Flew applied his principles to religious claims such as God's love for humans, arguing that if they are meaningful assertions they would deny a certain state of affairs. He argued that when faced with evidence against the existence of a loving God, such as the terminal illness of a child, theists will qualify their claims to allow for such evidence; for example they may suggest that God's love is different from human love. Such qualifications, Flew argued, make the original proposition meaningless; he questioned what God's love actually promises and what it guarantees against, and proposed that God's qualified love promises nothing and becomes worthless.\n\nFlew continued in many subsequent publications to maintain the falsifiability criterion for meaning; but in later life retracted the specific assertion in his 1945 paper that all religious language is unfalsifiable, and so meaningless. Drawing specifically on the emerging science of molecular genetics (which had not existed at the time of his original paper), Flew eventually became convinced that the complexity this revealed in the mechanisms of biological reproduction might not be consistent with the time known to have been available for evolution on Earth to have happened; and that this potentially suggested a valid empirical test by which the assertion \"that there is no creator God\" might be falsified; \"the latest work I have seen shows that the present physical universe gives too little time for these theories of abiogenesis to get the job done.\"\n\nThe analogy of a game was first proposed by Hans-Georg Gadamer in an attempt to demonstrate the epistemic unity of language. He suggested that language is like a game which everyone participates in and is played by a greater being. Gadamer believed that language makes up the fundamental structure of reality and that human language participates in a greater language; Christianity teaches this to be the divine word which created the world and was incarnate in Jesus Christ.\n\nLudwig Wittgenstein proposed a calculus theory of language, which maintained that all language should be analysable in a uniform way. Later in his life he rejected this theory, and instead proposed an alternative language-game analogy. He likened the differences in languages to the differences in games, arguing that just as there are many different games, each with different rules, so there are many different kinds of language. Wittgenstein argued that different forms of language have different rules which determine what makes a proposition meaningful; outside of its language-game, a proposition is meaningless. He believed that the meaning of a proposition depends on its context and the rules of that context. Wittgenstein presented a language game as a situation in which certain kinds of language are used. He provided some examples of language games: \"Asking, thanking, greeting, cursing, praying\".\n\nWittgenstein believed that religion is significant because it offers a particular way of life, rather than confirming the existence of God. He therefore believed that religious language is confessional – a confession of what someone feels and believes – rather than consisting of claims to truth. Wittgenstein believed that religious language is different from language used to describe physical objects because it occupies a different language game.\n\nDewi Zephaniah Phillips defended Wittgenstein's theory by arguing that although religious language games are autonomous, they should not be treated as isolated because they make statements about secular events such as birth and death. Phillips argued that because of this connection, religions can still be criticised based on human experiences of these secular events. He maintained that religion cannot be denounced as wrong because it is not empirical.\n\nPeter Donovan criticises the language-games approach for failing to recognise that religions operate in a world containing other ideas and that many religious people make claims to truth. He notes that many religious believers not only believe their religion to be meaningful and true in its own context, but claim that it is true against all other possible beliefs; if the language games analogy is accepted, such a comparison between beliefs is impossible. Donovan proposes that debates between different religions, and the apologetics of some, demonstrates that they interact with each other and the wider world and so cannot be treated as isolated language games.\n\nIn response to Flew's falsification principle, British philosopher R. M. Hare told a parable in an attempt to demonstrate that religious language is meaningful. Hare described a lunatic who believes that all university professors want to kill him; no amount of evidence of kindly professors will dissuade him from this view. Hare called this kind of unfalsifiable conviction a \"blik\", and argued that it formed an unfalsifiable, yet still meaningful, worldview. He proposed that all people – religious and non-religious – hold bliks, and that they cannot be unseated by empirical evidence. Nevertheless, he maintained that a blik is meaningful because it forms the basis of a person's understanding of the world. Hare believed that some bliks are correct and others are not, though he did not propose a method of distinguishing between the two.\n\nBasil Mitchell responded to Flew's falsification principle with his own parable. He described an underground resistance soldier who meets a stranger who claims to be leading the resistance movement. The stranger tells the soldier to keep faith in him, even if he is seen to be fighting for the other side. The soldier's faith is regularly tested as he observes the stranger fighting for both sides, but his faith remains strong. Mitchell's parable teaches that although evidence can challenge a religious belief, a believer still has reason to hold their views. He argued that although a believer will not allow anything to count decisively against his beliefs, the theist still accepts the existence of evidence which could count against religious belief.\n\nResponding to the verification principle, John Hick used his parable of the Celestial City to describe his theory of eschatological verificationism. His parable is of two travellers, a theist and an atheist, together on a road. The theist believes that there is a Celestial City at the end of the road; the atheist believes that there is no such city. Hick's parable is an allegory of the Christian belief in an afterlife, which he argued can be verified upon death. Hick believed that eschatological verification is \"unsymmetrical\" because while it could be verified if it is true, it cannot be falsified if not. This is in contrast to ordinary \"symmetrical\" statements, which can be verified or falsified.\n\nIn his biography of Hick, David Cheetham notes a criticism of Hick's theory: waiting for eschatological verification could make religious belief provisional, preventing total commitment to faith. Cheetham argues that such criticism is misapplied because Hick's theory was not directed to religious believers but to philosophers, who argued that religion is unverifiable and therefore meaningless.\n\nJames Morris notes that Hick's eschatological verification theory has been criticised for being inconsistent with his belief in religious pluralism. Morris argues that such criticism can be overcome by modifying Hick's parable to include multiple travellers, all with different beliefs, on the road. He argues that even if some beliefs about life after death are unverifiable, Hick's belief in bodily resurrection can still be verified.\n\n\n"}
{"id": "35457560", "url": "https://en.wikipedia.org/wiki?curid=35457560", "title": "RNA22", "text": "RNA22\n\nRna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.\n\nThe algorithm is conceptually distinct from other methods for predicting in that it does \"not\" use experimentally validated heteroduplexes for training, instead relying only on the sequences of\nknown mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.\n\nRna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does \"not\" require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.\n\nUse of rna22 led to the discovery of \"non-canonical\" microRNA targets in the coding regions of the mouse \"Nanog\", \"Oct4\" and \"Sox2\". Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.\n\nA recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that \"bona fide\" miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.\n"}
{"id": "14684661", "url": "https://en.wikipedia.org/wiki?curid=14684661", "title": "Racial formation theory", "text": "Racial formation theory\n\nRacial formation theory is an analytical tool in sociology, developed by Michael Omi and Howard Winant, which is used to look at race as a socially constructed identity, where the content and importance of racial categories are determined by social, economic, and political forces. Unlike other traditional race theories, \"In [Omi and Winant's] view, racial meanings pervade US society, extending from the shaping of individual racial identities to the structuring of collective political action on the terrain of the state\".\n\nIn order to delve further into the topic of racial formation, it is important to explore the question of what \"race\" is. Racial formation theory is a framework that has the objective of deconstructing race as it exists today in the United States. To do this, the authors first explore the historical development of race as a dynamic and fluid social construct. This goes against the dominant discourses on race, which see race as a static and unchanging concept based purely on physical and genetic criteria.\n\nInstead of claiming race as something that is concrete, where the person's biology and upbringing are what shape racial identity, Omi and Winant suggest that race is something that is fluid, where \"the racial order is organized and enforced by the continuity and reciprocity between micro-level and macro-level of social relations\".\n\nIn the above definition, the \"micro-level\" social relations refer to \"the ways in which we understand ourselves and interact with others, the structuring of our practical activity in work and family, as citizens and as thinkers\", basically, a person's individual interactions with other people.\n\nThe \"macro-level\" social relations refer to the social structures and common ideologies of a society. Relevant social structures include collective organizations like businesses, the media, and the government, and the common ideologies include cultural and stereotypical beliefs on race, class, sexuality, and gender.\n\nOmi and Winant also believe that \"race [is] an unstable and 'de-centered' complex of social meanings constantly being transformed by political struggle\". Because of this, people are able to constantly contest the definition of race both in the micro- and the macro-level.\n\nThroughout modern history, people have assigned identity based on race, both as a means of distinguishing one group from another, but more importantly as a means of control. The dominant culture assigns identity to minority groups as a means of separating them, diminishing their status, and maintaining control over them. Often, this distinction is made simply on the basis of skin color. Through this mechanism of assigning identity, race becomes a political weapon of the majority that has several limiting effects on the oppressed group: \n\nOmi and Winant argue that the concept of race developed gradually and was created to justify and explain inequality and genocide that is characteristic of European colonization. The expropriation of property, the denial of political rights, the introduction of slavery and other forms of coercive labor, well as outright extermination, all presupposed a worldview which distinguished European – children of God, human beings, etc. – from \"others\". Such a worldview was needed to explain why some should be \"free\" and others enslaved, why some had rights to land while others did not. Race and the interpretation of racial differences were central factors in that worldview.\nThe need for a justification for institutionalized racial discrimination led to the \"biological essentialist\" framework. In this framework, White European Americans were viewed as being born inherently superior. Religious debates also flared over the role of race in definitions of humanity: \"Arguments took place over creation itself, as theories of polygenesis questioned whether God had made only one species of humanity ('monogenesis').\"\n\nIn their book \"Racial Formation\", Omi and Winant present race as a relatively recent phenomenon in the United States. They describe how race becomes established in social consciousness, even without anyone having an explicit intention to perpetuate it:\n\nEverybody learns some combination, some version, of the rules of racial classification, and of their own racial identity, often without obvious teaching or conscious inculcation. Race becomes 'common sense' – a way of comprehending, explaining, and acting in the world.\n\nThere was also a scientific preoccupation with the idea of race. Throughout the 19th and 20th centuries in particular, some of the most respected scientists of the time took up the question of racial superiority. Many of them concluded that White Europeans were, in fact, superior based on studies on everything from cranial capacity to social Darwinism.\n\nThis scientific debate was not, however, a purely academic one. It was a central icon of public fascination, often in the popular magazines of the time. Even today, scientists are still working on finding a genetic basis for racial categorization. None of these efforts has been successful in defining race in an empirical and objective way.\n\nRacial formation theory examines race as a dynamic social construct with inherent structural barriers, ideologies and individual actions, whereas the biological essentialist understands individual deficiency as the basis for racial marginalization and oppression.\n\nAccording to Omi and Winant, a \"racial formation perspective\" is needed to explain race as \"an autonomous field of social conflict, political organizations, and cultural/ideological meaning\". The second part of their book is an elaboration of this racial formation perspective.\n\nOmi and Winant define \"racial formation\" as \"the process by which social, economic and political forces determine the content and importance of racial categories, and by which they are in turn shaped by racial meanings\". The racial formation perspective emphasizes the extent to which race is a social and political construction that operates at two levels, the micro (individual identity) and the macro (collective social structure). The two levels interact to form a racial social movement when individuals at the micro level are mobilized in response to political racial injustice at the macro level.\n\nBecoming a citizen of this society is the process of learning to see race – that is, to ascribe social meanings and qualities to otherwise meaningless biological features. And in turn, race consciousness figures centrally in the building of a collective body of knowledge without which we could not make sense of the world around us – a body of knowledge that Omi and Winant call \"racial common sense\". That describes the associations we make between individual characteristics, preferences, behaviors, and attitudes and a particular physical appearance or perceived group membership.\n\nThose expectations will guide all our daily interactions. Individuals that do not perform according to our racial expectations disrupt this micro-level process. Omi and Winant provide several illustrative examples of this disruption of expectations:\n\nThe black banker harassed by police while walking in casual clothes through his own well-off neighborhood, the Latino or white kid rapping in perfect Afro patois, the unending faux pas committed by whites who assume that the non-whites they encounter are servants or tradespeople, the belief that non-white colleagues are less qualified persons hired to fulfill affirmative action guidelines...\n\nWhen our racial expectations are violated, our reactions can betray our \"preconceived notions of a racialized social structure\". There are many racial projects dispersed throughout society that \"mediate between discursive or representational means in which race is identified and signified on the one hand, and the institutional and organizational forms in which is it routinized and standardized on the other\".\n\n\n"}
{"id": "2621864", "url": "https://en.wikipedia.org/wiki?curid=2621864", "title": "Shaping (psychology)", "text": "Shaping (psychology)\n\nShaping is a conditioning paradigm used primarily in the experimental analysis of behavior. The method used is differential reinforcement of successive approximations. It was introduced by B. F. Skinner with pigeons and extended to dogs, dolphins, humans and other species. In shaping, the form of an existing response is gradually changed across successive trials towards a desired target behavior by rewarding exact segments of behavior. Skinner's explanation of shaping was this:\nThe successive approximations reinforced are increasingly accurate approximations of a response desired by a trainer, the \"target\" response. As training progresses the trainer stops reinforcing the less accurate approximations. For example, in training a rat to press a lever, the following successive approximations might be reinforced:\n\nThe trainer starts by reinforcing all behaviors in the first category, here turning toward the lever. When the animal regularly performs that response (turning), the trainer restricts reinforcement to responses in the second category (moving toward), then the third, and so on, progressing to each more accurate approximation as the animal learns the one currently reinforced. Thus, the response gradually approximates the desired behavior until finally the target response (lever pressing) is established. At first the rat is not likely to press the lever; in the end it presses rapidly. \n\nShaping sometimes fails. An oft-cited example is an attempt by Marian and Keller Breland (students of B.F. Skinner) to shape a pig and a raccoon to deposit a coin in a piggy bank, using food as the reinforcer. Instead of learning to deposit the coin, the pig began to root it into the ground, and the raccoon \"washed\" and rubbed the coins together. That is, the animals treated the coin the same way that they treated food items that they were preparing to eat, referred to as “food-getting” behaviors. In the case of the raccoon, it was able to learn to deposit one coin into the box to gain a food reward, but when the contingencies were changed such that two coins were required to gain the reward, the raccoon could not learn the new, more complex rule. After what could be characterized as expressions of frustration, the raccoon resorts to basic “food-getting” behaviors common to its species. These results show a limitation in the raccoon’s cognitive capacity to even conceive of the possibility that two coins could be exchanged for food, irrespective of existing auto-shaping contingencies. Since the Breland's observations were reported many other examples of untrained responses to natural stimuli have been reported; in many contexts, the stimuli are called \"sign stimuli\", and the related behaviors are called \"sign tracking\". \n\nShaping is used in training operant responses in lab animals, and in applied behavior analysis to change human or animal behaviors considered to be maladaptive or dysfunctional. It also plays an important role in commercial animal training. Shaping assists in \"discrimination\", which is the ability to tell the difference between stimuli that are and are not reinforced, and in \"generalization\", which is the application of a response learned in one situation to a different but similar situation.\n\nShaping can also be used in a rehabilitation center. For example, training on parallel bars can approximate walking with a walker. Or shaping can teach patients how to increase the time between bathroom visits.\n\nAutoshaping (sometimes called sign tracking) is any of a variety of experimental procedures used to study classical conditioning. In autoshaping, in contrast to shaping, the reward comes irrespective of the behavior of the animal. In its simplest form, autoshaping is very similar to Pavlov's salivary conditioning procedure using dogs. In Pavlov's best-known procedure, a short audible tone reliably preceded the presentation of food to dogs. The dogs naturally, unconditionally, salivated (unconditioned response) to the food (unconditioned stimulus) given to them, but through learning, conditionally, came to salivate (conditioned response) to the tone (conditioned stimulus) that predicted food. In auto-shaping, a light is reliably turned on shortly before animals are given food. The animals naturally, unconditionally, display consummatory reactions to the food given them, but through learning, conditionally, came to perform those same consummatory actions directed at the conditioned stimulus that predicts food.\n\nAutoshaping provides an interesting conundrum for B.F. Skinner's assertion that one must employ shaping as a method for teaching a pigeon to peck a key. After all, if an animal can shape itself, why use the laborious process of shaping? Autoshaping also contradicts Skinner's principle of reinforcement. During autoshaping, food comes irrespective of the behavior of the animal. If reinforcement were occurring, random behaviors should increase in frequency because they should have been rewarded by random food. Nonetheless, key-pecking reliably develops in pigeons, even if this behavior had never been rewarded.\n\nBut, the clearest evidence that auto-shaping is under Pavlovian and not Skinnerian control was found using the omission procedure. In that procedure, food is normally scheduled for delivery following each presentation of a stimulus (often a flash of light), except in cases in which the animal actually performs a consummatory response to the stimulus, in which case food is withheld. Here, if the behavior were under instrumental control, the animal would stop attempting to consume the stimulus, as that behaviour is followed by the withholding of food. But, animals persist in attempting to consume the conditioned stimulus for thousands of trials (a phenomenon known as negative automaintenance), unable to cease their behavioural response to the conditioned stimulus even when it prevents them from obtaining a reward.\n\n\n"}
{"id": "28307", "url": "https://en.wikipedia.org/wiki?curid=28307", "title": "Sin", "text": "Sin\n\nIn a religious context, sin is an act of transgression against divine or natural law. Sin can also be viewed as any thought or action that endangers the ideal relationship between an individual and God; or as any diversion from the perceived ideal order for human living. In Jainism, sin refers to anything that harms the possibility of the \"jiva\" (being) to attain \"moksha\" (supreme emancipation). In Islamic ethics, Muslims see sin as anything that goes against the commands of Allah (God). Judaism regards the violation of any of the 613 commandments as a sin.\n\nThe word derives from \"Old English \"syn(n)\", for original *\"sunjō\". The stem may be related to that of Latin '\"sons\", \"sont-is\"' guilty. In Old English there are examples of the original general sense, ‘offence, wrong-doing, misdeed'\". The English Biblical terms translated as \"sin\" or \"syn\" from the Biblical Greek and Jewish terms sometimes originate from words in the latter languages denoting the act or state of missing the mark; the original sense of New Testament Greek \"hamartia\" \"sin\", is failure, being in error, missing the mark, especially in spear throwing; Hebrew \"hata\" \"sin\" originates in archery and literally refers to missing the \"gold\" at the centre of a target, but hitting the target, i.e. error. \"To sin\" has been defined from a Greek concordance as \"to miss the mark\".\n\nIn the Bahá'í Faith, humans are considered naturally good (perfect), fundamentally spiritual beings. Human beings were created because of God's immeasurable love. However, the Bahá'í teachings compare the human heart to a mirror, which, if turned away from the light of the sun (i.e. God), is incapable of receiving God's love.\n\nBuddhism believes in the principle of \"karma\", whereby suffering is the inevitable consequence of greed, anger, and delusion (known as the Three poisons). While there is no direct Buddhist equivalent of the Abrahamic concept of sin, wrongdoing is recognized in Buddhism. The concept of Buddhist ethics is consequentialist in nature and is not based upon duty towards any deity.\nKarma means action, and in Buddhist context, motivation is the most important aspect of an action. Whether karma done with mind, body and/or speech is called 'good' or 'bad', depends on whether it would bring pleasant or unpleasant results to the person who does the action. \nOne needs to purify negative karma Four Satipatthanas to free oneself from obstacles to liberation from the vicious circle of rebirth. The purification reduces suffering and in the end one reaches Nirvana, the ultimate purification by realizing selflessness or emptiness. An enlightened being is free of all the suffering and karmas, and will not be automatically born again.\n\nIn the Old Testament, some sins were punishable by death in different forms, while most sins are forgiven by burnt offerings. Christians consider the Old Covenant to be fulfilled by the Gospel.\n\nIn the New Testament the forgiveness of sin is effected through faith and repentance (Mark 1:15). Sin is forgiven when the sinner acknowledges, confesses, and repents for their sin as a part of believing in Jesus Christ. The sinner is expected to confess his sins to God as a part of an ongoing relationship, which also includes giving thanks to God. The sinful person has never before been in a favorable relationship with God. When, as a part of the process of salvation, a person is forgiven, they enter into a union with God which abides forever. In the Epistle to the Romans , it is mentioned that \"the wages of sin is death\", which is commonly interpreted as, if one repents for his sins, such person will inherit salvation.\n\nIn Jewish Christianity, sin is believed to alienate the sinner from God even though He has extreme love for mankind. It has damaged and completely severed the relationship of humanity to God. That relationship can only be restored through acceptance of Jesus Christ and his death on the cross as a satisfactory sacrifice for the sins of humanity. Humanity was destined for life with God when Adam disobeyed God. The Bible in says \"For God so loved the world, as to give his only begotten Son; that whosoever believeth in him, may not perish, but may have life everlasting.\"\n\nIn Eastern Christianity, sin is viewed in terms of its effects on relationships, both among people and likewise between people and God. Also as in Jewish Christianity, Sin is likewise seen as the refusal to follow God's plan and the desire to be \"like God\" (as stated in Genesis 3:5) and thus in direct opposition to God's will (see the account of Adam and Eve in Genesis).\nOriginal sin is a Western concept that states that sin entered the human world through Adam and Eve's sin in the Garden of Eden and that human beings have since lived with the consequences of this first sin.\n\nThe serpent who beguiled Eve to eat of the fruit was punished by having it and its kind being made to crawl on the ground and God set an enmity between them and Eve's descendants (Genesis 3:14-15). Eve was punished by the pains of childbirth and the sorrow of bringing about life that would eventually age, sicken and die (Genesis 3:16). The second part of the curse about being subordinate to Adam originates from her creation from one of Adam's ribs to be his helper (Genesis 2:18-25); the curse now clarifies that she must now obey her husband and desire only him. Adam was punished by having to work endlessly to feed himself and his family. The land would bring forth both thistles and thorns to be cleared and herbs and grain to be planted, nurtured, and harvested. The second part of the curse about his mortality is from his origin as red clay - he is from the land and he and his descendants would return to it when buried after death. When Adam's son Cain slew his brother Abel, he introduced murder into the world (Genesis 4:8-10). For his punishment, God banished him as a fugitive, but first marked him with a sign that would protect him and his descendants from harm (Genesis 4:11-16).\n\nOne concept of sin deals with things that exist on Earth, but not in Heaven. Food, for example, while a necessary good for the (health of the temporal) body, is not of (eternal) transcendental living and therefore its \"excessive\" savoring is considered a sin. The unforgivable sin (or eternal sin) is a sin that can never be forgiven;\nMatthew 12:30-32 :\n\" He that is not with me, is against me: and he that gathereth not with me, scattereth. And Therefore I say to you: Every sin and blasphemy shall be forgiven men, but the blasphemy of the Spirit shall not be forgiven. And whosoever shall speak a word against the Son of man, it shall be forgiven him: but he that shall speak against the Holy Ghost, it shall not be forgiven him, neither in this world, nor in the world to come.\"\n\nIn Catholic Christianity sins are classified into grave sins called mortal sins and less serious sins called venial sin. Mortal sins cause one to lose salvation unless the sinner repents and venial sins require some sort of penance either on Earth or in Purgatory.\n\nJesus was said to have paid for the complete mass of sins past, present, and to come in future. Even inevitable sin is said to have already been cleansed.\n\nThe Lamb of God was and is God himself and is therefore sinless. In the Old Testament, Leviticus 16:21 states that ‘the laying on of hands’ was the action that the High Priest Aaron was ordered to do yearly by God to take sins of Israel's nation onto a spotless young lamb.\n\nIn Hinduism, the term \"sin\" (\"\" in Sanskrit) is often used to describe actions that create negative Karma by violating moral and ethical codes, which automatically brings negative consequences. This is somewhat similar to Abrahamic sin in the sense that pāpa is considered a crime against the laws of God, which is known as (1) Dharma, or moral order, and (2) one's own self, but another term \"aparadha\" is used for grave offences. The term papa cannot be taken however, in literal sense as that of a sin. This is because there is no consensus regarding the nature of ultimate reality or God in Hinduism. Only, the vedanta school being unambiguously theistic, whereas no anthropomorphic God exists in the rest five schools namely Samkhya, Nyaya Yoga, Vaishashikha, and Purva-Mimansa . The term \"papa\" however in the strictest sense refers to actions which bring about wrong/unfavourable consequences, not relating to a specific divine will in the absolute sense. To conclude, considering a lack of consensus regarding the nature of ultimate reality in Hinduism, it can be considered that \"papa\" has lesser insistence on God for it be translated as Sin, and that there is no exact equivalent to Sin in Hinduism.\n\nIn Islamic ethics, Muslims see sin as anything that goes against the commands of Allah (God), a breach of the laws and norms laid down by religion. Islamic terms for sin include \"dhanb\" and \"khaṭīʾa\", which are synonymous and refer to intentional sins; \"khiṭʾ\", which means simply a sin; and \"ithm\", which is used for grave sins.\n\nIn Jainism, the word for sin is the Sanskrit word पाप (\"paap\"), which is the antithesis of पुण्य (\"punya\") meaning merit. A \"jiva\" (a being) acquires sin based on its karma, if it hurts anyone, causes someone to hurt anyone, or commends hurting anyone by thought, speech or action. Anyone, here, refers to literally any living organism, but not limited to human beings.\n\nNo \"jiva\" can achieve \"moksha\" (ultimate emancipation) without ceasing to accumulate karma and shedding off the already accumulated karma entirely. A \"jiva\" accumulates karma if it resorts to violence, non-chastity, falsehood, stealing, and possessiveness. A \"jiva\" ceases to accumulate karma if he resorts to the golden trio of \"samyak gyan\" (right knowledge), \"samyak darshan\" (right sight) and \"samyak charitra\" (right character). A \"jiva\" begins to shed off the accumulated karma by resorting to penance, repentance, vows and by exterminating foes of lust, anger, attachment, aversion, ignorance and fallacy.\n\nIf a \"jiva\" does not give up sin, his karma will keep accumulating and no sin can be absolved without getting its fruit or repenting for it. Thus such a \"jiva\" is bound to remain in the worldly cycle of constant reincarnation, wherein it will keep taking rebirths, into any of the four broad types of living organisms, depending on the magnitude and nature of karma accumulated in previous birth(s). The four types are \"dev\" (beings of heaven, including deities), \"manushya\" (human), \"tiryanch\" (plants, animals, insects, etc.) and \"naarki\" (beings of hell). \n\nDuring this cycle of getting born and dying for infinity, the \"jiva\" will have to then live the life of the organism he is and while living it, the \"jiva\" will again perform more karma. This will again lead to rebirth and again performing more karma. Thus, the cycle continues.\n\nIt is important to note that Jains believe that for complete liberation, not only the \"sinful karma\" but even the \"meritorious karma\" needs to be shed off. This means that a jiva can truly attain moksha, only if the soul is completely and absolutely pure and devoid of any accumulation. For instance, sins may cause the \"jiva\" to be reborn, inter alia, in hell and merits may cause it to be reborn in heaven. But heaven, like hell is a part of worldly cycle of reincarnation and not supreme moksha of the soul. Thus, if a person hypothetically keeps performing only and exclusively good deeds in his life, he may still not attain \"moksha\", because he has not yet shed off previously accumulated sins through repentance and knowledge.\n\nJains believe that only a human \"jiva\" has the capacity and the will to attain \"moksha\". Hence the \"jiva\" should use this extremely rare opportunity of being born as a human to walk on the path that brings him closer to \"moksha\". In fact, Jains take the concept of avoiding sin so seriously that not only are they completely vegetarian but some devout Jains also abstain from eating underground grown food like potatoes, onions, etc. to avoid killing small organisms. Most of the Jains are also nonalcoholics and eat before sunset each day.\n\nMainstream Judaism regards the violation of any of the 613 commandments of the Mosaic law for Jews, or the seven Noahide laws for Gentiles as a sin. Judaism teaches that all humans are inclined to sin from birth. Sin has many classifications and degrees. Some sins are punishable with death by the court, others with death by heaven, others with lashes, and others without such punishment, but no sins with willful intent go without consequence. Unwillful violations of the mitzvot (without negligence) do not count as sins. \"Sins by error\" are considered as less severe sins. When the Temple yet stood in Jerusalem, people would offer sacrifices for their misdeeds. The atoning aspect of \"korbanot\" is carefully circumscribed. For the most part, \"korbanot\" only expiate such \"sins by error\", that is, sins committed because a person forgot or did not know that this thing was a sin. In some circumstances, lack of knowledge is considered close to deliberate intent. No atonement is needed for violations committed under duress, and for the most part, \"korbanot\" cannot atone for a deliberate sin. In addition, \"korbanot\" have no expiating effect unless the person making the offering sincerely repents his or her actions before making the offering, and makes restitution to any person who suffered harm through the violation.\n\nJudaism teaches that all willful sin has consequences. The completely righteous suffer for their sins (by humiliation, poverty, and suffering that God sends them) in this world and receive their reward in the world to come. The in-between (not completely righteous or completely wicked), suffer for and repent their sins after death and thereafter join the righteous. The very evil do not repent even at the gates of hell. Such people prosper in this world to receive their reward for any good deed, but cannot be cleansed by and hence cannot leave \"gehinnom\", because they do not or cannot repent. This world can therefore seem unjust where the righteous suffer, while the wicked prosper. Many great thinkers have contemplated this.\n\nIn Mesopotamian mythology, Adamu (or Addamu/Admu, or Adapa) goes on trial for the \"sin of casting down a divinity\".\nHis crime is breaking the wings of the south wind.\n\nEvil deeds fall into two categories in Shinto: \"amatsu tsumi\", \"the most pernicious crimes of all\", and \"kunitsu tsumi\", \"more commonly called misdemeanors\".\n\n\n"}
{"id": "5499671", "url": "https://en.wikipedia.org/wiki?curid=5499671", "title": "Soil production function", "text": "Soil production function\n\nSoil production function refers to the rate of bedrock weathering into soil as a function of soil thickness. \n\nA general model suggested that the rate of physical weathering of bedrock (de/dt) can be represented as an exponential decline with soil thickness:\n\nformula_1\n\nwhere \"h\" is soil thickness [m], \"P\" [mm/year] is the potential (or maximum) weathering rate of bedrock and \"k\" [m] is an empirical constant. \n\nThe reduction of weathering rate with thickening of soil is related to the exponential decrease of temperature amplitude with increasing depth below the soil surface, and also the exponential decrease in average water penetration (for freely-drained soils). Parameters \"P\" and \"k\" are related to the climate and type of parent materials. found the value of \"P\" ranges from 0.08 to 2.0 mm/yr for sites in Northern California, and 0.05–0.14 mm/yr for sites in Southeastern Australia. Meanwhile values of \"k\" do not vary significantly, ranging from 2 to 4 m.\n\nA number of landscape evolution models have adopted the so-called \"humped\" model. This model dates back to G.K. Gilbert's \"Report on the Geology of the Henry Mountains\" (1877). Gilbert reasoned that the weathering of bedrock was fastest under an intermediate thickness of soil and slower under exposed bedrock or under thick mantled soil. This is because chemical weathering requires the presence of water. Under thin soil or exposed bedrock water tends to run off, reducing the chance of the decomposition of bedrock.\n\n\n"}
{"id": "42287389", "url": "https://en.wikipedia.org/wiki?curid=42287389", "title": "Sonochromatism", "text": "Sonochromatism\n\nSonochromatism or sonochromatopsia (Latin: Greek: Greek: \"-opsia\" (seeing)) is a neurological phenomenon in which colours are perceived as sounds. The phenomenon is created by the union between a brain and a colour-to-sound software or chip. People who report such experiences are known as sonochromats. The term was coined by Neil Harbisson to differentiate his experience of colour from people with chromesthesia or colour-to-sound synesthesia.\n\nThe main difference between chromesthesia and sonochromatism is that sonochromatism is not the union between two existing senses, instead, it is the creation of a new sense. Sonochromats do not necessarily sense colour as a visual experience and are able to perceive colours outside the limits of human vision such as infrareds and ultraviolets. Their experience of colour is not involuntary, instead, it is caused by voluntary action such as a colour-to-sound chip implant or constant use of an eyeborg. The sounds perceived for each colour are not subjective, they are objective and they are related to the light frequencies of colour.\n\nImplantable colour-to-sound chips, software, mobile apps and cyborg antennas use Harbisson's Sonochromatic Scales as a standard transposition of colour frequencies to sound frequencies.\n\nThe Sonochromatic Music Scale is a microtonal and logarithmic scale with 360 notes in an octave. Each note corresponds to a specific degree of the color wheel.\n\nHarbisson's Pure Sonochromatic Scale is a non-logarithmic scale based on the transposition of light frequencies to sound frequencies. The scale includes infrareds and ultraviolets, discards colour as being part of a colour wheel and ignores musical/logarithmic perception so it can overstep the limits of human perception.\n"}
{"id": "569650", "url": "https://en.wikipedia.org/wiki?curid=569650", "title": "Stimulus modality", "text": "Stimulus modality\n\nStimulus modality, also called sensory modality, is one aspect of a stimulus or what is perceived after a stimulus. For example, the temperature modality is registered after heat or cold stimulate a receptor. Some sensory modalities include: light, sound, temperature, taste, pressure, and smell. The type and location of the sensory receptor activated by the stimulus plays the primary role in coding the sensation. All sensory modalities work together to heighten stimuli sensation when necessary.\n\nMultimodal perception is the ability of the mammalian nervous system to combine all of the different inputs of the sensory nervous system to result in an enhanced detection or identification of a particular stimulus. Combinations of all sensory modalities are done in cases where a single sensory modality results in ambiguous and incomplete result.\n\nIntegration of all sensory modalities occurs when multimodal neurons receive sensory information which overlaps with different modalities. Multimodal neurons are found in the superior colliculus; they respond to the versatility of various sensory inputs. The multimodal neurons lead to change of behavior and assist in analyzing behavior responses to certain stimulus. Information from two or more senses is encountered. Multimodal perception is not limited to one area of the brain: many brain regions are activated when sensory information is perceived from the environment. In fact, the hypothesis of having a centralized multisensory region is receiving continually more speculation, as several regions previously uninvestigated are now considered multimodal. The reasons behind this are currently being investigated by several research groups, but it is now understood to approach these issues from a decentralized theoretical perspective. Moreover, several labs using invertebrate model organisms will provide invaluable information to the community as these are more easily studied and are considered to have decentralized nervous systems.\n\nLip reading is a multimodal process for humans. By watching movements of lips and face, humans get conditioned and practice lip reading. Silent lip reading activates the auditory cortex. When sounds are matched or mismatched with the movements of the lips, temporal sulcus of the left hemisphere becomes more active.\n\nMultimodal perception comes into effect when a unimodal stimulus fails to produce a response. Integration effect is applied when the brain detects weak unimodal signals and combines them to create a multimodal perception for the mammal. Integration effect is plausible when different stimuli are coincidental. This integration is depressed when multisensory information are not coincidentally presented.\n\n\"Polymodality\" is the feature of a single receptor of responding to multiple modalities, such as free nerve endings which can respond to temperature, mechanical stimuli (touch, pressure, stretch) or pain (nociception).\n\nThe stimulus modality for vision is light; the human eye is able to access only a limited section of the electromagnetic spectrum, between 380 and 760 nanometres. Specific inhibitory responses that take place in the visual cortex help create a visual focus on a specific point rather than the entire surrounding.\n\nTo perceive a light stimulus, the eye must first refract the light so that it directly hits the retina. Refraction in the eye is completed through the combined efforts of the cornea, lens and iris. The transduction of light into neural activity occurs via the photoreceptor cells in the retina. When there is no light, Vitamin A in the body attaches itself to another molecule and becomes a protein. The entire structure consisting of the two molecules becomes a photopigment. When a particle of light hits the photoreceptors of the eye, the two molecules come apart from each other and a chain of chemical reactions occurs. The chemical reaction begins with the photoreceptor sending a message to a neuron called the bipolar cell through the use of an action potential, or nerve impulse. Finally, a message is sent to the ganglion cell and then finally the brain.\n\nThe eye is able to detect a visual stimulus when the photons (light packets) cause a photopigment molecule, primarily rhodopsin, to come apart. Rhodopsin, which is usually pink, becomes bleached in the process. At high levels of light, photopigments are broken apart faster than can be regenerated. Because a low number of photopigments have been regenerated, the eyes are not sensitive to light. When entering a dark room after being in a well lit area, the eyes require time for a good quantity of rhodopsin to regenerate. As more time passes, there is a higher chance that the photons will split an unbleached photopigment because the rate of regeneration will have surpassed the rate of bleaching. This is called adaptation.\n\nHumans are able to see an array of colours because light in the visible spectrum is made up of different wavelengths (from 380 to 760 nm). Our ability to see in colour is due to three different cone cells in the retina, containing three different photopigments. The three cones are each specialized to best pick up a certain wavelength (420, 530 and 560 nm or roughly the colours blue, green and red). The brain is able to distinguish the wavelength and colour in the field of vision by figuring out which cone has been stimulated. The physical dimensions of colour include wavelength, intensity and purity while the related perceptual dimensions include hue, brightness and saturation.\n\nPrimates are the only mammals with colour vision.\n\nThe Trichromatic theory was proposed in 1802 by Thomas Young. According to Young, the human visual system is able to create any colour through the collection of information from the three cones. The system will put together the information and systematize a new colour based on the amount of each hue that has been detected.\n\nSome studies show that subliminal stimuli can affect attitude. In a 1992 study Krosnick, Betz, Jussim and Lynn conducted a study where participants were shown a series of slides in which different people were going through normal every day activities (i.e. going to the car, sitting in a restaurant). These slides were preceded by slides that caused either positive emotional arousal (i.e. bridal couple, a child with a Mickey Mouse doll) or negative emotional arousal (i.e. a bucket of snakes, a face on fire) for a period 13 milliseconds that participants consciously perceived as a sudden flash of light. None of the individuals were told of the subliminal images. The experiment found that during the questionnaire round, participants were more likely to assign positive personality traits to those in the pictures that were preceded by the positive subliminal images and negative personality traits to those in the pictures that were preceded by the negative subliminal images.\n\nSome common tests that measure visual health include visual acuity tests, refraction tests, visual field tests and colour vision tests. Visual acuity tests are the most common tests and they measure the ability to bring details into focus at different distances. Usually this test is conducted by having participants read a map of letters or symbols while one eye is covered. Refraction tests measure the eye's need for glasses or corrective lenses. This test is able to detect whether a person may be nearsighted or farsighted. These conditions occur when the light rays entering the eye are unable to converge on a single spot on the retina. Both refractive errors require corrective lenses in order to cure blurriness of vision. Visual field tests detect any gaps in peripheral vision. In healthy normal vision, an individual should be able to partially perceive objects to the left or right of their field of view using both eyes at one time. The center field of vision is seen in most detail. Colour vision tests are used to measure one's ability to distinguish colours. It is used to diagnose colour blindness. This test is also used as an important step in some job screening processes as the ability to see colour in such jobs may be crucial. Examples include military work or law enforcement.\n\nThe stimulus modality for hearing is sound. Sound is created through changes in the pressure of the air. As an object vibrates, it compresses the surrounding molecules of air as it moves towards a given point and expands the molecules as it moves away from the point. Periodicity in sound waves is measured in hertz. Humans, on average, are able to detect sounds as pitched when they contain periodic or quasi-periodic variations that fall between the range of 30 to 20000 hertz.\n\nWhen there are vibrations in the air, the eardrum is stimulated. The eardrum collects these vibrations and sends them to receptor cells. The ossicles which are connected to the eardrum pass the vibrations to the fluid-filled cochlea. Once the vibrations reach the cochlea, the stirrup (part of the ossicles) puts pressure on the oval window. This opening allows the vibrations to move through the liquid in the cochlea where the receptive organ is able to sense it.\n\nThere are many different qualities in sound stimuli including loudness, pitch and timbre.\n\nThe human ear is able to detect differences in pitch through the movement of auditory hair cells found on the basilar membrane. High frequency sounds will stimulate the auditory hair cells at the base of the basilar membrane while medium frequency sounds cause vibrations of auditory hair cells located at the middle of the basilar membrane. For frequencies that are lower than 200 Hz, the tip of the basilar membrane vibrates in sync with the sound waves. In turn, neurons are fired at the same rate as the vibrations. The brain is able to measure the vibrations and is then aware of any low frequency pitches.\n\nWhen a louder sound is heard, more hair cells are stimulated and the intensity of firing of axons in the cochlear nerve is increased. However, because the rate of firing also defines low pitch the brain has an alternate way of encoding for loudness of low frequency sounds. The number of hair cells that are stimulated is thought to communicate loudness in low pitch frequencies.\n\nAside from pitch and loudness, another quality that distinguishes sound stimuli is timbre. Timbre allows us to hear the difference between two instruments that are playing at the same frequency and loudness, for example. When two simple tones are put together they create a complex tone. The simple tones of an instrument are called harmonics or overtones. Timbre is created by putting the harmonics together with the fundamental frequency (a sound's basic pitch). When a complex sound is heard, it causes different parts in the basilar membrane to become simultaneously stimulated and flex. In this way, different timbres can be distinguished.\n\nA number of studies have shown that a human fetus will respond to sound stimuli coming from the outside world. In a series of 214 tests conducted on 7 pregnant women, a reliable increase in fetal movement was detected in the minute directly following the application of a sound stimulus to the abdomen of the mother with a frequency of 120 per second.\n\nHearing tests are administered to ensure optimal function of the ear and to observe whether or not sound stimuli is entering the ear drum and reaching the brain as should be. The most common hearing tests require the spoken response to words or tones. Some hearing tests include the whispered speech test, pure tone audiometry, the tuning fork test, speech reception and word recognition tests, otoacoustic emissions (OAE) test and auditory brainstem response (ABR) test.\n\nDuring a whispered speech test, the participant is asked to cover the opening of one ear with a finger. The tester will then step back 1 to 2 feet behind the participant and say a series of words in a soft whisper. The participant is then asked to repeat what is heard. If the participant is unable to distinguish the word, the tester will speak progressively louder until the participant is able to understand what is being said. The other ear is then tested.\n\nIn pure tone audiometry, an audiometer is used to play a series of tones using headphones. The participants listen to the tones which will vary in pitch and loudness. The test will play with the volume controls and the participant is asked to signal when he or she can no longer hear the tone being played. The testing is completed after listening to a range of pitches. Each ear is tested individually.\n\nDuring the tuning fork test, the tester will have the tuning fork vibrate so that it makes a sound. The tuning fork is placed in a specific place around the participant and hearing is observed. In some instances, individuals will show trouble hearing in places such as behind the ear.\n\nSpeech recognition and word recognition tests measure how well an individual can hear normal day-to-day conversation. The participant is told to repeat conversation being spoken at different volumes. The spondee threshold test is a related test that detects the loudness at which the participant is able to repeat half of a list of two syllable words or spondees.\n\nOtoacoustic emissions test (OAE) and auditory brainstem response (ABR) testing measures the brain's response to sounds. The OAE measures hearing of newborns by placing an emitting sound into the baby's ear through a probe. A microphone placed in the baby's ear canal will pick up the inner ear's response to sound stimulation and allows for observation. The ABR, also known as the brainstem auditory evoked response (BAER) test or auditory brainstem evoked potential (ABEP) test measure the brain's response to clicking sounds sent through headphones. Electrodes on the scalp and earlobes record a graph of the response.\n\nIn mammals, taste stimuli are encountered by axonless receptor cells located in taste buds on the tongue and\npharynx. Receptor cells disseminate onto different neurons and convey the message of a particular taste in a single medullar nucleus. This pheromone detection system deals with taste stimuli. The pheromone detection system is distinct from the normal taste system, and is designed like the olfactory system.\n\nIn insect and mammalian taste, receptor cells changes into attractive or aversive stimulus. The number of taste receptors in a mammalian tongue and on the tongue of the fly (labellum) is same in amount. Most of the receptors are dedicated to detect repulsive ligand.\n\nPerceptions of taste is generated by the following sensory afferents: gustatory, olfactory, and somatosensory fibers. Taste perception is created by combining multiple sensory inputs. Different modalities help determine perception of taste especially when attention is drawn to particular sensory characteristics which is different from taste.\n\nImpression of both taste and smell occurs in heteromodal regions of the limbic and paralimbic brain. Taste–odor\nintegration occurs at earlier stages of processing. By life experience, factors such as the physiological significance of a given stimulus is perceived. Learning and affective processing are the primary functions of limbic and paralimbic brain. Taste perception is a combination of oral somatosensation and retronasal olfaction.\n\nThe sensation of taste come from oral somatosensory stimulation and with retronasal olfaction. The perceived pleasure encountered when eating and drinking is influenced by:\n\nTemperature modality excites or elicits a symptom through cold or hot temperature. Different mammalian species have different temperature modality.\n\nThe cutaneous somatosensory system detects changes in temperature. The perception begins when thermal stimuli from a homeostatic set-point excite temperature specific sensory nerves in the skin. Then with the help of sensing range, specific thermosensory fibers respond to warmth and to cold. Then specific cutaneous cold and warm receptors conduct units that exhibit a discharge at constant skin temperature.\n\nWarm and cold sensitive nerve fibers differ in structure and function. The cold-sensitive and warm-sensitive nerve fibers are underneath the skin surface. Terminals of each temperature-sensitive fiber do not branch away to different organs in the body. They form a small sensitive point which are unique from neighboring fibers. Skin used by the single receptor ending of a temperature-sensitive nerve fiber is small. There are 20 cold points per square centimeter in the lips, 4 in the finger, and less than 1 cold point per square centimeter in trunk areas. There are 5 times as many cold sensitive points as warm sensitive points.\n\nThe sense of touch, or tactile perception, is what allows organisms to feel the world around them. The environment acts as an external stimulus, and tactile perception is the act of passively exploring the world to simply sense it. To make sense of the stimuli, an organism will undergo active exploration, or haptic perception, by moving their hands or other areas with environment-skin contact. This will give a sense of what is being perceived, and give information about size, shape, weight, temperature, and material. Tactile stimulation can be direct in the form of bodily contact, or indirect through the use of a tool or probe. Direct and indirect send different types messages to the brain, but both provide information regarding roughness, hardness, stickiness, and warmth. The use of a probe elicits a response based on the vibrations in the instrument rather than direct environmental information. Tactual perception gives information regarding cutaneous stimuli (pressure, vibration, and temperature), kinaesthetic stimuli (limb movement), and proprioceptive stimuli (position of the body). There are varying degrees of tactual sensitivity and thresholds, both between individuals and between different time periods in an individual's life. It has been observed that individuals have differing levels of tactile sensitivity between each hand. This may be due to callouses forming on the skin of the most used hand, creating a buffer between the stimulus and the receptor. Alternately, the difference in sensitivity may be due to a difference in the cerebral functions or ability of the left and right hemisphere. Tests have also shown that deaf children have a greater degree of tactile sensitivity than that of children with normal hearing ability, and that girls generally have a greater degree of sensitivity than that of boys.\n\nTactile information is often used as additional stimuli to resolve a sensory ambiguity. For example, a surface can be seen as rough, but this inference can only be proven through touching the material. When sensory information from each modality involved corresponds, the ambiguity is resolved.\n\nTouch messages, in comparison to other sensory stimuli, have a large distance to travel to get to the brain. Tactual perception is achieved through the response of mechanoreceptors in the skin that detect physical stimuli. The response from a mechanoreceptor detecting pressure can be experienced as a touch, discomfort, or pain, and the force of pressure is measured by a pressure algometer and a dolorimeter. Mechanoreceptors are situated in highly vascularized skin, and appear in both glabrous and hairy skin. Each mechanoreceptor is tuned to a different sensitivity, and will fire its action potential only when there is enough energy. The axons of these single tactile receptors will converge into a single nerve trunk, and the signal is then sent to the spinal cord where the message makes its way to the somatosensory systems in the brain.\n\nThere are four types of mechanoreceptors: Meissner corpuscles and merkel cell neurite complexes, located between the epidermis and dermis, and Pacinian corpuscles and Ruffini endings, located deep within the dermis and subcutaneous tissue. Mechanoreceptors are classified in terms of their adaptation rate and the size of their receptive field. Specific mechanoreceptors and their functions include:\n\n\nA common test used to measure the sensitivity of a person to tactile stimuli is measuring their two-point touch threshold. This is the smallest separation of two points at which two distinct points of contact can be sensed rather than one. Different parts of the body have different degrees of tactile acuity, with extremities such as the fingers, face, and toes being the most sensitive. When two distinct points are perceived, it means that your brain receives two different signals. The differences of acuity for different parts of the body are the result of differences in the concentration of receptors.\n\nTactile stimulation is used in clinical psychology through the method of prompting. Prompting is the use of a set of instructions designed to guide a participant through learning a behavior. A physical prompt involves stimulation in the form of physically guided behavior in the appropriate situation and environment. The physical stimulus perceived through prompting is similar to the physical stimulus that would be experienced in a real-world situation, and is makes the target behavior more likely in a real situation.\n\nThe sense of smell is called olfaction. All materials constantly shed molecules, which float into the nose or are sucked in through breathing. Inside the nasal chambers is the neuroepithelium, a lining deep within the nostrils that contains the receptors responsible for detecting molecules that are small enough to smell. These receptor neurons then synapse at the olfactory cranial nerve (CN I), which sends the information to the olfactory bulbs in the brain for initial processing. The signal is then sent to the remaining olfactory cortex for more complex processing.\n\nAn olfactory sensation is called an odor. For a molecule to trigger olfactory receptor neurons, it must have specific properties. The molecule must be:\n\nHowever, humans do not process the smell of various common molecules such as those present in the air.\n\nOur olfactory ability can vary due to different conditions. For example, our olfactory detection thresholds can change due to molecules with differing lengths of carbon chains. A molecule with a longer carbon chain is easier to detect, and has a lower detection threshold. Additionally, women generally have lower olfactory thresholds than men, and this effect is magnified during a woman's ovulatory period. People can sometimes experience a hallucination of smell, as in the case of phantosmia.\n\nOlfaction interacts with other sensory modalities in significant ways. The strongest interaction is that of olfaction with taste. Studies have shown that an odor coupled with a taste increases the perceived intensity of the taste, and that an absence of a corresponding smell decreases the perceived intensity of a taste. The olfactory stimulation can occur before or during the episode of taste stimulation. The dual perception of the stimulus produces an interaction that facilitates association of the experience through an additive neural response and memorization of the stimulus. This association can also be made between olfactory and tactile stimuli during the act of swallowing. In each case, temporal synchrony is important.\n\nA common psychophysical test of olfactory ability is the triangle test. In this test, the participant is given three odors to smell. Of these three odors, two are the same and one is different, and the participant must choose which odor is the unique one. To test the sensitivity of olfaction, the staircase method is often used. In this method, the odor's concentration is increased until the participant is able to sense it, and subsequently decreased until the participant reports no sensation.\n\n"}
{"id": "314383", "url": "https://en.wikipedia.org/wiki?curid=314383", "title": "Supertask", "text": "Supertask\n\nIn philosophy, a supertask is a countably infinite sequence of operations that occur sequentially within a finite interval of time. Supertasks are called \"hypertasks\" when the number of operations becomes uncountably infinite. A hypertask that includes one operation for each ordinal number is called an \"ultratask\". The term \"supertask\" was coined by the philosopher James F. Thomson, who devised Thomson's lamp. The term \"hypertask\" derives from Clark and Read in their paper of that name.\n\nThe origin of the interest in supertasks is normally attributed to Zeno of Elea. Zeno claimed that motion was impossible. He argued as follows: suppose our burgeoning \"mover\", Achilles say, wishes to move from A to B. To achieve this he must traverse half the distance from A to B. To get from the midpoint of AB to B Achilles must traverse half \"this\" distance, and so on and so forth. However many times he performs one of these \"traversing\" tasks there is another one left for him to do before he arrives at B. Thus it follows, according to Zeno, that motion (travelling a non-zero distance in finite time) is a supertask. Zeno further argues that supertasks are not possible (how can this sequence be completed if for each traversing there is another one to come?). It follows that motion is impossible.\n\nZeno's argument takes the following form:\n\n\nMost subsequent philosophers reject Zeno's bold conclusion in favor of common sense. Instead they turn his argument on its head (assuming it's valid) and take it as a proof by contradiction where the possibility of motion is taken for granted. They accept the possibility of motion and apply modus tollens (contrapositive) to Zeno's argument to reach the conclusion that either motion is not a supertask or not all supertasks are impossible.\n\nZeno himself also discusses the notion of what he calls \"Achilles and the tortoise\". Suppose that Achilles is the fastest runner, and moves at a speed of 1 m/s. Achilles chases a tortoise, an animal renowned for being slow, that moves at 0.1 m/s. However, the tortoise starts 0.9 metres ahead. Common sense seems to decree that Achilles will catch up with the tortoise after exactly 1 second, but Zeno argues that this is not the case. He instead suggests that Achilles must inevitably come up to the point where the tortoise has started from, but by the time he has accomplished this, the tortoise will already have moved on to another point. This continues, and every time Achilles reaches the mark where the tortoise was, the tortoise will have reached a new point that Achilles will have to catch up with; while it begins with 0.9 metres, it becomes an additional 0.09 metres, then 0.009 metres, and so on, infinitely. While these distances will grow very small, they will remain finite, while Achilles' chasing of the tortoise will become an unending supertask. Much commentary has been made on this particular paradox; many assert that it finds a loophole in common sense.\n\nJames F. Thomson believed that motion was not a supertask, and he emphatically denied that supertasks are possible. The proof Thomson offered to the latter claim involves what has probably become the most famous example of a supertask since Zeno. Thomson's lamp may either be on or off. At time t = 0 the lamp is off, at time t = 1/2 it is on, at time t = 3/4 (= 1/2 + 1/4) it is off, t = 7/8 (= 1/2 + 1/4 + 1/8) it is on, etc. The natural question arises: at t = 1 is the lamp on or off? There does not seem to be any non-arbitrary way to decide this question. Thomson goes further and claims this is a contradiction. He says that the lamp cannot be on for there was never a point when it was on where it was not immediately switched off again. And similarly he claims it cannot be off for there was never a point when it was off where it was not immediately switched on again. By Thomson's reasoning the lamp is neither on nor off, yet by stipulation it must be either on or off this is a contradiction. Thomson thus believes that supertasks are impossible.\n\nPaul Benacerraf believes that supertasks are at least logically possible despite Thomson's apparent contradiction. Benacerraf agrees with Thomson insofar as that the experiment he outlined does not determine the state of the lamp at t = 1. However he disagrees with Thomson that he can derive a contradiction from this, since the state of the lamp at t = 1 need not be logically determined by the preceding states. Logical implication does not bar the lamp from being on, off, or vanishing completely to be replaced by a horse-drawn pumpkin. There are possible worlds in which Thomson's lamp finishes on, and worlds in which it finishes off not to mention countless others where weird and wonderful things happen at t = 1. The seeming arbitrariness arises from the fact that Thomson's experiment does not contain enough information to determine the state of the lamp at t = 1, rather like the way nothing can be found in Shakespeare's play to determine whether Hamlet was right- or left-handed.\nSo what about the contradiction? Benacerraf showed that Thomson had committed a mistake. When he claimed that the lamp could not be on because it was never on without being turned off again this applied only to instants of time \"strictly less than 1\". It does not apply to 1 because 1 does not appear in the sequence {0, 1/2, 3/4, 7/8, …} whereas Thomson's experiment only specified the state of the lamp for times in this sequence.\n\nMost of the modern literature comes from the descendants of Benacerraf, those who tacitly accept the possibility of supertasks. Philosophers who reject their possibility tend not to reject them on grounds such as Thomson's but because they have qualms with the notion of infinity itself. Of course there are exceptions. For example, McLaughlin claims that Thomson's lamp is inconsistent if it is analyzed with internal set theory, a variant of real analysis.\n\nIf supertasks are possible, then the truth or falsehood of unknown propositions of number theory, such as Goldbach's conjecture, or even undecidable propositions could be determined in a finite amount of time by a brute-force search of the set of all natural numbers. This would, however, be in contradiction with the Church-Turing thesis. Some have argued this poses a problem for intuitionism, since the intuitionist must distinguish between things that cannot in fact be proven (because they are too long or complicated; for example Boolos's \"Curious Inference\") but nonetheless are considered \"provable\", and those which \"are\" provable by infinite brute force in the above sense.\n\nSome have claimed Thomson's lamp is physically impossible since it must have parts moving at speeds faster than the speed of light (e.g., the lamp switch). Adolf Grünbaum suggests that the lamp could have a strip of wire which, when lifted, disrupts the circuit and turns off the lamp; this strip could then be lifted by a smaller distance each time the lamp is to be turned off, maintaining a constant velocity. However, such a design would ultimately fail, as eventually the distance between the contacts would be so small as to allow electrons to jump the gap, preventing the circuit from being broken at all.\n\nOther physically possible supertasks have been suggested. In one proposal, one person (or entity) counts upward from 1, taking an infinite amount of time, while another person observes this from a frame of reference where this occurs in a finite space of time. For the counter, this is not a supertask, but for the observer, it is. (This could theoretically occur due to time dilation, for example if the observer were falling into a black hole while observing a counter whose position is fixed relative to the singularity.)\n\nDavies in his paper \"Building Infinite Machines\" concocted a device which he claims is physically possible up to infinite divisibility. It involves a machine which creates an exact replica of itself but has half its size and twice its speed. Still, for either a human or any device, to perceive or act upon the state of the lamp some measurement has to be done, for example the light from the lamp would have to reach an eye or a sensor. Any such measurement will take a fixed frame of time, no matter how small and, therefore, at some point measurement of the state will be impossible. Since the state at t=1 can not be determined even in principle, it is not meaningful to speak of the lamp being either on or off.\n\nGustavo E. Romero in the paper 'The collapse of supertasks' maintains that any attempt to carry out a supertask will result in the formation of a black hole, making supertasks physically impossible.\n\nThe impact of supertasks on theoretical computer science has triggered some new and interesting work, for example Hamkins and Lewis \"Infinite Time Turing Machine\".\n\nSuppose there is a jar capable of containing infinitely many marbles and an infinite collection of marbles labelled 1, 2, 3, and so on. At time \"t\" = 0, marbles 1 through 10 are placed in the jar and marble 1 is taken out. At \"t\" = 0.5, marbles 11 through 20 are placed in the jar and marble 2 is taken out; at \"t\" = 0.75, marbles 21 through 30 are put in the jar and marble 3 is taken out; and in general at time \"t\" = 1 − 0.5, marbles 10\"n\" + 1 through 10\"n\" + 10 are placed in the jar and marble \"n\" + 1 is taken out. How many marbles are in the jar at time \"t\" = 1?\n\nOne argument states that there should be infinitely many marbles in the jar, because at each step before \"t\" = 1 the number of marbles increases from the previous step and does so unboundedly. A second argument, however, shows that the jar is empty. Consider the following argument: if the jar is non-empty, then there must be a marble in the jar. Let us say that that marble is labeled with the number \"n\". But at time \"t\" = 1 − 0.5, the \"n\"th marble has been taken out, so marble \"n\" cannot be in the jar. This is a contradiction, so the jar must be empty. The Ross-Littlewood paradox is that here we have two seemingly perfectly good arguments with completely opposite conclusions.\n\nFurther complications are introduced by the following variant. Suppose that we follow the same process as above, but instead of taking out marble 1 at \"t\" = 0, one takes out marble 2. And, at \"t\" = 0.5 one takes out marble 3, at \"t\" = 0.75 marble 4, etc. Then, one can use the same logic from above to show that while at \"t\" = 1, marble 1 is still in the jar, no other marbles can be left in the jar. Similarly, one can construct scenarios where in the end, 2 marbles are left, or 17 or, of course, infinitely many. But again this is paradoxical: given that in all these variations the same number of marbles are added or taken out at each step of the way, how can the end result differ?\n\nSome people decide to simply bite the bullet and say that apparently, the end result does depend on which marbles are taken out at each instant. However, one immediate problem with that view is that one can think of the thought experiment as one where none of the marbles are actually labeled, and thus all the above variations are simply different ways of describing the same process; it seems unreasonable to say that the end result of the one actual process depends on the way we describe what happens.\n\nMoreover, Allis and Koetsier offer the following variation on this thought experiment: at \"t\" = 0, marbles 1 to 9 are placed in the jar, but instead of taking a marble out they scribble a 0 after the 1 on the label of the first marble so that it is now labeled \"10\". At \"t\" = 0.5, marbles 11 to 19 are placed in the jar, and instead of taking out marble 2, a 0 is written on it, marking it as 20. The process is repeated ad infinitum. Now, notice that the end result at each step along the way of this process is the same as in the original experiment, and indeed the paradox remains: Since at every step along the way, more marbles were added, there must be infinitely marbles left at the end, yet at the same time, since every marble with number \"n\" was taken out at \"t\" = 1 − 0.5, no marbles can be left at the end. However, in this experiment, no marbles are ever taken out, and so any talk about the end result 'depending' on which marbles are taken out along the way is made impossible.\n\nA bare-naked variation that really goes straight to the heart of all of this goes as follows: at \"t\" = 0, there is one marble in the jar with the number 0 scribbled on it. At \"t\" = 0.5, the number 0 on the marble gets replaced with the number 1, at \"t\" = 0.75, the number gets changed to 2, etc. Now, no marbles are ever added to or removed from the jar, so at t = \"1\", there should still be exactly that one marble in the jar. However, since we always replaced the number on that marble with some other number, it should have some number \"n\" on it, and that is impossible because we know precisely when that number was replaced, and never repeated again later. In other words, we can also reason that no marble can be left at the end of this process, which is quite a paradox.\n\nOf course, it would be wise to heed Benacerraf’s words that the states of the jars before \"t\" = 1 do not logically determine the state at \"t\" = 1. Thus, neither Ross’s or Allis’s and Koetsier’s argument for the state of the jar at \"t\" = 1 proceeds by logical means only. Therefore, some extra premise must be introduced in order to say anything about the state of the jar at \"t\" = 1. Allis and Koetsier believe such an extra premise can be provided by the physical law that the marbles have continuous space-time paths, and therefore from the fact that for each \"n\", marble \"n\" is out of the jar for \"t\" < 1, it must follow that it must still be outside the jar at \"t\" = 1 by continuity. Thus, the contradiction, and the paradox, remains.\n\nOne obvious solution to all these conundrums and paradoxes is to say that supertasks are impossible. If supertasks are impossible, then the very assumption that all of these scenarios had some kind of 'end result' to them is mistaken, preventing all of the further reasoning (leading to the contradictions) to go through.\n\nThere has been considerable interest in J. A. Benardete’s “Paradox of the Gods”:\n\nThis supertask, proposed by J. P. Laraudogoitia, is an example of indeterminism in Newtonian mechanics. The supertask consists of an infinite collection of stationary point masses. The point masses are all of mass \"m\" and are placed along a line \"AB\" that is \"a\" meters in length at positions \"B\", \"AB\" / 2, \"AB\" / 4, \"AB\" / 8, and so on. The first particle at \"B\" is accelerated to a velocity of one meter per second towards \"A\". According to the laws of Newtonian mechanics, when the first particle collides with the second, it will come to rest and the second particle will inherit its velocity of 1 m/s. This process will continue as an infinite amount of collisions, and after 1 second, all the collisions will have finished since all the particles were moving at 1 meter per second. However no particle will emerge from \"A\", since there is no last particle in the sequence. It follows that all the particles are now at rest, contradicting conservation of energy. Now the laws of Newtonian mechanics are time-reversal-invariant; that is, if we reverse the direction of time, all the laws will remain the same. If time is reversed in this supertask, we have a system of stationary point masses along \"A\" to \"AB\" / 2 that will, at random, spontaneously start colliding with each other, resulting in a particle moving away from \"B\" at a velocity of 1 m/s. Alper and Bridger have questioned the reasoning in this supertask invoking the distinction between actual and potential infinity.\n\nProposed by E. B. Davies, this is a machine that can, in the space of half an hour, create an exact replica of itself that is half its size and capable of twice its replication speed. This replica will in turn create an even faster version of itself with the same specifications, resulting in a supertask that finishes after an hour. If, additionally, the machines create a communication link between parent and child machine that yields successively faster bandwidth and the machines are capable of simple arithmetic, the machines can be used to perform brute-force proofs of unknown conjectures. However, Davies also points out that due to fundamental properties of the real universe such as quantum mechanics, thermal noise and information theory his machine can't actually be built.\n\n\n\n"}
{"id": "476197", "url": "https://en.wikipedia.org/wiki?curid=476197", "title": "Texas sharpshooter fallacy", "text": "Texas sharpshooter fallacy\n\nThe Texas sharpshooter fallacy is an informal fallacy which is committed when differences in data are ignored, but similarities are stressed. From this reasoning, a false conclusion is inferred. This fallacy is the philosophical or rhetorical application of the multiple comparisons problem (in statistics) and apophenia (in cognitive psychology). It is related to the clustering illusion, which is the tendency in human cognition to interpret patterns where none actually exist.\n\nThe name comes from a joke about a Texan who fires some gunshots at the side of a barn, then paints a target centered on the tightest cluster of hits and claims to be a sharpshooter.\n\nThe Texas sharpshooter fallacy often arises when a person has a large amount of data at his or her disposal, but only focuses on a small subset of that data. Some factor other than the one attributed may give all the elements in that subset some kind of common property (or pair of common properties, when arguing for correlation). If the person attempts to account for the likelihood of finding \"some\" subset in the large data with \"some\" common property by a factor other than its actual cause, then that person is likely committing a Texas sharpshooter fallacy.\n\nThe fallacy is characterized by a lack of a specific hypothesis prior to the gathering of data, or the formulation of a hypothesis only after data have already been gathered and examined. Thus, it typically does not apply if one had an \"ex ante\", or prior, expectation of the particular relationship in question before examining the data. For example one might, prior to examining the information, have in mind a specific physical mechanism implying the particular relationship. One could then use the information to give support or cast doubt on the presence of that mechanism. Alternatively, if additional information can be generated using the same process as the original information, one can use the original information to construct a hypothesis, and then test the hypothesis on the new data. (See hypothesis testing.) What one \"cannot\" do is use \"the same\" information to construct \"and\" test the same hypothesis (see hypotheses suggested by the data)—to do so would be to commit the Texas sharpshooter fallacy.\n\n\n\n\n"}
{"id": "42612939", "url": "https://en.wikipedia.org/wiki?curid=42612939", "title": "United Arab Emirates Legal Process", "text": "United Arab Emirates Legal Process\n\nThe United Arab Emirates is a middle-eastern country, located at the end of the Arabian Peninsula on the Persian Gulf, consisting of seven emirates known as Abu Dhabi, Dubai, Sharjah, Ajman, Umm al-Quwain, Ras al-Khaimah and Fujairah. Each emirate is governed by an emir who jointly forms the Federal Supreme Council (FSC). The Federal System of government includes the Supreme Council, Cabinet, Council of Ministers, parliamentary body, Federal National Council and independent judiciary. The Federal Supreme Council is the highest constitutional authority that has legislative and executive powers with the ability to ratify federal laws, decrees, and plans general policies. Its jurisdictions are derived from French, Roman, Egyptian and Islamic law.\n\nThere are three main branches of the court structure: civil, criminal, and sharia or Islamic (1) (the precepts are set forth in the Quranic verses and example set by the Islamic prophet Muhammad in the Sunnah). (2) Sharia or Islamic courts work alongside the civil and criminal courts. The responsibilities include civil matters between Muslims. Non-Muslims will not appear before a Sharia court in any matter due to the differences in religious beliefs. (3) Sharia or Islamic courts have the exclusive jurisdiction over family disputes divorce, inheritance, child custody, child abuse, and guardianship of minors. (3) However, at the federal level the Sharia court may hear appeals of criminal cases including rape, robbery, and driving under the influence of alcohol. (3)\nDubai is its own sovereign nation with its own federal judicial system with separate court systems that are not subjected to the federal Supreme Court. Dubai's court structure consists of:\n\n\nCriminal actions commence with a police investigation which is transferred to the prosecutor's office within 48 hours of filing a complaint. The prosecutor will then hear and document statements from witnesses to determine if charges will be pressed or dropped, which must be completed 14 days from receiving the case from the police. Once the prosecutor has decided if charges will be pressed, the parties can proceed in hiring an attorney. All attorneys must be licensed to practice law and must be approved by an official deed notarized by a notary public to try the case. (3)\n\nUnder the United Arab Emirates constitution all defendants are innocent until proven guilty. All trials are public except for national security cases trials that the honoring judge can rule to be conducted in privates if any evidence, testimonies or results are detrimental to societal morality. (4) Being that majority of the trials are public the emirs decided on having no juries in place in the courtroom. Also, all proceedings are delivered in the native language. Except the sentencing portion, translators are available for those who are not fluent in Arabic. For all indignant persons who are charged with felonies punishable by 3–15 years with no attorney, may have counsel provided for them upon the governments discretion. (4) Unlike the United States court system, the UAE prosecutors and defense lawyers have the ability to withhold any investigation from each other involving the case. (4) After, deliberations have been made of indictments, detainees may be released on bail informally. Authorities will accept a cash deposit, passport or an unsecured personal guarantee statement signed by a third party as a form of payment. Diya or blood money also qualifies as debt to a crime committed.\n\nIf one causes the death or injury of another person accidentally or intentionally he or she must pay the victim's family, what is known as diya or \"blood money\". It is a means of compensation to the loss or harm for the loved ones. The payment of diya protects the rights of the family in the event of further threats from any accomplices or known associates of the defendant. Diya is only payable if the defendant is found guilty under the criminal procedure or legally responsible for committing the crime. (5) However, if the defendant is found defending themselves, family, or property, diya will not be paid. Males rate of diya is Dh 200,000 (approx. US$54,450) and Dh 100,000 (approx. US$27,225) for females regardless of religion or nationality. (5)\n\nSection 2 A of the United Arab Emirates constitution provides freedom of speech and press. However, the law prohibits criticism and slander of public officials that may create or encourage social uproar. (4) Journalists undergo strict boundaries implemented from the government. A variety of information can be published and distributed without the content being harmful or insulting to others. All sources must be reliable and will not be published until a full investigation has been performed to phish out any fabricated information. (6) Writers have the full right to document and publish info gathered from the courtroom due to trials being open to the public (judges can ask for privacy). Writers are prohibited from publishing the names of the accused, victims, or witnesses. (6)\n\nWomen in the UAE have progressed through the years with rights that they haven’t had prior to the declaration. However, many are concerned with how women are treated in this country. In marriage, the women are bound by a marital contract that clearly states that she is to abide by her husband, take care of home and the children. If the women disobeys or neglect her duties as a wife, by law the husband has the right to use physical means including violence to correct her disdaining acts. (4) Nevertheless, some domestic abuse cases can be filed as an assault without the intent to kill that is punishable by 10 years in prison of death results, 7 years for permanent disability, or 1 year for temporary injury. (4) Rape in the UAE is not tolerated and is punishable by immediate death. However, many women do not report being raped because they could be accused of adultery which results in flogging or death by stoning and brings shame to the family. (7) Divorce is rare in the UAE due to the stipulations that derive from it. Article 110 of the Personal Status Code states, women can request to be divorced with the exception that she surrenders all of her finances; otherwise known as the \"khul\" divorce procedure. (7) If the divorce is processed, the custody of the children is determined. Women are considered to be physical guardians and only have the right to custody up to the age of 13 for girls and 10 for boys. Once that age has been reached the Sharia courts can reassess for further custody. If the woman decides to remarry she automatically forfeits her rights of custody for her children from the previous marriage. (7)\n\nLike the women if minors disobey or act out, the male has the right to utilize corporal punishment. Criminal responsibility begins at age seven. (8) Sharia principles permit corporal punishment of young children including flogging, amputation, and retaliation similar to the pain reflected on the victim. Article 8 of the Juvenile Delinquent and Vagrant Act states, that if a juvenile is over the age of 16 and commits an offence, under the Penal Code at the judge's discretion they may sentence them to the measures provided in crime, instead of the prescribed penalty. (9) Article 9 states, \"a juvenile may not be condemned to capital punishment\". Juveniles who reach 16 are to be punished under the penal code substituting a sentence of detention not to exceed 10 years. (10)\n\nIt is apparent that the UAE has applied firm penalties in accordance with maintaining the country's respect. Its vast success from new contemporary ideas has inspired other countries to engage in the innovations. With respect to UAE's religion, any showing of body parts such as buttocks, leg breast is deemed indecent and is strictly prohibited. Punishment for indecent exposure if jail or deportation. The body and romantic relationship is sacred and affection should be kept behind closed doors. Public affection is not allowed regardless of relationship status. Kissing and embracing of significant others is offensive and will be reported; holding hands is only affection acceptable.\n\nThe death penalty's purpose is to not only punish, but to rehabilitate criminals and if possible prevent others from committing the same crime. (13) The main goal of the United Arab Emirates is to protect the lives, religion, parentage, intellect, and property of its citizens through rules and regulations that are strictly enforced with no tolerance. There are five crimes that the death penalty is automatically administered for: adultery, rape, armed highway robbery, murder, and apostasy.\n\nIn the Middle East, men are lawfully allowed to marry up to four women provided that he equally divides his time and money to take care of the family needs. (13) If the wives believe they haven’t received their share of his time or expenses than they have the right to ask for a divorce, due to the lack of marital support. The concept of having multiple spouses is to discourage men from committing adultery, having any sexual intercourse outside of marriage. Basically to avoid any bastard children, this in turn can create confusion within the family. The child will not be considered to have a father because his mother was not legally married to him and places the child in a difficult position, psychologically. Adultery is forbidden in order to uphold is mission to protect the livelihood of its citizens.\n\nRape, is extremely unacceptable in this country. The age of sexual maturity is fourteen. (13) Both women and men can be considered the culprit. To be sentenced to the death penalty full intercourse must have occurred. (13) In contrast, it is difficult to prove based on the circumstances of how women are treated in the law. If a women confesses that she has been attack without consent she must immediately alert local authorities in order to assess the crime, if not she will be deemed as agreeing and being full fledge on in non-marital sex and sentenced to death. In addition, if a women is pregnant at the time of her sentencing she is allowed to give birth and suckle the child for two years before her death is complete. (13)\nArmed highway robbery is the impression of taking money from another under the threat of force of arms, thus imposing physical or financial assault. (13) Theft (Al Haraboh) occurs when an individual or group of people prey on others in attempt to take their valuables or property. Citizens have the natural right to obtain anything they have without the threat of someone without authority revoking it from them. The perpetrator who breaks that natural right is going against the ideologies of the country, which no pardon can be given.\n\nUnder Book 1, Article 1 of the Penal Code retributive penalties can apply under the Sharia law. The United Arab Emirates courts follow the Maliki School of Sunni Islam for the use of authority and interpretation from other schools to pronounce the sentences for murder cases. Islamic law defines any sane person who intentionally kills another with a weapon, is a sinner deserving perdition according to the Quran and the murderer is subject to retaliation. (11) Aggravated Murder, in the case of \"deliberate design\" or premeditated purpose is also punishable by death. (11) Unlawful killing such as: killing by fire, drowning in water, throwing from a high place, crushing by pushing down a wall, strangling, prevention of food and water, or exposing to a predatory animal are all likely to cause death, in the event death will be the punishment. (13) Homicides of public employees including ministries, government authorities, and members of the armed forces are considered an attack on the country itself and will be immediately prosecuted.\n\nApostasy or Al Reda is one who turns away from the Islamic religion, whether he or she embraces another religion or says they disbelieve after once believing. This crime is characterized and an intentional act unless under the individual is not at their sound capacity. (13) Any Muslim that denies one of the holy books, the prophets, or rejects any methods of the Islamic worship including prayer, paying Zakat (obligatory payment made annually under Islamic law on certain kinds of property and used for charitable and religious purpose), or fasting will be condemned to life for not abiding by the ordinances set in place for the country. (13) Moreover, if the person proclaims that they are in disbelief but is unaware of their statements they will be questioned once they regain their normal state of mind. Once they have gain clarity of themselves and deny any statements made in that stage than they will not be held liable for their action or comments. (13)\n\nOther crimes that can be accountable for readmission for the death penalty are perjury/calumny, inciting suicide, drinking (the use of drug-induced state of mind to incite a person to commit an offense; drinking and driving results in jail, fines, and deportation, (12) arson, kidnapping, drug trafficking (severe crime with a zero tolerance policy toward illegal drug use) (12), and human trafficking.\n\nAll of these crimes have been committed but are infrequent due to the punishment of death. During the process of execution a deputy of the ministry, doctor, convict's lawyer and one of the member of the public prosecution are present. Relatives of the criminal are able to visit that day prior to the implementation of the sentence.\n\nReferences:\n(1)\"The Story of the UAE.\" Zayed University, United Arab Emirates. N.p., n.d. Web. 23 Mar. 2014.\n\n(2)Berg, Herbert (2005). \"Islamic Law\" Berkshire Encyclopedia of World History 3. Pg. 1030.\n\n(3)\"The UAE Court System.\"dubai.usconsulate.gov. N.p., n.d. Web. 18 Mar. 2014.\n\n(4)United Arab Emirates.<http://www.state.gov.documents/oganization/160079.pdf>.N.p.,n.d. Web. 25 Mar. 2014.\n\n(5)Al Jandaly, Bassma. \"Blood Money In Islamic Law.\" Gulfnews.com. N.p., 20 Jan. 2009. Web 12 Apr. 2014.\n\n(6)\"Protection of News Sources in UAE.\" Free Trial and News Sources. N.p., n.d. Web. 12 Apr. 2014 http://uaemedialaw.wikispaces.com\n\n(7)\"Women’s Rights in the UAE.\" International Federation of Human Rights. N.p., Jan. 2010. http://fidh.org\n\n(8)Philippe, Marie. \"Realizing Children’s Rights in the United Arab Emirates.\" Humanium. N.p., 14 February 2013. Web. 25 March 2014 https://web.archive.org/web/20120608053806/http://humanium.org/\n\n(9)Juvenile Delinquent and Vagrant Act. Article 8. N.p., Web. 6 April 2014. <http://support.fatmaalmoosa.com>.\n\n(10)\"Inhuman Sentencing of Children in UAE.\" Child Rights International Network. N.p., Web 6 April 2014 www.crin.org/docs/UAE_Final.pdf\n\n(11)\"United Arab Emirates.\" Death Penalty Worldwide. N.p., 7 February 2011. Web. 25 March 2014. https://web.archive.org/web/20140329053557/http://www.deathpenaltyworldwide.org/\n\n(12)\"United Arab Emirates Crime and Safety Report: Abu Dhabi.\" OSAC. N.p., n.d. Web. 15 April 2013. https://web.archive.org/web/20101104231152/http://www.osac.gov/\n\n(13)Abdulla, Saleh, and Mirad Abdulla. \"The Use of the Death Penalty Under the Law of the United Arab Emirates.\" Diss. University of Aberystwyth, 2012. Print.\n"}
{"id": "18363322", "url": "https://en.wikipedia.org/wiki?curid=18363322", "title": "Urban metabolism", "text": "Urban metabolism\n\nUrban metabolism is a model to facilitate the description and analysis of the flows of the materials and energy within cities, such as undertaken in a material flow analysis of a city. It provides researchers with a metaphorical framework to study the interactions of natural and human systems in specific regions. From the beginning, researchers have tweaked and altered the parameters of the urban metabolism model. C. Kennedy and fellow researchers have produced a clear definition in the 2007 paper \"The Changing Metabolism of Cities\" claiming that urban metabolism is \"the sum total of the technical and socio-economic process that occur in cities, resulting in growth, production of energy and elimination of waste.\" With the growing concern of climate change and atmospheric degradation, the use of the urban metabolism model has become a key element in determining and maintaining levels of sustainability and health in cities around the world. Urban metabolism provides a unified or holistic viewpoint to encompass all of the activities of a city in a single model.\n\nWith deep roots in sociology, Karl Marx and fellow researcher Friedrich Engels may have been the first to raise concerns around issues which we would now call urban metabolism. Marx and Engels concentrated on the social organization of the harvesting of the Earth's materials by “ analysing the dynamic internal relationships between humans and nature.” Marx used the metaphor of metabolism to refer to the actual metabolic interactions that take place through humans’ exertion of physical labour to cultivate the Earth for sustenance and shelter (M. Fischer-Kowalski, 1998). In short, Marx and Engels found that when humans exerted such physical labour they ultimately altered the biophysical processes as well. This acknowledgement of altering the biophysical landscape is the first stepping stone for the creation of urban metabolism within social geography. They also used metabolism to describe the material and energy exchange between nature and society in as a critique of industrialization (1883)which created an interdependent set of societal needs brought into play through the concrete organization of human labour. Marx advocated that urban metabolism becomes a power in itself (like capitalism), and will control society unless society is able to control it.\n\nLater, in reaction against industrialization and coal use, Sir Patrick Geddes, a Scottish biologist, undertook an ecological critique of urbanization in 1885, making him the first scientist to attempt an empirical description of societal metabolism on a macroeconomic scale. Through his experimental study of urbanization he established a physical budget for urban energy and material throughput by way of an input output table.\n\nIt wasn't until 1965 when Abel Wolman fully developed and used the term urban metabolism in his work, “The Metabolism of Cities” which he developed in response to deteriorating air and water qualities in American cities. In this study Wolman developed a model which allowed him to determine the inflow and outflow rates of a hypothetical American City with a population of 1 million people. The model allows the monitoring and documentation of natural resources used (mainly water) and the consequential creation and out-put of waste. Wolman’s study highlighted the fact that there are physical limitations to the natural resources we use on a day-to-day basis and with frequent use, the compilation of waste can and will create problems. It also helped focus researchers and professionals of their time to focus their attention on the system wide impacts of consumption of goods and sequential production of waste within the urban environment.\n\nWorking off of Wolman’s pioneering work in the 60’s, environmentalist Herbert Girardet (1996) began to see and document his findings in the connection between urban metabolism and sustainable cities. Girardet laid the foundation for the industrial ecology approach to urban metabolism in which it is seen as the “conversion of nature into society.” Aside from being a great advocate and populariser for urban metabolism, Girardet significantly coined and drew the difference between a ‘circular’ and ‘linear’ metabolism. In a circular cycle, there is nearly no waste and almost everything is re-used. Girardet characterizes this as a natural world process. On the other hand, a ‘linear’ metabolism which is characterized as an urban world process has a clear resource in-put and waste out-put. Girardet emphasizes that the accelerated use of linear metabolisms in urban environments is creating an impending global crisis as cities grow.\n\nMore recently the metabolism frame of reference has been used in the reporting of environmental information in Australia where researchers such as Newman have begun to link urban metabolic measures to and it has been suggested that it can be used to define the sustainability of a city within the ecosystems capacity that can support it. This research has stayed mainly at a descriptive level and did not reach into the political or social forces of urban form and stages of flow. From this research there has been a strong theme in present literature on urban sustainability is that of the need to view the urban system as a whole if we are to best understand and solve the complex problems.\n\nDeveloped in 1970’s Howard T. Odum, a systems ecologist, wanted to emphasize the dependence on the source of almost all energy on the planet: the sun. Odum believed that previous research and development on urban metabolism was missing and did not account for qualitative differences of mass or energy flows. Odum’s study took this into count and he coined the term \"energy\" to track and account for the metabolic flows by measuring the solar energy used directly or indirectly to make a product or deliver a service. This method also emphasizes the use of a standard unit of measurement to calculate energy, nutrient and waste movement in the biophysical system; the unit chosen was \"solar energy joules\". At first glance, the notion to use standard units seems like a beneficial idea for calculating and comparing figures; in reality the ability to convert all urban processes into solar energy joules has proven to be a difficult feat, and difficult to understand.\n\nCurrently, the Urban Metabolism (UM) approach, as deducted from international literature, has been applied several times to assess and describe urban flows and impacts related to them, using different tools such as Material Flow Analysis (MFA) (Ioppolo et al., 2014). \nThe MFA, researched by Baccinni and Brunner in the 1990s, \"measures the materials flowing into a system, the stocks and flows within it, and the resulting outputs from the system to other systems in the form of pollution, waste, or exports.\" Much like Wolmans case model for a hypothetical American City, this method is based on the concept that the mass of the resources used will equal the mass \"plus\" stock changes out. The MFA technique has become the mainstream school of urban metabolism because it uses more practical units that the public, workers, government officials and researchers can understand.\n\nThere are four main uses of urban metabolism that are used today by urban planners and designers; sustainability reporting, urban greenhouse gas accounting, mathematical modelling for policy analysis and urban design.\n\nWith the issue of sustainability at the core of many environmental issues today, one of the main uses of Urban Metabolism in the modern era is to track and record levels of sustainability in cities and regions around the world. Urban metabolism collects important and very useful information about energy efficiency, material cycling, waste management and infrastructure in urban environments. The urban metabolism model records and analyzes environmental conditions and trends which are easily understood for policy makers and consequently comparable over time making it easier to find unhealthy patterns and develop a plan of action to better the level of sustainability.\n\nStaying in line with the notion of sustainability, urban metabolism is also a helpful tool for tracking greenhouse gas emissions on a city or regional level. As mentioned above, with the proliferation of linear metabolisms such as cars, the production of greenhouse gases has increased exponentially since the birth and mass production of the automobile causing a problem for our atmosphere. Urban metabolism has been proven to be a necessary tool for measuring levels of greenhouse gas because it is an out-put or waste product that is produced through human consumption. The model provides quantifiable parameters which allow officials to mark unhealthy levels of GHG emissions and again, develop a plan of action to lower them.\n\nAside from the two accounting applications above, urban metabolism has begun to develop mathematical models to quantify and predict levels of particles and nutrients within the urban metabolism model. Such models have mostly been created and used by MFA scholars and are helpful in determining present and future sub-processes and material stocks and flows within the urban environment With the ability to predict future levels, these mathematical models allow progress to be made and possible pollution prevention programs to be instated rather than end-of-the-pipe solutions which have been favoured in the past.\n\nThrough utilization of the 3 applications above, scholars and professionals are able to use urban metabolism as a design tool to create greener and more sustainable infrastructure from the beginning. By tracing flows of energy, materials and waste through urban systems as a whole, changes and alterations can be made to close the loops to create circular metabolisms where resources are recycled and almost no waste is produced. Such initiatives are being made around the world with technology and inventions which make building green that much easier and accessible.\n\n\n\n"}
{"id": "888187", "url": "https://en.wikipedia.org/wiki?curid=888187", "title": "War chest", "text": "War chest\n\nA war chest is a metaphor for any collection of tools or money intended to be used in a challenging or dangerous situation. Historically, it referred to the chest located in the homes or barracks of soldiers, in which the soldier kept arms and armor. In the modern era, it more often refers to a collection of funds (or less occasionally special tools or equipment) intended to allow a person or organization to get through a situation that requires much more readiness or money than usual.\n\nIn arms and armor, a war chest is a container for the personal weapons and protective gear of a citizen-soldier, kept in the household, and is the origin of the term. The term's modern meaning originates with the medieval practice of having a chest, literally, filled with money to open in time of war.\n\nIn politics, a war chest is funding obtained from donors well in advance of a campaign, usually accumulated by an incumbent for either re-election or to contest a more advanced office, or provided by a wealthy candidate to their own campaign. The possession of such excess funds may discourage otherwise viable candidates from a primary or general election challenge.\n\nIn business a war chest, or cash mountain is a stash of money set aside to deal with unexpected changes in the business environment, or to use when expansion possibilities arise.\n\nToday companies can use accumulated cash or rely on quickly raised debt which costs less to carry when you don't need it. This is not always a reasonable substitute, as the credit available to a company typically drops as a result of the same actions that require the war chest to be opened.\n\nCompanies can redistribute their war chests to shareholders by issuing larger or special dividends, or more commonly through share buyback operations. Companies do this because if actually held in cash, the companies will be earning a low rate of return in the money markets, whereas they could be using the funds to invest in more profitable projects. If they continue not to invest the funds, shareholders may sell the company's shares and make it vulnerable to a takeover. This would place the current management's jobs at risk.\n\nIn Association football it refers to the amount of money a manager has been given by a club's chairman, owner or investors to acquire new players, as in the newspaper headline, \"Defoe and Brown top Keegan wishlist as Ashley grants £25m war chest\".\n\nSimilar terms are also referred to as surplus cash, cash reserves, emergency reserves, acquisition funds, rainy day funds, or undistributed earnings within different contexts.\n"}
