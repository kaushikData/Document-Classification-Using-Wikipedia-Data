{"id": "14428994", "url": "https://en.wikipedia.org/wiki?curid=14428994", "title": "2007 HINDRAF rally", "text": "2007 HINDRAF rally\n\nHindu religious NGOs, including MHS, decided to form a special committee after the burial of Murthi according to Islamic rights, to study and make recommendations on the issues of conversions to Islam, namely grabbing of dead bodies and conversions of children. Mr.P. Waythamoorthy was made the Chairman of this special committee.However, after more cases of temple demolitions and conversions, this committee evolved and saw the formation of Hindraf in June 2007 with the involvement of religious NGO's, politicians and other Indian-based NGO's. Hindraf prepared a memorandum on all issues of the Indian community in Malaysia, starting from the British colonial days,right up to present situation of the Indian community being considered as second rate citizens of the country.They conducted various protests and rallies.Hindraf's proposals to the PM of Malaysia to resolve various issues affecting the plantation workers.MHS made several attempts, together with other Hindu and Indian NGO's through YSS to bring both MIC President and Hindraf leaders together to have a discussion. However, these efforts failed and eventually culminated in the massive Hindraf.\n\nThe 2007 HINDRAF rally was a rally held in Kuala Lumpur, Malaysia, on 25 November 2007. The rally organiser, the Hindu Rights Action Force, had called the protest over alleged discriminatory policies which favour ethnic Malays. The rally was the second such street protest after the 2007 Bersih rally in Kuala Lumpur on 10 November 2007. The rally started when a crowd estimated to be between 5,000 and 30,000 people gathered outside the Petronas Twin Towers at midnight, early Sunday morning.\n\nAt least 240 people were detained, but half of them were later released.\n\nReligious persecution has been formidable source of marginalization of the people of Indian origin in Malaysia.Between April to May 2006, several Hindu temples were demolished by city hall authorities in the country, accompanied by violence against Hindus. On 21 April 2006, the Malaimel Sri Selva Kaliamman Temple in Kuala Lumpur was destroyed by the City Hall authorities because of violation of construction laws.\n\nThe president of the Consumers Association of Subang and Shah Alam in Selangor State has been helping to organize efforts to stop the local authorities in the Muslim dominated city of Shah Alam from demolishing a 107-year-old Hindu temple. The growing Islamization in Malaysia is a cause for concern to many Malaysians who follow minority religions such as Hinduism.On 11 May 2006, armed city hall officers from Kuala Lumpur forcefully demolished part of a 60-year-old suburban temple that serves more than 1,000 Hindus because iwas builtld illegally.Moreover, the demolition of the Seri Maha Mariamman Temple in Padang Jawa, Shah Alam just a few days before Deepavali which is most important Hindu festival of lights in 2007.\n\nNot only that, an interesting case was the attempted demolition of the Sri Kaliamman Temple just outside Angkasapuri, which is the headquarters for Radio Televisyen Malaysia (RTM).The temple was asked to evacuate for security reasons stating that it was a security risk. The President of Temple confirmed that it was authorized by a former Minister of Information that the temple which was built by the staff of RTM remain at its present location which is part of the land of Angkasapuri. As the leader of Hindu delegation in the Committee, he enquired with the National Security Council representatives at the meeting whether the surau or the proposed site for the RM5million new mosque in the Angkasapuri compound was open for the staff only or the public as well.He stated that if the public is allowed to visit this Islamic place of worship, then how does the question of security arise when the temple is outside the compound. To this, the representative answered that it was open to the public as well.On pressing further, there were no answers and the matter was not brought up by the authorities after.\n\nThe Hindu Rights Action Force or HINDRAF, a coalition of several NGO's, have protested these demolitions by lodging complaints with the Prime Minister of Malaysia but with no response. Many Hindu advocacy groups have protested what they allege is a systematic plan of temple cleansing in Malaysia. The official reason given by the Malaysian government has been that the temples were built \"illegally\". However, several of the temples are centuries old.\nAccording to a lawyer for HINDRAF, a Hindu temple is demolished in Malaysia once every three weeks.\n\nAffirmative Action Policy is to ensure the peace and stability in the pluralist society of Malaysia.It was found to address Malay grievances.The government introduced a\n\nnumber of policies to improve the condition of the Bumiputeras in all sectors of life.In 1971 the New Economic Policy was launched, effectively imposing a regime of\n\npositive discrimination for Malays in a variety of sectors from education and business to the bureaucracy. Later revisions of this, such as the New Development Plan\n\n(1991-2000) as well as the New Vision Policy (2000-2010),all targeted the promotion of the local Malay population.Although, the NEP was meant to eliminate poverty in\n\nrural sectors,it had bypassed the private plantation which included the rubber plantation that employed a large number of Indian labourers.Statistics clearly showed that\n\nmore than 75% of Indians were in 1970 classified as low wages labourers. The socio-economic situation of the Indian Tamil plantation labour in particular was a major\n\nconcern of the Malaysian Indian Congress(MIC).After Independence,the MIC President Tun V.T.Sambanthan had established the National Land Finance plantations that\n\nwere facing fragmentation.Without government aid it was considered a gergantuan task,and plea was made to low wages,loss of jobs and homes due to the fragmentation\n\nexercise.\n\nOn 1974 MIC Bluebook,spearheaded by the MIC President V.Manickavasagam,contained key proposals which included increase of Indian share capital,ownership and\n\ncontrol achievement of racial balance in public employment,offer of Indian who were restricted under the Employment Act 1968 and abolish the Contract Labour\n\nSystem,allotment of new housing and Introduction of a systematic academic scheme for Tamil schools are recommended under the Aziz Commission Report. The MIC\n\nBluebook proposals indicate the socioeconomic grievances faced by the Indian Tamil plantation labour. The early efforts made by the MIC,none of the Bluebook\n\nrecommendation were taken into consideration.Following the fragmentation and redevelopment of rubber plantation land,the mid-1980 onward witnessed the gradual\n\nurbanization of the Indian Tamil Plantation labour and the continued disregard of this particular class-based group's grievances by government policy and poverty\n\nreduction programmes.The situation was found to worsen when rubber plantation were bought over through Permodalan Nasional Berhad(PNB),a Malay trust agency.As\n\nsuch,the complicity between the private corporations and government established an unequal bargaining position between the plantation corporations and the Indian\n\nTamil plantation labour.\n\nWithout adequate labour representation and political clout, the Indian Tamil plantation labour face serious socio-economic problems such as loss jobs,eviction from estate\n\nhomes and forced relocation to urban squatters.By the mid-1990s,it was reported that 70.5% of Indians were employed low-waged labour in agricultural, manufacturing\n\nand industrial jobs.The forced urbanization had led to the constitution of an urban Indian underclass that as facing social and economic grievances such as urban\n\npoverty,lack of education,unemployment and social problems such as the escalation of crime among youths. The development programmes initiated under the National\n\nDevelopment Policy(NDP) and the socioeconomic programmes under the NEP had failed to consider the socioeconomic situation of the Indian Tamil plantation labour as\n\nit was outside purview of the race-basd affirmative action policy.\n\nPolitical Representation.MIC is the biggest India political party and a constituent of the rulling coalition government at the centre since independence does not have\n\nmuch political clout and has not been able to do anything substantial to improve the lot of the Indians.Indian non-governmental organizations(NGOs),community and self-help groups among the urban Indian middle class have attempted to solve the socio-economic problems faced by the former Indian Tamil plantation labour.However,\n\ncontemporary Indian civil society is generally divided due to differences in ideology.The class-based approach taken by community groups such as Alaigal and JERIT\n\nhave successfully mobilized the Indian Tamil Plantation labour to seek their legal rights against unlawful eviction from homes and to obtain adequate and fair\n\ncompensation for retrenchment from plantation jobs. The urban Indian middle class had lost confidence in the political elites of the MIC who was seen as having a weak\n\nbargaining clout within the communal Barisan Nasional (BN) coalition.The race-based politics of the BN was seen to have failed in resolving the serious issues faced by\n\nclass based group among the Indians.\n\nMoreover,the unresolved Kampung Medan incident in 2001, which was reported to be a series of clashes between the Indian and Malay residents in the poorer urban\n\nsettlement of Kampung Medan in the state of Selangor, had deeply affected both the urban Indian underclass as well as the urban Indian middle-class. The failure of the\n\nMalaysian Human Rights Commission to investigate the causes of the incident had further exacerbated the loss of confidence in the government.Indian civil society had\n\nviewed the incident as a mob attack against the Malaysian Human Rights Commission for falling to hold an inquiry on the matter.The High Court in the case took a restrictive approach in inter receptivity to public policy reference to the victims of the Kampung Medan incident as being of Indian origin and of Tamil ethnicity,the court replied that: \"Whatever one uses to describe those victims,it makes not a whit of a difference. The judiciary disregarded the class based as well as the race based identity of the victims and only took into consideration the technically of legal procedural arguments.The judgement also meant that the victims of Kampong Medan did not have any further recourse to justice.\n\nThe issue of forced religious conversion has also been at the forefront. The trigger incidents were the 2007 series of legal cases involving the custody rights of non-\n\nMuslim Indian mothers against the unilateral rights of their newly converted Muslims husbands to convert their children to Islam. The two legal battles that became a\n\nserious concern among the Indians were Shamala's case in 2003, and Subashini's case in 2007.Inboth,h the cases, the originally Hindu husband converted to Islam and the\n\nattempted to unilaterally convert their children to Islam without the non-Muslims wife's legal permission. There are many such cases of conversion to Islam either\n\nvoluntary or forced upon the ethnic Indian community, which has caused fear and apprehensions among the group.\n\nOn 31 August 2007, the 50th anniversary of Malaysia's independence, P. Waytha Moorthy, a HINDRAF lawyer filed a class action suit against the Government of the United Kingdom at The Royal Courts of Justice in London for US$4 trillion (US$1 million for every Malaysian Indian) for \"withdrawing after granting independence and leaving us (Indians) unprotected and at the mercy of a majority Malay-Muslim government that has violated our rights as minority Indians\". as guaranteed in the Federal Constitution when independence was granted.\n\nThe lawsuit is not only claiming 4 trillion British Pounds as compensation, it is also seeking to strike out Article 153 of the Malaysian Constitution which acknowledges the special position of Malays and the legitimate rights of other races, but is often seen as endorsement of Malay Supremacy and for the court to declare that Malaysia is a secular state and not an Islamic state as declared by former Prime Minister Tun Dr. Mahathir Mohamad who is partly Indian himself.\n\nAs the group, which represents mainly working class Indian Malaysians, could not afford the legal fees required, a petition was circulated with 100,000 signatures to be presented to Queen Elizabeth II to appoint a Queen's counsel to argue the case. The purpose of the rally was to hand over a 100,000 signature memorandum to the British High Commission in Kuala Lumpur.\n\nHINDRAF organised the rally on Sunday, 25 November 2007 to submit the petition at the British High Commission.\n\nMalaysian police refused to grant a permit for the rally, and set up roadblocks in Klang Valley along roads leading up to the rally to screen motorists entering the city center and identify \"troublemakers\". They also advised the public not to participate in the rally, and arrested three leaders of HINDRAF. Many shops around Kuala Lumpur including Suria KLCC were closed on that day in fear of trouble from the rally.\n\nOne day before the rally, police arrested three HINDRAF lawyers, P. Uthayakumar, P. Waytha Moorthy and V. Ganabatirau for sedition charges. Uthayakumar and Ganabatirau posted bail of 800 Malaysian ringgits each, but Waytha Moorthy refused bail as a sign of protest.\n\nThe police roadblocks started the week before the rally to create massive traffic jams across the city and the outskirts of Kuala Lumpur. The Malaysian Opposition leader Lim Kit Siang of the DAP pointed out that this high-handed act by the police was unnecessary as it caused major inconvenience to everyone.\n\nOn the morning of the rally, about twenty thousand people gathered near the Petronas Twin Towers, a symbol of modern Malaysia in Kuala Lumpur, carrying life-size portraits of Elizabeth II and Mahatma Gandhi, to indicate the nonviolent nature of their protest. Five thousand members riot police dispatched to the scene used tear gas and water cannon to disperse the crowds. 136 people were arrested.\n\nAl-Jazeera's coverage of the event showed police officers using tear gas to disperse the protesters. A few hundred protesters and three police officers were injured.\n\nThe protest at the Batu Caves Hindu temple resulted in minor property damages, although the Hindu temple itself was not damaged.\n\nHINDRAF later claimed to have faxed the petition to the British High Commission staff. However, as of 28 November 2007, the British Envoy had not yet received any petition from the HINDRAF, though they did say they had received some unspecified information by fax.\n\n"}
{"id": "53033394", "url": "https://en.wikipedia.org/wiki?curid=53033394", "title": "Access to public information in Kosovo", "text": "Access to public information in Kosovo\n\nAccess to public information and freedom of information (FOI) refer to the right of access to information held by public bodies also known as \"right to know\". Access to public information is considered of fundamental importance for the effective functioning of democratic systems, as it enhances governments' and public officials' accountability, boosting people participation and allowing their informed participation into public life. The fundamental premise of the right of access to public information is that the information held by governmental institutions is in principle public and may be concealed only on the basis of legitimate reasons which should be detailed in the law.\n\nIn the Republic of Kosovo, the Law on Access to Official Documents was enacted in 2010. The law repealed and superseded various other legislation and administrative orders that were previously in force. The right of access to public information is guaranteed by the Constitution. Several secondary acts aims at realizing the right and implement the 2010 Law. However, full implementation is lacking, the courts are slow to respond to appeals due to persistent backlogs in the judicial system. Even when the appeals process work, the institutions appear willing to ignore the rulings of the court when it comes to transparency and access to public information. Currently, in 2016, the law on access to public documents is undergoing a procedure of amendment together with other laws to which it should be harmonized.\n\nIn Kosovo access to public information is a right guaranteed by Article 41 of the Constitution which establishes that every person is entitled of the right of access to public information and that documents held by public institutions and organs of state authorities are public, except for information that can be concealed by law for reasons due to privacy, business trade secrets or public security.\n\nAlso, the right of access to official documents is guaranteed by many international agreements and declarations, such as the Universal Declaration of Human Rights and the European Convention for the Protection of Human Rights and Fundamental Freedoms and Protocols which are directly applicable in the Republic of Kosovo and have priority in case of conflict of applicable laws.\n\nThere are also special laws regulating the application of the right of access to public information, such as the Law on Administrative procedure and the Law on Civil Service of the Republic of Kosovo.\n\nSpecifically, to implement the constitutional right of every person to have access, upon request, to documents of public interest held by public authorities, in 2010 the Assembly of the Republic of Kosovo approved the Law no. 03/L-215 on Access to Public Documents (LAPD). The LAPD Law applies to all documents maintained, drafted and received by public institutions and is based on ten principles:\n\nRequesting and obtaining public information is free of charge. A fee may only be charged to applicants for copies of the document requested and have to be reasonable and aiming at only covering the actual costs of reproduction and document delivery. Exceptions to the right of access to public information can be applied and are defined by law.\n\nAccording to the relevant law, access to public information may be restricted only for the purpose of protecting the public legitimate interests of life or other legitimate private interests, such as the right to privacy and for public security reasons. Other reasons for exceptions include: defence and international relations; detection and investigation of criminal activities, disciplinary investigations, atc.\n\nThe request can be made in writing or orally.\n\nPublic institutions have to respond to citizens' requests within seven days with their decision to fully or partially grant the access. The deadline for review may be exceptionally extended to 15 days.\n\nThe respect of the right of access to public information is guaranteed by the Ombudsperson Institution, an independent body supporting citizens to realize the right of access to documents that have been denied, according to the Law on Ombudsperson.\n\nIn case of rejection of the applicant's request, and non-response by the relevant authority within the deadline, the applicant can start an appeal procedure in front of the Ombudsperson Institutions or the competent courts, in accordance with applicable law.\n\nThe implementation of the laws on access to public information is problematic. According to the 2016 European Commission Progress Report, in practice the right of access to public information is “undermined by the authorities’ fragmented and unclear approach”.\n\nMoreover, according to the Balkan Investigative Reporting Network (BIRN) the realization of the right is hindered by a secretive mentality and culture of many public authorities in the country. Also, According to the news portal Kosovo 2.0, institutions tend to disclose information on less important issues and when lower level institutions, such as municipalities, are involved. Other flaws in the application of the law consists in its misinterpretation, for instance in distinguishing between what to consider private and what a public interest, and the application of \"ad hoc\" administrative measures by certain institutions. According to Kosovo 2.0, even when the law is very clear in specifying procedures, certain institutions have created their own procedures which are not always in line with the law, resulting in barriers for interested applicants. For instance, there are numerous institutions requiring the payment of 1 euro when filling a freedom of information request, which is in contradiction with the law. Another problem concerns the officials who are responsible for receiving and processing freedom of information requests. According to Kosovo 2.0 in many cases they are not properly trained to effectively execute the law.\n\nIn Kosovo, the majority of requests for accessing public information are submitted by journalists, activists and civil society organizations, and very few by citizens that are not fully aware of their right of access to public information. According to a survey carried out in 2012, the level of knowledge about the content of the law on access to public information among citizens was extremely deficient, as well as their interest in making requests for access to public documents.\n\nTo test and monitor the implementation of the law, BIRN has submitted a series of requests for information to the majority of public institutions, at all levels, in Kosovo. From June 2013 to May 2014, it submitted a total of 125 official requests: of the requests submitted, 68 were denied, 50 approved and 7 partially denied, meaning that requested documents were received in only 40 per cent of cases. This result is considered as “rather poor” by the organisation. During the same monitoring period, one of the major problems observed by BIRN concerned access to courts information, in particular indictments. Another common problem highlighted by the organization is that, in practice and despite the law’s provisions, the institution that does not have the information requested usually rejects the requests, rather than forwarding it to another institution that may have it as required by the law. According to another test, implemented between October 2013 to April 2016, only 25 per cent of BIRN’s requests were approved and 38 were rejected.\n\n"}
{"id": "6885821", "url": "https://en.wikipedia.org/wiki?curid=6885821", "title": "Beneficium inventarii", "text": "Beneficium inventarii\n\nBeneficium inventarii (literally benefit of the inventory) is a legal doctrine introduced into Roman law by Justinian I to limit the liability of heirs resulting from an insolvent estate.\n\nThe doctrine, which is in force today in many civil law systems, applies to both wills and intestate successions. An heir may accept a succession under \"beneficium inventarii\" without being liable for the debts attaching to the estate or to the claims of legatees beyond the estate's value as previously determined by inventory.\n"}
{"id": "515096", "url": "https://en.wikipedia.org/wiki?curid=515096", "title": "Canonical form", "text": "Canonical form\n\nIn mathematics and computer science, a canonical, normal, or standard form of a mathematical object is a standard way of presenting that object as a mathematical expression. The distinction between \"canonical\" and \"normal\" forms varies by subfield. In most fields, a canonical form specifies a \"unique\" representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.\n\nThe canonical form of a positive integer in decimal representation is a finite sequence of digits that does not begin with zero.\n\nMore generally, for a class of objects on which an equivalence relation is defined, a canonical form consists in the choice of a specific object in each class. For example, Jordan normal form is a canonical form for matrix similarity, and the row echelon form is a canonical form, when one considers as equivalent a matrix and its left product by an invertible matrix.\n\nIn computer science, and more specifically in computer algebra, when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a canonical form is a representation such that every object has a unique representation. Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. However canonical forms frequently depend on arbitrary choices (like ordering the variables), and this introduces difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, \"normal form\" is a weaker notion: A normal form is a representation such that zero is uniquely represented. This allows testing for equality by putting the difference of two objects in normal form.\n\nCanonical form can also mean a differential form that is defined in a natural (canonical) way.\n\nIn computer science, data that has more than one possible representation can often be canonicalized into a completely unique representation called its canonical form. Putting something into canonical form is canonicalization.\n\nSuppose we have some set \"S\" of objects, with an equivalence relation \"R\". A canonical form is given by designating some objects of \"S\" to be \"in canonical form\", such that every object under consideration is equivalent to exactly one object in canonical form. In other words, the canonical forms in \"S\" represent the equivalence classes, once and only once. To test whether two objects are equivalent, it then suffices to test their canonical forms for equality.\nA canonical form thus provides a classification theorem and more, in that it not just classifies every class, but gives a distinguished (canonical) representative.\n\nFormally, a canonicalization with respect to an equivalence relation \"R\" on a set \"S\" is a mapping \"c\":\"S\"→\"S\" such that for all \"s\", \"s\", \"s\" ∈ \"S\":\nProperty 3 is redundant, it follows by applying 2 to 1.\n\nIn practical terms, one wants to be able to recognize the canonical forms. There is also a practical, algorithmic question to consider: how to pass from a given object \"s\" in \"S\" to its canonical form \"s\"*? Canonical forms are generally used to make operating with equivalence classes more effective. For example, in modular arithmetic, the canonical form for a residue class is usually taken as the least non-negative integer in it. Operations on classes are carried out by combining these representatives and then reducing the result to its least non-negative residue.\nThe uniqueness requirement is sometimes relaxed, allowing the forms to be unique up to some finer equivalence relation, like allowing reordering of terms (if there is no natural ordering on terms).\n\nA canonical form may simply be a convention, or a deep theorem.\n\nFor example, polynomials are conventionally written with the terms in descending powers: it is more usual to write \"x\" + \"x\" + 30 than \"x\" + 30 + \"x\", although the two forms define the same polynomial. By contrast, the existence of Jordan canonical form for a matrix is a deep theorem.\n\nNote: in this section, \"up to\" some equivalence relation E means that the canonical form is not unique in general, but that if one object has two different canonical forms, they are E-equivalent.\n\n\n\nIn analytic geometry:\n\nBy contrast, there are alternative forms for writing equations. For example, the equation of a line may be written as a linear equation in point-slope and slope-intercept form.\n\nConvex polyhedra can be put into canonical form such that:\n\nStandard form is used by many mathematicians and scientists to write extremely large numbers in a more concise and understandable way.\n\n\n\n\n\n\n\nIn graph theory, a branch of mathematics, graph canonization is the problem finding a canonical form of a given graph \"G\". A canonical form is a labeled graph Canon(\"G\") that is isomorphic to \"G\", such that every graph that is isomorphic to \"G\" has the same canonical form as \"G\". Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs \"G\" and \"H\" are isomorphic, compute their canonical forms Canon(\"G\") and Canon(\"H\"), and test whether these two canonical forms are identical.\n\nCanonical differential forms include the canonical one-form and canonical symplectic form, important in the study of Hamiltonian mechanics and symplectic manifolds.\n\nIn computing, the reduction of data to any kind of canonical form is commonly called \"data normalization\".\n\nFor instance, database normalization is the process of organizing the fields and tables of a relational database to minimize redundancy and dependency. \n\nIn the field of software security, a common vulnerability is unchecked malicious input. The mitigation for this problem is proper input validation. Before input validation may be performed, the input must be normalized, i.e., eliminating encoding (for instance HTML encoding) and reducing the input data to a single common character set.\n\nOther forms of data, typically associated with signal processing (including audio and imaging) or machine learning, can be normalized in order to provide a limited range of values.\n\n\n"}
{"id": "9435784", "url": "https://en.wikipedia.org/wiki?curid=9435784", "title": "Comparative physiology", "text": "Comparative physiology\n\nComparative physiology is a subdiscipline of physiology that studies and exploits the diversity of functional characteristics of various kinds of organisms. It is closely related to evolutionary physiology and environmental physiology. Many universities offer undergraduate courses that cover comparative aspects of animal physiology. According to Clifford Ladd Prosser, \"Comparative Physiology\nis not so much a defined discipline as a viewpoint, a philosophy.\"\n\nOriginally, physiology focused primarily on human beings, in large part from a desire to improve medical practices. When physiologists first began comparing different species it was sometimes out of simple curiosity to understand how organisms work but also stemmed from a desire to discover basic physiological principles. This use of specific organisms convenient to study specific questions is known as the Krogh Principle.\n\nC. Ladd Prosser, a founder of modern comparative physiology, outlined a broad agenda for comparative physiology in his 1950 edited volume (see summary and discussion in Garland and Carter):\n\n1. To describe how different kinds of animals meet their needs.\n\n2. The use of physiological information to reconstruct phylogenetic relationships of organisms.\n\n3. To elucidate how physiology mediates interactions between organisms and their environments.\n\n4. To identify \"model systems\" for studying particular physiological functions.\n\n5. To use the \"kind of animal\" as an experimental variable.\n\nComparative physiologists often study organisms that live in \"extreme\" environments (e.g., deserts) because they expect to find especially clear examples of evolutionary adaptation. One example is the study of water balance in desert-inhabiting mammals, which have been found to exhibit kidney specializations.\n\nSimilarly, comparative physiologists have been attracted to \"unusual\" organisms, such as very large or small ones. As an example, of the latter, hummingbirds have been studied. As another example, giraffe have been studied because of their long necks and the expectation that this would lead to specializations related to the regulation of blood pressure. More generally, ectothermic vertebrates have been studied to determine how blood acid-base balance and pH change as body temperature changes.\n\nIn the United States, research in comparative physiology is funded by both the National Institutes of Health and the National Science Foundation.\n\nA number of scientific societies feature sections on comparative physiology, including:\n\nKnut Schmidt-Nielsen (1915–2007) was a major figure in vertebrate comparative physiology, serving on the faculty at Duke University for many years and training a large number of students (obituary). He also authored several books, including an influential text, all known for their accessible writing style.\n\nGrover C. Stephens (1925–2003) was a well-known invertebrate comparative physiologist, serving on the faculty of the University of Minnesota until becoming the founding chairman of the Department of Organismic Biology at the University of California at Irvine in 1964. He was the mentor for numerous graduate students, many of whom have gone on to further build the field (obituary). He authored several books and in addition to being an accomplished biologist was also an accomplished pianist and philosopher.\n\n\n\n"}
{"id": "49331727", "url": "https://en.wikipedia.org/wiki?curid=49331727", "title": "Coordinate systems for the hyperbolic plane", "text": "Coordinate systems for the hyperbolic plane\n\nIn the hyperbolic plane, as in the Euclidean plane, each point can be uniquely identified by two real numbers. Several qualitatively different ways of coordinatizing the plane in hyperbolic geometry are used.\n\nThis article tries to give an overview of several coordinate systems in use for the two-dimensional hyperbolic plane.\n\nIn the descriptions below the constant Gaussian curvature of the plane is −1. Sinh, cosh and tanh are hyperbolic functions.\n\nThe polar coordinate system is a two-dimensional coordinate system in which each point on a plane is determined by a distance from a reference point and an angle from a reference direction.\n\nThe reference point (analogous to the origin of a Cartesian system) is called the \"pole\", and the ray from the pole in the reference direction is the \"polar axis\". The distance from the pole is called the \"radial coordinate\" or \"radius\", and the angle is called the \"angular coordinate\", or \"polar angle\".\n\nFrom the hyperbolic law of cosines, we get that the distance between two points given in polar coordinates is\n\nThe corresponding metric tensor is: formula_2\n\nThe straight lines are described by equations of the form\n\nwhere \"r\" and θ are the coordinates of the nearest point on the line to the pole.\n\nThe Poincaré half-plane model is closely related to a model of the hyperbolic plane in the quadrant \"Q\" = {(\"x,y\"): \"x\" > 0, \"y\" > 0}. For such a point the geometric mean formula_4 and the hyperbolic angle formula_5 produce a point (\"u,v\") in the upper half-plane. The hyperbolic metric in the quadrant depends on the Poincaré half-plane metric. The motions of the Poincaré model carry over to the quadrant; in particular the left or right shifts of the real axis correspond to hyperbolic rotations of the quadrant. Due to the study of ratios in physics and economics where the quadrant is the universe of discourse, its points are said to be located by hyperbolic coordinates.\n\nIn hyperbolic geometry rectangles do not exist. The sum of the angles of a quadrilateral in hyperbolic geometry is always less than 4 right angles (see Lambert quadrilateral). Also in hyperbolic geometry there are no equidistant lines (see hypercycles). This all has influences on the coordinate systems.\n\nThere are however different coordinate systems for hyperbolic plane geometry. All are based on choosing a real (non ideal) point (the Origin) on a chosen directed line (the \"x\"-axis) and after that many choices exist.\n\nAxial coordinates \"x\" and \"y\" are found by constructing a \"y\"-axis perpendicular to the \"x\"-axis through the origin.\n\nLike in the Cartesian coordinate system, the coordinates are found by dropping perpendiculars from the point onto the \"x\" and \"y\"-axes. \"x\" is the distance from the foot of the perpendicular on the \"x\"-axis to the origin (regarded as positive on one side and negative on the other); \"y\" is the distance from the foot of the perpendicular on the \"y\"-axis to the origin.\n\nEvery point and most ideal points have axial coordinates, but not every pair of real numbers corresponds to a point.\n\nIf formula_6 then formula_7 is an ideal point.\n\nIf formula_8 then formula_7 is not a point at all.\n\nThe distance of a point formula_10 to the \"x\"-axis is formula_11. To the \"y\"-axis it is formula_12.\n\nThe relationship of axial coordinates to polar coordinates (assuming the origin is the pole and that the positive \"x\"-axis is the polar axis) is\n\nThe Lobachevsky coordinates \"x\" and \"y\" are found by dropping a perpendicular onto the \"x\"-axis. \"x\" is the distance from the foot of the perpendicular to the \"x\"-axis to the origin (positive on one side and negative on the other, the same as in axial coordinates).\n\n\"y\" is the distance along the perpendicular of the given point to its foot (positive on one side and negative on the other).\n\nThe Lobachevsky coordinates are useful for integration for length of curves and area between lines and curves.\n\nLobachevsky coordinates are named after Nikolai Lobachevsky one of the discoverers of hyperbolic geometry.\n\nConstruct a Cartesian-like coordinate system as follows. Choose a line (the \"x\"-axis) in the hyperbolic plane (with a standardized curvature of -1) and label the points on it by their distance from an origin (\"x\"=0) point on the \"x\"-axis (positive on one side and negative on the other). For any point in the plane, one can define coordinates \"x\" and \"y\" by dropping a perpendicular onto the \"x\"-axis. \"x\" will be the label of the foot of the perpendicular. \"y\" will be the distance along the perpendicular of the given point from its foot (positive on one side and negative on the other). Then the distance between two such points will be\n\nThis formula can be derived from the formulas about hyperbolic triangles.\n\nThe corresponding metric tensor is: formula_19.\n\nIn this coordinate system, straight lines are either perpendicular to the \"x\"-axis (with equation \"x\" = a constant) or described by equations of the form\nwhere \"A\" and \"B\" are real parameters which characterize the straight line.\n\nThe relationship of Lobachevsky coordinates to polar coordinates (assuming the origin is the pole and that the positive \"x\"-axis is the polar axis) is\n\nAnother coordinate system uses the distance from the point to the horocycle through the origin centered around formula_25 and the arclength along this horocycle.\n\nDraw the horocycle \"h\" through the origin centered at the ideal point formula_26 at the end of the \"x\"-axis.\n\nFrom point P draw the line \"p\" asymptotic to the \"x\"-axis to the right ideal point formula_26. \"P\" is the intersection of line \"p\" and horocycle \"h\".\n\nThe coordinate \"x\" is the distance from P to \"P\" – positive if P is between \"P\" and formula_26, negative if \"P\" is between P and formula_26.\n\nThe coordinate \"y\" is the arclength along horocycle \"h\" from the origin to \"P\".\n\nThe distance between two points given in these coordinates is\n\nThe corresponding metric tensor is: formula_31\n\nThe straight lines are described by equations of the form \"y\" = a constant or\n\nwhere \"x\" and \"y\" are the coordinates of the point on the line nearest to the ideal point formula_26 (i.e. having the largest value of \"x\" on the line).\n\nModel-based coordinate systems use one of the models of hyperbolic geometry and take the Euclidean coordinates inside the model as the hyperbolic coordinates.\n\nThe Beltrami coordinates of a point are the Euclidean coordinates of the point when the point is mapped in the Beltrami–Klein model of the hyperbolic plane, the \"x\"-axis is mapped to the segment and the origin is mapped to the centre of the boundary circle.\n\nThe following equations hold:\n\nThe Poincaré coordinates of a point are the Euclidean coordinates of the point when the point is mapped in the Poincaré disk model of the hyperbolic plane, the \"x\"-axis is mapped to the segment and the origin is mapped to the centre of the boundary circle.\n\nThe Poincaré coordinates, in terms of the Beltrami coordinates, are:\n\nThe Weierstrass coordinates of a point are the Euclidean coordinates of the point when the point is mapped in the hyperboloid model of the hyperbolic plane, the \"x\"-axis is mapped to the (half) hyperbola formula_36 and the origin is mapped to the point (0,0,1).\n\nThe point P with axial coordinates (\"x\", \"y\") is mapped to\n\nGyrovector space\n\nFrom Gyrovector space#triangle center\n\nThe study of triangle centers traditionally is concerned with Euclidean geometry, but triangle centers can also be studied in hyperbolic geometry. Using gyrotrigonometry, expressions for trigonometric barycentric coordinates can be calculated that have the same form for both euclidean and hyperbolic geometry. In order for the expressions to coincide, the expressions must \"not\" encapsulate the specification of the anglesum being 180 degrees.\n"}
{"id": "1060739", "url": "https://en.wikipedia.org/wiki?curid=1060739", "title": "Counterword", "text": "Counterword\n\nA counterword (also spelled counter word and counter-word) is a word such as \"so\" that is frequently used to answer (\"counter\") in a reflex-like manner and that has due to this frequent use quickly taken on a new, much less specific or much looser meaning or is even almost meaningless or performs a completely new function. The word \"so\", for example, is frequently used to begin an answer in the sense of \"Well...\" or to function as an indirect way of saying \"Before answering that, I'd like to...\" or even instead of saying \"On the contrary...\" or \"No, I...\".\n\nIn a more general sense, the term is used for such words also when they are not used as a reflex-like answer and even for any widely used words that (due to a similar change) now have a broad and vague range of meanings in many very different situations (e.g. case, awfully, fix, job, payoff).\n\nSince such change due to very frequent use occurs much more rapidly than the change in meaning all words go through, and since such words are even sometimes still simultaneously used in their original sense, the new usage is often considered incorrect by some speakers. Other examples include \"nice\", \"terrific\", \"terrible\", \"awful\", \"tremendous\", \"swell\", \"hopefully\" and \"very fine\" (degrading the meaning of \"fine\" to \"OK\").\n\nThe \"Oxford English Dictionary\" does not support this and defines counter-word as \"countersign\", noting that its usage is military and obsolete with a single quotation from 1678.\n"}
{"id": "2303812", "url": "https://en.wikipedia.org/wiki?curid=2303812", "title": "Dino Ciccarelli", "text": "Dino Ciccarelli\n\nDino Ciccarelli (born February 8, 1960) is a Canadian former professional hockey player who played 19 seasons in the National Hockey League, primarily with the Minnesota North Stars, but also notably with the Detroit Red Wings, with whom he had his third-highest scoring season. He scored 1,200 points in his NHL career. His 608 career NHL goals are also the most goals scored by a draft-eligible player who was not drafted by an NHL team. Ciccarelli was elected to the Hockey Hall of Fame in 2010.\n\nCiccarelli grew up playing minor hockey in his hometown of Sarnia, Ontario in the Southwestern Ontario Minor Hockey League of the Ontario Minor Hockey Association. He made Sarnia's Jr. 'B' hockey team as a 15-year-old in the fall of 1975 and ended up leading it in scoring with 45 goals and 43 assists for 88 points in just 40 games. He is one of two Sarnia Jr. 'B' graduates to go on to be elected to the Hockey Hall of Fame (the other being Phil Esposito, who led the Sarnia Legionnaires in scoring in 1961).\n\nCiccarelli joined the London Knights of the OMJHL as a 16-year-old for the 1976-77 season. In his first season with the Knights, Ciccarelli had 39 goals and 82 points in 66 games, finishing fifth in team scoring. In the playoffs, Ciccarelli had 11 goals and 24 points in 20 games, as London lost to the Ottawa 67's in the J. Ross Robertson Cup finals.\n\nIn his second season with the Knights in 1977-78, Ciccarelli scored a league high 72 goals, and added 70 assists for 142 points, which was third highest total in the OMJHL. In the post-season, Ciccarelli contributed six goals and 16 points in nine games. After the season, Ciccarelli was awarded the Jim Mahon Memorial Trophy, which is given to the highest scoring right winger in the OMJHL.\n\nInjuries cut short Ciccarelli's 1978-79 season, as he appeared in only 30 games, scoring eight goals and 19 points. In seven playoff games, Ciccarelli scored three goals and eight points. After not being selected in the NHL Entry Draft, Ciccarelli signed a contract with the Minnesota North Stars on September 28, 1979.\n\nThe North Stars kept Ciccarelli with the Knights for the 1979-80, and Ciccarelli rebounded, scoring 50 goals and 103 points. In five games in the playoffs, Ciccarelli scored two goals, and finished with eight points.\n\nCiccarelli made his professional hockey debut with the Oklahoma City Stars of the CHL at the end of the 1979-80 season. In six games with Oklahoma City, Ciccarelli had three goals and five points.\n\nHe spent most of the 1980-81 season with Oklahoma City, playing in 48 games, scoring 32 goals and 57 points. Ciccarelli was brought up to the National Hockey League, and in 32 games with the Minnesota North Stars, Ciccarelli had 18 goals and 30 points. In the playoffs, Ciccarelli scored 14 goals and 21 points in 19 games, as the North Stars lost to the New York Islanders in the 1981 Stanley Cup Finals.\n\nCiccarelli spent the entire 1981-82 NHL season with the North Stars, playing in 76 games, scoring a team high 55 goals, while earning 106 points for the season. During the season, Ciccarelli played in the 1982 NHL All-Star Game, hosted by the Washington Capitals, with the Campbell Conference, where he assisted on a goal by Wayne Gretzky in a 4-2 loss to the Wales Conference. In the playoffs, Ciccarelli had three goals and four points in four games.\n\nHe saw a decrease in offensive production during the 1982-83 season, scoring 37 goals and 75 points in 77 games, which was 18 fewer goals and 31 less points than the previous season. Ciccarelli did appear in the 1983 NHL All-Star Game, hosted by the New York Islanders. Ciccarelli had a goal and an assist as the Campbell Conference defeated the Wales Conference 9-3. In the post-season, Ciccarelli appeared in nine games, scoring four goals and 10 points.\n\nCiccarelli had another solid season with Minnesota in 1983-84, scoring 38 goals and 71 points in 79 games, helping the North Stars into the playoffs once again. In 16 playoff games, Ciccarelli had four goals and nine points, as the North Stars lost to the Edmonton Oilers in the Campbell Conference finals.\n\nInjuries cut short Ciccarelli's season in 1984-85, playing in only 51 games, Ciccarelli scored 15 goals and 32 points, his lowest point total since his rookie season in 1980-81, and the lowest goal total of his NHL career. In nine playoff games, Ciccarelli had three goals and six points.\n\nCiccarelli had a healthy 1985-86 season, playing in 75 games, he led the North Stars with 44 goals, while finishing second in points with 89, which were his highest totals since 1981-82. In five playoff games, Ciccarelli was held to an assist.\n\nIn 1986-87, Ciccarelli improved his offensive numbers once again, scoring 52 goals and 103 points in 80 games, which both led the club in scoring. The North Stars struggled during the season, and failed to qualify for the post season.\n\nCiccarelli had his third straight 40+ goal season in 1987-88, as he once again led Minnesota with 41 goals and 86 points in 67 games. It was another tough season for the team though, as they missed the playoffs for the second straight season.\n\nIn 1988-89, Ciccarelli made his first all-star game appearance in six years, as he played with the Campbell Conference in the 1989 NHL All-Star Game held in Edmonton, Alberta. In the game, Ciccarelli assisted on a goal by Steve Yzerman, as the Campbell Conference won the game over the Wales Conference by a score of 9-5. Overall with Minnesota, Ciccarelli played in 65 games, scoring 32 goals and 59 points. On March 7, 1989, the North Stars traded Ciccarelli and Bob Rouse to the Washington Capitals for Mike Gartner and Larry Murphy.\n\nCiccarelli finished the 1988-89 season with the Washington Capitals, playing just 11 games, Ciccarelli scored 12 goals and 15 points, helping the team into the playoffs. He played his first game as a Capital on March 8, 1989, getting no points in a 3-2 loss to the Montreal Canadiens. He scored his first goal with Washington on March 11, 1989, scoring against John Vanbiesbrouck in a 4-2 win over the New York Rangers. On March 18, 1989, Ciccarelli had a four-goal, seven point game in a huge 8-2 victory over the Hartford Whalers. Ciccarelli appeared in his first playoff game with the Capitals on April 5, 1989, getting no points in a 3-2 win over the Philadelphia Flyers. He scored his first playoff goal with Washington on April 6, 1989, scoring against Ron Hextall in a 3-2 loss. Ciccarelli played in six games, scoring three goals and six points as the Capitals were eliminated by the Flyers.\n\nIn Ciccarelli's first full season with the Capitals, he scored 41 goals in 1989-90, which marked his fifth consecutive season of 40+ goals, while finishing with a team high 79 points. Ciccarelli had a four-goal game against the Quebec Nordiques on February 6, 1990 in a 12-2 victory. In the playoffs, Ciccarelli had eight goals and 11 points in eight games before suffering an injury on April 21, 1990 against the New York Rangers that would end his season. Ciccarelli had a hat trick in the playoffs, scoring three goals and an assist against the New Jersey Devils in a 5-4 overtime win on April 5, 1990.\n\nCiccarelli missed 26 games during the 1990-91, in which he scored 21 goals and 39 points, his lowest totals since 1984-85. He did score a hat trick against the Edmonton Oilers on February 8, 1991 in a 6-3 win. Ciccarelli was productive in the playoffs, scoring five goals and nine points in 11 games.\n\nHe rebounded in the 1991-92, as Ciccarelli scored 38 goals, which was a team high, while earning 76 points, helping the Capitals into the playoffs once again. In the playoffs, Ciccarelli had a four-goal game on April 25, 1992, as Washington defeated the Pittsburgh Penguins 7-2. In seven games, he scored five goals and nine points. On June 20, 1992, Ciccarelli was traded from the Capitals to the Detroit Red Wings for Kevin Miller.\n\nCiccarelli made his debut with the Detroit Red Wings on October 6, 1992, getting no points in a 4-1 loss to the Winnipeg Jets. On October 8, 1992, Ciccarelli earned his first goal as a Red Wing, scoring against Kelly Hrudey of the Los Angeles Kings in a 5-3 victory. Ciccarelli finished his first season with Detroit with 41 goals and 97 points, which were both the second highest totals on the team. Ciccarelli's 97 points were his highest since 1986-87, when he recorded 103 with the Minnesota North Stars. In the playoffs, Ciccarelli played his first game with the Red Wings on April 19, 1993, earning an assist in a 6-3 win over the Toronto Maple Leafs. Ciccarelli scored his first playoff goal as a member of the Red Wings on April 27, 1993 against Felix Potvin in a 5-4 loss to the Maple Leafs. On April 29, 1993, Ciccarelli had a hat trick for the Red Wings in a 7-3 win over Toronto. Overall, Ciccarelli appeared in seven playoff games, scoring four goals and six points.\n\nCiccarelli saw his point total decline by 40 in the 1993-94 season, as he scored 28 goals and 57 points in 66 games. He did have a six-point game against the Vancouver Canucks on April 5, 1994, scoring four goals and adding two assists in an 8-3 victory. In the post-season, Ciccarelli had five goals and seven points in seven games.\n\nWith a shortened 1994-95 due to the lockout, Ciccarelli appeared in 42 games, scoring 16 goals and 43 points, which placed him third in team scoring. Ciccarelli had a four assist game against the Winnipeg Jets on March 22, 1995 in a 6-3 win. In the playoffs, Ciccarelli had a hat trick against the Dallas Stars on May 11, 1995 in a 5-1 victory. He finished the playoffs with nine goals and 11 points in 16 games, as the Red Wings lost to the New Jersey Devils in the 1995 Stanley Cup Finals.\n\nIn 1995-96, Ciccarelli scored 22 goals and 43 points in 64 games, helping the Red Wings set an NHL record for wins in a season with 62. In the post-season, Ciccarelli had six goals and eight points in 17 games. On August 27, 1996, the Red Wings traded Ciccarelli to the Tampa Bay Lightning for a fourth round draft pick in the 1998 NHL Entry Draft.\n\nCiccarelli played his first game as a member of the Tampa Bay Lightning on October 5, 1996, getting a goal and two points in a 4-3 win over the Pittsburgh Penguins. On November 8, 1996, Ciccarelli had a hat trick against the Pittsburgh Penguins in a 5-5 tie. Ciccarelli played in the 1997 NHL All-Star Game held in San Jose, California, where he had an assist for the Eastern Conference in an 11-7 win over the Western Conference. He finished the 1996-97 season playing in 77 games, scoring a team high 35 goals, while earning 60 points. Notably, he scored the final goal in the last game that the Hartford Whalers played in on April 13, 1997, scoring the only goal for the Lightning in a 2-1 loss.\n\nHe began the 1997-98 season with the Lightning, he played in 34 games with Tampa Bay, scoring 11 goals and 17 points. On January 15, 1998, the Lightning traded Ciccarelli and Jeff Norton to the Florida Panthers for Mark Fitzpatrick and Jody Hull.\n\nCiccarelli played his first game with the Florida Panthers on January 21, 1998. He scored his first goal with Florida on January 24, 1998 against Kelly Hrudey of the San Jose Sharks in a 1-1 tie. He finished the season with five goals and 16 points in 28 games with the Panthers.\n\nCiccarelli had an injury plagued 1998-99 season, as he missed the majority of the season after suffering a back injury against the Chicago Blackhawks on November 4, 1998. He played in only 14 games, scoring six goals and seven points. On August 31, 1999, Ciccarelli announced his retirement.\n\nCiccarelli played with Canada at various international events during his career. At the 1980 World Junior Ice Hockey Championships held in Helsinki, Finland, Ciccarelli had five goals and six points in five games, as Canada finished in fifth place. At the 1982 IIHF World Hockey Championship held in Finland, Ciccarelli had two goals and three points in nine games, as the Canadians won the Bronze Medal. Ciccarelli also played in the 1987 IIHF World Hockey Championship in Austria, getting four goals and six points in 10 games as Canada finished in fourth place.\n\nCiccarelli's career featured some controversial moments, both on and off the ice. In 1987 he pleaded guilty to indecent exposure and received probation. Then on January 6, 1988, in a game played at Maple Leaf Gardens, Ciccarelli attacked then-Maple Leafs rookie defenceman Luke Richardson with his stick. As a result of this incident, Ciccarelli was convicted of assault, fined $1,000, and sentenced to one day in jail.\n\nCiccarelli was inducted into the Hockey Hall of Fame in 2010, eight years after he first became eligible. Press reports speculated that his criminal activity was the reason it took him years to gain entrance.\n\nHis junior team, the London Knights, also retired Ciccarelli's number 8.\n\nCiccarelli owned the nightclub Club 22 in Shelby Charter Township, named for the jersey number he wore with the Capitals, Red Wings, Lightning and Panthers. It was closed in 2011. Shortly after closing, Ciccarelli opened a sports bar in the same location. The name of the establishment is Ciccarelli's Sports Bar Theater, named after the \"theater\" style atmosphere. He has since opened two additional sports bars, one across from the Palace of Auburn Hills; and the other at the site of the former Post Bar in downtown Detroit, near Cobo Center. The Detroit location is only open for Red Wings home games and other special events.\n\n\n\n"}
{"id": "783925", "url": "https://en.wikipedia.org/wiki?curid=783925", "title": "Emergentism", "text": "Emergentism\n\nIn philosophy, emergentism is the belief in emergence, particularly as it involves consciousness and the philosophy of mind, and as it contrasts (or not) with reductionism. A property of a system is said to be emergent if it is a new outcome of some other properties of the system and their interaction, while it is itself different from them. Emergent properties are not identical with, reducible to, or deducible from the other properties. The different ways in which this independence requirement can be satisfied lead to variant types of emergence.\n\nAll varieties of emergentism strive to be compatible with physicalism, the theory that the universe is composed exclusively of physical entities, and in particular with the evidence relating changes in the brain with changes in mental functioning. Many forms of emergentism, including proponents of complex adaptive systems, do not hold a material but rather a relational or processural view of the universe. Furthermore, they view mind–body dualism as a conceptual error insofar as mind and body are merely different types of relationships. As a theory of mind (which it is not always), emergentism differs from idealism, eliminative materialism, identity theories, neutral monism, panpsychism, and substance dualism, whilst being closely associated with property dualism. It is generally not obvious whether an emergent theory of mind embraces mental causation or must be considered epiphenomenal.\n\nSome varieties of emergentism are not specifically concerned with the mind–body problem, and instead suggest a hierarchical or layered view of the whole of nature, with the layers arranged in terms of increasing complexity with each requiring its own special science. Typically physics (mathematical physics, particle physics, and classical physics) is basic, with chemistry built on top of it, then biology, psychology, and social sciences. Reductionists respond that the arrangement of the sciences is a matter of convenience, and that chemistry is derivable from physics (and so forth) \"in principle\", an argument which gained force after the establishment of a quantum-mechanical basis for chemistry.\n\nOther varieties see mind or consciousness as specifically and anomalously requiring emergentist explanation, and therefore constitute a family of positions in the philosophy of mind. Douglas Hofstadter summarises this view as \"the soul is more than the sum of its parts\". A number of philosophers have offered the argument that qualia constitute the hard problem of consciousness, and resist reductive explanation in a way that all other phenomena do not. In contrast, reductionists generally see the task of accounting for the possibly atypical properties of mind and of living things as a matter of showing that, contrary to appearances, such properties are indeed fully accountable in terms of the properties of the basic constituents of nature and therefore in no way genuinely atypical.\n\nIntermediate positions are possible: for instance, some emergentists hold that emergence is neither universal nor restricted to consciousness, but applies to (for instance) living creatures, or self-organising systems, or complex systems.\n\nSome philosophers hold that emergent properties causally interact with more fundamental levels, an idea known as downward causation. Others maintain that higher-order properties simply supervene over lower levels without direct causal interaction.\n\nAll the cases so far discussed have been synchronic, i.e. the emergent property exists simultaneously with its basis.\nYet another variation operates diachronically. Emergentists of this type believe that \"genuinely novel properties\" can come into being, without being accountable in terms of the preceding history of the universe. (Contrast with indeterminism where it is only the \"arrangement or configuration\" of matter that is unaccountable). These evolution-inspired theories often have a theological aspect, as in the process philosophy of Alfred North Whitehead and Charles Hartshorne.\n\nA refinement of vitalism may be recognized in contemporary molecular histology in the proposal that some key organising and structuring features of organisms, perhaps including even life itself, are examples of emergent processes; those in which a complexity arises, out of interacting chemical processes forming interconnected feedback cycles, that cannot fully be described in terms of those processes since the system as a whole has properties that the constituent reactions lack.\n\nWhether emergent system properties should be grouped with traditional vitalist concepts is a matter of semantic controversy. In a light-hearted millennial vein, Kirshner and Michison call research into integrated cell and organismal physiology “molecular vitalism.”\n\nAccording to Emmeche \"et al.\" (1997):\n\n\"On the one hand, many scientists and philosophers regard emergence as having only a pseudo-scientific status. On the other hand, new developments in physics, biology, psychology, and crossdisciplinary fields such as cognitive science, artificial life, and the study of non-linear dynamical systems have focused strongly on the high level 'collective behaviour' of complex systems, which is often said to be truly emergent, and the term is increasingly used to characterize such systems.\"\nEmmeche \"et al.\" (1998) state that \"there is a very important difference between the vitalists and the emergentists: the vitalist's creative forces were relevant only in organic substances, not in inorganic matter. Emergence hence is creation of new properties regardless of the substance involved.\" \"The assumption of an extra-physical vitalis (vital force, entelechy, élan vital, etc.), as formulated in most forms (old or new) of vitalism, is usually without any genuine explanatory power. It has served altogether too often as an intellectual tranquilizer or verbal sedative—stifling scientific inquiry rather than encouraging it to proceed in new directions.\"\n\nJohn Stuart Mill outlined his version of emergentism in \"System of Logic\" (1843). Mill argued that the properties of some physical systems, such as those in which dynamic forces combine to produce simple motions, are subject to a law of nature he called the \"Composition of Causes\". According to Mill, emergent properties are not subject to this law, but instead amount to more than the sums of the properties of their parts.\n\nMill believed that various chemical reactions (poorly understood in his time) could provide examples of emergent properties, although some critics believe that modern physical chemistry has shown that these reactions can be given satisfactory reductionist explanations. For instance, it has been claimed by Dirac that the whole of chemistry is, in principle,\ncontained in the Schrödinger equation.\n\nBritish philosopher C. D. Broad defended a realistic epistemology in \"The Mind and its Place in Nature\" (1925) arguing that emergent materialism is the most likely solution to the mind–body problem.\n\nBroad defined emergence as follows:\nPut in abstract terms the emergent theory asserts that there are certain wholes, composed (say)\nof constituents A, B, and C in a relation R to each other; that all wholes composed of\nconstituents of the same kind as A, B, and C in relations of the same kind as R have certain\ncharacteristic properties; that A, B, and C are capable of occurring in other kinds of complex\nwhere the relation is not of the same kind as R; and that the characteristic properties of the\nwhole R(A, B, C) cannot, even in theory, be deduced from the most complete knowledge of\nthe properties of A, B, and C in isolation or in other wholes which are not of the form R(A, B,\nC).\nThis definition amounted to the claim that mental properties would count as emergent if and only if philosophical zombies were metaphysically possible. Many philosophers take this position to be inconsistent with some formulations of psychophysical supervenience.\n\nSamuel Alexander's views on emergentism, argued in \"Space, Time, and Deity\" (1920), were inspired in part by the ideas in psychologist C. Lloyd Morgan's \"Emergent Evolution\". Alexander believed that emergence was fundamentally inexplicable, and that emergentism was simply a \"brute empirical fact\":\n\n\"The higher quality emerges from the lower level of existence and has its roots therein, but it emerges therefrom, and it does not belong to that level, but constitutes its possessor a new order of existent with its special laws of behaviour. The existence of emergent qualities thus described is something to be noted, as some would say, under the compulsion of brute empirical fact, or, as I should prefer to say in less harsh terms, to be accepted with the “natural piety” of the investigator. It admits no explanation.\" (Space, Time, and Deity)\n\nDespite the causal and explanatory gap between the phenomena on different levels, Alexander held that emergent qualities were \"not\" epiphenomenal. His view can perhaps best be described as a form of non-reductive physicalism (NRP) or supervenience theory.\n\nLudwig von Bertalanffy founded general system theory (GST), which is a more contemporary approach to emergentism. A popularization of many of the elements of GST may be found in \"The Web of Life\" by Fritjof Capra.\n\nAddressing emergentism (under the guise of non-reductive physicalism) as a solution to the mind–body problem Jaegwon Kim has raised an objection based on causal closure and overdetermination.\n\nEmergentism strives to be compatible with physicalism, and physicalism, according to Kim, has a principle of causal closure according to which every physical event is fully accountable in terms of physical causes. This seems to leave no \"room\"\nfor mental causation to operate. If our bodily movements were caused by the preceding state of our bodies \"and\" our decisions and intentions, they would be overdetermined. Mental causation in this sense is not\nthe same as free will, but is only the claim that mental states are causally relevant. If emergentists respond by abandoning the idea of mental causation, their position becomes a form of epiphenomenalism.\n\nIn detail: he proposes (using the chart on the right) that \"M1\" causes \"M2\" (these are mental events) and \"P1\" causes \"P2\" (these are physical events). \"P1\" realises \"M1\" and \"P2\" realises \"M2\". However \"M1\" does not causally effect \"P1\" (i.e., \"M1\" is a consequent event of \"P1\"). If \"P1\" causes \"P2\", and \"M1\" is a result of \"P1\", then \"M2\" is a result of \"P2\". He says that the only alternatives to this problem is to accept dualism (where the mental events are independent of the physical events) or eliminativism (where the mental events do not exist).\n\n"}
{"id": "47763", "url": "https://en.wikipedia.org/wiki?curid=47763", "title": "Environmental economics", "text": "Environmental economics\n\nEnvironmental economics is a sub-field of economics that is concerned with environmental issues. It has become a widely studied topic due to growing concerns in regards to the environment in the twentyfirst century. Quoting from the National Bureau of Economic Research Environmental Economics program:\n\n... Environmental Economics ... undertakes theoretical or empirical studies of the economic effects of national or local environmental policies around the world ... . Particular issues include the costs and benefits of alternative environmental policies to deal with air pollution, water quality, toxic substances, solid waste, and global warming.\n\nEnvironmental economics is distinguished from ecological economics in that ecological economics emphasizes the economy as a subsystem of the ecosystem with its focus upon preserving natural capital. One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing \"strong\" sustainability and rejecting the proposition that natural capital can be substituted by human-made capital.\n\nCentral to environmental economics is the concept of market failure. Market failure means that markets fail to allocate resources efficiently. As stated by Hanley, Shogren, and White (2007) in their textbook \"Environmental Economics\": \"A market failure occurs when the market does not allocate scarce resources to generate the greatest social welfare. A wedge exists between what a private person does given market prices and what society might want him or her to do to protect the environment. Such a wedge implies wastefulness or economic inefficiency; resources can be reallocated to make at least one person better off without making anyone else worse off.\" Common forms of market failure include externalities, non-excludability and non-rivalry.\n\nAn externality exists when a person makes a choice that affects other people in a way that is not accounted for in the market price. An externality can be positive or negative, but is usually associated with negative externalities in environmental economics. For instance, water seepage in residential buildings happen in upper floor affect the lower floor. Another example concerns how the sale of Amazon timber disregards the amount of carbon dioxide released in the cutting. Or a firm emitting pollution will typically not take into account the costs that its pollution imposes on others. As a result, pollution may occur in excess of the 'socially efficient' level, which is the level that would exist if the market was required to account for the pollution. A classic definition influenced by Kenneth Arrow and James Meade is provided by Heller and Starrett (1976), who define an externality as \"a situation in which the private economy lacks sufficient incentives to create a potential market in some good and the nonexistence of this market results in losses of Pareto efficiency\". In economic terminology, externalities are examples of market failures, in which the unfettered market does not lead to an efficient outcome.\n\nWhen it is too costly to exclude some people from access to an environmental resource, the resource is either called a common property resource (when there is rivalry for the resource, such that one person's use of the resource reduces others' opportunity to use the resource) or a public good (when use of the resource is non-rivalrous). In either case of non-exclusion, market allocation is likely to be inefficient.\n\nThese challenges have long been recognized. Hardin's (1968) concept of the tragedy of the commons popularized the challenges involved in non-exclusion and common property. \"Commons\" refers to the environmental asset itself, \"common property resource\" or \"common pool resource\" refers to a property right regime that allows for some collective body to devise schemes to exclude others, thereby allowing the capture of future benefit streams; and \"open-access\" implies no ownership in the sense that property everyone owns nobody owns.\n\nThe basic problem is that if people ignore the scarcity value of the commons, they can end up expending too much effort, over harvesting a resource (e.g., a fishery). Hardin theorizes that in the absence of restrictions, users of an open-access resource will use it more than if they had to pay for it and had exclusive rights, leading to environmental degradation. See, however, Ostrom's (1990) work on how people using real common property resources have worked to establish self-governing rules to reduce the risk of the tragedy of the commons.\n\nThe mitigation of climate change effects is an example of a public good, where the social benefits are not reflected completely in the market price. This is a public good since the risks of climate change are both non-rival and non-excludable. Such efforts are non-rival since climate mitigation provided to one does not reduce the level of mitigation that anyone else enjoys. They are non-excludable actions as they will have global consequences from which no one can be excluded. A country's incentive to invest in carbon abatement is reduced because it can \"free ride\" off the efforts of other countries. Over a century ago, Swedish economist Knut Wicksell (1896) first discussed how public goods can be under-provided by the market because people might conceal their preferences for the good, but still enjoy the benefits without paying for them.\nAssessing the economic value of the environment is a major topic within the field. Use and indirect use are tangible benefits accruing from natural resources or ecosystem services (see the nature section of ecological economics). Non-use values include existence, option, and bequest values. For example, some people may value the existence of a diverse set of species, regardless of the effect of the loss of a species on ecosystem services. The existence of these species may have an option value, as there may be the possibility of using it for some human purpose. For example, certain plants may be researched for drugs. Individuals may value the ability to leave a pristine environment to their children.\n\nUse and indirect use values can often be inferred from revealed behavior, such as the cost of taking recreational trips or using hedonic methods in which values are estimated based on observed prices. Non-use values are usually estimated using stated preference methods such as contingent valuation or choice modelling. Contingent valuation typically takes the form of surveys in which people are asked how much they would pay to observe and recreate in the environment (willingness to pay) or their willingness to accept (WTA) compensation for the destruction of the environmental good. Hedonic pricing examines the effect the environment has on economic decisions through housing prices, traveling expenses, and payments to visit parks.\n\nSolutions advocated to correct such externalities include:\n\n\nEnvironmental economics is related to ecological economics but there are differences. Most environmental economists have been trained as economists. They apply the tools of economics to address environmental problems, many of which are related to so-called market failures—circumstances wherein the \"invisible hand\" of economics is unreliable. Most ecological economists have been trained as ecologists, but have expanded the scope of their work to consider the impacts of humans and their economic activity on ecological systems and services, and vice versa. This field takes as its premise that economics is a strict subfield of ecology. Ecological economics is sometimes described as taking a more pluralistic approach to environmental problems and focuses more explicitly on long-term environmental sustainability and issues of scale.\n\nEnvironmental economics is viewed as more pragmatic in a price system; ecological economics as more idealistic in its attempts not to use money as a primary arbiter of decisions. These two groups of specialists sometimes have conflicting views which may be traced to the different philosophical underpinnings.\n\nAnother context in which externalities apply is when globalization permits one player in a market who is unconcerned with biodiversity to undercut prices of another who is - creating a race to the bottom in regulations and conservation. This, in turn, may cause loss of natural capital with consequent erosion, water purity problems, diseases, desertification, and other outcomes which are not efficient in an economic sense. This concern is related to the subfield of sustainable development and its political relation, the anti-globalization movement.\n\nEnvironmental economics was once distinct from resource economics. Natural resource economics as a subfield began when the main concern of researchers was the optimal commercial exploitation of natural resource stocks. But resource managers and policy-makers eventually began to pay attention to the broader importance of natural resources (e.g. values of fish and trees beyond just their commercial exploitation). It is now difficult to distinguish \"environmental\" and \"natural resource\" economics as separate fields as the two became associated with sustainability. Many of the more radical green economists split off to work on an alternate political economy.\n\nEnvironmental economics was a major influence on the theories of natural capitalism and environmental finance, which could be said to be two sub-branches of environmental economics concerned with resource conservation in production, and the value of biodiversity to humans, respectively. The theory of natural capitalism (Hawken, Lovins, Lovins) goes further than traditional environmental economics by envisioning a world where natural services are considered on par with physical capital.\n\nThe more radical Green economists reject neoclassical economics in favour of a new political economy beyond capitalism or communism that gives a greater emphasis to the interaction of the human economy and the natural environment, acknowledging that \"economy is three-fifths of ecology\" - Mike Nickerson.\n\nThese more radical approaches would imply changes to money supply and likely also a bioregional democracy so that political, economic, and ecological \"environmental limits\" were all aligned, and not subject to the arbitrage normally possible under capitalism.\n\nAn emerging sub-field of environmental economics studies its intersection with development economics. Dubbed \"envirodevonomics\" by Michael Greenstone and B. Kelsey Jack in their paper \"Envirodevonomics: A Research Agenda for a Young Field,\" the sub-field is primarily interested in studying \"why environmental quality [is] so poor in developing countries.\" A strategy for better understanding this correlation between a country's GDP and its environmental quality involves analyzing how many of the central concepts of environmental economics, including market failures, externalities, and willingness to pay, may be complicated by the particular problems facing developing countries, such as political issues, lack of infrastructure, or inadequate financing tools, among many others.\n\nThe main academic and professional organizations for the discipline of Environmental Economics are the Association of Environmental and Resource Economists (AERE) and the European Association for Environmental and Resource Economics (EAERE). The main academic and professional organization for the discipline of Ecological Economics is the International Society for Ecological Economics (ISEE). The main organization for Green Economics is the Green Economics Institute.\n\n\n"}
{"id": "6457043", "url": "https://en.wikipedia.org/wiki?curid=6457043", "title": "Five Strengths", "text": "Five Strengths\n\nThe Five Strengths (Sanskrit, Pali: \"\") in Buddhism are faith, energy, mindfulness, concentration, and wisdom. They are one of the seven sets of \"qualities conducive to enlightenment.\" They are parallel facets of the five \"spiritual faculties.\"\n\n\"Pañca\" (Sanskrit, Pali) means \"five.\" \"Bala\" (Sanskrit, Pali) means \"power,\" \"strength,\" \"force.\"\n\nFaith and Wisdom balance each other, as do Energy and Concentration.\nThe Five Faculties are ‘controlling' faculties because they control or master their opposites.\nThe faculties and powers are two aspects of the same thing.\n\n\n\n\n"}
{"id": "845060", "url": "https://en.wikipedia.org/wiki?curid=845060", "title": "Fundamental theorem of Riemannian geometry", "text": "Fundamental theorem of Riemannian geometry\n\nIn Riemannian geometry, the fundamental theorem of Riemannian geometry states that on any Riemannian manifold (or pseudo-Riemannian manifold) there is a unique torsion-free metric connection, called the Levi-Civita connection of the given metric. Here a metric (or Riemannian) connection is a connection which preserves the metric tensor. More precisely:\n\nFundamental Theorem of Riemannian Geometry. Let (\"M\", \"g\") be a Riemannian manifold (or pseudo-Riemannian manifold). Then there is a unique connection ∇ which satisfies the following conditions:\n\nThe first condition means that the metric tensor is preserved by parallel transport, while the second condition expresses the fact that the torsion of ∇ is zero.\n\nAn extension of the fundamental theorem states that given a pseudo-Riemannian manifold there is a unique connection preserving the metric tensor with any given vector-valued 2-form as its torsion. The difference between an arbitrary connection (with torsion) and the corresponding Levi-Civita connection is the contorsion tensor.\n\nThe following technical proof presents a formula for Christoffel symbols of the connection in a local coordinate system. For a given metric this set of equations can become rather complicated. There are quicker and simpler methods to obtain the Christoffel symbols for a given metric, e.g. using the action integral and the associated Euler-Lagrange equations.\n\nA metric defines the curves which are geodesics ; but a connection also defines the geodesics (see also parallel transport). A connection formula_5 is said to be equal to another formula_6 in two different ways:\nThis means that two different connections can lead to the same geodesics while giving different results for some vector fields.\n\nBecause a metric also defines the geodesics of a differential manifold, for some metric there is not only one connection defining the same geodesics , and given a metric, the only connection which defines the same geodesics (which leaves the metric unchanged by parallel transport) and which is torsion-free is the Levi-Civita connection (which is obtained from the metric by differentiation).\n\nLet \"m\" be the dimension of \"M\" and, in some local chart, consider the standard coordinate vector fields\n\nLocally, the entry \"g\" of the metric tensor is then given by\n\nTo specify the connection it is enough to specify, for all \"i\", \"j\", and \"k\",\n\nWe also recall that, locally, a connection is given by \"m\" smooth functions\n\nwhere\n\nThe torsion-free property means\n\nOn the other hand, compatibility with the Riemannian metric implies that\n\nFor a fixed, \"i\", \"j\", and \"k\", permutation gives 3 equations with 6 unknowns. The torsion free assumption reduces the number of variables to 3. Solving the resulting system of 3 linear equations gives unique solutions\n\nThis is the first Christoffel identity.\n\nSince\n\nwhere we use Einstein summation convention. That is, an index repeated subscript and superscript implies that it is summed over all values. Inverting the metric tensor gives the second Christoffel identity:\n\nOnce again, with Einstein summation convention. The resulting unique connection is called the Levi-Civita connection.\n\nAn alternative proof of the Fundamental theorem of Riemannian geometry proceeds by showing that a torsion-free metric connection on a Riemannian manifold is necessarily given by the Koszul formula:\n\nThis proves the uniqueness of the Levi-Civita connection. Existence is proven by showing that this expression is tensorial in \"X\" and \"Z\", satisfies the Leibniz rule in \"Y\", and that hence defines a connection. This is a metric connection, because the symmetric part of the formula in \"Y\" and \"Z\" is the first term on the first line; it is torsion-free because the anti-symmetric part of the formula in \"X\" and \"Y\" is the first term on the second line.\n\n"}
{"id": "315563", "url": "https://en.wikipedia.org/wiki?curid=315563", "title": "Gemeinschaft and Gesellschaft", "text": "Gemeinschaft and Gesellschaft\n\nGemeinschaft () and Gesellschaft (), generally translated as \"community and society\", are categories which were used by the German sociologist Ferdinand Tönnies in order to categorize social ties into two dichotomous sociological types which define each other. Max Weber, who is generally recognized as being a founding figure in sociology, also wrote extensively about the relationship between \"Gemeinschaft\" and \"Gesellschaft\". Weber wrote in direct response to Tönnies.\n\nThe \"Gemeinschaft–Gesellschaft\" dichotomy was proposed by Tönnies as a purely conceptual tool rather than as an ideal type in the way it was used by Max Weber to accentuate the key elements of a historic/social change. According to the dichotomy, social ties can be categorized, on one hand, as belonging to personal social interactions, and the roles, values, and beliefs based on such interactions (\"Gemeinschaft\", German, commonly translated as \"community\"), or on the other hand as belonging to indirect interactions, impersonal roles, formal values, and beliefs based on such interactions (\"Gesellschaft\", German, commonly translated as \"society\").\n\nTönnies was a Thomas Hobbes scholarhe edited the standard modern editions of Hobbes's \"The Elements of Law\" and \"Behemoth\". It was his study of Hobbes that encouraged Tönnies to devote himself wholly to the philosophy of history and the philosophy of law. And it has been argued that he derived both categories from Hobbes's concepts of \"concord\" and \"union\".\n\nThe second edition, published in 1912, of the work in which Tönnies further promoted the concepts turned out to be an unexpected but lasting success after the first edition was published in 1887 with the subtitle \"Treatise on Communism and Socialism as Empirical Patterns of Culture\". Seven more German editions followed, the last in 1935, and it became part of the general stock of ideas with which pre-1933 German intellectuals were quite familiar. The book sparked a revival of corporatist thinking, including the rise of neo-medievalism, the rise of support for guild socialism, and caused major changes in the field of sociology.\n\nThe concepts \"Gemeinschaft\" and \"Gesellschaft\" were also used by Max Weber in \"Economy and Society\", which was first published in 1921. Weber wrote in direct response to Tönnies, and argued that \"Gemeinschaft\" is rooted in a \"subjective feeling\" that may be \"affectual or traditional\". \"Gesellschaft\"-based relationships, according to Weber, are rooted in \"rational agreement by mutual consent\", the best example of which is a commercial contract. To emphasize the fluidity and amorphousness of the relationship between \"Gemeinschaft\" and \"Gesellschaft\", Weber modified the terms in German to \"Vergemeinschaftung\", and \"Vergesellschaftung\", which are the gerund forms of the German words. Weber's distinction between \"Gemeinschaft\" and \"Gesellschaft\" is highlighted in the essay \"Classes, Stände, Parties\", which is the basis for Weber's three-component theory of stratification.\n\nHaving propounded his conception of the \"Gemeinschaft\"–\"Gesellschaft\" dichotomy, Tönnies was drawn into a sharp polemic with Émile Durkheim. In a review of Tönnies' book in 1889, Durkheim interpreted \"Gemeinschaft\" as an organic community, and \"Gesellschaft\" as a mechanical one, reproaching Tönnies for considering the second type of social organisation artificial, and not seeing the transition from the one type to the other. Tönnies did not agree with such an interpretation of his views, and in turn, when reviewing Durkheim's \"The Division of Labour in Society\" (1896), wrote that Durkheim's whole sociology was a modification and variant of Spencer's, which was also unjust.\n\nEric Hobsbawm argued that, as globalization turns the entire planet into an increasingly remote kind of \"Gesellschaft\", so too collective identity politics seeks for a fictitious remaking of the qualities of \"Gemeinschaft\" by artificially reforging group bonds and identities.\n\nFredric Jameson highlights the ambivalent envy felt by those constructed by \"Gesellschaft\" for remaining enclaves of \"Gemeinschaft\", even as they inevitably corrode their existence.\n\nIn business usage, \"Gesellschaft\" is the German term for \"company\", as in \"Aktiengesellschaft\" or \"Gesellschaft mit beschränkter Haftung\" (\"GmbH\"). \"Gemeinschaft\" is used to identify groups which have or are claimed to have an element of affective loyalty. One important usage is in the German name for the European Economic Community, \"Europäische Wirtschaftsgemeinschaft\".\n\nThe German phrase for \"mutual insurance company\" includes both words, \"mutual\" and \"company.\" In the 1980's, the Frankenmuth Mutual Insurance Company, headquartered in the German-American city of Frankenmuth, Michigan, released various promotional items such as matchbooks, featuring, in a traditional German Fraktur font, a translation of their company's name, \"Frankenmuth Gemeinschafts Versicherinungs Gesellschaft.\"\n\n"}
{"id": "1539290", "url": "https://en.wikipedia.org/wiki?curid=1539290", "title": "Group concept mapping", "text": "Group concept mapping\n\nGroup concept mapping is a structured methodology for organizing the ideas of a group on any topic of interest and representing those ideas visually in a series of interrelated maps. It is a type of integrative mixed method, combining qualitative and quantitative approaches to data collection and analysis. Group concept mapping allows for a collaborative group process with groups of any size, including a broad and diverse array of participants. Since its development in the late 1980s by William M.K. Trochim at Cornell University, it has been applied to various fields and contexts, including community and public health, social work, health care, human services, and biomedical research and evaluation.\n\nGroup concept mapping integrates qualitative group processes with multivariate analysis to help a group organize and visually represent its ideas on any topic of interest through a series of related maps. It combines the ideas of diverse participants to show what the group thinks and values in relation to the specific topic of interest. It is a type of structured conceptualization used by groups to develop a conceptual framework, often to help guide evaluation and planning efforts. Group concept mapping is participatory in nature, allowing participants to have an equal voice and to contribute through various methods. A group concept map visually represents all the ideas of a group and how they relate to each other, and depending on the scale, which ideas are more relevant, important, or feasible.\n\nGroup concept mapping involves a structured multi-step process, including brainstorming, sorting and rating, multidimensional scaling and cluster analysis, and the generation and interpretation of multiple maps. The first step requires participants to brainstorm a large set of statements relevant to the topic of interest, usually in response to a focus prompt. Participants are then asked to individually sort those statements into categories based on their perceived similarity and rate each statement on one or more scales, such as importance or feasibility.\n\nThe data is then analyzed using The Concept System software, which creates a series of interrelated maps using multidimensional scaling (MDS) of the sort data, hierarchical clustering of the MDS coordinates applying Ward's method, and the computation of average ratings for each statement and cluster of statements. The resulting maps display the individual statements in two-dimensional space with more similar statements located closer to each other, and grouped into clusters that partition the space on the map. The Concept System software also creates other maps that show the statements in each cluster rated on one or more scales, and absolute or relative cluster ratings between two cluster sets. As a last step in the process, participants are led through a structured interpretation session to better understand and label all the maps.\n\nGroup concept mapping was developed as a methodology in the late 1980s by William M.K. Trochim at Cornell University. Trochim is considered to be a leading evaluation expert, and he has taught evaluation and research methods at Cornell since 1980. Originally called \"concept mapping\", the methodology has evolved since its inception with the maturation of the field and the continued advancement of the software, which is now a Web application.\n\nGroup concept mapping can be used with any group for any topic of interest. It is often used by government agencies, academic institutions, national associations, not-for-profit and community-based organizations, and private businesses to help turn the ideas of the group into measurable actions. This includes in the areas of organizational development, strategic planning, needs assessment, curriculum development, research, and evaluation. Group concept mapping is well-documented, well-established methodology, and it has been used in hundreds of published papers.\n\nMore generally, concept mapping is any process used for visually representing relationships between ideas in pictures or diagrams. A concept map is typically a diagram of multiple ideas, often represented as boxes or circles, linked in a graph (network) structure through arrows and words where each idea is connected to another. The technique was originally developed in the 1970s by Joseph D. Novak at Cornell University. Concept mapping may be done by an individual or a group.\n\nA mind map is a diagram used to visually represent information, centering on one word or idea with categories and sub-categories radiating off of it in a tree structure. Popularized by Tony Buzan in the 1970s, mind mapping is often a spontaneous exercise done by an individual or group to gather information about what they think around a single topic.\n\nUnlike Novak's concept maps and Buzan's mind maps, \"group concept mapping\" has a structured mathematical process (sorting and rating, multidimensional scaling and cluster analysis) for organizing and visually representing multiple ideas of a group through a series of specific steps. In other words, in group concept mapping, the resulting visual representations are mathematically generated from mixed (qualitative and quantitative) data collected from a group of research subjects, whereas in Novak's concept maps and Buzan's mind maps the visual representations are drawn directly by the subjects resulting in diagrams that are qualitative data and final product at the same time.\n\n\n"}
{"id": "44178", "url": "https://en.wikipedia.org/wiki?curid=44178", "title": "Hanlon's razor", "text": "Hanlon's razor\n\nHanlon's razor is an aphorism expressed in various ways, including:\n\nIt suggests a way of eliminating unlikely explanations (\"attributions\") for human behavior and its consequences. Statements of this kind are known as philosophical razors. It is an eponymous law, probably named after a Robert J. Hanlon.\n\nInspired by Occam's razor, the aphorism was popularized in this form and under this name by the \"Jargon File\", a glossary of computer programmer slang. In 1990, it appeared in the \"Jargon File\" described as a \"'murphyism' parallel to Occam's Razor\".\nLater that same year, the \"Jargon File\" editors noted lack of knowledge about the term's derivation and the existence of a similar epigram by William James. In 1996, the \"Jargon File\" entry on Hanlon's Razor noted the existence of a similar quotation in Robert A. Heinlein's short story \"Logic of Empire\" (1941), with speculation that Hanlon's Razor might be a corruption of \"Heinlein's Razor\". (The character \"Doc\" in Heinlein's story described the \"devil theory\" fallacy, explaining, \"You have attributed conditions to villainy that simply result from stupidity.\")\n\nIn 2001, Quentin Stafford-Fraser published two blog entries citing e-mails from Joseph E. Bigler explaining that the quotation originally came from Robert J. Hanlon of Scranton, Pennsylvania, as a submission (credited in print) for a book compilation of various jokes related to Murphy's law published in Arthur Bloch's \"Murphy's Law Book Two: More Reasons Why Things Go Wrong!\" (1980). Subsequently, in 2002, the \"Jargon File\" entry noted the same.\n\n"}
{"id": "15024", "url": "https://en.wikipedia.org/wiki?curid=15024", "title": "ISO 8601", "text": "ISO 8601\n\nISO 8601 \"Data elements and interchange formats – Information interchange – Representation of dates and times\" is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data are transferred between countries with different conventions for writing numeric dates and times.\n\nIn general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, of times based on the 24-hour timekeeping system (with optional UTC offset), of , and combinations thereof. The standard does not assign any specific meaning to elements of the date/time to be represented; the meaning will depend on the context of its use. In addition, dates and times to be represented cannot include words with no specified numerical meaning in the standard (e.g., names of years in the Chinese calendar) or that do not use characters (e.g., images, sounds).\n\nIn representations for interchange, dates and times are arranged so the largest temporal term (the year) is placed to the left and each successively smaller term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and certain characters (such as \"-\", \":\", \"T\", \"W\", and \"Z\") that are given specific meanings within the standard; the implication is that some commonplace ways of writing parts of dates, such as \"January\" or \"Thursday\", are not allowed in interchange representations.\n\nThe first edition of the ISO 8601 standard was published as \"ISO 8601:1988\" in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition \"ISO 8601:2000\" in 2000 and by the current third edition \"ISO 8601:2004\" published on 2004-12-01. ISO 8601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.\n\nISO 2014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order . The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.\n\nISO 8601 is currently in the process of being updated and split into two parts anticipated to be released in . The draft ISO/DIS 8601-1:2016 represents the slightly updated contents of the current ISO 8601 standard, whereas the draft ISO/DIS 8601-2:2016 defines various extensions such as uncertainties or parts of the Extended Date/Time Format (EDTF).\n\n\nThe standard uses the Gregorian calendar, which serves as an international standard for civil use.\n\nISO 8601 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the (Metre Convention) was signed in Paris. However, ISO calendar dates before the Convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on . Earlier dates, in the proleptic Gregorian calendar, may be used by mutual agreement of the partners exchanging information. The standard states that every date must be consecutive, so usage of the Julian calendar would be contrary to the standard (because at the switchover date, the dates would not be consecutive).\n\nISO 8601 prescribes, as a minimum, a four-digit year [YYYY] to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD. However, years prior to 1583 are not automatically allowed by the standard. Instead \"values in the range [0000] through [1582] shall only be used by mutual agreement of the partners in information interchange.\"\n\nTo represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [±YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or − sign instead of the more common AD/BC (or CE/BCE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled −0001, and so on.\n\nCalendar date representations are in the form shown in the adjacent box. [YYYY] indicates a four-digit year, 0000 through 9999. [MM] indicates a two-digit month of the year, 01 through 12. [DD] indicates a two-digit day of that month, 01 through 31. For example, \"5 April 1981\" may be represented as either in the \"extended format\" or \"19810405\" in the \"basic format\".\n\nThe standard also allows for calendar dates to be written with reduced accuracy. For example, one may write to mean \"1981 April\". The 2000 version allowed writing to mean \"April 5\" but the 2004 version does not allow omitting the year when a month is present. One may simply write \"1981\" to refer to that year or \"19\" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the YYYY-MM-DD and YYYYMMDD formats for complete calendar date representations, if the day [DD] is omitted then only the format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).\n\nWeek date representations are in the formats as shown in the adjacent box. [YYYY] indicates the \"ISO week-numbering year\" which is slightly different from the traditional Gregorian calendar year (see below). [Www] is the \"week number\" prefixed by the letter \"W\", from W01 through W53. [D] is the \"weekday number\", from 1 through 7, beginning with Monday and ending with Sunday.\n\nThere are several mutually equivalent and compatible descriptions of week 01:\n\nAs a consequence, if 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.\n\nThe week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.\n\nThe \"ISO week-numbering year\" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The first ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is ending; if three, they are Monday, Tuesday and Wednesday. Similarly, the last ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is starting; if three, they are Friday, Saturday, and Sunday. The Thursday of each ISO week is always in the Gregorian calendar year denoted by the ISO week-numbering year.\n\nExamples:\n\nAn ordinal date is a simple form for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. As represented above, [YYYY] indicates a year. [DDD] is the day of that year, from 001 through 365 (366 in leap years). For example, is also .\n\nThis format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes referred to as \"Julian Date\", but this can cause confusion with the astronomical Julian day, a sequential count of the number of days since day 0 beginning Greenwich noon, Julian proleptic calendar (or noon on ISO date which uses the Gregorian proleptic calendar with a year [0000]).\n\nISO 8601 uses the 24-hour clock system. The \"basic format\" is [hh][mm][ss] and the \"extended format\" is [hh]:[mm]:[ss].\nSo a time might appear as either \"134730\" in the \"basic format\" or \"13:47:30\" in the \"extended format\".\n\nEither the seconds, or the minutes and seconds, may be omitted from the basic or extended time formats for greater brevity but decreased accuracy: [hh]:[mm], [hh][mm] and [hh] are the resulting reduced accuracy time formats.\n\n\"Midnight\" is a special case and may be referred to as either \"00:00\" or \"24:00\". The notation \"00:00\" is used at the beginning of a calendar day and is the more frequently used. At the end of a day use \"24:00\". \"2007-04-05T24:00\" is the same instant as \"2007-04-06T00:00\" (see \"Combined date and time representations\" below).\n\nDecimal fractions may be added to any of the three time elements. However, a fraction may only be added to the lowest order time element in the representation. A decimal mark, either a comma or a dot (without any preference as stated in resolution 10 of the 22nd General Conference CGPM in 2003, but with a preference for a comma according to ISO 8601:2004) is used as a separator between the time element and its fraction. To denote \"14 hours, 30 and one half minutes\", do not include a seconds figure. Represent it as \"14:30,5\", \"1430,5\", \"14:30.5\", or \"1430.5\". There is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties. For example, in Microsoft SQL Server, the precision of a decimal fraction is 3, i.e., \"yyyy-mm-ddThh:mm:ss[.mmm]\".\n\nTime zones in ISO 8601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.\n\nIf no UTC relation information is given with a time representation, the time is assumed to be in local time. While it \"may\" be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. Even within a single geographic time zone, some local times will be ambiguous if the region observes daylight saving time. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.\n\nIf the time is in UTC, add a \"Z\" directly after the time without a space. \"Z\" is the zone designator for the zero UTC offset. \"09:30 UTC\" is therefore represented as \"09:30Z\" or \"0930Z\". \"14:45:15 UTC\" would be \"14:45:15Z\" or \"144515Z\".\n\nThe \"Z\" suffix in the ISO 8601 time representation is sometimes referred to as \"Zulu time\" because the same letter is used to designate the Zulu time zone. However the ACP 121 standard that defines the list of military time zones makes no mention of UTC and derives the \"Zulu time\" from the Greenwich Mean Time which was formerly used as the international civil time standard. GMT is no longer precisely defined by the scientific community and can refer to either UTC or UT1 depending on context.\n\nThe offset from UTC is appended to the time in the same way that 'Z' was above, in the form ±[hh]:[mm], ±[hh][mm], or ±[hh]. So if the time being described is one hour ahead of UTC (such as the time in Berlin during the winter), the zone designator would be \"+01:00\", \"+0100\", or simply \"+01\". To represent a time behind UTC the offset is negative. For example, the time in New York during standard (not daylight saving) hours is and the zone designator would then be \"−05:00\", \"−0500\", or simply \"−05\". For other time offsets see List of UTC time offsets. To represent a negative offset, ISO 8601 specifies using either a hyphen–minus or a minus sign character. If the interchange character set is limited and does not have a minus sign character, then the hyphen–minus should be used. ASCII does not have a minus sign, so its hyphen–minus character (code is 45 decimal or 2D hexadecimal) would be used. If the character set has a minus sign, then that character should be used. Unicode has a minus sign, and its character code is U+2212 (2212 hexadecimal); the HTML character entity invocation is codice_1.\n\nThe following times all refer to the same moment: \"18:30Z\", \"22:30+04\", \"1130−0700\", and \"15:00−03:30\". Nautical time zone letters are not used with the exception of Z. To calculate UTC time one has to subtract the offset from the local time, e.g. for \"15:00−03:30\" do 15:00 − (−03:30) to get 18:30 UTC.\n\nAn offset of zero, in addition to having the special representation \"Z\", can also be stated numerically as \"+00:00\", \"+0000\", or \"+00\". However, it is not permitted to state it numerically with a negative sign, as \"−00:00\", \"−0000\", or \"−00\". The section dictating sign usage (section 3.4.2 in the 2004 edition of the standard) states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. Contrary to this rule, RFC 3339, which is otherwise a profile of ISO 8601, permits the use of \"-00\", with the same denotation as \"+00\" but a differing connotation.\n\nISO 8601 permits the hyphen (-) to be used as the minus (−) character when the character set is limited. In contrast, RFC 3339 explicitly requires the hyphen (-) symbol to represent negative offsets and does not allow for use of the minus (−) symbol.\n\nA single point in time can be represented by concatenating a complete date expression, the letter \"T\" as a delimiter, and a valid time expression. For example, .\n\nIf a time zone designator is required, it follows the combined date and time. For example, \"2007-04-05T14:30Z\" or \"2007-04-05T12:30-02:00\".\n\nEither basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time may be represented using a specified reduced accuracy format. It is permitted to omit the 'T' character by mutual agreement.\n\nDurations define the amount of intervening time in a time interval and are represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W as shown to the right. In these representations, the [n] is replaced by the value for each of the date and time elements that follow the [n]. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters \"P\", \"Y\", \"M\", \"W\", \"D\", \"T\", \"H\", \"M\", and \"S\" are designators for each of the date and time elements and are not replaced.\n\n\nFor example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds\".\n\nDate and time elements including their designator may be omitted if their value is zero, and lower order elements may also be omitted for reduced precision. For example, \"P23DT23H\" and \"P4Y\" are both acceptable duration representations. However, at least one element must be present, thus \"P\" is not a valid representation for a duration of 0 seconds. \"PT0S\" or \"P0D\", however, are both valid and represent the same duration. \n\nTo resolve ambiguity, \"P1M\" is a one-month duration and \"PT1M\" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in \"P0.5Y\" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in \"P0,5Y\" or \"P0.5Y\". The standard does not prohibit date and time values in a duration representation from exceeding their \"carry over points\" except as noted below. Thus, \"PT36H\" could be used as well as \"P1DT12H\" for representing the same duration. But keep in mind that \"PT36H\" is not the same as \"P1DT12H\" when switching from or to Daylight saving time.\n\nAlternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format . For example, the first duration shown above would be . However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).\n\nAlthough the standard describes durations as part of time intervals, which are discussed in the next section, the duration format is widely used independent of time intervals, as with the Java 8 Duration class.\n\nA time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.\n\nThere are four ways to express a time interval:\n\n\nOf these, the first three require two values separated by an \"interval designator\" which is usually a solidus (more commonly referred to as a forward slash \"/\"). Section 4.4.2 of the standard notes that: \"In certain application areas a double hyphen is used as a separator instead of a solidus.\" The standard does not define the term \"double hyphen\", but previous versions used notations like \"2000--2002\". Use of a double hyphen instead of a solidus allows inclusion in computer filenames. A solidus is a reserved character and not allowed in a filename in common operating systems.\n\nFor <start>/<end> expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be simply shown as \"2007-12-14T13:30/15:30\", where \"/15:30\" implies \"/2007-12-14T15:30\" (the same date as the start), or the beginning and end dates of a monthly billing period as \"2008-02-15/03-14\", where \"/03-14\" implies \"/2008-03-14\" (the same year as the start).\n\nIf greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted can start at any time on and end at any time on , whereas includes the start and end times.\nTo explicitly include all of the start and end dates, the interval would be represented as .\n\nRepeating intervals are specified in clause \"4.5 Recurring time interval\". They are formed by adding \"R[n]/\" to the beginning of an interval expression, where \"R\" is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. If the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of \"P1Y2M10DT2H30M\" five times starting at , use .\n\nISO 8601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used and the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO 8601:2004.\n\nOn the Internet, the World Wide Web Consortium (W3C) uses ISO 8601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software.\n\nRFC 3339 defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.\n\nRFC 3339 deviates from ISO 8601 in allowing a zero time zone offset to be specified as \"-00:00\", which ISO 8601 forbids. RFC 3339 intends \"-00:00\" to carry the connotation that it is not stating a preferred time zone, whereas the conforming \"+00:00\" or any non-zero offset connotes that the offset being used is preferred. This convention regarding \"-00:00\" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO 8601, and so was free to use this convention without conflict.\n\nISO 8601 is referenced by several specifications, but the full range of options of ISO 8601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO 8601.\nThe X.690 encoding standard's GeneralizedTime makes use of another subset of ISO 8601.\n\nThe ISO 8601 week date, as of 2006, appeared in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced to work weeks, and products can be correctly targeted for recall.\n\n\n\nImplementation overview\n"}
{"id": "23624339", "url": "https://en.wikipedia.org/wiki?curid=23624339", "title": "Indexed family", "text": "Indexed family\n\nIn mathematics, an indexed family is informally a collection of objects, each associated with an index from some index set. For example, a \"family of real numbers, indexed by the set of integers\" is a collection of real numbers, where a given function selects for each integer one real number (possibly the same).\n\nMore formally, an indexed family is a mathematical function together with its domain \"I\" and image \"X\". Often the elements of the set \"X\" are referred to as making up the family. In this view indexed families are interpreted as collections instead of as functions. The set \"I\" is called the \"index (set)\" of the family, and \"X\" is the \"indexed set\".\n\nDefinition. Let \"I\" and \"X\" be sets and formula_1 a surjective function, such that\nthen this establishes a family of elements in \"X\" indexed by \"I\" , which is denoted by (\"x\") or simply (\"x\"), when the index set is assumed to be known. Sometimes angle brackets or braces are used instead of parentheses, the latter with the risk of mixing-up families with sets.\n\nAn indexed family can be turned into a set by considering the set formula_3, that is, the image of \"I\" under \"x\". Since the mapping x is not required to be injective, there may exist formula_4 with formula_5 such that formula_6. Thus, formula_7 where |\"A\"| denotes the cardinality of the set \"A\".\n\nThe index set is not restricted to be countable, and, of course, a subset of a powerset may be indexed, resulting in an indexed family of sets. For the important differences in sets and families see below.\n\nWhenever index notation is used the indexed objects form a family. For example, consider the following sentence.\nHere (\"v\") denotes a family of vectors. The \"i\"-th vector \"v\" only makes sense with respect to this family, as sets are unordered and there is no \"i\"-th vector of a set. Furthermore, linear independence is only defined as the property of a collection; it therefore is important if those vectors are linearly independent as a set or as a family. \n\nIf we consider \"n\" = 2 and \"v\" = \"v\" = (1, 0), the \"set\" of them consists of only one element and is linearly independent, but the family contains the same element twice and is linearly dependent.\n\nSuppose a text states the following:\n\nAs in the previous example it is important that the rows of \"A\" are linearly independent as a family, not as a set. For Example, consider the matrix\nThe \"set\" of rows only consists of a single element (1, 1) and is linearly independent, but the matrix is not invertible. The \"family\" of rows contains two elements and is linearly dependent. The statement is therefore correct if it refers to the family of rows, but wrong if it refers to the set of rows. (The statement is also correct when \"the rows\" is interpreted as referring to a multiset, in which the elements are also kept distinct but which lacks some of the structure of an indexed family.)\n\nSurjective functions and families are formally equivalent, as any function \"f\" with domain \"I\" induces a family (\"f\"(\"i\")). In practice, however, a family is viewed as a collection, not as a function: being \"an element of a family\" is equivalent with being in the range of the corresponding function. A family contains any element exactly once, if and only if the corresponding function is injective. \n\nLike a set, a family is a container and any set \"X\" gives rise to a family (\"x\"). Thus any set naturally becomes a family. For any family (\"A\") there is the set of all elements {\"A\" | \"i\"∈\"I\"}, but this does not carry any information on multiple containment or the structure given by \"I\". Hence, by using a set instead of the family, some information might be lost.\n\nLet n be the finite set {1, 2, …, \"n\"}, where \"n\" is a positive integer.\n\nIndex sets are often used in sums and other similar operations. For example, if (\"a\") is a family of numbers, the sum of all those numbers is denoted by\n\nWhen (\"A\") is a family of sets, the union of all those sets is denoted by\n\nLikewise for intersections and cartesian products.\n\nA family (\"B\") is a subfamily of a family (\"A\"), if and only if \"J\" is a subset of \"I\" and for all \"i\" in \"J\" \n\nThe analogous concept in category theory is called a diagram. A diagram is a functor giving rise to an indexed family of objects in a category C, indexed by another category J, and related by morphisms depending on two indices.\n\n\n"}
{"id": "14645842", "url": "https://en.wikipedia.org/wiki?curid=14645842", "title": "International Day to End Violence Against Sex Workers", "text": "International Day to End Violence Against Sex Workers\n\nInternational Day to End Violence Against Sex Workers is observed annually on December 17 by sex workers, their advocates, friends, families and allies. Originally conceived as a memorial and vigil for the victims of the Green River Killer in Seattle Washington, United States (US), it has evolved into an annual international event. The day calls attention to hate crimes committed against sex workers worldwide, as well as the need to remove the social stigma and discrimination that have contributed to violence against sex workers and indifference from the communities they are part of. Sex worker activists also state that custom and prohibitionist laws perpetuate such violence.\n\nFirst observed in 2003, the International Day to End Violence Against Sex Workers was founded by Dr. Annie Sprinkle and the Sex Workers Outreach Project USA (SWOP-USA), an American sex worker rights organization. In a public letter, Sprinkle states:\nViolent crimes against sex workers go underreported, unaddressed and unpunished. There really are people who don't care when prostitutes are victims of hate crimes, beaten, raped, and murdered. No matter what you think about sex workers and the politics surrounding them, sex workers are a part of our neighborhoods, communities and families.\n\nThe red umbrella is an important symbol for sex worker rights and is used for events that are held on December 17. The red umbrella symbol was first used by sex workers in Venice, Italy in 2001. Slovenian artist Tadej Pogacar collaborated with sex workers to create the \"Prostitute Pavilion\" and CODE: RED art installation for the 49th Venice Biennale of Art. Sex workers also held a street demonstration, the Red Umbrellas March, to protest inhumane work conditions and human rights abuses.\n\nThe International Committee on the Rights of Sex Workers in Europe (ICRSE) adopted the red umbrella as a symbol of resistance to discrimination in 2005. A corresponding march was organised as the closing event to the European Conference on Sex Work, Human Rights, Labour and Migration conference, held in Brussels, Belgium, at which almost 200 participants appeared.\n\n\n"}
{"id": "1067848", "url": "https://en.wikipedia.org/wiki?curid=1067848", "title": "International Fellowship of Reconciliation", "text": "International Fellowship of Reconciliation\n\nThe International Fellowship of Reconciliation (IFOR) is a non-governmental organization founded in 1914 in response to the horrors of war in Europe. Today IFOR counts 72 branches, groups and affiliates in 48 countries on all continents. IFOR members promote nonviolence, human rights and reconciliation through public education efforts, training programs and campaigns. The IFOR International Secretariat in Alkmaar, Netherlands facilitates communication among IFOR members, links branches to capacity building resources, provides training in gender-sensitive nonviolence through the Women Peacemakers Program, and helps coordinate international campaigns, delegations and urgent actions. IFOR has ECOSOC status at the United Nations.\n\nThe first body to use the name \"Fellowship of Reconciliation\" was formed as a result of a pact made in August 1914 at the outbreak of the First World War by two Christians, Henry Hodgkin (an English Quaker) and Friedrich Siegmund-Schultze (a German Lutheran), who were participating in a Christian pacifist conference in Konstanz, southern Germany (near Switzerland). On the platform of the railway station at Cologne, they pledged to each other that, \"We are one in Christ and can never be at war\".\n\nTo take that pledge forward, Hodgkin organised a conference in Cambridge in 1915 and founded the \"Fellowship of Reconciliation\" (FOR England). The German branch, Versöhnungsbund, was founded later. It held its first conference in 1932, but in 1933, when Hitler came to power, it dissolved. Schultze was arrested twenty-seven times during World War I and was forced to live in exile during the Nazi period. FOR Germany was officially reestablished just in 1956 with Dr Siegmund Schultze as President. \nShortly after the Cambridge conference, in the autumn of 1915, Henry Hodgkin went over to America and, the 11th and 12 November, the American Fellowship was founded during a Conference at Garden City, Long Island. More than a thousand members enrolled in the American Fellowship before and during the war, which, for the U.S.A., begun on April 6, 1917.\nBecause of the war, it was not possible to travel to other countries and the Fellowship of Reconciliation focused its activities on trying to influence public opinion, to help victims of war and war prisoners. 600 people in England went to prison for helping more than 16.000 imprisoned during the war. When conscription began in Britain in 1916 and in the United States many FOR members refused military service.\n\nAfter the end of the war, in 1919, the different Fellowships of Reconciliation raised in those years all around Europe and in the USA agreed to found the International Fellowship of Reconciliation as an umbrella organisation to which they affiliated as members. In October 1919 Christian pacifists from 10 different countries met in the Netherlands, in the town of Bilthoven, to establish the \"Movement Towards a Christian International\" later called \"International Fellowship of Reconciliation\". IFOR first secretary was the Swiss pacifist Pierre Cérésole jailed several times for his peace witness. He established the Service Civil (International Voluntary Service for Peace), initially organizing work camps in areas torn apart by war, with volunteers from former enemy countries. Relief for the victims of war was carried out, and international conferences and meetings spread the work of peace to many other parts of the globe. Immediately after Bilthoven IFOR appointed travelling secretaries such as John Nevin Sayre, André Trocmé, Muriel Lester, Henri Rose and Percy Bartlett. They travelled carrying the Fellowship's messages around Europe, in Scandinavia, Central Europe, Poland, the Baltic States and the Balkans, giving life to several international conferences that took place between the two world wars. The first one gathered 200 delegates from 20 nations (also India, Burma and Ukraine) in Sonntagberg in Austria. Many others followed and, in such a tense historical moment, IFOR members discussed about the necessity of disarmament and of a new role of Churches, asking clergymen to make a strong stand against the idea of \"righteous wars\". In 1932, the IFOR led a Youth Crusade across Europe in support of the Geneva World Disarmament Conference. Protestants and Catholics from all over converged on Geneva by various routes, reaching over 50,000 people and presenting to the Conference a petition calling for total disarmament among the nations.\n\nAt the end of the 1930s, given the unstable international situation, IFOR established Embassies of Reconciliation that initiated peace efforts not only in Europe but in Japan and China as well. \"Ambassadors of Reconciliation\", such as George Lansbury, Muriel Lester and Anne Seesholtz, visited many world leaders, including Adolf Hitler, Benito Mussolini, Leon Blum and Franklin D Roosevelt. Muriel Lester, English social worker, served as IFOR travelling secretary throughout the world, helping to establish its work in many countries. She met Mahatma Gandhi, first in London when, in 1931, he spent some time at Kingsley Hall, a community center with educational, social and recreational purposes, run by her and her sister Doris, and then in India when she went with him in Bihar on his anti-untouchability tour during 1934. When World War II broke out, travels and communications became almost impossible. In many countries IFOR members suffered persecution for publicly preaching pacifism. IFOR's members, especially in America tried by inter-church mediation to find ways of ending the war, to help coscientious objectors, and struggled against internment of Japanese Americans. In France, IFOR members André and Magda Trocmé, with the help of the villagers of le Chambon sur Lignon, saved the lives of thousands of Jews escaping the Holocaust. In Belgium, feminist Magda Yoors Peeters defends Jewish refugees and conscientious objectors.\n\nAfter the war, travelling secretaries continued their work. IFOR branches and affiliates in Latin America, Asia, Africa, and the Middle East grew consistently also thanks to the work of Jean Goss and Hildegard Goss-Mayr from Paris and Vienna, three times nominated for the Nobel Peace Prize. New Zealand pacifist Ormond Burton represented the IFOR in that nation. From such labors arose Servicio Paz y Justicia (SERPAJ) throughout Latin America. SERPAJ's founder Adolfo Perez Esquivel from Argentina was awarded the Nobel Peace Prize in 1980. SERPAJ participated in the nonviolent resistance to Chile's 16-year-long military dictatorship, which culminates in free elections that restored democracy. Hildegard Goss-Mayrs' training in active nonviolence contributed significantly to the people power overthrow of the Ferdinand Marcos dictatorship in the Philippines in 1986, as well as the growth of nonviolent movements in Asia and Africa. The Goss-Mayrs, IFOR Honorary Presidents, were central to the global spread of active nonviolence movement.\n\nIFOR has 72 members in 48 countries in all continents.\n\nSince the initiation of the United Nations Decade for a Culture of Peace and Nonviolence for the Children of the World, in 2001, IFOR members have been active in working for peace education and in working to establish national coalitions to support the Decade.\n\nIFOR assists groups and individuals to find ways in which they can transform conflicts into positive and growth oriented interactions that involve dialogue and lead to reconciliation. The Nonviolent Education Program aims at supporting sustainable implementation of nonviolence/nonviolent education, peace education and violence prevention in compulsory kindergarten and school education and thus consequent implementation of Children’s Rights expressed in the Convention on the Rights of the Child. This is done through various presentations and training programs, as well as through the creation of resource materials and contact with trainers and resource people.\n\nThanks to the Youth Working Group, IFOR provides young people with the skills and opportunities to become active peacemakers. This is done through nonviolence and leadership training, campaigning, and through internships with IFOR branches and groups, or with the International Secretariat.\n\nReligion has on occasion played a central role in fomenting conflict but can also be a source of inspiration and leadership for peace. IFOR sponsors interfaith delegations to areas of conflict and publishes material on nonviolence from different religious traditions.\n\nSince our founding, IFOR has opposed war and preparations for war. IFOR members support conscientious objectors, campaign for a ban on land mines, and oppose nuclear weapons and all other weapons of mass destruction. During their annual meeting, European members from different branches of the International Fellowship of Reconciliation issued the Declaration of Prali, in occasion of the Global day of Action on Military Spending, 17 April 2012. The Peace and Constitution Committee Working Group promotes actions to raise awareness on the article 9 of the Japanese Constitution which denounces war and war-preparation activities.\n\nIn 2006 IFOR adopted a Gender Policy in which IFOR recognizes that there is a continuum of violence against women that must be confronted, from family violence in the private sphere to armed conflict in the public sphere. Unequal power relations between women and men are one root of violence, conflict and militarization, where women are often severely abused. Gender justice means that women and men can equally contribute to and benefit from peace building, non-violent conflict resolution and reconciliation. This gender policy recognizes that gender equality is an integral part of IFOR’s fundamental values and is a core spiritual value. A transformation of the power relations between women and men is a prerequisite for a culture of peace and non-violence, and must be promoted throughout IFOR. The Women Peacemaker Program (WPP) is an IFOR program that works to ensure the possibility for women's access to peace negotiations and promotes the application of IFOR’s Gender Policy. \n\nThe successful program has been institutionalized by the end of 2012.\n\nIFOR maintains permanent representatives at the United Nations (UN) in New York, Geneva, Vienna and Paris (UNESCO) who regularly participate in conferences and meetings of UN bodies, providing testimony and expertise from different regional perspectives, promoting non-violent alternatives in the fields of human rights, development, and disarmament. \nIFOR has observer and consultative status to the United Nations, United Nations Economic and Social Council (ECOSOC) and UNESCO organizations.\n\n\nSix Nobel Peace Prize recipients are members of IFOR:\n\n\n\n"}
{"id": "41924871", "url": "https://en.wikipedia.org/wiki?curid=41924871", "title": "Jen Lewin", "text": "Jen Lewin\n\nJen Lewin is an interactive artist with a studio based in New York City who specializes in large scale installations in public spaces, usually combining elements such as light, sound and complex engineering. Her interactive light installation \"The Pool\" debuted in 2008 and has been exhibited across the globe, in cities such as Singapore, Sydney, Denver, Montréal and Prague, and in events such as South By Southwest and Burning Man.\n\nLewin has written publications about CAD-related topics. Her work has appeared on \"National Geographic\".\n\nLewin grew up in Maui and obtained her BA in Architecture and Computer Aided Design from the University of Colorado Boulder, later obtaining her M.P.S. degree in Interactive Design from Tisch School of the Arts.\n\nHer most known work, \"The Pool\", featured on \"Wired\", consists of over 200 LED-lit platforms that change colours ...according to pressure and speed changes elicited by viewers' interactions with the system. Termed an \"interactive light sculpture\", where human interaction is necessary for its display, this installation has toured several countries. Specializing in works that involve technology and human interaction, she often includes musical elements into her creations. For instance, other projects or hers include laser harps, where people elicit music via interrupting the laser arrangements.\n\nAccording to the \"Boulder Weekly\", Google commissioned Lewin to \"create an Android app for tracking feedback between her innovations and the groups of people interacting with them\". Lewin, along with her \"Pool\" installation, was featured by the BBC News in May 2013.\n\nJen Lewin was co-founder of The Kitchen restaurants in Boulder, along with Hugo Matheson and Kimbal Musk. She designed its three original locations. Lewin is also the original designer behind the Learning Gardens created for Kimbal Musk and his foundation The Kitchen Community, now called Big Green.\n\nLewin also co-founded The Studio Boulder with business partner William Goodrich.\n\n\n\n\n\n\n2017\n\n2016\n\n2015\n\n2014\n\n2013\n\n2012\n\n"}
{"id": "21271278", "url": "https://en.wikipedia.org/wiki?curid=21271278", "title": "Kyriarchy", "text": "Kyriarchy\n\nKyriarchy, pronounced , is in feminist theory, a social system or set of connecting social systems built around domination, oppression, and submission. The word was coined by Elisabeth Schüssler Fiorenza in 1992 to describe her theory of interconnected, interacting, and self-extending systems of domination and submission, in which a single individual might be oppressed in some relationships and privileged in others. It is an intersectional extension of the idea of patriarchy beyond gender. Kyriarchy encompasses sexism, racism, speciesism, homophobia, classism, economic injustice, colonialism, militarism, ethnocentrism, anthropocentrism, and other forms of dominating hierarchies in which the subordination of one person or group to another is internalized and institutionalized.\n\nThe term was coined by Elisabeth Schüssler Fiorenza in 1992 when she published her book \"But She Said: Feminist Practices of Biblical Interpretation\". It is derived from , \"lord, master\" and , \"to lead, rule, govern\". The word \"kyriarchy\" (, a valid Greek formation, though it is not found in ancient Greek) can now be used to mean \"sovereignty\", i.e. the rulership of a sovereign.\n\nThe term was originally developed in the context of feminist theological discourse, and has been used in some other areas of academia as a non-gender based descriptor of systems of power, as opposed to patriarchy. It is also widely used outside of scholarly contexts.\n\nSchüssler Fiorenza describes interdependent \"stratifications of gender, race, class, religion, heterosexualism, and age\" as \"structural positions\" assigned at birth. She suggests that people inhabit several positions, and that positions with privilege become nodal points through which other positions are experienced. For example, in a context where gender is the primary privileged position (e.g., patriarchy), gender becomes the nodal point through which sexuality, race, and class are experienced. In a context where class is the primary privileged position (i.e., classism), gender and race are experienced through class dynamics. Fiorenza stresses that kyriarchy is not a hierarchical system as it does not focus on one point of domination. Instead it is described as a \"complex pyramidal system\" with those on the bottom of the pyramid experiencing the \"full power of kyriarchal oppression\". The kyriarchy is recognized as the status quo and therefore its oppressive structures may not be recognized.\n\nTo maintain this system, kyriarchy relies on the creation of a servant class, race, gender, or people. The position of this class is reinforced through \"education, socialization, and brute violence and malestream rationalization\". Tēraudkalns suggests that these structures of oppression are self-sustained by internalized oppression; those with relative power tend to remain in power, while those without tend to remain disenfranchised. Structures of oppression also amplify and feed into each other.\n\n"}
{"id": "18477835", "url": "https://en.wikipedia.org/wiki?curid=18477835", "title": "Language intensity", "text": "Language intensity\n\nMost investigators accept the definition of language intensity proposed by John Waite Bowers: a quality of language that \"indicates the degree to which the speaker's attitude toward a concept deviates from neutrality.\" Intensity as a lexical variable in communication studies has generated extensive empirical research.\n\nA theory proposed by Bradac, Bowers, and Courtright (1979, 1980) asserts causal relationships among intensity and a number of other psychological, social, and communication variables. An experimental study by Hamilton, Hunter, and Burgoon (1990) generally supports the relationships proposed by the theory at least in the limited domain of persuasion.\nIntensity has been related to:\n\n"}
{"id": "5341408", "url": "https://en.wikipedia.org/wiki?curid=5341408", "title": "Language politics", "text": "Language politics\n\nLanguage politics is the way language and linguistic differences between peoples are dealt with in the political arena. This could manifest as government recognition, as well as how language is treated in official capacities. Some examples:\n\nLanguage is also utilised in political matters to unify, organise and criticise in order to unify a political group.\n\n"}
{"id": "21788630", "url": "https://en.wikipedia.org/wiki?curid=21788630", "title": "Lazy user model", "text": "Lazy user model\n\nThe lazy user model of solution selection (LUM) is a model in information systems proposed by Tétard and Collan that tries to explain how an individual selects a solution to fulfill a need from a set of possible solution alternatives. LUM expects that a solution is selected from a set of available solutions based on the amount of effort the solutions require from the user – the user is supposed to select the solution that carries the least effort. The model is applicable to a number of different types of situations, but it can be said to be closely related to technology acceptance models.\n\nThe model draws from earlier works on how least effort affects human behaviour in information seeking and in scaling of language.\n\nEarlier research within the discipline of information systems especially within the topic of technology acceptance and technology adoption is closely related to the lazy user model.\n\nThe model starts from the observation that there is a \"user need\", i.e. it is expected that there is a \"clearly definiable, fully satisfiable want\" that the user wants satisfied (it can also be said that the user has a problem that he/she wants solved). So there is a place for a solution, product, or service.\n\nThe user need defines the set of possible solutions (products, services etc.) that fulfill the user need. The basic model considers for simplicity needs that are 100% satisfiable and services that 100% satisfy the needs. This means that only the solutions that solve the problem are relevant. This logically means that the need defines the possible satisfying solutions – a set of solutions (many different products/services) that all can fulfill the user need. LUM is not limited to looking at one solution separately.\n\nAll of the solutions in the set that fulfill the need have their own characteristics; some are good and suitable for the user, others unsuitable and unacceptable – for example, if the user is in a train and wants to know what the result from a tennis match is right now, he/she may only use the types of solutions to the problem that are available to him/her. The \"user state\" determines the set of available/suitable solutions for the user and thus limits the (available) set of possible solutions to fulfill the user need. The user state is a very wide concept, it is the user characteristics at the time of the need. The user state includes, e.g., age, wealth, location ... everything that determines the state of the user in relation to the solutions in the set of the possible solutions to fulfill the user need.\n\nThe model supposes that after the user need has defined the set of possible solutions that fulfill the user need and the user state has limited the set to the available plausible solutions that fulfill the user need the user will \"select\" a solution from the set to fulfill the need. Obviously if the set is empty the user does not have a way to fulfill the need. The lazy user model assumes that the user will make the selection from the limited set based on the lowest level of effort. Effort is understood as the combination of monetary cost + time needed + physical/mental effort needed.\n\nThe lazy user theory has implications when thinking about the effect of learning in technology adoption (for example in the adoption of new information systems).\n\n"}
{"id": "4916038", "url": "https://en.wikipedia.org/wiki?curid=4916038", "title": "Mechanical biological treatment", "text": "Mechanical biological treatment\n\nA mechanical biological treatment system is a type of waste processing facility that combines a sorting facility with a form of biological treatment such as composting or anaerobic digestion. MBT plants are designed to process mixed household waste as well as commercial and industrial wastes.\n\nThe terms \"mechanical biological treatment\" or \"mechanical biological pre-treatment\" relate to a group of solid waste treatment systems. These systems enable the recovery of materials contained within the mixed waste and facilitate the stabilisation of the biodegradable component of the material.\n\nThe sorting component of the plants typically resemble a materials recovery facility. This component is either configured to recover the individual elements of the waste or produce a refuse-derived fuel that can be used for the generation of power.\n\nThe components of the mixed waste stream that can be recovered include:\n\nMBT is also sometimes termed biological mechanical treatment (BMT), however this simply refers to the order of processing (i.e., the biological phase of the system precedes the mechanical sorting). MBT should not be confused with mechanical heat treatment (MHT).\n\nThe \"mechanical\" element is usually an automated mechanical sorting stage. This either removes recyclable elements from a mixed waste stream (such as metals, plastics, glass, and paper) or processes them. It typically involves factory style conveyors, industrial magnets, eddy current separators, trommels, shredders, and other tailor made systems, or the sorting is done manually at hand picking stations. The mechanical element has a number of similarities to a materials recovery facility (MRF).\n\nSome systems integrate a wet MRF to separate by density and flotation and to recover and wash the recyclable elements of the waste in a form that can be sent for recycling. MBT can alternatively process the waste to produce a high calorific fuel termed refuse derived fuel (RDF). RDF can be used in cement kilns or thermal combustion power plants and is generally made up from plastics and biodegradable organic waste. Systems which are configured to produce RDF include the Herhof and Ecodeco processes. It is a common misconception that all MBT processes produce RDF; this is not the case, and depends strictly on system configuration and suitable local markets for MBT outputs.\n\nThe \"biological\" element refers to either:\n\nAnaerobic digestion harnesses anaerobic microorganisms to break down the biodegradable component of the waste to produce biogas and soil improver. The biogas can be used to generate electricity and heat.\n\nBiological can also refer to a composting stage. Here the organic component is broken down by naturally occurring aerobic microorganisms. They breakdown the waste into carbon dioxide and compost. There is no green energy produced by systems employing only composting treatment for the biodegradable waste.\n\nIn the case of biodrying, the waste material undergoes a period of rapid heating through the action of aerobic microbes. During this partial composting stage the heat generated by the microbes result in rapid drying of the waste. These systems are often configured to produce a refuse-derived fuel where a dry, light material is advantageous for later transport and combustion.\n\nSome systems incorporate both anaerobic digestion and composting. This may either take the form of a full anaerobic digestion phase, followed by the maturation (composting) of the digestate. Alternatively a partial anaerobic digestion phase can be induced on water that is percolated through the raw waste, dissolving the readily available sugars, with the remaining material being sent to a windrow composting facility.\n\nBy processing the biodegradable waste either by anaerobic digestion or by composting MBT technologies help to reduce the contribution of greenhouse gases to global warming.\n\nUsable wastes for this system:\n\nPossible products of this system:\n\nFurther advantages:\n\nMBT systems can form an integral part of a region's waste treatment infrastructure. These systems are typically integrated with kerbside collection schemes. In the event that a refuse-derived fuel is produced as a by-product then a combustion facility would be required. This could either be an incineration facility or a gasifier.\n\nAlternatively MBT solutions can diminish the need for home separation and kerbside collection of recyclable elements of waste. This gives the ability of local authorities, municipalities and councils to reduce the use of waste vehicles on the roads and keep recycling rates high.\n\nFriends of the Earth suggests that the best environmental route for residual waste is to firstly maximise removal of remaining recyclable materials from the waste stream (such as metals, plastics and paper). The amount of waste remaining should be composted or anaerobically digested and disposed of to landfill, unless sufficiently clean to be used as compost.\n\nA report by Eunomia undertook a detailed analysis of the climate impacts of different residual waste technologies. It found that an MBT process that extracts both the metals and plastics prior to landfilling is one of the best options for dealing with our residual waste, and has a lower impact than either MBT processes producing RDF for incineration or incineration of waste without MBT.\n\nFriends of the Earth does not support MBT plants that produce refuse derived fuel (RDF), and believes MBT processes should occur in small, localised treatment plants.\n\n\n"}
{"id": "35538957", "url": "https://en.wikipedia.org/wiki?curid=35538957", "title": "Mixture theory", "text": "Mixture theory\n\nMixture theory is used to model multiphase systems using the principles of continuum mechanics generalised to several interpenetrable continua. The basic assumption is that, at any instant of time, all phases are present at every material point, and momentum and mass balance equations are postulated. Like other models, mixture theory requires constitutive relations to close the system of equations. Krzysztof Wilmanski extended the model by introducing a balance equation of porosity.\n"}
{"id": "33060538", "url": "https://en.wikipedia.org/wiki?curid=33060538", "title": "Moral universe", "text": "Moral universe\n\nIn literature, a moral universe is the moral nature of the universe as a whole in relation to human life, or a specific moral code.\n\nA moral universe implies that we live in a basically spiritual universe that is somehow ordered by a higher power, by invisible feelings of good and bad, a 'cosmic order' reminiscent of the early Greeks that underpins and motivates our actions. Or a 'moral force' that means our actions must have definite effects which we carry with us. In this respect its meaning comes close to the Hindu concept of Karma.\n\nThose who reject this idea tend to believe that the universe is just physical, has no spiritual component at all, that events are random and have no deeper meaning or purpose, and that there can be no consequences of any kind to our actions and thus that we live in an amoral or nihilistic universe, as in Nietzsche's \"God is dead,\" aphorism. Such might be the position of \"anti-moral free spirits-nihilists.\"\n\n\n"}
{"id": "101900", "url": "https://en.wikipedia.org/wiki?curid=101900", "title": "Musical instrument classification", "text": "Musical instrument classification\n\nThroughout history, various methods of musical instrument classification have been used. The most commonly used system divides instruments into string instruments, woodwind instruments, brass instruments and percussion instruments; however, other schemes have been devised.\n\nThe oldest known scheme of classifying instruments is Chinese and dates from the 3rd millennium BC. It grouped instruments according to the materials they are made of. Instruments made of stone were in one group, those of wood in another, those of silk are in a third, and those of bamboo in a fourth, as recorded in the \"Yo Chi\" (record of ritual music and dance), compiled from sources of the Chou period (9th-5th centuries BC) and corresponding to the four seasons and four winds.\n\nThe eight-fold system of pa yin (八音 \"eight sounds\", \"octave\"), from the same source, occurred gradually, and in the legendary Emperor Zhun's time (3rd millennium BC) it is believed to have been presented in the following order: metal (\"chin\"), stone (\"shih\"), silk (\"ssu\"), bamboo (\"che\"), gourd (\"piao\"), clay (\"t'u\"), leather (\"hoes\"), and wood (\"mutt\") classes, and it correlated to the eight seasons and eight winds of Chinese culture, autumn and west, autumn-winter and NW, summer and south, spring and east, winter-spring and NE, summer-autumn and SW, winter and north, and spring-summer and SE, respectively.\n\nHowever, the Chou-Li (Rites of Chou), an anonymous treatise compiled from earlier sources in about the 2nd century BC, had the following order: metal, stone, clay, leather, silk, wood, gourd, and bamboo. The same order was presented in the Tso Chuan (Commentary of Tso), attributed to Tso Chiu-Ming, probably compiled in the 4th century BC.\n\nMuch later, Ming dynasty (14th-17th century) scholar Chu Tsai Yu recognized three groups: those instruments using muscle power or used for musical accompaniment, those that are blown, and those that are rhythmic, a scheme which was probably the first scholarly attempt, while the earlier ones were traditional, folk taxonomies.\n\nMore usually, instruments are classified according to how the sound is initially produced (regardless of post-processing, i.e., an electric guitar is still a string-instrument regardless of what analog or digital/computational post-processing effects pedals may be used with it).\n\nThe modern system divides instruments into wind, strings and percussion. It is of Greek origin (in the Hellenistic period, prominent proponents being Nicomachus and Porphyry). The scheme was later expanded by Martin Agricola, who distinguished plucked string instruments, such as guitars, from bowed string instruments, such as violins. Classical musicians today do not always maintain this division (although plucked strings are grouped separately from bowed strings in sheet music), but distinguish between wind instruments with a reed (woodwinds) and those where the air is set in motion directly by the lips (brass instruments).\n\nMany instruments do not fit very neatly into this scheme. The serpent, for example, ought to be classified as a brass instrument, as a column of air is set in motion by the lips. However, it looks more like a woodwind instrument, and is closer to one in many ways, having finger-holes to control pitch, rather than valves.\n\nKeyboard instruments do not fit easily into this scheme. For example, the piano has strings, but they are struck by hammers, so it is not clear whether it should be classified as a string instrument or a percussion instrument. For this reason, keyboard instruments are often regarded as inhabiting a category of their own, including all instruments played by a keyboard, whether they have struck strings (like the piano), plucked strings (like the harpsichord) or no strings at all (like the celesta).\n\nIt might be said that with these extra categories, the classical system of instrument classification focuses less on the fundamental way in which instruments produce sound, and more on the technique required to play them.\n\nVarious names have been assigned to these three traditional Western groupings:\n\nAn ancient system of Indian origin, dating from the 4th or 3rd century BC, in the Natya Shastra, a theoretical treatise on music and dramaturgy, by Bharata Muni, divides instruments into four main classification groups: instruments where the sound is produced by vibrating strings (\"tata vadya\", \"stretched instruments\"); instruments where the sound is produced by vibrating columns of air (\"susira vadya\", \"hollow instruments\"); percussion instruments made of wood or metal (\"Ghana vadya\", \"solid instruments\"); and percussion instruments with skin heads, or drums (\"avanaddha vadya\", \"covered instruments\").\n\nVictor-Charles Mahillon later adopted a system very similar to this. He was the curator of the musical instrument collection of the conservatoire in Brussels, and for the 1888 catalogue of the collection divided instruments into four groups: strings, winds, drums, and other percussion. This scheme was later taken up by Erich von Hornbostel and Curt Sachs who published an extensive new scheme for classication in \"Zeitschrift für Ethnologie\" in 1914. Their scheme is widely used today, and is most often known as the Hornbostel–Sachs system (or the Sachs–Hornbostel system).\n\nThe original Sachs–Hornbostel system classified instruments into four main groups; the fifth group, electrophones, was added later by Sachs:\n\nIn the Hornbostel–Sachs classification of musical instruments, lamellophones are considered plucked idiophones, a category that includes various forms of jaw harp and the European mechanical music box, as well as the huge variety of African and Afro-Latin thumb pianos such as the mbira and marimbula.\n\nLater Sachs added a fifth category, electrophones, such as theremins, which produce sound by electronic means. Modern synthesizers and electronic instruments fall in this category. Within each category are many subgroups. The system has been criticised and revised over the years, but remains widely used by ethnomusicologists and organologists.\n\nOne notable example of this criticism is that there is care to be taken with electrophones, as some electronic instruments like the electric guitar (chordophone) and some electronic keyboards (sometimes idiophones or chordophones) can produce music without electricity or the use of an amplifier.\n\nIn 1932, comparative musicologist (ethnomusicologist) André Schaeffner developed a new classification scheme that was \"exhaustive, potentially covering all real and conceivable instruments\".\n\nSchaeffner's system has only two top-level categories which he denoted by Roman numerals:\n\nThe system agrees with Mahillon and Hornbostel–Sachs for chordophones, but groups percussion instruments differently.\n\n2nd-century Greek grammarian, sophist, and rhetoritician Julius Pollux, in the chapter called De Musica of his ten-volume \"Onomastikon\", presented the two-class system, percussion (including strings) and winds, which persisted in medieval and postmedieval Europe. It was used by St. Augustine (4th and 5th centuries), in his De Ordine, applying the terms rhythmic (percussion and strings), organic (winds), and adding harmonic (the human voice); Isidore of Seville (6th to 7th centuries); Hugh of Saint Victor (12th century), also adding the voice; Magister Lambertus (13th century), adding the human voice as well; and Michael Praetorius (17th century).\n\nThe Kpelle of West Africa also use this system. They distinguish the struck (\"yàle\"), including both beaten and plucked, and the blown (\"fêe\"). The \"yàle\" group is subdivided into five categories: instruments possessing lamellas (the sanzas); those possessing strings; those possessing a membrane (various drums); hollow wooden, iron, or bottle containers; and various rattles and bells. The Hausa, also of West Africa, classify drummers into those who beat drums and those who beat (pluck) strings (the other four player classes are blowers, singers, acclaimers, and talkers), Kartomi does not specify if these two classifications pre-date Schaeffner or Pollux. The concept, the way the person produces the sound, is human-centered, which is part of their traditional culture so presumably they at least pre-date Schaeffner.\n\nThe MSA (Multi-Dimensional Scalogram Analysis) of René Lysloff and Jim Matson, using 37 variables, including characteristics of the sounding body, resonator, substructure, sympathetic vibrator, performance context, social context, and instrument tuning and construction, corroborated Schaeffner, producing two categories, aerophones and the chordophone-membranophone-idiophone combination.\n\nAnother similar system is the five-class, physics-based organology that was presented by Steve Mann in 2007, comprises Gaiaphones (Chordophones, Membranophones, and Idiophones), Hydraulophones, Aerophones, Plasmaphones, and Quintephones (electrically and optically produced music), the names referring to the five essences, earth, water, wind, fire and the quintessence, thus adding three new categories to the Schaeffner taxonomy.\nElementary organology, also known as physical organology, is a classification scheme based on the elements (i.e. states of matter) in which sound production takes place. \"Elementary\" refers both to \"element\" (state of matter) and to something that is fundamental or innate (physical). The elementary organology map can be traced to Kartomi, Schaeffner, Yamaguchi, and others, as well as to the Greek and Roman concepts of elementary classification of all objects, not just musical instruments.\n\nElementary organology categorizes musical instruments by their classical element, i.e.\n\nInstruments can be classified by their musical range in comparison with other instruments in the same family. These terms are named after singing voice classifications:\n\nSome instruments fall into more than one category: for example, the cello may be considered either tenor or bass, depending on how its music fits into the ensemble, and the trombone may be alto, tenor, or bass and the French horn, bass, baritone, tenor, or alto, depending on which range it is played. In a typical concert band setting, the first alto saxophone covers soprano parts, while the second alto saxophone covers alto parts.\n\nMany instruments include their range as part of their name: soprano saxophone, alto saxophone, tenor saxophone, baritone saxophone, baritone horn, alto flute, bass flute, alto recorder, bass guitar, etc. Additional adjectives describe instruments above the soprano range or below the bass, for example: sopranino saxophone, contrabass clarinet.\n\nWhen used in the name of an instrument, these terms are relative, describing the instrument's range in comparison to other instruments of its family and not in comparison to the human voice range or instruments of other families. For example, a bass flute's range is from C to F, while a bass clarinet plays about one octave lower.\n\nInstruments can be categorized according to a common use, such as signal instruments, a category that may include instruments in different Hornbostel–Sachs categories such as trumpets, drums, and gongs. An example based on this criterion is Bonanni (e.g., festive, military, and religious). He separately classified them according to geography and era.\n\nJean-Benjamin de la Borde (1780) classified instruments according to ethnicity, his categories being black, Abyssinian, Chinese, Arabic, Turkisk, and Greek.\n\nInstruments can be classified according to the ensemble in which they play, or the role they play in the ensemble. For example, the horn section in popular music typically includes both brass instruments and woodwind instruments. The symphony orchestra typically has the strings in the front, the woodwinds in the middle, and the basses, brass, and percussion in the back.\n\nClassifications done for the Indonesian ensemble, the gamelan, were done by Jaap Kunst (1949), Martopangrawit, Poerbapangrawit, and Sumarsam (all in 1984). Kunst described five categories: nuclear theme (\"cantus firmus\" in Latin and \"balungan\" (\"skeletal ramework\") in Indonesian); colotomic ( a word invented by Kunst) (interpunctuating), the gongs; countermelodic; paraphrasing (\"panerusan\"), subdivided as close to the nuclear theme and ornamental filling; agogic (tempo-regulating), drums.\n\nR. Ng. Martopangrawit has two categories, irama (the rhythm instruments) and lagu (the melodic instruments), the former corresponds to Kunst's classes 2 and 5, and the latter to Kunst's 1, 3, and 4.\n\nKodrat Poerbapangrawit, similar to Kunst, derives six categories: \"balungan\", the \"saron\", \"demung\", and \"slenthem\"; \"rerenggan\" (ornamental), the \"gendèr\", \"gambang\", and \"bonang\"); \"wiletan\" (variable formulaic melodic), \"rebab\" and male chorus (\"gerong\"); \"singgetan\" (interpunctuating); \"kembang\" (floral), flute and female voice; jejeging wirama (tempo regulating), drums.\n\nSumarsam's scheme comprises:\n\nThe gamelan is also divided into front, middle, and back, much like the symphony orchestra.\n\nAn orally-transmitted Javanese taxonomy has 8 groupings:\n\nA Javanese classification transmitted in literary form is as follows:\n\nThis is much like the pa yin. It is suspected of being old but its age is unknown.\n\nMinangkabau musicians (of West Sumatra) use the following taxonomy for \"bunyi-bunyian\" (\"objects that sound\"): \"dipukua\" (\"beaten\"), \"dipupuik\" (\"blown), \"dipatiek\" (\"plucked\"), \"ditariek\" (\"pulled\"), \"digesek\" (\"bowed\"), \"dipusiang\" (\"swung\"). The last one is for the bull-roarer. They also distinguish instruments on the basis of origin because of sociohistorical contacts, and recognize three categories: Mindangkabau (\"Minangkabau asli\"), Arabic (\"asal Arab\"), and Western (\"asal Barat\"), each of these divided up according to the five categories. Classifying musical instruments on the basis sociohistorical factors as well as mode of sound production is common in Indonesia.\n\nThe Batak of North Sumatra recognize the following classes: beaten (\"alat pukul\" or \"alat palu\"), blown (\"alat tiup\"), bowed (\"alat gesek\"), and plucked (\"alat petik\") instruments, but their primary classification is of ensembles.\n\nIn West Africa, tribes such as the Dan, Gio, Kpelle, Hausa, Akan, and Dogon, use a human-centered system. It derives from 4 myth-based parameters: the musical instrument's nonhuman owner (spirit, mask, sorcerer, or animal), the mode of transmission to the human realm (by gift, exchange, contract, or removal), the making of the instrument by a human (according to instructions from a nonhuman, for instance), and the first human owner. Most instruments are said to have a nonhuman origin, but some are believed invented by humans, e.g., the xylophone and the lamellophone.\n\nIn 1960, German musicologist Kurt Reinhard presented a stylistic taxonomy, as opposed to a morphological one, with two divisions determined by either single or multiple voiced playing. Each of these two divisions was subdivided according to pitch changeability (not changeable, freely changeable, and changeable by fixed intervals), and also by tonal continuity (discontinuous (as the marimba and drums) and continuous (the friction instruments (including bowed) and the winds), making 12 categories. He also proposed classification according to whether or not they had dynamic tonal variability, a characteristic that separates whole eras (e.g., the baroque from the classical) as in the transition from the terraced dynamics of the harpsichord to the crescendo of the piano, grading by degree of absolute loudness, timbral spectra, tunability, and degree of resonance.\n\nAl-Farabi, Persian scholar of the 10th century, also distinguished tonal duration. In one of his four schemes, in his two-volume \"Kitab al-Musiki al-Kabir\" (\"Great Book of Music\") he identified five classes, in order of ranking, as follows: the human voice, the bowed strings (the \"rebab\") and winds, plucked strings, percussion, and dance, the first three pointed out as having continuous tone.\n\nIbn Sina, Persian scholar of the 11th century, presented a scheme in his \"Kitab al-Najat\" (Book of the Delivery), made the same distinction. He used two classes. In his \"Kitab al-Shifa\" (Book of Soul Healing), he proposed another taxonomy, of five classes: fretted instruments, unfretted (open) stringed, lyres and harps, bowed stringed, wind (reeds and some other woodwinds, such as the flute and bagpipe), other wind instruments such as the organ, and the stick-struck santur (a board zither). The distinction between fretted and open was in classic Persian fashion.\n\n"}
{"id": "512852", "url": "https://en.wikipedia.org/wiki?curid=512852", "title": "No Child Left Behind Act", "text": "No Child Left Behind Act\n\nThe No Child Left Behind Act of 2001 (NCLB) was a U.S. Act of Congress that reauthorized the Elementary and Secondary Education Act; it included Title I provisions applying to disadvantaged students. It supported standards-based education reform based on the premise that setting high standards and establishing measurable goals could improve individual outcomes in education. The Act required states to develop assessments in basic skills. To receive federal school funding, states had to give these assessments to all students at select grade levels.\n\nThe act did not assert a national achievement standard—each state developed its own standards. NCLB expanded the federal role in public education through further emphasis on annual testing, annual academic progress, report cards, and teacher qualifications, as well as significant changes in funding.\n\nThe bill passed in the Congress with bipartisan support. By 2015, criticism from right, left, and center had accumulated so much that a bipartisan Congress stripped away the national features of No Child Left Behind. Its replacement, the Every Student Succeeds Act, turned the remnants over to the states.\n\nIt was coauthored by Representatives John Boehner (R-OH), George Miller (D-CA), and Senators Edward Kennedy (D-MA) and Judd Gregg (R-NH). The United States House of Representatives passed the bill on December 13, 2001 (voting 381–41), and the United States Senate passed it on December 18, 2001 (voting 87–10). President Bush signed it into law on January 8, 2002.\n\nNo Child Left Behind requires all public schools receiving federal funding to administer a statewide standardized test annually to all students. Schools that receive Title I funding through the Elementary and Secondary Education Act of 1965 must make Adequate Yearly Progress (AYP) in test scores (e.g. each year, fifth graders must do better on standardized tests than the previous year's fifth graders).\n\nIf the school's results are repeatedly poor, then steps are taken to improve the school.\n\nStates must create AYP objectives consistent with the following requirements of the law:\n\nThe act requires states to provide \"highly qualified\" teachers to all students. Each state sets its own standards for what counts as \"highly qualified.\" Similarly, the act requires states to set \"one high, challenging standard\" for its students. Each state decides for itself what counts as \"one high, challenging standard,\" but the curriculum standards must be applied to all students, rather than having different standards for students in different cities or other parts of the state.\n\nThe act also requires schools to let military recruiters have students' contact information and other access to the student, if the school provides that information to universities or employers, unless the students opt out of giving military recruiters access. This portion of the law has drawn lots of criticism and has even led to political resistance. For instance, in 2003 in Santa Cruz, California, student-led efforts forced school districts to create an \"opt-in\" policy that required students affirm they wanted the military to have their information. This successful student organizing effort was copied in various other cities throughout the United States.\n\nSupporters of the NCLB claim one of the strong positive points of the bill is the increased accountability that is required of schools and teachers. According to the legislation, schools must pass yearly tests that judge student improvement over the fiscal year. These yearly standardized tests are the main means of determining whether schools live up to required standards. If required improvements are not made, the schools face decreased funding and other punishments that contribute to the increased accountability. According to supporters, these goals help teachers and schools realize the significance and importance of the educational system and how it affects the nation. Opponents of this law say that the punishments only hurt the schools and do not contribute to the improvement of student education.\n\nIn addition to and in support of the above points, proponents claim that No Child Left Behind:\n\nThe commonwealth of Pennsylvania has proposed tying teacher's salaries to test scores. If a district's students do poorly, the state cuts the district's budget the following year and the teachers get a pay cut. Critics point out that if a school does poorly, reducing its budget and cutting teacher salaries will likely hamper the school's ability to improve.\n\n\nThe act requires schools to rely on scientifically based research for programs and teaching methods. The act defines this as \"research that involves the application of rigorous, systematic, and objective procedures to obtain reliable and valid knowledge relevant to education activities and programs.\" Scientifically based research results in \"replicable and applicable findings\" from research that used appropriate methods to generate persuasive, empirical conclusions.\n\nPrior to the NCLB act, new teachers were typically required to have a bachelor's degree, be fully certified, and demonstrate subject matter knowledge—generally through tests. It is widely accepted that teacher knowledge has two components: specific subject matter knowledge (CK) such as an understanding of mathematics for a mathematics teacher, and pedagogical knowledge (PCK), which is knowledge of the subject of teaching/learning itself. Both types of knowledge, as well as experience in guided student teaching, help form the qualities needed by effective teachers.\n\nUnder NCLB, existing teachers—including those with tenure—were also supposed to meet standards. They could meet the same requirements set for new teachers or could meet a state-determined \"...high, objective, uniform state standard of evaluation,\" aka HOUSSE. Downfall of the quality requirements of the NCLB legislation have received little research attention, in part because state rules require few changes from pre-existing practice. There is also little evidence that the rules have altered trends in observable teacher traits. For years, American educators have been struggling to identify those teacher traits that are important contributors to student achievement. Unfortunately, there is no consensus on what traits are most important and most education policy experts agree that further research is required.\n\nSeveral analyses of state accountability systems that were in place before NCLB indicate that outcomes accountability led to faster growth in achievement for the states that introduced such systems. The direct analysis of state test scores before and after enactment of NCLB also supports its positive impact. A primary criticism asserts that NCLB reduces effective instruction and student learning by causing states to lower achievement goals and motivate teachers to \"teach to the test.\" A primary supportive claim asserts that systematic testing provides data that shed light on which schools don't teach basic skills effectively, so that interventions can be made to improve outcomes for all students while reducing the achievement gap for disadvantaged and disabled students.\n\nThe Department of Education points to the National Assessment of Educational Progress (NAEP) results, released in July 2005, showing improved student achievement in reading and math:\n\nThese statistics compare 2005 with 2000 though No Child Left Behind did not even take effect until 2003. Critics point out that the increase in scores between 2000 and 2005 was roughly the same as the increase between 2003 and 2005, which calls into question how any increase can be attributed to No Child Left Behind. They also argue that some of the subgroups are cherry-picked—that in other subgroups scores remained the same or fell. Also, the makers of the standardized tests have been blamed for making the tests easier so that it is easier for schools to sufficiently improve.\n\nEducation researchers Thomas Dee and Brian Jacob argue that NCLB showed statistically significant positive impact on students' performance on 4th-grade math exams (equal to two-thirds of a year's worth of growth), smaller and statistically insignificant improvements in 8th-grade math exam performance, and no discernible improvement in reading performance.\n\nCritics argue that the focus on standardized testing (all students in a state take the same test under the same conditions) encourages teachers to teach a narrow subset of skills that the school believes increases test performance, rather than achieve in-depth understanding of the overall curriculum. For example, a teacher who knows that all questions on a math test are simple addition problems (e.g., What is 2 + 3?) might not invest any class time on the practical applications of addition, to leave more time for the material the test assesses. This is colloquially referred to as \"teaching to the test.\" \"Teaching to the test\" has been observed to raise test scores, though not as much as other teaching techniques.\n\nMany teachers who practice \"teaching to the test\" misinterpret the educational outcomes the tests are designed to measure. On two state tests, New York and Michigan, and the National Assessment of Educational Progress (NAEP) almost two-thirds of eighth graders missed math word problems that required an application of the Pythagorean theorem to calculate the distance between two points. The teachers correctly anticipated the content of the tests, but incorrectly assumed each test would present simplistic items rather than higher-order items.\n\nAnother problem is that outside influences often affect student performance. Students who struggle to take tests may perform well using another method of learning such as project-based learning. Sometimes, factors such as home life can affect test performance. Basing performance on one test inaccurately measures student success overall. No Child Left behind has failed to account for all these factors. \n\nThose opposed to the use of testing to determine educational achievement prefer alternatives such as subjective teacher opinions, classwork, and performance-based assessments.\n\nUnder No Child Left Behind, schools were held almost exclusively accountable for absolute levels of student performance. But that meant that even schools that were making great strides with students were still labeled as \"failing\" just because the students had not yet made it all the way to a \"proficient\" level of achievement. Since 2005, the U.S. Department of Education has approved 15 states to implement growth model pilots. Each state adopted one of four distinct growth models: Trajectory, Transition Tables, Student Growth Percentiles, and Projection.\n\nThe incentives for improvement also may cause states to lower their official standards. Because each state can produce its own standardized tests, a state can make its statewide tests easier to increase scores. Missouri, for example, improved testing scores but openly admitted that they lowered the standards. A 2007 study by the U.S. Dept. of Education indicates that the observed differences in states' reported scores is largely due to differences in the stringency of their standards.\n\nMany argue that local government had failed students, necessitating federal intervention to remedy issues like teachers teaching outside their areas of expertise, and complacency in the face of continually failing schools. Some local governments, notably that of New York state, have supported NCLB provisions, because local standards failed to provide adequate oversight over special education, and NCLB would let them use longitudinal data more effectively to monitor Adequate Yearly Progress (AYP). States all over the United States have shown improvements in their progress as an apparent result of NCLB. For example, Wisconsin ranks first of all fifty states plus the District of Columbia, with ninety-eight percent of its schools achieving No Child Left Behind standards.\n\nStudent performance in other subjects (besides reading and math) will be measured as a part of overall progress.\n\nNCLB’s main focus is on skills in reading, writing, and mathematics, which are areas related to economic success. Combined with the budget crises in the late-2000s recession, some schools have cut or eliminated classes and resources for many subject areas that are not part of NCLB's accountability standards. Since 2007, almost 71% of schools have reduced instruction time in subjects such as history, arts, language, and music to provide more time and resources to mathematics and English.\n\nIn some schools, the classes remain available, but individual students who are not proficient in basic skills are sent to remedial reading or mathematics classes rather than arts, sports, or other optional subjects.\n\nAccording to Paul Reville, the author of \"Stop Narrowing of the Curriculum By Right-Sizing School Time,\" teachers are learning that students need more time to excel in the \"needed\" subjects. The students need more time to achieve the basic goals that should come by somewhat relevant to a student.\n\nPhysical Education, on the other hand, is one of the subjects least affected. Some might find this confusing because like many electives and non-core classes, No Child Left Behind does not address Physical Education directly. Two reasons why Physical Education is not adversely affected include the obesity crisis in the United States that the federal government is trying to reverse through programs like First Lady Michelle Obama's \"Let's Move Campaign,\" which among other things, looks to improve the quantity and quality of physical education. Secondly, there is research, including a 2005 study by Dr. Charles H. Hillmam of The University of Illinois at Urbana-Champaign that concludes that fitness is globally related to academic achievement.\n\nThe opportunities, challenges, and risks that No Child Left Behind poses for science education in elementary and middle schools—worldwide competition insists on rapidly improving science education. Adding science assessments to the NCLB requirements may ultimately result in science being taught in more elementary schools and by more teachers than ever before. 2/3 of elementary school teachers indicated that they were not familiar with national science standards. Most concern circulates around the result that, consuming too much time for language arts and mathematics may limit children's experience—and curiosity and interest—in sciences.\n\nBoth U.S. conservative and liberal critics have argued that NCLB's new standards in federalizing education set a negative precedent for further erosion of state and local control. Libertarians further argue that the federal government has no constitutional authority in education, which is why participation in NCLB is technically optional. They believe that states need not comply with NCLB so long as they forgo the federal funding that comes with it.\n\nNCLB pressures schools to guarantee that nearly all students meet the minimum skill levels (set by each state) in reading, writing, and arithmetic—but requires nothing beyond these minima. It provides no incentives to improve student achievement beyond the bare minimum. Programs not essential for achieving mandated minimum skills are neglected or canceled by those districts.\n\nIn particular, NCLB does not require any programs for gifted, talented, and other high-performing students. Federal funding of gifted education decreased by a third over the law's first five years. There was only one program that helped improve the gifted: they received $9.6 million. In the 2007 budget, President George W. Bush zeroed this out. While NCLB is silent on the education of academically gifted students, some states (such as Arizona, California, Virginia, and Pennsylvania) require schools to identify gifted students and provide them with an appropriate education, including grade advancement. Research tells us an IQ of 120 is needed. In other states, such as Michigan, state funding for gifted and talented programs was cut by up to 90% in the year after the Act became law.\n\n\"There's a fallacy in the law and everybody knows it,\" said Alabama State Superintendent Joe Morton on Wednesday, August 11, 2010. According to the No Child Left Behind Act, by 2014, every child is supposed to test on grade level in reading and math. \"That can't happen,\" said Morton. \"You have too many variables and you have too many scenarios, and everybody knows that would never happen.\" Alabama State Board Member Mary Jane Caylor said, \"I don't think that No Child Left Behind has benefited this state.\" She argued the goal of 100 percent proficiency is unattainable. Charles Murray wrote of the law: \"The United States Congress, acting with large bipartisan majorities, at the urging of the President, enacted as the law of the land that all children are to be above average.\"\n\nThe system of incentives and penalties sets up a strong motivation for schools, districts, and states to manipulate test results. For example, schools have been shown to employ \"creative reclassification\" of high school dropouts (to reduce unfavorable statistics). For example, at Sharpstown High School in Houston, Texas, more than 1,000 students began high school as freshmen, and four years later, fewer than 300 students were enrolled in the senior class. However, none of these \"missing\" students from Sharpstown High were reported as dropouts.\n\nThe act is promoted as requiring 100% of students (including disadvantaged and special education students) within a school to reach the same state standards in reading and mathematics by 2014; detractors charge that a 100% goal is unattainable, and critics of the NCLB requirement for \"one high, challenging standard\" claim that some students are simply unable to perform at the given level for their age, no matter how effective the teacher is. While statewide standards reduce the educational inequality between privileged and underprivileged districts in a state, they still impose a \"one size fits all\" standard on individual students. Particularly in states with high standards, schools can be punished for not being able to dramatically raise the achievement of students that may have below-average capabilities.\n\nIn fact, the \"all\" in NCLB means only 95% of students, because states must report the assessment scores of 95% of students when calculating Adequate Yearly Progress (AYP) scores. Students who have an Individual Education Plan (IEP) and who are assessed must receive the accommodations specified in the IEP during assessment; if these accommodations do not change the nature of the assessment, then these students' scores are counted the same as any other student's score. Common acceptable changes include extended test time, testing in a quieter room, translation of math problems into the student's native language, or allowing a student to type answers instead of writing them by hand.\n\nSimply being classified as having special education needs does not automatically exempt students from assessment. Most students with mild disabilities or physical disabilities take the same test as non-disabled students.\n\nIn addition to not requiring 5% of students to be assessed at all, regulations let schools use alternate assessments to declare up to 1% of all students proficient for the purposes of the Act. States are given broad discretion in selecting alternate assessments. For example, a school may accept an Advanced Placement test for English in lieu of the English test written by the state, and simplified tests for students with significant cognitive disabilities. The Virginia Alternate Assessment Program (VAAP) and Virginia Grade Level Alternative (VGLA) options, for example, are portfolio assessments.\n\nOrganizations that support NCLB assessment of disabled or limited English proficient (LEP) students say that inclusion ensures that deficiencies in the education of these disadvantaged students are identified and addressed. Opponents say that testing students with disabilities violates the Individuals with Disabilities Education Act (IDEA) by making students with disabilities learn the same material as non-disabled students.\n\nNCLB includes incentives to reward schools showing progress for students with disabilities and other measures to fix or provide students with alternative options than schools not meeting the needs of the disabled population. The law is written so that the scores of students with IEPs (Individualized Education Plans) and 504 plans are counted just as other students' scores are counted. Schools have argued against having disabled populations involved in their AYP measurements because they claim that there are too many variables involved.\n\nStemming from the Education for All Handicapped Children Act (EAHCA) of 1975, the Individuals with Disabilities Education Act (IDEA) was enacted in its first form in 1997, and then reenacted with new education aspects in 2006 (although still referred to as IDEA 2004). It kept the EAHCA requirements of free and accessible education for all children. The 2004 IDEA authorized formula grants to states and discretionary grants for research, technology, and training. It also required schools to use research-based interventions to assist students with disabilities.\n\nThe amount of funding each school would receive from its \"Local Education Agency\" for each year would be divided by the number of children with disabilities and multiplied by the number of students with disabilities participating in the schoolwide programs.\n\nParticularly since 2004, policymakers have sought to align IDEA with NCLB. The most obvious points of alignment include the shared requirements for Highly Qualified Teachers, for establishment of goals for students with special needs, and for assessment levels for these students. In 2004, George Bush signed provisions that would define for both of these acts what was considered a \"highly qualified teacher.\"\n\nThe National Council on Disability (NCD) looks at how NCLB and IDEA are improving outcomes for students with Down syndrome. The effects they investigate include reducing the number of students who drop out, increasing graduation rates, and effective strategies to transition students to post-secondary education. Their studies have reported that NCLB and IDEA have changed the attitudes and expectations for students with disabilities. They are pleased that students are finally included in state assessment and accountability systems. NCLB made assessments be taken \"seriously,\" they found, as now assessments and accommodations are under review by administrators.\n\nAnother organization that found positive correlations between NCLB and IDEA was the National Center on Educational Outcomes. It published a brochure for parents of students with disabilities about how the two (NCLB & IDEA) work well together because they \"provide both individualized instruction and school accountability for students and disabilities.\" They specifically highlight the new focus on \"shared responsibility of general and special education teachers,\" forcing schools to have disabled students more on their radar.\" They do acknowledge, however, that for each student to \"participate in the general curriculum [of high standards for all students] and make progress toward proficiency,\" additional time and effort for coordination are needed. The National Center on Educational Outcomes reported that now disabled students will receive \"...the academic attention and resources they deserved.\"\n\nParticular research has been done on how the laws impact students who are deaf or hard of hearing. First, the legislation makes schools responsible for how students with disabilities score—emphasizing \"...student outcomes instead of placement.\" It also puts the public’s eye on how outside programs can be utilized to improve outcomes for this underserved population, and has thus prompted more research on the effectiveness of certain in- and out-of-school interventions. For example, NCLB requirements have made researchers begin to study the effects of read aloud or interpreters on both reading and mathematics assessments, and on having students sign responses that are then recorded by a scribe.\n\nStill, research thus far on the positive effects of NCLB/IDEA is limited. It has been aimed at young students in an attempt to find strategies to help them learn to read. Evaluations also have included a limited number of students, which make it very difficult to draw conclusions to a broader group. Evaluations also focus only on one type of disabilities.\n\nThe National Council for Disabilities had reservations about how the regulations of NCLB fit with those of IDEA. One concern is how schools can effectively intervene and develop strategies when NCLB calls for group accountability rather than individual student attention. The Individual nature of IDEA is \"inconsistent with the group nature of NCLB.\" They worry that NCLB focuses too much on standardized testing and not enough on the work-based experience necessary for obtaining jobs in the future. Also, NCLB is measured essentially by a single test score, but IDEA calls for various measures of student success.\n\nIDEA's focus on various measures stems from its foundation in Individualized Education Plans for students with disabilities (IEP). An IEP is designed to give students with disabilities individual goals that are often not on their grade level. An IEP is intended for \"developing goals and objectives that correspond to the needs of the student, and ultimately choosing a placement in the least restrictive environment possible for the student.\" Under the IEP, students could be able to legally have lowered success criteria for academic success.\n\nA 2006 report by the Center for Evaluation and Education Policy (CEEP) and the Indiana Institute on Disability and Community indicated that most states were not making AYP because of special education subgroups even though progress had been made toward that end. This was in effect pushing schools to cancel the inclusion model and keep special education students separate. \"IDEA calls for individualized curriculum and assessments that determine success based on growth and improvement each year. NCLB, in contrast, measures all students by the same markers, which are based not on individual improvement but by proficiency in math and reading,\" the study states. When interviewed with the Indiana University Newsroom, author of the CEEP report Sandi Cole said, \"The system needs to make sense. Don't we want to know how much a child is progressing towards the standards?...We need a system that values learning and growth over time, in addition to helping students reach high standards.\" Cole found in her survey that NCLB encourages teachers to teach to the test, limiting curriculum choices/options, and to use the special education students as a \"scapegoat\" for their school not making AYP. In addition, Indiana administrators who responded to the survey indicated that NCLB testing has led to higher numbers of students with disabilities dropping out of school.\n\nLegal journals have also commented on the incompatibility of IDEA and NCLB; some say the acts may never be reconciled with one another. They point out that an IEP is designed specifically for individual student achievement, which gives the rights to parents to ensure that the schools are following the necessary protocols of Free Appropriate Public Education (FAPE). They worry that not enough emphasis is being placed on the child's IEP with this setup. In Board of Education for Ottawa Township High School District 140 v. Spelling, two Illinois school districts and parents of disabled students challenged the legality of NCLB’s testing requirements in light of IDEA’s mandate to provide students with individualized education. Although students there were aligned with \"proficiency\" to state standards, students did not meet requirements of their IEP. Their parents feared that students were not given right to FAPE. The case questioned which better indicated progress: standardized test measures, or IEP measures? It concluded that since some students may never test on grade level, all students with disabilities should be given more options and accommodations with standardized testing than they currently receive.\n\n\nAll students who are learning English would have an automatic three-year window to take assessments in their native language, after which they must normally demonstrate proficiency on an English-language assessment. However, the local education authority may grant an exception to any individual English learner for another two years' testing in his or her native language on a case-by-case basis.\n\nIn practice, however, only 10 states choose to test any English language learners in their native language (almost entirely Spanish speakers). The vast majority of English language learners are given English language assessments.\n\nMany schools test or assess students with limited English proficiency even when the students are exempt from NCLB-mandated reporting, because the tests may provide useful information to the teacher and school. In certain schools with large immigrant populations, this exemption comprises a majority of young students.\n\nNCLB testing under-reports learning at non-English-language immersion schools, particularly those that immerse students in Native American languages. NCLB requires some Native American students to take standardized tests in English. In other cases, the students could be legally tested in their native language, except that the state has not paid to have the test translated.\n\nOne study found that schools in California and Illinois that have not met AYP serve 75–85% minority students while schools meeting AYP have less than 40% minority students. Schools that do not meet AYP are required to offer their students' parents the opportunity to transfer their students to a non-failing school within the district, but it is not required that the other school accepts the student. NCLB controls the portion of federal Title I funding based upon each school meeting annual set standards. Any participating school that does not make Adequate Yearly Progress (AYP) for two years must offer parents the choice to send their child to a non-failing school in the district, and after three years, must provide supplemental services, such as free tutoring or after-school assistance. After five years of not meeting AYP, the school must make dramatic changes to how the school is run, which could entail state-takeover.\n\nAs part of their support for NCLB, the administration and Congress backed massive increases in funding for elementary and secondary education. Total federal education funding increased from $42.2 billion to $55.7 billion from 2001, the fiscal year before the law's passage, to fiscal year 2004. A new $1 billion Reading First program was created, distributing funds to local schools to improve the teaching of reading, and over $100 million for its companion, Early Reading First. Numerous other formula programs received large increases as well. This was consistent with the administration's position of funding formula programs, which distribute money to local schools for their use, and grant programs, where particular schools or groups apply directly to the federal government for funding. In total, federal funding for education increased 59.8% from 2000 to 2003.\nThe act created a new competitive-grant program called Reading First, funded at $1.02 billion in 2004, to help states and districts set up \"scientific, research-based\" reading programs for children in grades K–3 (with priority given to high-poverty areas). A smaller early-reading program sought to help states better prepare 3- to 5-year-olds in disadvantaged areas to read. The program's funding was later cut drastically by Congress amid budget talks.\n\nFunding Changes: Through an alteration in the Title I funding formula, the No Child Left Behind Act was expected to better target resources to school districts with high concentrations of poor children. The law also included provisions intended to give states and districts greater flexibility in how they spent a portion of their federal allotments.\n\nFunding for school technology used in classrooms as part of NCLB is administered by the Enhancing Education Through Technology Program (EETT). Funding sources are used for equipment, professional development and training for educators, and updated research. EETT allocates funds by formula to states. The states, in turn, reallocate 50% of the funds to local districts by Title I formula and 50% competitively. While districts must reserve a minimum of 25% of all EETT funds for professional development, recent studies indicate that most EETT recipients use far more than 25% of their EETT funds to train teachers to use technology and integrate it into their curricula. In fact, EETT recipients committed more than $159 million in EETT funds towards professional development during the 2004–05 school year alone. Moreover, even though EETT recipients are afforded broad discretion in their use of EETT funds, surveys show that they target EETT dollars towards improving student achievement in reading and mathematics, engaging in data-driven decision making, and launching online assessment programs.\n\nIn addition, the provisions of NCLB permitted increased flexibility for state and local agencies in the use of federal education money.\n\nThe NCLB increases were companions to another massive increase in federal education funding at that time. The Bush administration and congress passed very large increases in funding for the Individuals with Disabilities Education Act (IDEA) at the same time as the NCLB increases. IDEA Part B, a state formula-funding program that distributes money to local districts for the education of students with disabilities, was increased from $6.3 billion in 2001 to $10.1 billion in 2004. Because a district's and state's performance on NCLB measures depended on improved performance by students with disabilities, particularly, students with learning disabilities, this 60 percent increase in funding was also an important part of the overall approach to NCLB implementation.\n\nSome critics claim that extra expenses are not fully reimbursed by increased levels of federal NCLB funding. Others note that funding for the law increased massively following passage and that billions in funds previously allocated to particular uses could be reallocated to new uses. Even before the law's passage, Secretary of Education Rod Paige noted ensuring that children are educated remained a state responsibility regardless of federal support:\nVarious early Democratic supporters of NCLB criticize its implementation, claiming it is not adequately funded by either the federal government or the states. Ted Kennedy, the legislation's initial sponsor, once stated: \"The tragedy is that these long overdue reforms are finally in place, but the funds are not.\" Susan B. Neuman, U.S. Department of Education's former Assistant Secretary for Elementary and Secondary Education, commented about her worries of NCLB in a meeting of the International Reading Association:\nOrganizations have particularly criticized the unwillingness of the federal government to \"fully fund\" the act. Noting that appropriations bills always originate in the House of Representatives, it is true that during the Bush Administration, neither the Senate nor the White House has even requested federal funding up to the authorized levels for several of the act’s main provisions. For example, President Bush requested only $13.3 billion of a possible $22.75 billion in 2006. Advocacy groups note that President Bush's 2008 budget proposal allotted $61 billion for the Education Department, cutting funding by $1.3 billion from the year before. 44 out of 50 states would have received reductions in federal funding if the budget passed as it was. Specifically, funding for the Enhancing Education Through Technology Program (EETT) has continued to drop while the demand for technology in schools has increased (Technology and Learning, 2006). However, these claims focused on reallocated funds, as each of President Bush's proposed budgets increased funding for major NCLB formula programs such as Title I, including his final 2009 budget proposal.\n\nMembers of Congress have viewed these authorized levels as spending caps, not spending promises. Some opponents argue that these funding shortfalls mean that schools faced with the system of escalating penalties for failing to meet testing targets are denied the resources necessary to remedy problems detected by testing. However, federal NCLB formula funding increased by billions during this period and state and local funding increased by over $100 billion from school year 2001–02 through 2006–07.\n\nIn fiscal year 2007, $75 billion in costs were shifted from NCLB, adding further stresses on state budgets. This decrease resulted in schools cutting programs that served to educate children, which subsequently impacted the ability to meet the goals of NCLB. The decrease in funding came at a time when there was an increase in expectations for school performance. To make ends meet, many schools re-allocated funds that had been intended for other purposes (e.g., arts, sports, etc.) to achieve the national educational goals set by NCLB. Congress acknowledged these funding decreases and retroactively provided the funds to cover shortfalls, but without the guarantee of permanent aid.\n\nThe number one area where funding was cut from the national budget was in Title I funding for disadvantaged students and schools.\n\nAccording to the book \"NCLB Meets School Realities\", the act was put into action during a time of fiscal crisis for most states. While states were being forced to make budget cuts, including in the area of education, they had to incur additional expenses to comply with the requirements of the NCLB Act. The funding they received from the federal government in support of NCLB was not enough to cover the added expense necessary to adhere to the new law.\n\nThe Joint Organizational Statement on No Child Left Behind is a proposal by more than 135 national civil rights, education, disability advocacy, civic, labor, and religious groups that have signed on to a statement calling for major changes to the federal education law. The National Center for Fair & Open Testing (FairTest) initiated and chaired the meetings that produced the statement, originally released in October 2004. The statement's central message is that \"the law's emphasis needs to shift from applying sanctions for failing to raise test scores to holding states and localities accountable for making the systemic changes that improve student achievement.\" The number of organizations signing the statement has nearly quadrupled since it was launched in late 2004 and continues to grow. The goal is to influence Congress, and the broader public, as the law's scheduled reauthorization approaches.\n\nEducation critic Alfie Kohn argues that the NCLB law is \"unredeemable\" and should be scrapped. He is quoted saying \"[I]ts main effect has been to sentence poor children to an endless regimen of test-preparation drills\".\n\nIn February 2007, former Health and Human Services Secretary Tommy Thompson and Georgia Governor Roy Barnes, Co-Chairs of the Aspen Commission on No Child Left Behind, announced the release of the Commission's final recommendations for the reauthorization of the No Child Left Behind Act. The Commission is an independent, bipartisan effort to improve NCLB and ensure it is a more useful force in closing the achievement gap that separates disadvantaged children and their peers. After a year of hearings, analysis, and research, the Commission uncovered the successes of NCLB, as well as provisions that must be significantly changed.\n\nThe Commission's goals are:\n\nThe Forum on Educational Accountability (FEA), a working group of signers of the Joint Organizational Statement on NCLB has offered an alternative proposal. It proposes to shift NCLB from applying sanctions for failing to raise test scores to supporting state and communities and holding them accountable as they make systemic changes that improve student learning.\n\nWhile many critics and policymakers believe the NCLB legislation has major flaws, it appears the policy will be in effect for the long-term, though not without major modifications.\n\nPresident Barack Obama released a blueprint for reform of the Elementary and Secondary Education Act, the successor to No Child Left Behind, in March 2010. Specific revisions include providing funds for states to implement a broader range of assessments to evaluate advanced academic skills, including students’ abilities to conduct research, use technology, engage in scientific investigation, solve problems, and communicate effectively.\n\nIn addition, Obama proposes that the NCLB legislation lessen its stringent accountability punishments to states by focusing more on student improvement. Improvement measures would encompass assessing all children appropriately, including English language learners, minorities, and special needs students. The school system would be re-designed to consider measures beyond reading and math tests; and would promote incentives to keep students enrolled in school through graduation, rather than encouraging student drop-out to increase AYP scores.\n\nObama’s objectives also entail lowering the achievement gap between Black and White students and also increasing the federal budget by $3 billion to help schools meet the strict mandates of the bill. There has also been a proposal, put forward by the Obama administration, that states increase their academic standards after a dumbing down period, focus on re-classifying schools that have been labeled as failing, and develop a new evaluation process for teachers and educators.\n\nThe federal government’s gradual investment in public social provisions provides the NCLB Act a forum to deliver on its promise to improve achievement for all of its students. Education critics argue that although the legislation is marked as an improvement to the ESEA in de-segregating the quality of education in schools, it is actually harmful. The legislation has become virtually the only federal social policy meant to address wide-scale social inequities, and its policy features inevitably stigmatize both schools attended by children of the poor and children in general.\n\nMoreover, critics further argue that the current political landscape of this country, which favors market-based solutions to social and economic problems, has eroded trust in public institutions and has undermined political support for an expansive concept of social responsibility, which subsequently results in a disinvestment in the education of the poor and privatization of American schools.\n\nSkeptics posit that NCLB provides distinct political advantages to Democrats, whose focus on accountability offers a way for them to speak of equal opportunity and avoid being classified as the party of big government, special interests, and minority groups—a common accusation from Republicans who want to discredit what they see as the traditional Democratic agenda. Opponents posit that NCLB has inadvertently shifted the debate on education and racial inequality to traditional political alliances. Consequently, major political discord remains between those who oppose federal oversight of state and local practices and those who view NCLB in terms of civil rights and educational equality.\n\nIn the plan, the Obama Administration responds to critiques that standardized testing fails to capture higher level thinking by outlining new systems of evaluation to capture more in depth assessments on student achievement. His plan came on the heels of the announcement of the Race to the Top initiative, a $4.35 billion reform program financed by the Department of Education through the American Recovery and Reinvestment Act of 2009.\n\nObama says that accurate assessments \"...can be used to accurately measure student growth; to better measure how states, districts, schools, principals, and teachers are educating students; to help teachers adjust and focus their teaching, and to provide better information to students and their families.\" He has pledged to support state governments in their efforts to improve standardized test provisions by upgrading the standards they are set to measure. To do this, the federal government gives states grants to help develop and implement assessments based on higher standards so they can more accurately measure school progress. This mirrors provisions in the Race to the Top program that require states to measure individual achievement through sophisticated data collection from kindergarten to higher education.\n\nWhile Obama plans to improve the quality of standardized testing, he does not plan to eliminate the testing requirements and accountability measures produced by standardized tests. Rather, he provides additional resources and flexibility to meet new goals. Critics of Obama’s reform efforts maintain that high-stakes testing is detrimental to school success across the country, because it encourages teachers to \"teach to the test\" and places undue pressure on teachers and schools if they fail to meet benchmarks.\n\nThe re-authorization process has become somewhat of a controversy, as lawmakers and politicians continually debate about the changes that must be made to the bill to make it work best for the educational system.\n\nIn 2012, President Obama granted waivers from NCLB requirements to several states. \"In exchange for that flexibility, those states 'have agreed to raise standards, improve accountability, and undertake essential reforms to improve teacher effectiveness,' the White House said in a statement.\"\n\nEight of the 32 NCLB waivers granted to states are conditional, meaning those states have not entirely satisfied the administration's requirements and part of their plans are under review.\n\nThe waivers of Arizona, Oregon, and Kansas are conditional, according to Acting Assistant Secretary for Elementary and Secondary Education Michael Yudin. Arizona has not yet received state board approval for teacher evaluations, and Kansas and Oregon are both still developing teacher and principal evaluation guidelines.\n\nIn addition, five states that did not complete the waiver process—and one whose application was rejected—got a one-year freeze on the rising targets for standardized test scores: Alabama, Alaska, Idaho, Iowa, Maine, and West Virginia.\n\nOn April 30, 2015, a bill was introduced to Congress to replace the No Child Left Behind Act, the Every Student Succeeds Act, which was passed by the House on December 2 and the Senate on December 9, before being signed into law by President Obama on December 10, 2015. This bill affords states more flexibility in regards to setting their own respective standards for measuring school as well as student performance.\n\n\n\n\n\n"}
{"id": "72754", "url": "https://en.wikipedia.org/wiki?curid=72754", "title": "Organic farming", "text": "Organic farming\n\nOrganic farming is a phrase coined early in the 20th century in reaction to rapidly changing farming practices to describe what other species use, and used, to farm without synthetic chemicals. Organic farming continues to be developed by various organic agriculture organizations today. It relies on fertilizers of organic origin such as compost manure, green manure, and bone meal and places emphasis on techniques such as crop rotation and companion planting. Biological pest control, mixed cropping and the fostering of insect predators are encouraged. In general, organic standards are designed to allow the use of naturally occurring substances while prohibiting or strictly limiting synthetic substances. For instance, naturally occurring pesticides such as pyrethrin and rotenone are permitted, while synthetic fertilizers and pesticides are generally prohibited. Synthetic substances that are allowed include, for example, copper sulfate, elemental sulfur and Ivermectin. Genetically modified organisms, nanomaterials, human sewage sludge, plant growth regulators, hormones, and antibiotic use in livestock husbandry are prohibited. Reasons for advocation of organic farming include advantages in sustainability, openness, self-sufficiency, autonomy/independence, health, food security, and food safety.\n\nOrganic agricultural methods are internationally regulated and legally enforced by many nations, based in large part on the standards set by the International Federation of Organic Agriculture Movements (IFOAM), an international umbrella organization for organic farming organizations established in 1972. Organic agriculture can be defined as:\n\nSince 1990 the market for organic food and other products has grown rapidly, reaching $63 billion worldwide in 2012. This demand has driven a similar increase in organically managed farmland that grew from 2001 to 2011 at a compounding rate of 8.9% per annum.\n\nAs of 2016, approximately worldwide were farmed organically, representing approximately 1.2 percent of total world farmland.\n\nAgriculture was practiced for thousands of years without the use of artificial chemicals. Artificial fertilizers were first created during the mid-19th century. These early fertilizers were cheap, powerful, and easy to transport in bulk. Similar advances occurred in chemical pesticides in the 1940s, leading to the decade being referred to as the 'pesticide era'. These new agricultural techniques, while beneficial in the short term, had serious longer term side effects such as soil compaction, erosion, and declines in overall soil fertility, along with health concerns about toxic chemicals entering the food supply. In the late 1800s and early 1900s, soil biology scientists began to seek ways to remedy these side effects while still maintaining higher production.\n\nBiodynamic agriculture was the first modern system of agriculture to focus exclusively on organic methods. Its development began in 1924 with a series of eight lectures on agriculture given by Rudolf Steiner. These lectures, the first known presentation of what later came to be known as organic agriculture, were held in response to a request by farmers who noticed degraded soil conditions and a deterioration in the health and quality of crops and livestock resulting from the use of chemical fertilizers. The one hundred eleven attendees, less than half of whom were farmers, came from six countries, primarily Germany and Poland. The lectures were published in November 1924; the first English translation appeared in 1928 as \"The Agriculture Course\".\n\nIn 1921, Albert Howard and his wife Gabrielle Howard, accomplished botanists, founded an Institute of Plant Industry to improve traditional farming methods in India. Among other things, they brought improved implements and improved animal husbandry methods from their scientific training; then by incorporating aspects of the local traditional methods, developed protocols for the rotation of crops, erosion prevention techniques, and the systematic use of composts and manures. Stimulated by these experiences of traditional farming, when Albert Howard returned to Britain in the early 1930s he began to promulgate a system of natural agriculture.\n\nIn July 1939, Ehrenfried Pfeiffer, the author of the standard work on biodynamic agriculture (\"Bio-Dynamic Farming and Gardening\"), came to the UK at the invitation of Walter James, 4th Baron Northbourne as a presenter at the Betteshanger Summer School and Conference on Biodynamic Farming at Northbourne's farm in Kent. One of the chief purposes of the conference was to bring together the proponents of various approaches to organic agriculture in order that they might cooperate within a larger movement. Howard attended the conference, where he met Pfeiffer. In the following year, Northbourne published his manifesto of organic farming, \"Look to the Land\", in which he coined the term \"organic farming.\" The Betteshanger conference has been described as the 'missing link' between biodynamic agriculture and other forms of organic farming.\n\nIn 1940 Howard published his \"An Agricultural Testament\". In this book he adopted Northbourne's terminology of \"organic farming.\" Howard's work spread widely, and he became known as the \"father of organic farming\" for his work in applying scientific knowledge and principles to various traditional and natural methods. In the United States J.I. Rodale, who was keenly interested both in Howard's ideas and in biodynamics, founded in the 1940s both a working organic farm for trials and experimentation, The Rodale Institute, and the Rodale Press to teach and advocate organic methods to the wider public. These became important influences on the spread of organic agriculture. Further work was done by Lady Eve Balfour (the Haughley Experiment) in the United Kingdom, and many others across the world.\n\nIncreasing environmental awareness in the general population in modern times has transformed the originally supply-driven organic movement to a demand-driven one. Premium prices and some government subsidies attracted farmers. In the developing world, many producers farm according to traditional methods that are comparable to organic farming, but not certified, and that may not include the latest scientific advancements in organic agriculture. In other cases, farmers in the developing world have converted to modern organic methods for economic reasons.\n\nBiodynamic agriculturists, who based their work on Steiner's spiritually-oriented anthroposophy, used the term \"organic\" to indicate that a farm should be viewed as a living organism, in the sense of the following quotation:\nThe use of \"organic\" popularized by Howard and Rodale, on the other hand, refers more narrowly to the use of organic matter derived from plant compost and animal manures to improve the humus content of soils, grounded in the work of early soil scientists who developed what was then called \"humus farming.\" Since the early 1940s the two camps have tended to merge.\n\nOrganic farming methods combine scientific knowledge of ecology and modern technology with traditional farming practices based on naturally occurring biological processes. Organic farming methods are studied in the field of agroecology. While conventional agriculture uses synthetic pesticides and water-soluble synthetically purified fertilizers, organic farmers are restricted by regulations to using natural pesticides and fertilizers. An example of a natural pesticide is pyrethrin, which is found naturally in the Chrysanthemum flower. The principal methods of organic farming include crop rotation, green manures and compost, biological pest control, and mechanical cultivation. These measures use the natural environment to enhance agricultural productivity: legumes are planted to fix nitrogen into the soil, natural insect predators are encouraged, crops are rotated to confuse pests and renew soil, and natural materials such as potassium bicarbonate and mulches are used to control disease and weeds. Genetically modified seeds and animals are excluded.\n\nWhile organic is fundamentally different from conventional because of the use of carbon based fertilizers compared with highly soluble synthetic based fertilizers and biological pest control instead of synthetic pesticides, organic farming and large-scale conventional farming are not entirely mutually exclusive. Many of the methods developed for organic agriculture have been borrowed by more conventional agriculture. For example, Integrated Pest Management is a multifaceted strategy that uses various organic methods of pest control whenever possible, but in conventional farming could include synthetic pesticides only as a last resort.\n\nOrganic farming encourages Crop diversity. The science of agroecology has revealed the benefits of polyculture (multiple crops in the same space), which is often employed in organic farming. Planting a variety of vegetable crops supports a wider range of beneficial insects, soil microorganisms, and other factors that add up to overall farm health. Crop diversity helps environments thrive and protects species from going extinct.\n\nOrganic farming relies heavily on the natural breakdown of organic matter, using techniques like green manure and composting, to replace nutrients taken from the soil by previous crops. This biological process, driven by microorganisms such as mycorrhiza, allows the natural production of nutrients in the soil throughout the growing season, and has been referred to as \"feeding the soil to feed the plant.\" Organic farming uses a variety of methods to improve soil fertility, including crop rotation, cover cropping, reduced tillage, and application of compost. By reducing tillage, soil is not inverted and exposed to air; less carbon is lost to the atmosphere resulting in more soil organic carbon. This has an added benefit of carbon sequestration, which can reduce green house gases and help reverse climate change.\n\nPlants need nitrogen, phosphorus, and potassium, as well as micronutrients and symbiotic relationships with fungi and other organisms to flourish, but getting enough nitrogen, and particularly synchronization so that plants get enough nitrogen at the right time (when plants need it most), is a challenge for organic farmers. Crop rotation and green manure (\"cover crops\") help to provide nitrogen through legumes (more precisely, the \"Fabaceae\" family), which fix nitrogen from the atmosphere through symbiosis with rhizobial bacteria. Intercropping, which is sometimes used for insect and disease control, can also increase soil nutrients, but the competition between the legume and the crop can be problematic and wider spacing between crop rows is required. Crop residues can be ploughed back into the soil, and different plants leave different amounts of nitrogen, potentially aiding synchronization. Organic farmers also use animal manure, certain processed fertilizers such as seed meal and various mineral powders such as rock phosphate and green sand, a naturally occurring form of potash that provides potassium. Together these methods help to control erosion. In some cases pH may need to be amended. Natural pH amendments include lime and sulfur, but in the U.S. some compounds such as iron sulfate, aluminum sulfate, magnesium sulfate, and soluble boron products are allowed in organic farming.\n\nMixed farms with both livestock and crops can operate as ley farms, whereby the land gathers fertility through growing nitrogen-fixing forage grasses such as white clover or alfalfa and grows cash crops or cereals when fertility is established. Farms without livestock (\"stockless\") may find it more difficult to maintain soil fertility, and may rely more on external inputs such as imported manure as well as grain legumes and green manures, although grain legumes may fix limited nitrogen because they are harvested. Horticultural farms that grow fruits and vegetables in protected conditions often rely even more on external inputs.\n\nBiological research into soil and soil organisms has proven beneficial to organic farming. Varieties of bacteria and fungi break down chemicals, plant matter and animal waste into productive soil nutrients. In turn, they produce benefits of healthier yields and more productive soil for future crops. Fields with less or no manure display significantly lower yields, due to decreased soil microbe community. Increased manure improves biological activity, providing a healthier, more arable soil system and higher yields.\n\nOrganic weed management promotes weed suppression, rather than weed elimination, by enhancing crop competition and phytotoxic effects on weeds. Organic farmers integrate cultural, biological, mechanical, physical and chemical tactics to manage weeds without synthetic herbicides.\n\nOrganic standards require rotation of annual crops, meaning that a single crop cannot be grown in the same location without a different, intervening crop. Organic crop rotations frequently include weed-suppressive cover crops and crops with dissimilar life cycles to discourage weeds associated with a particular crop. Research is ongoing to develop organic methods to promote the growth of natural microorganisms that suppress the growth or germination of common weeds.\n\nOther cultural practices used to enhance crop competitiveness and reduce weed pressure include selection of competitive crop varieties, high-density planting, tight row spacing, and late planting into warm soil to encourage rapid crop germination.\n\nMechanical and physical weed control practices used on organic farms can be broadly grouped as:\n\nSome naturally sourced chemicals are allowed for herbicidal use. These include certain formulations of acetic acid (concentrated vinegar), corn gluten meal, and essential oils. A few selective bioherbicides based on fungal pathogens have also been developed. At this time, however, organic herbicides and bioherbicides play a minor role in the organic weed control toolbox.\n\nWeeds can be controlled by grazing. For example, geese have been used successfully to weed a range of organic crops including cotton, strawberries, tobacco, and corn, reviving the practice of keeping cotton patch geese, common in the southern U.S. before the 1950s. Similarly, some rice farmers introduce ducks and fish to wet paddy fields to eat both weeds and insects.\n\nOrganisms aside from weeds that cause problems on organic farms include arthropods (e.g., insects, mites), nematodes, fungi and bacteria. Organic practices include, but are not limited to:\n\n\nExamples of predatory beneficial insects include minute pirate bugs, big-eyed bugs, and to a lesser extent ladybugs (which tend to fly away), all of which eat a wide range of pests. Lacewings are also effective, but tend to fly away. Praying mantis tend to move more slowly and eat less heavily. Parasitoid wasps tend to be effective for their selected prey, but like all small insects can be less effective outdoors because the wind controls their movement. Predatory mites are effective for controlling other mites.\n\nNaturally derived insecticides allowed for use on organic farms use include \"Bacillus thuringiensis\" (a bacterial toxin), pyrethrum (a chrysanthemum extract), spinosad (a bacterial metabolite), neem (a tree extract) and rotenone (a legume root extract). Fewer than 10% of organic farmers use these pesticides regularly; one survey found that only 5.3% of vegetable growers in California use rotenone while 1.7% use pyrethrum. These pesticides are not always more safe or environmentally friendly than synthetic pesticides and can cause harm. The main criterion for organic pesticides is that they are naturally derived, and some naturally derived substances have been controversial. Controversial natural pesticides include rotenone, copper, nicotine sulfate, and pyrethrums Rotenone and pyrethrum are particularly controversial because they work by attacking the nervous system, like most conventional insecticides. Rotenone is extremely toxic to fish and can induce symptoms resembling Parkinson's disease in mammals. Although pyrethrum (natural pyrethrins) is more effective against insects when used with piperonyl butoxide (which retards degradation of the pyrethrins), organic standards generally do not permit use of the latter substance.\n\nNaturally derived fungicides allowed for use on organic farms include the bacteria \"Bacillus subtilis\" and \"Bacillus pumilus\"; and the fungus \"Trichoderma harzianum\". These are mainly effective for diseases affecting roots. Compost tea contains a mix of beneficial microbes, which may attack or out-compete certain plant pathogens, but variability among formulations and preparation methods may contribute to inconsistent results or even dangerous growth of toxic microbes in compost teas.\n\nSome naturally derived pesticides are not allowed for use on organic farms. These include nicotine sulfate, arsenic, and strychnine.\n\nSynthetic pesticides allowed for use on organic farms include insecticidal soaps and horticultural oils for insect management; and Bordeaux mixture, copper hydroxide and sodium bicarbonate for managing fungi. Copper sulfate and Bordeaux mixture (copper sulfate plus lime), approved for organic use in various jurisdictions, can be more environmentally problematic than some synthetic fungicides dissallowed in organic farming Similar concerns apply to copper hydroxide. Repeated application of copper sulfate or copper hydroxide as a fungicide may eventually result in copper accumulation to toxic levels in soil, and admonitions to avoid excessive accumulations of copper in soil appear in various organic standards and elsewhere. Environmental concerns for several kinds of biota arise at average rates of use of such substances for some crops. In the European Union, where replacement of copper-based fungicides in organic agriculture is a policy priority, research is seeking alternatives for organic production.\n\nRaising livestock and poultry, for meat, dairy and eggs, is another traditional farming activity that complements growing. Organic farms attempt to provide animals with natural living conditions and feed. Organic certification verifies that livestock are raised according to the USDA organic regulations throughout their lives. These regulations include the requirement that all animal feed must be certified organic.\n\nOrganic livestock may be, and must be, treated with medicine when they are sick, but drugs cannot be used to promote growth, their feed must be organic, and they must be pastured.\n\nAlso, horses and cattle were once a basic farm feature that provided labor, for hauling and plowing, fertility, through recycling of manure, and fuel, in the form of food for farmers and other animals. While today, small growing operations often do not include livestock, domesticated animals are a desirable part of the organic farming equation, especially for true sustainability, the ability of a farm to function as a self-renewing unit.\n\nA key characteristic of organic farming is the rejection of genetically engineered plants and animals. On 19 October 1998, participants at IFOAM's 12th Scientific Conference issued the Mar del Plata Declaration, where more than 600 delegates from over 60 countries voted unanimously to exclude the use of genetically modified organisms in food production and agriculture.\n\nAlthough opposition to the use of any transgenic technologies in organic farming is strong, agricultural researchers Luis Herrera-Estrella and Ariel Alvarez-Morales continue to advocate integration of transgenic technologies into organic farming as the optimal means to sustainable agriculture, particularly in the developing world, as does author and scientist Pamela Ronald, who views this kind of biotechnology as being consistent with organic principles.\n\nAlthough GMOs are excluded from organic farming, there is concern that the pollen from genetically modified crops is increasingly penetrating organic and heirloom seed stocks, making it difficult, if not impossible, to keep these genomes from entering the organic food supply. Differing regulations among countries limits the availability of GMOs to certain countries, as described in the article on regulation of the release of genetic modified organisms.\n\nOrganic farmers use a number of traditional farm tools to do farming. Due to the goals of sustainability in organic farming, organic farmers try to minimize their reliance on fossil fuels. In the developing world on small organic farms tools are normally constrained to hand tools and diesel powered water pumps.\n\nStandards regulate production methods and in some cases final output for organic agriculture. Standards may be voluntary or legislated. As early as the 1970s private associations certified organic producers. In the 1980s, governments began to produce organic production guidelines. In the 1990s, a trend toward legislated standards began, most notably with the 1991 EU-Eco-regulation developed for European Union, which set standards for 12 countries, and a 1993 UK program. The EU's program was followed by a Japanese program in 2001, and in 2002 the U.S. created the National Organic Program (NOP). As of 2007 over 60 countries regulate organic farming (IFOAM 2007:11). In 2005 IFOAM created the Principles of Organic Agriculture, an international guideline for certification criteria. Typically the agencies accredit certification groups rather than individual farms.\n\nOrganic production materials used in and foods are tested independently by the Organic Materials Review Institute.\n\nUsing manure as a fertilizer risks contaminating food with animal gut bacteria, including pathogenic strains of E. coli that have caused fatal poisoning from eating organic food. To combat this risk, USDA organic standards require that manure must be sterilized through high temperature thermophilic composting. If raw animal manure is used, 120 days must pass before the crop is harvested if the final product comes into direct contact with the soil. For products that don't directly contact soil, 90 days must pass prior to harvest.\n\nThe economics of organic farming, a subfield of agricultural economics, encompasses the entire process and effects of organic farming in terms of human society, including social costs, opportunity costs, unintended consequences, information asymmetries, and economies of scale. Although the scope of economics is broad, agricultural economics tends to focus on maximizing yields and efficiency at the farm level. Economics takes an anthropocentric approach to the value of the natural world: biodiversity, for example, is considered beneficial only to the extent that it is valued by people and increases profits. Some entities such as the European Union subsidize organic farming, in large part because these countries want to account for the externalities of reduced water use, reduced water contamination, reduced soil erosion, reduced carbon emissions, increased biodiversity, and assorted other benefits that result from organic farming.\n\nTraditional organic farming is labor and knowledge-intensive whereas conventional farming is capital-intensive, requiring more energy and manufactured inputs.\n\nOrganic farmers in California have cited marketing as their greatest obstacle.\n\nThe markets for organic products are strongest in North America and Europe, which as of 2001 are estimated to have $6 and $8 billion respectively of the $20 billion global market. As of 2007 Australasia has 39% of the total organic farmland, including Australia's but 97 percent of this land is sprawling rangeland (2007:35). US sales are 20x as much. Europe farms 23 percent of global organic farmland (), followed by Latin America with 19 percent (5.8 million hectares - 14.3 million acres). Asia has 9.5 percent while North America has 7.2 percent. Africa has 3 percent.\n\nBesides Australia, the countries with the most organic farmland are Argentina (3.1 million hectares - 7.7 million acres), China (2.3 million hectares - 5.7 million acres), and the United States (1.6 million hectares - 4 million acres). Much of Argentina's organic farmland is pasture, like that of Australia (2007:42). Spain, Germany, Brazil (the world's largest agricultural exporter), Uruguay, and the England follow the United States in the amount of organic land (2007:26).\n\nIn the European Union (EU25) 3.9% of the total utilized agricultural area was used for organic production in 2005. The countries with the highest proportion of organic land were Austria (11%) and Italy (8.4%), followed by the Czech Republic and Greece (both 7.2%). The lowest figures were shown for Malta (0.2%), Poland (0.6%) and Ireland (0.8%).\nIn 2009, the proportion of organic land in the EU grew to 4.7%. The countries with highest share of agricultural land were Liechtenstein (26.9%), Austria (18.5%) and Sweden (12.6%). 16% of all farmers in Austria produced organically in 2010. By the same year the proportion of organic land increased to 20%.: In 2005 168,000 ha (415,000 ac) of land in Poland was under organic management. In 2012, 288,261 hectares (712,308 acres) were under organic production, and there were about 15,500 organic farmers; retail sales of organic products were EUR 80 million in 2011. As of 2012 organic exports were part of the government's economic development strategy.\n\nAfter the collapse of the Soviet Union in 1991, agricultural inputs that had previously been purchased from Eastern bloc countries were no longer available in Cuba, and many Cuban farms converted to organic methods out of necessity. Consequently, organic agriculture is a mainstream practice in Cuba, while it remains an alternative practice in most other countries. Cuba's organic strategy includes development of genetically modified crops; specifically corn that is resistant to the palomilla moth.\n\nIn 2001, the global market value of certified organic products was estimated at USD $20 billion. By 2002, this was USD $23 billion and by 2015 more than USD $43 billion. By 2014, retail sales of organic products reached USD $80 billion worldwide. North America and Europe accounted for more than 90% of all organic product sales. In 2018 Australia accounted for 54% of the world's certified organic land with the country recording more than 35,000,000 verified organic hectares.\n\nOrganic agricultural land increased almost fourfold in 15 years, from 11 million hectares in 1999 to 43.7 million hectares in 2014. Between 2013 and 2014, organic agricultural land grew by 500,000 hectares worldwide, increasing in every region except Latin America. During this time period, Europe’s organic farmland increased 260,000 hectares to 11.6 million total (+2.3%), Asia’s increased 159,000 hectares to 3.6 million total (+4.7%), Africa’s increased 54,000 hectares to 1.3 million total (+4.5%), and North America’s increased 35,000 hectares to 3.1 million total (+1.1%). As of 2014, the country with the most organic land was Australia (17.2 million hectares), followed by Argentina (3.1 million hectares), and the United States (2.2 million hectares). Australia's organic land area has increased at a rate of 16.5% per annum for the past eighteen years.\n\nIn 2013, the number of organic producers grew by almost 270,000, or more than 13%. By 2014, there were a reported 2.3 million organic producers in the world. Most of the total global increase took place in the Philippines, Peru, China, and Thailand. Overall, the majority of all organic producers are in India (650,000 in 2013), Uganda (190,552 in 2014), Mexico (169,703 in 2013) and the Philippines (165,974 in 2014).\n\nStudies comparing yields have had mixed results. These differences among findings can often be attributed to variations between study designs including differences in the crops studied and the methodology by which results were gathered.\n\nA 2012 meta-analysis found that productivity is typically lower for organic farming than conventional farming, but that the size of the difference depends on context and in some cases may be very small. While organic yields can be lower than conventional yields, another meta-analysis published in Sustainable Agriculture Research in 2015, concluded that certain organic on-farm practices could help narrow this gap. Timely weed management and the application of manure in conjunction with legume forages/cover crops were shown to have positive results in increasing organic corn and soybean productivity.\n\nAnother meta-analysis published in the journal Agricultural Systems in 2011 analyzed 362 datasets and found that organic yields were on average 80% of conventional yields. The author's found that there are relative differences in this yield gap based on crop type with crops like soybeans and rice scoring higher than the 80% average and crops like wheat and potato scoring lower. Across global regions, Asia and Central Europe were found to have relatively higher yields and Northern Europe relatively lower than the average.\n\nA 2007 study compiling research from 293 different comparisons into a single study to assess the overall efficiency of the two agricultural systems has concluded that \"organic methods could produce enough food on a global per capita basis to sustain the current human population, and potentially an even larger population, without increasing the agricultural land base.\" The researchers also found that while in developed countries, organic systems on average produce 92% of the yield produced by conventional agriculture, organic systems produce 80% more than conventional farms in developing countries, because the materials needed for organic farming are more accessible than synthetic farming materials to farmers in some poor countries. This study's methodology and results were contested by D.J. Connor of The University of Melbourne, in a short communication published in Field Crops Research. Connor writes that errors in Badgley et al. result in \"major overestimation of the productivity of OA\".\n\nA study published in 2005 compared conventional cropping, organic animal-based cropping, and organic legume-based cropping on a test farm at the Rodale Institute over 22 years. The study found that \"the crop yields for corn and soybeans were similar in the organic animal, organic legume, and conventional farming systems\". It also found that \"significantly less fossil energy was expended to produce corn in the Rodale Institute’s organic animal and organic legume systems than in the conventional production system. There was little difference in energy input between the different treatments for producing soybeans. In the organic systems, synthetic fertilizers and pesticides were generally not used\". As of 2013 the Rodale study was ongoing and a thirty-year anniversary report was published by Rodale in 2012.\n\nA long-term field study comparing organic/conventional agriculture carried out over 21 years in Switzerland concluded that \"Crop yields of the organic systems averaged over 21 experimental years at 80% of the conventional ones. The fertilizer input, however, was 34 – 51% lower, indicating an efficient production. The organic farming systems used 20 – 56% less energy to produce a\ncrop unit and per land area this difference was 36 – 53%. In spite of the considerably lower pesticide input the quality of organic products was hardly discernible from conventional analytically and even came off better in food preference trials and picture creating methods\"\n\nIn the United States, organic farming has been shown to be 2.7 to 3.8 times more profitable for the farmer than conventional farming when prevailing price premiums are taken into account. Globally, organic farming is between 22 and 35 percent more profitable for farmers than conventional methods, according to a 2015 meta-analysis of studies conducted across five continents.\n\nThe profitability of organic agriculture can be attributed to a number of factors. First, organic farmers do not rely on synthetic fertilizer and pesticide inputs, which can be costly. In addition, organic foods currently enjoy a price premium over conventionally produced foods, meaning that organic farmers can often get more for their yield.\n\nThe price premium for organic food is an important factor in the economic viability of organic farming. In 2013 there was a 100% price premium on organic vegetables and a 57% price premium for organic fruits. These percentages are based on wholesale fruit and vegetable prices, available through the United States Department of Agriculture’s Economic Research Service. Price premiums exist not only for organic versus nonorganic crops, but may also vary depending on the venue where the product is sold: farmers' markets, grocery stores, or wholesale to restaurants. For many producers, direct sales at farmers' markets are most profitable because the farmer receives the entire markup, however this is also the most time and labor-intensive approach.\n\nThere have been signs of organic price premiums narrowing in recent years, which lowers the economic incentive for farmers to convert to or maintain organic production methods. Data from 22 years of experiments at the Rodale Institute found that, based on the current yields and production costs associated with organic farming in the United States, a price premium of only 10% is required to achieve parity with conventional farming. A separate study found that on a global scale, price premiums of only 5-7% percent were needed to break even with conventional methods. Without the price premium, profitability for farmers is mixed.\n\nFor markets and supermarkets organic food is profitable as well, and is generally sold at significantly higher prices than non-organic food.\n\nIn the most recent assessments of the energy efficiency of organic versus conventional agriculture, results have been mixed regarding which form is more carbon efficient. Organic farm systems have more often than not been found to be more energy efficient, however, this is not always the case. More than anything, results tend to depend upon crop type and farm size.\n\nA comprehensive comparison of energy efficiency in grain production, produce yield, and animal husbandry concluded that organic farming had a higher yield per unit of energy over the vast majority of the crops and livestock systems. For example, two studies - both comparing organically- versus conventionally-farmed apples - declare contradicting results, one saying organic farming is more energy efficient, the other saying conventionally is more efficient.\n\nIt has generally been found that the labor input per unit of yield was higher for organic systems compared with conventional production.\n\nMost sales are concentrated in developed nations. In 2008, 69% of Americans claimed to occasionally buy organic products, down from 73% in 2005. One theory for this change was that consumers were substituting \"local\" produce for \"organic\" produce.\n\nThe USDA requires that distributors, manufacturers, and processors of organic products be certified by an accredited state or private agency. In 2007, there were 3,225 certified organic handlers, up from 2,790 in 2004.\n\nOrganic handlers are often small firms; 48% reported sales below $1 million annually, and 22% between $1 and $5 million per year. Smaller handlers are more likely to sell to independent natural grocery stores and natural product chains whereas large distributors more often market to natural product chains and conventional supermarkets, with a small group marketing to independent natural product stores. Some handlers work with conventional farmers to convert their land to organic with the knowledge that the farmer will have a secure sales outlet. This lowers the risk for the handler as well as the farmer. In 2004, 31% of handlers provided technical support on organic standards or production to their suppliers and 34% encouraged their suppliers to transition to organic. Smaller farms often join together in cooperatives to market their goods more effectively.\n\n93% of organic sales are through conventional and natural food supermarkets and chains, while the remaining 7% of U.S. organic food sales occur through farmers' markets, foodservices, and other marketing channels.\n\nIn the 2012 Census, direct-to-consumer sales equaled $1.3 billion, up from $812 million in 2002, an increase of 60 percent. The number of farms that utilize direct-to-consumer sales was 144,530 in 2012 in comparison to 116,733 in 2002. Direct-to-consumer sales include farmers' markets, community supported agriculture (CSA), on-farm stores, and roadside farm stands. Some organic farms also sell products direct to retailer, direct to restaurant and direct to institution. According to the 2008 Organic Production Survey, approximately 7% of organic farm sales were direct-to-consumers, 10% went direct to retailers, and approximately 83% went into wholesale markets. In comparison, only 0.4% of the value of convention agricultural commodities were direct-to-consumers.\n\nWhile not all products sold at farmer’s markets are certified organic, this direct-to-consumer avenue has become increasingly popular in local food distribution and has grown substantially since 1994. In 2014, there were 8,284 farmer’s markets in comparison to 3,706 in 2004 and 1,755 in 1994, most of which are found in populated areas such as the Northeast, Midwest, and West Coast.\n\nOrganic production is more labor-intensive than conventional production. On the one hand, this increased labor cost is one factor that makes organic food more expensive. On the other hand, the increased need for labor may be seen as an \"employment dividend\" of organic farming, providing more jobs per unit area than conventional systems. The 2011 UNEP Green Economy Report suggests that \"[a]n increase in investment in green agriculture is projected to lead to growth in employment of about 60 per cent compared with current levels\" and that \"green agriculture investments could create 47 million additional jobs compared with BAU2 over the next 40 years.\" The United Nations Environment Programme (UNEP) also argues that \"[b]y greening agriculture and food distribution, more calories per person per day, more jobs and business opportunities especially in rural areas, and market-access opportunities, especially for developing countries, will be available.\"\n\nIn 2007 the United Nations Food and Agriculture Organization (FAO) said that organic agriculture often leads to higher prices and hence a better income for farmers, so it should be promoted. However, FAO stressed that by organic farming one could not feed the current mankind, even less the bigger future population. Both data and models showed then that organic farming was far from sufficient. Therefore, chemical fertilizers were needed to avoid hunger. Other analysis by many agribusiness executives, agricultural and ecological scientists, and international agriculture experts revealed the opinion that organic farming would not only increase the world's food supply, but might be the only way to eradicate hunger.\n\nFAO stressed that fertilizers and other chemical inputs can much increase the production, particularly in Africa where fertilizers are currently used 90% less than in Asia. For example, in Malawi the yield has been boosted using seeds and fertilizers. FAO also calls for using biotechnology, as it can help smallholder farmers to improve their income and food security.\n\nAlso NEPAD, development organization of African governments, announced that feeding Africans and preventing malnutrition requires fertilizers and enhanced seeds.\n\nAccording to a 2012 study in ScienceDigest, organic best management practices shows an average yield only 13% less than conventional. In the world's poorer nations where most of the world's hungry live, and where conventional agriculture's expensive inputs are not affordable by the majority of farmers, adopting organic management actually increases yields 93% on average, and could be an important part of increased food security.\n\nOrganic agriculture can contribute to ecological sustainability, especially in poorer countries. The application of organic principles enables employment of local resources (e.g., local seed varieties, manure, etc.) and therefore cost-effectiveness. Local and international markets for organic products show tremendous growth prospects and offer creative producers and exporters excellent opportunities to improve their income and living conditions.\n\nOrganic agriculture is knowledge intensive. Globally, capacity building efforts are underway, including localized training material, to limited effect. As of 2007, the International Federation of Organic Agriculture Movements hosted more than 170 free manuals and 75 training opportunities online.\n\nIn 2008 the United Nations Environmental Programme (UNEP) and the United Nations Conference on Trade and Development (UNCTAD) stated that \"organic agriculture can be more conducive to food security in Africa than most conventional production systems, and that it is more likely to be sustainable in the long-term\" and that \"yields had more than doubled where organic, or near-organic practices had been used\" and that soil fertility and drought resistance improved.\n\nThe value of organic agriculture (OA) in the achievement of the Millennium Development Goals (MDG), particularly in poverty reduction efforts in the face of climate change, is shown by its contribution to both income and non-income aspects of the MDGs. These benefits are expected to continue in the post-MDG era. A series of case studies conducted in selected areas in Asian countries by the Asian Development Bank Institute (ADBI) and published as a book compilation by ADB in Manila document these contributions to both income and non-income aspects of the MDGs. These include poverty alleviation by way of higher incomes, improved farmers' health owing to less chemical exposure, integration of sustainable principles into rural development policies, improvement of access to safe water and sanitation, and expansion of global partnership for development as small farmers are integrated in value chains.\n\nA related ADBI study also sheds on the costs of OA programs and set them in the context of the costs of attaining the MDGs. The results show considerable variation across the case studies, suggesting that there is no clear structure to the costs of adopting OA. Costs depend on the efficiency of the OA adoption programs. The lowest cost programs were more than ten times less expensive than the highest cost ones. However, further analysis of the gains resulting from OA adoption reveals that the costs per person taken out of poverty was much lower than the estimates of the World Bank, based on income growth in general or based on the detailed costs of meeting some of the more quantifiable MDGs (e.g., education, health, and environment).\n\nAgriculture imposes negative externalities (uncompensated costs) upon society through public land and other public resource use, biodiversity loss, erosion, pesticides, nutrient runoff, subsidized water usage, subsidy payments and assorted other problems. Positive externalities include self-reliance, entrepreneurship, respect for nature, and air quality. Organic methods reduce some of these costs. In 2000 uncompensated costs for 1996 reached 2,343 million British pounds or £208 per ha (£84.20/ac). A study of practices in the US published in 2005 concluded that cropland costs the economy approximately 5 to 16 billion dollars ($30–96/ha – $12–39/ac), while livestock production costs 714 million dollars. Both studies recommended reducing externalities. The 2000 review included reported pesticide poisonings but did not include speculative chronic health effects of pesticides, and the 2004 review relied on a 1992 estimate of the total impact of pesticides.\n\nIt has been proposed that organic agriculture can reduce the level of some negative externalities from (conventional) agriculture. Whether the benefits are private or public depends upon the division of property rights.\n\nSeveral surveys and studies have attempted to examine and compare conventional and organic systems of farming and have found that organic techniques, while not without harm, are less damaging than conventional ones because they reduce levels of biodiversity less than conventional systems do and use less energy and produce less waste when calculated per unit area.\n\nA 2003 to 2005 investigation by the Cranfield University for the Department for Environment, Food and Rural Affairs in the UK found that it is difficult to compare the Global warming potential, acidification and eutrophication emissions but \"Organic production often results in increased burdens, from factors such as N leaching and N2O emissions\", even though primary energy use was less for most organic products. NO is always the largest global warming potential contributor except in tomatoes. However, \"organic tomatoes always incur more burdens (except pesticide use)\". Some emissions were lower \"per area\", but organic farming always required 65 to 200% more field area than non-organic farming. The numbers were highest for bread wheat (200+ % more) and potatoes (160% more).\n\nResearchers at Oxford University analyzed 71 peer-reviewed studies and observed that organic products are sometimes worse for the environment. Organic milk, cereals, and pork generated higher greenhouse gas emissions per product than conventional ones but organic beef and olives had lower emissions in most studies. Usually organic products required less energy, but more land. Per unit of product, organic produce generates higher nitrogen leaching, nitrous oxide emissions, ammonia emissions, eutrophication, and acidification potential than conventionally grown produce. Other differences were not significant. The researchers concluded that public debate should consider various manners of employing conventional or organic farming, and not merely debate conventional farming as opposed to organic farming. They also sought to find specific solutions to specific circumstances.\n\nProponents of organic farming have claimed that organic agriculture emphasizes closed nutrient cycles, biodiversity, and effective soil management providing the capacity to mitigate and even reverse the effects of climate change and that organic agriculture can decrease fossil fuel emissions. \"The carbon sequestration efficiency of organic systems in temperate climates is almost double (575–700 kg carbon per ha per year – 510–625 lb/ac/an ) that of conventional treatment of soils, mainly owing to the use of grass clovers for feed and of cover crops in organic rotations.\"\n\nCritics of organic farming methods believe that the increased land needed to farm organic food could potentially destroy the rainforests and wipe out many ecosystems.\n\nAccording to a 2012 meta-analysis of 71 studies, nitrogen leaching, nitrous oxide emissions, ammonia emissions, eutrophication potential and acidification potential were higher for organic products, although in one study \"nitrate leaching was 4.4–5.6 times higher in conventional plots than organic plots\".\n\nExcess nutrients in lakes, rivers, and groundwater can cause algal blooms, eutrophication, and subsequent dead zones. In addition, nitrates are harmful to aquatic organisms by themselves.\n\nThe Oxford meta-analysis of 71 studies found that organic farming requires 84% more land for an equivalent amount of harvest, mainly due to lack of nutrients but sometimes due to weeds, diseases or pests, lower yielding animals and land required for fertility building crops. While organic farming does not necessarily save land for wildlife habitats and forestry in all cases, the most modern breakthroughs in organic are addressing these issues with success.\n\nProfessor Wolfgang Branscheid says that organic animal production is not good for the environment, because organic chicken requires twice as much land as \"conventional\" chicken and organic pork a quarter more. According to a calculation by Hudson Institute, organic beef requires three times as much land. On the other hand, certain organic methods of animal husbandry have been shown to restore desertified, marginal, and/or otherwise unavailable land to agricultural productivity and wildlife. Or by getting both forage and cash crop production from the same fields simultaneously, reduce net land use.\n\nIn England organic farming yields 55% of normal yields. In other regions of the world, organic methods have started producing record yields.\n\nIn organic farming synthetic pesticides are generally prohibited. A chemical is said to be synthetic if it does not already exist in the natural world. But the organic label goes further and usually prohibit compounds that exist in nature if they are produced by chemical synthesis. So the prohibition is also about the method of production and not only the nature of the compound.\n\nA non-exhaustive list of organic approved pesticides with their median lethal doses:\n\n\nWhile there may be some differences in the amounts of nutrients and anti-nutrients when organically produced food and conventionally produced food are compared, the variable nature of food production and handling makes it difficult to generalize results, and there is insufficient evidence to make claims that organic food is safer or healthier than conventional food. Claims that organic food tastes better are not supported by evidence.\n\nSupporters claim that organically managed soil has a higher quality and higher water retention. This may help increase yields for organic farms in drought years. Organic farming can build up soil organic matter better than conventional no-till farming, which suggests long-term yield benefits from organic farming. An 18-year study of organic methods on nutrient-depleted soil concluded that conventional methods were superior for soil fertility and yield for nutrient-depleted soils in cold-temperate climates, arguing that much of the benefit from organic farming derives from imported materials that could not be regarded as self-sustaining.\n\nIn \"Dirt: The Erosion of Civilizations\", geomorphologist David Montgomery outlines a coming crisis from soil erosion. Agriculture relies on roughly one meter of topsoil, and that is being depleted ten times faster than it is being replaced. No-till farming, which some claim depends upon pesticides, is one way to minimize erosion. However, a 2007 study by the USDA's Agricultural Research Service has found that manure applications in tilled organic farming are better at building up the soil than no-till.\n\nThe conservation of natural resources and biodiversity is a core principle of organic production. Three broad management practices (prohibition/reduced use of chemical pesticides and inorganic fertilizers; sympathetic management of non-cropped habitats; and preservation of mixed farming) that are largely intrinsic (but not exclusive) to organic farming are particularly beneficial for farmland wildlife. Using practices that attract or introduce beneficial insects, provide habitat for birds and mammals, and provide conditions that increase soil biotic diversity serve to supply vital ecological services to organic production systems. Advantages to certified organic operations that implement these types of production practices include: 1) decreased dependence on outside fertility inputs; 2) reduced pest management costs; 3) more reliable sources of clean water; and 4) better pollination.\n\nNearly all non-crop, naturally occurring species observed in comparative farm land practice studies show a preference for organic farming both by abundance and diversity. An average of 30% more species inhabit organic farms. Birds, butterflies, soil microbes, beetles, earthworms, spiders, vegetation, and mammals are particularly affected. Lack of herbicides and pesticides improve biodiversity fitness and population density. Many weed species attract beneficial insects that improve soil qualities and forage on weed pests. Soil-bound organisms often benefit because of increased bacteria populations due to natural fertilizer such as manure, while experiencing reduced intake of herbicides and pesticides. Increased biodiversity, especially from beneficial soil microbes and mycorrhizae have been proposed as an explanation for the high yields experienced by some organic plots, especially in light of the differences seen in a 21-year comparison of organic and control fields.\n\nBiodiversity from organic farming provides capital to humans. Species found in organic farms enhance sustainability by reducing human input (e.g., fertilizers, pesticides).\n\nThe USDA’s Agricultural Marketing Service (AMS) published a \"Federal Register\" notice on 15 January 2016, announcing the National Organic Program (NOP) final guidance on Natural Resources and Biodiversity Conservation for Certified Organic Operations. Given the broad scope of natural resources which includes soil, water, wetland, woodland and wildlife, the guidance provides examples of practices that support the underlying conservation principles and demonstrate compliance with USDA organic regulations § 205.200. The final guidance provides organic certifiers and farms with examples of production practices that support conservation principles and comply with the USDA organic regulations, which require operations to maintain or improve natural resources. The final guidance also clarifies the role of certified operations (to submit an OSP to a certifier), certifiers (ensure that the OSP describes or lists practices that explain the operator's monitoring plan and practices to support natural resources and biodiversity conservation), and inspectors (onsite inspection) in the implementation and verification of these production practices.\n\nA wide range of organisms benefit from organic farming, but it is unclear whether organic methods confer greater benefits than conventional integrated agri-environmental programs. Organic farming is often presented as a more biodiversity-friendly practice, but the generality of the beneficial effects of organic farming is debated as the effects appear often species- and context-dependent, and current research has highlighted the need to quantify the relative effects of local- and landscape-scale management on farmland biodiversity. There are four key issues when comparing the impacts on biodiversity of organic and conventional farming: (1) It remains unclear whether a holistic whole-farm approach (i.e. organic) provides greater benefits to biodiversity than carefully targeted prescriptions applied to relatively small areas of cropped and/or non-cropped habitats within conventional agriculture (i.e. agri-environment schemes); (2) Many comparative studies encounter methodological problems, limiting their ability to draw quantitative conclusions; (3) Our knowledge of the impacts of organic farming in pastoral and upland agriculture is limited; (4) There remains a pressing need for longitudinal, system-level studies in order to address these issues and to fill in the gaps in our knowledge of the impacts of organic farming, before a full appraisal of its potential role in biodiversity conservation in agroecosystems can be made.\n\nOrganic agriculture is often considered to be more socially just and economically sustainable for farmworkers than conventional agriculture. However, there is little social science research or consensus as to whether or not organic agriculture provides better working conditions than conventional agriculture. As many consumers equate organic and sustainable agriculture with small-scale, family-owned organizations it is widely interpreted that buying organic supports better conditions for farmworkers than buying with conventional producers. Organic agriculture is generally more labor-intensive due to its dependence on manual practices for fertilization and pest removal and relies heavily upon hired, non-family farmworkers rather than family members. Although illnesses from synthetic inputs pose less of a risk, hired workers still fall victim to debilitating musculoskeletal disorders associated with agricultural work. The USDA certification requirements outline growing practices and ecological standards but do nothing to codify labor practices. Independent certification initiatives such as the Agricultural Justice Project, Domestic Fair Trade Working Group, and the Food Alliance have attempted to implement farmworker interests but because these initiatives require voluntary participation of organic farms, their standards cannot be widely enforced.Despite the benefit to farmworkers of implementing labor standards, there is little support among the organic community for these social requirements. Many actors of the organic industry believe that enforcing labor standards would be unnecessary, unacceptable, or unviable due to the constraints of the market.\n\nIn India, in 2016, the northern state of Sikkim achieved its goal of converting to 100% organic farming. Other states of India, including Kerala, Mizoram, Goa, Rajasthan, and Meghalaya, have also declared their intentions to shift to fully organic cultivation.\n\nThe Dominican Republic has successfully converted a large amount of its banana crop to organic. The Dominican Republic accounts for 55% of the world’s certified organic bananas.\n\nIn Thailand, the Institute for Sustainable Agricultural Communities (ISAC) was established in 1991 to promote organic farming (among other sustainable agricultural practices). The national target via the National Plan for Organic Farming is to attain, by 2021, 1.3 million rai of organically farmed land. Another target is for 40% of the produce from these farmlands to be consumed domestically.\n\nMuch progress has been made:\n\n\n\n\n"}
{"id": "13816853", "url": "https://en.wikipedia.org/wiki?curid=13816853", "title": "Overengineering", "text": "Overengineering\n\nOverengineering (or over-engineering) is the act of designing a product to be more robust or have more features than necessary for its intended use, or for a process to be unnecessarily complex or inefficient.\n\nOverengineering is often done to increase a factor of safety, add function, or overcome perceived design flaws that most users would accept. Overengineering can be desirable when safety or performance is critical (e.g. in aerospace vehicles), or when extremely broad functionality is required (e.g. diagnostic tools), but it is generally criticized in terms of value engineering as wasteful of resources such as materials, time and money. As a design philosophy, it is the opposite of the minimalist ethos of \"less is more\" and a violation of the KISS principle.\n\nOverengineering generally occurs in high-end products or specialized markets. In one form, products are \"overbuilt\" and have performance far in excess of expected normal operation (a city car that can travel at 300 km/h, or a home video recorder with a projected lifespan of 100 years), and hence are more expensive, bulkier, and heavier than necessary. Alternatively, they may become \"overcomplicated\" – the extra functions may be unnecessary, and potentially reduce the usability of the product by overwhelming end users.\n\nOverengineering can decrease the productivity of the design team because of the need to build and maintain unwanted features.\n\nA related issue is market segmentation – making different products for different market segments. In this context, a particular product may be more or less suited (and thus considered over- or under-engineered) for a particular market segment.\n\nA story about very precise engineering is given in the 1858 story \"\" by Oliver Wendell Holmes, Sr., which tells of a carriage (one-horse shay)\n<poem>That was built in such a logical way\nIt ran a hundred years to a day,\nAnd then,\nwent to pieces all at once, --\nAll at once, and nothing first, --\nJust as bubbles do when they burst.\n</poem>\nBecause it had been engineered so that no single piece failed first – no piece was over-engineered relative to the others, and they thus all collapsed at the same time.\n\nA similar quote by Ferdinand Porsche claimed \"the perfect race car crosses the finish line in first place and immediately falls into pieces.\"\n\nA modern example is Juicero, a wi-fi \"smart\" juicing press. But after its release, Bloomberg News published a story that showed that the juice packs could be squeezed by hand faster than the press, and that hand-squeezing produced juice that was indistinguishable in quantity and quality from the output of the machine.\n\n\n"}
{"id": "26170795", "url": "https://en.wikipedia.org/wiki?curid=26170795", "title": "Person of Jewish ethnicity", "text": "Person of Jewish ethnicity\n\nPerson of Jewish ethnicity () is а Russian euphemism that was invented as a politically correct alternative term for an ethnic Jew. It was invented because the word \"Jew\" became derogatory during Soviet antisemitic campaigns. \n\nSeveral officially sanctioned antisemitic campaigns took place in the Soviet Union, most notably the Doctor's Plot and the struggle against the \"rootless cosmopolitans\". However, the entire Jewish population was never openly and officially declared the enemy of the people. Instead, several euphemisms were used, such as zionists, rootless cosmopolitans and \"persons of Jewish ethnicity\". There was an important distinction between these words: \"zionist\" and \"rootless cosmopolitans\" served as a label for \"bad Jews\" as enemies of the state, whereas \"persons of Jewish ethnicity\" was a politically correct expression for good, loyal Jews who were called by some ordinary folks as \"trained\" Jews (the original Russian word \"дресированные\" (dressage) typically refers to animals trained to perform in circus). However most people realized that all these euphemisms denoted \"all\" Jews. A dean of Marxism-Leninism department at one of Soviet Universities explained the policy to his students:\n\nThe analogous euphemism \"person of Caucasian ethnicity\" is used less frequently in modern Russian to refer to peoples of the Caucasus, such as Georgians or Armenians in Russia.\n\n"}
{"id": "39098", "url": "https://en.wikipedia.org/wiki?curid=39098", "title": "Physical law", "text": "Physical law\n\nA physical law or a law of physics is a statement \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present.\" Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors.\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from \"physis\", the Greek word (translated into Latin as \"natura\") for \"nature\".\n\nSeveral general properties of physical laws have been identified. Physical laws are:\n\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his \"Philosophiae Naturalis Principia Mathematica\", and in Albert Einstein's theory of relativity. Other examples of laws of nature include Boyle's law of gases, conservation laws, the four laws of thermodynamics, etc.\n\nMany scientific laws are couched in mathematical terms (e.g. Newton's Second law \"F\" = , or the uncertainty principle, or the principle of least action, or causality). While these scientific laws explain what our senses perceive, they are still empirical, and so are not \"mathematical\" laws. (Mathematical laws can be proved purely by mathematics and not by scientific experiment.)\n\nOther laws reflect mathematical symmetries found in Nature (say, Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, Lorentz transformations reflect rotational symmetry of space–time). Laws are constantly being checked experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed.\n\nWell-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies can be said to generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations (see below), to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nMany fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nCompared to pre-modern accounts of causality, laws of nature fill the role played by divine causality on the one hand, and accounts such as Plato's theory of forms on the other.\n\nThe observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws \"per se\", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nIn Europe, systematic theorizing about nature (\"physis\") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\n\nFor the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's \"Natural Questions\", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of \"The World\", René Descartes described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\n\n\n"}
{"id": "166029", "url": "https://en.wikipedia.org/wiki?curid=166029", "title": "Police brutality", "text": "Police brutality\n\nPolice brutality is one of several forms of police misconduct which involves undue violence by police members.\nWidespread police brutality exists in many countries and territories, even those that prosecute it. Although illegal, it can be performed under the color of law.\n\nThe term \"police brutality\" was in use in the American press as early as 1872, when the \"Chicago Tribune\" reported on the beating of a civilian under arrest at the Harrison Street Police Station.\n\nThe origin of 'modern' policing based on the authority of the nation state is commonly traced back to developments in seventeenth and 18th century France, with modern police departments being established in most nations by the nineteenth and early twentieth centuries. Cases of police brutality appear to have been frequent then, with \"the routine of citizens by patrolmen armed with nightsticks or blackjacks\". Large-scale incidents of brutality were associated with labor strikes, such as the Great Railroad Strike of 1877, the Pullman Strike of 1894, the Lawrence textile strike of 1912, the Ludlow massacre of 1914, the Steel strike of 1919, and the Hanapepe massacre of 1924.\n\nPortions of the populations may perceive the police to be oppressors. In addition, there is a perception that victims of police brutality often belong to relatively powerless groups, such as minorities, the disabled, the young, and the poor.\n\nHubert Locke writes,\n\nWhen used in print or as the battle cry in a black power rally, police brutality can by implication cover a number of practices, from calling a citizen by his or her first name to a death by a policeman's bullet. What the average citizen thinks of when he hears the term, however, is something midway between these two occurrences, something more akin to what the police profession knows as \"alley court\"—the wanton vicious beating of a person in custody, usually while handcuffed, and usually taking place somewhere between the scene of the arrest and the station house.\nIn March 1991, members of the Los Angeles Police Department harshly beat an African American suspect, Rodney King, while a white civilian videotaped the incident, leading to extensive media coverage and criminal charges against several of the officers involved. In April 1992, hours after the four police officers involved were acquitted at trial, the Los Angeles riots of 1992 commenced, causing 53 deaths, 2,383 injuries, more than 7,000 fires, damage to 3,100 businesses, and nearly $1 billion in financial losses. After facing federal trial, two of the four officers were convicted and received 32-month prison sentences. The case was widely seen as a key factor in the reform of the Los Angeles Police Department.\n\nAccording to data released by the Bureau of Justice Statistics (2011), between 2003 and 2009 at least 4,813 people died in the process of being arrested by local police. Of the deaths classified as law enforcement homicides, 2,876 deaths occurred of which 1,643 or 57.1% of the people who died were \"people of color\".\n\nIn Vienna there tends to be an association made between Vienna's drug problem and the city's African migrants. This has led to the existence of negative cultural stereotypes which have then led to the racial profiling of African migrants, due to the negative associations with their ethnicity.\n\nThere have been a number of highly publicised incidents in Austria where police have either tortured, publicly humiliated, or violently beaten people - in some cases to the point of death. The most notorious of these incidents occurred in the late 1990s, however recent reports in 2015 show that police are still treating civilians in this way.\n\n\nThere has been a notable lack of commitment to addressing the violation of civilians' rights in Austria, with Amnesty International reporting that in 1998/1999 very few people who committed a violation of human rights were brought to justice. This was worsened by the fact that many people who made a complaint against police were brought up on counter-charges such as resisting arrest, defamation and assault. \nIn 2014–2015, there were 250 accusations of police misconduct made against officers in Vienna, and not a single person was charged - however 1,329 people were charged with 'civil disorder' in a similar time period. The Council of Europe's Committee for the Prevention of Torture (CPT)'s 2014 report included a number of complaints of police using excessive force with detainees and psychiatric patients. The culture of excusing police officers for their misconduct has continued well into the present day, and any complaints of mistreatment are often met with inadequate investigations and judicial proceedings.\n\nAustria has legislation in place which makes hate speech against anyone's race, religion, nationality or ethnicity illegal. Laws like this, which discourage discrimination, are able to help with altering public perceptions of different ethnic and cultural groups and subsequently reducing the number of racially motivated incidents of police brutality. Along with these efforts, Austria has a number of NGOs who are trying to implement programs which encourage positive cross-cultural relations, and more targeted programs such as racial sensitivity training for police. The Austrian police are also trying to find their own ways to prevent police brutality and to make the prosecution of police misconduct a smoother process.\n\nStarting in January 2016, Austrian police forces will be trialling the use of body cameras, which will be used to film their interactions with civilians. The hope is that this will make the prosecution of any officers who are excessively violent or forceful a lot easier as there will be solid evidence, and also that it will deter officers from behaving violently in the first place, as they will know they are being monitored. It is unsure how long the trial will last, however as of July 2016 it is still ongoing.\n\nIncidents of police brutality seem to still be occurring at a consistent rate, however it is yet to be seen whether the trial of body cameras will make a difference to the number of incidents occurring or to the number of police who are prosecuted for misconduct. Additionally, there needs to be more work done by the government to break down negative social stereotypes that can lead to prejudice, racial profiling and the kind of aggressive hatred which is the driving force behind many instances of police brutality - the involvement of NGOs is valuable however the Austrian government needs to take a strong stance against abuse of power by police in order for real change to happen. One way to do this, as suggested by Amnesty International Austria, would be to disband the \"Bereitschaftspolizei\", Vienna's riot police, as these officers have frequently been involved with human rights violations and situations of police brutality. Amnesty also suggest that Austria should adopt a National Action Plan against Racism (as is required by the 2001 Durban Declaration and Programme of Action) - something which they have previously refused to do. On a whole, Austria is moving slowly towards eradicating police brutality, however there needs to be much more done in order to ensure the rights of citizens are sufficiently protected.\n\nMany have been viciously beaten by police in Bangladesh. Various protesters were beaten with bats and sticks while protesting .\n\nRecently, a young man named Shamim Reja was killed by police in Sonargaon police station. The victim’s father claimed that his son was brutally tortured in the police station as the police wanted 6 lakh taka (BDT 600,000). Police investigated this and found the officer in charge Arup Torofar and SI Paltu Ghush and ASP Uttam Prashad guilty as charged.\n\nIn Shahbag, Bangladesh on January 26th, 2017, hundreds of protestors of Bangladesh India Friendship Power Company were taken into custody under extreme force by police officers. The protestors were violently mistreated. They were also struck by police officers, and had a water cannon, tear gas, and baton charges used on them.\n\nPreviously a three tier system, Belgian law enforcement now consists of two police forces operating on a federal and local level. While the two services remain independent, they integrate for purposes of recruitment and common training. This structural reform occurred in 2001 following a national parliamentary report into a series of paedophile murders which proved police negligence and severely diminished public confidence. Currently, approximately 33,000 local police and 900 civilians work across 196 regional police forces.\n\nThe United Nations (UN) Basic Principles on the Use of Force and Firearms by Law Enforcement Officials (1990) are replicated in Belgian law through The Criminal Code and the Police Functions Act. These principles dictate that use of force should be proportionate, appropriate, reported and delivered in a timely manner. However, the UN Human Rights Committee reported complaints of ill-treatment against both property and person by police escalated between 2005 and 2011, most commonly involving assault against persons no longer posing danger. Not only this, but Belgian judicial authorities failed to notify national police watchdog, Committee P, of resulting criminal convictions against police. This is a direct breach of Belgian judicial procedure, as well as a failure to comply with Article 40 of the International Covenant on Civil and Political Rights.\n\nAn extreme instance in January 2010 led to the death of Jonathan Jacob in Mortsel. The 26-year-old man was apprehended by local Mortsel police behaving strangely under the influence of amphetamines. Footage depicting how eight officers belonging to Antwerp police's Special Intervention Unit restrained and beat Jacob after he had been injected with a sedative has sparked public outrage. Jacob died from internal bleeding following the incident, but police claim they didn't make any mistakes and \"acted carefully, respecting the necessary precautions\".\n\nIn 2013, the Grand Chamber of the European Court of Human Rights (ECtHR) convicted Belgium of human rights violations in a reverse judgement on the treatment of two brothers in custody who had been slapped. The Grand Chamber voiced its concern that, \"A slap inflicted by a law-enforcement officer on an individual who is entirely under his control constitutes a serious attack on the individual's dignity\".\n\nWith cases such as these being downplayed by Belgian courts, the Belgian League of Human Rights (LDH) strive to fight police abuse through the Observatory of Police Violence (OBSPOL). Operating since 2013, OBSPOL collect testimonies on its website and create a safe space for victims of police brutality by informing them of their rights and strongly advocating to adapt public policy for victim protection.\n\nThe Police in Brazil has a history of violence against the lower classes, which dates back to the nineteenth century, when it served primarily as an instrument of control over the mass of slaves. Later with the abolition of slavery, in a largely rural country, the police forces came under strong influence of local large landowners known as 'colonels'.\n\nThroughout the second half of the twentieth century, the country experienced a strong urbanization, while over its last military dictatorship, its police forces came under the responsibility of state governments, experiencing a strong process of militarization.\n\nThe continuous militarist approach in dealing with social issues, gradually led the country to record violence levels and in 2015 Brazil have more violent deaths than Syrian Civil War, with most people fearing the police. In this context, amidst an environment entangled by corruption, Brazilian police have its routine selective brutality matching a traditional impunity.\n\nThere have been a number of high-profile cases of alleged police brutality, including 2010 G-20 Toronto summit protests, the 2012 Quebec student protests, the Robert Dziekański Taser incident, and the shooting of Sammy Yatim. The recent public incidents in which police judgments or actions have been called into question have raised fundamental concerns about police accountability and governance.\n\nOn March 16, 2014, 300 people were arrested in Montreal during a protest against police brutality.\n\nIn the recent years, Chile's police force Carabineros de Chile has been under investigation because of various cases of power abuse and police brutality, especially towards students that participate in riots for better education, and the indigenous people Mapuche, where there have been countless cases of violence to this group, guilting them for committing crimes. It was later discovered that some Carabineros officers were responsible for this crimes, to then blame Mapuches for the various incidents\n\nOne of the most recent cases involving Mapuche kill spree was Camilo Catrillanca's death. First reports of his death came from Carabineros itself, saying that Camilo was shooting at a police officer along other people after an investigation of 3 stolen cars, in which Camilo was a supposed suspect of involvement in this robbery. Carabinero's special forces Comando Jungla have been in the Araucanía Region on a mission of finding terrorists (as an excuse for killing Mapuches indiscriminately), and one of them was finding the robbers of the three cars. After finding Camilo \"attacking\" policemen with a gun in an attempt of escaping, was shot and killed by a headshot. It was later discovered that this wasn't what happened after one of the partners of the police officer that killed Camilo showed the video of the policeman killing him while he was driving a tractor. It was then asked to Carabineros why he didn't have a recording of him being shot at by Camilo. The institution responded, saying that he destroyed the SD card because it had private photos and videos with his wife. Most people were not satisfied with the answer and everyone knew it was a lie to cover what really happened. The policeman was later discharged and prosecuted. \n\nThe Constitution of Croatia prohibits torture, mistreatment and cruel and degrading punishment under Article 17, and accords arrested and convicted persons humane treatment under Article 25. Croatia has a centralised police force under the command of the Ministry of the Interior, with approximately 20,000 police officers.\n\nFrom 1991 to 1995, the Croatian police were a militarised force, charged with the role of defending the country during the secession from Yugoslavia, in addition to their regular police tasks. Military training taught police officers to use firearms before exhausting other procedures, which has affected the philosophy and behaviour of police officers in using excessive force. Significant developments have been made to achieve democratic policing in a modern, professional force that is also accountable to the public. However, citizen complaints of violent police behaviour suggest that the militarisation of the police force in the early 1990s continues to influence the level of force accepted as legitimate and reasonable by Croatian police officers.\n\nOn numerous occasions the European Court of Human Rights has found that Croatian police authorities have failed to fulfil their obligations under Article 3 of the European Convention on Human Rights and Fundamental Freedoms by failing to carry out effective investigations to protect its citizens, and tourists, from violent attacks. In 2009, the European Court of Human Rights delivered a judgement condemning Croatian police authorities for failing to take any steps to bring perpetrators of a violent attack on a Croatian citizen to justice by ignoring requests to conduct an investigation.\n\nThe Croatian police have a history of discriminatory abuse and failing to recognise violence against the ethnic minority Romani population living in Croatia. The European Commission against Racism and Intolerance has noted that Croatian police abuse against minority groups including Roma people are continually reported. There is an ongoing reluctance by police authorities to take violence against Roma seriously. Police investigations into black market selling in Croatia have been excessively violent towards Romani vendors, with reports of physical violence and abusive racism being directed at Roma. Romani Women's Association, \"Better Future\", reported in 2002 that police had beaten a pregnant Romani woman who attempted to evade arrest for black market selling.\n\nCroatian police violence has been used to intimidate refugees travelling from Serbia into Croatia. This has included segregation of nationalities, with Syrian, Iraqi and Afghani nationals gaining entry to Croatia as refugees much more easily than other nationalities. An unaccompanied sixteen-year-old from Morocco recounted his experience in attempting to gain asylum in Croatia after lying about being a Syrian national: \"We had to get into a police car…They told us this is Slovenia, but then it was Serbia…One of my friends tried to run away, but the Croatian police caught [sic] him and beat him.\" Police beatings are illustrative of the systemic discriminatory violence that exists within the Croatian police force.\n\nDenmark currently has a police force consisting of approximately 11,000 officers. These officers serve with the Danish National Police, in the 12 police districts and in the two Danish overseas territories. The Danish Independent Police Complaints Authority (Den Uafhængige Politiklagemyndighed) (the Authority) handles the investigation of police misconduct allegations. Annual statistics released by the Authority reveal a reduction in the number of complaints against police during the period from 2012 until 2015. For example, in 2012, the Authority received a total of 726 conduct complaints from across Denmark. However, in 2015, this number had fallen to 509. This represents approximately 0.05 complaints per officer. A majority of complaints extend from general misconduct, such as traffic violations and unprofessional behaviour (e.g. swearing).\n\nHowever, the 2015 Annual Report does identify some instances where the Police of Denmark have used excessive force. For example, the Authority is currently investigating a complaint made about alleged violence against an arrested person in Christianshavn on 15 March 2016. Another open investigation relates to the alleged use of force against a 16-year-old boy on 28 June 2016. This has resulted in charges being laid against the two offending police officers from the Sydsjællands- and Lolland-Falster police department. Furthermore, although examples of police brutality are not common, highly publicised incidents have been reported.\n\nIn 2002, 21 year-old Jens Arne Orskov Mathiason died while in police custody and on the way to prison. The incident raised concerns over the behaviour of the officers involved, the thoroughness of the subsequent investigation and the willingness of the Director of Public Prosecutions' to hold the officers accountable for their alleged failings. As a result, Amnesty International has called for the establishment of new mechanisms to investigate human rights violations and to enforce compliance with obligations under the European Convention on Human Rights.\n\nIn January 2016, another man died in police custody after being arrested by seven officers from the Copenhagen Police.\n\nIn August 2009, police in Copenhagen were heavily criticised for their response to an attempt to dislodge Iraqi refugees who were living in a city church. Amateur video allegedly showed the police using violence against the refugees and their supporters. Between 12,000 and 20,000 people subsequently protested against these actions.\n\nIn 2012, the Danish Court of Appeal held that the Danish Police had violated Article 3 (against abusive treatment and torture) and Articles 5, 10 and 11 (dealing with the right to liberty, the right to information about the accusation and the freedom of peaceful assembly) of the European Convention of Human Rights, when, in 2009, they had made mass arrests during protests at the 2009 United Nations Climate Change Conference in Copenhagen.\n\nIn April 2016, video emerged of officers hitting people with batons and violently detaining a man, despite onlookers saying he couldn't breathe.\n\nIn order to ensure that police are well trained and to mitigate the risk of police brutality, police recruits undergo approximately three years of training. Firstly, at the National Police College, recruits learn about police theory, the Road Traffic Act, criminal law, physical training, other legislation, first aid, radio communication, securing evidence, identifying drugs, preventing crime, management, human rights and cultural sociology to name a few. After this three-year probationary and training period, recruits are promoted to the position of police constable. By comparison, US police academies only provide an average of 19 weeks of classroom instruction. This lengthy training in Denmark increases the ability of police to effectively de-escalate conflicts and enact their duties professionally and responsibly.\n\nFurthermore, in order to keep police officers accountable and to ensure that they perform their duties in compliance with Danish, European and international laws, the Independent Police Complaints Authority has the power to handle investigations of criminal cases against police officers and decide complaints of police misconduct. This body is independent of both the police and prosecutors. By way of example, police:\nTherefore, police in Denmark are held to high standards and will face consequences if they breach their obligations. This encourages compliance. Victims of police misconduct are encouraged to lodge a report with the Authority.\n\nThe Estonian Police force ended in 1940 when they lost their independence to the Soviet Union. The Police Act which was passed in 1990, set out the dissolution of the Russian Military and re-established the formation of the Estonian Police. In 2010, the Public Order Police, Police Board, Central Criminal Police, Border Guard, Citizenship and Migration Board merged into one. Hence forming The Police and Border Guard Board. It is the currently the largest state agency in Estonia, with more than 5000 people in employment. The main objectives for this organisation is to maintain security and public order, crime prevention, detection and investigation, securing the European Union (EU) border, citizenship and identity documentation administration.\n\nAccording to the Estonian Ministry of Justice, crime figures have dropped by 10% since 2013 to 2015. Those who find themselves detained by the police should comply with their instructions. Those who experience a language barrier are allowed to \"request the presence of an interpreter and should not sign any documents or reports until they are confident that the documents contents are consistent with the details of the incident or the victim's statement\"\n\nIncidents of police abuse are very rare, however if it is witnessed, report it to the Office of Procurator General of Estonia. Although uncommon, powers are sometimes abused and hence this leads to brutality from police officers. An example of this, is the riots that took place in 2007.\nThe controversy and riots, more commonly referred to as the 'Bronze Night' that surrounded Estonia in April 2007 when the Bronze Soldier of Tallinn was relocated. The Government wanted to relocate the statue and rebury the associated remains near the Tallinn Military Cemetery; however, this led to massive uproar and protests. Historically in Estonia this Bronze Soldier served as a symbol of Soviet occupation and repression. Furthermore, to its current Russian citizens residing, it also represented Soviet's victory over Germany in World War II and their claim to equal rights in Estonia.\nDuring the riots, one Russian rioter was killed and many other protesters were arrested. Due to the overcrowded detention centres many of the detainees were taken to cargo terminals in Tallinn's seaport. Andrei Zarenkov who was the chairman of the Constitution Party stated \"people were forced to squat for hours or lie on the concrete floor with their hands tied behind their backs. The police used plastics handcuffs which caused great pain. The police selectively beat the detainees including women and teenagers. We have pictures of a toilet which is stained with blood of the injured\"\n\nThe police department denied all claims made against them. On the 22nd of May 2007, the Office of Procurator General of Estonia received more than fifty complaints on the police brutality that occurred during Bronze Night and hence opened seven criminal cases against them. In November 2007, the United Nations Committee Against Torture expressed concern over the excessive use of force and brutality by law enforcement personnel with regards to the Bronze night incident. The Council of Europe published in its report, that those detained were not granted all the fundamental safeguards. This includes the right to access a doctor, a lawyer and to inform a relative or a third party of their arrest. Furthermore, it was later discovered that accused were only allowed to contact someone and be assisted by a lawyer when brought before a judge and a number of detainees were denied access to a doctor whilst in police custody even though they displayed visible injuries.\n\nThe use of excessive force can be seen as \"legal boundaries/ duty\" in the eyes of policing agencies. Police misconduct is regarded as illegal in many countries; it can be hidden when performed under the 'colour of law'\n\nAlthough police brutality is fairly uncommon in Estonia, it is vital that the Police and Border Guard Board to keep the fundamental safeguards in check and not breach these rights, regardless of the situation.\n\nThe policing structure of nineteenth century France has been linked to the outcomes of France's reorganisation during 1789–1799 which were France's time of revolution. Throughout France's history, there has been a wide array of instances of violent enforcement stemming from issues around racial and geographic differences. Further, there have been reports conducted by the Human Rights Watch and Amnesty International concerning human rights violations by France including physical and psychological abuse as a result of excessive force towards Muslims when undertaking house raids.\n\nFrance's police ombudsman is currently dealing with 48 judicial inquiries into police brutality against its citizens, in which 1,000 individuals have been arrested, within a three-month period. Further, there have been a number of high profile cases of alleged police brutality which have gained significant media attention, including, the death of Lamine Dieng on 17 June 2007 who died after suffocating in a police van while he was constrained. The investigation of Lamine's death is ongoing, and grey areas around police accountability have come to light, including questions over how his body was covered in bruises and whether or not carotid restraint was used against him. Carotid restraint is a form of restriction, which compresses one or both carotid arteries, and is used by police enforcement to control dangerous individuals The European Court of Human Rights has condemned France in 1998 for their apparent use of carotid constriction. This same method of restraint was seen to be used against Hakim Sjimi who died of positional asphyxia as a result of overwhelming pressure being placed on his chest and neck by police.\n\nMoreover, recent protests over disputed labor laws have brought to light the extreme nature of police brutality in France, as many videos have surfaced in the media depicting police using disproportionate force on protesters. French officials have forced these aggressive videos to be destroyed, as they demonstrated the unnecessary forced nature of individuals in Frances police department.\n\nUltimately, as a result of the increased number of cases of police brutality in French communities, a group has formed called the Collective of Stolen Lives who represent families of those who have been affected by police brutality. This group strongly demand the government to act against police brutality and to reduce racism present across the police force in France.\n\nHistorically, anti-communist police brutality was commonplace during the 1920s and 1930s – in the wake of the Finnish Civil War. Some local sections of the secret police (Etsivä Keskuspoliisi) routinely beat up arrested communists.\n\nAs of 2006, there were 7700 police officers in Finland. That police force was shown to be more law-abiding than firemen. However, it was revealed there are a few dozen cases each year in which police officers are convicted of crimes committed while on duty, 5 to 10 percent of the hundreds of such crimes prosecuted annually – the number of such crimes being shown to increase yearly. Police officers are most often suspected of traffic related crimes (endangering road safety, vehicular collisions etc.) which constitute approximately 50% of all cases. These types of cases were also the most likely to be dismissed before proceeding to the prosecutor for consideration. Second most numerous category is the use of force, approximately 20%, which proceed to the prosecutor without a fail apart from few off-duty petty assaults. In Finland, a petty assault could mean a slap on the cheek.\n\nIn 2006, a 51-year-old police constable attracted a 16-year-old girl to his house by showing her his badge, where he got her drunk and raped her twice. The constable was fired and sentenced to a two-year suspended sentence. In 2007, an Iranian-born immigrant, Rasoul Pourak, was beaten in a cell at Pasila Police Station, Helsinki. The ill-treatment caused Pourak bruises all over his body, an open wound over his eyebrow, and a fractured skull. In addition, facial bones were broken and the victim was left permanently damaged. One guard participating in the assault was sentenced to an 80-day suspended prison sentence. In 2010, two police officers assaulted a man in a wheelchair in connection with an arrest. The police twisted the man's hands and pushed him backward causing him to break a femur. In 2013, two policemen were sentenced to 35 day fines for assault and breach of duty in connection with stamping on a man's head onto the asphalt thrice. According to the police, the man of Romani descent resisted, yet according to eyewitnesses, the man did not resist. The event was captured in surveillance video, which was stored but accidentally destroyed according to a third officer present. However the third officer, having seen the surveillance footage, testified that the video didn't show any resistance on the part of the Romani, but also that the assault happened out of view from the camera.\n\nWhilst Germany may be sensitive towards its history in implementing policing practices, this hasn't appeared to stop international bodies from identifying a clear pattern of police ill-treatment to foreigners and members of ethnic minorities. Every year, around 2,000 complaints of police brutality are reported, with the figure most likely being a less than accurate representation given that not every incident is reported. As high profile cases like the Cologne New Year's Eve incident become more prevalent, racist and xenophobic attitudes have been reflected in instances of police brutality. Whilst this incident occurred in 2014, high profile cases of police brutality have been reported to occur as far back as the 1990s.\n\nHistory of Police Brutality:\n\n28 May 1999: Sudanese national Aamir Ageeb died of asphyxia during his forced deportation from Frankfurt. Prior to departure, Ageeb was forcibly restrained by tape and rope. During take-off, police officers allegedly forced his head and upper body between his knees. \n8 December 2000: Josef Hoss was accused by his neighbour (a serving police officer) of harbouring firearms, which resulted in him being ambushed near his home, beaten and handcuffed. He woke up in the police station with a cloth bag over his head and had sustained multiple injuries that would prevent him from working and being able to financially support his family. No firearms were found upon investigation. \n\nMay 2002: Prior to his death, Stephen Neisius has spent 13 days in hospital on life support, after being repeatedly kicked and hit by a group of police officers as he lay handcuffed on the floor of a police station in the city. Although the Cologne District Court convicted all six police officers of bodily harm resulting in death, none of the accused served prison sentences. \n\n2012: After a fight with her boyfriend got out of hand, Teresa Z. called the police but was quickly arrested. Whilst in detention, she was punched by police officer Frank W. and left with a broken nose and eye socket. Frank W. spent ten months in jail and was forced to pay a fine of 3,000 euros.\n\nAs law enforcement is vested solely with the states of Germany, each state's police force (or 'Land' police) follows a different system of law. Accordingly, there is an absence of a federal comprehensive register, compiling and publishing regular, uniform and comprehensive figures on complaints about police ill-treatment. Even though Germany is bound to obligate its many international treaties and conventions, Amnesty International (2002) highlights the authorities failure to protect a range of human rights as guaranteed by international human rights law and standards.\n\nDespite this objective lack of accountability for policing practice, levels of trust in police remain amongst the highest in the EU, only behind Scandinavian countries and Switzerland. This allows Germany to maintain one of the lowest levels of public order and safety spending in the EU, at 1.5 percent of gross domestic profit, as compared to the EU average of 1.8 percent. As a result, Germany has a police force of only 300 officers per 100,000 of its population. These numbers are only less in Scandinavian countries and the UK, highlighting that despite these instances of police brutality, Germany is attempting to build the impression of having a more laissez-faire approach to policing. Additionally, German police officers rarely use their guns, as there have only been 8 fatalities in the past two years and only 109 deaths by service weapons since 1998.\n\nThe Greek Police, known officially as the Hellenic Police, assumed their current structure in 1984. This structure was the result of the merging of the Gendarmerie (Chorofylaki) and the Urban Police Forces (Astynomia Poleon). Composed of central and regional departments, the Hellenic Police have a relatively long history of police brutality. One of the first documented dates back to 1976, within which 16-year-old activist Sideris Isidoropoulos was killed by police while he put up campaign posters on a public building. Only a few years later and 1980 saw the death of 20-year-old protester Stamatina Kanelopoulou at the hands of the Greek police. Kanelopoulou was beaten to death by members of the police force during a demonstration to commemorate the 1973 uprising against the military junta. It is still common for protesters to commemorate the 1973 uprising, and protests are still rife with police brutality around the time of this event today, over three decades after Kanelopoulou's death.\n\nThe level and severity of police brutality in Greece over the last few years has been quite alarming and surprising. Due to recent financial crisis many austerity measures have been put in place, meaning that many individuals and families are struggling to survive. Greek citizens have opposed these austerity measures from the beginning, showing their disapproval with strikes and demonstrations. In response, police brutality has increased significantly, with consistent reports on the use of tear gas, severe injuries inflicted by police force, and unjustified detention of protesters.\n\nIn 2013 Greek police allegedly tortured four young men believed to be suspected of bank-robbery following their arrest. It was alleged that the men were hooked and severely beaten in detention. The media published photos of the men, all with severe bruising, yet in the police released digitally manipulated photos of the four to make it appear that they lacked any injuries. The Greek minister of citizen protection - Nikos Dendias - protected the police, claiming that the police needed to use photoshop to ensure the suspects were recognisable. In October 2012 15 anti-fascists protesters were arrested in Athens when they clashed with supporters of the fascist party 'Golden Dawn.' Victims claimed they were tortured during detention at the Attica General Police Directorate, stating that police officers slapped them, spat on them, burnt their arms with cigarette lighters and kept them awake with torches and lasers. Again, Nikos Dendias responded by accusing the British newspaper that published the details of these crimes of lying. It was proven by forensic examination that the torture had in fact taken place. The two Greek journalists who commented on the Guardian report the next day were fired.\n\nPolice brutality on Greece today predominantly manifests itself in the form of unjustified and extreme physical violence towards protesters and journalists. Amnesty International highlights that the continued targeting of journalists is very concerning as it infringes on the right to freedom of expression. According to a recent Amnesty International report there have been multiple instances in which police have used excessive brutal force, have misused less-lethal weapons against protesters, have attacked journalists, and have subjected bystanders to ill treatment, particularly over the course of the anniversary of the 1973 student uprising against the Military Junta, as mentioned previously, which took place on November 17, 2014. Allegations against police have emerged specifically in relation to their use of brutal force, completely unprovoked, towards journalists documenting the demonstration, and against many students who partook in a peaceful protest. Allegedly police sprayed protesters with chemical irritants from close range – in one instance a 17-year-old girl with asthma had been treated in hospital after this attack and when she informed police of her condition they merely laughed.\n\nVideo footage confirms that just days prior, on November 13, 2014, riot police began to strike students who attempted to run away from the grounds of the Athens Polytechnic. Media reports suggest that around 40 protesters had to seek subsequent medical attention to injuries sustained from brutal police beatings. Amnesty international also calls for action on prosecuting those who are behind these inhumane acts, stating that within the Greek police there is a culture of \"abuse and impunity\" which remains as authorities have taken very little action to address the crux of the problem.\n\nA German exchange student said he was beaten randomly by riot police in the Exarheia district, his only reason for being there that he had accompanied other students to eat. The student gives a horrifying description of the violence he endured, he cowered in a corner when he saw police because a few weeks before he had witnessed police beating a man they had arrested. He claims that upon spotting him, about six police officers started beating him with their batons, and when they left they were replaced by another group of police. The student was unarmed and posed no threat but the police were ruthlessly brutal in their actions. It has been indicated that riot police left beaten and gravely individuals without any medical assistance. Amnesty International urges Greece to effectively and promptly investigate these crimes against civilians, which clearly violate human rights, and hold perpetrators accountable.\n\nMay 2011, student Yannis Kafkas, suffered an almost fatal head injury after a police officer hit him with a fire extinguisher the riot police carry around. Kafkas spent 20 days in intensive care.\n\nJune 2011, Manolis Kipraios, journalist, was covering protests against austerity measures when a member of the riot police fired a stun grenade against him. He now suffers from permanent hearing loss.\n\nFebruary 2012, photojournalist Marios Lolos had to have surgery following being beaten in the head by police at a protest. The day before this attack another journalist Rena Maniou was reportedly severely beaten by security forces. Dimitris Trimis, the head of The Greek Journalist Association (ESEA) broke his arm after he was violently pushed and kicked by police.\n\nThere have been some instances where protesters have been used as human shields – a photo of a female protester in handcuffs ahead of policeman as people threw rocks at the police has gained considerable media attention.\n\nNone of the above cases of police brutality resulted in any prosecution of police force members, this severe lack of accountability and punishment for this type of crime is a major issue for human rights activists currently. One case which sparked nationwide riots was that of 15-year-old Alexis Grigoropoulos, who was shot dead by a police officer in December 2008 during demonstrations in Athens. In this case, unlike the majority of cases, the police officer responsible was convicted of murder.\n\nDuring the 2014 Hong Kong protests, there had been numerous instances of police brutality. Seven police officers had been caught on video kicking and beating a prominent political activist who was already handcuffed. There had also been more than hundreds of incidents of police beating passers-by with batons. Pictures on local TV and social media show demonstrators being dragged behind police lines, circled by police officers so that onlookers' views were blocked, and in some cases re-emerging with visible injuries. The officer involved, retired police officer Frankly Chu King-wai, was sentenced to three months in prison for causing serious body harm.\n\nIn 2008 when Hungary signed the Schengen Agreement, its two law enforcement bodies, the Police (Rendőrség) and the Border Guards merged. Border Guards became police officers. The police force in Hungary consists of the National Bureau of Investigation and the Operational police, these bodies dealing with investigating severe crimes and dealing with riots, respectively. In addition to these, Terrorelhárítási Központ, a police force with jurisdiction in all of Hungary deals mainly with counter-terrorism. 44,923 employees make up the Rendőrség force in Hungary. Brutality and corruption exist within Rendőrség.\n\nThe 1998 Human Rights Watch World Report revealed that the Roma minority in Hungary were continually discriminated against. This discrimination was also evident in the police force, with reports of police mistreatment and brutality on the minority group.\n\nThe 2006 protests in Hungary demonstrate the brutal and disproportionate measures police may use, especially evident in these protests was police brutality on non-violent civilians. The protests were in response to Prime minister Ferenc Gyurcsány's speech where he said that the Socialist Party lied their way into office. Furthermore, his speech revealed that in the four years he was in office, his party had not done anything of great importance.\n\nPolice threw gas grenades and used rubber bullets to shoot protesters. Picked and tackled by the police, protesters and non-violent civilians just passing by were injured by the police. Police broke the fingers of a handcuffed man, raided restaurants and bars to find radical demonstrators. Police brutality ranged from offensive language to physically attacking protesters. Reports show that brutality extended to bypassers, tourists, news reporters and paramedics.\n\nRather than acting reactively, Hungary should work to improve their police training programs and work to provide ongoing training and assessments to ensure that police officers in the Rendőrség, are competent and fair in their ethical judgements when it comes to proportionality of a crime or situation and the use of force. The requirements to become a police officer in Hungary are: high school education, pass matriculation exam, and two years of police academy. Compared to other countries around the world, the two-year program is shorter than Denmark (3-year program), and longer than Australia (33-week program) and the United States (18 weeks). The current two-year program is quite lengthy, however time isn't the issue. Most of what the Hungarian police academy teaches is academic theory; there is not much on practice. If practical work was given more attention in the Hungarian police academy, it is likely that the number of police brutality incidents will decrease.\n\nOn January 23, 2017, a pro-jallikattu silent protest in Tamil Nadu turned violent. The National Human Rights Commission took note of reports that police used violent methods, including beatings and the damaging of private property, without prior warnings, to disperse of the protesters in Chennai. There were widespread social media reports of police setting vehicles on fire. The \"Lathi Charge\" very well known in India which are an excessive use of force done by police during mass protests or riot are also considered as brutality done by law enforcement officials.\n\nIslamic extremists in Indonesia have been targeted by police as terrorists in the country. Police may either capture or kill dissidents. Cases of police corruption with hidden bank accounts and retaliation against journalists who attempt to uncover these cases have occurred such as in June 2012, when Indonesian magazine \"Tempo\" had journalist activists beaten by police. Separately, on August 31, 2013 police officers in Central Sulawesi province fired into a crowd of people protesting the death of a local man in police custody. Five people were killed and 34 injured. History of violence goes back to the military-backed Suharto regime (1967–1998), from which Suharto seized power during an anti-Communist purge.\n\nCriminal investigations into human rights violations by the police are rare, punishments light and Indonesia has no independent national body to deal effectively with public complaints. Amnesty International has called on Indonesia to review police tactics during arrests and public order policing, to ensure that they meet international standards.\n\nThe legacy of police brutality has long plagued Northern Ireland, due to unsavoury police procedures used during the Troubles to obtain admissions of guilt. The Troubles in Northern Ireland lasted from 1968 until 1998, and was essentially was a civil war between those who wanted Northern Ireland to remain in the United Kingdom (unionists/loyalists, who are mostly Protestants) and those who didn't (Irish nationalists/republicans, who are mostly Catholics). During this time as many as 50,000 people were physically maimed or injured, a portion of which was done by the Northern Ireland Police (Royal Ulster Constabulary). Instances of Northern Irish Police brutality were confirmed by the decision in 1978 of the European Court of Human Rights, which concluded that five interrogation techniques used by the Police which included wall standing, deprivation of food, drink or sleep, subjection to noise and forcing detainees to remain in the same position for hours were instances of cruel and degrading treatment. It was not until 2010 however that such brutality was recognized by domestic courts, where 113 people came forward to have their application heard, some of whom were minors.\n\nAt present Northern Ireland still faces policing issues, though not to the extent of the Troubles. There are concerns about harassment by police of children aged 14–18 in low socio-economic areas of Northern Ireland which has led to a deep level of mistrust between the youth and the police. Further, Catholics in Northern Ireland find that they are treated differently by police due to the police force being largely Protestant. 48% of Catholics that were surveyed in Northern Ireland reported harassment by the police. Instances of harassment include police officials spitting on individuals or enforcing laws in a discriminatory fashion, for example only to those who are Catholic. The Northern Ireland Police force has moved away from police brutality given the focus on accountability for the past and the significant decrease in the use of the baton amongst police members (guns are rarely used) however harassment continues to be a key issue for Northern Ireland.\n\nThe Republic of Ireland's police force is called the Garda Síochána (Garda) and employs around 14,500 staff. Ireland's criminal laws allow 'reasonable force' to be used by the police with regard to all the circumstances, which eludes to officers actions being proportionate in the circumstances. Excessive use of force is unlawful however s76(7) of the Criminal Justice and Immigration Act 2008 allows the following considerations when deciding on what force is reasonable.\nA person acting for a legitimate purpose may not be able to weigh up the exact necessary action at the time or may act instinctively but honestly – in these instances the use of force may be considered reasonable.\n\nThis is acknowledged by the Garda, who state; 'Unfortunately, even in the most civilised democratic jurisdictions, tragedies resulting from police use of force will continue to devastate families and communities.'\n\nThe use of force by Irish Police officers has been of international concern, where the European Committee for the Prevention of Torture reported on this issue in the Republic three times in the space of a decade. Incidents that prompted this concern centred around the death of John Carty, a man suffering from mental illness who was shot by police, the prosecution of seven Garda police members due to assaults on protesters in 2002 and in 2005, a fifteen year old boy died after spending time in Garda custody. Given this state of events the Garda engaged independent Human Rights experts to conduct a review of the force, who found numerous deficiencies. The government responded by implementing new procedures based on this report. These include a new complaints procedure available against the Garda (Ombudsman Commission), disciplinary procedures and whistle-blowing protections.\n\nWhen addressing police brutality in Latvia, it is important to look at the history of the country and how this affects its police and brutality towards the population. Latvia became an independent Republic in 1918 and attempted to develop an effective and accepted police force, moving away from the untrusted Russian Tsarist Administration. Despite positive post-independence aims to reform the police system and to maintain public order and security, the Latvian police were underfunded and under-resourced. The National Militia was created in response, being a voluntary force for the protection of public order. Policing during this period was quite successful, being assimilated to what is today referred to as community policing.\n\nFrom 1940 to 1991, Latvia was occupied by Soviet Union, and all previous regulations and practices were over-ruled by the Communist Regime. Due to Soviet ideals on policing whereby criminals were the enemy, a high level of institutional secrecy existed and meant that there was no independent review of policing. More significantly, the approach of community policing was replaced with a militarised authority based on Marxist power ideologies. During this time, a clear imbalance existed between police actions and the rights of citizens. Despite lack of statistics, it is clear that police brutality was a major issue. This is illustrated by the case where former head of police Alfons Noviks was sentenced to life imprisonment for genocide against the Latvian people during this period.\n\nIn 1991, independence of the state of Latvia was again restored, which saw another change in the police system with the implementation of the Law on Police on June 5. This saw the restructuring of police forces into separate State, Security and Local Government levels. The Law on Police 1991 reiterated ethical requirements, whereby police officers were prohibited from performing or supporting acts relating to \"torture or other cruel, inhuman or demeaning treatment or punishment\". However, despite these reforms, issues regarding police brutality arose in light of the Russian population remaining in Latvia. In 1998, police forces were accused of dispersing a rally of predominately Russian pensioners through the use of excessive force and brutality. This hostility towards Russians remained in the proceeding years, and despite lack of official statistics, police brutality continued to be an issue after the independence of Latvia.\n\nIn 2005, the Latvian Center for Human Rights and Ethnic Studies (LCHRES) found a number of instances of brutality and \"severe abuse\" within police authorities, especially of persons in custody. Reports have shown high levels of corruption within Latvian law enforcement authorities, with 42 members convicted of corruption offences between 2003 and 2004. For the Latvian community, this means that should an incident of police brutality occur, they may not have an independent body to report to nor is it guaranteed to be handled impartially without corruption.\n\nLatvian prisons illustrate cases where police batons were used to inflict serious harm to inmates, including causing broken ribs, which often were not medically assessed for up to two days. To address levels of police brutality, LCHRES conducted a study whereby it set up an anonymous hotline. During this four-day study, LCHRES received almost 300 calls and written complaints regarding police brutality and misconduct. This identifies fundamental flaws in the Latvian police authorities.\n\nSince joining the European Union in 2004, the European Committee for the Prevention of Torture (CPT) has assessed the Latvian criminal justice system a number of times. Whilst the CPT gives appropriate authorities recommendations for improvements such as a review board for ill-treatment, they found that in 2011, Latvian authorities had not enacted any of their 2007 recommendations. Furthermore, their 2011 report outlined a number of cases of police brutality within the prison system, with ill-treatment allegations such as punching, kicking and a few cases of misuse of police batons and excessively tight handcuffing. This was alleged to occur mostly at the time of apprehension or during their time at the police station (including during questioning).\n\nDespite the flaws within the Latvian Police system, CPT has found that the number of allegations for ill-treatment are decreasing over the years. The Latvian Police force operates under the Professional Ethics and Conduct Code of the State Police Personnel, which states \"A Police officer shall use force, special facilities or weapon only in the cases stipulated by due course of law and to attain a legal aim. The use of spontaneous or ill-intentioned force, special facilities or weapon shall not be justified.\" This identifies that the authorities are conscious of police brutality, and given more time, it is likely that the figures will continue to decrease.\n\nThe Luxembourg Police force has 1,603 officers and is known as the 'Grand Ducal Police'. The Grand Ducal Police is the primary law enforcement agency in Luxembourg and has been operating since January 1, 2000, when the Grand Ducal Gendarmerie (previous Luxembourg military) merged with the police force. Due to Luxembourg's relatively small population of approximately 500, 000 people the Grand Ducal Police are in charge of several duties that are often separated jurisdictions such as; Border Control and Internal Military operations.\n\nPolice brutality is not perceived to be a serious threat to society in Luxembourg. The European Union's 2014 Anti-Corruption report placed Luxembourg, along with Denmark and Finland, as having the lowest experiences of Police brutality within the European Union. Due to many positive characteristics of their society, such as freedom of media, the encouragement of public participation in the legal system and transparency mechanisms, the public also have great trust in the Grand Ducal police force.\n\nLaws in Luxembourg specifically distinguish between coercion and force in the 1973 Act on Regulating the Use of Force. This Act regulates the use of police weapons and specific technical means of physical force used by police. However, this act does not cover other forms of physical coercion by police officers such as the use of handcuffs as these are seen as basic police measures that do not require specific legislation. The officer must be legitimately executing his duty and his actions and must be compatible under the principles of proportionality, subsidiarity, reasonability and measure in order to use force. To ensure the Grand Ducal Police do not engage in police brutality numerous safeguards and prevention methods are implemented. The police inspector (which is the term for an everyday officer) must undergo legal and tactical training lasting an intensive 26 months followed by further training at an allocated police station. By way of comparison, the Victoria Police Academy only provides 33 weeks of tactical and legal training. The 2015 Human Right Report on Government practices by the United States indicated no cases of police brutality in Luxembourg. This report suggests the Grand Ducal Police have effective mechanisms in place to investigate and punish potential abuse and corruption.\n\nAlthough police brutality is almost nonexistent in Luxembourg, there are effective procedures in place for the investigation and punishment of any potential misconduct by the Grand Ducal Police.\n\nMalta's Police Force (MPF) is one of the oldest in Europe, with the Maltese government taking over the force in 1921, following the grant of self-governance. Currently, there are approximately 1,900 members in the Force.\n\nUnder the Police Act of 1961, Part V deals with the use of force, whereby \"police officers may use such moderate and proportionate force as may be necessary…\" (Article 96.), however, according to Article 100., \"It shall be considered as an offence against discipline if a police officer uses force for considerations extraneous to those permitted by law and the circumstances of the case\". As such, Malta does recognize the illegality of police brutality and can prosecute offending officials on these grounds.\n\nMalta is expected to abide by the 2001 European Code of Ethics being a member of the European Union, whereby \"The police may use force only when strictly necessary and only to the extent required to obtain a legitimate objective.\"\n\nSimilarly, the Council of Europe (of which Malta is a member) follows the five principles developed by the European Court of Human Rights, whereby definition 16 states that police officers \"may use reasonable force when lawfully exercising powers\".\n\nIn 2008, Lawrence Gonzi (The Minister for Justice and Home Affairs) called upon Mr Martin Scicluna, a former civil servant and currently expert on security issues at the Prime Minister's Office, to conduct an independent inquiry into the 24 March 2008 police brutality incident. the inquiry required the investigation of \"allegations of beatings carried out on detainees at Safi Detention Centre by members of the Detention Service on 24 March 2008 and to make any recommendations necessary in the light of [his] findings\". Following the results of the inquiry of Mr. Scicluna, made public by the Maltese Government, it was concluded that \"excessive force was used by Detention Service Personnel\".\n\nMr Scicluna made recommendations that \"appropriate [action] should be taken to reprimand the Detention Service officers involved in this operation and the relevant Senior NCOs for the acts of 25 excessive force used by some personnel in their charge\". Simultaneously, Home Affairs Minister Carm Mifsud Bonnici has said \"95 percent of the members of the police force were doing their duties, but the remainder needed to be addressed\", leading to the establishment of the Internal Affairs Unit (IAU) to \"maintain and safeguard the integrity of the Malta Police Force through an internal system of investigation that is objective, fair, equitable, impartial and just\", where complaints or allegations on the use of force can be monitored and responded to.\n\nAlthough Malta has attempted to tackle the police brutality through the implementation of independent systems such as the IAU, the US Department of State 2010 report on Malta's human rights found that \"authorities detained irregular immigrants under harsh conditions for up to 18 months during the review of their protected status.\" In addition, the 2013 US Department of State report found that although there were no government reports of the use of brutality in detention centers, on December 2, 2013, media reported the sentencing of two former prison guards to five years prison and another two guards to three months prison after finding them guilty of beating an escaped prisoner in 2008, illustrating the gradual development of the IAU in limiting the use of police brutality.\n\nFollowing the implementation of the IAU, The Human Rights Committee has raised questions on the use of force by state officials with respect to the countering of detention center riots, where police have been accused of punching and striking detainees. An enquiry was consequently conducted in 2011 and 2012 following riots, resulting in criminal action against the law enforcement officials responsible. In addition, Giacomo Santini and Tina Acketoft (The Chairs of the Migration and Equality Committees of the Parliamentary Assembly of the Council of Europe) expressed \"grave concern at an increasing number of incidents of state violence against migrants and refugees\". They have called upon Maltese authorities to conduct a rapid investigation emphasising the need to stamp out violence against migrants and refugees, whether by state actors or by individuals.\n\nThe Committee on the Elimination of Racial Discrimination in relation to the conditions of migrants in detention, recommended that the \"State party take appropriate measures to improve detention conditions and refrain from resorting to excessive use of force to counter riots by immigrants in detention centers, and also to avoid such riot\".\n\nPolice brutality was a major drive behind 2011 Egyptian revolution; the incident of Khaled Said's death and other stories, yet very little has changed since. One of the \"demands\" around which people decided to take it to the streets in Egypt is \"purging the Ministry of Interior\" for its brutality and torture practices.\n\nThe GCC states have seen many cases of brutality, some even involving senior figures. For example, Sheikh Issa bin Zayed al Nahyan a UAE sheikh, was involved in the torture of many business associates and he often recorded some of the abuse. Sheikh Issa was eventually arrested but a court found him not guilty and released him. Amnesty International has also reported that a UAE worker was subjected to a wide array of torture methods during his time in jail, including beatings and sleep deprivation. Authorities in Saudi Arabia have also been filmed lashing civilians for different reasons.\n\nThe Netherlands is signatory to the European Convention on Human Rights detailing the limits and responsibilities of police powers, and as such demonstrates a public commitment to the restricted legal use of police powers. These powers include the use of reasonable force to enable the effective discharge of duties, with the stipulation force be used proportionately and only as a last resort\n\nThe police force of the Netherlands is divided into 25 regional forces and one central force. A Regional Police Board, made up of local mayors and the chief public prosecutor, heads each regional force, with a chief officer placed in charge of police operations. Police accountability procedures include the mandatory reporting of any incident during the discharge of duty that requires the use of force. The Rijksrecherche is the national agency responsible for the investigation of serious breaches of police conduct resulting in death or injury. In 2007 the Rijksrecherche conducted 67 inquiries related to police officers, 21 of which were for shooting incidents.\n\nWhile Dutch society has a history of support for liberal values, it has not been immune to what can be regarded as a broader international trend toward the practice of racial profiling and increased levels of police violence towards racial minorities. Suspicion and mistrust of some racial groups is evident, and is perpetuated by police attitudes at all levels of command. This trend in police behaviour has drawn comment from Amnesty International, which in a 2015 report describes Dutch law enforcement officers as having a tendency to correlate suspicious criminal behaviour with specific ethnic characteristics, most notably those typical of persons of Moroccan heritage. Current political discourse in the Netherlands often supports the notion of inferiority of some cultures and is evidenced by the growth in support for far right political ideologies in recent decades.\n\nInstances highlighting the convergence of racial profiling and the use of police force came to the forefront of public attention in the Netherlands in June 2015 with the death of Aruban man Mitch Henriquez. Henriquez died of asphyxiation while in police custody after claiming to have a firearm and being arrested at a music festival in The Hague. The first anniversary of his death in June this year provided a catalyst for protest against police brutality in The Hague, an area with a significant proportion of residents of non-European back-ground. Eleven protesters were arrested for failing to comply with instructions from the Mayor to limit protest to certain areas of the city, leading to some protesters to claim authorities were attempting to criminalize the right to peaceful protest. The five officers alleged to be involved in Mitch Hendriquezs' death have been suspended but are yet to be charged.\n\n Pakistan's law enforcement is divided into multiple tiers including forces under provincial and federal government control. The law strictly prohibits any physical abuse of suspected or convicted criminals, however, due to certain training lacks, there have been reported instances of suspected police brutality. Reported cases are often investigated by police authorities as well as civil courts leading to mixed outcomes. \nRECENT CASES\n\nPolitically motivated riots and protests have occurred historically in China, notably with the Tiananmen Square protests of 1989. Within the past decade, groups such as Falun Gong have protested party measures and been broken up by riot police. Chinese dissidents have been able to arrange effective mobilization through use of social media and informal communication like Twitter and its Chinese counterparts Weibo or microblogs.\n\nForeign journalists from Switzerland have reported cases of police harassment. Media suppression has increased in the wake of the Jasmine Revolution in Tunisia. Plainclothes policemen are often deployed during demonstrations to suppress violence. Censorship is often maintained as a measure to maintain political stability in China. Web activists can be charged by the police for using false identities to surf the Internet. After arrests, homes of the arrested individual are often searched for incriminating evidence such as computers, hard drives, and flash drives.\n\nThe Polish police force aims to 'serve and protect the people, and to maintain public order and security'. Polish laws prohibit torture or degrading treatment and set out punishment for police officers including demotion and removal from the police force.\n\nA key factor influencing the levels of police brutality in Poland has been the move from a communist state to a democracy. It is argued that Poland's transition has resulted in a more transparent system, decreasing levels of police brutality. Although police brutality exists within Poland cases are much more likely to be handled by the criminal justice system with a greater chance for resolution through the courts.\n\nThis change can also been seen through the increased trust within the Polish police. While there are still instances of police brutality, trust in the police has steadily increased in Poland from 62% - 75% between 2002 and 2008. This statistic demonstrates the improvement in trust between the police and general public.\n\nAlthough there is a more open police force within Poland, many organizations still hold issues with police brutality within Poland. The 2013 United States Department of State report on Poland raised several issues of police brutality. The report cited a case of police officers using violence to gain a confession for armed robbery in 2012. However, it also noted that these police officers were eventually indicted for police brutality.\n\nIn recent years one of the main sources of controversy amongst Polish police brutality has been in the use of rubber bullets to disperse crowd trouble at sporting events.\n\nIn 1998, major riots occurred when a young basketball fan was killed by the police. In 2004, a man was killed and a woman injured in a riot when Polish police accidentally shot live ammunition instead of rubber bullets into the crowd after an association football game. Another set of riots occurred in 2015 in response to a pitch invasion during a football match. Although rubber bullets were used, one man was hit on the neck and later died at the hospital. A former Polish police officer justified this use of weapons as a means to combat football hooliganism. Protesters have characterized the detainment of sports fans protesting against the government as unfair and undemocratic.\n\nThe Polish police also have a history of police brutality within the Roma community. There are multiple cases of police beatings and other discriminatory acts against Roma from Polish police. The European Roma Rights Centre also argues that police investigations into police brutality cases are very rare with systematic police brutality against the Roma minority.\n\nOne particular case of police brutality against Roma occurred in 1998 when the police took four Roma men to a field and beat them. Whilst the men that were beaten were charged with vulgar words and behavior in public this police brutality resulted in broken bones and hospitalisation. This incident demonstrates the need for further procedures in order to stop police brutality against Roma and the continued need for police checks to stop police misconduct.\n\nPortugal is ranked the fourth most heavily policed country in the world. The police force is divided into five main organisations, with the Polícia de Segurança Pública (PSP) having the most prominent urban presence. The PSP has a diverse range of duties and responsibilities, which include protecting the rights of citizens and ensuring democratic legality.\n\nThe use of weapons by Portuguese police is permitted only when:\n\nThis is severely restrictive. By way of example, police will not be permitted to use their firearms when an offender is running away.\n\nPortugal has recently been criticised for the excessive use of force by police. High profile incidents at football matches, as well as reports of racially motivated force used against minority communities, have highlighted the issue of police brutality in Portugal.\n\nPortuguese police have adopted an aggressive position in combating football hooliganism. Despite appearing disproportionate, the police view the heavy-handed nature of their tactics as a necessary and successful approach towards community protection and maintaining social order.\n\nIn 2015, a viral video depicted a Benfica fan being heavily beaten in front of his two children outside a football stadium. The footage, filmed by a local television station, shows Jose Magalhaes leaving the football match early with his children and elderly father before being confronted by police officers. Although the family appeared calm, Magalhaes was tackled to the ground by police and repeatedly hit with a metal baton, whilst his father was punched in the face twice. More police rushed to the scene to shield the obviously distraught children, aged nine and thirteen.\n\nA statement released by the PSP acknowledged the controversial incident and announced that an investigation was launched against the officer responsible for initiating the attack. Subsequently, the officer was suspended for 90 days by the Ministry of Internal Affairs.\n\nThe statement also defended the policing of the large crowds in the aftermath of the football match. Riot police had clashed with supporters the following day in Lisbon as fans celebrated Benfica's title victory. The harsh approach was described as proportionate and necessary to prevent social disorder from escalating.\n\nIn a similar incident in 2016, another football club, Sporting Lisbon, complained about 'barbaric' police assaulting their fans.\n\nThere have also been suggestions of institutionalised racism within the Portuguese police force, with activists claiming that discrimination is the deep-rooted cause of police brutality in Portugal. In its 2015/2016 annual report on Portugal, Amnesty International condemned the excessive force used by police against migrant and minority communities.\n\nDespite a good record in migrant integration, historical parallels can be drawn with Portugal's colonial past and modern police racism. According to activists, police have killed 14 young black men since 2001, however, no police officer has been held responsible for the deaths.\n\nRacially influenced police actions are illustrated by the violence in Cova de Moura, a low socio-economic area housing a significant migrant population. Notably, during an incident in February 2015, a young man named Bruno was aggressively searched and physically abused. When bystanders protested the excessive force, police responded by firing shotguns loaded with rubber bullets at the witnesses.\n\nOn the same day, two human rights workers and five youth entered the Alfragide police station requesting information on Bruno's situation. Upon arrival, the group was allegedly attacked by police officers shouting racist slurs. The policemen dragged and kept the young men in the police station, where they detained them for two days, mistreated and mocked them.\n\nEventually 17 police officers from the Alfragide police station went to trial on a variety of charges, including physical aggression, torture, document forging and aggravated kidnapping. As of October 2018, the trial was ongoing, with victims being heard in court. \n\nThe European Commission against Racism and Intolerance (ECRI) has expressed concerns about police mistreatment of minorities in Portugal in all of its reports on the country. In its fifth country report of 2018, ECRI mentions the Alfragide case in connection to the failure of IGAI (Inspeção-Geral da Administração Interna) or officers upper in the chain of command to stop the abuses. Currently, IGAI is the body responsible for scrutinizing police activities in the country, but it is part of the Ministry of the Interior, the same way Police forces are. In its 2018 report, ECRI recommends that such work be carried out by the country's Ombudsman, an equality body, or by a new and (entirely) independent body that can be created for that purpose. \n\nPortuguese people of Roma ethnicity have also been victims of police harassment and brutality in the country. There are several examples that have been publicized by the media. One such case is from 2007 and involved a Roma man and his son. The two walked to the Nelas police station, in Porto, to get some information, but the police allegedly ended up abusing them. Two officers were convicted in 2011 for physically assaulting the father. \n\nAn example of police brutality during raids of Roma camps or neighbourhoods, is the night raid of a Roma camp site by the GNR (Guarda Nacional Republicana), in Cabanelas, Vila Verde, in 2012. Some of the people living in the camp, including children and women, were reportedly attacked by GNR officers. The six Roma detained in the violent operation allege that they were later tortured and humiliated in the GNR station of Amares; the GNR denied the accusations, while SOS Racismo promised to file a complaint against this force. A last remnant of overt institutional racism, in Portugal, is article 81 of GNR's regulation law, which provides for an increased policing of nomadic people - who, in general, are known to be mostly Roma; the regulation's constitutionality was unsuccessfully challenged in the 80's.\n\nRussian protests have gained media attention with the reelection of Vladimir Putin in 2012. Attention has been given to incidence of violence via posting videos online. President Dmitry Medvedev has initiated reforms of the police force, in an attempt to minimize the violence by firing the Moscow police chief and centralizing police powers. Police divisions in Russia are often based on loyalty systems that favor bureaucratic power among political elites. Phone tapping and business raids are common practice in the country, and often fail to give due process to citizens. Proper investigations of police officials still remains lacking by western standards.\n\nIn 2012, Russia's top investigative agency investigated charges that four police officers had tortured detainees under custody. Human rights activists claim that Russian police use torture techniques to extract false confessions from detainees. Police regulations require quotas of officers for solved crimes, a practice that encourages false arrests to meet their numbers.\n\nPolice brutality in Slovakia is systematic and widely documented, but is almost exclusively brought about against the Romani minority. The nation state itself has particularly racist attitudes toward the Romani minority, dating back prior to the split of Czechoslovakia. In fact, it is widely known that the government undertook, and still undertakes forced sterilisation of Romani women, and continues to segregate the Romani into walled-off settlements. This discrimination has undoubtedly filtered down to the police force. Excessive use of force against the Romani minority by police has been publicly criticised by the United Nations. The police force has been repeatedly condemned by a number of organisations for lengthy pre-trial detention, and its treatment of suspects in custody.\n\nIn 2001, a 51-year-old Romani man died as a result of abuse in police custody at the hands of the Mayor of Magnezitovce and his police officer son. The victim, Mr Sendrei, was allegedly chained to a radiator and fatally beaten, after being forcefully removed from his home.\nWhilst the mayor's son was immediately removed from the police force, and the mayor suspended from his position, he was reinstated just 4 months later. In response to this incident the minister for internal affairs attempted to establish new measures to prevent police brutality including mandatory psychological testing for law enforcement and better training around affective use of coercion.\n\nHowever, police brutality toward the Roma minority remains a serious issue.\n\nGraphic video footage shot by law enforcement officers in 2009 shows 6 Romani boys aged between 6-16 being forced to strip naked, kiss, and slap each other. It is alleged that the boys were then set upon by police dogs, with at least two sustaining serious injury. Officers attempted to justify their behaviour on the grounds that the boys were suspected of theft against an elderly citizen. However, cruel, inhuman or degrading treatment by police, regardless of whether a crime has been suspected or committed, is prohibited under international law.\n\nThe 10 law enforcement officers involved have since been acquitted after the judge ruled the video inadmissible in court as it was obtained illegally. As the footage was the main piece of evidentiary support for the crime, without it a conviction was not achieved.\n\nHuman rights watchdog's have raised concerns around police selectivity in making recordings of raids after a raid in the settlement of Vrbica in 2015 as they claim to have not thought the settlement would be problematic. This raid saw 15 men seriously injured.\n\nIt is often the experience of the Roma that on pressing charges in relation to police brutality, a counter- charge is often threatened by law enforcement, in an attempt to pressure the alleged victim into dropping the charges, which is particularly effective as the attitude toward the roma in Slovakia is so entrenched that lawyers are often reluctant to represent Romani victims.\n\nWhile Slovenia is a fairly peaceful country, it is not without its faults. Minority groups in Slovenia, particularly the Roma and any residents from former Yugoslav republics face discrimination and sometimes brutality by Slovenian police. The Roma, in particular, are targets because of how stereotyped they are as an inherently criminal population. The Roma often live in illegal settlements in very low socio-economic conditions, which contributes to their discrimination and their reputation as criminals. The Roma are one of the ethnic minorities from former Yugoslavic states known as 'the erased' who, after Slovenia's declaration of independence in 1991, lost all legal status, social, civil and political rights. This made them particularly vulnerable to police brutality. Their rights have still not been fully restored. Due to their lack of rights and legal status, it is difficult to hold police accountable for offences committed against them.\n\nThe police have been known to occasionally use excessive force against detainees in prisons, as well as foreigners and other minority groups, although no police officer has ever been arrested or charged. This made them particularly vulnerable to police brutality. Their rights have still not been fully restored. Due to their lack of rights and legal status, it is difficult to hold police accountable for offences committed against them. It is argued that authorities turn a blind eye to any allegations that arise because often the victims are from ethnic minorities, and there is a culture of racism amongst parts of the police force. When investigations are made, they are often ineffective. Several cases have been brought before the European Court concerning people who have died in police custody because Slovenian Police used excessive force and failed to properly investigate it. Most of these cases are still pending and there has yet to be an outcome.\n\nThe worst case of police brutality was the November 2012 protests. Political dissatisfaction spurred a series of protests in Maribor, Slovenia. For the most part, the protests were peaceful. The crowds were chanting and non-violently, and for about two hours on 26 November 2012 (also known as, \"the second Maribor uprising\") they remained that way. However, crowds moved towards an area with a heavy police presence and that's when the violence started. Police used excessive force to disperse the crowds, including tear gas, dragging and beating protesters, police dogs and even mounted police who indiscriminately charged into the crowd. Civilians, violent and non-violent protesters, and journalists alike were all targeted. Authorities attempted to justify the use of force by claiming protesters were violent and the use of force was necessary, not excessive. Slovenian media sources reported that the protest only turned violent after the police started using force. This level of violence was unprecedented and entirely unexpected in Slovenia.\n\nSince 2003, Slovenian authorities have attempted to rectify this discrimination by introducing a two-day training programme on policing in a multi-ethnic community. The programme involved teaching police about Roma culture and their language which served to break down some of the stereotypes that caused tension. The Roma were made aware of their rights, and the police were educated about national and international standards regarding treatment of minorities. The programme involved teaching police about Roma culture and their language which served to break down some of the stereotypes that caused tension. It also helped to build trust between the Roma community and police. Tensions still exist between the two groups, especially concerning police who have not participated in this programme, however they have been greatly reduced.\n\nReports of police brutality skyrocketed by over 300% in just a decade, with only one in 100 leading to a conviction. There were also 720 deaths in police custody due to police action in 2011/2012.\n\nIn 2015, due to the police committing crimes such as rape, torture and murder, the cost of civil liabilities claims were so great that there was concern the costs would strain the South African Police Service national budget.\n\nSpanish police have developed a global reputation for brutality after images of clashes between demonstrators and police were spread on social networks and international news 2011 and 2012. Two notable demonstrations are those that occurred in Barcelona on 27 May 2011, and in Madrid on 25 September 2012. Video footage made available online shows the use of force by police against peaceful demonstrators on both occasions. Images show officers using hand-held batons to repeatedly hit peaceful demonstrators, some of them in the face and neck, and the injuries caused. Police also used rubber bullets and pepper spray.\n\nHowever, in spite of public outrage the Spanish government has made no attempt to reform policing and police mistreatment of the public. On the contrary, in July 2016 new reforms to the law on Public Security and the Criminal Code came into force which limit the right to freedom of assembly and give police officers the broad discretion to fine people who show a 'lack of respect' towards them. The Law on Public Security also includes an offence of spreading images of police officers in certain cases. The UN Human Rights Commission has expressed concern at the impact this legislation could have on human rights and police accountability. Fines for insulting a police officer can be up to €600 euros, and as much as €30,000 for spreading damaging photos of police officers. Amnesty International identifies three main areas of concern about police action during demonstrations and assemblies: excessive use of force and inappropriate use of riot equipment, excessive use of force when arresting demonstrators, and ill-treatment of detainees in police custody.\n\nThe 2014 report of Torture in the Spanish State found at least 941 people were tortured by law enforcement in 2014- both in the context of demonstrations and other public situations and in police stations and prisons. 'The practice of torture is an everyday reality in Spain' claims Jorge del Cura, a spokesman for the Committee for the Prevention of Torture which collected 6621 complaints between 2004 and 2014. 'Day after day we receive information from people who have suffered all kinds of abuse and torture from stress positions, to push-ups, rape or physical assault.' There were only 752 convictions of police for mistreatment during this 10-year period. \nPau Perez, an advisor to the National Mechanism for the Prevention of Torture notes that of the torture allegations made against police 50% were from people belonging to social movements and 40% were from immigrants- indicating these are the two groups who suffer most from police brutality.\n\nAmnesty International and ACODI (Acción Contra la Discriminación) have both called out Spain for racial profiling and ethnic discrimination.\nACODI documented 612 cases of racial discrimination in a single year, emphasising that many of these did not lead to official complaints because victims fear police retaliation or believe their complaints will be ignored. This belief is not unfounded; in 2005 Beauty Solomon, an African American immigrant working as a prostitute, filed two criminal complaints against Spanish policemen for repeated harassment and physical assault. In spite of eyewitness testimony and medical reports confirming her injuries the Spanish Courts dismissed her claims on the grounds of insufficient evidence. Solomon then took her case to the European Court of Human Rights, who unanimously ruled in her favour that Spain had violated Article 3 (prohibition of inhuman and degrading treatment) and Article 14 (prohibition of discrimination) of the European Convention of Human Rights. They also condemned Spain for failing to investigate both Solomon's assault and other racist and sexist acts of violence by police officers.\n\nSolomon's case is one of hundreds of similar cases in the ACODI report. Under Spanish law the police can check the identity of anyone in a public space when there is a security concern; however African and Latin American immigrant are most frequently targeted, and often without a legitimate security concern. \"People who do not 'look Spanish' can be stopped by police as often as four times a day,\" said Izza Leghtas, an Amnesty International researcher.\n\nSince the REVA (Legally Certain and Efficient Enforcement) project has been applied in Sweden in an attempt to deport illegal immigrants, it has exposed the brutal and illegal methods used by police. They harass and racially profile non-white Swedes who often live in segregated suburbs. The marginalised such as the poor, homeless, people of colour, users of illicit drugs and the mentally ill are facing Sweden as a Police State. This has resulted in social disobedience with ordinary people in Sweden updating others on Twitter and Facebook on the whereabouts of police.\n\nIn 2013 police shot a man in his own home in front of his wife in a town called Husby. The police said the man had been wielding and threatening them with a machete. The Stockholm riots, where more than 100 cars were torched, were set off after the Husby shooting. When the police showed up they had stones thrown at them. People said the police called them 'monkeys' and used batons against them in the clash.\n\nAlso in 2013 a Swede of African origin was refused entry into a local club in Malmo for wearing traditional African clothes. The police picked him up and in the process of his arrest his arm was broken and he was locked in a cell for nearly six hours with no medical aid. Socially excluded groups have been targeted and the result of police investigations often mean the police officers are not deemed at fault.\n\nThe common denominator for people on a special police list is being or married to a Romani person. A register of 4029 Romani people is kept by police. The police say the document is a register of criminal people and their associates used for fighting crime in Skane despite people being on it that have no connection with Skane or any association with criminal people.\n\nPolice target apparent ethnicity at Stockholm subways for ID checks to see if they are illegal migrants. The police say they are 'following orders', the 'rule of law' and 'democratic process'.\n\nIn February 2016, in Malmo, a nine-year-old was accused of not paying for a railway ticket. The police asked the security guards to stop the child. One guard tackled him to the ground and sat on him. He then pushed the child's face into the pavement hard and covered his mouth. The child can be heard screaming and gasping on the video that has gone viral on the internet. The police then put him in handcuffs.\n\nTurkey has a history of police brutality, including (particularly between 1977 and 2002) the use of torture. Police brutality featuring excessive use of tear gas (including targeting protesters with tear gas canisters), pepper spray and water cannon as well as physical violence against protesters has been seen, for example, in the suppression of Kurdish protests and May Day demonstrations. The 2013 protests in Turkey were in response to the brutal police suppression of an environmentalist sit-in protesting the removal of Taksim Gezi Park.\n\nIn 2012 a number of officials received prison sentences for their role in the death in custody of political activist Engin Çeber.\n\nThe European Court of Human Rights has noted the failure of the Turkish investigating authorities to carry out effective investigations into allegations of ill-treatment by law enforcement personnel during demonstrations.\n\nIn 2015 the United Kingdom employed approximately 126,818 police officers in the 43 police forces of England, Wales and the British Transport Police, the lowest number since March 2002.\n\nThe Criminal Law Act 1967, Common Law and the Criminal Justice and Immigration Act 2008, the Police and Criminal Evidence Act 1984, and the European Convention on Human Rights (ECHR) set out the law and acceptable use of force in the UK. The use of unnecessary physical force is in principle an infringement of ECHR Article 3. The use of force should be 'reasonable' in the circumstances. Physical force is appropriate if:\n\n\nThis requires a consideration of the degree of force used. Any excessive use of force by a police officer is unlawful and an officer could thus be prosecuted under criminal law.\n\nSince 2004/05, the Independent Police Complaints Commission (IPCC) have published complaint statistics reports for England and Wales. In the 2014/15 annual report, the IPCC reported that there were 17 deaths in or following police custody and only one fatal police shooting in the last 3 years. These figures were more than doubled when the IPCC was first erected. The annual report for 2015/16 is due to be published on the 26th of July 2016. A total of 37, 105 complaints were recorded in 2014/15, marking a 6% increase to the previous year, and a 62% overall increase since 2004/05. Allegations of 'neglect or failure in duty' accounted for 34% of all allegations recorded whilst 'other assault' and 'oppressive conduct' or harassment made up only 8% and 6% respectively.\n\nDespite an average reduction in deaths in custody since 2004, a 2014 Public Confidence Survey revealed that public satisfaction following contact with the police was falling and that there was a greater willingness to complain. The Metropolitan Police, who operate in some of the most ethnically diverse parts of the UK, received the greatest number of complaints in 2014/15 with 6,828. However, young people and people from black or minority ethic groups were much less likely to come forward with complaints.\n\nWhilst instances of police brutality in the UK is comparatively less than its US counterparts, there are nonetheless high profile incidents that have received wide media coverage. As of 2016, more than 140 people from black or other minority ethnic groups have died under police custody from 1990. The use of excessive force has been used on an array of demographics of British citizens, however police brutality against ethnic and minority groups often attract wide media coverage. Whilst some have argued that this is discriminatory or evidence of institutional racism, others have asserted that it is largely due to over policing in areas that are perceived as high-risk areas such as Northumberland or Bedfordshire.\n\nIn 2009, Ian Tomlinson was killed when he was hit in the head with a baton and shoved to the ground at the G20 protests in the City of London. PC Simon Harwood was an officer of the Territorial Support Group (TSG), a unit of the Metropolitan Police Service, until he was sacked for the altercation. The incident attracted criticism of both the \"militaristic approach\" of the TSG and the small number of complaints upheld by the Metropolitan Police despite referrals by the IPCC.\n\nIn May 2013, 21-year-old Julian Cole was arrested outside a nightclub in Bedford by six police officers. The altercation left Mr Cole in a vegetative state due to a severed spinal cord. Expert evidence indicated that Mr Cole was struck with considerable force on his neck whilst his head was pulled back. Despite calls by the IPCC to suspend the officers, Bedfordshire chief constable Colette Paul refused to place the six police officers on restricted duties despite being under criminal investigation. The Bedfordshire police deny allegations that the use of excessive force on the unarmed 5 ft 5in student was race-related.\n\nOn 20 February 2014, Bedfordshire Police Constables Christopher Thomas and Christopher Pitts, chased Faruk Ali before allegedly knocking him over and punching him in the face outside his family home. Mr Ali was described an autistic man who had the mental age of a five-year-old. The police officers who were accused of laughing throughout the ordeal, were cleared of misconduct in public office by the Aylesbury Crown Court. Following an investigation by the IPCC, the officers were sacked following breaches of standards of professional conduct including standards of honesty, integrity, authority, equality and diversity.\n\nOn 13 July 2016, 18-year-old Mzee Mohammed died in police custody after being detained by Merseyside police at a Liverpool shopping centre. Officers were called to the scene after Mzee was allegedly behaving aggressive and erratic whilst arming himself with a knife. After successfully detaining Mzee, the police called an ambulance after Mzee suffered a \"medical episode\" and was pronounced dead. Video evidence has surfaced showing Mohammed surrounded by officers and paramedics, seemingly fully unconscious whilst being placed face down with his hands handcuffed behind his back. Questions remain about how appropriate medical condition could have been administered given how the handcuffs would restrict breathing. Mzee Mohammed is the 21st black person to die in police custody in six years.\n\nIn the United States, major political and social movements have involved excessive force by police, including the civil rights movement of the 1960s, anti-war demonstrations, the War on Drugs, and the Global War on Terrorism. In 2014, the UN Committee against Torture condemned police brutality and excessive use of force by law enforcement in the US, and highlighted the \"frequent and recurrent police shootings or fatal pursuits of unarmed black individuals.\" According to a 2016 report by the United Nations' Working Group of Experts on People of African Descent, \"contemporary police killings and the trauma that they create are reminiscent of the past racial terror of lynching.\"\n\nSeven members of the United States Maryland military police were convicted for the Abu Ghraib torture and prisoner abuse incidents in Iraq. Detainees were abused within the prison by being forced to jump on their naked feet, being videotaped in sexually exploited positions, having chains around their neck for photos, and being kept naked for days.\n\nThe United States has developed a notorious reputation for cases of police brutality, having reported far more incidents of killings by police officers than the rest of the western world. According to an FBI homicide report from 2012, while blacks represent 13% of the US population, they amounted for 31% of those killed by police.\n\nPolice officers are legally permitted to use force, and their superiors — and the public — expect them to do so. According to Jerome Herbert Skolnick, in dealing largely with disorderly elements of the society, some people working in law enforcement may gradually develop an attitude or sense of authority over society, particularly under traditional reaction-based policing models; in some cases the police believe that they are above the law.\n\nThere are many reasons as to why police officers can sometimes be excessively aggressive. It is thought that psychopathy makes some officers more susceptible to the use of excessive force than others. In one study, police psychologists were surveyed on officers who had used excessive force. The information obtained allowed the researchers to develop five unique types of officers, only one of which was similar to the bad apple stereotype. These include personality disorders, previous traumatic job-related experience, young inexperienced or authoritarian officers; officers who learn inappropriate patrol styles, and officers with personal problems. Schrivers categorizes groups of officers, separating the group that most likely use excessive force. However, this \"bad apple paradigm\" is considered by some to be an \"easy way out\". A broad report commissioned by the Royal Canadian Mounted Police on the causes of misconduct in policing calls it \"a simplistic explanation that permits the organization and senior management to blame corruption on individuals and individual faults – behavioural, psychological, background factors, and so on, rather than addressing systemic factors.\" The report goes on to discuss the systemic factors, which include:\n\nPolice use of force is not kept in check in many jurisdictions by the issuance of a use of force continuum. A use of force continuum sets levels of force considered appropriate in direct response to a victims behavior. This power is granted by the government, with few if any limits set out in statutory law as well as common law.\n\nViolence used by police can be excessive despite being lawful, especially in the context of political repression. Indeed, \"police brutality\" is often used to refer to violence used by the police to achieve politically desirable ends (terrorism) and, therefore, when none should be used at all according to widely held values and cultural norms in the society (rather than to refer to excessive violence used where at least some may be considered justifiable).\n\nStudies show that there are officers who believe the legal system they serve is failing and that it is their duty to pick up the slack. This is known as \"vigilantism\", where the officer involved may think the suspect deserves more punishment than what they may have to serve under the court system.\n\nDuring high-speed pursuits of suspects, officers can become angry and filled with adrenaline, which can affect their judgment when they finally assault the victim. The resulting loss of judgment and heightened emotional state can result in inappropriate use of force. The effect is colloquially known as \"high-speed pursuit syndrome.\"\n\nPolice brutality is the misuse of power by police force to intentionally harm individuals. The excessive force imposed by police officers has grown dramatically over the past decade, causing social misinterpretations of the role that police officers play in the community.\n\n\"In 2015, the percentage of people who have confidence in the police hit its lowest since 1993 at 52 percent. Of this 52 percent democrats saw the biggest drop in confidence. Democrats' confidence in police dropped 13 percentage points over the last two years compared with 2012–2013, a larger change than for any other subgroup. Over the same period, Independents' and Republicans' confidence in police has not changed. As a result, Democrats (42%) now have less confidence in police than independents (51%) and remain much less confident than Republicans (69%). Most importantly however is the number of Black people that have lost trust in the police over the last two years: Black people's confidence in police has averaged 30 percent, well below the national average of 53% and much lower than for any other subgroup. Black people's confidence is down six points from 2012 to 2013, similar to the four-point drop among all Americans.\"\n\n\"The same study found that the number of White people who identify as democrat have lost the same amount of confidence in the police as Black people, as White Democrats' confidence declined 11 points over the last two years, similar to the 14-point decline among non-White Democrats the sample sizes are not large enough to break out Black Democrats separately, but the limited data suggest their confidence declined no more than that of White Democrats. This decreased confidence in police officers is harming relations and affecting how many people deal with and respond to police which in turn leads to more hostility from police in the United States. This has also lead to many cities making police wear body armor and cameras on them at all times.\" \n\nThe repercussions of police officers’ “excessive force“ of those who find themselves in confrontation with authorities are non-existent. Authorities have legal right over how the respond and treat suspects, police officers may justifiably escalate the use of force with mere commands… but commands from who? Therefore, police officers receive little to no punishment for willingly and forcefully hurting civilians.\n\nHands Up Don’t Shoot\n\nThe Constitution states that police officers are legally allowed to shoot in the instance that they feel the need to protect their lives or an innocent life or to prevent the suspect from escaping and posing a dangerous threat to the of bystanders in society. The Supreme Court Decision of Tennessee v. Garner made it possible to shoot a fleeing suspect ONLY if they may cause harm to innocent people so that authorities are not just shoot every suspect that tries to escape.\n\n\"In the United States there are one hundred sixty million more Caucasian people than there are Black people. However, being thirteen percent of the country's population, Black people are twenty four percent of the number of people killed by cops as of 2015. Due to the increased rate of the number of Black people killed by the police there has been an increased distrust of the police in the United States.\"  \n\nSociety would like to believe that police officers and protectors are not biased towards the victims of police brutality, we hope that everything law enforcement does is to better protect us. As history repeats and more and more Black Americans lose their lives, this gives reason to believe that different geographic locations carry different political and social views, therefore police officers are biased towards those they decide to abuse, instead of allowing the justice system to properly serve justice.\n\nStereotypes\n\nLorie Fridell, Associate Professor of criminology at University of South Florida states that \"racial profiling was the number one issue facing police [in the 1990's].” Which leads her to understand two things: “bias in policing was not just a few officers in a few departments and, overwhelmingly, the police in this country are well-intentioned.” The country as a whole sets stereotypes as well as biases against Black Americans which inevitably leads to social misinterpretation of the safety of Americans when a Black person is present.\n\nAn experiment done in Mekawi during 2014 conducted on White undergraduate female students suggests that there is a higher degree of fear of racial minorities which gives reason for authorities to believe racial minorities are dangerous, hence so many shootings of minorities. The  experiment exemplifies the dehumanization and lack of empathetic concern for minorities displayed by citizen of racial majority.\n\nIncidents resulting in high profile deaths of innocent Black men such like Eric Garner in New York City, Tamir Rice in Cleveland, and Freddie Gray in Baltimore shows the Black community that they can no longer trust the police force. As a result of this lack of trust in police officers, the Black society have formulated many social organizations; founded in 2013, the Black Lives Matter Movement made a social impact on the world in a response the violent and systematic racism that Black people still face by police officers.\n\nWhile the Justice Department reported that Cleveland police officers used “excessive deadly force, including shootings and head strikes with impact weapons; unnecessary, excessive, and retaliatory force, including Tasers, chemical sprays, and their fists” on the victim, there was no real repercussions from their actions.\n\nBlack Americans V. US Police Department\n\nAnother report released concerning the Michael Brown shooting in Ferguson, Missouri, the Justice Department admits to the pattern of racial bias of the Police Department in Ferguson. The department argues that it is typically an effort to ticket as many low-income Black residents as possible in an attempt to raise local budget revenue through fines and court fees. The Justice Department explains, police encounters could get downright abusive when the person being questioned by the police officers gets disrespectful or challenges their authority.\n\nIn addition, The Department of Justice released a statement that confronts police officers’ susceptibility to implicit bias: One of the things they looked at is what they called threat perception failure. The officer believed that the person was armed and it turned out not to be the case. And these failures were more likely to occur when the subject was Black.\n\nStatistics\n\n\"In the United States in the late 2010s there has been a increase in the number of police brutality cases. The number of deaths caused by a police officer have slightly increased from 397 to 426 deaths in the last reporting year which was 2013.\"\n\nIn addition, In a study done by Research Triangle Institute in 2015, Arrest Related Deaths were ranked higher than Supplementary Homicide Reports in US deaths by approximately 4%. Police officers are killing more citizens at a higher rate than citizens are killing each other.\n\nIn the year of 2017 there were 1,147 deaths accounted for by police, 13 of which police officers were charged with a crime. 640 of the deaths caused by police officers that year, were responses to non-violent offenses and no crime was reported. 149 people killed by the police were unarmed.\n\nAlso, studies have shown that \"Black people are three times more likely to be killed by police in the United States than White people. More unarmed Black people were killed by police than unarmed White people last year,” give. the fact that only 14% of the population are Black people.\n\n\nIn England and Wales, an independent organization known as the Independent Police Complaints Commission investigates reports of police misconduct. They automatically investigate any deaths caused by, or thought to be caused by, police action.\n\nA similar body operates in Scotland, known as the Police Investigations and Review Commissioner. In Northern Ireland the Police Ombudsman for Northern Ireland has a similar role to that of the IPCC and PIRC.\n\nIn Africa, there exists two such bodies, one in South Africa and another one in Kenya known as the Independent Policing Oversight Authority.\n\nIn the United States, police are increasingly using police body-worn cameras during this Age of Ferguson. Since Michael Brown's death in Ferguson, Missouri, the US Department of Justice has made a call to action for police departments across the nation to implement body-worn cameras into their departments so further investigation will be possible.\n\nPolice brutality is measured based on the accounts of people who have experienced or seen it, as well as the juries who are present for trials involving police brutality cases. This is because there is no way to quantify the use of excessive force for any particular situation. Back is 1985 only one out of five people thought that police brutality was a serious problem. Police brutality is relative to a situation, it depends on if the suspected person(s) is(are) resisting. Out of the people who were surveyed about their account with the police brutality in 2008, only about 12% felt as if they had been resisting. Although police force itself cannot be quantified, the opinion of brutality among various races, genders, and ages can. African Americans, women, and younger people are more likely to have negative opinions about police than Caucasians, men, and middle-aged to elderly individuals.\n\nVarious community groups have criticized police brutality. These groups often stress the need for oversight by independent civilian review boards and other methods of ensuring accountability for police action.\n\nUmbrella organizations and justice committees (often named after a deceased individual or those victimized by police violence) usually engage in a solidarity of those affected. Amnesty International is another organization active in the issue of police brutality. Amnesty International, also known as AI, is a non-governmental organization focused on human rights with over 3\nmillion members and supporters around the world. The stated objective of the organization is \"to conduct research and generate action to prevent and end grave abuses of human rights, and to demand justice for those whose rights have been violated.\"\n\nTools used by these groups include video recordings, which are sometimes broadcast using websites such as YouTube.\n\nCivilians have begun independent projects to monitor police activity in an effort to reduce violence and misconduct. These are often called \"Cop Watch\" programs.\n\nProper supervision by competent police supervisors and administration can reduce police misconduct.\n\n\n\n\n"}
{"id": "36188739", "url": "https://en.wikipedia.org/wiki?curid=36188739", "title": "Qing (concept)", "text": "Qing (concept)\n\nIn Chinese philosophy, qing (情) is a concept translated variously as \"reality\", \"feelings,\" \"genuine\", \"essence\", \"disposition\", or \"emotion\". Neo-Confucians understand \"qing\" as products of environmental circumstances affecting \"xing\", or innate human nature. This interpretation of \"qing\" as an emotional or dispositional concept, especially as connected to \"xing\", arose after the Warring States period. A broader, or at least earlier, Confucian interpretation would be the behavioral quality of a person given their context. For Confucians, who emphasized cultivation of \"ren\" (humaneness), \"li\" (ritual propriety), and \"yi\" (righteousness) to build \"de\", or virtuous moral character.\n\n"}
{"id": "37090222", "url": "https://en.wikipedia.org/wiki?curid=37090222", "title": "Rebecca Jordan-Young", "text": "Rebecca Jordan-Young\n\nRebecca M. Jordan-Young (born 1963), is an American sociomedical scientist whose research focuses on sex, gender and sexuality, as well as the epidemiology of HIV/AIDS. She is the Tow Associate Professor for Distinguished Scholars and the Chair of the Department of Women's, Gender and Sexuality Studies at Barnard College.\n\nJordan-Young completed her undergraduate work at Bryn Mawr College. She earned her master's degree and Ph.D. from Columbia University.\n\nJordan-Young was a principal investigator and deputy director of the Social Theory Core at the Center for Drug Use and HIV Research of the National Development and Research Institutes. She has served as a health disparities scholar sponsored by the National Institutes of Health. In 2008, Jordan-Young was a visiting scholar in cognitive neuroscience at the International School for Advanced Studies.\n\nShe is the author of \"Brain Storm: The Flaws in the Science of Sex Differences\", a critical analysis of scientific research supporting the theory that psychological sex differences in humans are \"hard-wired\" into the brain. Jordan-Young argues that studies of “human brain organization theory,” fail to meet scientific standards.\n\nIn \"Out of Bounds? A Critique of the New Policies on Hyperandrogenism in Elite Female Athletes\", a collaborative article with Katrina Karkazis, Georgiann Davis, and Silvia Camporesi, published in 2012 in the \"American Journal of Bioethics\", the authors argue that a new sex testing policy by the International Association of Athletics Federations aimed at intersex women athletes will not protect against breaches of privacy, will require athletes to undergo unnecessary treatment in order to compete, and will intensify \"gender policing\". They recommend that athletes be able to compete in accordance with their legal gender.\n\nIn 2016, Jordan-Young was awarded a Guggenheim Fellowship to work on a book on testosterone, \"T: The Unauthorized Biography\", with co-author Katrina Karkazis.\n\n\n\n"}
{"id": "26515094", "url": "https://en.wikipedia.org/wiki?curid=26515094", "title": "Robert Black (serial killer)", "text": "Robert Black (serial killer)\n\nRobert Black (21 April 1947 – 12 January 2016) was a Scottish serial killer and paedophile who was convicted of the kidnap, rape, sexual assault and murder of four girls aged between 5 and 11 in a series of killings committed between 1981 and 1986 in the United Kingdom.\n\nBlack was convicted of the kidnapping, rape and murder of three girls on 19 May 1994. He was also convicted of the kidnapping of a fourth girl, and had earlier been convicted of the kidnapping and sexual assault of a fifth. He was sentenced to life imprisonment, with a recommendation that he serve a minimum of 35 years.\n\nBlack was further convicted of the 1981 sexual assault and murder of nine-year-old Jennifer Cardy in 2011, and at the time of his death was regarded as the prime suspect in the 1978 disappearance and murder of 13-year-old Genette Tate. Black may also have been responsible for several other unsolved child murders throughout Britain, Ireland and continental Europe between 1969 and 1987.\n\nThe nationwide manhunt for Black was one of the most exhaustive UK murder investigations of the 20th century. He died in prison in 2016.\n\nRobert Black was born in Grangemouth, Stirlingshire, on 21 April 1947, the illegitimate child of Jessie Hunter Black and an unknown father. His mother originally planned to have him adopted before she emigrated to Australia to escape the stigma of his birth. He was not adopted, and at six months old was placed with an experienced, middle-aged foster couple in Kinlochleven named Tulip. He adopted their surname.\n\nBlack showed antisocial tendencies and became known as an aggressive child with few friends. He was prone to tantrums and vandalised school property. He was also a target for bullying among children his own age, and became a bully towards younger children. Though his foster mother insisted upon cleanliness, he cared little for his own hygiene and was called \"Smelly Bobby Tulip\" by classmates.\n\nAt the age of five, Black and a girl the same age compared their genitalia, triggering a childhood belief within Black that he should have been born female, and he developed a deep interest in his genitalia, the genitals of female children, and body orifices. From the age of eight he would regularly insert objects in his own anus, a practice he carried into adulthood.\n\nLocals later recalled seeing bruises on Black's face and limbs, suggesting he had been physically abused by his foster parents. Black stated he could not recollect their origin, and they may have resulted from childhood fights. Despite being adamant he could not recall the origin of these bruises, Black was a chronic bed wetter, and freely admitted to being berated and beaten by his foster mother for each offence.\n\nBy 1958 the Tulips had both died, and he was placed with another foster family in Kinlochleven. He soon committed his first known sexual assault, dragging a young girl into a public lavatory and fondling her. His foster mother reported the offence and insisted he be removed from her home.\n\nBlack was placed in a mixed-sex children's home on the outskirts of Falkirk. Here he regularly exposed himself to girls, and on one occasion, he forcibly removed the underwear of a girl. As a result, he was sent to Red House Care Home, a high-discipline, all-male establishment in Musselburgh. At this new location Black was sexually abused by a male staff member for three years; typically by being forced to perform fellatio. During this time he studied at Musselburgh Grammar School, developing an interest in football and swimming. Other students recall him as taciturn, with few friends.\n\nIn 1963, Black left the Red House Care Home. With assistance from child welfare agencies, he moved to another boys' home in Greenock and obtained a job as a butcher's delivery boy. He later said that he had fondled thirty to forty young girls while making deliveries if, upon calling at the house, he discovered young girls were alone in the premises. None of these incidents seem to have been reported.\n\nOn a summer evening in 1963, Black encountered a seven-year-old girl playing alone in a park; he lured the child to a deserted air-raid shelter on the pretext of showing her some kittens. There he held the girl by the throat until she lost consciousness, then masturbated over her body. The following day, Black was arrested and charged with lewd and libidinous behaviour. A psychiatric examination suggested the incident was an isolated one, and that Black was not in need of treatment; as a result he was admonished for the offence.\n\nShortly after, Black moved to Grangemouth, where he lodged with an elderly couple and worked for a builders' supply company. He began dating a young woman he met at a local youth club. This was his only known girlfriend, and they dated for several months. According to Black, he had asked this woman to marry him, and was devastated when she abruptly ended their relationship, in part due to his unusual sexual demands.\n\nIn 1966 Black's landlords discovered he had molested their nine-year-old granddaughter whenever she visited their household. They evicted him but did not inform police, wanting to spare their granddaughter further trauma. Black lost his job soon after, and he returned to Kinlochleven, where he lodged with a married couple who had a six-year-old daughter.\n\nWithin a year, Black's new landlords informed police that he had repeatedly molested their daughter. He pleaded guilty to three counts of indecent assault against a child. He was sentenced to a year at Polmont Borstal in Brightons, which specialised in training and rehabilitating of serious youthful offenders. Although he later spoke freely about every aspect of his youth and adolescence—including the sexual abuse he had suffered at the Red House Care Home—he refused to discuss Polmont Borstal beyond saying he had vowed to never again be imprisoned; this has led to speculation that he may have been brutalised there.\n\nIn September 1968, six months after his release from Polmont Borstal, Black moved to London, where he initially found lodgings in a bedsit close to King's Cross station. Between 1968 and 1970, he supported himself through various—often casual—jobs. One of these was as a lifeguard at a Hornsey swimming pool, where he was soon fired for fondling a young girl; no charges were brought.\n\nVia a contact he had met at a King's Cross bookshop, Black began to collect child pornography. Initially, much of this material was in magazine and photograph format, although he later expanded this material to include videos depicting graphic child sexual abuse. As Black was a keen photographer, he sometimes also discreetly photographed children (mostly girls between eight and twelve) at locations such as swimming pools; he stored these images alongside his pornographic material in locked suitcases.\n\nBlack frequented the Three Crowns, a Stamford Hill pub, where he became known as a proficient darts player. There he also met a Scottish couple, Edward and Kathy Rayson. In 1972, he moved into their attic. The Raysons considered Black a responsible if somewhat reclusive tenant who gave them no cause for complaint beyond his poor hygiene. They suspected Black of viewing pornographic material, but had no idea it might be paedophilic. Black remained their lodger until his arrest in 1990.\n\nTo increase his scope for casual work, in the mid-1970s Black bought a white Fiat van to enable him to commit to driving for a living. In 1976, Black obtained a permanent job as a van driver for Poster, Dispatch and Storage Ltd, a Hoxton-based firm whose fleet delivered posters—typically depicting pop stars—and billboard advertisements to locations across the UK, Ireland and continental Europe. To his employers, Black was a conscientious employee who was willing to undertake the long-distance deliveries some of his married co-workers disliked.\n\nWhile working as a driver, Black developed a thorough knowledge of much of the UK road network, subsequently enabling him to snatch children across the entire country and dispose of their bodies hundreds of miles from the site of their abduction. To reduce the chance of being identified by eyewitnesses, Black often adjusted his appearance by alternately growing a beard or appearing clean-shaven, and occasionally shaved his head completely bald. Black also owned over a dozen pairs of spectacles, and would wear a pair significantly different from those he regularly wore when abducting children. He also covered the rear windows of his van with opaque black curtains.\n\nThe first murder Black is proven to have committed was that of nine-year-old Jennifer Cardy, who was abducted, sexually assaulted and murdered on 12 August 1981. Cardy was last seen by her mother at 1:40 p.m. as she cycled to a friend's house in Ballinderry, County Antrim; she never arrived.\n\nHours later, Cardy's bicycle was discovered less than a mile from her home, covered with branches and leaves. The stand of the bicycle was down, suggesting that she had stopped her bicycle to converse with her abductor. A search aided by 200 volunteers, found nothing further.\n\nSix days later, hunters discovered Cardy's body from her home, in a reservoir near a lay-by in Hillsborough. A pathologist noted signs of sexual abuse on Cardy's body and underwear; the autopsy concluded she had died of drowning—most likely accompanied by ligature strangulation. The watch she had been wearing had stopped at 5:40 p.m.\n\nThe location of the body near a major arterial road between Belfast and Dublin led police to suspect her murderer had been familiar with the area. The reservoir she had been found in was near a route frequented by long-distance delivery drivers, suggesting that the killer may have travelled extensively.\nBlack's second confirmed victim was 11-year-old Susan Claire Maxwell, who lived in Cornhill-on-Tweed on the English side of the Anglo-Scottish border. Maxwell was abducted on 30 July 1982 as she walked home from playing tennis in Coldstream. She was last seen alive at 4:30 p.m., crossing the bridge over the River Tweed, and was likely abducted by Black shortly after.\n\nThe following day a search was mounted. Search dogs were used, and at peak 300 officers were assigned full-time; a thorough search was made of every property in both Cornhill and Coldstream and over 80 square miles of terrain. Several people reported having seen a white van in the locality; one said a van had been parked in a field gateway off the A697.\n\nOn 12 August, Maxwell's body was found by a lorry driver; her body was covered with undergrowth, and was clothed save for her shoes and underwear. The precise date and cause of her death could not be determined due to decomposition. Maxwell had been bound, and gagged with sticking plaster, and her underwear had been removed and folded beneath her head, suggesting that she had been sexually assaulted.\n\nA coroner's inquest concluded Maxwell had died shortly after being abducted. Evidently, Maxwell remained in Black's van—alive or dead—for over 24 hours, as his delivery schedule encompassed Edinburgh, Dundee, and finally Glasgow, where he made his final delivery close to midnight on 30 July. The following day, Black returned from Glasgow to London, discarding the body in a copse beside the A518 road near Uttoxeter, from where Maxwell had been abducted.\n\nFive-year-old Caroline Hogg, Black's youngest known victim, disappeared while playing outside her Beach Lane home in the Edinburgh suburb of Portobello in the early evening of 8 July 1983. When she failed to return home by 7:15 p.m., her family searched the surrounding streets. A boy told them he had seen Caroline with a man on the nearby promenade, which they searched before calling Lothian and Borders Police.\n\nThe ensuing search was the largest in Scottish history at that time, with 2,000 local volunteers and 50 members of the Royal Scots Fusiliers searching first Portobello, then expanding their scope to all of Edinburgh. By 10 July, Hogg's disappearance was headline news across the UK. Nine known paedophiles were identified as having been in Portobello on 8 July; all were eliminated from the inquiry.\n\nNumerous eyewitnesses had seen an unkempt, balding, \"furtive-looking\" man wearing horn-rimmed glasses, watching Hogg as she played; then following her to a nearby fairground. En route, a 14-year-old girl named Jennifer Booth had seen Hogg sitting with this man on a bench. Booth overheard Hogg reply \"Yes please\" to some question posed to her by the man before the two walked to the fairground holding hands.\nThere, the man paid for Caroline to ride a carousel as he watched. A witness stated to police that as they left, Caroline seemed frightened.\n\nHogg remained in Black's van for at least 24 hours. Black delivered posters to Glasgow several hours after the abduction, and refuelled his van in Carlisle early the following morning.\n\nOn 18 July, Hogg's naked body was found in a ditch close to the M1 motorway in Twycross, from where she had been abducted and just from where Maxwell's body had been found the previous year.\nThe precise cause of death could not be determined due to the extent of decomposition. Insect activity suggested the body had been placed where it was found on or after 12 July; Black had made a delivery to Bedworth on that date. The absence of clothing again suggested a sexual motive.\n\nThe following March, a televised reconstruction of the abduction was broadcast nationally. Appealing for witnesses to come forward, Hogg's father said: \"You think it can never happen to you, but it has proven time and time again that it can, and it could again if this man isn't caught in the near future.\"\n\nAfter the discovery of Hogg's body, a conference of senior Staffordshire and Leicestershire detectives unanimously concluded that Hogg's and Maxwell's murderers were the same person, to a large degree because of the distance between the abduction and discovery sites. (Cardy's murder was not linked to this series until 2009.)\n\nDue to the distances involved, police suspected that the murderer of Maxwell and Hogg worked as a lorry or van driver, or a sales representative, which required him to travel extensively to locations which included the Scottish Borders. Both girls had been bound and likely subjected to a sexual assault prior to the murders, and each had been wearing white ankle socks at the time of her abduction, which may have triggered a fetish in the perpetrator's psyche. Due to the geographical and circumstantial nature of the offences, the killer was most likely an opportunist.\n\nBased upon the day of the week when Maxwell and Hogg had been abducted (a Friday), the killer was likely tied to a delivery or production schedule. Following the August 1982 discovery of Maxwell's body, numerous transport firms with links between Scotland and the Midlands of England were contacted, and drivers were questioned about their whereabouts on the date of her abduction. This line of inquiry was repeated following the discovery of Hogg's body, but in both instances failed to yield results.\n\nDespite frustration at the lack of a breakthrough in their search for the murderer, there was complete cooperation between the detectives from the four police forces involved in the manhunt. Initially, a satellite incident room in Coldstream coordinated the efforts of the forces involved in the hunt for Maxwell's killer, with incident rooms in Leith and Portobello coordinating the search for Hogg's; within hours of Hogg's body being discovered, the chief constables of all forces now involved in investigating the murders agreed to appoint a senior investigating officer to coordinate the inquiries. Hector Clark, the assistant chief constable of Northumbria Police, took overall charge of the investigation. Clark established incident rooms in Northumberland and Leith police stations, to liaise between the four police forces involved.\n\nAll information relating to both child murders was initially logged within a card filing system, which contained 500,000 index cards relating to the Maxwell case alone. Mindful of the criticisms of the recent investigation into the Yorkshire Ripper, which had become overwhelmed due to the volume of information filed in a card filing system, one of Clark's first decisions upon taking overall charge of the murder investigation was to introduce computer technology into the investigation; he and other senior officers agreed that the most efficient way to cooperate in an investigation of this scope was to collate their information on the Hogg murder into a computerised database, which all forces involved in the manhunt could access. Information relating to Maxwell's murder was also later entered onto this database.\n\nBy January 1987, all information relating to the murders initially linked to Black was entered into the newly established HOLMES information technology system, with the £250,000 cost to implement it provided by the Home Office. Information continued to be entered into the database, and police forces nationwide could cross-check all data fed into this system. This database—based at the Child Murder Bureau in Bradford—expanded to hold information upon over 189,000 people, 220,000 vehicles, and details of interviews held with over 60,000 people. Much of the information came through three confidential hotlines established in 1984. As a result of the investigation into the killings, several unrelated crimes, including offences relating to child abuse, were solved.\n\nAt about 7:50 p.m. on 26 March 1986, 10-year-old Sarah Jayne Harper disappeared from the Leeds suburb of Morley, having left her home to buy a loaf of bread from a corner shop 100 yards from her home. The owner of the shop confirmed that Harper had bought a loaf of bread and two packets of crisps from her at 7:55 p.m., and that a balding man had briefly entered the shop moments later, then left as Harper made her purchases.\n\nSarah Harper was last seen alive by two girls walking into an alley leading towards her Brunswick Place home; when she had not returned by 8:20 p.m., her mother, Jackie, and younger sister, Claire, briefly searched the surrounding streets, before Jackie Harper reported her daughter missing to West Yorkshire Police. Immediately, an extensive search was launched to find the child. Over 100 police officers were assigned full-time to the search, which saw house-to-house inquiries across Morley, over 3,000 properties searched, more than 10,000 leaflets distributed, and 1,400 witness statements obtained. A police search of the surrounding land was bolstered by 200 local volunteers, and a reservoir in nearby Tingley was searched by underwater units.\n\nExtensive inquiries by West Yorkshire Police established that a white Ford Transit van had been in the area where Harper had been abducted. Two suspicious men had been seen loitering near the route Harper would have taken to the corner shop, and one of them was stocky and balding. Mindful of the possibility Harper had been abducted and murdered, West Yorkshire Police dispatched a telex to all forces nationwide, requesting that they search all locations where they had previously discovered child murder victims.\n\nAt a press conference on 3 April, Sarah's mother, Jackie, informed journalists that she feared her daughter was dead, and that the worst torment she and her family endured was the uncertainty. She made a direct appeal to her daughter's abductor to reveal the whereabouts of the body. On 19 April, a man discovered Sarah's partially dressed, gagged and bound body floating in the River Trent near Nottingham, from the site of her abduction. An autopsy showed she had died between five and eight hours after her she was last seen alive, and that the cause of her death was drowning; injuries she had received to her face, forehead, head and neck had most likely rendered her unconscious prior to being thrown into the water. Harper had also been the victim of a violent and sustained sexual assault prior to being thrown into the river, causing pre-mortem internal injuries which were described by the pathologist as \"simply terrible\".\n\nDays after Harper's body had been found, a further witness contacted West Yorkshire Police to say that at approximately 9:15 p.m. on 26 March, he had seen a white van with a stocky, balding man standing by the passenger door, parked close to the River Soar. As the Soar is a tributary to the Trent, and the description of the vehicle and driver matched those obtained from Morley residents, investigators took this eyewitness account seriously. Black refuelled his van in Newport Pagnell the following afternoon, and it is likely that he had driven Harper to Ratcliffe on Soar, and discarded her body in the Soar in the late evening of the date of her abduction, or the early hours of the following day.\n\nRealising the likelihood that Harper's murderer had travelled on the M1 motorway prior to disposing of her body in the river, and that he would have had to refuel his vehicle as he made this journey, officers from both West Yorkshire and Nottinghamshire Police questioned staff and motorists at all service stations on the M1 motorway between Woolley, West Yorkshire and Trowell, Nottinghamshire, asking whether they had noted anything unusual on 26 or 27 March. Staff at one station had noted a white Transit van which had seemed out of place on the evening of 26 March, but could not give a clear description of the driver.\n\nDetective John Stainthorpe, head of the Leeds South Division of West Yorkshire Police, initially stated his doubt that Harper's disappearance was linked to those of Maxwell and Hogg: in one interview, he said that although he would not discount the possibility, he believed that Harper's abductor had close, personal connections with Morley. Upon the discovery of her body in the River Trent, he revised his opinion.\n\nNumerous similarities linked the murder of Sarah Harper to those of Maxwell and Hogg: she had been a prepubescent, white female, abducted from Northern England and found murdered in the Midlands. All three victims had been discovered within of Ashby-de-la-Zouch, with little effort being made to conceal the bodies.\n\nDespite these similarities, several investigators initially doubted whether Harper's murder should be linked to the series due to the differences in the circumstances of her abduction and the fact that the child had been subjected to a serious sexual assault prior to her murder, whereas decomposition had erased any such clear traces on the bodies of the two previous victims. Harper had been abducted on a rainy Wednesday evening from a suburb in the north of England, wearing a hooded anorak covering much of her face, as opposed to being abducted on a summer Friday afternoon in southern Scotland while wearing summer clothing. Investigators remained open-minded as to whether Harper's murder had been committed by the same person, and telephone and computer connections were established between the incident room in the Leeds district of Holbeck and Leith. Harper's murder was formally linked to the series in November 1986.\n\nFollowing the murder of Sarah Harper, with six police forces now involved in the hunt for the offender, the police forces involved in the manhunt agreed that Hector Clark (by this time Detective Chief Constable of Lothian and Borders Police) should maintain overall command of the investigation. Clark created a new headquarters in Wakefield to act as a liaison between the six forces.\n\nOn 21 April 1986, the head of Scotland Yard's Criminal Intelligence Branch, Phillip Corbett, hosted a summit meeting at Scotland Yard, to discuss how best to share information between the forces involved in the manhunt, and to investigate potential links with 19 other unsolved child murders. Senior officers attended from 16 UK police forces. At this stage, the inquiry had cost in excess of a million pounds.\n\nOne of the outcomes of this meeting was that investigators contacted the FBI to request that they compose a psychological profile of the murderer for UK investigators. The FBI completed this profile in early 1988.\n\nFor the HOLMES database, investigators concluded only those with convictions for serious sexual offences against children would warrant further investigation. Those to be checked were to have been convicted of child murder, child abduction or attempted child abduction, or the indecent assault of a child. Every police force in the UK was asked to check their databases for people who had received convictions for any of these offences within 10 years of the 1982 murder of Susan Maxwell. This narrowed the number of people to be checked to 40,000 men, and Black's name was not on the list, as his sole conviction had been in 1967.\n\nIn January 1988, the UK investigators received the psychological profile of the killer from the FBI. This profile described the killer as a white male aged between 30 and 40 (likely closer to 40), who was a classic loner. This offender would be unkempt in appearance, and had received less than 12 years of formal education. He likely lived alone, in rented accommodation, in a lower-middle class neighbourhood. This profile also deduced that the motive for the child killings was sexual, that the offender held a fixation with child pornography, that he retained souvenirs from his victims, and he most likely engaged in necrophilia with his victims' bodies shortly after their death, before disposing of them.\n\nOn 23 April 1988, an attempted abduction of a teenage girl occurred in the Nottingham district of Radford which was not initially deemed by Nottinghamshire Police to be linked to the three child killings, and thus remained unreported to Clark or senior investigators in the national manhunt, despite the fact that all chief constables across the UK had been requested to report incidents of this nature to the inquiry team. The victim of this attempted abduction was Teresa Thornhill, a 15-year-old who was tall, which may have led Black to think she was younger than she was.\n\nThat evening, Thornhill had been at a social gathering in a local park with her boyfriend, Andrew Beeston, and other teenagers, before walking home with Beeston. The pair had parted company at the end of Norton Street when Thornhill noted a blue Transit van slowing to a stop ahead of her; the driver of this van then got out, raised the van's bonnet and asked Thornhill, \"Can you fix engines?\" When Thornhill replied that she could not and began walking at a much brisker pace, Black clasped his arms across her mouth and navel and attempted to drag her into his vehicle.\n\nThornhill resisted him: writhing and kicking as she attempted to free herself from what she later described as his \"bear hug\" grasping of her body. As her would-be abductor wrestled her to his van, Thornhill squeezed his testicles, causing him to loosen his grasp sufficiently enough for her to bite into his right forearm. Black shouted, \"Oh! You... bitch!\", as Thornhill began to scream for her mother, wedging her feet on each side of the door frame as she struggled to resist being forced into the van. At the same time, Beeston ran towards the van shouting, \"Let go of her, you fat bastard!\" Upon hearing this, Black loosened his grip on Thornhill, who fell into the road, sobbing. Black got into the driver's seat of the van and drove off.\n\nBoth Thornhill and Beeston ran to Thornhill's home and informed her parents what had occurred; they immediately reported the attempted abduction to Nottinghamshire Police, who questioned both youngsters. Both Thornhill and Beeston described her would-be abductor as an unkempt, overweight, balding and heavily built man aged between 40 and 50, and about in height.\n\nBlack was arrested in Stow on 14 July 1990. David Herkes, a 53-year-old retired postmaster, was mowing his front garden when he saw a blue Transit van slow to a standstill across the road. The driver exited the van—ostensibly to clean his windscreen—as the six-year-old daughter of Herkes' neighbour passed his field of view. As Herkes stooped to clear cuttings from his lawnmower, he saw the girl's feet lifting from the pavement; he then straightened himself to observe the vehicle's driver hastily pushing something through the passenger door before clambering across to the driver's seat, closing the passenger door, and starting the engine.\n\nRealising he had witnessed an abduction, Herkes noted the registration number of the van as it sped away. Herkes ran to the girl's home; the girl's mother called police.\n\nWithin minutes, six police vehicles had arrived in the village. As Herkes described the van to officers, he observed it driving in their direction and exclaimed, \"That's him! That's the same van!\" An officer jumped in the van's path, forcing it to halt. Police removed the driver from his seat and handcuffed him.\n\nOne of the officers, who was the father of the abducted girl, opened the rear of the van and clambered inside, calling his daughter's name. Seeing movement in a sleeping bag,}} he untied its drawstring to discover his daughter inside, her wrists bound behind her back, her legs tied together, her mouth bound and gagged with sticking plaster, and a hood tied over her head.\n\nEn route to Selkirk police station, Black said: \"It was a rush of blood to the head; I have always liked little girls since I was a lad. I tied her up because I wanted to keep her until I had dropped a parcel off. I was going to let her go.\" Black claimed he had interfered with his victim only \"a little\". A doctor found the victim had been subjected to a serious sexual assault.\n\nThe girl was able to pinpoint the lay-by on the A7 where Black had sexually assaulted her. Black's intention had been to quickly make a final scheduled delivery to Galashiels before further abusing and almost certainly killing his victim.\n\nAt Selkirk police station, Black admitted to sexually assaulting the girl, saying he had not done more to her because he \"didn't have much time\". Black was charged with plagium, and held on remand. As Black awaited a scheduled 16 July Selkirk Sheriff court appearance, the detective superintendent—noting the similarities between the Stow abduction and the three child killings—notified Hector Clark of Black's arrest. On 16 July, Clark travelled from Wakefield to interview Black at Edinburgh's St Leonards police station. Although Black's answers in this brief interview were largely monosyllabic, Clark left feeling that Black was the man he had sought since 1982. At Black's initial remand hearing he was ordered to stand trial at Edinburgh High Court for the abduction of the Stow girl; he was then transferred to Saughton Prison.\n\nA search of Black's van found restraining devices including assorted ropes, sticking plaster, and hoods; a Polaroid camera; numerous articles of girls' clothing; a mattress; and a selection of sexual aids. Black claimed that on his long-distance deliveries he would pull into a lay-by and dress in the children's clothing before masturbating; he gave no plausible explanation for the sexual aids.\n\nBlack's Stamford Hill lodgings were searched at the request of Scottish detectives, yielding a large collection of child pornography in magazine, book, photographic and video format, including 58 videos and films depicting graphic child sexual abuse which Black later claimed to have bought in continental Europe. Also found were several items of children's clothing, a semen-stained copy of a Nottingham newspaper detailing the 1988 attempted abduction of Teresa Thornhill, and a variety of sex aids.\n\nBlack's appointed defence lawyer, Herbert Kerrigan QC, and the Edinburgh procurator fiscal both ordered psychiatric evaluations of Black, which were undertaken by prominent psychiatrists. Both reports were uncompromising regarding Black's deviancy and proclivities towards children. Black told Kerrigan he intended to plead guilty to the abduction charges.\n\nOn 10 August 1990, Black was tried for the abduction and sexual assault of the Stow schoolgirl. He was tried at the Edinburgh High Court before Lord Donald MacArthur Ross. The trial lasted one day.\n\nIn his opening statement, Kerrigan stated his client would plead guilty to all charges. The Lord Advocate of Scotland, Lord Fraser of Carmyllie QC then outlined the facts of the case, terming the implements found in Black's van a clear sign of premeditation, and citing a medical expert's testimony that the girl would likely have suffocated within 15 minutes had she not been rescued.\n\nTestimony was given that Black drove his victim to a lay-by to sexually abuse her, then returned through the village. In a statement read to the court, the victim stated she \"didn't know he [Black] was a bad man\" as Black had stared at her before bundling her into his van.\n\nIn rebuttal, Kerrigan asserted again that the abduction had been unplanned, and that Black had intended to release the girl after assaulting her. He pointed out that Black freely admitted his paedophilic preferences, and claimed to have successfully fought against the urge to abduct young girls prior to the incident at issue. He also said that Black accepted that he was a danger to children and wished to undergo treatment.\n\nPrior to sentencing, Ross paid tribute to Herkes, whose vigilance had led to Black's arrest. Sentencing Black to life imprisonment for what he described as \"a horrific, appalling case\", Ross said he was greatly influenced by the opinion of the psychiatrists, who had concluded that Black was, and would remain, an extreme danger to children.\n\nIn September 1990, Black announced his intention to appeal his life sentence, but he later abandoned this. In November 1990, he was transferred to Peterhead Prison.\n\nTwo weeks after the Stow trial, Clark conducted a second, recorded interview with Black. He had appointed Andrew Watt and Roger Orr to conduct the interview, with instructions they were to tell Black that they would not be judgemental.\n\nIn the six-hour interview, Black freely discussed his early sexual experiences, his experimentation with various forms of self-abuse, and his attraction towards young children; he also described his penchant for wearing young girls' clothing, and admitted to having sexually assaulted in excess of 30 young girls between the 1960s and 1980s. He was largely uncommunicative in response to questions even loosely pertaining to any unsolved child murders and disappearances, but said he had enticed two young girls into his van in Carlisle upon the pretext of asking for directions in late 1985, then allowed them to leave when eyewitnesses appeared.\n\nThe latter stages of this interview saw both men steer their questioning directly to the subject of child abduction and murder, specifically in relation to the murder of Caroline Hogg. Informing Black that police had already established he had been in Portobello on the date of Hogg's abduction, Watt and Orr then tacitly informed him they had eyewitness accounts and petrol-station receipts, further proving that he was near Portobello on the date of Hogg's abduction. Orr then produced a composite drawing of the man with whom Hogg had left the funfair, and placed this composite alongside photographs of Black dating from the early 1980s—highlighting their similarities. Black's replies then became evasive and monosyllabic. Asked directly, at the end of the interview, to confess in order to end the suffering of his victims' families, he did not respond.\n\nDespite the fact the information gleaned in this interview did little to advance the murder inquiry, upon the conclusion of the interview, Clark informed his two colleagues: \"That's our man. I'd bet my life on it.\"\n\nDetectives from all forces in the UK linked to the joint manhunt then began an intense and painstaking endeavour to gather sufficient evidence to convince the Crown Prosecution Service to instigate legal proceedings against Black, with a reasonable chance of securing convictions. As was his legal right, Black refused to cooperate with the detectives in their investigation.\n\nInvestigators contacted Poster, Dispatch and Storage Ltd to establish whether travel records could confirm his whereabouts on crucial dates linked to the investigation. Staff at this firm were able to confirm that Black had always bought petrol using credit cards, the receipts of which he would then submit to his firm to claim expenses. These files, plus several historical delivery schedules, were still in the company's archives. Investigators discovered that Black had made scheduled delivery runs to the areas where the abductions had occurred on the relevant dates, and although the precise times he had been in the area were difficult to adduce, petrol receipts confirmed he had bought fuel close to where each girl had been abducted on the date of her disappearance. For example, on the date of Sarah Harper's disappearance, Black had been scheduled to make a series of deliveries across the Midlands and Northern England. The two final deliveries on this schedule had been in West Yorkshire: in Brighouse, then a final delivery in Morley at a firm 150 yards from Harper's home. Black had refuelled his van between these two destinations shortly before Harper had last been seen alive.\n\nInvestigators discovered that upon his return to London from his long-distance deliveries to Northern England or Scotland, Black had regularly slept overnight in a house in Donisthorpe which belonged to his landlord's son. This was close to where all three bodies had been discovered. Leeds detectives also discovered that, on his regular deliveries to Morley, Black often slept in his van overnight in the premises to which he delivered, which was close to Sarah Harper's home.\n\nInvestigators learned that Poster, Dispatch and Storage Ltd had accounts with several oil companies, which allowed their drivers to buy fuel. With the cooperation of the companies, investigators obtained seven million archived microfiched credit card slips detailing fuel purchases paid via this method at every one of their nationwide premises between 1982 and 1986. These were sent to the reopened incident room in Newcastle upon Tyne, where a team of officers searched them for Black's distinctive signature in an effort to pinpoint precisely when and where he had bought his fuel. This laborious task bore fruit: beginning in October 1990, investigators began to discover evidence proving the precise times Black had bought fuel at petrol stations close to each abduction site. In each instance, the time of purchase had been shortly before or after each child had been abducted.\n\nBy December 1990, the inquiry team decided they had sufficient circumstantial evidence to convince the Crown that there was a reasonable prospect of securing convictions against Black, although Clark was worried that the inquiry had not uncovered any forensic evidence to tie Black to the murders. All the evidence was submitted to the Crown in May 1991. In March 1992, Crown lawyers decided that the evidence was sufficient to try Black for the three murders and the attempted abduction of Teresa Thornhill. At a news conference held on 11 March, Hector Clark informed the press that \"criminal proceedings have been issued on the authority of the Crown Prosecution Service against Robert Black\".\n\nSeveral pretrial hearings were held between July 1992 and March 1994; these hearings saw Black's defence counsel submit contentions that their client be tried on each count separately, and that the prosecution not be allowed to demonstrate any similarity between the modus operandi of each offence at the upcoming trial. In the penultimate pretrial hearing, in January 1994, Judge William Macpherson ruled against defence motions to try Black on each charge separately, and also ruled to allow the prosecution to submit similar fact evidence between the cases. This ruling allowed the prosecution to make these similarities between the cases known, and to introduce into evidence Black's recent conviction for the abduction and sexual assault of the Stow schoolgirl. The prosecution was prohibited from introducing into evidence the transcript of the August 1990 interview between Black and detectives Watt and Orr.\n\nOn 13 April 1994, Robert Black stood trial before Judge William Macpherson at Moot Hall, Newcastle upon Tyne. Black pleaded not guilty to each of the 10 charges of kidnap, murder, attempted kidnap, and preventing the lawful burial of a body.\n\nIn his opening statement on behalf of the Crown, prosecutor John Milford QC described the case to be tried as \"every parent's nightmare\" as he outlined the prosecution's contention that Robert Black had committed the three child murders and the attempted abduction, and the similarities between these offences and the 1990 abduction and sexual assault of the Stow schoolgirl for which Black was already serving a life sentence. Milford then described the circumstances of each abduction and murder for the jury; contending that each victim had remained alive in Black's van for several hours before her murder, and that each had been killed near the location Black had disposed of her body. In the latter stages of this five-hour opening statement, Milford contended that Black had kidnapped each victim for his own sexual gratification, and pointed out Black's extensive record of child sexual abuse and the paraphernalia discovered in his vehicle and at his London address. Milford closed his speech by stating that the petrol receipts and travel records would prove Black had been at all the abduction, attempted abduction and body recovery sites on the dates in question.\n\nOn the second day of the trial, the prosecution began to introduce witnesses, witness statements, circumstantial evidence, and forensic testimony. This saw witnesses describing the circumstances surrounding the abduction and subsequent discovery of each victim, and investigators describing the evidence uncovered of Black's movements on the dates of each abduction, the attempted abduction of Teresa Thornhill, and the kidnapping and assault of the Stow schoolgirl. Contemporary statements made by the mother of each murder victim at the time of her child's abduction were also read to the court, alongside testimony from the pathologists who had examined the bodies. Upon hearing the details of the kidnaps and murders, relatives of the three murder victims wept openly in court. Black rarely displayed any interest throughout the proceedings, typically remaining expressionless.\n\nSeveral of these initial witnesses were subjected to intense cross-examination by Black's defence counsel, Ronald Thwaites, upon issues such as memory accuracy and minor discrepancies between times logged in record books at a firm to which Black had made a delivery on the date of Susan Maxwell's disappearance and those of petrol receipts introduced as evidence (this discrepancy was proven to be an administrative error), and earlier police statements given by the witnesses. Most witnesses maintained their insistence of the accuracy and honesty of their testimony.\n\nOne of the witnesses cross-examined on the third day of the trial was James Fraser, a forensic scientist, who had examined more than 300 items recovered from Black's van and his London lodgings; Fraser conceded that in over 1,800 microscopic comparisons, no forensic link had been established between Black and the three victims. In direct re-examination by John Milford, Fraser said that the interval between the offences and Black's arrest, and the fact Black had only bought the van in which he was arrested in 1986, would make establishing a forensic link between the three murders unlikely.\n\nThe final prosecution witnesses to testify were detectives from the police forces involved in the manhunt; they testified on 29 April, and much of their testimony described the scope of the investigation while Black had been at large, and the painstaking inquiries to gather evidence. The final detective to testify was Hector Clark, who testified that Black's name had never been entered into the HOLMES database during the manhunt due to his conviction pre-dating the timescale of those judged to warrant further investigation. Clark further explained he could not recall any other cases where children had been abducted, killed and their bodies transported considerable distances, before stating: \"I don't believe there has been a bigger crime investigation in the United Kingdom, ever.\"\n\nOn 4 May, Ronald Thwaites began to outline his case in defence of Black. Thwaites reminded the jury the police had been unsuccessfully investigating these crimes for eight years before Black's 1990 arrest and conviction for the Stow abduction, and asserted that the investigators had seized on this case in an attempt to scapegoat his client to appease their feelings of \"frustration and failure\", and in an effort to restore broken reputations. Thwaites claimed that, although the paraphernalia introduced into evidence attested to his client's admitted obsession with paedophilic material, no direct evidence existed to prove Black had progressed from molester to murderer. Describing his decision not to permit Black to testify on his own behalf in relation to the petrol receipts and travel records, Thwaites informed the jury: \"No man can be expected to remember the ordinary daily routine of his life going back many years.\" Thwaites then began to introduce witnesses to testify on behalf of the defence, and continued to do so until 10 May.\n\nTo support Thwaites' contention that the three murders were not part of a series and had not been committed by Black, much of the testimony delivered by the defence witnesses referred to sightings of alternative suspects and suspicious vehicles near each abduction. The evidence delivered by these eyewitnesses contradicted that of those who had earlier testified on behalf of the prosecution. For example, Thomas Ball testified that on the date of Susan Maxwell's abduction, he had observed a girl matching her description striking a maroon Triumph saloon with a tennis racket. This car had contained at least two men, and the location Ball had seen this incident was very close to the site of Maxwell's abduction.\n\nOn 12 May, both counsels delivered their closing arguments to the jury. Prosecutor John Milford argued first; opening his final address to the jury by describing the circumstances of Black's 1990 arrest and recounting the extensive circumstantial evidence presented throughout the trial, and emphasising the fact no physical evidence existed due to the interval between the offences and Black's arrest. In reference to the defence argument that Black's close proximity to each of the abduction and body disposal sites of the dates in question was mere coincidence, Milford stated that if this defence contention were true, it would be \"the coincidence to end all coincidences\". Milford then requested that the jury reach a guilty verdict.\n\nThwaites delivered his closing arguments on behalf of the defence. He began by asking the jury: \"Where is the jury that will acquit a pervert of multiple murder?\" before describing his client as someone against whom ample prejudice existed, but no hard evidence. Thwaites pressed upon the jury the necessity to differentiate between a child sex pervert and an alleged child killer, before attacking the credibility of several prosecution witnesses, and pouring particular scorn upon the nationwide manhunt, stating: \"The police have become exhausted in not finding anyone; the public are clamouring for a result. What good are you if you can't catch a child killer? Is he [Black] their salvation, or a convenient, expendable scapegoat?\" Thwaites then referred to defence witness testimony which indicated someone else had committed the three murders, before resting his case.\n\nJudge Macpherson delivered his final instructions to the jury on 16 May and the following morning. In his final address, Judge Macpherson implored the jury to discard any emotion or personal distaste for Black's extensive history of sexual offences against children when considering their verdict, and not to prejudge his guilt because of his 1990 conviction for the abduction and sexual assault of the Stow schoolgirl. Judge Macpherson further directed the jury to instead focus on the evidence presented at the trial and decide whether the \"interlocking similarities\" between the cases presented were sufficient to convince them of Black's guilt, before reminding them that any conclusions of guilt on one charge must not determine guilt on the remaining nine charges they were to debate. The jury then received strict instructions against reading newspapers, watching television or making any telephone calls, before retiring to consider their verdict. These deliberations continued for two days.\n\nOn 19 May, the jury found Black guilty of three counts of kidnapping, three counts of murder, three counts of preventing the lawful burial of a body and—in relation to Teresa Thornhill—one count of attempted abduction. He was sentenced to a term of life imprisonment for each of these counts, with a recommendation that he serve a minimum of 35 years on each of the three murder charges. These life sentences were to be served concurrently. Passing sentence, Judge Macpherson described Black as being the perpetrator of \"offences which are unlikely ever to be forgotten and which represent a man at his most vile\".\n\nBlack remained unmoved upon receipt of this sentence, but as he prepared to leave the dock, he turned to the detectives from the various forces present at his sentencing who, since 1982, had been involved in his manhunt and proclaimed, \"Tremendous. Well done, boys.\" This statement caused several of the detectives to weep. Black was then taken to Wakefield prison, to begin his sentence in the segregation unit, as a Category A prisoner.\n\nImmediately following these convictions, the more than 20 detectives involved in the manhunt who had been present at his sentencing addressed the press assembled outside Moot Hall, with Hector Clark stating: \"The tragedy is these three beautiful children who should never have died. Black is the most evil of characters and I hope there is not now or ever another one like him.\" When asked his personal feelings towards Robert Black, Clark stated: \"Black is a man of the most evil kind, but no longer important to me. I care not about him.\"\n\nOn 15 December 2009, Black was served with a summons to attend trial in Northern Ireland for the 1981 murder of Jennifer Cardy. He was charged the following day.\n\nBlack's trial for the sexual assault and murder of Jennifer Cardy began at Armagh Crown Court on 22 September 2011. He was tried before Judge Ronald Weatherup, and acknowledged that he may have been in Northern Ireland on the date of Cardy's abduction, but pleaded not guilty to the charges.\n\nCircumstantial evidence attesting to Black's guilt of Cardy's murder had been obtained by Northern Ireland investigators searching through petrol receipts—560,000 in total—stored in his former employer's archives to ascertain Black's whereabouts on the dates surrounding the abduction and murder. Black's trial began with the prosecutor, Toby Hedworth, stating that the discovery of Black's signature upon these receipts was as good as signing his own confession.\n\nOn the second day of the trial, prosecutors introduced into evidence petrol receipts proving he had been near Ballinderry on the date of her abduction. Further evidence presented at trial included a salary ledger proving Black had been paid £50, which had only been given to drivers from his firm who made deliveries to Northern Ireland, and an order book confirming a delivery of billboard posters had been due near Ballinderry on the date of the abduction. Black was one of only two employees of Poster Dispatch and Storage Ltd willing to travel to Northern Ireland due to the Troubles, and travel records from all other drivers employed at this firm eliminated them from any culpability on the date of Cardy's abduction. The records also showed that, on the night of the abduction, Black had boarded an overnight ferry from Northern Ireland to Liverpool, before refuelling his van at Coventry the following day, en route to London.\n\nIn an effort to discredit the prosecution's contention that Black had been making deliveries to Ireland, Black's defence counsel, David Spens, suggested on the fourth day of the trial the Coventry petrol receipt could only indicate Black had been making deliveries to Coventry on the day after Cardy's murder; in rebuttal, Toby Hedworth questioned a colleague of Black's, who confirmed the firm did not make deliveries to Coventry in the early 1980s.\n\nTo further support the prosecution's contention that Cardy's murder had been committed by Black, Nathaniel Cary, a forensic pathologist, testified on the 11th day of the trial to the similarities between Cardy's abduction and murder, and that of Sarah Harper. Cary testified that the circumstances of the two girls' deaths were \"remarkably similar\", and that the injuries inflicted upon both girls' bodies strongly suggested both girls had been alive, albeit likely unconscious, when their bodies had been placed in water.\n\nBlack's second murder trial lasted six weeks, and the jury deliberated for four hours before delivering their verdict. On 27 October, he was found guilty of Cardy's abduction, sexual assault, and murder. Black was given a further life sentence, with hearings deferred on the minimum term to be served.\n\nOn 8 December, Judge Weatherup imposed a minimum term of 25 years. Weatherup informed Black: \"Your crime was particularly serious; you subjected a vulnerable child to unpardonable terror and took away her life.\" Prior to his final sentencing for this fourth murder, Black's defence lawyer, David Spens, had informed the court that no plea for mercy could be offered for his client; stating the case in question was \"one of those rare cases in which there is no mitigation, and so I propose to say nothing in that regard.\"\n\nAt his sentencing, Black was informed that he would be at least 89 before he would be considered for release.\n\nPolice believe that Black had committed more murders than the four for which he was convicted, with senior detectives believing the true number to be at least eight. In July 1994, a meeting was convened between senior detectives from the six police forces involved in the nationwide manhunt for Black, and representatives from other UK forces with unsolved missing child and child murder cases. The meeting assessed the evidence investigators had assembled to establish whether Black had killed other children.\n\nIn 2008, the Crown Prosecution Service stated that insufficient evidence existed to charge Black with any further murders.\n\nBlack has been linked to 13 further child murders and disappearances across the UK, Ireland, and continental Europe committed between 1969 and 1987.\n\n8 April 1969: April Fabb (13). Fabb was last seen cycling from Metton towards her sister's home in Roughton, Norfolk. Her bicycle was found in a field on the route she had taken, but her body has never been found.\n\n21 May 1973: Christine Markham (9). A Scunthorpe schoolgirl last seen walking to school. Her body has never been found. Black was questioned about potential involvement in her abduction in 2004.\n\n19 August 1978: Genette Tate (13). Abducted while delivering newspapers in Aylesbeare, Devon. Her bicycle was found in a country lane by two girls she had spoken to minutes before, but her body has never been found. Black made numerous deliveries of posters to the south-west of England in 1978. At the time of Black's death, the Devon and Cornwall Police were due to submit a fresh file to the Crown Prosecution Service, seeking formal abduction and murder charges in relation to this case.\n\n28 July 1979: Suzanne Lawrence (14). Lawrence was last seen leaving her sister's home in Harold Hill, northeast London. Although her body has never been found, Lawrence's name was added to a list of Black's possible victims in July 1994.\n\n16 June 1980: Patricia Morris (14). Disappeared from the grounds of her comprehensive school; her fully clothed body was found in Hounslow Heath two days after her disappearance. She had been strangled with a ligature.\n\n4 November 1981: Pamela Hastie (16). Her bludgeoned and strangled body was found in Johnstone, Renfrewshire, in November 1981. One eyewitness was adamant he had seen a man matching Black's description running from the crime scene, but police do not believe Black was near Renfrewshire at the time of Hastie's murder.\n\n18 March 1977: Mary Boyle (6). A Kincasslagh schoolgirl who disappeared while visiting her grandparents in Ballyshannon. Black was in County Donegal at the time of her disappearance. Her body has never been found.\n\n20 June 1985: Silke Garben (10). Garben was a Detmold schoolgirl who disappeared on her way to a dental appointment. Her body was found in a stream the day after her disappearance; she had been sexually assaulted and strangled. Black is known to have made a delivery of posters to a British Army base close to Garben's home on the date of her disappearance.\n\n5 August 1986: Cheryl Morriën (7). Morriën disappeared as she walked to her friend's home in the Dutch city of IJmuiden. Her body has never been found. Black made regular trips to nearby Amsterdam to buy child pornography.\n\n5 May 1987: Virginie Delmas (10). Abducted from Neuilly-sur-Marne on 5 May 1987. Her nude body was found in a Paris orchard on 9 October. Delmas had been strangled; the extent of decomposition prevented the pathologist determining whether she had been raped before death. Black made several deliveries in and around Paris on the date of Delmas's disappearance.\n\n30 May 1987: Hemma Greedharry (10). Discovered in the Paris suburb of Malakoff two hours after she was last seen alive. She had been raped and strangled. Black is known to have regularly travelled upon the road where Greedharry's body was found when making deliveries in northern France.\n\n3 June 1987: Perrine Vigneron (7). Vigneron disappeared on her way to buy a Mother's Day card in Bouleurs en route to attending a pottery course; her strangled body was discovered in a rapeseed field in Chelles on 27 June. A white van had been seen in Bouleurs on the day of Vigneron's disappearance.\n\n27 June 1987: Sabine Dumont (9). A Paris schoolgirl last seen alive in Bièvres on 27 June. Her strangled and sexually assaulted body was found the following day in the commune of Vauhallan. Black was named as a prime suspect in Dumont's murder in 2011.\n\nThe eight-year, nationwide inquiry which culminated in the 1990 arrest of Robert Black was one of the longest, most exhaustive and costly British murder investigations of the 20th century. By the time investigators had amassed sufficient evidence to convince the Crown to instigate criminal proceedings against Black for the three child murders and the attempted abduction of Thornhill, the dossier they had assembled was estimated to weigh 22 tonnes. The total cost of the inquiry is estimated to be £12 million.\n\nRobert Black appealed against his 1994 convictions. His appeal was heard before Lord Taylor at the Court of Appeal on 20 February 1995. Black contended he had been denied a fair trial due to details of his 1990 abduction and sexual assault charges being introduced as similar fact evidence at his trial, a ruling his defence counsel had fundamentally objected to. Black also contended that the final instructions delivered to the jury by Judge Macpherson had been unbalanced. Black's appeal hearing had been expected to last three days, but at the end of the first day, Lord Taylor refused leave to appeal the conviction on the grounds that Black's trial had been fair, and that none of his contentions could be substantiated.\n\nIn July 1995, Black was attacked in his cell at Wakefield prison by two fellow inmates, who threw boiling water mixed with sugar over him, bludgeoned him with a table leg, then stabbed him in the back and neck with an improvised knife. Black sustained superficial wounds, burns and bruising in this attack; his attackers were jailed for three further years after admitting wounding Black with intent to cause grievous bodily harm.\n\nBlack never admitted culpability in any of the murders of which he was convicted or suspected, and refused to cooperate with investigators, in spite of having little hope of ever being freed. According to Dr Ray Wyre, a pioneer in the treatment of sex offenders who conducted several interviews with Black between 1990 and 1993, the prime reason for this was an issue of control for Black. Wyre summarised the psychology behind Black's refusal to cooperate with investigators: He's the sort of person for whom it's all about power and control. Having information about what he's done gives him power. He has no desire to ease his conscience, and he's not going to give up the one thing that gives him power over the pain that his victims' families are suffering. The closest Black ever came to confessing to any of his crimes was in response to a question put to him shortly before his 1994 murder trial by Wyre. Wyre had asked Black why he never denied any of the charges brought against him. According to Wyre, Black had replied to this question with the words, \"Because I couldn't.\"\n\nBlack died from a heart attack in HMP Maghaberry on 12 January 2016. His body was cremated at Roselawn Crematorium, outside Belfast, on 29 January. No family or friends were present at this service. In this short service, the Presbyterian chaplain of HMP Maghaberry, the Reverend Rodney Cameron, read a section of Psalm 90. Black's ashes were scattered at sea in February 2016.\n\nSeveral television programmes have been made about Black.\n\nCitations\n\nBibliography\n\n\n"}
{"id": "473856", "url": "https://en.wikipedia.org/wiki?curid=473856", "title": "Seal script", "text": "Seal script\n\nSeal script () is an ancient style of writing Chinese characters that was common throughout the latter half of the 1st millennium BCE. It evolved organically out of the Zhou dynasty script. The Qin variant of seal script eventually became the standard, and was adopted as the formal script for all of China during the Qin dynasty. It was still widely used for decorative engraving and seals (name chops, or signets) in the Han dynasty. The literal translation of the Chinese name for seal script, (\"\"), is \"decorative engraving script\", a name coined during the Han dynasty, which reflects the then-reduced role of the script for the writing of ceremonial inscriptions.\n\nThe general term seal script can be used to refer to several types of seal script, including the Large or Great Seal script ( '; Japanese '; Korean '; Vietnamese ') and the lesser or Small Seal Script ( '; Japanese '; Korean '; Vietnamese '). Most commonly, without any other clarifying terminology, it refers to the latter of these. The term \"Large Seal script\" itself can also cover a broad variety of scripts, including a variation of Qin writing earlier than the small seal characters, but also the earlier Western Zhou forms, or even oracle bone characters as well. Since the term is an imprecise one, not clearly referring to any specific historical script and not used with any consensus in meaning, modern scholars tend to avoid it, and when referring to \"seal script\", generally mean the (small) seal script of the Qin system, that is, the lineage which evolved in the state of Qin during the Spring and Autumn and Warring States period and which was standardized under the First Emperor.\n\nThere were several different variants of seal script which developed independently in each kingdom during the Spring and Autumn and Warring States periods. One of these, the \"bird-worm\" seal script (), is named for its intricate decorations on the defining strokes, and was used in the Kingdoms of Wu, Chu, and Yue. It was found on several artifacts including the Spear of Fuchai and the Sword of Goujian. This seal script variant is very difficult to read.\n\nAs a southern state, Chu was close to the Wu-Yue influences. Chu produced broad bronze swords that were similar to Wuyue swords, but not as intricate. Chu also used the bird-worm style, which was borrowed by the Wu and Yue states.\n\nThe script of the Qin system (the writing as exemplified in bronze inscriptions in the state of Qin before unification) had evolved organically from the Zhou script starting in the Spring and Autumn period. Beginning around the Warring States period, it became vertically elongated with a regular appearance. This was the period of maturation of Small Seal script. It was systematized by prime minister Li Si during the reign of the First Emperor of China Qin Shi Huang through elimination of most variant structures, and was imposed as the nationwide standard. Through Chinese commentaries, it is known that Li Si compiled the \"Cangjiepian\", a partially-extant wordbook listing some 3,300 Chinese characters in small seal script. Their form is characterized by being less rectangular and more squarish.\n\nIn the popular history of Chinese characters, the Small Seal script is traditionally considered to be the ancestor of the clerical script, which in turn gave rise to all of the other scripts in use today. However, recent archaeological discoveries and scholarship have led some scholars to conclude that the direct ancestor of clerical script was proto-clerical script, which in turn evolved out of the little-known \"vulgar\" or \"popular\" writing of the late Warring States to Qin period.\n\nThe first known character dictionary was the 3rd century BC \"Erya\", collated and bibliographed by Liu Xiang and his son Liu Xin. It is no longer extant. Not long after, the \"Shuowen Jiezi\" (AD 100–121), the lifework of Xu Shen, was written. Its 9,353 entries reproduce the standardized small-seal script variant for each entry, and for some entries other pre-Han variants from the late Zhou era. Entries are categorized under 540 section headers.\n\nIt has been anticipated that the small seal script will some day be encoded in Unicode. Codepoints U+31400 to U+33D1F (Plane 3, Tertiary Ideographic Plane) have been tentatively allocated.\n\n\n\n"}
{"id": "42081426", "url": "https://en.wikipedia.org/wiki?curid=42081426", "title": "Sectoral balances", "text": "Sectoral balances\n\nThe sectoral balances (also called sectoral financial balances) are a sectoral analysis framework for macroeconomic analysis of national economies developed by British economist Wynne Godley. The balances represent an accounting identity resulting from rearranging the components of aggregate demand, showing how the flow of funds affects the financial balances of the private sector, government sector and foreign sector. This corresponds approximately to Balances Mechanics developed by Wolfgang Stützel in the 1950s.\n\nThe approach is used by scholars at the Levy Economics Institute to support macroeconomic modelling and by Modern Monetary Theorists to illustrate the relationship between government budget deficits and private saving.\n\nThe government fiscal balance is one of three major financial sectoral balances in the national economy, the others being the foreign financial sector and the private financial sector. The sum of the surpluses or deficits across these three sectors must be zero by definition. A surplus balance represents a net savings or net financial asset building position, while a deficit balance represents a net borrowing or net financial asset reducing position. Each sector may be defined as follows, using the U.S. as an example:\n\nTo summarize, in the U.S. in 2017, there was a private sector surplus of 1.2% GDP due to household savings exceeding business investment. There was also a trade deficit of 2.3% GDP, meaning the foreign sector was in surplus. By definition, there must therefore exist a government budget deficit of 3.5% GDP so all three net to zero. This is a relatively balanced position, but such is not always the case. For example, the U.S. government budget deficit in 2011 was approximately 10% GDP (8.6% GDP of which was federal), offsetting a foreign sector surplus of 4% GDP and a private sector surplus of 6% GDP.\n\nExpressed as a formula, the sectoral balance identity is: (Savings - Investment) + (Imports - Exports) + (Tax Revenues - Outlays) = 0; or (S-I) + (M-X) + (T-G) =0, as described below.\n\nThe U.S. Congressional Budget Office discussed sectoral balances in its August 2018 economic outlook: \"For example, the unique pattern of the balances in the early years of this century reflected increased borrowing by households and businesses that later proved to be unsustainable. Starting with the recession of 2001 and continuing through the expansion of the early- to mid-2000s, both the federal government and the U.S. domestic private sector were net borrowers. That borrowing was funded by foreign investors, and current-account\ndeficits climbed throughout the period, reaching an all-time high of 6.0% of gross domestic product (GDP) in fiscal year 2006. Following the onset of the 2007–2009 recession, the private sector drastically cut its borrowing while the federal government’s borrowing dramatically increased.\" CBO also provided supplemental data used to calculate the three sectoral balances, which it defines as the federal budget balance, current account balance, and nonfederal domestic balance.\n\nEconomist Wynne Godley explained in 2004-2005 how U.S. sector imbalances posed a significant risk to the U.S. and global economy. The combination of a high and growing foreign sector surplus and high government sector deficit meant that the private sector was moving towards a net borrowing position (from surplus to deficit) as a housing bubble developed, which he warned was an unsustainable combination.\nEconomist Martin Wolf cites as an example the U.S., where sudden shifts in the private sector from deficit to surplus due to the Great Recession forced the government balance into deficit. \"The financial balance of the private sector shifted towards surplus by the almost unbelievable cumulative total of 11.2 per cent of gross domestic product between the third quarter of 2007 and the second quarter of 2009, which was when the financial deficit of US government (federal and state) reached its peak...No fiscal policy changes explain the collapse into massive fiscal deficit between 2007 and 2009, because there was none of any importance. The collapse is explained by the massive shift of the private sector from financial deficit into surplus or, in other words, from boom to bust.\"\n\nEconomist Paul Krugman also explained in December 2011 the causes of the sizable shift from private deficit to surplus: \"This huge move into surplus reflects the end of the housing bubble, a sharp rise in household saving, and a slump in business investment due to lack of customers.\"\n\nEconomists at the New Policy Institute explained in 2011 that: \"In a healthy economy, businesses invest using money borrowed from households who are saving for future consumption. In the ideal world, this business sector deficit and household sector surplus are accompanied by net exports (a deficit for the rest of the world) and a small government deficit (to fund its own investment).\"\n\nGDP (Gross Domestic Product) is the value of all goods and services sold within a country during one year. GDP measures flows rather than stocks (example: the public deficit is a flow, the government debt is a stock). Flows are derived from the National Accounting relationship between aggregate spending and income. Ergo:\n\nwhere Y is GDP (expenditure), C is consumption spending, I is private investment spending, G is government spending, X is exports and M is imports (so X – M = net exports).\n\nAnother perspective on the national income accounting is to note that households can use total income (Y) for the following uses:\n\n(2) Y = C + S + T\n\nwhere S is total saving and T is total taxation (the other variables are as previously defined).\n\nYou then bring the two perspectives together (because they are both just “views” of Y) to write:\n\nYou can then drop the C (common on both sides) and you get:\n\n(4) S + T = I + G + (X – M) \n\nThen you can convert this into the following sectoral balances accounting relations, which allow us to understand the influence of fiscal policy over private sector indebtedness. Hence, equation (4) can be rearranged to get the accounting identity for the three sectoral balances – private domestic, government budget and external:\n\n\nThe sectoral balances equation says that total private savings (S) minus private investment (I) has to equal the public deficit (spending, G minus taxes, T) plus net exports (exports (X) minus imports (M)), where net exports represent the net savings of non-residents.\n\nAnother way of saying this is that total private savings (S) is equal to private investment (I) plus the public deficit (spending, G minus taxes, T) plus net exports (exports (X) minus imports (M)), where net exports represent the net savings of non-residents.\n\nAll these relationships (equations) hold as a matter of accounting and not matters of opinion.\n\nThus, when an external deficit (X – M < 0) and public surplus (G - T < 0) coincide, there must be a private deficit. While private spending can persist for a time under these conditions using the net savings of the external sector, the private sector becomes increasingly indebted in the process.\n\nIn macroeconomics, the Modern Money Theory uses sectoral balances to define any transactions between the government sector and the non-government sector as a vertical transaction.\n\nThe government sector is considered to include the treasury and the central bank, whereas the non-government sector includes private individuals and firms (including the private banking system) and the external sector – that is, foreign buyers and sellers.\n\nIn any given time period, the government’s budget can be either in deficit or in surplus. A deficit occurs when the government spends more than it taxes; and a surplus occurs when a government taxes more than it spends. Sectoral balances analysis states that as a matter of accounting, it follows that government budget deficits add net financial assets to the private sector. This is because a budget deficit means that a government has deposited more money into private bank accounts than it has removed in taxes. A budget surplus means the opposite: in total, the government has removed more money from private bank accounts via taxes than it has put back in via spending.\n\nTherefore, budget deficits, by definition, are equivalent to adding net financial assets to the private sector; whereas budget surpluses remove financial assets from the private sector.\n\nThis is represented by the identity:\n\n(G – T) = (S – I) – NX\n\nwhich is\n\n(State sector balance) = (Private sector balance) – External sector balance\n\nwhere G is government spending, T is taxes, S is savings, I is investment and NX is net exports.\n\nThe conclusion drawn from this is that private net saving is only possible when running a trade deficit if the government runs budget deficits; alternately, the private sector is forced to dis-save when the government runs a budget surplus and the trade deficit exists.\n\nAccording to the sectoral balances framework, budget surpluses remove net savings; in a time of high effective demand, this may lead to a private sector reliance on credit to finance consumption patterns. Hence, continual budget deficits are necessary for a growing economy that wants to avoid deflation. Therefore, budget surpluses are required only when the economy has excessive aggregate demand, and is in danger of inflation.\n\nAccording to the sectoral balances approach, austerity can be counterproductive in a downturn due to a significant private-sector financial surplus, in which consumer savings is greater than business investment. In a healthy economy, the amount borrowed or invested by companies is greater than or equal to the private-sector savings placed into the banking system by consumers. However, if consumers have increased their savings but companies are not investing, a surplus develops in the banking system. Business investment is one of the major components of GDP.\n\nEconomist Richard Koo described similar effects for several of the developed world economies in December 2011: \"Today private sectors in the U.S., the U.K., Spain, and Ireland (but not Greece) are undergoing massive deleveraging [paying down debt rather than spending] in spite of record low interest rates. This means these countries are all in serious balance sheet recessions. The private sectors in Japan and Germany are not borrowing, either. With borrowers disappearing and banks reluctant to lend, it is no wonder that, after nearly three years of record low interest rates and massive liquidity injections, industrial economies are still doing so poorly. Flow of funds data for the U.S. show a massive shift away from borrowing to savings by the private sector since the housing bubble burst in 2007. The shift for the private sector as a whole represents over 9 percent of U.S. GDP at a time of zero interest rates. Moreover, this increase in private sector savings exceeds the increase in government borrowings (5.8 percent of GDP), which suggests that the government is not doing enough to offset private sector deleveraging.\"\n\n\n"}
{"id": "25348242", "url": "https://en.wikipedia.org/wiki?curid=25348242", "title": "Sex differences in sensory systems", "text": "Sex differences in sensory systems\n\nAn organism is said to be sexually dimorphic when male and female conspecifics have anatomical differences in features such as body size, coloration, or ornamentation, but disregarding differences of reproductive organs. Sexual dimorphism is usually a product of sexual selection, with female choice leading to elaborate male ornamentation (e.g., tails of male peacocks) and male-male competition leading to the development of competitive weaponry (e.g., antlers on male moose). However, evolutionary selection also acts on the sensory systems that receivers use to perceive external stimuli. If the benefits of perception to one sex or the other are different, sex differences in sensory systems can arise. For example, female production of signals used to attract mates can put selective pressure on males to improve their ability to detect those signals. As a result, only males of this species will evolve specialized mechanisms to aid in detection of the female signal. This article uses examples of sex differences in the olfactory, visual, and auditory systems of various organisms to show how sex differences in sensory systems arise when it benefits one sex and not the other to have enhanced perception of certain external stimuli. In each case, the form of the sex difference reflects the function it serves in terms of enhanced reproductive success.\n\nMale sphinx moths, \"Manduca sexta\", rely on female released sex-pheromones to guide typical zig-zagging flight behaviors used to locate mates. Although both males and females respond to host plant olfactory cues to locate food sources, detection of and response to sex-pheromones appears to be male specific. Males that are better at detecting female sex-pheromones are able to find signaling females faster, providing them with a reproductive advantage. Since females gain no such advantage for having olfactory systems that are more sensitive to pheromones, enhanced pheromone detection has evolved only in the olfactory system of male \"M. sexta\". The three main sex differences are as follows:\n\n1) Male antennae are enlarged and contain elongated sensilla (sensory organules) not present in females. The pheromone elicits a male response by stimulating male specific receptor cells on a large number of these sensilla, which are located on the antennal flagellum. The sexually dimorphic sensilla are called male specific type-1 trichoid sensilla, a type of hair-like olfactory sensilla. In contrast, the flagella of female antennae lack these trichoid sensilla projections that make the male antennae appear to be larger and more feather-like. Each trichoid sensilla is innervated by two male specific olfactory receptor cells, with each cell being tuned (most sensitive) to one of two major chemical components of the pheromone. By evolving larger, pheromone-specific receptors in the peripheral olfactory system, male \"M. sexta\" have an improved sensitivity to female pheromones that enhances mate detection.\n\n2) There is also a sex difference in the neural basis of pheromone detection. In a number of insect species, first order olfactory processing centers in the neuropil of the antennal lobe contain a structure called the macroglomerulus in males. Such a structure, called the macroglomerular complex in this species, has been identified in the antennal lobe of \"M. sexta\", and it has been discovered that axons from the male specific olfactory receptor cells found in trichoid sensilla project exclusively to the macroglomerular complex. Further, all the antennal lobe neurons which respond to sex pheromone also have arborizations to the macroglomerular complex, providing more evidence that it plays a key role in the processing of pheromone sensory information in males.\n\n3) Lastly, there is evidence that the male antennal lobe contains male-specific macroglomerular complex projection neurons that relay pheromone information to higher brain structures in the protocerebrum. Since males have significantly larger populations of medial group antennal lobe neurons, it is thought that some of these extra neurons may belong to this male-specific class. Kanzaki et al. have since characterized the responses and structures of some of these projection neurons, and have found that projection neurons with dendritic arborizations in the macroglomerular complex and ordinary glomeruli were excited or inhibited by different stimuli (pheromonal vs. non-pheromonal stimuli respectively). These selective response properties indicate that a specialized role in relaying pheromone information is likely.\n\nThe male-specific features listed above, found at the levels of primary detection and neural processing in \"M. sexta\", demonstrate how males of this species have evolved sex differences in the sensory system that improve their ability to detect and locate females.\n\nSex differences in the visual system of flies are extremely common, with males generally possessing specialized eye features. Flies in the family Bibionidae display a particularly notable dimorphism, with males possessing large dorsal eyes that are absent in females. Female eyes and the ventral eyes of males are very similar, however, suggesting that the male dorsal eyes serve some extra sex specific function. Research on visually guided behavior in flies suggests that male dorsal eyes may be specialized for the detection and capture of females, for which there is a great deal of competition among males. Work by Jochen Zeil during the 1970s and 80's demonstrated that the dorsal eyes of Bibionids are functionally adapted to mate detection during high speed female chases for three main reasons.\n\n1) Firstly, male dorsal eyes contain significantly longer rhabdomeres (basic light sensing organelles in the ommatidia of eyes – 2 to 3 times longer in males) than those found in the female eyes or ventral eyes of males.\n\n2) Second, the enlarged retina prominent in male dorsal eyes is characterized by larger facet diameters than those found in male ventral and female eyes, meaning that there is an increase in the aperture area of each facet.\n\n3) Thirdly, Zeil was able to measure interommatidial angles by illuminating fly heads so that light traveled antidromically up the rhabdomeres. This technique revealed that male dorsal eyes have smaller interommatidial angles and different rhabdomere arrangements than ventral or female eyes.\n\nAt a functional level, increased aperture area and longer rhabdomere length both serve to increase photon capture efficiency in the male dorsal eye. Also, the smaller ommatidial angles and different rhabdomere arrangement observed in dorsal eyes are central to the function of the dorsal eye because they are neural superposition eyes, meaning that neural pooling of information from neighboring ommatidia is used to enhance sensitivity. Using a model that takes longer rhabdomeres, larger facet diameters, smaller ommatidial angles, and neural superposition into account, Zeil shows that the dorsal eye of males is able to detect small objects against a homogeneous background at a much greater distance than ventral or female eyes. The optical properties of longer rhabdomeres, increased facet diameter, and smaller ommatidial angles also aid in detection of small objects by increasing the resolution up to six times that of ventral or female eyes when neural pooling resulting from superposition is taken into account. Greater sensitivity to small light changes due to longer rhabdomeres and increased facet diameter, in combination with the ability to detect females at farther distances with higher resolution, allows male Bibionid flies to search for females at lower light levels (greater portion of the day) and to respond quickly to the presence of a female in order to catch her and initiate the \"marriage by capture\" that occurs in this family of flies. Unlike other fly families, the extreme dimorphism seen in Bibionids may be particularly relevant because these species do not swarm under a landmark, causing the course of females to be relatively unpredictable. As in the olfactory example above, the functional consequences of sex differences in Bibionids are linked to sex-specific behavior for which these sex differences play a key adaptive role.\n\n It is well known that the auditory systems of anurans are well adapted to detect species specific vocalizations, and that the behavioral response to these vocalizations often differs between the sexes. Sex differences in auditory systems have been found to underlie these gender specific behaviors in multiple species. In one particularly well-studied example, Peter Narins and his colleagues have examined sex differences in the auditory system of the Puerto Rican rain frog, \"Eleutherodactylus coqui\", in which males give a two-note, species specific call. In \"E. coqui\", the call is not only species specific, but also divided into sex specific components. The two-note call, from which the “coqui” frog derives its name, consists of a 100 ms “Co” note at around 1.2 kHz, followed by a much longer duration “Qui” note at around 2 kHz. Males of this species use this call in both territory defense and mate attraction, with the “Qui” note often being dropped out in aggressive interactions between males. Using playback experiments, Narins et al. found that males and females respond to different aspects of the call, with males showing a strong vocal response to calls containing “Co” notes, and females being preferentially attracted to calls containing “Qui” notes. Since the peripheral auditory system of anurans has been implicated in the detection of temporal and spectral features of male calls, follow up experiments were performed on the two auditory organs of the inner ear, the amphibian papilla and basilar papilla. Three major differences discovered in the auditory system help explain why, at a mechanistic level, male and female \"E. coqui\" are sensitive to different notes of the male call.\n\n1) Electrophysiological recordings from the eighth cranial nerve of males and females reveal that primary auditory neurons of the two sexes are maximally excited by different frequencies. Of the three main classes of primary auditory units (low, mid, and high frequency), the high frequency units in females are tuned (maximally sensitive) to sounds of approximately 2 kHz (the frequency of the “Qui” note), whereas the same units in males appear to be tuned to >3 kHz. The mid frequency units, on the other hand, are tuned to around 1.2 kHz (frequency of the \"Co\" note) in males, and to significantly lower frequencies in females. Increased sensitivity in females to the \"Qui\" note and in males to the \"Co\" note explains the difference in behavioral responsiveness of each sex to the note of the call that is biologically relevant. The low and mid frequency units are thought to derive from the amphibian papilla, whereas the high frequency units correspond to the basilar papilla. Although no difference was found for low frequency units, the sex differences in mid and high frequency unit response suggest that there may be a neural basis for male \"Co\" note selectivity in the amphibian papilla, and for female “Qui” note selectivity in the basilar papilla.\n\n2) By measuring Q10 values, it has also been found that the nerve fibers innervating the basilar papilla of males are more sharply tuned than those in females. Since the \"Qui\" note is a wide-band signal that sweeps upward in frequency, it appears as though the fibers innervating the female basilar papilla are better suited for detection of this component of the call. Sharp tuning in males decreases male selectivity for the \"Qui\" note, whereas comparatively broad tuning in females is suited for the detection of \"Qui\" notes that target females for mate attraction.\n\n3) Finally, there is a sex difference in call duration sensitivity, with males showing the greatest response to 100 ms duration calls, approximately the length of the shorter, “Co” note of the call. Recordings from male cells in the torus semicircularis led to the identification of cells that respond preferentially to stimuli of 100-150 ms duration at 1000 Hz (frequency of the “Co” note). Similar recordings from fibers of the eighth cranial nerve found no equivalent cells that are duration sensitive. The discovery of duration sensitive “off-cells” (they fire upon cessation of a signal) in males is evidence of a neural basis for the male preference for “Co” length notes, and is in agreement with other studies that have found cells sensitive to preferred stimulus durations in the torus semicircularis of other anurans. Although duration sensitive cells which respond preferentially to longer stimuli have not been identified in females, this discovery shows that there may be male-specific cells that help explain the enhanced male response to short, aggressive, \"Co\" notes in \"E. coqui\".\n\nAltogether, sex specific tuning differences in the primary auditory neurons of the basilar papilla and duration sensitive cells in the torus semicircularis of males suggest a mechanism to explain the sex specific response behavior observed in \"E. coqui\". Like sex differences in the olfactory system of \"M. sexta\", and in the visual system of Bibionids, sex differences in the auditory system of \"E. coqui\" benefit receivers at a functional level by maximizing sensitivity to aggressive or mate attraction signals based on the sex of the receiver and which signal is relevant. In all three systems, the sensory systems of males and females are differently adapted to receive signals that are biologically useful and beneficial to survival or reproduction.\n\n\nPage with information and link to the sound of the coqui frog \n"}
{"id": "50084266", "url": "https://en.wikipedia.org/wiki?curid=50084266", "title": "Shedun", "text": "Shedun\n\nShedun is a family of malware software (also known as Kemoge, Shiftybug and Shuanet) targeting the Android (operating system) first identified in late 2015 by mobile security company Lookout (company), affecting roughly 20,000 popular Android applications. Lookout claimed the HummingBad malware was also a part of the Shedun family, however, these claims were refuted.\n\nAvira Protection Labs stated that Shedun family malware is detected to cause approximately 1500-2000 infections per day.\nAll three variants of the virus are known to share roughly ~80% of the same source code.\n\nIn mid 2016, arstechnica reported that approximately 10.000.000 devices would be infected by this malware and that new infections would still be surging.\n\nThe malware's primary attack vector is repackaging legitimate Android applications (e.g. Facebook, Twitter, WhatsApp, Candy Crush, Google Now, Snapchat) with adware included, the app which remains functional is then released to a third party app store; once downloaded, the application generates revenue by serving ads (estimated to amount to $2 US per installation), most users cannot get rid of the virus without getting a new device, as the only other way to get rid of the malware is to root affected devices and re-flash a custom ROM.\n\nIn addition, Shedun-type malware has been detected pre-installed on 26 different types of Chinese Android-based hardware such as Smartphones and Tablet computers.\nShedun-family malware is known for auto-rooting the Android OS using well-known exploits like ExynosAbuse, Memexploit and Framaroot (causing a potential privilege escalation) and for serving trojanized adware and install themselves within the system partition of the operating system, so that not even a factory reset can remove the malware from infected devices.\n\nShedun malware is known for targeting the Android Accessibility Service, as well as for downloading and installing arbitrary applications (usually adware) without permission, it is classified as \"aggressive adware\" for installing potentially unwanted program applications and serving ads.\n\nAs of April 2016, Shedun malware is, by most security researchers, considered to be next to impossible to remove entirely.\n\nAvira Security researcher Pavel Ponomariov, specialized in Android malware detection tools, mobile threats detection and mobile malware detection automation research, has published an in-depth analysis of the computer virus.\n\n\n \n"}
{"id": "31305408", "url": "https://en.wikipedia.org/wiki?curid=31305408", "title": "Social competence", "text": "Social competence\n\nSocial competence consists of social, emotional, cognitive and behavioral skills needed for successful social adaptation. Social competence also reflects having an ability to take another's perspective concerning a situation, learn from past experiences, and apply that learning to the changes in social interactions. \n\nSocial competence is the foundation upon which expectations for future interaction with others is built, and upon which individuals develop perceptions of their own behavior. Social competence frequently encompasses social skills, social communication, and interpersonal communication.\n\nPast and current research intends to further the understanding of how and why social competence is important in healthy social development. The study of social competence began in the early 20th century. A noteworthy discovery was that social competence was related to future mental health, thus fueling research on how children interact with their peers and function in social situations. As research developed, different definitions and measurement techniques developed to suit these new findings.\n\nIn the 1930s, researchers began investigating peer groups and how children's characteristics affected their positions within these peer groups. In the 1950s and 1960s, research established that children's social competence was related to future mental health (such as maladaptive outcomes in adulthood), as well as problems in school settings. Research on social competence expanded greatly from this point on, as increasing amounts of evidence demonstrated the importance of social interactions.\n\nMid-century, researchers began to view social competence in terms of problem-solving skills and strategies in social situations. Social competence was now conceptualized in terms of effective social functioning and information processing. In the 1970s and 1980s, research began focusing on the impact of children's behavior on relationships, which influenced the study of the effectiveness of teaching children social skills that are age, gender, and context specific.\n\nIn an effort to determine why some children were not exhibiting social skills in some interactions, many researchers devised social information processing models to explain what happens in a social interaction. These models concentrated on factors in interactions such as behavior, how people process and judge each other, and how they process social cues. They also focus on how people select social goals, decide on the best response to a situation and enacting the chosen response. Studies such as this often looked at the relationship between social cognition and social competence.\n\nA prominent researcher of social competence in the mid-1980s was Frank Gresham. He identified three sub-domains of social competence: adaptive behavior, social skills, and peer acceptance (peer acceptance is often used to assess social competence). Research during this time often focused on children who were not displaying social skills in efforts to identify and help these children who were potentially at risk of long-term negative outcomes due to poor social interactions. Gresham proposed that these children could have one of four deficits: skill deficits, in which children did not have the knowledge or cognitive abilities to carry out a certain behavior, performance deficits, self-control skill deficits, and self-control performance deficits, in which children had excessive anxiety or impulsivity that prohibited proper execution of the behaviors or skills they knew and understood.\n\nDespite all the developments and changes in the conceptualization of social competence throughout the 20th century, there was still a general lack of agreement about the definition and measurement of social competence during the 1980s. The definitions of the 1980s were less ambiguous than previous definitions, but they often did not acknowledge the age, situation, and skill specificity implicit in the complex construct of social competence.\n\nThese approaches define social competence based on how popular one is with his peers. The more well-liked one is, the more socially competent they are.\n\nThese approaches use behaviors as a guideline. Behaviors that demonstrate social skills are compiled and are collectively identified as social competence.\n\nAccording to these approaches, social competence is assessed by the quality of one's relationships and the ability to form relationships. Competence depends on the skills of both members of the relationship; a child may appear more socially competent if interacting with a socially skilled partner. Commentators on some online incel communities have advocated government programs wherein socially awkward men are helped or women are incentivized to go on dates with them.\n\nThe functional approach is context-specific and concerned with the identification of social goals and tasks. This approach also focuses on the outcomes of social behavior and the processes leading to those outcomes. Information-processing models of social skills are important here, and based on the idea that social competence results from social-cognitive processes.\n\nEarly models of social competence stress the role of context and situation specificity in operationalizing the competence construct. These models also allow for the organization and integration of the various component skills, behaviors and cognitions associated with social competence. Whereas global definitions focus on the \"ends\" rather than the \"means\" by which such ends are achieved, a number of models directly attend to the theorized processes underlying competence. These process models are context specific and seek to identify critical social goals and tasks associated with social competence. Other models focus on the often overlooked distinction between social competence and the indices (i.e., skills and abilities) used to gauge it.\n\nGoldfried and D'Zurilla developed a five-step behavioral-analytic model outlining a definition of social competence.\n\nThe specific steps proposed in the model include: (1) situational analysis, (2) response enumeration, (3) response evaluation, (4) measure development, and (5) evaluation of the measure.\n\nIn the last two steps (4 and 5) a measure for assessing social competence is developed and evaluated.\n\nA social information-processing model is a widely used means for understanding social competence. The social information-processing model focuses more directly on the cognitive processes underlying response selection, enactment, and evaluation. Using a computer metaphor, the reformulated social information-processing model outlines a six-step nonlinear process with various feedback loops linking children's social cognition and behavior. Difficulties that arise at any of the steps generally translates into social competence deficits. \n\nThe six steps are:\n\nAnother way to conceptualize social competence is to consider three underlying subcomponents in a hierarchical framework.\n\nThe top of the hierarchy includes the most advanced level, social adjustment. Social adjustment is defined as the extent to which an individual achieves society's developmentally appropriate goals. The goals are conceived of as different \"statuses\" to be achieved by members of a society (e.g., health, legal, academic or occupational, socioeconomic, social, emotional, familial, and relational statuses). The next level is social performance – or the degree to which an individual's responses to relevant social situations meet socially valid criteria. The lowest level of the hierarchy is social skills, which are defined as specific abilities (i.e. overt behavior, social cognitive skills, and emotional regulation) allowing for the competent performance within social tasks.\n\nThe essential core elements of competence are theorized to consist of four superordinate sets of skills, abilities, and capacities: (1) cognitive skills and abilities, (2) behavioral skills, (3) emotional competencies, and (4) motivational and expectancy sets.\n\nSocial competence develops over time, and the mastery of social skills and interpersonal social interactions emerge at various time points on the developmental continuum (infancy to adolescence) and build on previously learned skills and knowledge. Key facets and markers of social competence that are remarkably consistent across the developmental periods (early childhood, middle/late childhood, adolescence) include prosocial skills (i.e., friendly, cooperative, helpful behaviors) and self-control or regulatory skills (i.e., anger management, negotiation skills, problem-solving skills). However, as developmental changes occur in the structure and quality of interactions, as well as in cognitive and language abilities, these changes affect the complexity of skills and behaviors contributing to socially competent responding.\n\nTemperament is a construct that describes a person's biological response to the environment. Issues such as soothability, rhythmicity, sociability, and arousal make up this construct. Most often sociability contributes to the development of social competence.\n\nSocial experiences rest on the foundation of parent–child relationships, and are important in the later development of social skills and behaviors. Attachment of an infant to a care-giver is important for the development of later social skills and behaviors that develop social competence. Attachment helps the infant learn that the world is predictable and trustworthy or in other instances capricious and cruel. Ainsworth describes four types of attachment styles in infancy, including secure, anxious–avoidant, anxious–resistant and disorganized/disoriented. The foundation of the attachment bond allows the child to venture out from his/her mother to try new experiences and new interactions. Children with secure attachment styles tend to show higher levels of social competence relative to children with unsecure attachment, including anxious–avoidant, anxious–resistant, and disorganized/disoriented.\n\nParents are the primary source of social and emotional development in infancy, early, and middle/late childhood. The socialization practices of parents influence whether their child will develop social competence. Parenting style captures two important elements of parenting: parental warmth/responsiveness and parental control/demandingness. Parental responsiveness (warmth or supportiveness) refers to \"the extent to which parents intentionally foster individuality, self-regulation, and self-assertion by being attuned, supportive, and acquiescent to children's special needs and demands.\" Parental demandingness (behavioral control) refers to \"the claims parents make on children to become integrated into the family whole, by their maturity demands, supervision, disciplinary efforts and willingness to confront the child who disobeys.\" Categorizing parents according to whether they are high or low on parental demandingness and responsiveness creates a typology of four parenting styles: indulgent/permissive, authoritarian, authoritative, and indifferent/uninvolved. Each of these parenting styles reflects patterns of parental values, practices, and behaviors and a distinct balance of responsiveness and demandingness.\n\nParenting style contributes to child well-being in the domains of social competence, academic performance, psychosocial development, and problem behavior. Research based on parent interviews, child reports, and parent observations consistently find that:\n\nOther factors that contribute to social competence include teacher relationships, peer groups, neighborhood, and community.\n\nAn important researcher in the study of social competence, Voeller, states that there are three clusters of problem behaviors that lead to the impairment of social competence. Voeller clusters include: (1) an aggressive and hostile group, (2) a perceptual deficits subgroup, and (3) a group with difficulties in self-regulation.\n\nWhile understanding the components of social competence continue to be empirically validated, the assessment of social competence is not well-studied and continues to develop in procedures. There are a variety of methods for the assessment of social competence and often include one (or more) of the following:\nFollowing the increased awareness of the importance of social competence in childhood, interventions are used to help children with social difficulties. Historically, intervention efforts did not improve children's peer status or yield long-lasting effects. Interventions did not take into account that social competence problems do not occur in isolation, but alongside other problems as well. Thus, current intervention efforts target social competence both directly and indirectly in varying contexts.\n\nEarly childhood interventions targeting social skills directly improve the peer relations of children. These interventions focus on at-risk groups such as single, adolescent mothers and families of children with early behavior problems. Interventions targeting both children and families have the highest success rates. When children reach preschool age, social competence interventions focus on the preschool context and teach prosocial skills. Such interventions generally entail teaching problem-solving and conflict management skills, sharing, and improving parenting skills. Interventions improve children's social competence and interactions with peers in the short-term and they also reduce long-term risk, such as substance abuse or delinquent behavior.\n\nSocial competence becomes more complicated as children grow older, and most intervention efforts for this age group target individual skills, the family, and the classroom setting. These programs focus on training skills in problem solving, emotional understanding, cooperation, and self-control. Understanding one's emotions, and the ability to communicate these emotions, is strongly emphasized. The most effective programs give children the opportunity to practice the new skills that they learn. Results of social competence interventions include decreased aggression, improved self-control, and increased conflict resolution skills.\n\n"}
{"id": "1069074", "url": "https://en.wikipedia.org/wiki?curid=1069074", "title": "Special rights", "text": "Special rights\n\nSpecial rights is a term originally used by conservatives and libertarians to refer to laws granting rights to one or more groups that are not extended to other groups. Ideas of special rights are controversial, as they clash with the principle of equality before the law.\n\nPotential examples of special rights include affirmative action policies or hate crime legislation with regard to ethnic, religious or sexual minorities or state recognition of marriage as a group with different taxation from those who are not married.\n\nConcepts of \"special rights\" are closely aligned with notions of group rights and identity politics.\n\nMore recently, social conservatives have used the term to more narrowly refer to measures that extend existing rights for heterosexual couples to gays and lesbians, such as in the case of same sex marriage, or that include sexual orientation as a civil rights minority group.\n\nThe term is also used internationally, for example \"Sonderrechte\" in Germany, but it is used also about special traffic right-of-way exceptions given to emergency response and military vehicles.\n\nThe basis behind the argument of the term is based on whether it should be considered just and legal for a law to treat various parties unequally. For example, in the US Constitution the prohibition on bills of attainder require that laws do not single out a single person or group of persons for specific treatment.\n\nAnother example is the equal protection clause in the Fourteenth Amendment. Both sides argue that the other side is or has traditionally been singled out and so the law is either needed or unnecessary.\n\nIn some cases, such as those with social implications, the universal definition of rights also often conflict with other, often more regional or local, laws that require certain public standards or behavior based on cultural norms.\n\nIn \"The Encyclopedia of Libertarianism\", Eric Mack states:\n\nA too-ready acceptance of alleged rights leads to an oppressive list of enforceable obligations. As the list of others' rights grows, each of us is subject to a growing burden made up of the obligations correlative to those rights; correspondingly the ability of rights to be protective of individual choice dissolves. Moreover, as the list of rights grows, so too does the legitimate role of political and legal institutions, and the libertarian case for radically limiting the scope and power of such institutions withers away. Libertarian theories of rights avoid generating an oppressive list of obligations through the employment of two crucial distinctions – the distinction between negative and positive rights and the distinction between general and special rights.\nMinority rights advocacy groups often contend that such protections confer no special rights, and describe these laws instead as protecting equal rights, due to past conditions or legal privileges for specific groups.\n\n\nPotential Examples:\n"}
{"id": "9062245", "url": "https://en.wikipedia.org/wiki?curid=9062245", "title": "T-criterion", "text": "T-criterion\n\nThe T-failure criterion is a set of material failure criteria that can be used to predict both brittle and ductile failure. \nThese criteria were designed as a replacement for the von Mises yield criterion which predicts the unphysical result that pure hydrostatic tensile loading of metals never leads to failure. The T-criteria use the volumetric stress in addition to the deviatoric stress used by the von Mises criterion and are similar to the Drucker Prager yield criterion. T-criteria have been designed on the basis of energy considerations and the observation that the reversible elastic energy density storage process has a limit which can be used to determine when a material has failed. \n\nThe strain energy density stored in the material and calculated by the area under the formula_1-formula_2 curve represents the total amount of energy stored in the material only in the case of pure shear. In all other cases, there is a divergence between the actual and calculated stored energy in the material, which is maximum in the case of pure hydrostatic loading, where, according to the von Mises criterion, no energy is stored. This paradox is resolved if a second constitutive equation is introduced, that relates hydrostatic pressure p with the volume change formula_3. These two curves, namely formula_4 and (p-formula_3) are essential for a complete description of material behaviour up to failure. Thus, two criteria must be accounted for when considering failure and two constitutive equations that describe material response. According to this criterion, an upper limit to allowable strains is set either by a critical value Τ of the elastic energy density due to volume change (dilatational energy) or by a critical value Τ of the elastic energy density due to change in shape (distortional energy). The volume of material is considered to have failed by extensive plastic flow when the distortional energy Τ reaches the critical value Τ or by extensive dilatation when the dilatational energy Τ reaches a critical value Τ. The two critical values Τ and Τ are considered material constants independent of the shape of the volume of material considered and the induced loading, but dependent on the strain rate and temperature.\n\nFor the development of the criterion, a continuum mechanics approach is adopted. The material volume is considered to be a continuous medium with no particular form or manufacturing defect. It is also considered to behave as a linear elastic isotropically hardening material, where stresses and strains are related by the generalised Hook’s law and by the incremental theory of plasticity with the von Mises flow rule. For such materials, the following assumptions are considered to hold:\n(a) The total increment of a strain component formula_6 is decomposed into the elastic and the plastic formula_7 increment and formula_8 respectively:\nformula_9 (1)\n(b) The elastic strain increment formula_7 is given by Hooke’s law:\nformula_11(2)\nwhere formula_12the shear modulus, formula_13 the Poisson’s ratio and formula_14 the Krönecker delta.\n(c) The plastic strain increment formula_8 is proportional to the respective deviatoric stress:\nformula_16(3)\nwhere formula_17 and formula_18 an infinitesimal scalar. (3) implies that the plastic strain increment:\n(d) The increment in plastic work per unit volume using (4.16) is:\nformula_19 (4)\nand the increment in strain energy, formula_20, equals to the total differential of the potential formula_21:\nformula_22(5)\nwhere\nformula_23, formula_24 and for metals following the von Mises yield law, by definition\nformula_25(6)\nformula_26(7)\nare the equivalent stress and strain respectively.\nIn (5) the first term of the right hand side, formula_27 is the increment in elastic energy for unit volume change due to hydrostatic pressure. Its integral over a load path is the total amount of dilatational strain energy density stored in the material. The second term formula_28 is the energy required for an infinitesimal distortion of the material. The integral of this quantity is the distortional strain energy density. The theory of plastic flow permits the evaluation of stresses, strains and strain energy densities along a path provided that formula_18 in (3) is known. In elasticity, linear or nonlinear, formula_18. In the case of strain hardening materials, formula_18 can be evaluated by recording the formula_32 curve in a pure shear experiment. The hardening function after point “y” in Figure 1 is then:\nformula_33(8)\nand the infinitesimal scalar formula_18 is:\nformula_35 (9)\nwhere formula_36is the infinitesimal increase in plastic work (see Figure 1). The elastic part of the total distortional strain energy density is:\nformula_37 (10)\nwhere formula_38 is the elastic part of the equivalent strain. When there is no nonlinear elastic behaviour, by integrating (4.22) the elastic distortional strain energy density is:\nformula_39 (11)\nSimilarly, by integrating the increment in elastic energy for unit volume change due to hydrostatic pressure, formula_40, the dilatational strain energy density is:\nformula_41 (12)\nassuming that the unit volume change\" formula_42 is the elastic straining, proportional to the hydrostatic pressure, p (Figure 2):formula_43 or formula_44 (13)\nwhere formula_23, formula_24 and formula_47 the bulk modulus of the material. \nIn summary, in order to use (12) and (13) to determine the failure of a material volume, the following assumptions hold:\n\nThe criterion will not predict any failure due to distortion for elastic-perfectly plastic, rigid-plastic, or strain softening materials. For the case of nonlinear elasticity, appropriate calculations for the integrals in and (12) and (13) accounting for the nonlinear elastic material properties must be performed. The two threshold values for the elastic strain energy formula_48 and formula_49 are derived from experimental data. A drawback of the criterion is that elastic strain energy densities are small and comparatively hard to derive. Nevertheless, example values are presented in the literature as well as applications where the T-criterion appears to perform quite well.\n"}
{"id": "40910086", "url": "https://en.wikipedia.org/wiki?curid=40910086", "title": "Weyl−Lewis−Papapetrou coordinates", "text": "Weyl−Lewis−Papapetrou coordinates\n\nIn general relativity, the Weyl−Lewis−Papapetrou coordinates are a set of coordinates, used in the solutions to the vacuum region surrounding an axisymmetric distribution of mass–energy. They are named for Hermann Weyl, T. Lewis, and Achilles Papapetrou.\n\nThe square of the line element is of the form:\n\nwhere (\"t\", \"ρ\", \"ϕ\", \"z\") are the cylindrical Weyl−Lewis−Papapetrou coordinates in 3 + 1 spacetime, and \"λ\", \"ν\", \"ω\", and \"B\", are unknown functions of the spatial non-angular coordinates \"ρ\" and \"z\" only. Different authors define the functions of the coordinates differently.\n\n\n\n"}
{"id": "229058", "url": "https://en.wikipedia.org/wiki?curid=229058", "title": "Young adult (psychology)", "text": "Young adult (psychology)\n\nA young adult is generally a person ranging in age from their late teens or early twenties to their thirties, although definitions and opinions, such as Erik Erikson's stages of human development, vary. The young adult stage in human development precedes middle adulthood. A person in the middle adulthood stage ages from 40 or 41 to 64. In old age, a person is 65 years old or older.\n\nFor a variety of reasons, timeliness on young adulthood cannot be exactly defined—producing different results according to the different mix of overlapping indices (legal, maturational, occupational, sexual, emotional and the like) employed, or on whether 'a \"developmental perspective\"... [or] the \"socialization perspective\" is taken. 'Sub-phases in this timetable of psychosocial growth patterns... are not rigid, and both social change and individual variations must be taken into account'—not to mention regional and cultural differences. Arguably indeed, with people living longer, and also reaching puberty earlier, 'age norms for major life events have become highly elastic' by the twenty-first century.\n\nSome have suggested that, after \"Pre-adulthood\"... in the first 20 years or so... the second era, \"Early Adulthood\", lasts from about age 17 to 45... the adult era of greatest energy and abundance and of greatest contradiction and stress.' Within that framework, 'the \"Early Adult Transition\" (17–22) is a developmental bridge between pre-adulthood and early adulthood', recognizing that 'the transition into adulthood is not a clear-cut dividing line'. One might alternatively speak of 'a \"Provisional Adulthood (18–30)\"... [&] the initiation to First Adulthood' as following that.\n\nDespite all such fluidity, there is broad agreement that it is essentially the twenties and thirties which constitute \"Early adulthood\"... the basis for what Levinson calls \"the Dream\"—a vision of his [or her] goal's in life which provide motivation and enthusiasm for the future'.\n\nYoung/prime adulthood can be considered the healthiest time of life and young adults are generally in good health, subject neither to disease nor the problems of senescence. Strength and physical performance reach their peak from 18–39 years of age. Flexibility may decrease with age throughout adulthood.\n\nWomen reach their peak fertility in their early 20s.\n\nIn developed countries, mortality rates for the 18–40 age group are typically very low. Men are more likely to die at this age than women, particularly in the 18–25 group: reasons include car accidents and suicide. Mortality statistics among men and women level off during the late twenties and thirties, due in part to good health and less risk-taking behavior.\n\nRegarding disease, cancer is much less common in young than in older adults. Exceptions are testicular cancer, cervical cancer, and Hodgkin's lymphoma. In sub-Saharan Africa, HIV/AIDS has hit the early adult population particularly hard. According to a United Nations report, AIDS has significantly increased mortality of between ages 20 to 55 for African males and 20 to 45 for African females, reducing the life expectancy in South Africa by 18 years and in Botswana by 34 years.\n\nAccording to Erikson, in the wake of the adolescent emphasis upon identity formation, 'the young adult, emerging from the search for and insistence on identity, is eager and willing to fuse their identity with that of others. He [or she] is ready for intimacy, that is, the capacity to commit... to concrete affiliations and partnerships.' To do so means the ability 'to face the fear of ego loss in situations which call for self-abandon: in the solidarity of close affiliations, in orgasms and sexual unions, in close friendships and in physical combat'. Avoidance of such experiences 'because of a fear of ego-loss may lead to a deep sense of isolation and consequent self-absorption'.\n\nWhere isolation is avoided, the young adult may find instead that 'satisfactory sex relations... in some way take the edge off the hostilities and potential rages caused by the oppositeness of male and female, of fact and fancy, of love and hate'; and may grow into the ability to exchange intimacy, love and compassion.\n\nIn modern societies, young adults in their late teens and early 20s encounter a number of issues as they finish school and begin to hold full-time jobs and take on other responsibilities of adulthood; and 'the young adult is usually preoccupied with self-growth in the context of society and relationships with others.' The danger is that in 'the second era, \"Early Adulthood\"... we must make crucially important choices regarding marriage, family, work, and lifestyle before we have the maturity or life experience to choose wisely.'\n\nWhile 'young adulthood is filled with avid quests for intimate relationships and other major commitments involving career and life goals', there is also \"a parallel pursuit for the formulation of a set of moral values\". Erikson has argued that it is only now that what he calls the 'ideological mind' of adolescence gives way to 'that \"ethical sense\" which is the mark of the adult.'\n\nReaching adulthood in modern society is not always a linear or clean transition. As generations continue to adapt, new markers of adulthood are created that add different social expectations of what it means to be an adult.\n\nDaniel Levinson suggested that the first phase of early adulthood comes to a close around twenty-eight to thirty, when 'at about 28 the provisional character of the twenties is ending and life is becoming more serious... the \"age-thirty crisis\".' between age 28 and 32', stressing that 'it is not uncommon, at the approach to the thirties, to tear up the life structure one put together to support the original dream of the twenties,' and to start anew—'to create the basis for the next life structure.\nWhen 'the Age Thirty Transition' is a difficult one, 'in a severe crisis [s/]he experiences a threat to life itself, the danger of chaos and dissolution, the loss of hope for the future.\n\nAfter the relative upheaval of the early 30s, the middle to late 30s are often characterized by settling down: 'the establishment phase', involving 'what we would call \"major life investments\"—work, family, friends, community activities, and values.' What has been termed 'the \"Culminating Life Structure for Early Adulthood\" (33–40) is the vehicle for completing this era and realizing our youthful aspirations.' People in their thirties may increase the financial and emotional investments they make in their lives, and may have been employed long enough to gain promotions and raises. They often become more focused on advancing their careers and gaining stability in their personal lives—'with marriage and child-rearing,' starting a family, coming to the fore as priorities.\n\nGail Sheehy, however, signposts the same twenties/thirties division rather differently, arguing that nowadays 'the twenties have stretched out into a long Provisional Adulthood', and that in fact 'the transition to the Turbulent Thirties marks the initiation to First Adulthood.'\n\nYoung Adulthood then draws to its close with 'the \"Midlife Transition\", from roughly age 40 to 45'—producing 'a brand-new passage in the forties, when First Adulthood ends and Second Adulthood begins.'\n\n\n"}
