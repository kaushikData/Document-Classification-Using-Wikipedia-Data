{"id": "47318952", "url": "https://en.wikipedia.org/wiki?curid=47318952", "title": "2015 Iglesia ni Cristo leadership controversy", "text": "2015 Iglesia ni Cristo leadership controversy\n\nThe 2015 Iglesia ni Cristo leadership controversy is a dispute between senior members of the Christian denomination Iglesia ni Cristo (INC) in the Philippines. In July 2015, it was reported that the INC had expelled some of its ministers, along with high-profile members Felix Nathaniel \"Angel\" and Cristina \"Tenny\" Manalo. Angel is the brother of current INC Executive Minister Eduardo Manalo and Tenny is the mother of Eduardo, and widow of former Executive Minister Eraño Manalo.\n\nAngel and Tenny were expelled from the INC for allegedly \"sowing disunity\" in the Church. The INC administration released a statement claiming that Eduardo had agreed to the expulsion of his brother and mother from the INC, as decided upon by its overall leadership. However, both Angel and Tenny claimed their lives were threatened by the Iglesia administration. Angel and Tenny had reportedly been illegally detained at the Iglesia's Central Office Complex in Tandang Sora, Quezon City, and that at least ten ministers of the Church were missing and alleged to have been abducted.\n\nFormer INC ministers Roel Rosal and Isaias Samson, Jr., claimed that the \"Sanggunián\" (the highest administrative council of the INC) had unlawfully abducted and detained ministers, along with members of the Manalo family, to cover up corruption surrounding the chief auditor, Glicerio \"Jun\" Santos, Jr. On July 24, 2015, INC, represented by Glicerio B. Santos, IV, head counsel and son of Santos, Jr. filed a libel complaint against Samson. Detained INC minister Lowell Menorca stated that he was forcibly detained by the INC administration, and was kidnapped at gunpoint by police officers in the employ of INC leaders and was forced to deny his captivity under duress. Menorca later fled to Canada and filed for refugee status, which was granted in 2018, with the Immigration and Refugee Board of Canada stating: “When the panel considers the links between the INC and the law enforcement authorities in the Philippines, the general climate of impunity that pervades Philippines law enforcement, particularly with respect to the issue of extrajudicial killing, and the level of corruption that exists in the Philippines government and law enforcement apparatus, the panel is satisfied Menorca would be unable to avail himself of state protection, from the risks that he fears in that country...\"\n\nOn July 22, 2015, a video was uploaded to YouTube featuring Felix Nathaniel \"Angel\" Manalo, the brother to nominal INC leader Eduardo V. Manalo and a former General Manager of GemNet (formerly the TV station of INC, replaced by INCTV), along with his mother, Cristina \"Tenny\" Manalo, the widow of Eraño Manalo. In the video, they claimed their lives had been threatened by the Iglesia leadership, along with that of Angel's sister, Lottie Hemedez. Tenny Manalo also claimed that there had been a mass kidnapping of ministers who had not been accounted for, implying that their disappearance somehow traced to the INC leadership, and that she had not been able to speak to Eduardo V. Manalo.\n\nHowever, INC denies the claims. Bienvenido Santiago, INC's chief evangelist, announced a few hours after the video's release that INC was expelling Cristina \"Tenny\" Manalo, along with Felix Nathaniel \"Angel\" Manalo, for allegedly causing divisions in the Church and wanting to seize power. Santiago claimed that their video was a sympathy ploy to gather support to meddle in the leadership of the INC and that the INC administration was acting on the wishes of Eduardo V. Manalo to prevent discord in the Church. Edwil Zabala, a spokesman for the INC stated that the INC would consider having Tenny and Angel Manalo return to the Church if they would pledge to uphold and obey their teachings. Marco Eraño \"Marc\" Villanueva Manalo and Lolita \"Lottie\" Manalo Hemedez, the younger siblings of Eduardo and Angel, were also expelled.\n\nAfter the response from the INC, an unidentified individual was photographed holding a sign saying \"\"Tulong\", hostage \"kami\"\" (Tagalog: \"Help, we're [held] hostage\") in a building that is part of the INC Central Office in Tandang Sora, Quezon City. An interview to Angel Manalo later confirmed that it was just a \"playing child\" who was the promotor of the sign and is nothing serious at all. When contacted by reporters, INC spokesman Edwil Zabala referred them to a CD recording of Santiago's statement regarding the expulsions.\n\nIn a press briefing that same afternoon, Santiago confirmed the expulsions. Santiago said in Tagalog, \"Painful as it is to our brother Eduardo Manalo, they have decided to expel those who have been causing division within the Church. That is why in future worship services of the Church of Christ, from this day, the Overall Leadership will let this be known by all of our brothers\".\n\nThe Philippine National Police, through its Quezon City Police District (QCPD), sent its personnel to the INC compound on July 23. They were able to contact Angel in the evening, and Angel himself talked to the reporters around 1:00 PST, denying he was detained and saying that sign pleading for help was done by a child joking around. QCPD Joel Pagdilao said that the police is ruling out case of abduction and described the concerned persons inside as peaceful and free.\n\nAngel Manalo also spoke to the reporters and criticized the Church's Sanggunian, denied that he is challenging the leadership of his brother, Eduardo Manalo. He also advised his brother Eduardo against trusting people within the Sanggunian he alleged to be corrupt. He also criticized the decision's to build the Philippine Arena instead of more houses of worship. He lamented the situation that his Church is facing saying in Tagalog, \"Why is it that these days, there are many anomalies? Others might say we're challenging our brother. No, we love our brother. But our problem is with people around him.\" He added \"The doctrine of the Iglesia ni Cristo is now damaged. In the same way that you looked up to us before, now you can see there are many anomalies. There are many acts of corruption in the Church. That's what we want to avoid\" \n\nOn December 15, 2015, church guards prevented Manalo and his housekeepers from allowing members of the media to enter their ancestral house in Tandang Sora, following the inspection of the sheriff of the Quezon City RTC. The court inspection was requested by church officials in order to identify the occupants of property, following reports that unknown masked and hooded persons were going in and out of the compound which is right beside the INC central office, as well as in response to several ex-Marines who tried to go inside the INC Compound, led by renegade soldier Capt. Nicanor Faeldon, and who were only stopped by INC security. INC spokesman Zabala also claimed that the #36 TS property was owned by INC. Angel Manalo also protested the construction of the high fences in the compound. The next day, the Quezon City RTC branch 22 ordered INC to restore the electricity supply of the compound, remove and dismantle the blocking fences, and remove the guardhouse and portalet that blocked the vehicle gate #3 of the Manalo's home.\n\nINC planned to file an ejectment case against Manalo and Hemedez if the siblings refused to leave their residence in Tandang Sora. On her part, Manalo-Hemedez said that she is the real owner of the #36 Tandang Sora compound, having been owned the property since 1983. She maintained that she and her household would not leave the compound.\n\nPresidential aspirant Roy Señeres visited the compound in January 2016. Guards barred Señeres from entering and refused to let him provide the occupants with food and bottled water.\n\nThat same day, allegations from former and current INC ministers claimed that the Manalo family were hostages. Roel Rosal, a former Iglesia ni Cristo minister who stated that he and his wife were expelled from the church for trying to expose corruption, made a public statement that ten ministers along with the Manalos were under house arrest at the INC Central Office, at the behest of INC auditor general Glicerio B. \"Jun\" Santos, Jr. in order to prevent the revelation of financial corruption by Santos and his staff. Rosal claims that Santos has pilfered and misused church offerings, and is holding the Manalos for \"revenge.\" Supporters of Tenny and Angel Manalo gathered outside the compound demanding to know the whereabouts of the Manalos and missing ministers. GMA News reported that they saw an individual resembling Angel Manalo inside of the house, along with guards wielding long firearms.\n\nAt least ten ministers are reported to be missing according to expelled minister Roel Rosal. One of the reported ministers, Lowell Menorca II was reported to be arrested by the Philippine National Police in Dasmariñas after two construction workers were allegedly threatened by Menorca with a grenade. Menorca's lawyer, Allen Blair Boy from the New Era University Legal Aid, questioned whether he was abducted and Boy said that Menorca told him that he was indeed arrested. It is not clear why he was in Dasmariñas, because Menorca is based in Sorsogon.\n\nOn July 29, 2015, Rosal reiterated his claims against the INC leadership. He also stated that the Quezon City Police were taking sides in the conflict by supporting the INC administration, and denying Angel and Tenny Manalo deliveries of food and water. Rosal also claimed that the captives in the Manalo compound were running out of food and water.\n\nIn a press conference held July 23, INC minister Isaias Samson, Jr. cited Menorca's case alleging that the military had taken him from Sorsogon to Dasmariñas. He said that the missing ministers were held in Rosalia Compound and Caringal in Quezon City.\n\nArnel Tumanan, Joven Sepillo of Tacloban, and Nolan Olarte of Cebu are among the other reported missing ministers.\n\nINC, thru attorney Serafin Cuevas, Jr., had also filed libel charges against Samson and Joy Yuson, former administrative coordinator of GEM TV, then the TV arm of INC, for accusing INC and its Sanggunian members of involvement in various anomalies and alleged abductions of ministers, preliminary investigations for the said case started on September 16, 2015 at the Quezon City RTC.\n\nMenorca released a video on October 23, 2015, a few days after he was allegedly rescued from abduction by the Sanggunian and the filing of writ of \"habeas corpus\" by his brother Anthony and sister-in-law Jungko Azuka. In it he supported the claim that he was abducted and illegally detained from July 25-October 21 in the Central Compound together with his wife, children and housekeeper, and orchestrated by the Sanggunian. He further claimed that he and his family never asked to the Church to give them a house, or to be put into protective custody. The supposed rescue of the Menorca Family was first supported by attorney Trixie Cruz-Angeles, the lawyer of the INC's expelled ministers, including Jun Samson.\n\nDuring the alleged illegal detention they claimed to have not been allowed to leave the premises nor had the right to accept guests. Cellphones and gadgets were also said to not be allowed in the premises. After the video was uploaded, INC expelled him and his family. The expulsion papers was signed by minister Radel Cortez.\n\nIsaias Samson, Jr., was a second-generation INC minister, former Sanggunian member and head of Foreign Department. He had been demoted due to some cases in his department to be an editor of INC's \"Pasugo\" magazine (a section of INC's Evangelism Department) and part of the Philippine government's Official Prayer Day and regular minister during prayers in SONA. He held a press conference on July 23 confirming his own detention, stating that he and his family were placed under house arrest by the INC administration, with the involvement of military and police officials. Samson stated that he was detained because of his opposition to Santos and gave a rough estimate of the amount of fund misused by Santos as in the \"millions\" of pesos, but he gave no evidence. Samson also stated that communication from the outside world was cut off and that passports, computers, phones and other electronic devices were confiscated. He was accused by INC leadership of being \"Antonio Ebangelista,\" a pseudonym used by a minister who had written critical articles that reveal details on Church administration and Santos, a charge Samson denies. Samson and his family escaped by telling the guards they wanted to go to church service. Samson described his imprisonment as \"unlawful detention,\" and called some in the INC administration \"dishonest,\" but stated that he retained his faith in the INC. Samson later claimed corruption within the INC leadership regarding misuse of funds, such as in the \"Lingap sa Mamamayan\" fund, use of private planes and oil business. On July 25, 2015, INC spokesman Edwil Zabala stated that Samson and his family were expelled from the church for refusing to submit to authority.\n\nSamson was replaced by Dennis C. Lovendino, a minister who became the new editor of \"Pasugo\" as of the June 2015 issue.\n\nIn a separate interview with CNN Philippines, INC Spokesperson Zabala said that the so-called house arrest of the family of Samson, Jr. was a disciplinary act within the Church (preventive suspension) in which Samson was under investigation, due to his connection to the group in social media that was claiming corruption issues inside the church.\n\nIn November 17, the Department of Justice dismissed the criminal raps filed by Samson and fellow expelled member Lito de Luna Fruto against the INC leaders for lack of probable cause. Both Samson and Fruto accused INC of holding them against their will.\n\nOn July 24, 2015, the Department of Justice ordered the National Bureau of Investigation (NBI) to investigate allegations of abduction of INC members. However it added that it would not interfere. On July 27, 2015, in a statement by Manuel Eduarte, chief of the NBI Anti-Organized Transnational Crime Division, said that the NBI concluded that insufficient evidence showed that the Iglesia ni Cristo abducted some of its ministers. However, on July 29, 2015, Former Justice department secretary (now Senator) Leila de Lima clarified that the case had not been closed.\n\nFormer Vice President Jejomar Binay criticized de Lima's \"unwarranted statements\" asserting that the case remained open and noted her statement contradicts the reported findings of NBI Anti-Organized Transnational Crime Division. \"By your actuations, you are promoting the image of disunity, discord and even corruption in the INC to its clear prejudice and detriment,\" Binay said that De Lima must respect the separation of church and state and \"unwarranted interference\" from the government in the church must not happen. Binay also describe De Lima's dealings with the Iglesia ni Cristo controversy as similar to what he describes as a \"policy of demolition\" targeted towards his own person.\n\nOn late evening of August 27, about 2,000 Iglesia members led by INC spokesman Edwil Zabala held a vigil outside the Department of Justice office and later moved to EDSA. The demonstrators protested against the government agency, led by De Lima, for allegedly persecuting their church and called for upholding the separation of church and state. They also called for the agency to prioritize more important issues.\n\nINC general evangelist Bienvenido Santiago on July 23, 2015, responding to allegations made by the Manalos in a July 22 video, dismissed the accusations that some INC ministers were abducted and claimed that the two were attacking the church to make it appear that it was involved in the abductions. He also added that the video was an attempt to rally support from church members and to distract the church's management. Santiago said that the church's decision to expel the two Manalos was to show that the INC is a religion that follows the teachings of God found in the Bible, not a business.\n\nIn a morning Sunday worship in July 26, INC executive minister Eduardo Manalo repeated that members should strictly follow the church's leadership and rules to attain salvation.\n\nThe media reported that an INC member said that provincial ministers issued a set of \"tagubilin\" (instructions) towards the church's members during the weekend ahead of the church's 101st anniversary celebrations to be held in July 26 in Ciudad de Victoria complex in Bulacan. Among these reported instructions were that members decline media inquiries and travel by private cars to the celebrations. During the centennial celebrations, members were provided with service vehicles. Stickers for vehicles with \"One with EVM\" were reportedly distributed.\n\nThe media reported an unusually high number of worshipers and vehicles at the INC church in Los Baños. Several taurpalins bearing the words \"One with EVM\" were posted outside the building.\n\nExpelled INC minister Roel and his spouse, Shirley Rosal told reporters on July 23, 2015 that at least 10 ministers were under house arrest. Roel Rosal claimed that auditor Jun Santos and his staff were using donations for the INC to purchase luxury vehicles. He also said that Angel and Tenny Manalo were hostages. Roel said that the expulsion of the Manalos might have been influenced by a \"third party\" in the church administration.\n\nIsaias Samson, Jr. at a July 23 press conference, said that government intervention in the crisis would help resolve the issue. Samson was expelled from the INC the day after his press conference.\n\nFewer than 12 supporters of the church wearing white armbands organized a vigil in support of the Manalos in front of the INC compound in Tandang Sora. By 2:00 am of July 25, no INC member was seen camping outside the compound.\n\nSome members used social media to express their support for Eduardo V. Manalo and the Sanggunian. Some members changed their profile photo to an image bearing the words \"I am one with EVM,\" in support of the leadership of Eduardo Manalo.\n\nOn July 25, 2015, Louie Cayabyab, the minister of the Iglesia ni Cristo locale in Fremont, California, resigned his position as minister as a response to the expulsions of Samson and Tenny and Angel Manalo. Cayabyab stated \"Beloved brethren, there are two circulars that are supposed to be read this morning. The first circular is about the expulsion from the Church of the wife and children of Bro. Erano G. Manalo. And another circular is the expulsion from the Church of Bro. Isaias T. Samson Jr., former editor-in-chief of the \"Pasugo\". I decided, brethren, that I won’t read those circulars. You might be asking, 'why?' Because in my heart, in my heart of hearts, I can’t take it. It is, it is just so so difficult to betray one’s heart...\" While Cayabyab asked INC members to remain in the church, he urged them to support peaceful vigils held in their areas, and implored them to support brethren in the Philippines \"who are risking their lives\" to expose corruption and reform the church.\n\nIn response to Cayabyab's resignation, members of the INC in Northern California, numbering in the \"dozens,\" according to ABS-CBN News, held a protest at the INC's main United States office in Burlingame, California, demanding an investigation and tribunal into the corruption allegations, along with the reinstatement of Angel and Tenny Manalo. About a week later those who were identified as having taken part in the protests were expelled from the church.\n\nMembers of INC from 26 ecclesiastical districts picketed the Department of Justice office in Manila, when Secretary Leila de Lima, on her birthday, officially announced her 2016 senatorial bid. The assembly happened a few days after former minister Samson, Jr., his wife and son, filed illegal detention charges against eight members of the Sanggunian. A respondent of the case, Bienvenido Santiago, Sr., the church's general evangelist, wanted De Lima to focus instead on the SAF 44 case.\n\nIn an interview with Eagle News Service/Net 25 on July 29, 2015, a broadcaster owned and operated by INC affiliate Eagle Broadcasting Corporation, ministers Lowell \"Boyet\" Menorca II, Joel San Pedro, Jojo Nemis and Arnel Tumanan denied they were held captive or tortured. They also affirmed their loyalty to the church administration and executive minister Eduardo Manalo and that they were still church members. The ministers also stated that members should not believe the claims of abduction.\n\nHowever, Anthony Menorca, Boyet Menorca's brother, stated that his older brother was forced to deny his abduction because INC Sanggunian was holding Boyet Menorca's wife, Seiko Otsuka Menorca, and one year old child. Anthony stated that in a meeting with his brother, he told him \"Binantaan kami ng kapatid ko na kami ay papatayin\" (Tagalog: \"My brother warned us we will be killed\") Anthony Menorca stated that he was in an undisclosed location and feared for the life of his brother.\n\nIn a new video released after he was allegedly rescued from the captivity of the Sanggunian on October 21, Menorca clarified, claiming that everything he said in the interview on Net 25 were \"scripted\".\n\nOn August 1, 2015, Boyet Menorca, along with his wife and child, were interviewed in person outside the INC compound, wherein he denied all allegations of his abduction and pleaded for his brother to stop making claims. He added that he and his family voluntarily decided to stay within the compound because of safety concerns. He was worried that he could potentially be attacked in order to direct the resulting blame on the INC. After the interview, he and his family returned to the INC compound. Later, Menorca stated the interview was held \"under duress.\"\n\nOn October 21, 2015, Lowell Menorca's brother Anthony petitioned the Supreme Court of the Philippines to issue writs of \"habeas corpus\" and \"amparo\" against the INC administration. The request was granted and on October 25, Menorca stated that he was illegally detained by INC, kidnapped by police officers allegedly in INC's employ and had been interrogated for 17 hours. He also stated that police officials tried to kill him afterwards using a hand grenade and that he had to beg for his life, successfully convincing a police officer allegedly hired to murder him not to go through with the killing.\n\nOn January 20, 2016, on his way to the Court of Appeals to attend a hearing on the illegal detention case, policemen wearing civilian clothes and without badges led by Supt. Ed Leonardo and INC member, arrested Menorca on libel charges filed by the SCAN International in Kapatagan, Lanao del Norte. A standoff between the police, Menorca and his wife Jinky occurred along Roxas Boulevard and Quirino Avenue when Menorca refused the arrest and claimed that the court had not notified him of libel charges. After the commotion, 20 policemen in uniform showed up to arrest Menorca. MPD Director Chief Supt. Rolando Nana stopped the standoff and told Menorca to go to MPD Station 5 for booking.\n\nThe church, thru its spokesman Edwil Zabala, said the INC strongly denied any involvement with the standoff and told Menorca to face the libel charges against him.\n\nMenorca fled to Vietnam in March 2016. Moises Tolentino Jr., a lawyer for the Iglesia ni Cristo responded that: \"Beset by several libel suits and an adultery charge to boot, [Menorca] took the cowardly option. He took a hurried flight last night bound for Vietnam. He is now a fugitive from justice..\" and \"Menorca is supposed to be the aggrieved party. If they were sincere, if they believed that they were in the right, they should not leave the country. They should face all opportunities that should be given to respondents...\" Menorca left Vietnam two weeks later and arrived in Vancouver. He then filed an application for refugee status with the Government of Canada claiming that his life and that of his family members were threatened by the administration of the Iglesia ni Cristo. The Immigration and Refugee Board of Canada granted Menorca refugee status in 2018, stating that Menorca was \"a person in need of protection from a risk of cruel and unusual treatment or punishment and a risk to his life.\" The IRB further stated, \"When the panel considers the links between the INC and the law enforcement authorities in the Philippines, the general climate of impunity that pervades Philippines law enforcement particularly with respect to the issue of extrajudicial killing, and the level of corruption that exists in the Philippines government and law enforcement apparatus, the panel is satisfied [Menorca] would be unable to avail himself of state protection, from the risks that he fears in that country...\" and that \"[The INC's] power and influence extends to an ability to utilize [police] to target the claimant.\"\n\nThe Bulacan chapter of the League of Municipalities of the Philippines posted tarpaulin streamers with photographs of 20 Bulacan mayors greeting the church on its 101st anniversary. Pandi Mayor Enrico Roque, president of LMP Bulacan, said that the taurpalin was displayed to express their continuing friendship and respect to the church, in response to a report that a Bulacan mayor complained to church leaders that a minister from the INC attempted to extort money from the official. Roque said that the group says that they are in complete support of the INC.\n\nOn his Twitter account, Eli Soriano of Members Church of God International expressed support for De Lima and urged De Lima not to resign.\n\nCommission on Human Rights chairperson José Luis Martin Gascon on July 24 said that the human rights body is monitoring the situation and was yet to launch a formal investigation. The rights body stated that it was avoiding a \"premature conclusion\" regarding the situation. It described the situation as \"primarily a police matter\" and is looking into allegations before taking action.\n\nLingayen-Dagupan Archbishop Socrates Villegas, president of the Catholic Bishops' Conference of the Philippines, on July 24 requested prayers for Iglesia members. Villegas said in Tagalog, \"As with their plea, I also ask that we pray for our brethren in the Iglesia ni Cristo... Brother Angel and his relatives are requesting prayers, that we pray for them.\" He added that he respected the decision of the Iglesia, which like the church, is \"bound by rules and regulations\".\n"}
{"id": "6934934", "url": "https://en.wikipedia.org/wiki?curid=6934934", "title": "Acedia", "text": "Acedia\n\nAcedia (; also accidie or accedie , from Latin \"acedĭa\", and this from Greek ἀκηδία, \"negligence\", ἀ- \"lack of\" -κηδία \"care\") is a state of listlessness or torpor, of not caring or not being concerned with one's position or condition in the world. It can lead to a state of being unable to perform one's duties in life. Its spiritual overtones make it related to but arguably distinct from depression. Acedia was originally noted as a problem among monks and other ascetics who maintained a solitary life.\n\nThe \"Oxford Concise Dictionary of the Christian Church\" defines acedia (or accidie) as \"a state of restlessness and inability either to work or to pray\". Some see it as the precursor to sloth—one of the seven deadly sins. In his sustained analysis of the vice in Q. 35 of the \"Second Part (Secunda Secundae)\" of his \"Summa Theologica\", the 13th-century theologian Thomas Aquinas identifies acedia with \"the sorrow of the world\" (compare \"Weltschmerz\") that \"worketh death\" and contrasts it with that sorrow \"according to God\" described by St. Paul in 2 Cor. 7:10. For Aquinas, acedia is \"sorrow about spiritual good in as much as it is a Divine good.\" It becomes a mortal sin when reason consents to man's \"flight\" (\"fuga\") from the Divine good, \"on account of the flesh utterly prevailing over the spirit.\" Acedia is essentially a flight from the divine that leads to not even caring that one does not care. The ultimate expression of this is a despair that ends in suicide.\n\nAquinas's teaching on acedia in Q. 35 contrasts with his prior teaching on charity's gifted \"spiritual joy\", to which acedia is directly opposed, and which he explores in Q. 28 of the \"Secunda Secundae\". As Aquinas says, \"One opposite is known through the other, as darkness through light. Hence also what evil is must be known from the nature of good.\"\n\nMoral theologians, intellectual historians and cultural critics have variously construed acedia as the ancient depiction of a variety of psychological states, behaviors or existential conditions: primarily laziness, apathy, ennui or boredom.\n\nThe demon of acedia holds an important place in early monastic demonology and proto-psychology. In the late fourth century Evagrius of Pontus, for example, characterizes it as \"the most troublesome of all\" of the eight genera of evil thoughts. As with those who followed him, Evagrius sees acedia as a temptation, and the great danger lies in giving in to it. Evagrius' contemporary, the Desert Father John Cassian, depicted the apathetic restlessness of \"acedia\", \"the noonday demon\", in the coenobitic monk:\n\nHe looks about anxiously this way and that, and sighs that none of the brethren come to see him, and often goes in and out of his cell, and frequently gazes up at the sun, as if it was too slow in setting, and so a kind of unreasonable confusion of mind takes possession of him like some foul darkness.\n\nIn the medieval Latin tradition of the seven deadly sins, acedia has generally been folded into the sin of sloth. The Benedictine Rule directed that a monk displaying the outward signs of \"acedia\" \n\"should be reproved a first and a second time. If he does not amend he must be subjected to the punishment of the rule so that the others may have fear.\n\nAcedia is indicated by a range of signs. These signs (or symptoms) are typically divided into two basic categories: somatic and psychological. Acedia frequently presents signs somatically. Such bodily symptoms range from mere sleepiness to general sickness or debility, along with a host of more specific symptoms: weakness in the knees, pain in the limbs, and fever. An anecdote attributed to the Desert Mother Amma Theodora also connects somatic pain and illness with the onset of acedia. A host of psychological symptoms can also signify the presence of acedia, which affects the mental state and behavior of the afflicted. Some commonly reported psychological signs revolve around a lack of attention to daily tasks and an overall dissatisfaction with life. The best-known of the psychological signs of acedia is tedium, boredom or general laziness. Author Kathleen Norris in her book \"Acedia and Me\" asserts that dictionary definitions such as torpor and sloth fail to do justice to this \"temptation\"; she believes a state of restlessness, of not living in the present and seeing the future as overwhelming is more accurate a definition than straight laziness: it is especially present in monasteries, due to the cutting off of distractions, but can invade any vocation where the labor is long, the rewards slow to appear, such as scientific research, long term marriages, etc. Another sign is a lack of caring, of being unfeeling about things, whether that be your appearance, hygiene, your relationships, your community's welfare, the world's welfare etc.; all of this, Norris relates, is connected to the hopelessness and vague unease that arises from having too many choices, lacking true commitment, of being \"a slave from within\". She relates this to forgetfulness about \"the one thing needful\": remembrance of God.\n\n\n"}
{"id": "4092681", "url": "https://en.wikipedia.org/wiki?curid=4092681", "title": "Air-to-cloth ratio", "text": "Air-to-cloth ratio\n\nThe air-to-cloth ratio is the volumetric flow rate of air (m³/minute; SI m³/second) flowing through a dust collector's inlet duct divided by the total cloth area (m²) in the filters. The result is expressed in units of velocity.\n\nThe air-to-cloth ratio is typically between 1.5 and 3.5 metres per minute, mainly depending on the concentration of dust loading.\n\n"}
{"id": "22759719", "url": "https://en.wikipedia.org/wiki?curid=22759719", "title": "Anupadaka", "text": "Anupadaka\n\nAnupadaka, anupapadaka, aupapaduka (Skt., 'parentless; self-existing,') is a philosophical term about reality such as the 'anupadaka plane' or gods or Dhyani-Buddhas that fit the definition. Actually beyond anupadaka is 'adi' (Skt., 'first,') having to do with the first cause, itself from causeless cause.\n\nHence, it does not matter if scientists think there are no gods or Dhyani-Buddhas: anupadaka is still an idea about something that is an effect of the first cause or causality. In that sense it may have to do with a scientific idea or the term can be applied to one: the term came to the West from Theosophy, which focuses on science as much as religion, and 'anupadaka' may have distinct meanings in Theosophy.\n\n"}
{"id": "2591266", "url": "https://en.wikipedia.org/wiki?curid=2591266", "title": "Anāgāmi", "text": "Anāgāmi\n\nIn Buddhism, an anāgāmi (Sanskrit and Pāli for \"non-returning\") is a partially enlightened person who has cut off the first five chains that bind the ordinary mind. Anāgāmis are the third of the four aspirants.\n\nAnagamis are not reborn into the human world after death, but into the heaven of the Pure Abodes, where only anāgāmis live. There they attain full enlightenment (arahantship).\n\nThe Pali terms for the specific chains or fetters (Pali: ) of which an anāgāmi is free are:\n\nThe fetters from which an anāgāmi is not yet free are:\n\nKāmarāga and Byāpāda, which they are free from, can also be interpreted as craving for becoming and non-becoming, respectively.\nAnāgāmis are at an intermediate stage between sakadagamis and arahants. Arahants enjoy complete freedom from the ten fetters. An anāgāmi's mind is very pure.\n\n\n"}
{"id": "54425285", "url": "https://en.wikipedia.org/wiki?curid=54425285", "title": "Baked Alaska (entertainer)", "text": "Baked Alaska (entertainer)\n\nAnthime \"Tim\" Gionet (born November 16, 1987), more commonly known as Baked Alaska and Tim \"Treadstone\", is an American alt-right/far right, neo-Nazi Internet troll, white nationalist and social media personality.\n\nGionet is known for posting anti-Semitic and pro-Nazi messages on Twitter. He has published the \"Fourteen Words,\" a neo-Nazi mantra, on his Twitter feed, retweeted videos saying that \"Hitler did nothing wrong,\" and tweeted images of people in gas chambers.\n\nDuring the 2016 U.S. Presidential election, Gionet was a pro-Trump Internet troll. He marched at the far-right Charlottesville rally in 2017.\n\nGionet was born in Anchorage, Alaska to a family of eight. His father, Paul Gionet, is a pharmacist and his mother is Susanne Gionet. He was born and raised following the Christian faith. Gionet's family operates a non-profit organization, Russian Encouragement, which aims to spread the gospel and provide medical supplies to orphanages in eastern Russia. He attended Azusa Pacific University and graduated with Bachelors of Science in marketing.\n\nAfter graduating from Azusa Pacific University in 2010, Gionet began working at Warner Bros. Records. Through his work at Warner Bros. Records, Gionet landed a job working marketing and social media for the Warped Tour, where he first developed his nickname, Baked Alaska. In 2011 Gionet worked for Capitol Records, developing musical videos for songs such as \"Alaska Vacation\" and \"I Climbed Mountains\". Following his work at Capitol Records, Gionet worked as a social media strategist at BuzzFeed, where he helped develop content and ideas for BuzzFeed's social media accounts such as Tasty. After leaving BuzzFeed in 2016, Gionet traveled as Milo Yiannopoulos's \"Dangerous Faggot Tour\" manager.\n\nGionet is a supporter of Donald Trump. In May 2016, Gionet was introduced to Donald Trump and received the candidate's signature on his arm next to his Trump tattoo. Later that month, Gionet released \"MAGA Anthem,\" which featured pro-Trump lyrics and amassed more than 100,000 views on YouTube. Mike Cernovich then hired Gionet to work on a project dedicated to gather Trump supporters together, MAGA3X. In July 2016, Gionet received special access to the Republican National Convention through his work with Milo Yiannopoulos. Following the election, Gionet continued to stay active in his pro-Trump efforts by giving speeches and participating in multiple rallies.\n\nGionet was also largely responsible for spearheading the #DumpKellogs and #TrumpCup hashtag movements. #TrumpCup was a trend that took place in November 2016 on Twitter. It began after allegations that a Starbucks employee refused a customer service because he asked for the name \"Trump\" to be written on the cup. The Twitter hashtag trend had more than 27,000 tweets in the span of 2 days. In April 2017, Gionet headed the #KeepBannon and #FireKushner hashtag trends after reports of conflict between two of Donald Trump's advisers.\n\nIn late 2016, following Trump's electoral victory, conflict arose between Cernovich and Gionet regarding Gionet's views on race. Gionet was disinvited from \"DeploraBall\" after making anti-Semitic remarks on Twitter. Gionet later mended his relationship with Cernovich and said he had misspoken.\n\nIn February 2017, Gionet called for a boycott of Netflix in response to the announcement of \"Dear White People\", a show Gionet said supported white genocide.\n\nGionet participated in the \"Freedom of Speech\" rally outside the Lincoln Memorial in Washington, DC on June 25, 2017, and he was scheduled to address white nationalists at the Unite the Right rally in Charlottesville, Virginia on August 11, 2017.\n\nAs of November 15, 2017, his Twitter account has been suspended.\n\nGionet supports the white supremacist Fourteen Words and has stated that there's \"nothing wrong\" with the slogan. Attempting to distance himself from its origins, he has claimed that \"just because others have used [it] doesn't change the meaning\". He has promoted the slogan on social media frequently, including with monetary receipts, polls, questions and memes.\n"}
{"id": "47812921", "url": "https://en.wikipedia.org/wiki?curid=47812921", "title": "Beatriz Ramírez Abella", "text": "Beatriz Ramírez Abella\n\nBeatriz Ramírez Abella (born 1956) is a Uruguayan feminist and activist working for Afro-Uruguayan rights. She is an anthropologist and educator teaching about class, ethnicity and gender and the biases surrounding these issues. She is the current director of the Uruguyan Instituto Nacional de las Mujeres (INMUJERES) (National Institute of Woman) and served as vice-chair on the Inter-American Commission of Women from 2013-2015.\n\nAngelica Beatriz Ramírez Abella was born on 18 September 1956 in Montevideo, Uruguay. She completed both grammar and secondary school in the public school system. Beginning in 1972, she started joining black youth groups and in 1973, founded the Grupos de Jóvenes de Asociación Cultural y Social Negro (ACSUN) (Black Cultural and Social Association Youth Group). She was also a co-founder of the Organización Mundo Afro (African World Organization).\n\nIn 1988, she attended the School of Social Service and the following year enrolled in gender studies at Grupo de Estudios sobre la Condicion de la Mujer en Uruguay (GRECMU) (Study Group on the Status of Women in Uruguay). In 1994 completed further studies on ethnicity and gender at Geledés Instituto da Mulher Negra (Geledés Institute of Black Women) in Brazil. She is a member of the Faculty of Humanities and teaches anthropology at the University of Uruguay. She has taught courses on ethnicity and gender at the Instituto Centro Latinoamericano de Economía Humana (CLAEH) (Central Latin American Institute for Human Economy); at the Black Association for the Defense and Promotion of Human Rights in Peru in 1993; and at the Global Afro-Latin and Caribbean Initiative in New York and Washington, DC. Since 2000 she has been part of the teaching network of the Instituto Superior de Formacion Afro (Afro-Training Institute of Higher Education).\n\nThroughout her career, Ramírez Abella has worked to improve access for people of African descent living in Latin America. She co-founded Proyecto Social Capitanes de la Arena, (CIPFE) (Social Captains of the Sand) in 1988 and served as its coordinator to assist street children and homeless persons. In 1992, she co-founded of the Network of Afro-Latin and Afro-Caribbean Women to better address issues which effect double minorities. She has also participated in conferences, such as the Third World Conference against Racism, and Xenophobia Related Intolerance held in Santiago, Chile in 2000; regional meetings of black parliamentarians held in Brasilia and Costa Rica; and as a member of the Delegation of Peace Mission to Colombia in 2002, among many others. She researches bias and has published numerous articles in newspapers and magazines such as \"Fem Press\" and \"Revista de Cotidiano Mujer\".\n\nIn 2005, she joined the Ministry of Social Development's National Women's Institute and in 2009 became head of the Instituto Nacional de las Mujeres (INMUJERES) (National Institute of Woman). As the director, she oversees governmental programs dealing with class, ethnicity and gender. In 2012, she was elected to serve as a Vice-chair on the Inter-American Commission of Women for the 213-2015 term.\n"}
{"id": "22026903", "url": "https://en.wikipedia.org/wiki?curid=22026903", "title": "Between Scylla and Charybdis", "text": "Between Scylla and Charybdis\n\nBeing between Scylla and Charybdis is an idiom deriving from Greek mythology, meaning \"having to choose between two evils\". Several other idioms, such as \"on the horns of a dilemma\", \"between the devil and the deep blue sea\", and \"between a rock and a hard place\" express similar meanings.\n\nScylla and Charybdis were mythical sea monsters noted by Homer; Greek mythology sited them on opposite sides of the Strait of Messina between Sicily and the Italian mainland. Scylla was rationalized as a rock shoal (described as a six-headed sea monster) on the Italian side of the strait and Charybdis was a whirlpool off the coast of Sicily. They were regarded as maritime hazards located close enough to each other that they posed an inescapable threat to passing sailors; avoiding Charybdis meant passing too close to Scylla and vice versa. According to Homer, Odysseus was forced to choose which monster to confront while passing through the strait; he opted to pass by Scylla and lose only a few sailors, rather than risk the loss of his entire ship in the whirlpool.\n\nBecause of such stories, having to navigate between the two hazards eventually entered idiomatic use. Another equivalent English seafaring phrase is, \"Between a rock and a hard place\". The Latin line \"incidit in scyllam cupiens vitare charybdim\" (he runs into Scylla, wishing to avoid Charybdis) had earlier become proverbial, with a meaning much the same as jumping from the frying pan into the fire. Erasmus recorded it as an ancient proverb in his \"Adagia\", although the earliest known instance is in the \"Alexandreis\", a 12th-century Latin epic poem by Walter of Châtillon.\n\nThe myth was given an allegorical interpretation by the French poet Barthélemy Aneau in his emblem book \"Picta Poesis\" (1552). There one is advised to choose the risk of being envied for wealth or reputation rather than swallowed by the Charybdis of poverty. \"Choose the lesser of these evils. A wise man would rather be envied than miserable.\"\n\nThe story was often applied to political situations at a later date. In James Gillray's cartoon, \"Britannia between Scylla and Charybdis\" (3 June 1793), 'William Pitt helms the ship \"Constitution\", containing an alarmed Britannia, between the rock of democracy (with the liberty cap on its summit) and the whirlpool of arbitrary power (in the shape of an inverted crown), to the distant haven of liberty'. This was in the context of the effect of the French Revolution on politics in Britain. That the dilemma had still to be resolved in the aftermath of the revolution is suggested by Percy Bysshe Shelley's returning to the idiom in his 1820 essay \"A Defence of Poetry\": \"The rich have become richer, and the poor have become poorer; and the vessel of the state is driven between the Scylla and Charybdis of anarchy and despotism.\"\n\nA later \"Punch\" caricature by John Tenniel, dated 10 October 1863, pictures the Prime Minister Lord Palmerston carefully steering the British ship of state between the perils of Scylla, a craggy rock in the form of a grim-visaged Abraham Lincoln, and Charybdis, a whirlpool which foams and froths into a likeness of Jefferson Davis. A shield emblazoned \"Neutrality\" hangs on the ship's thwarts, referring to how Palmerston tried to maintain a strict impartiality towards both combatants in the American Civil War. American satirical magazine \"Puck\" also used the myth in a caricature by F. Graetz, dated November 26, 1884, in which the unmarried President-elect Grover Cleveland rows desperately between snarling monsters captioned \"Mother-in-law\" and \"Office Seekers\".\n\nVictor Hugo uses the equivalent French idiom (\"tomber de Charybde en Scylla\") in his novel \"Les Miserables\" (1862), again in a political context, as a metaphor for the staging of two rebel barricades during the climactic uprising in Paris, around which the final events of the book culminate. The first chapter of the final volume is entitled \"The Charybdis of the Faubourg Saint Antoine and the Scylla of the Faubourg du Temple\".\n\nBy the time of Nicholas Monsarrat's 1951 war novel, \"The Cruel Sea\", however, the upper-class junior officer, Morell, is teased by his middle-class peer, Lockhart, for using such an old-fashioned phrase. Nevertheless, the idiom has since taken on new life in pop lyrics. In The Police's 1983 single \"Wrapped Around Your Finger\", the second line uses it as a metaphor for being in a dangerous relationship; this is reinforced by a later mention of the similar idiom of \"the devil and the deep blue sea\". American heavy metal band Trivium also referenced the idiom in \"Torn Between Scylla and Charybdis\", a track from their 2008 album \"Shogun\", in which the lyrics are about having to choose \"between death and doom\".\n\nIn 2014 Graham Waterhouse composed a piano quartet, \"Skylla and Charybdis\", premiered at the Gasteig in Munich. According to his programme note, though its four movements \"do not refer specifically to the protagonists or to events connected with the famous legend\", their dynamic is linked subjectively to images connected with it \"conjoured up in the composer's mind during the writing\".\n\n\n"}
{"id": "893668", "url": "https://en.wikipedia.org/wiki?curid=893668", "title": "Bias blind spot", "text": "Bias blind spot\n\nThe bias blind spot is the cognitive bias of recognizing the impact of biases on the judgment of others, while failing to see the impact of biases on one's own judgment. The term was created by Emily Pronin, a social psychologist from Princeton University's Department of Psychology, with colleagues Daniel Lin and Lee Ross. The bias blind spot is named after the visual blind spot. Most people appear to exhibit the bias blind spot. In a sample of more than 600 residents of the United States, more than 85% believed they were less biased than the average American. Only one participant believed that he or she was more biased than the average American. People do vary with regard to the extent to which they exhibit the bias blind spot. It appears to be a stable individual difference that is measurable (for a scale, see Scopelliti et al. 2015).\n\nThe bias blind spot appears to be a true blind spot in that it is unrelated to actual decision making ability. Performance on indices of decision making competence are not related to individual differences in bias blind spot. In other words, everyone seems to think they are less biased than other people, regardless of their actual decision making ability.\n\nBias blind spots may be caused by a variety of other biases and self-deceptions.\n\nSelf-enhancement biases may play a role, in that people are motivated to view themselves in a positive light. Biases are generally seen as undesirable, so people tend to think of their own perceptions and judgments as being rational, accurate, and free of bias. The self-enhancement bias also applies when analyzing our own decisions, in that people are likely to think of themselves as better decision makers than others.\n\nPeople also tend to believe they are aware of \"how\" and \"why\" they make their decisions, and therefore conclude that bias did not play a role. Many of our decisions are formed from biases and cognitive shortcuts, which are unconscious processes. By definition, people are unaware of unconscious processes, and therefore cannot see their influence in the decision making process.\n\nWhen made aware of various biases acting on our perception, decisions, or judgments, research has shown that we are still unable to control them. This contributes to the bias blind spot in that even if one is told that they are biased, they are unable to alter their biased perception.\n\nEmily Pronin and Matthew Kugler have argued that this phenomenon is due to the introspection illusion. In their experiments, subjects had to make judgments about themselves and about other subjects. They displayed standard biases, for example rating themselves above the others on desirable qualities (demonstrating illusory superiority). The experimenters explained cognitive bias, and asked the subjects how it might have affected their judgment. The subjects rated themselves as less susceptible to bias than others in the experiment (confirming the bias blind spot). When they had to explain their judgments, they used different strategies for assessing their own and others' bias.\n\nPronin and Kugler's interpretation is that, when people decide whether someone else is biased, they use overt behaviour. On the other hand, when assessing whether they themselves are biased, people look inward, searching their own thoughts and feelings for biased motives. Since biases operate unconsciously, these introspections are not informative, but people wrongly treat them as reliable indication that they themselves, unlike other people, are immune to bias.\n\nPronin and Kugler tried to give their subjects access to others' introspections. To do this, they made audio recordings of subjects who had been told to say whatever came into their heads as they decided whether their answer to a previous question might have been affected by bias. Although subjects persuaded themselves they were unlikely to be biased, their introspective reports did not sway the assessments of observers.\n\nPeople tend to attribute bias in an uneven way. When people reach different perceptions, they tend to label one another as biased while labelling themselves as accurate and unbiased. Pronin hypothesizes that this bias misattribution may be a source of conflict and misunderstanding between people. For example, in labeling another person as biased, one may also label their intentions cynically. But when examining one's own cognitions, people judge themselves based on their good intentions. It is likely that in this case, one may attribute another's bias to \"intentional malice\" rather than an unconscious process.\n\nPronin also hypothesizes ways to use awareness of the bias blind spot to reduce conflict, and to think in a more \"scientifically informed\" way. Although we are unable to control bias on our own cognitions, one may keep in mind that biases are acting on everyone. Pronin suggests that people might use this knowledge to separate other's intentions from their actions.\n\nInitial evidence suggests that the bias blind spot is not related to actual decision-making ability. Participants who scored better or poorer on various tasks associated with decision making competence were no more or less likely to be higher or lower in their susceptibility to bias blind spot. Bias blind spot does, however, appear to increase susceptibility to related biases. People who are high in bias blind spot are more likely to ignore the advice of other people, and are less likely to benefit from training geared to reduce their commission of other biases.\n\n"}
{"id": "45784", "url": "https://en.wikipedia.org/wiki?curid=45784", "title": "Biomimetics", "text": "Biomimetics\n\nBiomimetics or biomimicry is the imitation of the models, systems, and elements of nature for the purpose of solving complex human problems. The terms \"biomimetics\" and \"biomimicry\" derive from (\"bios\"), life, and μίμησις (\"mīmēsis\"), imitation, from μιμεῖσθαι (\"mīmeisthai\"), to imitate, from μῖμος (\"mimos\"), actor. A closely related field is bionics.\n\nLiving organisms have evolved well-adapted structures and materials over geological time through natural selection. Biomimetics has given rise to new technologies inspired by biological solutions at macro and nanoscales. Humans have looked at nature for answers to problems throughout our existence. Nature has solved engineering problems such as self-healing abilities, environmental exposure tolerance and resistance, hydrophobicity, self-assembly, and harnessing solar energy.\n\nOne of the early examples of would-be biomimicry was the study of birds to enable human flight. Although never successful in creating a \"flying machine\", Leonardo da Vinci (1452–1519) was a keen observer of the anatomy and flight of birds, and made numerous notes and sketches on his observations as well as sketches of \"flying machines\". The Wright Brothers, who succeeded in flying the first heavier-than-air aircraft in 1903, allegedly derived inspiration from observations of pigeons in flight. During the 1950s the American biophysicist and polymath Otto Schmitt developed the concept of \"biomimetics\". During his doctoral research he developed the Schmitt trigger by studying the nerves in squid, attempting to engineer a device that replicated the biological system of nerve propagation. He continued to focus on devices that mimic natural systems and by 1957 he had perceived a converse to the standard view of biophysics at that time, a view he would come to call biomimetics.\n\nIn 1960 Jack E. Steele coined a similar term, \"bionics\", at Wright-Patterson Air Force Base in Dayton, Ohio, where Otto Schmitt also worked. Steele defined bionics as \"the science of systems which have some function copied from nature, or which represent characteristics of natural systems or their analogues\". During a later meeting in 1963 Schmitt stated,\n\nIn 1969 Schmitt used the term “biomimetic“ in the title one of his papers, and by 1974 it had found its way into Webster's Dictionary, bionics entered the same dictionary earlier in 1960 as \"a science concerned with the application of data about the functioning of biological systems to the solution of engineering problems\". Bionic took on a different connotation when Martin Caidin referenced Jack Steele and his work in the novel \"Cyborg\" which later resulted in the 1974 television series \"The Six Million Dollar Man\" and its spin-offs. The term bionic then became associated with \"the use of electronically operated artificial body parts\" and \"having ordinary human powers increased by or as if by the aid of such devices\". Because the term \"bionic\" took on the implication of supernatural strength, the scientific community in English speaking countries largely abandoned it.\n\nThe term \"biomimicry\" appeared as early as 1982. Biomimicry was popularized by scientist and author Janine Benyus in her 1997 book \"Biomimicry: Innovation Inspired by Nature\". Biomimicry is defined in the book as a \"new science that studies nature's models and then imitates or takes inspiration from these designs and processes to solve human problems\". Benyus suggests looking to Nature as a \"Model, Measure, and Mentor\" and emphasizes sustainability as an objective of biomimicry.\n\nBiomimetics could in principle be applied in many fields. Because of the diversity and complexity of biological systems, the number of features that might be imitated is large. Biomimetic applications are at various stages of development from technologies that might become commercially usable to prototypes. Murray's law, which in conventional form determined the optimum diameter of blood vessels, has been re-derived to provide simple equations for the pipe or tube diameter which gives a minimum mass engineering system. \n\nAircraft wing design and flight techniques are being inspired by birds and bats. Biorobots based on the physiology and methods of locomotion of animals include BionicKangaroo which moves like a kangaroo, saving energy from one jump and transferring it to its next jump. Kamigami Robots, a children's toy, mimic cockroach locomotion to run quickly and efficiently over indoor and outdoor surfaces . \n\nResearchers studied the termite's ability to maintain virtually constant temperature and humidity in their termite mounds in Africa despite outside temperatures that vary from 1.5 °C to 40 °C (35 °F to 104 °F). Researchers initially scanned a termite mound and created 3-D images of the mound structure, which revealed construction that could influence human building design. The Eastgate Centre, a mid-rise office complex in Harare, Zimbabwe, stays cool without air conditioning and uses only 10% of the energy of a conventional building of the same size.\n\nIn structural engineering, the Swiss Federal Institute of Technology (EPFL) has incorporated biomimetic characteristics in an adaptive deployable \"tensegrity\" bridge. The bridge can carry out self-diagnosis and self-repair. The arrangement of leaves on a plant has been adapted for better solar power collection.\n\nAnalysis of the elastic deformation happening when a pollinisator lands on the sheath-like perch part of the flower \"Strelitzia reginae\" (known as Bird-of-Paradise flower) has inspired architects and scientists from the University of Freiburg and University of Stuttgart for the creation of hingeless shading systems that can react to their environment. These bio-inspired products is sold under the name Flectofin.\n\nOther hingeless bioinspired system includes Flectofold. Flectofold has been inspired from the trapping system developed by the carnivorous plant \"Aldrovanda vesiculosa\".\n\nThere is a great need for new structural materials that are light weight but offer exceptional combinations of stiffness, strength and toughness. \n\nSuch materials would need to be manufactured into bulk materials with complex shapes at high volume and low cost and would serve a variety of fields such as construction, transportation, energy storage and conversion. In a classic design problem, strength and toughness are more likely to be mutually exclusive i.e., strong materials are brittle and tough materials are weak. However, natural materials with complex and hierarchical material gradients that span from nano- to macro-scales are both strong and tough. Generally, most natural materials utilize limited chemical components but complex material architectures that give rise to exceptional mechanical properties. Understanding the highly diverse and multi functional biological materials and discovering approaches to replicate such structures will lead to advanced and more efficient technologies. Bone, nacre (abalone shell), teeth, the dactyl clubs of stomatopod shrimps and bamboo are great examples of damage tolerant materials. The exceptional resistance to fracture of bone is due to complex deformation and toughening mechanisms that operate at spanning different size scales - nanoscale structure of protein molecules to macroscopic physiological scale. Nacre exhibits similar mechanical properties however with rather simpler structure. Nacre shows a brick and mortar llike structure with thick mineral layer (0.2∼0.9-μm) of closely packed aragonite structures and thin organic matrix (∼20-nm). While thin films and micrometer sized samples that mimic these structures are already produced, successful production of bulk biomimetic structural materials is yet to be realized. However, numerous processing techniques have been proposed for producing nacre like materials. \n\nBiomorphic mineralization is a technique that produces materials with morphologies and structures resembling those of natural living organisms by using bio-structures as templates for mineralization. Compared to other methods of material production, biomorphic mineralization is facile, environmentally benign and economic. \n\nFreeze casting (Ice templating), an inexpensive method to mimic natural layered structures was employed by researchers at Lawrence Berkeley National Laboratory to create alumina-Al-Si and IT HAP-epoxy layered composites that match the mechanical properties of bone with an equivalent mineral/ organic content. Various further studies also employed similar methods to produce high strength and high toughness composites involving a variety of constituent phases. \n\nRecent studies demonstrated production of cohesive and self supporting macroscopic tissue constructs that mimic living tissues by printing tens of thousands of heterologous picoliter droplets in software-defined, 3D millimeter-scale geometries. Efforts are also taken up to mimic the design of nacre in artificial composite materials using fused deposition modelling and the helicoidal structures of stomatopod clubs in the fabrication of high performance carbon fiber-epoxy composites. \n\nVarious established and novel additive manufacturing technologies like PolyJet printing, direct ink writing, 3D magnetic printing, multi-material magnetically assisted 3D printing and magnetically-assisted slip casting have also been utilized to mimic the complex micro-scale architectures of natural materials and provide huge scope for future research. \n\nSpider web silk is as strong as the Kevlar used in bulletproof vests. Engineers could in principle use such a material, if it could be reengineered to have a long enough life, for parachute lines, suspension bridge cables, artificial ligaments for medicine, and other purposes. The self-sharpening teeth of many animals have been copied to make better cutting tools. \n\nNew ceramics that exhibit giant electret hysteresis have also been realized. \n\nIn general in biological systems, self healing occurs via chemical signals released at the site of fracture which initiate a systemic response that transport repairing agents to the fracture site thereby promoting autonomic healing. To demonstrate the use of micro-vascular networks for autonomic healing, researchers developed a microvascular coating–substrate architecture that mimics human skin. Bio-inspired self-healing structural color hydrogels that maintain the stability of an inverse opal structure and its resultant structural colors were developed. A self-repairing membrane for inspired by rapid self-sealing processes in plants was developed for inflatable light weight structures such as rubber boats or Tensairity® constructions. The researchers applied a thin soft cellular polyurethane foam coating on the inside of a fabric substrate, which closes the crack if the membrane is punctured with a spike. Self-healing materials, polymers and composite materials capable of mending cracks have been produced based on biological materials. \n\nSurfaces that recreate properties of shark skin are intended to enable more efficient movement through water. Efforts have been made to produce fabric that emulates shark skin. \n\nSurface tension biomimetics are being researched for technologies such as hydrophobic or hydrophilic coatings and microactuators.\n\nSome amphibians, such as tree and torrent frogs and arboreal salamanders, are able to attach to and move over wet or even flooded environments without falling. This kind of organisms have toe pads which are permanently wetted by mucus secreted from glands that open into the channels between epidermal cells. They attach to mating surfaces by wet adhesion and they are capable of climbing on wet rocks even when water is flowing over the surface. Tire treads have also been inspired by the toe pads of tree frogs.\n\nMarine mussels can stick easily and efficiently to surfaces underwater under the harsh conditions of the ocean. Mussels use strong filaments to adhere to rocks in the inter-tidal zones of wave-swept beaches, preventing them from being swept away in strong sea currents. Mussel foot proteins attach the filaments to rocks, boats and practically any surface in nature including other mussels. These proteins contain a mix of amino acid residues which has been adapted specifically for adhesive purposes. Researchers from the University of California Santa Barbara borrowed and simplified chemistries that the mussel foot uses to overcome this engineering challenge of wet adhesion to create copolyampholytes, and one-component adhesive systems with potential for employment in nanofabrication protocols. Other research has proposed adhesive glue from mussels.\n\nLeg attachment pads of several animals, including many insects (e.g. beetles and flies), spiders and lizards (e.g. geckos), are capable of attaching to a variety of surfaces and are used for locomotion, even on vertical walls or across ceilings. Attachment systems in these organisms have similar structures at their terminal elements of contact, known as setae. Such biological examples have offered inspiration in order to produce climbing robots, boots and tape . Synthetic setae have also been developed for the production of dry adhesives.\n\nBiomimetic materials are gaining increasing attention in the field of optics and photonics. There are still little known bioinspired or biomimetic products involving the photonic properties of plants or animals. However, understanding how nature designed such optical materials from biological resources is worth pursuing and might lead to future commercial products. \nFor instance, the chiral self-assembly of cellulose inspired by the \"Pollia condensata\" berry has been exploited to make optically active films. Such films are made from cellulose which is a biodegradable and biobased ressource obtained from wood or cotton. The structural colours can potentially be everlasting and have more vibrant colour than the ones obtained from chemical absorption of light. \"Pollia condensata\" is not the only fruit showing a structural coloured skin, other berries such as \"Margaritaria nobilis\" does. These fruits show iridescent colors in the blue-green region of the visible spectrum which gives the fruit a strong metallic and shiny visual appearance. The structural colours come from the organisation of cellulose chains in the fruit's epicarp, a part of the fruit skin. Each cell of the epicarp is made of a multilayered envelope that behaves like a Bragg reflector. However, the light which is reflected from the skin of these fruits is not polarised unlike the one arising from man-made replicates obtained from the self-assembly of cellulose nanocrystals into helicoids, which only reflect left-handed circularly polarised light. \n\nThe fruit of \"Elaeocarpus angustifolius\" also show structural colour that come arises from the presence of specialised cells called iridosomes which have layered structures. Similar iridosomes have also been found in Delarbrea michieana fruits.\n\nIn plants, multi layer structures can be found either at the surface of the leaves (on top of the epidermis), such as in \"Selaginella willdenowii\" or within specialized intra-cellular organelles, the so-called iridoplasts, which are located inside the cells of the upper epidermis. For instance, the rain forest plants Begonia pavonina have iridoplasts located inside the epidermal cells. \n\nStructural colours have also been found in several algae, such as in the red alga \"Chondrus crispus\" (Irish Moss).\n\nStructural coloration produces the rainbow colours of soap bubbles, butterfly wings and many beetle scales. Phase-separation has been used to fabricate ultra-white scattering membranes from polymethylmethacrylate, mimicking the beetle \"Cyphochilus\".\n\n\"Morpho\" butterfly wings are structurally coloured to produce a vibrant blue that does not vary with angle. This effect can be mimicked by a variety of technologies. Lotus Cars claim to have developed a paint that mimics the \"Morpho\" butterfly's structural blue colour. In 2007, Qualcomm commercialised an interferometric modulator display technology, \"Mirasol\", using \"Morpho\"-like optical interference. In 2010, the dressmaker Donna Sgro made a dress from Teijin Fibers' Morphotex, an undyed fabric woven from structurally coloured fibres, mimicking the microstructure of \"Morpho\" butterfly wing scales.\n\nProtein folding has been used to control material formation for self-assembled functional nanostructures. Polar bear fur has inspired the design of thermal collectors and clothing. The light refractive properties of the moth's eye has been studied to reduce the reflectivity of solar panels. \n\nThe Bombardier beetle's powerful repellent spray inspired a Swedish company to develop a \"micro mist\" spray technology, which is claimed to have a low carbon impact (compared to aerosol sprays). The beetle mixes chemicals and releases its spray via a steerable nozzle at the end of its abdomen, stinging and confusing the victim.\n\nMost viruses have an outer capsule 20 to 300 nm in diameter. Virus capsules are remarkably robust and capable of withstanding temperatures as high as 60 °C; they are stable across the pH range 2-10. Viral capsules can be used to create nano device components such as nanowires, nanotubes, and quantum dots. Tubular virus particles such as the tobacco mosaic virus (TMV) can be used as templates to create nanofibers and nanotubes, since both the inner and outer layers of the virus are charged surfaces which can induce nucleation of crystal growth. \n\nThis was demonstrated through the production of platinum and gold nanotubes using TMV as a template. Mineralized virus particles have been shown to withstand various pH values by mineralizing the viruses with different materials such as silicon, PbS, and CdS and could therefore serve as a useful carriers of material. A spherical plant virus called cowpea chlorotic mottle virus (CCMV) has interesting expanding properties when exposed to environments of pH higher than 6.5. Above this pH, 60 independent pores with diameters about 2 nm begin to exchange substance with the environment. The structural transition of the viral capsid can be utilized in Biomorphic mineralization for selective uptake and deposition of minerals by controlling the solution pH. Possible applications include using the viral cage to produce uniformly shaped and sized quantum dot semiconductor nanoparticles through a series of pH washes. This is an alternative to the apoferritin cage technique currently used to synthesize uniform CdSe nanoparticles. Such materials could also be used for targeted drug delivery since particles release contents upon exposure to specific pH levels.\n\n\n\n"}
{"id": "1298497", "url": "https://en.wikipedia.org/wiki?curid=1298497", "title": "Book of the Civilized Man", "text": "Book of the Civilized Man\n\nBook of the Civilized Man by Daniel of Beccles (, also known as Liber Urbani, Urbanus Magnus, or Civilized Man) is believed to be the first English courtesy book (or book of manners), dating probably from the beginning of the 13th century. The book is significant because in the later Middle Ages dozens of such courtesy books were produced. Because this appears to be the first in English history, it represented a new awakening to etiquette and decorum in English court society, which occurred in the 13th century. As a general rule, a book of etiquette is a mark of a dynamic rather than a stable society, one in which there is an influx of \"new\" men, who have not been indoctrinated with the correct decorum from an early age and who are avid to catch up in a hurry.\n\n\"Civilized Man\" is a 3000-line Latin verse poem that gives proper advice on a wide range of social situations that the typical medieval person might have encountered in day-to-day life.\n\nExamples include:\n\nHistorians believe that Daniel of Beccles may have been a member of Henry II's court. John Bale (16th century) wrote that he had seen a document showing Daniel in Henry's court for over 30 years. This, the fact that a Henry is mentioned in the text, and some of the manuscripts can be dated to the early 13th century, make it very probable the poem dates from that period. There a reference to a Daniel of Beccles in the \"Seventh Regnal Year of King John\" (circa 1206) secretly being given the patronage (advowson) of the church of Endgate in Beccles by the Abbot of Bury St Edmund's.\n\nThere are three major recurrent themes in the poem: social hierarchy, self-control and sexual morality.\n\nThe first theme is the emphasis on social hierarchy and how to behave around those of higher or lower status (lords and servants). The poem takes the general tone of addressing the reader as someone who is a \"householder\". This can be seen in the opening lines of the poem \"Reader, if you wish to be adorned with good manners, if you wish to be respected and lead a civilized life as a noble householder...\". In other words, it addresses a very minor upper percentage of the general population who own households and have servants, the class that from the 16th century might be identified as \"country gentry\".\n\nThe second recurrent theme is self-control, in general holding oneself inward when it comes to speaking, eating and bodily emissions. When it comes to speaking, \"Be careful to whom, what, why and when you speak\". He suggests it is better to keep your thoughts to yourself. When it comes to eating, he suggests small bites, not overeating, not playing with food, no using fingers to clean bowls. Also, guests and servants should not urinate in the dining hall, but the host may.\n\nThe third recurrent theme is sexual morality. The \"Civilized Man\" was clearly written for men. It offers advice on prostitutes: \"If you are overcome with erotic desire when you are young and your penis drives you to go to a prostitute, do not go to a common whore; empty your testicles quickly and depart quickly.\" He offers advice on how to pick a wife, which includes looking at her property value and personal traits. Following a tradition inherited from antiquity, Daniel describes women as lustful and untrustworthy. The poem describes a woman lying in bed with her husband, with her thoughts on to her secret lover: \"The lascivious woman throws herself around the neck of her lover, her fingers give him those secret touches that she denies to her husband in bed; one wicked act with her lover pleases the lascivious adulteress more than a hundred with her husband; women's minds always burn for the forbidden.\" He says she is always ready to fornicate \"with a cook or a half-wit, a peasant or a ploughman, or a chaplain... what she longs for is a thick, leaping, robust piece of equipment, long, smooth and stiff... such are the things that charm and delight women\". Daniel had a nearly pathological view of the promiscuity of women, but despite this he says \"Whatever your wife does, do not damage your marriage\" and he goes on to say \"if you are jealous, do not whisper a word about it... when you are jealous, learn to look up at the ceiling.\" The message is clearly in the same theme of holding inward and avoiding any embarrassments at all cost.\n\nDaniel's advice comes to a climax in what is perhaps the most difficult situation of all: the wife of one's lord makes a sexual proposition. It is a combination of the three problems: hierarchical relationships, control of bodily emissions, and sexual morality. Daniel's solution? Pretend to be ill.\n\n\n"}
{"id": "38017421", "url": "https://en.wikipedia.org/wiki?curid=38017421", "title": "Bounding point", "text": "Bounding point\n\nIn functional analysis, a branch of mathematics, a bounding point of a subset of vector space is a conceptual extension of the boundary of the set.\n\nLet formula_1 for some vector space formula_2. Then formula_3 is a \"bounding point\" for formula_4 if it is neither an internal point for formula_4 nor its complement.\n"}
{"id": "1378866", "url": "https://en.wikipedia.org/wiki?curid=1378866", "title": "Brute fact", "text": "Brute fact\n\nIn contemporary philosophy, a brute fact is a fact that has no explanation. More narrowly, brute facts may instead be defined as those facts which cannot be explained (as opposed to simply having no explanation). To reject the existence of brute facts is to think that everything can be explained. (\"Everything can be explained\" is sometimes called the principle of sufficient reason). There are two ways to explain something: say what \"brought it about\", or describe it at a more \"fundamental\" level. For example, a cat displayed on a computer screen can be explained, more \"fundamentally\", as there being certain voltages in bits of metal in the screen, which in turn can be explained, more \"fundamentally\", as certain subatomic particles moving in a certain manner. If we keep explaining the world in this way and reach a point at which no more \"deeper\" explanations can be given, then we would have found some facts which are brute or inexplicable, in the sense that we . As it might be put, there may exist some things that just \"are\". The same thing can be done with \"causal explanations\". If nothing made the Big Bang expand at the velocity it did, then this is a brute fact in the sense that it lacks a causal explanation.\n\nHenri Poincaré distinguished between brute facts and their scientific descriptions, pointing to how the conventional nature of the latter always remained constrained by the brute fact in question.\n\nPierre Duhem argued that just as there may be several scientific descriptions of the same brute fact, so too there may be many brute facts with the same scientific description.\n\nG. E. M. Anscombe wrote about how facts can be brute relative to other facts. Simply put, some facts cannot be reducible to other facts, such that if some set of facts holds true, it does not entail the fact brute relative to it.\n\nThe example she uses is that of someone owing a grocer money for supplying them with potatoes. In such a case, the set of facts, e.g. that the customer asked for the potatoes, that the grocer supplied them with the potatoes, etc., does not necessarily entail that the customer owes the grocer money. After all, this could all have transpired on the set of a film as a bit of acting, in which case the customer would not \"actually\" owe anything.\n\nOne might argue that if the institutional context is taken into account, putatively brute facts \"can\" be reduced to constituent facts. That is, in the context of something like the institution of a market, a customer ordering potatoes, etc. would entail that they owe the grocer compensation equal to the service that was provided. While Anscombe does acknowledge that an institutional context is necessary for a particular description to make sense, it does not necessarily follow that a particular set of facts holding true in an institutional context entails the fact brute relative to it. To wit, if the example is indeed considered in the institutional context necessary for descriptions of 'owing', it could still be the case that the customer does not owe the grocer, per the counterexample of a film production. This fundamental ambiguity is essentially what makes a fact brute relative to other facts.\n\nThat being said, Anscombe does argue that under normal circumstance, such a fact \"is\" actually entailed. That is, if it is true that a customer requested potatoes, etc., then \"under normal circumstances\" the customer would indeed owe the grocer money. However, because such entailment is conditional on such a set of facts holding true under a particular set of circumstances, the fact entailed is still fundamentally brute relative to such facts, just that in such a case the leap in inference occurs at the level of the circumstances, not that of the facts themselves.\n\nFinally, if a fact brute relative to other facts holds true, it follows that some set of facts it is brute relative to is also true, e.g. if the customer owes the grocer money, then it follows that the grocer supplied them with potatoes. After all, had they not done so, then the customer would not owe them money. As such, given some fact brute relative to other facts, there is a range of facts, such that a set of them will hold if the fact brute relative to them also holds. That being said, Anscombe argues that the full range of facts that some fact can be brute relative to cannot be known exhaustively. The rough range can be sketched out with relevant, paradigmatic examples, but the full range of such facts cannot be known, as one can theoretically always suppose a new special context that changes the range.\n\nJohn Searle developed Anscombe's concept of brute facts into what he called brute physical facts—such as that snow is on Mt. Everest—as opposed to social or institutional facts, dependent for their existence on human agreement. Thus, he considered money to be an institutional fact, which nevertheless rested ultimately on a brute physical fact, whether a piece of paper or only an electronic record.\n\nSearle thought that the pervasiveness of social facts could disguise their social construction and ultimate reliance upon the brute fact: thus, we are for example trained from infancy (in his words) to see \"cellulose fibres with green and gray stains, or enamel-covered iron concavities containing water...[as] dollar bills, and full bathtubs\".\n\nThe principle of sufficient reason is sometimes understood to entail that there are no brute facts.\n\nAccording to the Principle of Sufficient Reason all facts must have an explanation and this seems to exclude brute facts. But the truth of this principle is not accepted by all. After all, if no brute facts exist, how are fundamental laws possible? One could argue that the principle of sufficient reason is true a priori but that is something that has famously been disputed by David Hume.\n\nIn 2018 Elly Vintiadis edited a collection of papers on brute facts that is the first systematic exploration of bruteness and which includes original papers by a number of philosophers and scientists. The collection focuses on physical, emergent and modal brute facts rather than social facts. Vintiadis argues that a properly understood naturalistic attitude requires that we accept the existence of ontological brute facts and also, possibly, emergent brute facts.\n\nBeyond the initial definition given above of brute facts as facts that that do not have explanations, there is a distinction drawn by Eric Barnes (1994) between epistemically brute facts and ontologically brute facts. The former are for which we do not have an explanation, they are brute for us (e.g., Vintiadis cites the fact that gases behave in a manner described by the Boyle-Charles law was an epistemologically brute fact until its explanation in terms of the kinetic theory of gases). The latter, ontologically brute facts are facts for which there is no explanation in virtue of the way the world is (e.g., the fundamental laws of physics). Which facts we accept as ontologically brute though depends on what kind of theory of explanation we accept (e.g. the properties of fundamental particles will be brute facts under a mereological view of explanation, but a fundamental law will be brute under a covering law model of explanation).\n\nJohn Heil has argued that brute facts can only be contingent facts, since otherwise asking for an explanation for something that couldn’t be otherwise doesn’t make sense. Joseph Levine agrees with this since for him explanation means removing different possibilities. But not all agree, because some philosophers argue that it is a natural question to ask why some things are necessary. For instance philosopher James Van Cleve believes that brute necessities cannot be excluded.\n\nAccording to explanatory infinitism, the chain of explanations goes on infinitely and there is no fundamental explanation. This, then, is another way of objecting to the existence of explanatory brute facts, but also metaphysical brute facts, if bruteness is understood in terms of ontological independence.\n\n"}
{"id": "483048", "url": "https://en.wikipedia.org/wiki?curid=483048", "title": "Bypass ratio", "text": "Bypass ratio\n\nThe bypass ratio (BPR) of a turbofan engine is the ratio between the mass flow rate of the bypass stream to the mass flow rate entering the core. A 10:1 bypass ratio, for example, means that 10 kg of air passes through the bypass duct for every 1 kg of air passing through the core.\n\nTurbofan engines are usually described in terms of bpr, which together with engine pressure ratio, turbine inlet temperature and fan pressure ratio are important design parameters. In addition bpr is quoted for turboprop and unducted fan installations because their high propulsive efficiency gives them the overall efficiency characteristics of very high bypass turbofans. This allows them to be shown together with turbofans on plots which show trends of reducing sfc (specific fuel consumption) with increasing bpr. Bpr is also quoted for lift fan installations where the fan airflow is remote from the engine and doesn't physically touch the engine core.\n\nBypass provides a lower fuel consumption for the same thrust, measured as thrust specific fuel consumption (grams/second fuel per unit of thrust in kN using SI units). Lower fuel consumption that comes with high bypass ratios applies to turboprops, using a propeller rather than a ducted fan. High bypass designs are the dominant type for commercial passenger aircraft and both civilian and military jet transports.\n\nBusiness jets use medium bpr engines.\n\nCombat aircraft use engines with low bypass ratios to compromise between fuel economy and the requirements of combat: high power-to-weight ratios, supersonic performance, and the ability to use afterburners.\n\nIf all the gas power from a gas turbine is converted to kinetic energy in a propelling nozzle, the aircraft is best suited to high supersonic speeds. If it is all transferred to a separate big mass of air with low kinetic energy, the aircraft is best suited to zero speed (hovering). For speeds in between, the gas power is shared between a separate airstream and the gas turbine's own nozzle flow in a proportion which gives the aircraft performance required. The first jet aircraft were subsonic and the poor suitability of the propelling nozzle for these speeds due to high fuel consumption was understood, and bypass proposed, as early as 1936 (U.K. Patent 471,368). \nThe underlying principle behind bypass is trading exhaust velocity for extra mass flow which still gives the required thrust but uses less fuel. Frank Whittle called it \"gearing down the flow\". Power is transferred from the gas generator to an extra mass of air, i.e. a bigger diameter propelling jet, moving more slowly. The bypass spreads the available mechanical power across more air to reduce the velocity of the jet. The trade off between mass flow and velocity is also seen with propellers and helicopter rotors by comparing disc loading and power loading. For example, the same helicopter weight can be supported by a high power engine and small diameter rotor or, for less fuel, a lower power engine and bigger rotor with lower velocity through the rotor.\n\nBypass usually refers to transferring gas power from a gas turbine to a bypass stream of air to reduce fuel consumption and jet noise. Alternatively, there may be a requirement for an afterburning engine where the sole requirement for bypass is to provide cooling air. This sets the lower limit for bpr and these engines have been called \"leaky\" or continuous bleed turbojets (General Electric YJ-101 bpr 0.25) and low bpr turbojets (Pratt & Whitney PW1120). Low bpr (0.2) has also been used to provide surge margin as well as afterburner cooling for the Pratt & Whitney J58.\n\nIn a zero-bypass (turbojet) engine the high temperature and high pressure exhaust gas is accelerated by expansion through a propelling nozzle and produces all the thrust. The compressor absorbs all the mechanical power produced by the turbine. In a bypass design extra turbines drive a ducted fan that accelerates air rearward from the front of the engine. In a high-bypass design, the ducted fan and nozzle produce most of the thrust. Turbofans are closely related to turboprops in principle because both transfer some of the gas turbine's gas power, using extra machinery, to a bypass stream leaving less for the hot nozzle to convert to kinetic energy. Turbofans represent an intermediate stage between turbojets, which derive all their thrust from exhaust gases, and turbo-props which derive minimal thrust from exhaust gases (typically 10% or less). Extracting shaft power and transferring it to a bypass stream introduces extra losses which are more than made up by the improved propulsive efficiency. The turboprop at its best flight speed gave significant fuel savings over a turbojet even though an extra turbine, a gearbox and a propeller were added to the turbojet's low-loss propelling nozzle. The turbofan has additional losses from its extra turbines, fan, bypass duct and extra propelling nozzle compared to the turbojet's single nozzle.\n\nTo see the influence of increasing bpr alone on overall efficiency in the aircraft, i.e. sfc, a common gas generator has to be used, i.e. no change in Brayton cycle parameters or component efficiencies. Bennett shows in this case a relatively slow rise in losses transferring power to the bypass at the same time as a fast drop in exhaust losses with a significant improvement in sfc. In reality increases in bpr over time come along with rises in gas generator efficiency masking, to some extent, the influence of bpr.\n\nOnly the limitations of weight and materials (e.g., the strengths and melting points of materials in the turbine) reduce the efficiency at which a turbofan gas turbine converts this thermal energy into mechanical energy, for while the exhaust gases may still have available energy to be extracted, each additional stator and turbine disk retrieves progressively less mechanical energy per unit of weight, and increasing the compression ratio of the system by adding to the compressor stage to increase overall system efficiency increases temperatures at the turbine face. Nevertheless, high-bypass engines have a high propulsive efficiency because even slightly increasing the velocity of a very large volume and consequently mass of air produces a very large change in momentum and thrust: thrust is the engine's mass flow (the amount of air flowing through the engine) multiplied by the difference between the inlet and exhaust velocities in—a linear relationship—but the kinetic energy of the exhaust is the mass flow multiplied by one-half the square of the difference in velocities. A low disc loading (thrust per disc area) increases the aircraft's energy efficiency, and this reduces the fuel use.\n\nThe Rolls–Royce Conway turbofan engine, developed in the early 1950s, was an early example of a bypass engine. The configuration was similar to a 2-spool turbojet but to make it into a bypass engine it was equipped with an oversized low pressure compressor: the flow through the inner portion of the compressor blades went into the core while the outer portion of the blades blew air around the core to provide the rest of the thrust. The bypass ratio for the Conway varied between 0.3 and 0.6 depending on the variant\n\nThe growth of bypass ratios during the 1960s gave jetliners fuel efficiency that could compete with that of piston-powered planes.\nToday (2015), most jet engines have some bypass. Modern engines in slower aircraft, such as airliners, have bypass ratios up to 12:1; in higher-speed aircraft, such as fighters, bypass ratios are much lower, around 1.5; and craft designed for speeds up to Mach 2 and somewhat above have bypass ratios below 0.5.\n\nTurboprops have bypass ratios of 50-100, although the propulsion airflow is less clearly defined for propellers than for fans and propeller airflow is slower than the airflow from turbofan nozzles.\n"}
{"id": "42130800", "url": "https://en.wikipedia.org/wiki?curid=42130800", "title": "Children's use of information", "text": "Children's use of information\n\nChildren's use of information is an issue in ethics and child development. Information is learned from many different sources and source monitoring (see also source-monitoring error) is important in understanding how people use information and decide which information is credible. Consider the example of a parent whose child has been diagnosed with hyperactivity; the parent searches the internet for information, reads books, participates in an online chat room with other parents in the same situation, and consults various medical professionals. Some of these sources will be credible (contain reliable information), and others will not. To be well-informed, the parent must filter information according to the reliability of the source.\n\nChildren learn about the world in much the same way. They are told things by numerous people (e.g., teachers, parents, siblings, and friends), see things on the television or internet, and read information in books. Can children be effective consumers of information? At what age are they able to do this? How do they deal with ambiguous resources? This page will detail answers to those questions (and others) by drawing on peer-reviewed scientific research.\n\nYoung children have more difficulty with understanding and recalling the sources of information than adults do. Although episodic memory improves throughout childhood, development in the area of source monitoring tends to occur between the ages of 3 and 8 years. At 3 years, children who are able to immediately recognize the source of the information they obtain have difficulty recalling this information after a short delay. The development of source monitoring is gradual, and children achieve and display competency in certain aspects of source monitoring before others. The developmental trajectory of source monitoring provides insight into what cognitive factors are necessary prerequisites. While there is no generally accepted unified theory for the development of source monitoring, five major theories contribute ideas about how source monitoring develops in children: source monitoring theory, fuzzy-trace theory, schema theory, person-based perspective, and the mental-state reasoning model.\n\nAccording to source monitoring theory, the source of information is attributed through a decision-making process, where source is inferred based on various characteristics inherent in the memory itself. This means that the sources are not directly encoded, but rather reconstructed, when information is recalled. This decision-making process can either be through automatic, unconscious processing, or through heightened demanding and systematic processing that may require reasoning, and the retrieval of supporting memories. This theory implicates the development of episodic memory and memory strategies in the development of source monitoring more generally.\n\nFuzzy-trace theory hypothesizes that the source-monitoring errors that children make are caused by problems with memory storage and retrieval. Memories are simultaneously stored in two different formats: the \"gist\" level (extracted from the experience), and the \"verbatim\" representation (information in exact detail). It is proposed that the source of information is encoded in memory as a verbatim detail. Memories for verbatim details decay more quickly over time than gist representations, and young children demonstrate faster decay of verbatim information than older children or adults. Younger children are more likely to experience memory intrusions due to weaker memory traces, which leads to a susceptibility to misleading information replacing memory traces from a previous event. For this reason, developmental changes in episodic memory performance are viewed as the driving factor in source monitoring development.\n\nSchema theory, as a derivative of script theory, states that after repeated exposure to similar events, individuals form a general representation of what typically happens. Some details are the same at each instance of repeated events, and others can vary from instance to instance. In the script for what usually happens, there is a \"slot\" for each variable detail and the detail for a particular time is chosen from a list of possible variations. If source information is encoded as a slot, errors in source monitoring can be the result of incorrect retrieval of a specific detail. This framework allows for the storage of a large amount of detailed information about specific events, however is very cognitively demanding for children. Because of their limited cognitive resources, young children require more experience with repeated events in order to generate a schema. Details that vary from instance to instance can be lost, and children rely instead on the generalized event representation when attempting to recall a particular instance. The main assumption is that the development of source monitoring depends on an increase in cognitive processing capacity.\n\nPerson-based perspective emphasizes prospective processes. Prospective processes relate actions to one another through operations such as sequencing and planning. These processes are affected by the perspective taken (self vs. other) or the goals and meaning to the individual. The meaning attached to the goal of an action can interfere with source monitoring by removing attention from the source details. The person-based perspective is social-cognitive in nature (more so than other theories of source monitoring) and assumes that development is reliant on socialization and theory of mind development.\n\nThe mental-state reasoning model highlights possible mechanisms behind young children's suggestibility. Children who have difficulty with reasoning about conflicting mental representations are likely to overwrite their original memories with misinformation because they cannot reconcile two contradicting views of what actually occurred. Source monitoring and the understanding of knowledge states, play a key role in resisting suggestions. The development of source monitoring is presumed to be based on better understanding of knowledge states, metacognition and theory of mind.\n\nBeing aware of how we have acquired information is particularly difficult for young children (specifically 3 to 5 year-olds). After feeling a soft ball, 3 and 4 year-olds can correctly identify whether they know the ball is soft or hard, but cannot always say how they know. Perhaps it is because they felt the ball, saw the ball, or were told it was soft. The ability to recognize the origin of their knowledge requires the understanding of how knowledge is acquired.\n\nWhen asked knowledge questions (\"Do you know what is in the box?\") then justification questions (\"How do [or why don't] you know what is in the box?\"), children who can correctly answer are able to reflect on knowledge that they have gained from a particular source and should be able to identify the source of information.\n\n3 and 4 year-olds are better at answering these questions when the questions refer to themselves in comparison to referring to another person. When these children are shown a hidden object they can correctly report their perceptual access (responding correctly to \"Did you look into the box?\") and their knowledge access of what is in the box (responding correctly to \"Do you know [not know] what is in the box?\"). Despite their ability to answer correctly when referring to the self, 3 and 4 year-olds have difficulty responding to the \"other's\" knowledge, where they deny the \"other\" has the knowledge. 3-4 year-olds can correctly identify that the other person has in fact looked in a box, but when asked \"Does [other] know what is in the box?\" the child will deny that the other person knows what is in the box. The key assumption is then that although children are aware that perceptual access is needed, they are unable to acknowledge that the knowledge was gained from perceptual access.\n\nGaining knowledge is knowing how to acquire new information. The ability to recognize how specific knowledge can be gained by perceptual access (looking, feeling or smelling) is the understanding of Aspectuality. Aspectuality understanding is the awareness that an object is made up of many different properties (colour, weight, odour), which can be determined by a specific perceptual action (looking, feeling, smelling). Identifying the colour of a car outside the window could involve asking someone for the information or looking out the window. In most cases, adults would acquire this knowledge from simply looking out the window. However, not all information can be gained in this manner. If you were asked to find out how old the driver of the car is, simply looking would not provide accurate information; you would have to ask. These two situations are dependent on whether the information being sought out is visible or invisible (respectively). Knowledge development depends on children's ability to efficiently pursue their informational goal. Children do not always make the most effective or efficient decision when acquiring new information.\n\n6 year-olds are able to distinguish when gaining knowledge requires looking (the information is visible) or when gaining knowledge requires asking (the information is invisible). However, 4 year-olds do not perform as consistently. Even with an expert present, 4 year-olds will overestimate the knowledge they can gain through looking. However, when the information to be gained is regarding a group of friends and the expert is a friend of the group, 4 year-olds tend to overestimate knowledge acquisition through asking.\n\nWhen given access to pairs of objects, which could either be identified by seeing (identical objects: different in colour) or by touch/feeling (identical objects: 1 soft, 1 hard), children perform relatively well, generally recognizing when they have adequate information and when they have inadequate information (i.e., knowing the object is blue after seeing it or knowing the object is soft after feeling it). But, when young children are not given access (of seeing or feeling), they have difficulty predicting and identifying which mode of access would allow them to identify the object. In this case, 3 and 4 year-olds overestimate the knowledge that could be gained through feeling the object. 3 to 4 year-olds may not understand the perceptual access needed to acquire the specific knowledge.\n\nWaters and Beck (2012) state \"understanding the link between perceptual access and consequent information (knowledge access) is a crucial component in the development of theory of mind\". In a typical adult population, change in the phrasing of a particular question would not affect the understanding of what action needs to occur. \"What colour is the bike that is outside?\" or \"Is the bike red?\" should not change the action of \"looking\" to gain the needed information. However, children's performance is susceptible to this type of question phrasing. There are language effects on knowledge access through 3 different question types:\n\n\n4 and 5 year-olds are more likely to respond correctly to aspect and dimension questions in comparison to identity questions. Aspect and dimension questions are more explicit, which may be why performance is better in these categories. Identity questions require more cognitive effort as one has to remember how the objects were similar and how they differed.\n\nMuch of people's knowledge about the world is not obtained from direct experience, but indirectly from what other people or sources tell them. With widespread use of the internet people have access to nearly unlimited sources of information. Some of that information might be conflicting, and different sources of information vary in their accuracy and credibility. People can also deliberately deceive, be misunderstood, or be mistaken. It is important that people develop the necessary skills to assess the accuracy of what they are being told.\n\nAdults make credibility judgments based on two factors: expertise and trustworthiness. For example, we might trust information from published research articles more than information from blogs because we know that anyone can write a blog, but scientists (experts) who write peer-reviewed articles are highly trained. With regards to trustworthiness, adults are less likely to believe someone who they think is trying to deceive them, and they take into account the intentions of the information-provider. For example, if someone's intention is to sell you something, you might be more skeptical of the accuracy of the information they provide because their motives cause you to question their honesty. Adults can effectively use these cues to make judgments about the credibility of different sources, but whether children can also do so is an important area for research.\n\nEven very young children show an early sensitivity to issues of source credibility. By the age of 4, children show similar patterns to adults in a preference for perception over testimony; that is, they would rather see something with their own eyes than be told about it. Children also feel more confident in their knowledge when they have directly perceived it than if they have been told by someone else, even if the speaker is well informed. However, children are not always able to directly perceive information, and they learn much of what they know from others. Some people are more credible sources of information than others, so children must actively evaluate information and decide whether or not to believe it. There are many factors or cues that children, by the age of 4, take into consideration when making judgements about whether or not to trust what a person says. For instance, knowledge and experience, traits, motivations, age, and reasoning or support.\n\nYoung children have a reputation for credulity, or believing something without having any proof that it is true. Young children often trust what adults tell them, especially when they have no prior knowledge or expectations about the topic of the testimony. Because children tend to interact with adults who are more knowledgeable than themselves, if they have no reason to believe otherwise, they will trust what adults tell them. In particular, 3 year-olds tendency to believe others is based on a selective bias to trust what people tell them. This selective trust is adaptive in the early years as they are learning language and their way around the world.\n\nChildren have difficulty disregarding information provided to them, even if the source has been unreliable in the past. 3 year-olds will often continue to believe what a person tells them even after being repeatedly deceived by that person, but 4 year-olds are far better able to disregard this unreliable information. Three-year-olds are better at making trust judgments when they are able to choose between two sources of knowledge, rather than deciding whether or not to believe a single person, and in this situation they are often able to choose the more reliable of two speakers. By age 4, young children take an informant's knowledge, expertise, and reliability into account in order to avoid learning from unreliable or problematic sources. They believe statements made by knowledgeable speakers more than ignorant speakers, before they can explicitly answer questions about who has access to knowledge. They also prefer to seek information from sources who have been knowledgeable in the past. 4 year-olds can spontaneously use others' past performance to guide their learning.\n\nAlmost all human institutions (e.g., family, community, business, government) are built on a foundation of trust. There are many factors that influence children's trust in people and one of the most important is honesty. There are various schools of philosophical thought that posits honesty to be morally right and lying to be morally wrong. On one end of the continuum, philosophers like Bok, Kant, and St. Augustine hold a deontological view that focuses on intrinsic duties relating to the treatment of others. In other words, telling the truth is intrinsically right and lying is intrinsically wrong. On the other end of the continuum is the utilitarian view that emphasizes the greater good, specifically with respect to the outcome of one's act. Therefore, lying and its moral implications are context dependent. In some situations, such as when being polite to spare another person's feelings, making a \"prosocial lie\" or deliberate false statement are endorsed.\n\nChildren consider motivation, as well as traits, when deciding who to seek information from and who to believe. In both Eastern and Western cultures, both adults and children adhere to the utilitarian perspective when giving moral evaluations of truths and lies in different social situations. In terms of people's characteristics, children tend to place trust in people who are honest, smart, or kind over people who are dishonest, not smart, or not kind. However, they also consider a person's intent or motivation. From age 7, children consider both honesty and benevolence when making trust judgments about other people, and older children are more likely to trust people who tell prosocial lies (to avoid hurting another person's feelings or to help another person) than young children. For younger children, honesty is more important than a person's intention. As children get older, they increasingly attend to motivation as a key factor. The relationship between telling the truth and trusting a person is stable, but when it comes to lying, children consider the motivation of the speaker when deciding whether or not to trust them.\n\nChildren evaluate the credibility of a speaker by evaluating how reliable that person has been in the past and can make predictions about a speaker based on their age. Children as young as 3 years-old prefer to trust an unfamiliar adult rather than an unfamiliar child. When considering both age and reliability, age is often the primary cue used to determine another's credibility. For example, 3 and 4 year-olds found adults to be more trustworthy than peers, unless the peer demonstrated greater reliability (i.e., adult incorrectly mislabelled objects, whereas peer correctly labelled them). Children also consider both the prior history of accuracy and the level of perceptual access the speaker has when they provide information. Young children spontaneously keep track of the prior history of a person's accuracy or inaccuracy (reliability) and prefer to learn from someone with a good track record. Children commonly interpret the speaker's history of inaccuracy as a lasting trait and so the speaker is considered an unreliable informant, at least within the domain they have been wrong about. However, under certain conditions, children may excuse a person's past inaccuracy and later trust that person for information. If a speaker has limited information (e.g., lack of perceptual access) in making a claim – for example, inaccurately identifying a toy while blindfolded – then children as young as 3 years-old appropriately excuse their past inaccuracy especially when they are later well-informed. On the other hand, if a speaker has full access to information while making an inaccurate claim, children continue to regard him/her as unreliable.\n\nYoung children appreciate that people generally think and act sensibly when reasons are present, and they can evaluate a claim by assessing the quality of the reasoning provided. Thus, children create an epistemic profile of a person based on the quality of the reasons they offer when making a claim. As young as 3 years-old, children understand the difference between weak versus strong reasoning to support a statement. Children are more likely to trust someone when strong support is provided through: reliable testimony (\"My teacher told me there's a book in the bag. I think that there's a book\"), looking (\"Before I came here, I looked and saw a ball in the bag. I think there's a ball in there\"), and inference (\"It's a backpack. Backpack holds books. I think there's a book in there\"). On the other hand, desire (\"I like crayons. I want there to be crayons in the bag. I think that there are crayons in there\"), pretense (\"I like to pretend. I'm going to pretend that there's a sandwich in the bag. I think there's a sandwich in there\"), and guessing (\"I don't know. I'm going to guess that there's a toy in the bag\") are not viewed as strong support for a claim. Children recognize that the mental states that they and others may hold are not always reliable means for drawing specific conclusions. 3 and 4 year-olds can also choose the more reasonable of two people and continue to seek, as well as, accept new information from the more credible person (the one who had better reasons in the past).\n\nPeople are not always reliable in their provision of information and it is important to be able to differentiate between good sources and bad sources. Assessing someone's reliability is based not only on the knowledgeability of the speaker, but their motives/intentions as well. People may not always be motivated to tell the truth; instead, they may potentially lie to promote their own interest, or the interest of others. At about the age they begin preschool, children become better at distinguishing between helpful and deceptive people. 3 year-olds are not able to identify who is trying to help or trick another person and accept advice from both helpers and trickers. On the other hand, 4 year-olds are more sceptical and could differentiate between helpers and trickers, but have no preference in choosing whom to accept advice from. There may be a mismatch between knowledge and behaviour among 4 year-olds, in which they do not understand the implications of their knowledge or how to successfully apply it to their behaviour. 5 year-olds systematically preferred advice from helpers. Ultimately theory of mind, or children's understanding of mental states, is related to selective trust in helpers (versus trickers). Beginning at 5 years-old, children use a person's prior history of deception to make reliability judgments about that person.\n\nMetacognition is an ability that allows people to think about their own thinking process, and how to use their thinking process to help themselves learn better. Metacognition includes two separate abilities: (1) knowledge of cognition and (2) regulation of cognition. Knowledge of one's thinking process is not enough to regulate an individual's behaviour, and are required to use specific strategies to help them regulate their behaviour.\n\nAn important skill children need to learn is being able to identify when a message is incomplete and they don't have enough information to make an accurate interpretation. Being aware that an ambiguous situation has arisen is difficult for young children. Children accurately \"know when they know\", but often overestimate when they don't know. Children's behaviour does not seem to match their verbal ability to acknowledge their \"lack of knowledge\". Despite incorrectly stating that they \"know\" something, children are still capable of changing their response upon hearing contradicting information to an initial interpretation and/or event. Language plays an important role in children's accuracy in assessing their own knowledge. For children to accurately \"know what they know\" it is important for them to understand the various meanings of the word \"know\" as well as language used to describe certainty and uncertainty.\n\nAmbiguous information is \"a piece of information (word, message, or view) with multiple interpretations\". Adults not only have the awareness to realize when ambiguity exists, but they also have strategies to deal with ambiguous input. Young children have difficulty with recognizing ambiguity and understanding how to handle it. Typically, it is not until the age of 6 or 7 that children have the ability to successfully deal with ambiguous input. However, it seems in certain contexts with certain tasks, younger children also display some ability to deal with ambiguous information.\n\nLanguage can play an important role in the ability to correctly decipher ambiguous input. One can \"know\" someone in many different ways, for example, seeing them, talking to them, having a prior history with them, etc. This makes questions like \"Do you know?\" very complicated and difficult to respond to. More explicit definitions of \"to know\" seem to assist children in better assessing their own knowledge. Children as young as 4 years were able to make far more accurate statements about their actual knowledge when a question was phrased \"Have you \"heard of\"\" rather than \"Do you \"know\"\". By the age of 6, children are typically able to accurately assess their knowledge with very little impact on their future behaviours regardless of the language used. 4-5 year-old's, on the other hand, were so susceptible to change that the phrase used altered their response to future questions. 4-5 year-old's were also less likely to overestimate their knowledge of a target person if the initial question was phrased \"Have you \"heard of\"\" rather than \"Do you \"know\"\". Not only can responses from children be altered by the phrasing of the question, but the suggestion of lack of prior experience with the target is enough for children to change their response. In a study where children were asked if they really knew who a specific person was (between 2 pictures of people they had never seen before), they were more accurate in assessing that they \"didn't know\" when it was suggested that the target person had never been to the child's city before. While experience is important, children tend not to over-rely on prior experiences and only use it when they've had significant experience that would actually assist them in assessing their knowledge.\n\nSometimes when confronted with ambiguous information, more than one piece of information is required to make and accurate interpretation. For example, in a study where children had to pick one of four pictures presented to them after hearing ambiguous information, they were capable of making tentative interpretations and then correctly changing their interpretations upon hearing contradicting, clarifying information. This strategy uses multiple pieces of information and has been seen in children as young as 15 months old. In situations where only partial information is available, young children make the best interpretation possible with the information given and go on to change this interpretation only when contradicting information emerges.\n\nThe presentation of clarifying information is not always immediately accessible. In these situations, adults seem to delay interpretation and seek clarifying information when appropriate. Even more difficult than acknowledging that an ambiguous situation has arisen, is children's understanding in what actions they need to take for clarification. There are 2 different types of delay in interpreting ambiguous information: one that is intentional and one that is instinctual. The explicit decision to delay interpretation and seek further information is a difficult one as it involves being aware that the current information is not sufficient, and knowing how to acquire appropriate information necessary for clarification. Children are typically not successful in this process until the age of 7. However, when this explicit decision to delay is simplified, children aged 5 and up showed some ability in successfully choosing to delay their response. While the ability for intentionally delaying interpretation seems to be difficult, the ability to delay interpretation instinctively seems to be easier for young children. In one example, children were asked to stamp the correct snowman once they knew which one it was. Their knowledge of the correct snowman was based on a researcher slowly revealing the target snowman. The children's cards contained snowmen that differed by some feature visible on the 2nd half of the snowman only and therefore the children were required to wait until the 2nd half of the snowman was revealed in order to accurately assess which snowman was the correct one. In this scenario focusing on instinct, children as young as 5 were able to accurately delay interpretation. Therefore, while it is difficult for children to explicitly demonstrate their awareness of an ambiguous situation and how to resolve it, they are implicitly able to handle situations in which delaying interpretation may be beneficial.\n\nIt is difficult for younger children to grasp the idea that objects can be referred to in different ways and that people can have partial knowledge of the different references (i.e. a \"bouncy ball\" might also be referred to as a \"rubber sphere\"). A child might know by looking that a toy is a toy truck, but they may not be aware that the toy truck is also a present. Referential opacity is the concept of whether or not referring to an object changes its meaning. If something is referentially \"transparent\" (substitution insensitive), altering the referent term does not alter the meaning, and something that is referentially \"opaque\" (substitution sensitive) means that altering the referent term would alter the meaning. An example of this is a study with a puppet named Heinz. There is a ball in a box and children are told that Heinz knows that there is a ball in the box but does not know that the ball is a present. Children are then asked substitution-insensitive questions (i.e. Does Heinz know the ball is a present? – asking, \"Does Heinz know the rubber sphere is a present?\" \"does not\" alter the meaning of the question) and substitution sensitive questions (i.e. Does Heinz know there's a present in the box? – asking \"Does Heinz know there is a rubber sphere in the box\" \"does\" alter the meaning of the question). Regardless of age, substitution-\"insensitive\" questions seem to be easier than substitution-\"sensitive\" questions. The ability to correctly answer substitution-sensitive questions improves with age. The ability to answer these types of questions is closely related to effectively evaluating ambiguous messages. Success on substitution-insensitive questions is necessary but not sufficient for success on evaluation ambiguous messages. Alternatively, success on substitution-sensitive questions is necessary and sufficient for success at evaluating ambiguous messages.\n\nChildren can change their interpretation of information based on the context in which the interpretation is made. Robinson and colleagues (2006) studied children's interpretation of information in two different: physical and epistemic uncertainty. Physical uncertainty occurs when an event has not yet happened, and therefore the outcome of that event has not been determined (i.e. the dice has not yet been rolled). Epistemic uncertainty occurs when an event has already occurred, but the child is not aware of the outcome of the event (i.e. the dice have been rolled, but the dice are hidden from the participant). 4 to 8 year old children have the ability to realize multiple possibilities for an event that has not yet occurred (\"physical uncertainty\"), however they do not seem to acknowledge that there are exactly the same possibilities for an event that has already happened when they don't know the outcome (\"epistemic uncertainty\"). Under the conditions of \"epistemic uncertainty\", children simply guess one of the possibilities. Beck and colleagues (2011) propose that this happens because it is much easier to imagine the outcome during epistemic uncertainty, basically knowing that there is only one outcome. Similarly, adults also prefer to make predictions or guess in epistemic uncertainty.\n\nFeeling-of-knowing occurs when people are unable to easily recall a memory or a fact, but they know that they learned it and are able to recognize it, such as in a multiple-choice test. Adults' accuracy of feeling-of-knowing judgments is well above chance but not nearly perfect. It seems to be the same for children as well. Lockl and Schneider (2002) did not find any developmental trends in the accuracy of feeling-of-knowing judgments. Instead, similar to adults, children's accuracy of feeling-of knowing judgments was low, but still above chance for all age groups studied (grade 1 through grade 4). Getting a child to attend to this feeling-of-knowing (through language or prompting of prior experience) is one way to assist them in more accurately assessing their actual knowledge, allowing them to handle ambiguous situations at a much younger age.\n\nChildren are highly susceptible to a \"suggestibility effect\", producing \"false memories\" and/or \"incorrect, post-event information\" (see misinformation effect) when asked to engage in memory recall. This has important implications for forensic interviewing and child witness testimony. Consider the prominent case of Kelly Michael's (see Wee Care Nursery School abuse trial), where improper interviewing techniques lead to a miscarriage of justice. In order to develop reliable and age-appropriate instruments for interviewing children, it is imperative to consider their cognitive development, verbal and mental abilities.\n\nIt is critical for a forensic interviewer to recognize their power during an interview, especially with a child witness. An interviewer can impact the course of a child's testimony in numerous ways, including:\n\nThe age of a child is also an important factor during interviewing. Younger children are more likely to provide shorter, less detailed accounts of an event in comparison to older children. Preschool children are more likely to disclose information in an \"accidental way\" through triggers and obvious cues, whereas older school children are more likely to make \"intentional disclosures\" based on the nature of the question they are asked.\n\nThere are varying techniques and extraneous factors that can influence the way a child discloses an event during child witness testimony (e.g., experiences of abuse by a parent or caregiver). There are two major types of barriers in forensic interviews: (1) improper interviewing and (2) clumsy interviewing\n\n\"Improper interviewing\" includes forensic techniques that are considered to be \"risky and ineffective\". Each of the following techniques can create critically negative consequences in witness testimony and result in false allegations or the potential for a reduced conviction:\n\n\"Clumsy interviewing\" involves interviews that fail to include best practices and recommended techniques. Interviewers who are not properly trained in forensic techniques can fail to follow structured interview guidelines and impact the outcome of a child's testimony and/or responses to questioning. This type of interviewing most often occurs when an interviewer lacks skill, forgets important procedures, and when there is a lack of necessary supervision.\n\nIn order to prevent improper forensic interviewing, numerous methods to reduce suggestibility and the misinformation effect have been proven effective, including: taping interviews, recording transcripts, ensuring supervision by a qualified professional, experience in working with children, training in forensic interviewing, and maintaining a comfortable, safe environment.\n\nThe cognitive interview utilizes cognitive theory as a focal point in its interview methodology. The cognitive interview, first developed in 1992 by researchers Fisher and Geiselman, was originally developed for adults and later modified for children. It utilizes two major perspectives from cognitive theory, including the \"encoding specificity principle\" and a \"multi-component view of memory traces\".\n\nSpecifically, this method utilizes four major techniques:\n\nThe National Institute of Child Health and Human Development (NICHD) Protocol\ndeveloped an investigative protocol in 2000 to create a structured interview technique for children, specifically those of child sexual abuse.\n\nIt uses the following techniques:\n\nStepwise interview\nutilizes open-ended questions through a \"funnel-like strategy\". It is primarily used by legal professionals, and is most often used in North America. This interview begins with open-ended questions and/or free recall and slowly incorporates more focused and detailed questions.\n\nAllegation blind interviews\nstress that an interviewer should refrain from gathering information prior to an interview in order to reduce suggestibility and increase interviewer patience and attentiveness. This also enhances the interviewer's ability to be non-judgmental and objective.\n\nTruth-lie discussions\nare most useful prior to commencing abuse-related questioning. This method allows the interviewer to create a baseline with the child about what the \"truth\" is and what a \"lie\" is. The interviewer is encouraged to ask questions with general examples, such as \"tell me a lie about this chair\". This strategy has been proven to result in a less misinformed child testimony.\n\nTouch survey\nwas developed on the basis that \"touch falls on a continuum\", and is beneficial to screen for child abuse. It includes questions surrounding the child's experiences with touch (e.g., kissing, hugging, hitting), including where they have been touched and by whom. This tool might be more useful when used in conjunction with other forensic strategies.\n\nAlthough there are varying suggestions for structured forensic interviewing, experts provide context into best practices that can significantly reduce suggestibility, false memories and the misinformation effect:\n\n"}
{"id": "14397644", "url": "https://en.wikipedia.org/wiki?curid=14397644", "title": "Civil Rights Commission (Puerto Rico)", "text": "Civil Rights Commission (Puerto Rico)\n\nThe Civil Rights Commission () is an official entity within the legislative branch of the government of Puerto Rico charged with investigating violations of citizens' civil rights. The commission is empowered to educate citizens about their civil rights, investigate alleged civil rights violations, and carry out studies and investigations. Amendments to its organic law also empowers it to appear as a \"friend of the court\" in cases that the commission deems to have an important effect on civil rights in Puerto Rico.\n\nThe commission was created in 1965 under governor Roberto Sánchez Vilella as an independent agency under the Department of Justice. But it was not until 1996 that the commission was made part of the legislative branch, due, in part, to the efforts of governor Pedro Rosselló.\n\nStructurally, the entity is composed of five members that are appointed to a six-years term by the governor of Puerto Rico, subject to the advice and consent of the Senate. Its current members include its chair, Georgina Candal Segurola, Esther Vicente Rivera, Rosemary Borges Capó, Ruth Miriam Perez Maldonado and Hiram A. Meléndez Juarbe. Other individuals that have served in the commission include former Sen. Luis Muñoz Rivera, newspaper publisher Antonio Luis Ferré, former Resident Commissioner Baltasar Corrada del Río, and the late Efraín González Tejera.\n\nIn spite of complaints that the commission is chronically underfunded, the entity has had a significant influence in Puerto Rican government operations.\n\nThe commission also annually selects the recipients of the Thurgood Marshall Award, which recognizes the commitment to civil rights of one law student from each of Puerto Rico's three law schools.\n\n"}
{"id": "19555586", "url": "https://en.wikipedia.org/wiki?curid=19555586", "title": "Classical mechanics", "text": "Classical mechanics\n\nClassical mechanics describes the motion of macroscopic objects, from projectiles to parts of machinery, and astronomical objects, such as spacecraft, planets, stars and galaxies. \n\nIf the present state of an object is known it is possible to predict by the laws of classical mechanics how it will move in the future (determinism) and how it has moved in the past (reversibility).\n\nThe earliest development of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts employed by and the mathematical methods invented by Isaac Newton and Gottfried Wilhelm Leibniz and others in the 17th century to describe the motion of bodies under the influence of a system of forces.\n\nLater, more abstract methods were developed, leading to the reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances, made predominantly in the 18th and 19th centuries, extend substantially beyond Newton's work, particularly through their use of analytical mechanics. They are, with some modification, also used in all areas of modern physics.\n\nClassical mechanics provides extremely accurate results when studying large objects that are not extremely massive and speeds not approaching the speed of light. When the objects being examined have about the size of an atom diameter, it becomes necessary to introduce the other major sub-field of mechanics: quantum mechanics. To describe velocities that are not small compared to the speed of light, special relativity is needed. In case that objects become extremely massive, general relativity becomes applicable. However, a number of modern sources do include relativistic mechanics into classical physics, which in their view represents classical mechanics in its most developed and accurate form. \n\nThe following introduces the basic concepts of classical mechanics. For simplicity, it often models real-world objects as point particles (objects with negligible size). The motion of a point particle is characterized by a small number of parameters: its position, mass, and the forces applied to it. Each of these parameters is discussed in turn.\n\nIn reality, the kind of objects that classical mechanics can describe always have a non-zero size. (The physics of \"very\" small particles, such as the electron, is more accurately described by quantum mechanics.) Objects with non-zero size have more complicated behavior than hypothetical point particles, because of the additional degrees of freedom, e.g., a baseball can spin while it is moving. However, the results for point particles can be used to study such objects by treating them as composite objects, made of a large number of collectively acting point particles. The center of mass of a composite object behaves like a point particle.\n\nClassical mechanics uses common-sense notions of how matter and forces exist and interact. It assumes that matter and energy have definite, knowable attributes such as location in space and speed. Non-relativistic mechanics also assumes that forces act instantaneously (see also Action at a distance).\nThe \"position\" of a point particle is defined in relation to a coordinate system centered on an arbitrary fixed reference point in space called the origin \"O\". A simple coordinate system might describe the position of a particle \"P\" with a vector notated by an arrow labeled r that points from the origin \"O\" to point \"P\". In general, the point particle does not need to be stationary relative to \"O\". In cases where \"P\" is moving relative to \"O\", r is defined as a function of \"t\", time. In pre-Einstein relativity (known as Galilean relativity), time is considered an absolute, i.e., the time interval that is observed to elapse between any given pair of events is the same for all observers. In addition to relying on absolute time, classical mechanics assumes Euclidean geometry for the structure of space.\n\nThe \"velocity\", or the rate of change of position with time, is defined as the derivative of the position with respect to time:\n\nIn classical mechanics, velocities are directly additive and subtractive. For example, if one car travels east at 60 km/h and passes another car traveling in the same direction at 50 km/h, the slower car perceives the faster car as traveling east at . However, from the perspective of the faster car, the slower car is moving 10 km/h to the west, often denoted as -10 km/h where the sign implies opposite direction. Velocities are directly additive as ; they must be dealt with using vector analysis.\n\nMathematically, if the velocity of the first object in the previous discussion is denoted by the vector and the velocity of the second object by the vector , where \"u\" is the speed of the first object, \"v\" is the speed of the second object, and d and e are unit vectors in the directions of motion of each object respectively, then the velocity of the first object as seen by the second object is\n\nSimilarly, the first object sees the velocity of the second object as\n\nWhen both objects are moving in the same direction, this equation can be simplified to\n\nOr, by ignoring direction, the difference can be given in terms of speed only:\n\nThe \"acceleration\", or rate of change of velocity, is the derivative of the velocity with respect to time (the second derivative of the position with respect to time):\n\nAcceleration represents the velocity's change over time. Velocity can change in either magnitude or direction, or both. Occasionally, a decrease in the magnitude of velocity \"v\" is referred to as \"deceleration\", but generally any change in the velocity over time, including deceleration, is simply referred to as acceleration.\n\nWhile the position, velocity and acceleration of a particle can be described with respect to any observer in any state of motion, classical mechanics assumes the existence of a special family of reference frames in which the mechanical laws of nature take a comparatively simple form. These special reference frames are called inertial frames. \n\nAn inertial frame is a frame of reference within which an object interacting with no forces (an idealized situation) appears either at rest or moving uniformly in a straight line. This is the fundamental definition of an inertial frame. These are characterized by the requirement that all forces entering the observer's physical laws originate from identifiable sources caused by fields, such as electro-static field (caused by static electrical charges), electro-magnetic field (caused by moving charges), gravitational field (caused by mass), and so forth.\n\nA key concept of inertial frames is the method for identifying them. For practical purposes, reference frames that do not accelerate with respect to distant stars (an extremely distant point) are regarded as good approximations to inertial frames. Non-inertial reference frames accelerate in relation to an existing inertial frame. They form the basis for Einstein's relativity. Due to the relative motion, particles in the non-inertial frame appear to move in ways not explained by forces from existing fields in the reference frame. Hence, it appears that there are other forces that enter the equations of motion solely as a result of the relative acceleration. These forces are referred to as fictitious forces, inertia forces, or pseudo-forces. \n\nConsider two reference frames \"S\" and S'. For observers in each of the reference frames an event has space-time coordinates of (\"x\",\"y\",\"z\",\"t\") in frame \"S\" and (x',y',z',t') in frame S'. Assuming time is measured the same in all reference frames, and if we require when , then the relation between the space-time coordinates of the same event observed from the reference frames S' and \"S\", which are moving at a relative velocity of \"u\" in the \"x\" direction is:\n\nThis set of formulas defines a group transformation known as the Galilean transformation (informally, the \"Galilean transform\"). This group is a limiting case of the Poincaré group used in special relativity. The limiting case applies when the velocity \"u\" is very small compared to \"c\", the speed of light.\n\nThe transformations have the following consequences:\n\nFor some problems, it is convenient to use rotating coordinates (reference frames). Thereby one can either keep a mapping to a convenient inertial frame, or introduce additionally a fictitious centrifugal force and Coriolis force.\n\nNewton was the first to mathematically express the relationship between force and momentum. Some physicists interpret Newton's second law of motion as a definition of force and mass, while others consider it a fundamental postulate, a law of nature. Either interpretation has the same mathematical consequences, historically known as \"Newton's Second Law\":\n\nThe quantity \"m\"v is called the (canonical) momentum. The net force on a particle is thus equal to the rate of change of the momentum of the particle with time. Since the definition of acceleration is , the second law can be written in the simplified and more familiar form:\n\nSo long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the \"equation of motion\".\n\nAs an example, assume that friction is the only force acting on the particle, and that it may be modeled as a function of the velocity of the particle, for example:\n\nwhere \"λ\" is a positive constant, the negative sign states that the force is opposite the sense of the velocity. Then the equation of motion is\n\nThis can be integrated to obtain\n\nwhere v is the initial velocity. This means that the velocity of this particle decays exponentially to zero as time progresses. In this case, an equivalent viewpoint is that the kinetic energy of the particle is absorbed by friction (which converts it to heat energy in accordance with the conservation of energy), and the particle is slowing down. This expression can be further integrated to obtain the position r of the particle as a function of time.\n\nImportant forces include the gravitational force and the Lorentz force for electromagnetism. In addition, Newton's third law can sometimes be used to deduce the forces acting on a particle: if it is known that particle \"A\" exerts a force F on another particle \"B\", it follows that \"B\" must exert an equal and opposite \"reaction force\", −F, on \"A\". The strong form of Newton's third law requires that F and −F act along the line connecting \"A\" and \"B\", while the weak form does not. Illustrations of the weak form of Newton's third law are often found for magnetic forces.\n\nIf a constant force F is applied to a particle that makes a displacement Δr, the \"work done\" by the force is defined as the scalar product of the force and displacement vectors:\n\nMore generally, if the force varies as a function of position as the particle moves from r to r along a path \"C\", the work done on the particle is given by the line integral\n\nIf the work done in moving the particle from r to r is the same no matter what path is taken, the force is said to be conservative. Gravity is a conservative force, as is the force due to an idealized spring, as given by Hooke's law. The force due to friction is non-conservative.\n\nThe kinetic energy \"E\" of a particle of mass \"m\" travelling at speed \"v\" is given by\n\nFor extended objects composed of many particles, the kinetic energy of the composite body is the sum of the kinetic energies of the particles.\n\nThe work–energy theorem states that for a particle of constant mass \"m\", the total work \"W\" done on the particle as it moves from position r to r is equal to the change in kinetic energy \"E\" of the particle:\n\nConservative forces can be expressed as the gradient of a scalar function, known as the potential energy and denoted \"E\":\n\nIf all the forces acting on a particle are conservative, and \"E\" is the total potential energy (which is defined as a work of involved forces to rearrange mutual positions of bodies), obtained by summing the potential energies corresponding to each force\nThe decrease in the potential energy is equal to the increase in the kinetic energy\n\nThis result is known as \"conservation of energy\" and states that the total energy,\n\nis constant in time. It is often useful, because many commonly encountered forces are conservative.\n\nClassical mechanics also describes the more complex motions of extended non-pointlike objects. Euler's laws provide extensions to Newton's laws in this area. The concepts of angular momentum rely on the same calculus used to describe one-dimensional motion. The rocket equation extends the notion of rate of change of an object's momentum to include the effects of an object \"losing mass\".\n\nThere are two important alternative formulations of classical mechanics: Lagrangian mechanics and Hamiltonian mechanics. These, and other modern formulations, usually bypass the concept of \"force\", instead referring to other physical quantities, such as energy, speed and momentum, for describing mechanical systems in generalized coordinates.\n\nThe expressions given above for momentum and kinetic energy are only valid when there is no significant electromagnetic contribution. In electromagnetism, Newton's second law for current-carrying wires breaks down unless one includes the electromagnetic field contribution to the momentum of the system as expressed by the Poynting vector divided by \"c\", where \"c\" is the speed of light in free space.\n\nMany branches of classical mechanics are simplifications or approximations of more accurate forms; two of the most accurate being general relativity and relativistic statistical mechanics. Geometric optics is an approximation to the quantum theory of light, and does not have a superior \"classical\" form.\n\nWhen both quantum mechanics and classical mechanics cannot apply, such as at the quantum level with many degrees of freedom, quantum field theory (QFT) is of use. QFT deals with small distances and large speeds with many degrees of freedom as well as the possibility of any change in the number of particles throughout the interaction. When treating large degrees of freedom at the macroscopic level, statistical mechanics becomes useful. Statistical mechanics describes the behavior of large (but countable) numbers of particles and their interactions as a whole at the macroscopic level. Statistical mechanics is mainly used in thermodynamics for systems that lie outside the bounds of the assumptions of classical thermodynamics. In the case of high velocity objects approaching the speed of light, classical mechanics is enhanced by special relativity. In case that objects become extremely heavy (i.e. their Schwarzschild radius is not negligibly small for a given application), deviations from Newtonian mechanics become apparent and can be quantified by using the Parameterized post-Newtonian formalism. In that case, General relativity (GR) becomes applicable. However, until now there is no theory of Quantum gravity unifying GR and QFT in the sense that it could be used when objects become extremely small and heavy.[4][5]\n\nIn special relativity, the momentum of a particle is given by\nwhere \"m\" is the particle's rest mass, v its velocity, \"v\" is the modulus of v, and \"c\" is the speed of light.\n\nIf \"v\" is very small compared to \"c\", \"v\"/\"c\" is approximately zero, and so\nThus the Newtonian equation is an approximation of the relativistic equation for bodies moving with low speeds compared to the speed of light.\n\nFor example, the relativistic cyclotron frequency of a cyclotron, gyrotron, or high voltage magnetron is given by\nwhere \"f\" is the classical frequency of an electron (or other charged particle) with kinetic energy \"T\" and (rest) mass \"m\" circling in a magnetic field. The (rest) mass of an electron is 511 keV. So the frequency correction is 1% for a magnetic vacuum tube with a 5.11 kV direct current accelerating voltage.\n\nThe ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is\n\nwhere \"h\" is Planck's constant and \"p\" is the momentum.\n\nAgain, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.\n\nMore practical examples of the failure of classical mechanics on an engineering scale are conduction by quantum tunneling in tunnel diodes and very narrow transistor gates in integrated circuits.\n\nClassical mechanics is the same extreme high frequency approximation as geometric optics. It is more often accurate because it describes particles and bodies with rest mass. These have more momentum and therefore shorter De Broglie wavelengths than massless particles, such as light, with the same kinetic energies.\n\nThe study of the motion of bodies is an ancient one, making classical mechanics one of the oldest and largest subjects in science, engineering and technology,\n\nSome Greek philosophers of antiquity, among them Aristotle, founder of Aristotelian physics, may have been the first to maintain the idea that \"everything happens for a reason\" and that theoretical principles can assist in the understanding of nature. While to a modern reader, many of these preserved ideas come forth as eminently reasonable, there is a conspicuous lack of both mathematical theory and controlled experiment, as we know it. These later became decisive factors in forming modern science, and their early application came to be known as classical mechanics.\n\nIn his \"Elementa super demonstrationem ponderum\", medieval mathematician Jordanus de Nemore introduced the concept of \"positional gravity\" and the use of component forces.\nThe first published causal explanation of the motions of planets was Johannes Kepler's \"Astronomia nova,\" published in 1609. He concluded, based on Tycho Brahe's observations on the orbit of Mars, that the planet's orbits were ellipses. This break with ancient thought was happening around the same time that Galileo was proposing abstract mathematical laws for the motion of objects. He may (or may not) have performed the famous experiment of dropping two cannonballs of different weights from the tower of Pisa, showing that they both hit the ground at the same time. The reality of that particular experiment is disputed, but he did carry out quantitative experiments by rolling balls on an inclined plane. His theory of accelerated motion was derived from the results of such experiments and forms a cornerstone of classical mechanics.\n\nNewton founded his principles of natural philosophy on three proposed laws of motion: the law of inertia, his second law of acceleration (mentioned above), and the law of action and reaction; and hence laid the foundations for classical mechanics. Both Newton's second and third laws were given the proper scientific and mathematical treatment in Newton's \"Philosophiæ Naturalis Principia Mathematica.\" Here they are distinguished from earlier attempts at explaining similar phenomena, which were either incomplete, incorrect, or given little accurate mathematical expression. Newton also enunciated the principles of conservation of momentum and angular momentum. In mechanics, Newton was also the first to provide the first correct scientific and mathematical formulation of gravity in Newton's law of universal gravitation. The combination of Newton's laws of motion and gravitation provide the fullest and most accurate description of classical mechanics. He demonstrated that these laws apply to everyday objects as well as to celestial objects. In particular, he obtained a theoretical explanation of Kepler's laws of motion of the planets.\n\nNewton had previously invented the calculus, of mathematics, and used it to perform the mathematical calculations. For acceptability, his book, the \"Principia\", was formulated entirely in terms of the long-established geometric methods, which were soon eclipsed by his calculus. However, it was Leibniz who developed the notation of the derivative and integral preferred today.\nNewton, and most of his contemporaries, with the notable exception of Huygens, worked on the assumption that classical mechanics would be able to explain all phenomena, including light, in the form of geometric optics. Even when discovering the so-called Newton's rings (a wave interference phenomenon) he maintained his own corpuscular theory of light.\n\nAfter Newton, classical mechanics became a principal field of study in mathematics as well as physics. Several re-formulations progressively allowed finding solutions to a far greater number of problems. The first notable re-formulation was in 1788 by Joseph Louis Lagrange. Lagrangian mechanics was in turn re-formulated in 1833 by William Rowan Hamilton.\n\nSome difficulties were discovered in the late 19th century that could only be resolved by more modern physics. Some of these difficulties related to compatibility with electromagnetic theory, and the famous Michelson–Morley experiment. The resolution of these problems led to the special theory of relativity, often still considered a part of classical mechanics.\n\nA second set of difficulties were related to thermodynamics. When combined with thermodynamics, classical mechanics leads to the Gibbs paradox of classical statistical mechanics, in which entropy is not a well-defined quantity. Black-body radiation was not explained without the introduction of quanta. As experiments reached the atomic level, classical mechanics failed to explain, even approximately, such basic things as the energy levels and sizes of atoms and the photo-electric effect. The effort at resolving these problems led to the development of quantum mechanics.\n\nSince the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields. Also, it has been extended into the complex domain where complex classical mechanics exhibits behaviors very similar to quantum mechanics.\n\nClassical mechanics was traditionally divided into three main branches:\n\nAnother division is based on the choice of mathematical formalism:\n\nAlternatively, a division can be made by region of application:\n\n\n"}
{"id": "18734771", "url": "https://en.wikipedia.org/wiki?curid=18734771", "title": "Classification of Instructional Programs", "text": "Classification of Instructional Programs\n\nThe Classification of Instructional Programs (CIP) is a taxonomy of academic disciplines at institutions of higher education in the United States and Canada.\n\nThe CIP was originally developed by the National Center for Education Statistics (NCES) of the United States Department of Education in 1980 and was revised in 1985, 1990, 2000 and 2010. The 2010 edition (CIP 2010) is the fourth and current revision of the taxonomy. Instructional programs are classified by a six-digit CIP at the most granular level and are classified according to the two-digit and four-digit prefixes of the code. For example, \"Forensic Science and Technology\" has the six-digit code 43.0106, which places it in \"Criminal Justice and Corrections\" (43.01) and \"Homeland Security, Law Enforcement, Firefighting and Related Protective Services\" (two-digit CIP 43).\n\n"}
{"id": "2814867", "url": "https://en.wikipedia.org/wiki?curid=2814867", "title": "Complexity measure", "text": "Complexity measure\n\nComplexity measure / measure of complexity may refer to any measure defined in various branches of complexity theory, specifically:\n"}
{"id": "11196230", "url": "https://en.wikipedia.org/wiki?curid=11196230", "title": "Coon song", "text": "Coon song\n\nCoon songs were a genre of music that presented a stereotyped image of black people. They were popular in the United States and the United Kingdom from around 1880 to 1920, though the earliest such songs date from minstrel shows as far back as 1848.\n\nThe first explicitly coon-themed song, published in 1880, may have been \"The Dandy Coon's Parade\" by J.P. Skelley. Other notable early coon songs included \"The Coons Are on Parade\", \"New Coon in Town\" (by J.S. Putnam, 1883), \"Coon Salvation Army\" (by Sam Lucas, 1884), \"Coon Schottische\" (by William Dressler, 1884).\n\nBy the mid-1880s, coon songs were a national craze; over 600 such songs were published in the 1890s. The most successful songs sold millions of copies. To take advantage of the fad, composers \"add[ed] words typical of coon songs to previously published songs and rags\". \n\nAfter the turn of the century, coon songs began to receive criticism for their racist content. In 1905, Bob Cole, an African-American composer who had gained fame largely by writing coon songs, made somewhat unprecedented remarks about the genre. When asked in an interview about the name of his earlier comedy \"A Trip to Coontown\", he replied, \"That day has passed with the softly flowing tide of revelations.\" Following further criticism the use of \"coon\" in song titles greatly decreased after 1910.\n\nOn August 13, 1920, the Universal Negro Improvement Association and African Communities League created the Red, Black and Green flag as a response to the song \"Every Race Has a Flag but the Coon\" by Heelan and Helf. That song along with \"Coon, Coon, Coon\" and \"All Coons Look Alike to Me\" were identified by H.L. Mencken as being the songs which firmly established the derogatory term \"coon\" in the American vocabulary. Originally in the 1830s, the term had been associated with the Whig Party. The Whigs used a raccoon as its emblem, but also had a more tolerant attitude towards blacks than the other political factions. The latter opinion is likely what transformed the term \"coon\" from mere political slang into a racial slur.\n\nIt is possible that the popularity of coon songs may be explained in part by their historical timing: coon songs arose precisely as the popular music business exploded in Tin Pan Alley. However, James Dormon, a former professor of history and American studies at the University of Southwestern Louisiana, has also suggested that coon songs can be seen as \"a necessary sociopsychological mechanism for justifying segregation and subordination.\" The songs portrayed Blacks as posing a threat to the American social order, and implied that they had to be controlled.\n\nAt the height of their popularity, \"just about every songwriter in the country\" was writing coon songs \"to fill the seemingly insatiable demand\". Writers of coon songs included some of the most important Tin Pan Alley composers, including Gus Edwards, Fred Fisher (who wrote the 1905 \"If the Man in the Moon Were a Coon\", which sold three million copies), and Irving Berlin. Even one of John Philip Sousa's assistants, Arthur Pryor, composed coon songs. (This was meant to ensure a steady supply to Sousa's band, which performed the songs and popularized several coon song melodies.)\n\nMany coon songs were written by whites, but some were written by African-Americans. Important black composers of coon songs include Ernest Hogan (who wrote \"All Coons Look Alike to Me\", the most famous coon song); Sam Lucas (who wrote the most racist early coon songs by modern standards); minstrel and songwriter Sidney L. Perrin (who wrote \"Black Annie,\" \"Dat's De Way to Spell Chicken,\" \"Mamma's Little Pumpkin Colored Coons,\" \"Gib Me Ma 15 cents,\" and \"My Dinah\"); Bob Cole (who wrote dozens of songs, including \"I Wonder What The Coon's Game Is?\" and \"No Coons Allowed\"); and Bert Williams and George Walker. Even classic ragtime composer Scott Joplin wrote at least one coon song (\"I Am Thinking of my Pickaninny Days\"), and may have composed the music for several more, using lyrics written by others.\n\nCoon songs almost always aimed to be funny and incorporated the syncopated rhythms of ragtime music. A coon song's defining characteristic, however, was its caricature of African Americans. In keeping with the older minstrel image of blacks, coon songs often featured \"watermelon- and chicken-loving rural buffoon[s]\". However, \"blacks began to appear as not only ignorant and indolent, but also devoid of honesty or personal honor, given to drunkenness and gambling, utterly without ambition, sensuous, libidinous, even lascivious.\" Blacks were portrayed as making money through gambling, theft, and hustling, rather than working to earn a living, as in the Nathan Bivins song \"Gimme Ma Money\":\n\n<poem>Last night I did go to a big Crap game,\nHow dem coons did gamble wuz a sin and a shame...\nI'm gambling for my Sadie,\nCause she's my lady,\nI'm a hustling coon, ... dat's just what I am.</poem>\n\nCoon songs portrayed blacks as \"hot\", in this context meaning promiscuous and libidinous. They suggested that the most common living arrangement was a \"honey\" relationship (unmarried cohabitation), rather than marriage.\n\nBlacks were portrayed as inclined toward acts of provocative violence. Razors were often featured in the songs and came to symbolize blacks' wanton tendencies. However, violence in the songs was uniformly directed at blacks instead of whites (perhaps to discharge the threatening notion of black violence amongst the coon songs' predominantly white consumers). Hence, the spectre of black-on-white violence remained but an allusion. The street-patrolling \"bully coon\" was often used as a stock character in coon songs.\n\nThe songs showed the social threats that whites believed were posed by blacks. Passing was a common theme, and blacks were portrayed as seeking the status of whites, through education and money. However, blacks rarely, except during dream sequences, actually succeeded at appearing white; they only aspired to do so.\n\nCoon songs were popular in vaudeville theater, where they were delivered by \"coon shouters\", who were typically White females. Notable coon shouters included Artie Hall, Sophie Tucker, May Irwin, Mae West, Fanny Brice, Fay Templeton, Lotta Crabtree, Marie Dressler, Emma Carus, Nora Bayes, Clarice Vance, Elsie Janis, Trixie Friganza, Eva Tanguay and Julia Gerity. \n\nAs with minstrel shows earlier, a whole genre of skits and shows grew up around coon songs, and often coon songs were featured in legitimate theater productions.\n\nCoon songs contributed to the development and acceptance of authentic African-American music. Elements from coon songs were incorporated into turn-of-the-century African American folk songs, as was revealed by Howard W. Odum's 1906-1908 ethnomusicology fieldwork. Similarly, coon songs lyrics influenced the vocabulary of the blues, culminating with Bessie Smith's singing in the 1920s.\n\nBlack songwriters and performers who participated in the creation of coon songs profited commercially, enabling them to go on to develop a new type of African American musical theater based at least in part on African-American traditions. Coon songs also contributed to the mainstream acceptance of ragtime music, paving the way for the acceptance of other African-American music. Ernest Hogan, when discussing his \"All Coons Look Alike to Me\" shortly before his death, commented:\n\nCoon songs became tremendously popular in Britain after the 1880s. The role of racial stereotyping in Britain was different from what it was in America, in that the theatre audiences listening to the song often did not see Black people on a day-to-day basis in their lives.\n\nIn Britain, the most important stars to popularize coon songs were Eugene Stratton, G. H. Elliott and G. H. Chirgwin. \"Coon imitators\" singing these songs were a staple part of British music-hall until at least 1920. Initially reserved to male artists; however, a small number of women and children increasingly began to sing coon songs at the beginning of the twentieth century.\n\n\n\n"}
{"id": "3559731", "url": "https://en.wikipedia.org/wiki?curid=3559731", "title": "DUAL (cognitive architecture)", "text": "DUAL (cognitive architecture)\n\nDUAL is a general cognitive architecture integrating the connectionist and symbolic approaches at the micro level. DUAL is based on decentralized representation and emergent computation. It was inspired by the Society of Mind idea proposed by Marvin Minsky but departs from the initial proposal in many ways. Computations in DUAL emerge from the interaction of many micro-agents each of which is hybrid symbolic/connectionist device. The agents exchange messages and activation via links that can be learnt and modified, they form coalitions which collectively represent concepts, episodes, and facts. \n\nSeveral models have been developed on the basis of DUAL. These include: AMBR (a model of analogy-making and memory), JUDGEMAP (a model of judgment), PEAN (a model of perception), etc.\n\nDUAL is developed by a team at the New Bulgarian University led by Boicho Kokinov. The second version was co-authored by Alexander Petrov. The third version is co-authored by Georgi Petkov and Ivan Vankov.\n\n"}
{"id": "177694", "url": "https://en.wikipedia.org/wiki?curid=177694", "title": "Ecological economics", "text": "Ecological economics\n\nEcological economics (also called eco-economics, ecolonomy or bioeconomics of Georgescu-Roegen) is both a transdisciplinary and an interdisciplinary field of academic research addressing the interdependence and coevolution of human economies and natural ecosystems, both intertemporally and spatially. By treating the economy as a subsystem of Earth's larger ecosystem, and by emphasizing the preservation of natural capital, the field of ecological economics is differentiated from environmental economics, which is the mainstream economic analysis of the environment. One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing strong sustainability and rejecting the proposition that natural capital can be substituted by human-made capital (see the section on Weak versus strong sustainability below).\n\nEcological economics was founded in the 1980s as a modern discipline on the works of and interactions between various European and American academics (see the section on History and development below). The related field of green economics is, in general, a more politically applied form of the subject.\n\nAccording to ecological economist , ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation. Ecological economists have questioned fundamental mainstream economic approaches such as cost-benefit analysis, and the separability of economic values from scientific research, contending that economics is unavoidably normative rather than positive (i.e. descriptive). Positional analysis, which attempts to incorporate time and justice issues, is proposed as an alternative. Ecological economics shares several of its perspectives with feminist economics, including the focus on sustainability, nature, justice and care values.\n\nThe antecedents of ecological economics can be traced back to the Romantics of the 19th century as well as some Enlightenment political economists of that era. Concerns over population were expressed by Thomas Malthus, while John Stuart Mill predicted the desirability of the \"stationary state\" of an economy. Mill thereby anticipated later insights of modern ecological economists, but without having had their experience of the social and ecological costs of the Post–World War II economic expansion. In 1880, Marxian economist Sergei Podolinsky attempted to theorize a labor theory of value based on embodied energy; his work was read and critiqued by Marx and Engels. \n\nThe debate on energy in economic systems can also be traced back to Nobel prize-winning radiochemist Frederick Soddy (1877–1956). In his book \"Wealth, Virtual Wealth and Debt\" (1926), Soddy criticized the prevailing belief of the economy as a perpetual motion machine, capable of generating infinite wealth—a criticism expanded upon by later ecological economists such as Nicholas Georgescu-Roegen and Herman Daly.\n\nEuropean predecessors of ecological economics include K. William Kapp (1950) Karl Polanyi (1944), and Romanian economist Nicholas Georgescu-Roegen (1971). Georgescu-Roegen, who would later mentor Herman Daly at Vanderbilt University, provided ecological economics with a modern conceptual framework based on the material and energy flows of economic production and consumption. His \"magnum opus\", \"The Entropy Law and the Economic Process\" (1971), is credited by Daly as a fundamental text of the field, alongside Soddy's \"Wealth, Virtual Wealth and Debt\". Some key concepts of what is now ecological economics are evident in the writings of Kenneth Boulding and E.F. Schumacher, whose book \"Small Is Beautiful – A Study of Economics as if People Mattered\" (1973) was published just a few years before the first edition of Herman Daly's comprehensive and persuasive \"Steady-State Economics\" (1977). \n\nThe first organized meetings of ecological economists occurred in the 1980s. These began in 1982, at the instigation of Lois Banner, with a meeting held in Sweden (including Robert Costanza, Herman Daly, Charles Hall, Bruce Hannon, H.T. Odum, and David Pimentel). Most were ecosystem ecologists or mainstream environmental economists, with the exception of Daly. In 1987, Daly and Costanza edited an issue of \"Ecological Modeling\" to test the waters. A book entitled \"Ecological Economics\", by Juan Martinez-Alier, was published later that year. 1989 saw the foundation of the International Society for Ecological Economics and publication of its journal, \"Ecological Economics\", by Elsevier. Robert Costanza was the first president of the society and first editor of the journal, which is currently edited by Richard Howarth. Other figures include ecologists C.S. Holling and H.T. Odum, biologist Gretchen Daily, and physicist Robert Ayres. In the Marxian tradition, sociologist John Bellamy Foster and CUNY geography professor David Harvey explicitly center ecological concerns in political economy.\n\nArticles by Inge Ropke (2004, 2005) and Clive Spash (1999) cover the development and modern history of ecological economics and explain its differentiation from resource and environmental economics, as well as some of the controversy between American and European schools of thought. An article by Robert Costanza, David Stern, Lining He, and Chunbo Ma responded to a call by Mick Common to determine the foundational literature of ecological economics by using citation analysis to examine which books and articles have had the most influence on the development of the field. However, citations analysis has itself proven controversial and similar work has been criticized by Clive Spash for attempting to pre-determine what is regarded as influential in ecological economics through study design and data manipulation. In addition, the journal Ecological Economics has itself been criticized for swamping the field with mainstream economics.\n\nA simple circular flow of income diagram is replaced in ecological economics by a more complex flow diagram reflecting the input of solar energy, which sustains natural inputs and environmental services which are then used as units of production. Once consumed, natural inputs pass out of the economy as pollution and waste. The potential of an environment to provide services and materials is referred to as an \"environment's source function\", and this function is depleted as resources are consumed or pollution contaminates the resources. The \"sink function\" describes an environment's ability to absorb and render harmless waste and pollution: when waste output exceeds the limit of the sink function, long-term damage occurs. Some persistent pollutants, such as some organic pollutants and nuclear waste are absorbed very slowly or not at all; ecological economists emphasize minimizing \"cumulative pollutants\". Pollutants affect human health and the health of the ecosystem.\n\nThe economic value of natural capital and ecosystem services is accepted by mainstream environmental economics, but is emphasized as especially important in ecological economics. Ecological economists may begin by estimating how to maintain a stable environment before assessing the cost in dollar terms. Ecological economist Robert Costanza led an attempted valuation of the global ecosystem in 1997. Initially published in \"Nature\", the article concluded on $33 trillion with a range from $16 trillion to $54 trillion (in 1997, total global GDP was $27 trillion). Half of the value went to nutrient cycling. The open oceans, continental shelves, and estuaries had the highest total value, and the highest per-hectare values went to estuaries, swamps/floodplains, and seagrass/algae beds. The work was criticized by articles in \"Ecological Economics\" Volume 25, Issue 1, but the critics acknowledged the positive potential for economic valuation of the global ecosystem.\n\nThe Earth's carrying capacity is a central issue in ecological economics. Early economists such as Thomas Malthus pointed out the finite carrying capacity of the earth, which was also central to the MIT study \"Limits to Growth\". Diminishing returns suggest that productivity increases will slow if major technological progress is not made. Food production may become a problem, as erosion, an impending water crisis, and soil salinity (from irrigation) reduce the productivity of agriculture. Ecological economists argue that industrial agriculture, which exacerbates these problems, is not sustainable agriculture, and are generally inclined favorably to organic farming, which also reduces the output of carbon.\n\nGlobal wild fisheries are believed to have peaked and begun a decline, with valuable habitat such as estuaries in critical condition. The aquaculture or farming of piscivorous fish, like salmon, does not help solve the problem because they need to be fed products from other fish. Studies have shown that salmon farming has major negative impacts on wild salmon, as well as the forage fish that need to be caught to feed them.\n\nSince animals are higher on the trophic level, they are less efficient sources of food energy. Reduced consumption of meat would reduce the demand for food, but as nations develop, they tend to adopt high-meat diets similar to that of the United States. Genetically modified food (GMF) a conventional solution to the problem, presents numerous problems – Bt corn produces its own Bacillus thuringiensis toxin/protein, but the pest resistance is believed to be only a matter of time. The overall effect of GMF on yields is contentious, with the USDA and FAO acknowledging that GMFs do not necessarily have higher yields and may even have reduced yields.\n\nGlobal warming is now widely acknowledged as a major issue, with all national scientific academies expressing agreement on the importance of the issue. As the population growth intensifies and energy demand increases, the world faces an energy crisis. Some economists and scientists forecast a global ecological crisis if energy use is not contained – the Stern report is an example. The disagreement has sparked a vigorous debate on issue of discounting and intergenerational equity.\n\nMainstream economics has attempted to become a value-free 'hard science', but ecological economists argue that value-free economics is generally not realistic. Ecological economics is more willing to entertain alternative conceptions of utility, efficiency, and cost-benefits such as positional analysis or multi-criteria analysis. Ecological economics is typically viewed as economics for sustainable development, and may have goals similar to green politics.\n\nIn international, regional, and national policy circles, the concept of the \"green economy\" grew in popularity as a response to the financial predicament at first then became a vehicle for growth and development.\n\nThe \"United Nations Environment Program\" (UNEP) defines a ‘green economy’ as one that focuses on the human aspects and natural influences and an economic order that can generate high-salary jobs. In 2011, its definition was further developed as the word ‘green’ is made to refer to an economy that is not only resourceful and well-organized but also impartial, guaranteeing an objective shift to an economy that is low-carbon, resource-efficient, and socially-inclusive.\n\nThe ideas and studies regarding the green economy denote a fundamental shift for more effective, resourceful, environment-friendly and resource‐saving technologies that could lessen emissions and alleviate the adverse consequences of climate change, at the same time confront issues about resource exhaustion and grave environmental dilapidation.\n\nAs an indispensable requirement and vital precondition to realizing sustainable development, the Green Economy adherents robustly promote good governance. To boost local investments and foreign ventures, it is crucial to have a constant and foreseeable macroeconomic atmosphere. Likewise, such an environment will also need to be transparent and accountable. In the absence of a substantial and solid governance structure, the prospect of shifting towards a sustainable development route would be insignificant. In achieving a green economy, competent institutions and governance systems are vital in guaranteeing the efficient execution of strategies, guidelines, campaigns, and programmes. \n\nShifting to a Green Economy demands a fresh mindset and an innovative outlook of doing business. It likewise necessitates new capacities, skills set from labor and professionals who can competently function across sectors, and able to work as effective components within multi-disciplinary teams. To achieve this goal, vocational training packages must be developed with focus on greening the sectors. Simultaneously, the educational system needs to be assessed as well in order to fit in the environmental and social considerations of various disciplines.\n\nVarious competing schools of thought exist in the field. Some are close to resource and environmental economics while others are far more heterodox in outlook. An example of the latter is the \"European Society for Ecological Economics\". An example of the former is the Swedish \"Beijer International Institute of Ecological Economics.\" Clive Spash has argued for the classification of the ecological economics movement, and more generally work by different economic schools on the environment, into three main categories. These are the mainstream new resource economists, the new environmental pragmatists, and the more radical social ecological economists. International survey work comparing the relevance of the categories for mainstream and heterodox economists shows some clear divisions between environmental and ecological economists.\n\nSome ecological economists prioritise adding natural capital to the typical capital asset analysis of land, labor, and financial capital. These ecological economists then use tools from mathematical economics as in mainstream economics, but may apply them more closely to the natural world. Whereas mainstream economists tend to be technological optimists, ecological economists are inclined to be technological sceptics. They reason that the natural world has a limited carrying capacity and that its resources may run out. Since destruction of important environmental resources could be practically irreversible and catastrophic, ecological economists are inclined to justify cautionary measures based on the precautionary principle.\n\nThe most cogent example of how the different theories treat similar assets is tropical rainforest ecosystems, most obviously the Yasuni region of Ecuador. While this area has substantial deposits of bitumen it is also one of the most diverse ecosystems on Earth and some estimates establish it has over 200 undiscovered medical substances in its genomes - most of which would be destroyed by logging the forest or mining the bitumen. Effectively, the instructional capital of the genomes is undervalued by analyses which view the rainforest primarily as a source of wood, oil/tar and perhaps food. Increasingly the carbon credit for leaving the extremely carbon-intensive (\"dirty\") bitumen in the ground is also valued - the government of Ecuador set a price of US$350M for an oil lease with the intent of selling it to someone committed to never exercising it at all and instead preserving the rainforest.\n\nWhile this natural capital and ecosystems services approach has proven popular amongst many it has also been contested as failing to address the underlying problems with mainstream economics, growth, market capitalism and monetary valuation of the environment. Critiques concern the need to create a more meaningful relationship with Nature and the non-human world than evident in the instrumentalism of shallow ecology and the environmental economists commodification of everything external to the market system.\n\nAmong the topics addressed by ecological economics are methodology, allocation of resources, weak versus strong sustainability, energy economics, energy accounting and balance, environmental services, cost shifting, and modeling.\n\nA primary objective of ecological economics (EE) is to ground economic thinking and practice in physical reality, especially in the laws of physics (particularly the laws of thermodynamics) and in knowledge of biological systems. It accepts as a goal the improvement of human well-being through development, and seeks to ensure achievement of this through planning for the sustainable development of ecosystems and societies. Of course the terms development and sustainable development are far from lacking controversy. Richard B. Norgaard argues traditional economics has hi-jacked the development terminology in his book \"Development Betrayed\".\n\nWell-being in ecological economics is also differentiated from welfare as found in mainstream economics and the 'new welfare economics' from the 1930s which informs resource and environmental economics. This entails a limited preference utilitarian conception of value i.e., Nature is valuable to our economies, that is because people will pay for its services such as clean air, clean water, encounters with wilderness, etc.\n\nEcological economics is distinguishable from neoclassical economics primarily by its assertion that the economy is embedded within an environmental system. Ecology deals with the energy and matter transactions of life and the Earth, and the human economy is by definition contained within this system. Ecological economists argue that neoclassical economics has ignored the environment, at best considering it to be a subset of the human economy.\n\nThe neoclassical view ignores much of what the natural sciences have taught us about the contributions of nature to the creation of wealth e.g., the planetary endowment of scarce matter and energy, along with the complex and biologically diverse ecosystems that provide goods and ecosystem services directly to human communities: micro- and macro-climate regulation, water recycling, water purification, storm water regulation, waste absorption, food and medicine production, pollination, protection from solar and cosmic radiation, the view of a starry night sky, etc.\n\nThere has then been a move to regard such things as natural capital and ecosystems functions as goods and services. However, this is far from uncontroversial within ecology or ecological economics due to the potential for narrowing down values to those found in mainstream economics and the danger of merely regarding Nature as a commodity. This has been referred to as ecologists 'selling out on Nature'. There is then a concern that ecological economics has failed to learn from the extensive literature in environmental ethics about how to structure a plural value system.\n\nResource and neoclassical economics focus primarily on the efficient allocation of resources and less on the two other problems of importance to ecological economics: distribution (equity), and the scale of the economy relative to the ecosystems upon which it relies. Ecological economics makes a clear distinction between growth (quantitative increase in economic output) and development (qualitative improvement of the quality of life), while arguing that neoclassical economics confuses the two. Ecological economists point out that beyond modest levels, increased per-capita consumption (the typical economic measure of \"standard of living\") may not always lead to improvement in human well-being, but may have harmful effects on the environment and broader societal well-being. This situation is sometimes referred to as uneconomic growth (see diagram above).\n\nEcological economics challenges the conventional approach towards natural resources, claiming that it undervalues natural capital by considering it as interchangeable with human-made capital—labor and technology.\n\nThe impending depletion of natural resources and increase of climate-changing greenhouse gasses should motivate us to examine how political, economic and social policies can benefit from alternative energy. Shifting dependence on fossil fuels with specific interest within just one of the above-mentioned factors easily benefits at least one other. For instance, photo voltaic (or solar) panels have a 15% efficiency when absorbing the sun's energy, but its construction demand has increased 120% within both commercial and residential properties. Additionally, this construction has led to a roughly 30% increase in work demands (Chen).\n\nThe potential for the substitution of man-made capital for natural capital is an important debate in ecological economics and the economics of sustainability.\nThere is a continuum of views among economists between the strongly neoclassical positions of Robert Solow and Martin Weitzman, at one extreme and the 'entropy pessimists', notably Nicholas Georgescu-Roegen and Herman Daly, at the other.\n\nNeoclassical economists tend to maintain that man-made capital can, in principle, replace all types of natural capital. This is known as the weak sustainability view, essentially that every technology can be improved upon or replaced by innovation, and that there is a substitute for any and all scarce materials.\n\nAt the other extreme, the strong sustainability view argues that the stock of natural resources and ecological functions are irreplaceable. From the premises of strong sustainability, it follows that economic policy has a fiduciary responsibility to the greater ecological world, and that sustainable development must therefore take a different approach to valuing natural resources and ecological functions.\n\nRecently, Stanislav Shmelev developed a new methodology for the assessment of progress at the macro scale based on multi-criteria methods, which allows consideration of different perspectives, including strong and weak sustainability or conservationists vs industrialists and aims to search for a 'middle way' by providing a strong neo-Keynesian economic push without putting excessive pressure on the natural resources, including water or producing emissions, both directly and indirectly.\n\nA key concept of energy economics is net energy gain, which recognizes that all energy requires energy to produce. To be useful the energy return on energy invested (\"EROEI\") has to be greater than one. The net energy gain from production coal, oil and gas has declined over time as the easiest to produce sources have been most heavily depleted.\n\nEcological economics generally rejects the view of energy economics that growth in the energy supply is related directly to well being, focusing instead on biodiversity and creativity - or natural capital and individual capital, in the terminology sometimes adopted to describe these economically. In practice, ecological economics focuses primarily on the key issues of uneconomic growth and quality of life. Ecological economists are inclined to acknowledge that much of what is important in human well-being is not analyzable from a strictly economic standpoint and suggests an interdisciplinary approach combining social and natural sciences as a means to address this.\n\nThermoeconomics is based on the proposition that the role of energy in biological evolution should be defined and understood through the second law of thermodynamics, but also in terms of such economic criteria as productivity, efficiency, and especially the costs and benefits (or profitability) of the various mechanisms for capturing and utilizing available energy to build biomass and do work. As a result, thermoeconomics is often discussed in the field of ecological economics, which itself is related to the fields of sustainability and sustainable development.\n\nExergy analysis is performed in the field of industrial ecology to use energy more efficiently. The term \"exergy\", was coined by Zoran Rant in 1956, but the concept was developed by J. Willard Gibbs. In recent decades, utilization of exergy has spread outside of physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics.\n\nAn energy balance can be used to track energy through a system, and is a very useful tool for determining resource use and environmental impacts, using the First and Second laws of thermodynamics, to determine how much energy is needed at each point in a system, and in what form that energy is a cost in various environmental issues. The energy accounting system keeps track of energy in, energy out, and non-useful energy versus work done, and transformations within the system.\n\nScientists have written and speculated on different aspects of energy accounting.\n\nEcological economists agree that ecosystems produce enormous flows of goods and services to human beings, playing a key role in producing well-being. At the same time, there is intense debate about how and when to place values on these benefits.\n\nA study was carried out by Costanza and colleagues to determine the 'value' of the services provided by the environment. This was determined by averaging values obtained from a range of studies conducted in very specific context and then transferring these without regard to that context. Dollar figures were averaged to a per hectare number for different types of ecosystem e.g. wetlands, oceans. A total was then produced which came out at 33 trillion US dollars (1997 values), more than twice the total GDP of the world at the time of the study. This study was criticized by pre-ecological and even some environmental economists - for being inconsistent with assumptions of financial capital valuation - and ecological economists - for being inconsistent with an ecological economics focus on biological and physical indicators.\n\nThe whole idea of treating ecosystems as goods and services to be valued in monetary terms remains controversial. A common objection is that life is precious or priceless, but this demonstrably degrades to it being worthless within cost-benefit analysis and other standard economic methods. Reducing human bodies to financial values is a necessary part of mainstream economics and not always in the direct terms of insurance or wages. Economics, in principle, assumes that conflict is reduced by agreeing on voluntary contractual relations and prices instead of simply fighting or coercing or tricking others into providing goods or services. In doing so, a provider agrees to surrender time and take bodily risks and other (reputation, financial) risks. Ecosystems are no different from other bodies economically except insofar as they are far less replaceable than typical labour or commodities.\n\nDespite these issues, many ecologists and conservation biologists are pursuing ecosystem valuation. Biodiversity measures in particular appear to be the most promising way to reconcile financial and ecological values, and there are many active efforts in this regard. The growing field of biodiversity finance began to emerge in 2008 in response to many specific proposals such as the Ecuadoran Yasuni proposal or similar ones in the Congo. US news outlets treated the stories as a \"threat\" to \"drill a park\" reflecting a previously dominant view that NGOs and governments had the primary responsibility to protect ecosystems. However Peter Barnes and other commentators have recently argued that a guardianship/trustee/commons model is far more effective and takes the decisions out of the political realm.\n\nCommodification of other ecological relations as in carbon credit and direct payments to farmers to preserve ecosystem services are likewise examples that enable private parties to play more direct roles protecting biodiversity, but is also controversial in ecological economics. The United Nations Food and Agriculture Organization achieved near-universal agreement in 2008 that such payments directly valuing ecosystem preservation and encouraging permaculture were the only practical way out of a food crisis. The holdouts were all English-speaking countries that export GMOs and promote \"free trade\" agreements that facilitate their own control of the world transport network: The US, UK, Canada and Australia.\n\nEcological economics is founded upon the view that the neoclassical economics (NCE) assumption that environmental and community costs and benefits are mutually canceling \"externalities\" is not warranted. Joan Martinez Alier, for instance shows that the bulk of consumers are automatically excluded from having an impact upon the prices of commodities, as these consumers are future generations who have not been born yet. The assumptions behind future discounting, which assume that future goods will be cheaper than present goods, has been criticized by David Pearce and by the recent Stern Report (although the Stern report itself does employ discounting and has been criticized for this and other reasons by ecological economists such as Clive Spash).\n\nConcerning these externalities, some like the eco-businessman Paul Hawken argue an orthodox economic line that the only reason why goods produced unsustainably are usually cheaper than goods produced sustainably is due to a hidden subsidy, paid by the non-monetized human environment, community or future generations. These arguments are developed further by Hawken, Amory and Hunter Lovins to promote their vision of an environmental capitalist utopia in \"\".\n\nIn contrast, ecological economists, like Joan Martinez-Alier, appeal to a different line of reasoning. Rather than assuming some (new) form of capitalism is the best way forward, an older ecological economic critique questions the very idea of internalizing externalities as providing some corrective to the current system. The work by Karl William Kapp explains why the concept of \"externality\" is a misnomer. In fact the modern business enterprise operates on the basis of shifting costs onto others as normal practice to make profits. Charles Eisenstein has argued that this method of privatising profits while socialising the costs through externalities, passing the costs to the community, to the natural environment or to future generations is inherently destructive. As social ecological economist Clive Spash has noted, externality theory fallaciously assumes environmental and social problems are minor aberrations in an otherwise perfectly functioning efficient economic system. Internalizing the odd externality does nothing to address the structural systemic problem and fails to recognize the all pervasive nature of these supposed 'externalities'.\n\nMathematical modeling is a powerful tool that is used in ecological economic analysis. Various approaches and techniques include: evolutionary, input-output, neo-Austrian modeling, entropy and thermodynamic models, multi-criteria, and agent-based modeling, the environmental Kuznets curve, and Stock-Flow consistent model frameworks. System dynamics and GIS are techniques applied, among other, to spatial dynamic landscape simulation modeling. The Matrix accounting methods of Christian Felber provide a more sophisticated method for identifying \"the common good\"\n\nAssigning monetary value to natural resources such as biodiversity, and the emergent ecosystem services is often viewed as a key process in influencing economic practices, policy, and decision-making. While this idea is becoming more and more accepted among ecologists and conservationist, some argue that it is inherently false.\nMcCauley argues that ecological economics and the resulting ecosystem service based conservation can be harmful. He describes four main problems with this approach:\n\nFirstly, it seems to be assumed that all ecosystem services are financially beneficial. This is undermined by a basic characteristic of ecosystems: they do not act specifically in favour of any single species. While certain services might be very useful to us, such as coastal protection from hurricanes by mangroves for example, others might cause financial or personal harm, such as wolves hunting cattle. The complexity of Eco-systems makes it challenging to weigh up the value of a given species. Wolves play a critical role in regulating prey populations; the absence of such an apex predator in the Scottish Highlands has caused the over population of deer, preventing afforestation, which increases the risk of flooding and damage to property.\nSecondly, allocating monetary value to nature would make its conservation reliant on markets that fluctuate. This can lead to devaluation of services that were previously considered financially beneficial. Such is the case of the bees in a forest near former coffee plantations in Finca Santa Fe, Costa Rica. The pollination services were valued to over US$60,000 a year, but soon after the study, coffee prices dropped and the fields were replanted with pineapple. Pineapple does not require bees to be pollinated, so the value of their service dropped to zero.\n\nThirdly, conservation programmes for the sake of financial benefit underestimate human ingenuity to invent and replace ecosystem services by artificial means. McCauley argues that such proposals are deemed to have a short lifespan as the history of technology is about how Humanity developed artificial alternatives to nature’s services and with time passing the cost of such services tend to decrease. This would also lead to the devaluation of ecosystem services.\n\nLastly, it should not be assumed that conserving ecosystems is always financially beneficial as opposed to alteration. In the case of the introduction of the Nile perch to Lake Victoria, the ecological consequence was decimation of native fauna. However, this same event is praised by the local communities as they gain significant financial benefits from trading the fish.\n\nMcCauley argues that, for these reasons, trying to convince decision-makers to conserve nature for monetary reasons is not the path to be followed, and instead appealing to morality is the ultimate way to campaign for the protection of nature.\n\n\n\nSchools and institutes:\n\nEnvironmental data:\n\nMiscellaneous:\n"}
{"id": "35182952", "url": "https://en.wikipedia.org/wiki?curid=35182952", "title": "Embodied language processing", "text": "Embodied language processing\n\nEmbodied cognition occurs when an organism’s sensorimotor capacities (ability of the body to respond to its senses with movement), body and environment play an important role in thinking. The way in which a person’s body and their surroundings interacts also allows for specific brain functions to develop and in the future to be able to act. This means that not only does the mind influence the body’s movements, but the body also influences the abilities of the mind, also termed the bi-directional hypothesis. \nThere are three generalizations that are assumed to be true relating to embodied cognition. A person's motor system (that controls movement of the body) is activated when (1) they observe manipulable objects, (2) process action verbs, and (3) observe another individual's movements.\n\nEmbodied semantics is one of two theories concerning the location and processing of sensory motors inputs within the human brain. The theory of embodied semantics involves the existence of specialized hubs where the meaning of a word is tied with the sensory motor processing unit associated with the word meaning. For example, the concept of kicking would be represented in the sensory motor areas that control kicking actions. As a result, the theory assumes that individuals must possess a body to understand English.\n\nThe overlap between various semantic categories with sensory motor areas suggests that a common mechanism is used by neurons to process action, perception, and semantics. The correlation principle states that neurons that fire together, wire together. Also, neurons out of sync, delink. When an individual pronounces a word, the activation pattern for articulatory motor systems of the speaker leads to activation of auditory and somatosensory systems due to self-perceived sounds and movements. If a word meaning is grounded in the visual shapes of the objects, the word form circuit is active together with neural activity in the ventral-temporal visual stream related to processing of visual object information. Correlation learning links the word and object circuits, resulting in an embodied object-semantic relationship.\n\nA semantic hub represents a focal point in the brain where all semantic information pertaining to a specific word is integrated. For example, the color, shape, size, smell, and sound associated with the word “cat” would be integrated at the same semantic hub. Some candidate regions for semantic hubs include:\nSemantic integration mechanisms involve various hub sites listed above, which contradicts the idea that there is one center where all integration occurs. However, each individual hub is compliant with the amodal model. Collectively, all of the hubs provide evidence for the theory that there are areas within the brain where emotional, sensory, and motor information all converge in one area.\n\nEach potential semantic hub is activated to a specific degree according to the category that the perceived word belongs to. For example, lesions to each of the five potential hubs do not affect all words. Instead, experimental data determines that one semantic category suffers more than another as it pertains to the word.\nSome of the category differences are thought to be produced by the adjacent hubs. For example, category specificity is greatest close to the piriform and anterior insular olfactory cortex. Here, odor words such as “cinnamon” lead to greater activation than control words. In the gustatory cortex in the anterior insula and frontal operculum, taste words such as “sugar” lead to a stronger activation.\n\nExperiential Trace Hypothesis states that each time an individual interacts with the world, traces of that particular experience are left in our brain. These traces can be accessed again when a person thinks of words or sentences that remind them of that experience. Additionally, these traces in our brain are linked to the action that they are related to. Words and sentences become those cues that retrieve these traces from our mind. Researchers have studied if the previous experience with a word, such as its location (up or down) in space, affects how people understand and then respond to that word. In one experiment, researchers hypothesized that if reading an object word also activates a location that is linked to that noun, then the following action response should be compatible with that association. They found that participants were faster to push a button higher than another button when the word was associated with being \"up\" or \"above\" than when the button was lower than the other for words associated with \"up\" and \"above\". The results of this study displayed that participants were faster to respond when the location of the word and the action they had to perform were similar. This demonstrates that language processing and action are connected. This research also found that the location information of a word is automatically activated after seeing the word. In a similar study, it was discovered that participants were equally as fast at responding to words that were associated with either an upward or downward location when the buttons to respond to these words were horizontal – meaning that the experiential trace effect was ruled out when the responding action did not link to either of the locations that were activated.\n\nSome theorists have proposed an experiential-simulation approach of language understanding. They argue that previous experiential traces related to a word may be reactivated at a later stage when accessing the meaning of the same word. This has been highlighted through the example of encountering the word ‘airplane’ in a situation where someone points to an airplane in the sky, thus making one look upwards. These experiential traces, e.g. ‘looking upwards’ are later reactivated when accessing the meaning of the word ‘airplane'. Similarly, another example might be when a person accesses the meaning of the word ‘snail’, they might also access experiential traces associated with this word, e.g. ‘looking downwards’ (likely towards the ground).\n\nAs a result of previous experience to certain words, several studies have found that the action associated with a certain word is also activated in the motor cortices when processing that same word. For example, using event-related functional magnetic resonance imaging (fMRI), it was discovered that exposure to concrete action verbs referring to face, arm, or leg actions (e.g., to lick, pick, kick) activated motor regions that are stimulated when making actions with the foot, hand, or mouth.\n\nHowever, findings are not as clear cut when abstract verbs are involved. Embodied theories of language comprehension assume that abstract concepts, as well as concrete ones, are grounded in the sensorimotor system (Jirak et al., 2010). Some studies have investigated the activation of motor cortices using abstract and also concrete verbs, examining the stimulation of the motor cortices when comprehending literal action verbs (concrete) vs. the metaphorical usage of the same action verbs (abstract). One such study used fMRI to study participants whilst they viewed actions performed by the mouth, hand or foot, and read literal and metaphorical sentences related to the mouth hand or foot. This study found activation in the premotor cortex for literal action (e.g. “grasping the scissors”) but not for metaphorical usage (e.g. “grasping the idea”). These findings suggest that the assumption of embodied theories that abstract concepts, as well as concrete ones, are grounded in the sensorimotor system may not be true.\n\nHowever, in contrast, other research has found motor cortex activation for the metaphorical usage of action verbs. One such study investigated cortical activation during comprehension of literal and idiomatic sentences using Magnetoencephalography (MEG). During a silent reading task, participants were presented with stimuli which included both literal and metaphorical arm-related action verbs, e.g. “Mary caught the fish” versus “Mary caught the sun”, and also literal and metaphorical leg-related action verbs, e.g. “Pablo jumped on the armchair” versus “Pablo jumped on the bandwagon”. This study found that processing of abstract verbs (idioms in this case) did indeed activate motor regions of the brain, activating anterior fronto-temporal activity very early compared to literal verbs.\n\nHauk and colleagues found that reading words associated with foot, hand, or mouth actions (examples: kick, pick, lick) activated by motor areas adjacent or overlapping with areas activated by making actions with the hand, foot, or mouth. Additionally, neurolinguist Tettmanti and colleagues found that listening to action related sentences activated the premotor cortex in a somatotopic fashion. Example: leg sentences showed premotor activity dorsal to hand sentences dorsal to mouth sentences.\n\nAziz-Zadeh and colleagues localized foot, hand, and mouth premotor regions of interest in each subject by having subjects watch actions associated with each effector and read phrases associated with the foot, hand, and mouth. In each subject, regions most activated for watching a foot action were also most active for language related to foot actions. The same was true for the hand and mouth. Rizzolatti and colleagues have suggested that action plan (manipulating, reaching) is more important than the actual effector involved.\n\nOther studies have investigated activation of the motor system during comprehension of concrete and abstract sentences. Using Transcranial Magnetic Stimulation (TMS) and a behavioural paradigm, one study investigated whether listening to action-related sentences activated activity within the motor cortices. This was investigated using Motor Evoked Potentials (MEPs) from the TMS which were recorded from hand muscles when stimulating the hand motor area, and from foot and leg muscles when stimulating the foot motor area. Participants were presented with sentences relating to hand or foot actions. As control, participants listened to sentences containing abstract content. The study found there was indeed activation of the motor cortices whilst listening to sentences expressing foot/leg and hand/arm actions. This activation specifically concerned the areas of the motor system ‘where the effector involved in the processed sentence is motorically represented’ (pp. 360). Specifically, the results showed that listening to hand-action-related sentences prompted a decrease of MEP amplitude recorded from hand muscles and listening to foot-action-related sentences prompted a decrease of MEP amplitude recorded from foot muscle.\n\nAziz-Zadeh found that although both action observation and reading phrases about actions caused activity in premotor and prefrontal regions in the vicinity of Broca’s area, the activated regions largely did not overlap. Activations for reading phrases were anterior and medial to activations for action observation.\n\nAziz-Zadeh’s research contradicts that of Hamzei who stressed the overlap of language and action observation activations in the inferior frontal gyrus. However, the difference in results most likely was due to the difference in language tasks. Hamzei used a verb generation task that caused widespread activation in the inferior frontal gyrus and the premotor cortex. The action observation task led to a small activation area within the larger activation area. Therefore, Hamzei noticed the overlap between areas. Aziz-Zadeh used a less extensive frontal activation task which allowed for areas activated by reading and by action observation to be clearly distinguished.\n\n\"See also:\" The Bi-Directional Hypothesis of Language and Action\n\nSentence processing can facilitate activation of motor systems based on the actions referred to in the sentence. In one study, researchers asked participants to make judgments on whether a sentence was sensible or not. For example, \"You handed Courtney the notebook\" versus \"Courtney handed you the notebook\". They asked participants in one condition to push a button farther away from their body if the sentence was logical and a button close to their body when it wasn't logical. The results of this study demonstrated that participants were faster at pushing the \"sentence is logical\" button when the action in the sentence matched the action required by them to push the correct button. This means if the sentence read \"you handed Courtney the notebook\", the participants were faster to push the button that was farther away from them when this button meant the sentence was logical. The depicted motion in these sentences affected the amount of time required to understand the sentences that described the motion that is in the same direction. This effect has been shown to apply to sentences that describe concrete actions (putting a book on a shelf) as well as more abstract actions (you told the story to the policeman).\n\nOther studies have tried to understand the ACE phenomenon by examining the modulation of motor resonance during language comprehension. In one study participants were asked to read sentences containing a frame of between one and three words. The participants had to rotate a knob, in one direction for half of the experiment and in the other direction for the other half. Each 5° of rotation induced the presentation of a new frame. Each of the sentences described actions involving manual rotation. In these, the rotation direction would or would not match the rotation direction implied by the sentence. Earlier studies, such as that by Glenberg & Kaschak (2002), examined motor resonance in responses to sentences presumably given after the sentence was read. In contrast, results of this study revealed that motor resonance had dissipated before the end of the sentence, with motor resonance occurring on the verb. This study made use of comprehension questions rather than sensibility sentences. The researchers have argued that this created a more naturalistic reading situation, so it could be argued that the results of this study are deemed more suitable because they are in regards to more naturalistic language. Overall, the researchers have concluded that motor resonance is quite immediate and short-lived and that duration of the effect is modified by linguistic context.\n\nNeurophysiological evidence has also been presented to prove an ACE. This research used a behavioural paradigm as well as Event-Related Potential (ERP) to record brain activity, allowing the researchers to explore the neural brain markers of the ACE paradigm in semantic processing and motor responses. ERP was particularly beneficial in helping the researchers to investigate the bi-directional hypothesis of action-sentence comprehension, which proposes that language processing facilitates movement and movement also facilitates language comprehension. In the study participants listened to sentences describing an action that involved an open hand, a closed hand or no manual action. They were then required to press a button to indicate their understanding of the sentence. Each participant was assigned a hand-shape, either closed or open, which was required to activate the button. As well as two groups (closed or open hand-shapes), there were three different categories relating to hand-shape: compatible, incompatible and neutral. Behavioural results from the study showed that participants responded quicker when the hand-shape required to press the response-button was compatible with the hand-shape inferred by the sentence. ERP results provided evidence to support the bi-directional hypothesis, showing that cortical markers of motor processes were affected by sentence meaning, therefore providing evidence for a semantics-to-motor effect. ERP results also demonstrated a motor-to-semantics effect as brain markers of comprehension were modified by motor effects.\n\nThe Action-Compatibility Effect also states that the brain resources used to plan and carry out actions are also used in language comprehension; therefore, if an action implied in a sentence is different from the suggested response, there is interference within these brain resources.\n\nOther studies have demonstrated that reading an object name interferes with how a person plans on grasping that object. It was also found that similar words can prime similar actions. Playing the piano and using a typewriter both utilize similar motor actions; these words prime each other in a word decision task. These studies have concluded that activation of motor decisions occur automatically when exposed to action-related words.\n\nAziz-Zadeh investigated congruent somatotopic organization of semantic representations for metaphorical sentences in either hemisphere. Aziz-Zadeh presented subjects with stimuli such as “kick the bucket” or “bite the bullet” to read and then followed by presenting subjects with videos of hand, foot, and mouth actions. Evidence could not be found in either hemisphere to support this theory.\n\nThe metaphors used in the experiment are common in the English language however. Therefore, the argument stands that if a metaphor is heard often enough it will not activate the same network of processing that it did initially.\n\nMany studies have shown how body movements and speech can be combined to emphasize meaning (often called gesturing). A person can observe the actions of another to help them comprehend what that person is saying. For example, if a person is pointing repeatedly, it helps the listener to understand that the direction being inferred is very important; whereas if it was a casual point in the general direction, the location of the object may not be as necessary to comprehend what the speaker is saying. Another example may be the stomping of one’s foot. This can help the listener to understand the anger and frustration being conveyed by the speaker.\n\nMany studies have demonstrated that people’s understanding of words and sentences can influence their movements and actions as well as the opposite – peoples’ actions can influence how quickly they can comprehend a word or sentence. This knowledge is important for many reasons. One study looked at the impact of embodied cognition in a classroom setting to facilitate and enhance language learning. For a child, there is a difference between oral language learning and reading. In oral language learning, the mapping between a symbol (word) and the object is common – often brought about by gesturing to the object. However, when a child is learning to read, they focus on the letter-sound combinations and the correct pronunciation of the words. Usually, the object the words are referring to, are not immediately connected with the word so an association between the word and object isn't immediately made. The researchers of this study suggest the \"Moved by Reading\" intervention which consists of two parts – Physical Manipulation stage and an Imagined Manipulation stage. In physical manipulation, the child reads a sentence and then is instructed to act out that sentence with available toys. This forces the child to connect words with objects and their actions. In the imagined manipulation stage, the child reads the sentence and is then asked to imagine how they would interact with toys to act out the sentence. They studied this further and discovered that it is possible for these children to still benefit from the effects of embodied cognition when they manipulate objects on a computer screen. This embodied cognition software can help children facilitate language comprehension. Other implications for language instruction that would enhance acquisition and retention are to offer activities which invite learners to actively use their body in the process, or at least observe the teacher doing so, thus activating their mirror neurons.\n\n"}
{"id": "2155575", "url": "https://en.wikipedia.org/wiki?curid=2155575", "title": "Erich Goode", "text": "Erich Goode\n\nErich Goode is an American sociologist specializing in the sociology of deviance. He has written a number of books on the field in general, as well as on specific deviant topics.\nHe is a professor at the State University of New York at Stony Brook.\n\nGoode received a B.A. from Oberlin College (1960) and a Ph.D. in sociology from Columbia University (1966). He has taught at Columbia University, New York University, Florida Atlantic University, the University of North Carolina at Chapel Hill, and Hebrew University in Jerusalem, Israel. He is currently employed as a professor at the State University of New York at Stony Brook. He also teaches at the University of Maryland.\n\nGoode takes a constructionist approach to deviance. In his view, a behavior is deviant if and only if society at large considers it so. The broader social factors that go into the classification of a behavior as deviant are thus considered a valid subject of study. His research focuses on the deviant individuals (and behaviors) themselves, as well as the particular individuals and groups that play a part in classifying the behavior as deviant. \n\nAs a sociologist, Goode makes no judgment about whether a particular is \"bad\" or \"evil\", and considers deviance as a topic to be entirely dependent on whether the society at large considers the behavior deviant. In this view, a particular behavior can be deviant in one society, but normal in another. This is in contrast to the perspective of essentialism, which would say that a behavior either \"really is\" deviant or \"really isn't\", and that it is the task of the sociologist to discover and report on the truth of the matter, and what society at large believes is mostly irrelevant.\n\nAccording to the constructionist framework as espoused by Goode, an instance of \"deviance\" can exist as a social construct exclusively, completely separate from any actual behavior. In other words, \"imaginary deviance\" can exist that causes a frenzy of interesting sociological behavior in response to a non-existence phenomenon. Satanic ritual abuse is an example of this in modern times, and the case of witch hunts is an example from antiquity. These are often called moral panics, and Goode considers them a valid subject (perhaps the ideal subject) for deviance studies.\n\nErich Goode is known for his exploration and exposure of the \"moral panic\" concept. He takes a \"harm reductionist\" approach to studying social deviance. This commitment aims to reduce social harm without engaging in value judgments or essentialist claims about those being studied.\n\n1. Legal instrumental use - Taking prescribed drugs and over the counter drugs to relieve or treat symptoms. \n\n2. Legal recreational use - Using legal (tobacco, alcohol, caffeine) drugs to achieve a certain mental state. \n\n3. Illegal instrumental use - Taking non prescription drugs to accomplish a task or goal. \n\n4.Illegal recreational use - Taking illegal drugs for fun or pleasure to experience euphoria.\n\nIn \"The Marijuana Smokers\" (1971), Goode looked at marijuana through a sociological lens.\n\nIn \"Drugs in American Society\", Goode argued that the effect of a drug is dependent on the societal context in which it is taken. Thus, in one society (or social context) a particular drug may be a depressant, and in another it may be a stimulant.\n\n\"Deviant Behavior\" is a textbook intended for undergrad students. In it, Goode takes the position of a weak constructionist.\n\n\"Moral Panics: The Social Construction of Deviance\", written with Nachman Ben-Yehuda, is a book about moral panics, from a sociological perspective. \n\nIn \"Paranormal Beliefs: A Sociological Introduction\" (1999), Goode studies paranormal beliefs such as UFOs, ESP, and creationism using the methods of the sociology of deviance. Consistent in tone with the rest of his works, he takes the position that whether the phenomenon in question is real is not important to a sociologist. Rather, sociologists should be concerned with how the paranormalist is labeled as deviant, and what effect the label has on them and society.\n\nAs a sociologist, Goode relies heavily on informants for his research. For example, Goode consulted with and interviewed actual drug users for his books on drugs. In 1999, Goode admitted through the sociology journal circuit that he had engaged in sexual intercourse with many of his deviant informants, and discussed how this influenced his perspective on the subject he was studying. This caused a firestorm of articles defending or denouncing his work.\n"}
{"id": "32830827", "url": "https://en.wikipedia.org/wiki?curid=32830827", "title": "Gayane Khachaturian", "text": "Gayane Khachaturian\n\nGayane Khachaturian () (May 9, 1942 – May 1, 2009) was a Georgian-Armenian painter and graphic artist.\n\nGayane Khachaturian was born into an Armenian family in Tbilisi, capital of Georgia, and studied art at the Nikoladze Art School. She became seriously involved in the art scene after graduating from the Secondary School of Working Youth in 1960. She met Sergei Parajanov in 1967 at Elene Akhvlediani's house and they maintained a close friendship which lasted until his death. Some of Khachaturian’s works are permanently exhibited at the Yerevan Museum of Modern Art, the National Gallery of Armenia, Sergei Parajanov Museum in Yerevan as well as are in a number of private collections, including those owned by Valerie Khanukaev, Bagrat Nikogosyan, and Artashes Aleksanyan. When she was alive, her tiny studio on Bakinskaya Street had become a tourist attraction. According to Russian art critic Vitaly Patsyukov, \"Khachaturian is among those pioneers of new artistic consciousness who draw into their focus all phenomenal aspects of European 'actual view' and the radical sensuousness and natural freedom of plastic gesture.\" \n\nKhachaturian died on May 1, 2009 and is buried in the Armenian Pantheon of Tbilisi (Khojivank).\n\nGayane Khachaturian's first informal solo exhibition was at the Skvoznyachok Café in Yerevan in 1967 by the invitation of Sergei Parajanov. Since then her work began to appear in various shows and exhibitions:\n\n\n"}
{"id": "14073", "url": "https://en.wikipedia.org/wiki?curid=14073", "title": "Hydropower", "text": "Hydropower\n\nHydropower or water power (from , \"water\") is power derived from the energy of falling water or fast running water, which may be harnessed for useful purposes. Since ancient times, hydropower from many kinds of watermills has been used as a renewable energy source for irrigation and the operation of various mechanical devices, such as gristmills, sawmills, textile mills, trip hammers, dock cranes, domestic lifts, and ore mills. A trompe, which produces compressed air from falling water, is sometimes used to power other machinery at a distance.\n\nIn the late 19th century, hydropower became a source for generating electricity. Cragside in Northumberland was the first house powered by hydroelectricity in 1878 and the first commercial hydroelectric power plant was built at Niagara Falls in 1879. In 1881, street lamps in the city of Niagara Falls were powered by hydropower.\n\nSince the early 20th century, the term has been used almost exclusively in conjunction with the modern development of hydroelectric power. International institutions such as the World Bank view hydropower as a means for economic development without adding substantial amounts of carbon to the atmosphere,\nbut dams can have significant negative social and environmental impacts.\n\nIn India, water wheels and watermills were built, possibly as early as the 4th century BC, although records of that era are spotty at best.\n\nIn the Roman Empire, water-powered mills produced flour from grain, and were also used for sawing timber and stone; in China, watermills were widely used since the Han dynasty. In China and the rest of the Far East, hydraulically operated \"pot wheel\" pumps raised water into crop or irrigation canals.\n\nThe power of a wave of water released from a tank was used for extraction of metal ores in a method known as hushing. The method was first used at the Dolaucothi Gold Mines in Wales from 75 AD onwards, but had been developed in Spain at such mines as Las Médulas. Hushing was also widely used in Britain in the Medieval and later periods to extract lead and tin ores. It later evolved into hydraulic mining when used during the California Gold Rush.\n\nIn the Middle Ages, Islamic mechanical engineer Al-Jazari described designs for 50 devices, many of them water powered, in his book, \"The Book of Knowledge of Ingenious Mechanical Devices\", including clocks, a device to serve wine, and five devices to lift water from rivers or pools, though three are animal-powered and one can be powered by animal or water. These include an endless belt with jugs attached, a cow-powered shadoof, and a reciprocating device with hinged valves.\n\nIn 1753, French engineer Bernard Forest de Bélidor published \"Architecture Hydraulique\" which described vertical- and horizontal-axis hydraulic machines. By the late nineteenth century, the electric generator was developed by a team led by project managers and prominent pioneers of renewable energy Jacob S. Gibbs and Brinsley Coleberd and could now be coupled with hydraulics. The growing demand for the Industrial Revolution would drive development as well.\n\nAt the beginning of the Industrial Revolution in Britain, water was the main source of power for new inventions such as Richard Arkwright's water frame. Although the use of water power gave way to steam power in many of the larger mills and factories, it was still used during the 18th and 19th centuries for many smaller operations, such as driving the bellows in small blast furnaces (e.g. the Dyfi Furnace) and gristmills, such as those built at Saint Anthony Falls, which uses the 50-foot (15 m) drop in the Mississippi River.\n\nIn the 1830s, at the early peak in the US canal-building, hydropower provided the energy to transport barge traffic up and down steep hills using inclined plane railroads. As railroads overtook canals for transportation, canal systems were modified and developed into hydropower systems; the history of Lowell, Massachusetts is a classic example of commercial development and industrialization, built upon the availability of water power.\n\nTechnological advances had moved the open water wheel into an enclosed turbine or water motor. In 1848 James B. Francis, while working as head engineer of Lowell's Locks and Canals company, improved on these designs to create a turbine with 90% efficiency. He applied scientific principles and testing methods to the problem of turbine design. His mathematical and graphical calculation methods allowed the confident design of high-efficiency turbines to exactly match a site's specific flow conditions. The Francis reaction turbine is still in wide use today. In the 1870s, deriving from uses in the California mining industry, Lester Allan Pelton developed the high efficiency Pelton wheel impulse turbine, which utilized hydropower from the high head streams characteristic of the mountainous California interior.\n\nHydraulic power networks used pipes to carry pressurized water and transmit mechanical power from the source to end users. The power source was normally a head of water, which could also be assisted by a pump. These were extensive in Victorian cities in the United Kingdom. A hydraulic power network was also developed in Geneva, Switzerland. The world-famous Jet d'Eau was originally designed as the over-pressure relief valve for the network.\n\nWhere there is a plentiful head of water it can be made to generate compressed air directly without moving parts. In these designs, a falling column of water is purposely mixed with air bubbles generated through turbulence or a venturi pressure reducer at the high-level intake. This is allowed to fall down a shaft into a subterranean, high-roofed chamber where the now-compressed air separates from the water and becomes trapped. The height of the falling water column maintains compression of the air in the top of the chamber, while an outlet, submerged below the water level in the chamber allows water to flow back to the surface at a lower level than the intake. A separate outlet in the roof of the chamber supplies the compressed air. A facility on this principle was built on the Montreal River at Ragged Shutes near Cobalt, Ontario in 1910 and supplied 5,000 horsepower to nearby mines.\n\nHydropower is used primarily to generate electricity. Broad categories include:\nA hydropower resource can be evaluated by its available power. Power is a function of the hydraulic head and rate of fluid flow. The head is the energy per unit weight (or unit mass) of water. The static head is proportional to the difference in height through which the water falls. Dynamic head is related to the velocity of moving water. Each unit of water can do an amount of work equal to its weight times the head.\n\nThe power available from falling water can be calculated from the flow rate and density of water, the height of fall, and the local acceleration due to gravity.\nIn SI units, the power is:\n\nformula_1\n\nwhere\n\nTo illustrate, power is calculated for a turbine that is 85% efficient, with water at 1000 kg/cubic metre (62.5 pounds/cubic foot) and a flow rate of 80 cubic-meters/second (2800 cubic-feet/second), gravity of 9.81 metres per second squared and with a net head of 145 m (480 ft).\n\nIn SI units:\n\nIn English units, the density is given in pounds per cubic foot so acceleration due to gravity is inherent in the unit of weight. A conversion factor is required to change from foot lbs/second to kilowatts:\n\nOperators of hydroelectric stations will compare the total electrical energy produced with the theoretical potential energy of the water passing through the turbine to calculate efficiency. Procedures and definitions for calculation of efficiency are given in test codes such as ASME PTC 18 and IEC 60041. Field testing of turbines is used to validate the manufacturer's guaranteed efficiency. Detailed calculation of the efficiency of a hydropower turbine will account for the head lost due to flow friction in the power canal or penstock, rise in tail water level due to flow, the location of the station and effect of varying gravity, the temperature and barometric pressure of the air, the density of the water at ambient temperature, and the altitudes above sea level of the forebay and tailbay. For precise calculations, errors due to rounding and the number of significant digits of constants must be considered.\n\nSome hydropower systems such as water wheels can draw power from the flow of a body of water without necessarily changing its height. In this case, the available power is the kinetic energy of the flowing water. Over-shot water wheels can efficiently capture both types of energy.\nThe water flow in a stream can vary widely from season to season. Development of a hydropower site requires analysis of flow records, sometimes spanning decades, to assess the reliable annual energy supply. Dams and reservoirs provide a more dependable source of power by smoothing seasonal changes in water flow. However reservoirs have significant environmental impact, as does alteration of naturally occurring stream flow. The design of dams must also account for the worst-case, \"probable maximum flood\" that can be expected at the site; a spillway is often included to bypass flood flows around the dam. A computer model of the hydraulic basin and rainfall and snowfall records are used to predict the maximum flood.\n\nAs with other forms of economic activity, hydropower projects can have both a positive and a negative environmental and social impact, because the construction of a dam and power plant, along with the impounding of a reservoir, creates certain social and physical changes.These environmental and social impact are not limited to but include the removal of communities near the dam, as well as the hindrance of fish migration. Since water is being taken away, this also affects local communities access and amount of drinking water. There is also evidence that dams affect the soil, which then alters the ability of vegetation to grow.\nHydropower projects can also have indirect consequences, contributing to global warming: reservoirs accumulate plant material, which then decomposes, emitting methane in uneven bursts.\n\nThere are several tools to assess the impact of hydropower projects:\n\n\n\n\n\n\n\n"}
{"id": "26446891", "url": "https://en.wikipedia.org/wiki?curid=26446891", "title": "Hyperthecosis", "text": "Hyperthecosis\n\nHyperthecosis (or ovarian hyperthecosis) is hyperplasia of the theca interna of the ovary. Hyperthecosis occurs when an area of luteinization occurs along with stromal hyperplasia. The luteinized cells produce androgens, which may lead to hirsutism and virilization (or masculinization) in affected women.\n\nThe term hyperthecosis refers to the presence of nests of luteinized theca cells in the ovarian stroma due to differentiation of the ovarian interstitial cells into steroidogenically active luteinized stromal cells. These nests or islands of luteinized theca cells are scattered throughout the stroma of the ovary, rather than being confined to areas around cystic follicles as in polycystic ovary syndrome (PCOS). These luteinized theca cells result in greater production of androgens.\n\nSeen as a severe form of PCOS, the clinical features of hyperthecosis are similar to those of PCOS. Women with hyperthecosis often have more markedly elevated testosterone, more hirsutism, and are much more likely to be virilized. While elevated androgens in postmenopausal women is rare, hyperthecosis can present in both premenopausal or postmenopausal women. Women with hyperthecosis may or may not have always had underlying PCOS.\n\nThe etiology of hyperthecosis is unknown, however evidence suggests a possibility of genetic transmission. Hyperthecosis has been documented in familiar patterns. Insulin resistance may also play a role in the pathogenesis of hyperthecosis. Women with hyperthecosis have a significant degree of insulin resistance and insulin may stimulate the ovarian stromal androgen synthesis.\n\nAlthough no large studies showing the long term outcomes for women with hyperthecosis exist, a diagnosis of hyperthecosis may suggest an increased risk for metabolic complications of hyperlipidemia and type 2 diabetes . In postmenopausal women, hyperthecosis may also contribute to the pathogenesis of endometrial polyp, endometrial hyperplasia, and endometrioid adenocarcinoma due to the association of hyperestrinism (excess estrins in the body) and hyperthecosis. Treatment for hyperthecosis is based upon each case, but may range from pharmacological interventions to surgical.\n\n"}
{"id": "38277667", "url": "https://en.wikipedia.org/wiki?curid=38277667", "title": "Hypertopology", "text": "Hypertopology\n\nIn the mathematical branch of topology, a hyperspace (or a space equipped with a hypertopology) is a topological space, which consists of the set \"CL(X)\" of all closed subsets of another topological space \"X\", equipped with a topology so that the canonical map\n\nformula_1\n\nis a homeomorphism onto its image. As a consequence, a copy of the original space \"X\" lives inside hyperspace \"CL(X)\". \nEarly examples of hypertopology include the Hausdorff metric and Vietoris topology.\n\n\n"}
{"id": "163901", "url": "https://en.wikipedia.org/wiki?curid=163901", "title": "Information society", "text": "Information society\n\nAn information society is a society where the creation, distribution, use, integration and manipulation of information is a significant economic, political, and cultural activity. Its main drivers are digital information and communication technologies, which have resulted in an information explosion and are profoundly changing all aspects of social organization, including the economy, education, health, warfare, government and democracy. The people who have the means to partake in this form of society are sometimes called digital citizens, defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen labels that have been identified to suggest that humans are entering a new phase of society.\n\nThe markers of this rapid change may be technological, economic, occupational, spatial, cultural, or some combination of all of these.\nInformation society is seen as the successor to industrial society. Closely related concepts are the post-industrial society (Daniel Bell), post-fordism, post-modern society, knowledge society, telematic society, Information Revolution, liquid modernity, and network society (Manuel Castells).\n\nThere is currently no universally accepted concept of what exactly can be termed information society and what shall rather not so be termed. Most theoreticians agree that a transformation can be seen that started somewhere between the 1970s and today and is changing the way societies work fundamentally. Information technology goes beyond the internet, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information.\n\nKasiwulaya and Gomo (Makerere University) allude that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase.\n\nSome people, such as Antonio Negri, characterize the information society as one in which people do immaterial labour. By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It \"is\" now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development.\n\nMichael Buckland characterizes information in society in his book \"Information and Society.\" Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences.\n\nConsidering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society) as an information society because we think of it as such.\nThe word information may be interpreted in many different ways. According to Buckland in \"Information and Society\", most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing.\n\nThe growth of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of 60 CD-ROM per person in 2007 and represents a sustained annual growth rate of some 25%. The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades.\n\nJames R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175)\n\nOne of the first people to develop the concept of the information society was the economist Fritz Machlup. In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study \"The production and distribution of knowledge in the United States\" in 1962. This book was widely regarded and was eventually translated into Russian and Japanese. The Japanese have also studied the information society (or \"jōhōka shakai\", ).\n\nThe issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy, post-industrial society, postmodern society, network society, the information revolution, informational capitalism, network capitalism, and the like, have been debated over the last several decades.\n\nFritz Machlup (1962) introduced the concept of the knowledge industry. He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries.\n\nPeter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy.\n\nPorat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy.\n\nFor Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\".\n\nAlain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning.\n\nJean-François Lyotard has argued that \"knowledge has become the force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society.\n\nSimilarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55).\n\nNico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a).\n\nAlso Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994).\n\nAt the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells, network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm.\n\nJan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\".\n\nThe major critique of concepts such as information society, knowledge society, network society, postmodern society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adopt to existing political realities (kasiwulaya 2002b: 267).\n\nThese critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital. They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006).\n\nFor describing contemporary society based on a dialectic of the old and the new, continuity and discontinuity, other critical scholars have suggested several terms like:\n\nOther scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism (Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham is critical of Castells and argues that the latter’s account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society.\n\nAntonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.).\n\nOverall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation (David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment, rising poverty, social exclusion, the deregulation of the welfare state and of labour rights, the lowering of wages, welfare, etc.\n\nConcepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.\n\nInformation society is the means of getting information from one place to another. As technology has advanced so too has the way people have adapted in sharing this information with each other.\n\n\"Second nature\" refers a group of experiences that get made over by culture. They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted.\n\nHowever, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from else where. It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society.\n\nTherefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction.\n\nIn sociology, informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck, Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale.\n\nAs steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century.\n\nIn the book \"Future Shock\", Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \"post-industrial society\" and \"post-modern industrial society\" with a similar meaning.\n\nA number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy, rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution, and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well known terms agricultural revolution and industrial revolution.\n\nToday, It is important to selectively select the information. Due to information revolution, the amount of information is puzzling. Among these, we need to develop techniques that refine information. This is called \"data mining.\" It is an engineering term, but it is used in sociology. In other words, if the amount of information was competitive in the past, the quality of information is important today.\n\nOne of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property. Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially \"rejected\" because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment.\n\nResponses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see DRM) circumvention illegal, to the free software, open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share).\n\nCaveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society.\n\n\n\n "}
{"id": "255710", "url": "https://en.wikipedia.org/wiki?curid=255710", "title": "Kawaii", "text": "Kawaii\n\nThe cuteness culture, or \"kawaii\" aesthetic, has become a prominent aspect of Japanese popular culture, entertainment, clothing, food, toys, personal appearance and mannerisms. \n\nThe word \"kawaii\" originally derives from the phrase \"kao hayushi\", which literally means \"(one's) face (is) aglow,\" commonly used to refer to flushing or blushing of the face. The second morpheme is cognate with \"-bayu\" in \"mabayui\" (眩い, 目映い, or 目映ゆい) \"dazzling, glaring, blinding, too bright; dazzlingly beautiful\" (\"ma-\" is from 目 \"me\" \"eye\") and \"-hayu\" in \"omohayui\" (面映い or 面映ゆい) \"embarrassed/embarrassing, awkward, feeling self-conscious/making one feel self-conscious\" (\"omo-\" is from 面 \"omo\", an archaic word for \"face, looks, features; surface; image, semblance, vestige\"). Over time, the meaning changed into the modern meaning of \"cute\", and the pronunciation changed to \"kawayui\" and then to the modern \"kawaii\". It is most commonly written in hiragana, , but the ateji, , has also been appended. The kanji in the ateji literally translates to \"able to be loved, can/may love, lovable.\"\n\nThe original definition of \"kawaii\" came from Lady Murasaki's \"The Tale of Genji\", where it referred to pitiable qualities. During the Shogunate period under the ideology of neo-Confucianism, women came to be included under the term \"kawaii\" as the perception of women being animalistic was replaced with the conception of women as docile. However, the earlier meaning survives in the modern Standard Japanese adjectival noun かわいそう \"kawaisō\" (often written with \"ateji\" as 可哀相 or 可哀想) \"piteous, pitiable, arousing compassion, poor, sad, sorry\" (etymologically from 顔映様 \"face / projecting, reflecting, or transmitting light, flushing, blushing / seeming, appearance\"). Forms of \"kawaii\" and its derivatives \"kawaisō\" and \"kawairashii\" (with the suffix \"-rashii\" \"-like, -ly\") are used in modern dialects to mean \"embarrassing/embarrassed, shameful/ashamed\" or \"good, nice, fine, excellent, superb, splendid, admirable\" in addition to the standard meanings of \"adorable\" and \"pitiable.\"\n\nThe rise of cuteness in Japanese culture emerged in the 1970s as part of a new style of writing. Many teenage girls began to write laterally using mechanical pencils. These pencils produced very fine lines, as opposed to traditional Japanese writing that varied in thickness and was vertical. The girls would also write in big, round characters and they added little pictures to their writing, such as hearts, stars, emoticon faces, and letters of the Latin alphabet.\n\nThese pictures would be inserted randomly and made the writing difficult to read. As a result, this writing style caused a lot of controversy and was banned in many schools. During the 1980s, however, this new \"cute\" writing was adopted by magazines and comics and was put onto packaging and advertising.\n\nFrom 1984 to 1986, studied the development of cute handwriting, which he called Anomalous Female Teenage Handwriting, in depth. This type of cute Japanese handwriting has also been called: , meaning \"round writing\", , meaning \"kitten writing\", , meaning \"comic writing\", and , meaning \"fake-child writing\". Although it was commonly thought that the writing style was something that teenagers had picked up from comics, he found that teenagers had come up with the style themselves, spontaneously, as an underground trend. His conclusion was based on an observation that cute handwriting predates the availability of technical means for producing rounded writing in comics.\n\n, author of Cool Japan, says cute fashion in Japan can be traced back to the Edo period with the popularity of netsuke.\n\nBecause of this growing trend, companies such as Sanrio came out with merchandise like Hello Kitty. Hello Kitty was an immediate success and the obsession with cute continued to progress in other areas as well. More recently, Sanrio has released kawaii characters with deeper personalities that appeal to an older audience, such as Gudetama and Aggretsuko. These characters have enjoyed strong popularity as fans are drawn to their unique quirks in addition to their cute aesthetics. The 1980s also saw the rise of cute idols, such as Seiko Matsuda, who is largely credited with popularizing the trend. Women began to emulate Seiko Matsuda and her cute fashion style and mannerisms, which emphasized the helplessness and innocence of young girls. The market for cute merchandise in Japan used to be driven by Japanese girls between 15 and 18 years old. No longer limited to teenagers, the spread of making things as cute as possible, even common household items, is embraced by people of all ages.\n\n, in his work \"Kawaii Syndrome\", claims \"cute\" and \"neat\" have taken precedence over the former Japanese aesthetics of \"beautiful\" and \"refined\". As a cultural phenomenon, cuteness is increasingly accepted in Japan as a part of Japanese culture and national identity. , author of \"Cool Japan\", believes that \"cuteness\" is rooted in Japan's harmony-loving culture, and , a sociology professor at Musashi University in Tokyo, has stated that \"cute\" is a \"magic term\" that encompasses everything that is acceptable and desirable in Japan.\n\nJapanese women who feign kawaii behaviors (e.g., high-pitched voice, squealing giggles) that could be viewed as forced or inauthentic are called \"burikko\" and this is considered a gender performance. The neologism developed in the 1980s, perhaps originated by comedian .\n\nIn Japan, being cute is acceptable for both men and women. A trend existed of men shaving their legs to mimic the neotenic look. Japanese women often try to act cute to attract men. A study by Kanebo, a cosmetic company, found that Japanese women in their 20s and 30s favored the \"cute look\" with a \"childish round face\". Women also employ a look of innocence in order to further play out this idea of cuteness. Having large eyes is one aspect that exemplifies innocence; therefore many Japanese women attempt to alter the size of their eyes. To create this illusion, women may wear large contact lenses, false eyelashes, dramatic eye makeup, and even have an East Asian blepharoplasty, commonly known as double eyelid surgery.\n\n are media personalities in their teens and twenties who are considered particularly attractive or cute and who will, for a period ranging from several months to a few years, regularly appear in the mass media, e.g. as singers for pop groups, bit-part actors, TV personalities (\"tarento\"), models in photo spreads published in magazines, advertisements, etc. (But not every young celebrity is considered an idol. Young celebrities who wish to cultivate a rebellious image, such as many rock musicians, reject the \"idol\" label.) Speed, Morning Musume, AKB48, and Momoiro Clover Z are examples of popular idol groups in Japan during the 2000s & 2010s.\n\nLolita fashion is a very well-known and recognizable style in Japan. Based on Victorian fashion and the Rococo period, girls mix in their own elements along with gothic style to achieve the porcelain-doll look. The girls who dress in Lolita fashion try to look cute, innocent, and beautiful. This look is achieved with lace, ribbons, bows, ruffles, bloomers, aprons, and ruffled petticoats. Parasols, chunky Mary Jane heels, and Bo Peep collars are also very popular.\n\nSweet Lolita is a subset of Lolita fashion that includes even more ribbons, bows, and lace, and is often fabricated out of pastels and other light colors. Another subset of Lolita fashion related to \"sweet Lolita\" is Fairy Kei. Head-dresses such as giant bows or bonnets are also very common, while lighter make-up is also used to achieve a more natural look. Curled hair extensions, sometimes accompanied by eyelash extensions, are also popular in helping with the baby doll look.\n\nThemes such as fruits, flowers and sweets are often used as patterns on the fabrics used for dresses. Purses often go with the themes and are shaped as hearts, strawberries, or stuffed animals. \"Baby, the Stars Shine Bright\" is one of the more popular clothing stores for this style and often carries themes. Mannerisms are also important to many Sweet Lolitas. Sweet Lolita is not only a fashion, but also a lifestyle. This is evident in the 2004 film \"Kamikaze Girls\" where the main Lolita character, Momoko, drinks only tea and eats only sweets.\n\nDecora is a style that is characterized by wearing lots of \"decorations\" on oneself. It is considered to be self-decoration. The goal of this fashion is to become as vibrant and characterized as possible. People who take part in this fashion trend wear accessories such as multicolor hair pins, bracelets, rings, necklaces, etc. By adding on multiple layers of accessories on an outfit, the fashion trend tends to have a childlike appearance. It also includes toys and multicolor clothes.\n\nAlthough kawaii is typically a female-dominated fashion, there are men who decide to partake in this trend. Some men decide to transform themselves into women, more specifically kawaii women. They are able to transform themselves by wearing wigs, false eyelashes, applying makeup, and wearing kawaii female clothing. This is seen predominately in male entertainers, such as Torideta-san, a DJ who transforms himself into a kawaii woman when working at his nightclub.\n\nJapanese pop stars and actors often have longer hair, such as Takuya Kimura of SMAP. Men are also noted as often aspiring to a neotenic look. While it doesn't quite fit the exact specifications of what cuteness means for females, men are certainly influenced by the same societal mores - to be attractive in a specific sort of way that the society finds acceptable. In this way both Japanese men and women conform to the expectations of Kawaii in some way or another.\n\nThe concept of \"kawaii\" has had an influence on a variety of products, including candy, such as Hi-Chew, Koala's March and Hello Panda. Cuteness can be added to products by adding cute features, such as hearts, flowers, stars and rainbows. Cute elements can be found almost everywhere in Japan, from big business to corner markets and national government, ward, and town offices. Many companies, large and small, use cute mascots to present their wares and services to the public. For example:\n\n\"Cute\" can be also used to describe a specific fashion sense of an individual, and generally includes clothing that appears to be made for young children, apart from the size, or clothing that accentuates the cuteness of the individual wearing the clothing. Ruffles and pastel colors are commonly (but not always) featured, and accessories often include toys or bags featuring anime characters.\n\nThere have been occasions in which popular Western products failed to meet the expectations of \"kawaii\", and thus did not do well in the Japanese market. For example, Cabbage Patch Kids dolls did not sell well in Japan, because the Japanese considered their facial features to be \"ugly\" and \"grotesque\" compared to the flatter and almost featureless faces of characters such as Hello Kitty. Also, the doll Barbie, portraying an adult woman, did not become successful in Japan compared to Takara's Licca, a doll that was modeled after an 11-year-old girl.\n\nKawaii has gradually gone from a small subculture in Japan to an important part of Japanese modern culture as a whole. There is an overwhelming amount of modern items featuring kawaii themes, not only in Japan, but worldwide. And characters associated with kawaii have an astounding popularity these days. We can see the \"global cute\" by the billion-dollar sellers like Pokémon and Hello Kitty. \"Fueled by Internet subcultures, Hello Kitty alone has hundreds of entries on eBay, and is selling in more than 30 countries, including Argentina, Bahrain, and Taiwan.\"\n\nJapan has become a powerhouse in the kawaii industry and images of Doraemon, Hello Kitty, Pikachu, Sailor Moon and Hamtaro are popular in mobile phone accessories. However, Professor Tian Shenliang says that Japan's future is dependent on how much of an impact kawaii brings to humanity.\n\nThe Japanese Foreign Ministry has also recognized the power of cute merchandise and have sent three 18-year-old women overseas in the hopes of spreading Japanese culture around the world. The women are dressed in uniforms and maid costumes that are commonplace in Japan.\n\nKawaii manga and magazines have brought tremendous profit to Japanese press industry. Moreover, the worldwide revenue from the computer game and its merchandising peripherals are closing in on $5 billion, according to a Nintendo press release titled \"It's a Pokémon Planet\".\n\nKawaii products are seemingly gaining more popularity beyond the borders of Japan into other Asian markets, and it's seemingly becoming more popular in the US, especially among the young anime and manga fans as well as among those who are influenced by Japanese culture. Cute merchandise and products are especially popular in other parts of East Asia, such as China, Hong Kong, South Korea, Taiwan and Southeast Asian countries like the Philippines, Singapore, Thailand, and Vietnam.\n\nSebastian Masuda, owner of 6%DOKIDOKI and a global advocate for kawaii influence, takes the quality from Harajuku to Western markets in his stores and artwork. The underlying belief of this Japanese designer is that \"kawaii\" actually saves the world. The infusion of kawaii into other world markets and cultures is achieved by introducing kawaii via modern art; audio, visual, and written media; and the fashion trends of Japanese youth, especially in high school girls.\n\nJapanese kawaii seemingly operates as a center of global popularity due to its association with making cultural productions and consumer products \"cute\". This mindset pursues a global market, giving rise to numerous applications and interpretations in other cultures.\n\nThe dissemination of Japanese youth fashion and \"kawaii culture\" is usually associated with the Western society and trends set by designers borrowed or taken from Japan. With the emergence of China, South Korea and Singapore as economic centers in Asia, the Kawaii merchandise and product popularity has shifted back to the East. In these Asian markets, the kawaii concept takes on various forms and different types of presentation depending on the target audience.\n\nTaiwanese culture, the government in particular, has embraced and elevated kawaii to a new level of social consciousness. The introduction of the A-Bian doll was seen as the development of a symbol to advance democracy and assist in constructing a collective imagination and national identity for Taiwanese people. The A-Bian dolls are kawaii likeness of sports figure, famous individuals, and now political figures that use kawaii images as a means of self-promotion and potential votes. The creation of the A-Bian doll has allowed Taiwanese President Chen staffers to create a new culture where the \"kawaii\" image of a politician can be used to mobilize support and gain election votes.\n\nJapanese popular \"kawaii culture\" has had an effect on Singaporean youth. The emergence of Japanese culture can be traced back to the mid-1980s when Japan became one of the economic powers in the world. Kawaii has developed from a few children's television shows to an Internet sensation. Japanese media is used so abundantly in Singapore that youths are more likely to imitate the fashion of their Japanese idols, learn the Japanese language, and continue purchasing Japanese oriented merchandise.\n\nThe Asian countries of China, Hong Kong, South Korea, and Thailand either produce kawaii items for international consumption or have websites that cater for kawaii as part of the youth culture in their country. Kawaii has taken on a life of its own, spawning the formation of kawaii websites, kawaii home pages, kawaii browser themes and finally, kawaii social networking pages. While Japan is the origin and Mecca of all things kawaii, artists and businesses around the world are imitating the kawaii theme.\n\nKawaii has truly become \"greater\" than itself. The interconnectedness of today's world via the Internet has taken kawaii to new heights of exposure and acceptance, producing a kawaii \"movement\".\n\nThe Kawaii concept has become something of a global phenomenon. The aesthetic cuteness of Japan is very appealing to people globally. The wide popularity of Japanese kawaii is often credited with it being \"culturally odorless\". The elimination of exoticism and national branding has helped kawaii to reach numerous target audiences and span every culture, class, and gender group. The palatable characteristics of kawaii have made it a global hit, resulting in Japan's global image shifting from being known for austere rock gardens to being known for \"cute-worship\".\n\nIn 2014 the \"Collins English Dictionary\" in the United Kingdom entered \"kawaii\" into their then latest edition, defining as a \"Japanese artistic and cultural style that emphasizes the quality of cuteness, using bright colours and characters with a childlike appearance.\"\n\n\n"}
{"id": "1447904", "url": "https://en.wikipedia.org/wiki?curid=1447904", "title": "Kerr–Newman metric", "text": "Kerr–Newman metric\n\nThe Kerr–Newman metric is a solution of the Einstein–Maxwell equations in general relativity that describes the spacetime geometry in the region surrounding a charged, rotating mass. This solution has not been especially useful for describing astrophysical phenomena, because observed astronomical objects do not possess an appreciable net electric charge. The solution has instead been of primarily theoretical and mathematical interest. (It is assumed that the cosmological constant equals zero.)\n\nIn 1965, Ezra \"Ted\" Newman found the axisymmetric solution of Einstein's field equation for a black hole which is both rotating and electrically charged. This formula for the metric tensor formula_1 is called the Kerr–Newman metric. It is a generalisation of the Kerr metric for an uncharged spinning point-mass, which had been discovered by Roy Kerr two years earlier.\n\nFour related solutions may be summarized by the following table:\n\nwhere \"Q\" represents the body's electric charge and \"J\" represents its spin angular momentum.\n\nThe Kerr–Newman metric describes the geometry of spacetime for a rotating charged black hole with mass \"M\", charge \"Q\" and angular momentum \"J\". The formula for this metric depends upon what coordinates or coordinate conditions are selected. One way to express this metric is by writing down its line element in a particular set of spherical coordinates, also called Boyer–Lindquist coordinates:\n\nwhere the coordinates (\"r\", \"θ\", \"ϕ\") are standard oblate spheroidal coordinate system, and the length-scales:\n\nhave been introduced for brevity. Here \"r\" is the Schwarzschild radius of the massive body, which is related to its total mass-equivalent \"M\" by\n\nwhere \"G\" is the gravitational constant, and \"r\" is a length-scale corresponding to the electric charge \"Q\" of the mass\n\nwhere 1/(4π\"ε\") is Coulomb's force constant.\n\nThe total mass-equivalent \"M\", which also contains the electric field-energy and the rotational energy, and the irreducible mass \"M\" are related by\n\nIn order to electrically charge and/or spin a neutral and static body, energy has to be applied to the system. Due to the mass–energy equivalence, this energy also has a mass-equivalent; therefore \"M\" is always higher than \"M\". If for example the rotational energy of a black hole is extracted via the Penrose processes, the remaining mass-energy will always stay greater than or equal to \"M\".\n\nThe components of the Kerr–Newman metric can be read off after a simple algebraic re-arrangement:\n\nThe electromagnetic potential in Boyer–Lindquist coordinates is\n\nwhile the Maxwell-tensor is defined by\n\nIn combination with the Christoffel symbols the second order equations of motion can be derived with\n\nwhere formula_13 is the charge per mass of the testparticle.\n\nThe Kerr–Newman metric can be expressed in \"Kerr–Schild\" form, using a particular set of Cartesian coordinates as follows. These solutions were proposed by Kerr and Schild in 1965.\n\nNotice that k is a unit vector. Here \"M\" is the constant mass of the spinning object, \"Q\" is the constant charge of the spinning object, \"η\" is the Minkowski tensor, and \"a\" is a constant rotational parameter of the spinning object. It is understood that the vector formula_18 is directed along the positive z-axis. The quantity \"r\" is not the radius, but rather is implicitly defined like this:\n\nNotice that the quantity \"r\" becomes the usual radius \"R\"\n\nwhen the rotational parameter \"a\" approaches zero. In this form of solution, units are selected so that the speed of light is unity (\"c\" = 1). In order to provide a complete solution of the Einstein–Maxwell Equations, the Kerr–Newman solution not only includes a formula for the metric tensor, but also a formula for the electromagnetic potential:\n\nAt large distances from the source (R » a), these equations reduce to the Reissner–Nordström metric with:\n\nIn the Kerr–Schild form of the Kerr–Newman metric, the determinant of the metric tensor is everywhere equal to negative one, even near the source.\n\nSetting formula_23 to 0 and solving for formula_24 gives the inner and outer event horizon, which is located at the Boyer-Lindquist coordinate\nRepeating this step with formula_26 gives the inner and outer ergosphere\n\nFor brevity, we further use dimensionless natural units of formula_28, with Coulomb's constant formula_29, where formula_30 reduces to formula_31 and formula_32 to formula_33, and the equations of motion for a testparticle of charge formula_13 become\n\nwith formula_40 for the total energy and formula_41 for the axial angular momentum. formula_42 is the Carter constant:\n\nwhere formula_44 is the poloidial component of the testparticle's angular momentum, and formula_45 the orbital inclination angle.\n\nand\n\nare also conserved quantities. \n\nis the frame dragging induced angular velocity. The shorthand term formula_49 is defined by\n\nThe relation between the coordinate derivatives formula_51 and the local 3-velocity formula_52 is\n\nfor the radial,\n\nfor the poloidial, \n\nfor the axial and\n\nfor the total local velocity, where\n\nis the axial radius of gyration (local circumference divided by 2π), and\n\nthe gravitational time dilation component. The local radial escape velocity for a neutral particle is therefore\n\nThe Kerr–Newman metric is a generalization of other exact solutions in general relativity:\n\nThe Kerr–Newman solution (with cosmological constant equal to zero) is also a special case of more general exact solutions of the Einstein–Maxwell Equations.\n\nNewman's result represents the simplest stationary, axisymmetric, asymptotically flat solution of Einstein's equations in the presence of an electromagnetic field in four dimensions. It is sometimes referred to as an \"electrovacuum\" solution of Einstein's equations.\n\nAny Kerr–Newman source has its rotation axis aligned with its magnetic axis. Thus, a Kerr–Newman source is different from commonly observed astronomical bodies, for which there is a substantial angle between the rotation axis and the magnetic moment.\n\nIf the Kerr–Newman potential is considered as a model for a classical electron, it predicts an electron having not just a magnetic dipole moment, but also other multipole moments, such as an electric quadrupole moment. An electron quadrupole moment has not been detected empirically yet.\n\nIn the G=0 limit, the electromagnetic fields are those of a charged rotating disk inside a ring where the fields are infinite. The total field energy for this disk is infinite, and so this G=0 limit does not solve the problem of infinite self-energy.\n\nLike the Kerr metric for an uncharged rotating mass, the Kerr–Newman interior solution exists mathematically but is probably not representative of the actual metric of a physically realistic rotating black hole due to stability issues. Although it represents a generalization of the Kerr metric, it is not considered as very important for astrophysical purposes since one does not expect that realistic black holes have an important electric charge.\n\nThe Kerr–Newman metric defines a black hole with an event horizon only when the following relation is satisfied:\n\nAn electron's \"a\" and \"Q\" (suitably specified in geometrized units) both exceed its mass \"M\", in which case the metric has no event horizon and thus there can be no such thing as a black hole electron — only a naked spinning ring singularity. Such a metric has several seemingly unphysical properties, such as the ring's violation of the cosmic censorship hypothesis, and also appearance of causality-violating closed timelike curves in the immediate vicinity of the ring.\n\nThe Russian theorist Alexander Burinskii wrote in 2007: \"In this work we obtain an exact correspondence between the wave function of the Dirac equation and the spinor (twistorial) structure of the Kerr geometry. It allows us to assume that the Kerr–Newman geometry reflects the specific space-time structure of electron, and electron contains really the Kerr–Newman circular string of Compton size\". The Burinskii paper describes an electron as a gravitationally confined ring singularity without an event horizon. It has some, but not all of the predicted properties of a black hole.\n\nThe electric and magnetic fields can be obtained in the usual way by differentiating the four-potential to obtain the electromagnetic field strength tensor. It will be convenient to switch over to three-dimensional vector notation.\n\nThe static electric and magnetic fields are derived from the vector potential and the scalar potential like this:\nUsing the Kerr–Newman formula for the four-potential in the Kerr–Schild form yields the following concise complex formula for the fields:\n\nThe quantity omega (formula_66) in this last equation is similar to the Coulomb potential, except that the radius vector is shifted by an imaginary amount. This complex potential was discussed as early as the nineteenth century, by the French mathematician Paul Émile Appell.\n\n"}
{"id": "2513891", "url": "https://en.wikipedia.org/wiki?curid=2513891", "title": "Kuleshov effect", "text": "Kuleshov effect\n\nThe Kuleshov effect is a film editing (montage) effect demonstrated by Soviet film-maker Lev Kuleshov in the 1910s and 1920s. It is a mental phenomenon by which viewers derive more meaning from the interaction of two sequential shots than from a single shot in isolation.\n\nKuleshov edited a short film in which a shot of the expressionless face of Tsarist matinee idol Ivan Mosjoukine was alternated with various other shots (a plate of soup, a girl in a coffin, a woman on a divan). The film was shown to an audience who believed that the expression on Mosjoukine's face was different each time he appeared, depending on whether he was \"looking at\" the plate of soup, the girl in the coffin, or the woman on the divan, showing an expression of hunger, grief or desire, respectively. The footage of Mosjoukine was actually the same shot each time. Vsevolod Pudovkin (who later claimed to have been the co-creator of the experiment) described in 1929 how the audience \"raved about the acting... the heavy pensiveness of his mood over the forgotten soup, were touched and moved by the deep sorrow with which he looked on the dead child, and noted the lust with which he observed the woman. But we knew that in all three cases the face was exactly the same.\"\n\nKuleshov used the experiment to indicate the usefulness and effectiveness of film editing. The implication is that viewers brought their own emotional reactions to this sequence of images, and then moreover attributed those reactions to the actor, investing his impassive face with their own feelings. Kuleshov believed this, along with montage, had to be the basis of cinema as an independent art form.\n\nThe effect has also been studied by psychologists, and is well-known among modern film-makers. Alfred Hitchcock refers to the effect in his conversations with François Truffaut, using actor James Stewart as the example.\n\nHitchcock, in the famous \"Definition of Happiness\" interview, also explains in detail many types of editing. The final form, which he calls \"pure editing\", is explained visually using the Kuleshov effect. In the first version of the example, Hitchcock is squinting, and the audience sees footage of a woman with a baby. The screen then returns to Hitchcock's face, now smiling. In effect, he is a kind old man. In the second example, the woman and baby are replaced with a woman in a bikini, Hitchcock explains: \"What is he now? He's a dirty old man.\"\n\nThe experiment itself was created by assembling fragments of pre-existing film from the Tsarist film industry, with no new material. Mosjoukine had been the leading romantic \"star\" of Tsarist cinema, and familiar to the audience.\n\nKuleshov demonstrated the necessity of considering montage as the basic tool of cinema art. In Kuleshov's view, the cinema consists of fragments and the assembly of those fragments, the assembly of elements which in reality are distinct. It is therefore not the content of the images in a film which is important, but their combination. The raw materials of such an art work need not be original, but are pre-fabricated elements which can be disassembled and re-assembled by the artist into new juxtapositions.\n\nThe montage experiments carried out by Kuleshov in the late 1910s and early 1920s formed the theoretical basis of Soviet montage cinema, culminating in the famous films of the late 1920s by directors such as Sergei Eisenstein, Vsevolod Pudovkin and Dziga Vertov, among others. These films included \"The Battleship Potemkin\", \"\", \"Mother\", \"The End of St. Petersburg\", and \"The Man with a Movie Camera\".\n\nThe Kuleshov effect has only been studied by psychologists in recent years. Prince and Hensley (1992) recreated the original study design but did not find the alleged effect. The study had 137 participants but was a single-trial between-subject experiment, which is prone to noise in the data. Mobbs et al. (2006) did a within-subject fMRI study and found an effect for negative, positive, or neutral valence. When a neutral face was shown behind a sad scene, it seemed sad, when it was shown behind a happy scene it seemed happy. More recently, Barratt, Rédei, Innes-Ker, and van de Weijer (2016) tested 36 participants using 24 film sequences across five emotional conditions (happiness, sadness, hunger, fear, and desire) and a neutral control condition. Again, they were able to show that neutral faces were rated in accordance with the stimuli material, confirming Mobbs et al. (2006) findings. \n\nThus, despite the initial problems in testing the Kuleshov effect experimentally, researchers now agree that the context in which a face is shown has a significant effect on how the face is perceived.\n\nTo find out whether the Kuleshov effect can also be induced auditorily, Baranowski and Hecht intercut different clips of faces with neutral scenes, featuring either happy music, sad music, or no music at all. They found that the music significantly influenced participants’ emotional judgments of facial expression.\n\n\n\n"}
{"id": "13048612", "url": "https://en.wikipedia.org/wiki?curid=13048612", "title": "Law of Azerbaijan", "text": "Law of Azerbaijan\n\nThe legal system of Azerbaijan is based on civil law. As the country was a republic of the Soviet Union until 1991, its legal history has also been influenced heavily by socialist law. However, after the collapse of the Soviet Union, Azerbaijan became independent by enactment of the constitutional act of national independence on October 18, 1991. \n\nThe term civil law in Azerbaijan refers to private law, which is opposed to the common law system of Criminal law and Civil law. The major body of statutes and laws governing civil law and procedure are set out in the Civil Code of Azerbaijan, established in December 1999.\n\nThe current Criminal Code of Azerbaijan came into force in September 2000, replacing the older Criminal Code of 1960 which had been based on the principles of Soviet law. Article 1 of the Criminal Code states that \"the Criminal legislation of the Republic of Azerbaijan consists of this Code. New laws defining criminal responsibility are subject to inclusion in this Code\"; this is characteristic of civil law legal systems such as France and Italy. In 1998 Azerbaijan was the first country in the Orient to abolish the capital punishment.\n\nUnlike common law systems such as the United States and United Kingdom, Azerbaijani courts do not rely extensively on case law and judicial precedent. Except for decisions of the Constitutional Court of Azerbaijan, decisions of the courts are not usually counted as a source of law. The sources of law in the Azerbaijani legal system are:\n\nThe Supreme Court of Azerbaijan is a supreme judicial body on civil, criminal and other cases related to the execution of general and specialized courts. The Constitutional Court of Azerbaijan is the supreme body of constitutional justice on the matters attributed to its jurisdiction by the Constitution, with authority to interpret and apply the Constitution of Azerbaijan.\n\nThe 1995 constitution provides for public trials in most cases, the presumption of innocence in criminal cases, and a defendant’s right to legal counsel. Both defendants and prosecutors have the right of appeal. In practice, however, the courts are politically oriented, seeming to overlook the government’s human rights violations. In July 1993, Heydar Aliyev ousted the Supreme Court chief justice because of alleged political loyalties to the opposition. The president directly appoints lower level judges. The president also appoints the Constitutional Court and Supreme Court judges with confirmation by the legislature. Prosecutors (procurators) are appointed by the president with confirmation by the legislature. The minister of justice organizes prosecutors into offices at the district, municipal, and republic levels. The constitution provides equal status for prosecutors and defense attorneys before the courts, but in practice the arrest and investigatory powers of the prosecutors have dominant influence before the courts. Judges will often remand a case for further prosecutory investigation rather than render an innocent verdict. Investigations often rely on obtaining confessions rather than on gathering evidence.\n\nThe Azerbaijan government’s human rights record is poor, although some public policy debate is allowed and human rights organizations operate. The government restricts freedom of assembly, religion, and association. Numerous cases of arbitrary arrest, beatings (some resulting in deaths), unwarranted searches and seizures, and other human rights abuses are reported. Political oppositionists are harassed and arrested, and there are dozens of political prisoners in Azerbaijan. The conflict between Armenians and Azerbaijanis contributed to widespread human rights violations by both sides. Some opposition newspapers are allowed to exist.\n\nAfter gaining of independence Azerbaijani legal system has undergone fundamental reforms. An important part of the legal reforms in the country is related to adoption of a new Constitution in Azerbaijan in 1995. Since that Constitution was amended three times in 2002, 2009 and 2016. Adoption of the Constitution has been followed by reforms related to progressive legislative acts such as, \"The Law on Constitutional court\", \"The Law on Courts and Judges\", \"The Law on Police\", \"The Law on Operational-investigational activity\", Law Judicial Legal Council, Criminal and Civil Procedure Codes and etc. Moreover, there have been made a number of amendements to the Civil Code, the Code of Civil Procedure, the Criminal Code, the Code of Criminal Procedure, etc.\n\nLegal reforms introduced new three stage - first instance, appelate and supreme court system in Azerbaijan and ensured its independence from other branches of power. At present, there are district (city) courts acts as the first degree of jurisdictional courts, military courts and local administrative-economic courts as territorial jurisdictional courts, also Court of Azerbaijan Republic on Felonies and Court of Azerbaijan on Grave Military Crimes.\n\nThe Supreme Court of Azerbaijan Republic is cassation degree of jurisdiction and consists of 4 chambers - Civil Chamber, Criminal Chamber, Military Chamber and Administrative-Economic Chamber. Court hearings are public, and defendants are free to choose their own attorney and have the right to appeal the verdict. In cases involving national security or other defined by law a judge may decide to hold a closed trial.\n\nAzerbaijan as a member of the Council of Europe ratificated European Convention on Human Rights . Further, Azerbaijani \"individuals, group of individuals and non-governmental organisations\" can refer cases to the ECHR.\n\nJudiciary of Azerbaijan\n\nConstitution of Azerbaijan\n\nConstitutional Court of Azerbaijan\n\n"}
{"id": "39323412", "url": "https://en.wikipedia.org/wiki?curid=39323412", "title": "List of textbooks in electromagnetism", "text": "List of textbooks in electromagnetism\n\nFollowing is a list of notable textbooks in electromagnetism.\n\n\n\n\n\n"}
{"id": "30468637", "url": "https://en.wikipedia.org/wiki?curid=30468637", "title": "Lonclass", "text": "Lonclass\n\nThe BBC's Lonclass (\"London Classification\") is a subject classification system used internally at the BBC throughout its archives.\n\nLonclass is derived from the Universal Decimal Classification (UDC), itself a reworking of the earlier Dewey Decimal Classification (DDC). Lonclass dates from the 1960s, whereas UDC was created from DDC in the late 19th century. The BBC adaptation of UDC preserves the core features that distinguish UDC from DDC: an emphasis on a compositional semantics that allows new items to be expressed in terms of relationships between known items.\n\nLonclass and UDC (like DDC) are expressed using codes based on decimal numbers. Unlike DDC, the Lonclass and UDC codes use additional punctuation to express patterns of relationships and re-usable qualifiers. While Lonclass makes a few structural adjustments to the UDC system to support its emphasis on TV and radio content, its main distinction is in the actual set of topics that are recorded within its authority file and within specific BBC catalogue records. Unlike UDC and DDC, which are widely used across the library community, Lonclass has remained a BBC-internal system since its creation in the 1960s.\nThere are 300,000 subject terms in the Lonclass vocabulary.\n\nFor example, a Lonclass or UDC code may represent “Report on the environmental impact of the decline of tin mining in Sweden in the 20th century“. This would be represented as a sequence of numbers and punctuation. The complex code can be broken down into primitives such as \"Sweden\" or \"tin mining\" (which itself could be broken down to \"Tin\" and \"Mining\").\n\nSome specific BBC Lonclass examples (with explanatory labels):\n\n\n"}
{"id": "7948226", "url": "https://en.wikipedia.org/wiki?curid=7948226", "title": "MIXR", "text": "MIXR\n\nThe Mixed Reality Simulation Platform (MIXR) is an open-source software project designed to support the development of robust, scalable, virtual and constructive, and stand-alone and distributed simulation applications. Its most common use case is to support the development of executable simulation applications used to assemble real-time, interactive, distributed virtual environments (DVEs).\n\nThe genesis of MIXR software codebase can be traced to the late 1980s when it was written in the C programming language and executed on a Commodore Amiga 1000. Because C doesn't directly support the object-oriented (OO) programming paradigm, the codebase defined an OO-like infrastructure to support programming from this perspective. In the early 1990's, the C-based OO system was converted to C++, where applications were developed and executed on Silicon Graphics (i.e., SGI) workstations. The transition away from Silicon Graphics workstations to personal computers (PCs) occurred in 1997.\n\nInitially, the codebase had no official name associated with it; that changed in 2002 when it was named the Enhanced Air-to-Air, Air-to-Ground Linked Environment Simulation (EAAGLES); later updated to mean the Extensible Architecture for the Analysis and Generation of Linked Simulations. The update removed domain-specific terminology such as \"air-to-air\" and \"air-to-ground\" to emphasize the more general purpose modeling and simulation capabilities the software is designed to support.\n\nIn 2003, EAAGLES became more visible within the DoD community when an EAAGLES-based fighter cockpit application was demonstrated at the Interservice/ Industry Training, Simulation and Education Conference (I/ITSEC); the world's largest modeling, simulation, and training conference. Even though the fighter cockpit simulator was developed in only a few months, it performed flawlessly; a testament to both the design of the application and the underlying framework.\n\nIn July of 2006, a significant subset of the original EAAGLES codebase was released into the public domain; it became what is known as . At the same time, a website was set up to provide information, documentation, and releases. In 2009, the book \"Design & Implementation of Virtual and Constructive Simulations Using \" was published. Since then, a steady stream of releases has been posted.\n\nIn 2017, the previously named project was renamed to MIXR for a number of reasons. These include:\n\n"}
{"id": "2990689", "url": "https://en.wikipedia.org/wiki?curid=2990689", "title": "Modeling perspective", "text": "Modeling perspective\n\nA modeling perspective in information systems is a particular way to represent pre-selected aspects of a system. Any perspective has a different focus, conceptualization, dedication and visualization of what the model is representing. \n\nThe traditional way to distinguish between modeling perspectives is structural, functional and behavioral/processual perspectives. This together with rule, object, communication and actor and role perspectives is one way of classifying modeling approaches.\n\nThis approach concentrates on describing the static structure. The main concept in this modeling perspective is the entity, this could be an object, phenomena, concept, thing etc. \n\nThe data modeling languages have traditionally handled this perspective, examples of such being: \n\nLooking at the ER-language we have the basic components:\n\nLooking at the generic semantic modeling language we have the basic components:\n\nThe functional modeling approach concentrates on describing the dynamic process. The main concept in this modeling perspective is the process, this could be a function, transformation, activity, action, task etc. A well-known example of a modeling language employing this perspective is data flow diagrams.\n\nThe perspective uses four symbols to describe a process, these being:\n\nNow, with these symbols, a process can be represented as a network of these symbols.\nThis decomposed process is a DFD, data flow diagram.\n\nBehavioral perspective gives a description of system dynamics. The main concepts in behavioral perspective are states and transitions between states. State transitions are triggered by events. State Transition Diagrams (STD/STM), State charts and Petri-nets are some examples of well-known behaviorally oriented modeling languages. Different types of State Transition Diagrams are used particularly within real-time systems and telecommunications systems.\n\nRule perspective gives a description of goals/means connections. The main concepts in rule perspective are rule, goal and constraint. A rule is something that influences the actions of a set of actors. The standard form of rule is “IF condition THEN action/expression”. Rule hierarchies (goal-oriented modeling), Tempora and Expert systems are some examples of rule oriented modeling.\n\nThe object-oriented perspective describes the world as autonomous, communicating objects. An object is an “entity” which has a unique and unchangeable identifier and a local state consisting of a collection of attributes with assignable values. The state can only be manipulated with a set of methods defined on the object. The value of the state can only be accessed by sending a message to the object to call on one of its methods. An event is when an operation is being triggered by receiving a message, and the trace of the events during the existence of the object is called the object’s life cycle or the process of an object. Several objects that share the same definitions of attributes and operations can be parts of an object class. The perspective is originally based on design and programming of object oriented systems. Unified Modelling Language (UML) is a well known language for modeling with an object perspective.\n\nThis perspective is based on language/action theory from philosophical linguistics. The basic assumption in this perspective is that person/objects cooperate on a process/action through communication within them.\n\nAn illocutionary act consists of five elements: Speaker, hearer, time, location and circumstances. It is a reason and goal for the communication, where the participations in a communication act is oriented towards mutual agreement. In a communication act, the speaker generally can raise three claims: truth (referring an object), justice (referring a social world of the participations) and claim to sincerity (referring the subjective world of the speaker).\n\nActor and role perspective is a description of organisational and system structure. An actor can be defined as a phenomenon that influences the history of another actor, whereas a role can be defined as the behaviour which is expected by an actor, amongst other actors, when filling the role. Modeling within these perspectives is based both on work with object-oriented programming languages and work with intelligent agents in artificial intelligence. I* is an example of an actor oriented language.\n\n\n"}
{"id": "5621398", "url": "https://en.wikipedia.org/wiki?curid=5621398", "title": "NTT InterCommunication Center", "text": "NTT InterCommunication Center\n\nNTT InterCommunication Center (ICC) is a media art gallery in Tokyo Opera City Tower in Shinjuku, Tokyo, Japan. It was established by NTT to commemorate the 100th anniversary of telephone service in Japan and opened in 1997. In addition to permanent and temporary exhibitions featuring international and Japanese artists, ICC holds workshops, performances, symposia, and produces publications with the goal of advancing communication between artists and scientists.\n\nTokyo Opera City Tower 4F, 3-20-2 Nishishinjuku, Shinjuku, Tokyo 163-1404 Japan\n"}
{"id": "29534297", "url": "https://en.wikipedia.org/wiki?curid=29534297", "title": "Nature Farming", "text": "Nature Farming\n\n\"Nature Farming\" was established in 1936 by Mokichi Okada, the founder of the Church of World Messianity, an agricultural system originally called \"no fertilizer farming\" or in Japanese.\n\nOffshoots such as the Sekai Kyusei Kyo, promoting ‘Kyusei nature farming’, and the Mokichi Okada Association formed after his death to continue promoting the work in Japan and South-East Asia.\n\nZZ2, a farming conglomerate in South Africa has translated the term to Afrikaans, \"Natuurboerdery\".\n\nAccording to the International Nature Farming Research Center in Nagano, Japan, it is based on the theories that:\nThe term is sometimes used for an alternative farming philosophy of Masanobu Fukuoka.\n\nAnother Japanese farmer and philosopher, Masanobu Fukuoka, conceived of an alternative farming system in the 1930s separately from Okada and used the same Japanese characters to describe it. This is generally translated in English as \"Natural Farming\" although agriculture researcher Hu-lian Xu claims that \"nature farming\" is the correct literal translation of the Japanese term.\n\n\n"}
{"id": "22484495", "url": "https://en.wikipedia.org/wiki?curid=22484495", "title": "Probability plot", "text": "Probability plot\n\nIn statistics, a probability plot is a graphical technique for comparing two data sets, either two sets of empirical observations, one empirical set against a theoretical set, or (more rarely) two theoretical sets against each other. It commonly means one of:\n\n\nThe term \"probability plot\" may be used to refer to both of these types of plot, or the term \"probability plot\" may be used to refer specifically to a P-P plot.\n\n\n"}
{"id": "33297737", "url": "https://en.wikipedia.org/wiki?curid=33297737", "title": "Radial set", "text": "Radial set\n\nIn mathematics, given a linear space formula_1, a set formula_2 is radial at the point formula_3 if for every formula_4 there exists a formula_5 such that for every formula_6, formula_7. Geometrically, this means formula_8 is radial at formula_9 if for every formula_4 a line segment emanating from formula_9 in the direction of formula_12 lies in formula_8, where the length of the line segment is required to be non-zero but can depend on formula_12.\n\nThe set of all points at which formula_2 is radial is equal to the algebraic interior. The points at which a set is radial are often referred to as internal points.\n\nA set formula_2 is absorbing if and only if it is radial at 0. Some authors use the term \"radial\" as a synonym for \"absorbing\", i. e. they call a set radial if it is radial at 0.\n"}
{"id": "580355", "url": "https://en.wikipedia.org/wiki?curid=580355", "title": "Rigpa", "text": "Rigpa\n\nIn Dzogchen teaching, rigpa (; Skt. vidyā; \"knowledge\") is the knowledge of the ground. The opposite of rigpa is marigpa (avidyā, ignorance).\n\n\"Rigpa\" is the knowledge of the ground. Erik Pema Kunsang translates a text which provides basic definitions of rigpa and marigpa in a Dzogchen context:\n\nIn Dzogchen, a fundamental point of practice is to distinguish rigpa from sems (\"citta\", (grasping) mind).\n\nRigpa has two aspects, namely \"kadag\" and \"lhun grub\". \"Kadag\" means \"purity\" or specifically \"primordial purity\". \"Lhun grub\" in Tibetan normally implies automatic, self-caused or spontaneous actions or processes. As quality of \"rigpa\" it means \"spontaneous presence\" It may also mean \"having a self-contained origin\", being primordially Existent, without an origin, self-existent. This division is the Dzogchen-equivalent of the more common Mahayana wisdom and compassion division.\n\nCiting Dodrupchen Jikme Tenpe Nyima, the 14th Dalai Lama states the full measure of rigpa occurs with the third vision.\n\nDzogchen practices aim to attain rigpa and integrate this into everyday life:\nThe Menngagde or 'Instruction Class' of Dzogchen teachings are divided into two parts: \"Trekchö\" and \"Tögal\" (thod rgal). Ron Garry: \n\n\n\n"}
{"id": "685179", "url": "https://en.wikipedia.org/wiki?curid=685179", "title": "Schwinger's quantum action principle", "text": "Schwinger's quantum action principle\n\nThe Schwinger's quantum action principle is a variational approach to quantum mechanics and quantum field theory. This theory was introduced by Julian Schwinger. In this approach, the quantum action is an operator. Although it is superficially different from the path integral formulation where the action is a classical function, the modern formulation of\nthe two formalisms are identical.\n\nSuppose we have two states defined by the values of a complete set of commuting operators at two times. Let the early and late states be formula_1 and formula_2, respectively. Suppose that there is a parameter in the Lagrangian which can be varied, usually a source for a field. The main equation of Schwinger's quantum action principle is:\n\nwhere the derivative is with respect to small changes in the parameter.\n\nIn the path integral formulation, the transition amplitude is represented by the sum\nover all histories of formula_4, with appropriate boundary conditions representing the states formula_1 and formula_2. The infinitesimal change in the amplitude is clearly given by Schwinger's formula. Conversely, starting from Schwinger's formula, it is easy to show that the fields obey canonical commutation relations and the classical equations\nof motion, and so have a path integral representation. Schwinger's formulation was most significant because it could treat fermionic anticommuting fields with the same formalism as bose fields, thus implicitly introducing differentiation and integration\nwith respect to anti-commuting coordinates.\n\n"}
{"id": "162986", "url": "https://en.wikipedia.org/wiki?curid=162986", "title": "Second language", "text": "Second language\n\nA person’s second language or L2 is a language that is not the native language/first language/L1 of the speaker, but is learned by the speaker after his/her native language (usually a foreign language, see below). Additionally, a person’s second language can be explained as the second language in the country the speaker lives in and they may be both used in daily life. For example, Canada has two official languages (English and French) and some citizens speak and use both of them.<br>\nA person’s dominant language, which is the language the speaker uses most or is most comfortable with, is not necessarily to be his/her first language. The second language can also be the dominant one. For example, the Canadian census defines first language for its purposes as \"the first language learned in childhood and still spoken\", recognizing that for some, the earliest language may be lost, a process known as language attrition. This can happen when young children move, with or without their family (because of immigration or international adoption), to a new language environment.<br>\n\nThe distinction between acquiring and learning was made by Stephen Krashen (1982) as part of his Monitor Theory. According to Krashen, the \"acquisition\" of a language is a natural process; whereas \"learning\" a language is a conscious one. In the former, the student needs to partake in natural communicative situations. In the latter, error correction is present, as is the study of grammatical rules isolated from natural language. Not all educators in second language agree to this distinction; however, the study of how a second language is \"learned/acquired\" is referred to as \"second-language acquisition\" (SLA).\n\nResearch in SLA \"...focuses on the developing knowledge and use of a language by children and adults who already know at least one other language... [and] a knowledge of second-language acquisition may help educational policy makers set more realistic goals for programmes for both foreign language courses and the learning of the majority language by minority language children and adults.\" (Spada & Lightbown, p. 115).\n\nSLA has been influenced by both linguistic and psychological theories. One of the dominant linguistic theories hypothesizes that a \"device\" or \"module\" of sorts in the brain contains innate knowledge. Many psychological theories, on the other hand, hypothesize that cognitive mechanisms, responsible for much of human learning, process language.\n\nOther dominant theories and points of research include 2nd language acquisition studies (which examine if L1 findings can be transferred to L2 learning), verbal behaviour (the view that constructed linguistic stimuli can create a desired speech response), morpheme studies, behaviourism, error analysis, stages and order of acquisition, structuralism (approach that looks at how the basic units of language relate to each other according to their common characteristics), 1st language acquisition studies, contrastive analysis (approach where languages were examined in terms of differences and similarities) and inter-language (which describes L2 learners’ language as a rule-governed, dynamic system) (Mitchell, Myles, 2004). \nThese theories have all influenced second-language teaching and pedagogy. There are many different methods of second-language teaching, many of which stem directly from a particular theory. Common methods are the grammar-translation method, the direct method, the audio-lingual method (clearly influenced by audio-lingual research and the behaviourist approach), the Silent Way, Suggestopedia, community language learning, the Total Physical Response method, and the communicative approach (highly influenced by Krashen’s theories) (Doggett, 1994). Some of these approaches are more popular than others, and are viewed to be more effective. Most language teachers do not use one singular style, but will use a mix in their teaching. This provides a more balanced approach to teaching and helps students of a variety of learning styles succeed.\n\nThe defining difference between a first language (L1) and a second language (L2) is the age the person learned the language. For example, linguist Eric Lenneberg used \"second language\" to mean a language consciously acquired or used by its speaker after puberty. In most cases, people never achieve the same level of fluency and comprehension in their second languages as in their first language. These views are closely associated with the critical period hypothesis.\n\nIn acquiring an L2, Hyltenstam (1992) found that around the age of six or seven seemed to be a cut-off point for bilinguals to achieve native-like proficiency. After that age, L2 learners could get \"near-native-like-ness\" but their language would, while consisting of few actual errors, have enough errors to set them apart from the L1 group. The inability of some subjects to achieve native-like proficiency must be seen in relation to the \"age of onset\" (AO). Later, Hyltenstam & Abrahamsson (2003) modified their age cut-offs to argue that after childhood, in general, it becomes more and more difficult to acquire native-like-ness, but that there is no cut-off point in particular.<br>\n\nAs we are learning more and more about the brain, there is a hypothesis that when a child is going through puberty, that is the time that accents \"start\". Before a child goes through puberty, the chemical processes in the brain are more geared towards language and social communication. Whereas after puberty, the ability for learning a language without an accent has been rerouted to function in another area of the brain—most likely in the frontal lobe area promoting cognitive functions, or in the neural system of hormone allocated for reproduction and sexual organ growth.\n\nAs far as the relationship between age and eventual attainment in SLA is concerned, Krashen, Long, and Scarcella, say that people who encounter foreign language in early age, begin natural exposure to second languages and obtain better proficiency than those who learn the second language as an adult. However, when it comes to the relationship between age and rate SLA, “Adults proceed through early stages of syntactic and morphological development faster than children (where time and exposure are held constant)” (Krashen, Long, Scarcella 573). Also, “older children acquire faster than younger children do (again, in early stages of morphological and syntactic development where time and exposure are held constant)” (573). In other words, adults and older children are fast learners when it comes to the initial stage of foreign language education.\n\nGauthier and Genesee (2011) have done a research which mainly focuses on the second language acquisition of internationally adopted children and results show that early experiences of one language of children can affect their ability to acquire a second language, and usually children learn their second language slower and weaker even during the critical period.<br>\n\nAs for the fluency, it is better to do foreign language education at an early age, but being exposed to a foreign language since an early age causes a “weak identification” (Billiet, Maddens and Beerten 241). Such issue leads to a \"double sense of national belonging,\" that makes one not sure of where he or she belongs to because according to Brian A. Jacob, multicultural education affects students' \"relations, attitudes, and behaviors\" (Jacob 364). And as children learn more and more foreign languages, children start to adapt, and get absorbed into the foreign culture that they “undertake to describe themselves in ways that engage with representations others have made” (Pratt 35). Due to such factors, learning foreign languages at an early age may incur one’s perspective of his or her native country.\nAcquiring a second language can be a lifelong learning process for many. Despite persistent efforts, most learners of a second language will never become fully \"native-like\" in it, although with practice considerable fluency can be achieved. However, children by around the age of 5 have more or less mastered their first language with the exception of vocabulary and a few grammatical structures, and the process is relatively very fast because language is a very complex skill. Moreover, if children start to learn a second language when they are 7 years old or younger, they will also be fully fluent with their second language in a faster speed comparing to the speed of learning by adults who start to learn a second language later in their life. <br>\n\nIn the first language, children do not respond to systematic correction. Furthermore, children who have limited input still acquire the first language, which is a significant difference between input and output. Children are exposed to a language environment of errors and lack of correction but they end up having the capacity to figure out the grammatical rules. Error correction does not seem to have a direct influence on learning a second language. Instruction may affect the rate of learning, but the stages remain the same. Adolescents and adults who know the rule are faster than those who do not. <br>\n\nIn the learning of a second language the correction of errors remains a controversial topic with many differing schools of thought. Throughout the last century much advancement has been made in research on the correction of students’ errors. In the 1950s and 60s the viewpoint of the day was that all errors must be corrected at all costs. Little thought went to students’ feelings or self-esteem in regards to this constant correction (Russell, 2009).\n\nIn the 1970s Dulay and Burt’s studies showed that learners acquire grammar forms and structures in a pre-determined, inalterable order, and that teaching or correcting styles would not change this (Russell, 2009).\n\nIn this same decade Terrell (1977) did studies that showed that there were more factors to be considered in the classroom than the cognitive processing of the students (Russell, 2009). He contested that the affective side of students and their self-esteem were equally important to the teaching process (Russell, 2009).\n\nA few years later in the 1980s, the strict grammar and corrective approach of the 1950s became obsolete. Researchers asserted that correction was often unnecessary and that instead of furthering students’ learning it was hindering them (Russell, 2009). The main concern at this time was relieving student stress and creating a warm environment for them. Stephen Krashen was a big proponent in this hands-off approach to error correction (Russell, 2009).\n\nThe 1990s brought back the familiar idea that explicit grammar instruction and error correction was indeed useful for the SLA process. At this time, more research started to be undertaken to determine exactly which kinds of corrections are the most useful for students. In 1998, Lyster concluded that “recasts” (when the teacher repeats a student’s incorrect utterance with the correct version) are not always the most useful because students do not notice the correction (Russell, 2009). His studies in 2002 showed that students learn better when teachers help students recognize and correct their own errors (Russell, 2009). Mackey, Gas and McDonough had similar findings in 2000 and attributed the success of this method to the student’s active participation in the corrective processes.\n\nAccording to Noam Chomsky, children will bridge the gap between input and output by their innate grammar because the input (utterances they hear) is so poor but all children end up having complete knowledge of grammar. Chomsky calls it the Poverty of Stimulus. And second language learners can do this by applying the rules they learn to the sentence-construction, for example. So learners in both their native and second language have knowledge that goes beyond what they have received, so that people can make correct utterances (phrases, sentences, questions, etc) that they have never learned or heard before.<br>\n\nBilingualism has been an advantage to today's world and being bilingual gives the opportunity to understand and communicate with people with different cultural backgrounds. However, a study done by Optiz and Degner in 2012 shows that sequential bilinguals (i.e. learn their L2 after L1) often relate themselves to the emotions more when they perceive these emotions by their first language/native language/L1, but feel less emotional when by their second language even though they know the meaning of words clearly. The emotional distinction between L1 and L2 indicates that the \"effective valence\" of words is processed less immediate in L2 because of the delayed vocabulary/lexical access to these two languages.<br>\n\nSuccess in language learning can be measured in two ways: likelihood and quality. First language learners \"will\" be successful in both measurements. It is inevitable that all people will learn a first language and with few exceptions, they will be fully successful. For second language learners, success is not guaranteed. For one, learners may become fossilized or \"stuck\" as it were with ungrammatical items. (Fossilization occurs when language errors become a permanent feature. See Canale & Swain (1980), Johnson (1992), Selinker (1972), and Selinker and Lamendella (1978).) The difference between learners may be significant. As noted elsewhere, L2 learners rarely achieve complete \"native-like\" control of the second language. <br>\nFor L2 pronounciation, there are two principles that haven been put forth by Levis (2005). The first is nativeness which means the speakers' ability to approximately reach the speaking pattern of the second language of speakers; and the second, understanding, refers to the speaker's ability to make themselves understood.\nBeing successful in learning a second language can seem like a daunting task. Research has been done to look into why some students are more successful than others. Stern (1975), Rubin (1975) and Reiss (1985) are just a few of the researchers who have dedicated time to this subject. They have worked to determine what qualities make a \"good language learner\" (Mollica, Neussel, 1997). Some of their common findings are that a good language learner uses positive learning strategies, is an active learner who is constantly searching for meaning. Also a good language learner demonstrates a willingness to practice and use the language in real communication. He also monitors himself and his learning, has a strong drive to communicate, and has a good ear and good listening skills (Mollica, Neussel, 1997).<br>\n\nÖzgür and Griffiths have designed an experiment in 2013 about the relationship between different motivations and second language acquisition. They have looked at four types of motivations—intrinsic (inner feelings of learner), extrinsic (reward from outside), integrative (attitude towards learning), and instrumental (practical needs). According to the test results, the intrinsic part has been the main motivation for these student who learn English as their second language. However, students report themselves being strongly instrumentally motivated. In conclusion, learning a second language and being successful depend on every individual.<br>\n\nIn pedagogy and sociolinguistics, a distinction is made between second language and foreign language, the latter is being learned for use in an area where that language is originally from another country and not spoken in the native country of the speakers. And in other words, foreign language is used from the perspective of countries; the second language is used from the perspective of individuals.\n\nFor example, arguably, English in countries such as India, Pakistan, Bangladesh, the Philippines, the Nordic countries and the Netherlands can be considered a second language for many of its speakers, because they learn it young and use it regularly; indeed in southern Asia it is the official language of the courts, government and business. The same can be said for French in Algeria, Morocco and Tunisia, although French is nominally not an official language in any of these Arabic-speaking countries. In practice, French is widely used in a variety of contexts in these countries, and public signs are normally printed in both Arabic and French. A similar phenomenon exists in post-Soviet states such as the Ukraine, Uzbekistan, Kyrgyzstan and Kazakhstan, where Russian can be considered a second language, and there are large Russophone communities there.\n\nHowever, in China (with the exception perhaps of Hong Kong), English must be considered a foreign language due to the lack of opportunities for use, such as historical links, media, conversation between people, and similar vocabulary. Likewise, French would be considered a foreign language in Romania and Moldova. This is despite Romanian and French being Romance languages (unlike Chinese and English, which come from two different language families: Sino-Tibetan and Indo-European). This is also despite Romania and Moldova being the only two countries in the world where Romanian is an official language at the national level, Romania's historical links to France, and both Romanian-speaking countries' membership in the Francophonie.\nPsychological studies have found that speaking two or more languages is good for people's cognitive process and the differences between brains of bilinguals and single language speakers usually provides some mental benefits, according to an article on The Telegraph in 2013. Including but not limited to these: <br>\nBecoming smarter<br>\nSpeaking a second language improves the functions of the brain by thinking and using the different language systems.<br>\nBuilding multitasking skills<br>\nAccording to a study from the Pennsylvania State University, \"juggling language can make better brains\". Because multilingual people are usually good at switching between different language systems, they can be good multitaskers as well. <br>\nImproving memory<br>\nThe vocabulary capacity for a high school graduate student is about 45000 words, according to Nagy and Anderson (1984), and being a bilingual will double this number because learning a language involves memorizing rules and vocabulary.<br>\nSee more in references.<br>\nGeorge H. J. Weber, a Swiss businessman and independent scholar, founder of the Andaman Association and creator of the encyclopedic andaman.org Web site, made a report in December 1997 about the number of secondary speakers of the world's leading languages. Weber used the Fischer Weltalmanach of 1986 as his only source for the L2-speakers data, in preparing the data in the following table. These numbers are here compared with those referred to by Ethnologue, a popular source in the linguistics field. See below Table 1.\n\nCollecting the number of second language speakers of every language is extremely difficult and even the best estimates contain the guess work. Data below updated June 2013 from Ethnologue.com See below Table 2&3.\n\n\n"}
{"id": "35534004", "url": "https://en.wikipedia.org/wiki?curid=35534004", "title": "Signavio", "text": "Signavio\n\nSignavio is a vendor of Business Process Management (BPM) software based in Berlin and Silicon Valley. Its main product is Signavio Process Manager, a web-based business process modeling tool.\n\nThe company was founded by a team of alumnae from Hasso Plattner Institute (HPI) in Potsdam, Germany.\n\nPrior to Signavio, the founders were involved in development of the world's first web modeler for BPMN at HPI. This technology, known as the \"Oryx project\", was published under an Open Source license and served as blueprint for the Signavio Process Manager.\n\nSignavio is headquartered in Berlin, Germany. In 2012 the company was incorporated in the United States as Signavio Inc. with an office in Burlington, Massachusetts. The company is fully owned by the founders and staff. \nIn 2011 the German Federal Ministry of Economy and Technology named Signavio \"ICT startup of the Year\" and selected the firm for its \"German Silicon Valley Accelerator\" program.\n"}
{"id": "894421", "url": "https://en.wikipedia.org/wiki?curid=894421", "title": "Sociolect", "text": "Sociolect\n\nIn sociolinguistics, a sociolect or social dialect is a variety of language (a register) used by a socioeconomic class, a profession, an age group or other social group.\n\nSociolects involve both passive acquisition of particular communicative practices through association with a local community, as well as active learning and choice among speech or writing forms to demonstrate identification with particular groups.\n\nIndividuals who study sociolects are called sociolinguists. Sociolinguists study language variation. Sociolinguists define a sociolect by examining the social distribution of specific linguistic terms. For example, a sociolinguist would examine the use of the second person pronoun \"you\" for its use within the population. If one distinct social group used 'yous' as the plural form of the pronoun then this could indicate the existence of a sociolect. A sociolect is distinct from a dialect because social class rather than geographical subdivision substantiates the unique linguistic features.\n\nA sociolect, defined by Peter Trudgill, a leading sociolinguist and philosopher, is \"a variety or lect which is thought of as being related to its speakers' social background rather than geographical background\". This idea of sociolect began with the commencement of dialectology, the study of different dialects in relation to social society, which has been established in countries such as England for many years, but only recently has the field garnered more attention. However, as opposed to a dialect, the basic concept of a sociolect is that a person speaks in accordance with their social group whether it is with regard to one's ethnicity, age, gender, etc. As William Labov once said, \"the sociolinguistic view…is that we are programmed to learn to speak in ways that fit the general pattern of our communities\". Therefore, what we are surrounded with in unison with our environment determines how we speak; hence, our actions and associations.\n\nThe main distinction between a sociolect and a dialect, which are continually confused, is the settings in which it is created. A dialect's main identifier is geography: a certain region uses specific phonological, morphosyntactic or lexical rules. Asif Agha expands the concept by stating that \"the case where the demographic dimension marked by speech are matters of geographic provenance along, such as speaker's birth locale, extended residence and the like\". However, a sociolect's main identifier is a socioeconomic class, age, gender, and ethnicity in a certain speech community.\n\nAn example of a dialectal difference, based on region, is the use of the words soda or pop and coke in different parts of the United States. As Thomas E. Murray states, \"coke is used generically by thousands of people, especially in the southern half of the country.\" On the other hand, pop is known to be a term that is used by many citizens in the northern half of the country.\n\nAn example of a sociolect difference, based on social grouping, is the zero copula in African American Vernacular English. It occurs in a specific ethnic group but in all areas of the United States. William Labov gives an example: \"he here\" instead of \"he's here\".\n\nCode switching is \"the process whereby bilingual or bidialectal speakers switch back and forth between one language or dialect and another within the same conversation\".\n\nDiglossia, associated with the American linguist Charles A. Ferguson, which describes a sociolinguistic situation such as those that obtain in Arabic-speaking countries and in German-speaking Switzerland. In such a diglossic community, the prestigious standard of 'High'(or H) variety, which is linguistically related to but significantly different from the vernacular or 'Low' (or L) varieties, has no native speakers.\n\nDomain is \"different language, dialects, or styles are used in different social contexts\".\n\nLanguage attitudes are \"social in origin, but that they may have important effects on language behavior, being involved in acts of identity, and on linguistic change.\"\n\nLinguistic variable is \"a linguistic unit…initially developed...in order to be able to handle linguistics variation. Variables may be lexical and grammatical, but are most often phonological\". Example of British English (h) which is sometimes present and sometimes not.\n\nPragmatics is the meaning of a word in social context, while semantics has \"purely linguistic meaning\".\n\nRegister is \"a language variety that is associated with a particular topic, subject, or activity...\" Usually, it is defined by vocabulary, but has grammatical features as well.\n\nExample 1\n\nThe following is an example of the lexical distinction between the Mudaliyar and the Iyengar groups of the Tamil-speaking caste in India. The Iyengar group is part of the Brahmin caste which is scholarly and higher in the caste hierarchy than the non-Brahmin or Mudaliyar, caste. The Mudaliyars use many of the same words for things that are differentiated within the Iyengars' speech. For example, as you can see below, the difference between drinking water, water in general, and non-potable water is used by one word in the non-Brahmin caste and three separate words in the Brahmin caste. Furthermore, Agha references how the use of different speech reflects a \"departure from a group-internal norm\". For example, if the non-Brahmin caste uses Brahmin terms in their mode of speech it is seen as self-raising, whereas if people within the Brahmin caste use non-Brahmin speech it is seen as pejoratives. Therefore, depending on which castes use certain words the pragmatics change. Hence, this speech system is determined by socioeconomic class and social context.\n\nExample 2\n\nIn the following example, we see the difference between the national standard and the colloquial speech found in Norway where the phonology and pronunciation differ. As Agha states, \"Some lexical contrasts are due to the phonological difference (e.g., R makes more consonantal and vocalic distinctions than B), while others are due to the morphological difference (e.g., difference in plural suffixes and certain verb inflections) between two varieties.\n\nExample 3\n\nThe chart below gives an example of diglossia in Arab-speaking nations and where it is used. Diglossia is defined by Mesthrie as \"[a] situation where two varieties of a language exist side by side\". The Classical Arabic is known as الفصحى, or al-fuṣḥā, while the colloquial dialect depends on the country. For example, شامي, or šāmi, is spoken in Lebanon and parts of Syria. In many situations, there is a major lexical difference among words in the classical and colloquial speech, as well as pronunciation differences, such as a difference in short vowels, when the words are the same. Although a specific example of diglossia was not given, its social context is almost if not more important. For example, Halliday tells us that \"in areas with Diglossia, the link between language and success is apparent as the higher, classical register is learned through formal education\".\n\nExample 4\n\nBelow is an example of the addition of the verbal -s not just on 3rd person singular verbs in the present tense like in SAE, but added onto infinitives, first-person present verbs, and 3rd person past perfect verbs.\n\n\nFurther examples of the phenomenon in AAVE are provided below.\n\nBelow are examples of the lack of the possessive ending -s is usually absent in AAVE but contains a rule\nAs Labov shows states, \"[the] use -s to indicate possession by a single noun or pronoun, but never between the possessor and the possessed.\"\n\n\"This is hers, This is mines, This is John's, but not in her book, my book, John book\"\n\n\"Interview with Bryan A., seven years old, a struggling reader in a West Philadelphia elementary school: \n\nMany times within communities that contain sociolects that separate groups linguistically it is necessary to have a process where the independent speech communities can communicate in the same register; even if the change is as simple as different pronunciation. Therefore, the act of codeswitching becomes essential. Codeswitching is defined as \"the process whereby bilingual or bidialectal speakers switch back and forth between one language or dialect and another within the same conversation\". At times codeswitching can be situational, depending on the situation or topical, depending on the topic. Halliday terms this the best when he defines the role of discourse stating that \"it is this that determines, or rather correlates with, the role played by the language activity in the situation\". Therefore, meaning that which register is used depends on the situation and lays out the social context of the situation, because if the wrong register is used, then the wrong context is placed on the words. Furthermore, referring back to the diglossia expressed in the Arab-speaking world and the Tamil caste system in India, which words are used must be appropriate to not only the social class of the speaker, but the situation, the topic, and the need for courtesy. A more comprehensive definition is stated, \"Code-switching is not only a definition of the situation but an expression of social hierarchy.\"\n\nFor examples of the use of speech within certain situation refer back to the chart on Classical and Colloquial Arabic.\n\nFor examples of dialect selection based on topic, refer below:\n\nWhen Albania was created in 1912, the educational rights of the Greek communities in Albanian territory were granted by the Protocol of Corfu (1914) and with the statement of Albania's representatives in the League of Nations (1921). However, under a policy of assimilation, the Greek schools (there were over 360 until 1913) were gradually forced to close and Greek education was virtually eliminated by 1934. Following the intervention by the League of Nations, a limited number of schools, only those inside the \"official minority zones\", were reopened. \nEthnic Greeks living outside those areas were not counted as such. This has had a practical effect in the area of education: With the exception of the officially recognized Greek minority zones, where teaching was held in both the Greek and Albanian languages, in all other areas of Albania lessons were taught only in the Albanian language.If a few Albanian families moved into a town or village, the minority's right to be educated in Greek and publish in Greek newspapers was revoked.In accordance with the communist Albanian policy of unification and homogenization, the use of the Greek language in Himarë was forbidden in public, and many Greek-speaking people were forced to move to places in northern or central Albania.As a consequence, Greek schools in the Himarë area were closed, and the local communities stuck to their language, which slowly became archaic when they started to emigrate to Greece (1991) in the aftermath of the communist regime's collapse.\n\nAs Trudgill defines it, the Arvanikita is \"the name given in Greece given to the language of the indigenous Albanian-speaking linguistic minority in that country\". This community is different linguistically than the surrounding area and must use their language accordingly. For example, nowadays, Arvanitika is only used at home and other situations, such as in school during games, on the playground, or for \"chatting up girls\", while only Greek is spoken in class. \n\nTherefore, it is both topical and situational in context.\n\nHuman rights in Albania are violated by the Government which have targeted the Greek-speaking population via police and secret service according to Human Rights organisations.According to Amnesty International there were cases of mistreatment of members of the Greek-speaking minority by the authorities.Also, the Greek-speaking minority complained about the government’s unwillingness to recognize Greek-speaking towns outside communist-era “minority zones,” to utilize Greek in official documents and on public signs in Greek-speaking areas, or to include more ethnic Greeks in public administration.\nAlbanian sources often use the pejorative term 'filogrek' (pro-Greek) in relation to Greeks-speaking minority groups, usually in a context disputing their Greek ancestry.The 2012 USA annual report mention that the emergence of strident nationalist groups like the Red and Black Alliance (RBA) increased ethnic tensions with the Greek-speaking minority groups.\n\nThe Arvanitika community also suffers from discrimination because they are cast under stereotypes by the use of their native language. As Garrett writes, \"a number of Arvanites had suffered from what they regarded as discrimination, particularly during military service, and at school\". Even though, the language is the only thing that differentiates them from the surrounding Greeks, it still defines them as a distinct class and places them within a social hierarchy. \n\nFurthermore, within societies that maintain a diglossic state, the High ('H') and Low ('L') forms serve as a basis for discrimination. As Mesthrie writes, \"Since the H form is learned via formal education, diglossia can be a means of excluding people from access to full participation in society\".\n"}
{"id": "3037804", "url": "https://en.wikipedia.org/wiki?curid=3037804", "title": "Sociotropy", "text": "Sociotropy\n\nSociotropy is a personality trait characterized by excessive investment in interpersonal relationships and usually studied in the field of social psychology.\n\nPeople with sociotropy tend to have a strong need for social acceptance, which causes them to be overly nurturant towards people who they do not have close relationships with. Sociotropy can be seen as the opposite of autonomy, because those with sociotropy are concerned with interpersonal relationships, whereas those with autonomy are more concerned with independence and do not care so much for others. Sociotropy has been correlated with feminine sex-role orientation in many research experiments.\n\nSociotropy is notable in that it interacts with interpersonal stress or traumatic experience to influence subsequent depression.\n\nThe Sociotropy-Autonomy Scale (SAS) was introduced by Aaron T. Beck as a means of assessing two cognitive-personality constructs hypothesized as risk factors in depression. The scale focuses on the two personality traits of Sociotropy (social dependency) and Autonomy (satisfying independency). The development of the SAS was gathered through patient self-reports and patient records collected from therapists. Using psychometrics, from the sample of 378 psychiatric patients questions were placed into a two-factor structure where the final pool of items was 60-109. From there each 30 items generated three factors for sociotropy: Concern About Disapproval, Attachment/Cocern About Separation, and Pleasing Others; and three for autonomy: Individualistic or Autonomous Achievement, Mobility/Freedom from Control of Others, and Preference for Solitude. The SAS has 60 items rated on a 5 point scale (ranging from 0 to 4). Scores are then totaled separately on each dimension. The scale has been modified since its development. The current SAS decomposes Sociotropy into two factors (\"neediness\" and \"connectedness\"). Neediness is associated with the symptoms of depression—and connectedness is a sensitivity towards others, and associated with valuing relationships.\n\nSince the development of the SAS, many other measures of personality constructs have been developed to assess other personality traits with some overlapping with the SAS, but examining for different traits.\n\nSociotropic individuals react differently when faced with situations that involve self-control. Sociotropic individuals consume more food, or try to match a peer's eating habits when they believe doing so makes the peer more comfortable. This is often hypothesized as being a result of the individual attempting to achieve social approval and avoid social rejection. The social pressure and dependence can cause a loss of self-control in an individual, especially if they are unaware of their desire for social acceptance.\n\nMuch research on Sociotropy focuses on links between personality and the risk for depression. People who are very dependent are classified as sociotropic individuals, and are more prone to depression as they seek to sustain their low self-esteem by establishing secure interpersonal relationships. Sociotropic individuals are heavily invested into their relationships with other people and have higher desires for acceptance, support, understanding, and guidance—which is problematic when relationships fail. People who are sociotropic and going through failed relationships are likely to become depressed due to intensified feelings of abandonment and loss. Researchers have a hard time figuring out exactly how much personality affects risk for depression, as it is hard to isolate traits for research, though they conclude that a person can either be sociotropic or independent, but not both.\n\nSociotropy has been linked to other personality traits such as introversion and lack of assertion. Lack of assertion has been hypothesized to be due to the need to please others to build interpersonal relationships. Individuals who are sociotropic avoid confrontation to prevent abandonment.\n\nAlong the lines of lack of assertion there has also been research studying the connection between sociotropy and shyness. The characteristic interpersonal dependence and fear of social rejection are also attributes of shyness. Research shows that many items from the SAS relate to dimensions of dependence and preoccupations for receiving approval of others, which is problematic in interpersonal relationships for people who are shy. Individuals who are shy and sociotropic have internal conflicts to want to avoid others as well as having strong motives to approach people. The results from such research concludes that sociotropy predicts other symptoms of discomfort in assertive situations and in conversations.\n\nResearch on the subject also seems to connect a link between higher levels of anxiety and sociotropy. Putting excessive amounts of energy into dependent relationships increases anxiety. The behavioral disposition that causes an individual to depend on others for personal satisfaction can also have an effect on their anxiety levels. The research concluded that anxiety and sociotropy are positively correlated in many situations such as Social Evaluation, Physical Danger, and Ambiguous Situations. Sociotropy and anxiety are present in these situations because they are social by definition, and therefore associated with emphasis on social relationships that are characteristic of sociotropic individuals.\n\n"}
{"id": "35356715", "url": "https://en.wikipedia.org/wiki?curid=35356715", "title": "Top-lit updraft gasifier", "text": "Top-lit updraft gasifier\n\nA top-lit updraft gasifier (also known as a TLUD) is a micro-kiln used to produce charcoal, especially biochar, and heat for cooking. A TLUD pyrolyzes organic material, including wood or manure, and uses a reburner to eliminate volatile byproducts of pyrolization. The process leaves mostly carbon as a residue, which can be incorporated into soil to create terra preta.\n\nDr Thomas B Reed and the Norwegian architect Paal Wendelbo independently developed the working idea of a TLUD gasifier in the 1990s.\n\nA TLUD gasifier takes it further from a rocket stove in more efficient way of smoke-free, highly efficient combustion of the fuel.\n\nA TLUD gasifier stove is commonly constructed with two concentric cylindrical containers.\n\nThe inner cylinder is the fuel pot. The fuel pot has holes in the base. These holes are the primary air inlet. The fuel pot also has holes on the neck, like the skirt, serving as a secondary air inlet.\n\nThe outer cylinder has holes near the bottom on the sides. During combustion, air enters these holes, either by natural air draft or forced with a DC fan depending on requirement and construction model.\n\nAny biomass with less than 20% water content can be used as fuel. The user fills the fuel pot up to the neck, just below the secondary air inlet holes. The user ignites the top layer of fuel for the pyrolysis to start. Air then flows in through the primary and secondary air inlets. The primary inlet helps the draft of pyrolysed wood gas flow up.\n\nThe secondary air inlet blows hot air by the time it travels around the fuel pot. The secondary inlet above the fuel layer helps burn the wood gas. \n\nYouTube has many instructional videos, with further explanation on other websites.\nA range of construction plans to make TLUD gasifier stoves\n\n\n"}
{"id": "92171", "url": "https://en.wikipedia.org/wiki?curid=92171", "title": "Webcam", "text": "Webcam\n\nA webcam is a video camera that feeds or streams its image in real time to or through a computer to a computer network. When \"captured\" by the computer, the video stream may be saved, viewed or sent on to other networks travelling through systems such as the internet, and e-mailed as an attachment. When sent to a remote location, the video stream may be saved, viewed or on sent there. Unlike an IP camera (which connects using Ethernet or Wi-Fi), a webcam is generally connected by a USB cable, or similar cable, or built into computer hardware, such as laptops.\n\nThe term \"webcam\" (a clipped compound) may also be used in its original sense of a video camera connected to the Web continuously for an indefinite time, rather than for a particular session, generally supplying a view for anyone who visits its web page over the Internet. Some of them, for example, those used as online traffic cameras, are expensive, rugged professional video cameras.\n\nWebcams are known for their low manufacturing cost and their high flexibility, making them the lowest-cost form of videotelephony. Despite the low cost, the resolution offered at present (2015) is rather impressive, with low-end webcams offering resolutions of 320×240, medium webcams offering 640×480 resolution, and high-end webcams offering 1280×720 (aka 720p) or even 1920×1080 (aka 1080p) resolution.\n\nThey have also become a source of security and privacy issues, as some built-in webcams can be remotely activated by spyware.\n\nThe most popular use of webcams is the establishment of video links, permitting computers to act as videophones or videoconference stations. Other popular uses include security surveillance, computer vision, video broadcasting, and for recording social videos.\n\nThe video streams provided by webcams can be used for a number of purposes, each using appropriate software:\n\nMost modern webcams are capable of capturing arterial pulse rate by the use of a simple algorithmic trick. Researchers claim that this method is accurate to ±5 bpm.\n\nWebcams may be installed at places such as childcare centres, offices, shops and private areas to monitor security and general activity.\n\nWebcams have been used for augmented reality experiences online. One such function has the webcam act as a \"magic mirror\" to allow an online shopper to view a virtual item on themselves. The Webcam Social Shopper is one example of software that utilizes the webcam in this manner.\n\nWebcam can be added to instant messaging, text chat services such as AOL Instant Messenger, and VoIP services such as Skype, one-to-one live video communication over the Internet has now reached millions of mainstream PC users worldwide. Improved video quality has helped webcams encroach on traditional video conferencing systems. New features such as automatic lighting controls, real-time enhancements (retouching, wrinkle smoothing and vertical stretch), automatic face tracking and autofocus, assist users by providing substantial ease-of-use, further increasing the popularity of webcams.\n\nWebcam features and performance can vary by program, computer operating system, and also by the computer's processor capabilities. Video calling support has also been added to several popular instant messaging programs.\n\nWebcams can be used as security cameras. Software is available to allow PC-connected cameras to watch for movement and sound, recording both when they are detected. These recordings can then be saved to the computer, e-mailed, or uploaded to the Internet. In one well-publicised case, a computer e-mailed images of the burglar during the theft of the computer, enabling the owner to give police a clear picture of the burglar's face even after the computer had been stolen.\n\nUnauthorized access of webcams can present significant privacy issues (see \"Privacy\" section below).\n\nIn December 2011, Russia announced that 290,000 Webcams would be installed in 90,000 polling stations to monitor the Russian presidential election, 2012.\n\nWebcams can be used to take video clips and still pictures. Various software tools in wide use can be employed for this, such as PicMaster (for use with Windows operating systems), Photo Booth (Mac), or Cheese (with Unix systems). For a more complete list see Comparison of webcam software.\n\nSpecial software can use the video stream from a webcam to assist or enhance a user's control of applications and games. Video features, including faces, shapes, models and colors can be observed and tracked to produce a corresponding form of control. For example, the position of a single light source can be tracked and used to emulate a mouse pointer, a head-mounted light would enable hands-free computing and would greatly improve computer accessibility. This can be applied to games, providing additional control, improved interactivity and immersiveness.\n\nFreeTrack is a free webcam motion-tracking application for Microsoft Windows that can track a special head-mounted model in up to six degrees of freedom and output data to mouse, keyboard, joystick and FreeTrack-supported games. By removing the IR filter of the webcam, IR LEDs can be used, which has the advantage of being invisible to the naked eye, removing a distraction from the user. TrackIR is a commercial version of this technology.\n\nThe EyeToy for the PlayStation 2, PlayStation Eye for the PlayStation 3, and the Xbox Live Vision camera and Kinect motion sensor for the Xbox 360 and are color digital cameras that have been used as control input devices by some games.\n\nSmall webcam-based PC games are available as either standalone executables or inside web browser windows using Adobe Flash.\n\nWith very-low-light capability, a few specific models of webcams are very popular to photograph the night sky by astronomers and astro photographers. Mostly, these are manual-focus cameras and contain an old CCD array instead of comparatively newer CMOS array. The lenses of the cameras are removed and then these are attached to telescopes to record images, video, still, or both. In newer techniques, videos of very faint objects are taken for a couple of seconds and then all the frames of the video are \"stacked\" together to obtain a still image of respectable contrast.\n\nA webcam's CCD response is linear proportional to the incoming light. Therefore, webcams are suitable to record laser beam profiles, after the lens is removed. The resolution of a laser beam profiler depends on the pixel size. Commercial webcams are usually designed to record color images. The size of a webcam's color pixel depends on the model and may lie in the range of 5 to 10µm. However, a color pixel consists of four black and white pixels each equipped with a color filter (for details see Bayer filter). Although these color filters work well in the visible, they may be rather transparent in the near infra-red. By switching a webcam into the Bayer-mode it is possible to access the information of the single pixels and a resolution below 3µm was possible. \n\nFirst developed in 1991, a webcam was pointed at the Trojan Room coffee pot in the Cambridge University Computer Science Department (initially operating over a local network instead of the web). The camera was finally switched off on August 22, 2001. The final image captured by the camera can still be viewed at its homepage. In 2004, the oldest webcam still operating was FogCam at San Francisco State University, which had been running continuously since 1994.\n\nThe first commercial webcam, the black-and-white QuickCam, entered the marketplace in 1994, created by the U.S. computer company Connectix (which sold its product line to Logitech in 1998). QuickCam was available in August 1994 for the Apple Macintosh, connecting via a serial port, at a cost of $100. Jon Garber, the designer of the device, had wanted to call it the \"Mac-camera\", but was overruled by Connectix's marketing department; a version with a PC-compatible parallel port and software for Microsoft Windows was launched in October 1995. The original QuickCam provided 320x240-pixel resolution with a grayscale depth of 16 shades at 60 frames per second, or 256 shades at 15 frames per second. These cam were tested on several Delta II launch using a variety of communication protocols including CDMA, TDMA, GSM and HF.\n\nIn 2010, Time Magazine named the QuickCam as one of the top computer devices of all time.\n\nVideoconferencing via computers already existed, and at the time client-server based videoconferencing software such as CU-SeeMe had started to become popular.\n\nOne of the most widely reported-on webcam sites was JenniCam, created in 1996, which allowed Internet users to observe the life of its namesake constantly, in the same vein as the reality TV series \"Big Brother\", launched four years later. Other cameras are mounted overlooking bridges, public squares, and other public places, their output made available on a public web page in accordance with the original concept of a \"webcam\". Aggregator websites have also been created, providing thousands of live video streams or up-to-date still pictures, allowing users to find live video streams based on location or other criteria.\n\nAround the turn of the 21st century, computer hardware manufacturers began building webcams directly into laptop and desktop screens, thus eliminating the need to use an external USB or FireWire camera. Gradually webcams came to be used more for telecommunications, or videotelephony, between two people, or among several people, than for offering a view on a Web page to an unknown public.\n\nFor less than US$100 in 2012, a three-dimensional space webcam became available, producing videos and photos in 3D anaglyph image with a resolution up to 1280 × 480 pixels. Both sender and receiver of the images must use 3D glasses to see the effect of three dimensional image.\n\nWebcams typically include a lens, an image sensor, support electronics, and may also include a microphone for sound. Various lenses are available, the most common in consumer-grade webcams being a plastic lens that can be screwed in and out to focus the camera. Fixed-focus lenses, which have no provision for adjustment, are also available. As a camera system's depth of field is greater for small image formats and is greater for lenses with a large f-number (small aperture), the systems used in webcams have a sufficiently large depth of field that the use of a fixed-focus lens does not impact image sharpness to a great extent.\n\nImage sensors can be CMOS or CCD, the former being dominant for low-cost cameras, but CCD cameras do not necessarily outperform CMOS-based cameras in the low-price range. Most consumer webcams are capable of providing VGA-resolution video at a frame rate of 30 frames per second. Many newer devices can produce video in multi-megapixel resolutions, and a few can run at high frame rates such as the PlayStation Eye, which can produce 320×240 video at 120 frames per second.\n\nSupport electronics read the image from the sensor and transmit it to the host computer. The camera pictured to the right, for example, uses a Sonix SN9C101 to transmit its image over USB. Typically, each frame is transmitted uncompressed in RGB or YUV or compressed as JPEG. Some cameras, such as mobile-phone cameras, use a CMOS sensor with supporting electronics \"on die\", i.e. the sensor and the support electronics are built on a single silicon chip to save space and manufacturing costs. Most webcams feature built-in microphones to make video calling and videoconferencing more convenient.\n\nThe USB video device class (UVC) specification allows interconnectivity of webcams to computers without the need for proprietary device drivers. Microsoft Windows XP SP2, Linux and Mac OS X (since October 2005) have UVC support built in and do not require extra device drivers, although they are often installed to add additional features.\n\nMany users do not wish the continuous exposure for which webcams were originally intended, but rather prefer privacy. Such privacy is lost when malware allow malicious hackers to activate the webcam without the user's knowledge, providing the hackers with a live video and audio feed. This is a particular concern on many laptop computers, as such cameras normally cannot be physically disabled if hijacked by such a Trojan Horse program or other similar spyware programs.\n\nCameras such as Apple's older external iSight cameras include lens covers to thwart this. Some webcams have built-in hardwired LED indicators that light up whenever the camera is active, sometimes only in video mode. However, it is possible for malware to circumvent the indicator and activate the camera surrepticiously, as researchers demonstrated in the case of a MacBook's built-in camera in 2013.\n\nVarious companies sell sliding lens covers and stickers that allow users to retrofit a computer or smartphone to close access to the camera lens as needed. One such company reported having sold more than 250,000 such items from 2013 to 2016. However, any opaque material will work. Prominent users include former FBI director James Comey.\n\nThe fraudulent process of attempting to hack into a person's webcam and activate it without the webcam owner's permission has been called camfecting. The remotely activated webcam can be used to watch anything within the webcam's field of vision, sometimes the webcam owner itself. Camfecting is most often carried out by infecting the victim's computer with a virus that can provide the hacker access to the victim's webcam. This attack is specifically targeted at the victim's webcam, and hence the name \"camfecting\", a portmanteau of the words \"cam\" and \"infecting\". \n\nIn January 2005, some search engine queries were published in an online forum which allow anyone to find thousands of Panasonic- and Axis high-end web cameras, provided that they have a web-based interface for remote viewing. Many such cameras are running on default configuration, which does not require any password login or IP address verification, making them viewable by anyone.\n\nIn the 2010 \"Robbins v. Lower Merion School District\" \"WebcamGate\" case, plaintiffs charged that two suburban Philadelphia high schools secretly spied on students - by surreptitiously remotely activating iSight webcams embedded in school-issued MacBook laptops the students were using at home — and thereby infringed on their privacy rights. School authorities admitted to secretly snapping over 66,000 photographs, including shots of students in the privacy of their bedrooms, including some with teenagers in various state of undress. The school board involved quickly disabled their laptop spyware program after parents filed lawsuits against the board and various individuals.\n\nWebcams allow for inexpensive, real-time video chat and webcasting, in both amateur and professional pursuits. They are frequently used in online dating and for online personal services offered mainly by women when camgirling. However, the ease of webcam use through the Internet for video chat has also caused issues. For example, moderation system of various video chat websites such as Omegle has been criticized as being ineffective, with sexual content still rampant. In a 2013 case, the transmission of nude photos and videos via Omegle from a teenage girl to a schoolteacher resulted in a child pornography charge. \n\nYouTube is a popular website hosting many videos made using webcams. News websites such as the BBC also produce professional live news videos using webcams rather than traditional cameras.\n\nWebcams can also encourage telecommuting, enabling people to work from home via the Internet, rather than traveling to their office.\n\nThe popularity of webcams among teenagers with Internet access has raised concern about the use of webcams for cyber-bullying. Webcam recordings of teenagers, including underage teenagers, are frequently posted on popular Web forums and imageboards such as 4chan.\n\n\"Videophone calls\" (also: \"videocalls\" and \"video chat\"), differ from videoconferencing in that they expect to serve individuals, not groups. However that distinction has become increasingly blurred with technology improvements such as increased bandwidth and sophisticated software clients that can allow for multiple parties on a call. In general everyday usage the term \"videoconferencing\" is now frequently used instead of \"videocall\" for point-to-point calls between two units. Both videophone calls and videoconferencing are also now commonly referred to as a \"video link\".\n\nWebcams are popular, relatively low cost devices which can provide live video and audio streams via personal computers, and can be used with many software clients for both video calls and videoconferencing.\n\nA videoconference system is generally higher cost than a videophone and deploys greater capabilities. A \"videoconference\" (also known as a \"videoteleconference\") allows two or more locations to communicate via live, simultaneous two-way video and audio transmissions. This is often accomplished by the use of a multipoint control unit (a centralized distribution and call management system) or by a similar non-centralized multipoint capability embedded in each videoconferencing unit. Again, technology improvements have circumvented traditional definitions by allowing multiple party videoconferencing via web-based applications. \nA separate webpage article is devoted to videoconferencing.\n\nA telepresence system is a high-end videoconferencing system and service usually employed by enterprise-level corporate offices. Telepresence conference rooms use state-of-the art room designs, video cameras, displays, sound-systems and processors, coupled with high-to-very-high capacity bandwidth transmissions.\n\nTypical use of the various technologies described above include calling or conferencing on a one-on-one, one-to-many or many-to-many basis for personal, business, educational, deaf Video Relay Service and tele-medical, diagnostic and rehabilitative use or services. New services utilizing videocalling and videoconferencing, such as teachers and psychologists conducting online sessions, personal videocalls to inmates incarcerated in penitentiaries, and videoconferencing to resolve airline engineering issues at maintenance facilities, are being created or evolving on an ongoing basis.\n\n\n\n"}
