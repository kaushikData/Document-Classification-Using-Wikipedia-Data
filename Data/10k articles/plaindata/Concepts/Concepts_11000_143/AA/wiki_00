{"id": "54747586", "url": "https://en.wikipedia.org/wiki?curid=54747586", "title": "Abuse during childbirth", "text": "Abuse during childbirth\n\nAbuse during childbirth (or obstetric violence) is the neglect, physical abuse and lack of respect during childbirth. This treatment is regarded as a violation of the woman's rights. It also has the affect of preventing women from seeking pre-natal care and using other health care services. Abuse during childbirth is one form of violence against women.\n\nInvestigations into the prevalence of these practices have been conducted by the World Health Organization. Their studies demonstrate that this is a global problem. Women experience disrespectful, abusive or neglectful treatment during their childbirth when the birth occurs in medical and health facilities. The abusive relationship and trust between women and health providers can create a great reluctance to obtain medical assistance during birth. Disrespectful and abusive treatment can be experienced during pregnancy. During childbirth, a woman is very vulnerable and cannot protect herself. The results of this abuse can have very negative consequences for the infant and the mother.\n\nWHO has found situations and circumstances where health facilities have participated in physical abuse, the withholding of pain medication, disrespect, humiliation, lack of confidentiality, lack of privacy, lack of informed consent, forced sterilization, refusing to be admitted to the facility, neglect during the birth, involuntary medical procedures, lack of confidentiality, withholding medical information and detention of the women at the facility because she is unable to pay the facility. In addition, the neglect during childbirth has resulted in life-threatening complications that could have responded to appropriate medical intervention.\n\nSome women are more likely to experience abuse during childbirth. Adolescents, migrant women, women infected with HIV, and ethnic minority women are more likely than others to receive abuse.\n\nThe term \"obstetric violence\" is particularly used in Latin American countries, where the law prohibits such behavior. Such laws exist in several countries, including Argentina, Puerto Rico and Venezuela.\n\nSome sources refer to North American obstetricians and gynecologists, especially between the 1950s and 1980s, practicing what was called \"the husband's stitch\": placing extra stitches in the woman's vagina after the episiotomy or natural tearing, supposedly to increase the husband's future sexual pleasure and often causing long-term pain and discomfort to the woman. However, there is no proof that such a practice was widespread in North America, but mentions of it frequently appear in studies about episiotomy, also in other American countries such as Brazil.\n"}
{"id": "36175304", "url": "https://en.wikipedia.org/wiki?curid=36175304", "title": "Actuarial Society of South Africa HIV/AIDS models", "text": "Actuarial Society of South Africa HIV/AIDS models\n\nThe Actuarial Society of South Africa HIV/AIDS models, also known as ASSA AIDS models, are a series of mathematical models developed to assist the actuarial profession and the Actuarial Society of South Africa in assessing and addressing the impact of the HIV and AIDS epidemic in South Africa. The models have been developed by the AIDS Committee of the Society and the Center for Actuarial Research (CARe) at the University of Cape Town in Cape Town, South Africa.\n\nIn 1996 the first and earliest version of ASSA AIDS model ASSA500 was developed by the AIDS Committee of the society, While the ASSA2008 model is the most recent version of ASSA AIDS and demographic model which was released in March 2011. The models have assisted insurers and those insured to have fair and balanced insurance policies in a country ravaged by AIDS.\n\nA joint task team of the South African National Department of Health and the Treasury used the ASSA model to estimate the impact of extending the life of AIDS patients.\n\nASSA is one of only two models used in South Africa; both are capable of making projections on the progression of the epidemic through an interaction of many factors. Four population risk groups are included, along with assumptions about sexual behaviour, rates of infection, conception rates, the median duration to mortality of those with HIV/AIDS, and the rates of transmission between mother and child. The impact of drug treatments at birth and while nursing, enhanced STD treatment, and risk avoidance is projected by a lower rate scenario The model makes provisions for migration of the population, separate modeling of racial groups, and separate modeling of provincial populations.\n\nA study by the South African National Department of Health and the Treasury used the ASSA model to project a positive result for the increased implementation of antiretroviral drugs (ARV) in the country, although their cost is a problem.\n\n\n"}
{"id": "49658385", "url": "https://en.wikipedia.org/wiki?curid=49658385", "title": "Andreas Heusser", "text": "Andreas Heusser\n\nAndreas Heusser (born 1976) is a Swiss conceptual artist and curator, based in Zurich and Johannesburg.\n\nAndreas Heusser's artistic approach can be described as creating long-term fictional realities made from contemporary real-world patterns. The works react upon and interact with a specific cultural or political situation while being created within that context, with that particular audience, and within the social space of everyday life. Andreas Heusser uses various media and skills in order to accomplish the aimed – often political – effects. The works can be enacted at any site – in galleries and museums, on the internet, on the street, or even in remote areas.\n\nSince 2013, Andreas Heusser has devoted himself to the exploration of nothing in art, philosophy and science. This has developed into the No Show Museum, a museum dedicated to nothing and its various manifestations throughout the history of art. The museum's collection includes around 500 works and documents from over 150 international artists of the 20th and 21st centuries. The museum also has a mobile presentation space in a converted postal car. In 2015, the No Show Museum started a world tour with the mission to spread nothing: The first stage led from July to October 2015 through Europe, staging around 30 exhibitions in 20 different countries, before the museum was arriving in Italy and participating at the 56th Venice Art Biennale.\nIn the summer of 2016, the mobile museum was shipped from Europe to America: The 80-day exhibition tour led from New York to Canada, then to the West Coast of the U.S., and finally down to Baja California Sur, Mexico. The third stage of the world tour took place from November 2017 to January 2018 and led from Mexico through the countries of Central America to Colombia.\n\nThe museum reproduces the typical structures, mechanisms, rituals and strategies of established art institutions. As a result, the project becomes a model in which we can observe the contextual conditions that are required for the recognition of something (or nothing) as art. Implicit in the project, then, is the question: What is needed for a successful promotion and marketing of art?\n\nBetween 2009 and 2011, Andreas Heusser produced a series of large scale projects that bridge the gap between art and activism. Designed to reach not only the art scene, but to achieve a political effect on the general public, those projects often took place outside of art institutions, and involved public interventions, websites, social media, press releases, and other forms of dissemination. Using methods like parody, satire, interventions, and tactical obfuscation, he created fake organizations and corporations (including counterfeiting sites, media hoaxes and artificial characters) through imitating real corporations and political organizations. Tactics like subversive affirmation and overidentification were used to create controversy and generate media coverage in dominant media outlets. Despite the satiric content, those fake identities were often mistaken for the real thing and their critiques were not readily intelligible to audiences.\nFor example, in cooperation with Nüssli / Oeschger, he launched a counter-propaganda in order to subvert a xenophobic campaign of the swiss right wing party (SVP) by creating a twin-organization that was even more extreme in its claims – to a degree that it became ridiculous (OLAF, 2010). Although the media coverage in the Swiss and international press was considerable, it did not put the Swiss electorate off from accepting the deportation initiative.\nAnother example is the CHASOS Campaign, launched in 2011 as a satirical reaction on how media and politicians portrayed negative images and reinforces prejudices and fears by evoking the dystopia of gigantic waves of refugees which are alleged to flood Switzerland after the Arab Spring.\n\nAndreas Heusser is the founder and director of the Openair Literatur Festival Zürich, an international literary festival which annually takes place for the duration of a week in the Old Botanical Garden in Zurich, since 2011. The festival is jointly presented by Kaufleuten and Literaturhaus Zurich, in cooperation with other cultural institutions. The festival has featured artists of international renown, such as John M. Coetzee, Junot Diaz, David Mitchell (author), John Cleese, Sven Regener, Teju Cole, Xiaolu Guo, Gerhard Rühm and Friedrich Achleitner, Roger Willemsen, Gerhard Polt.\n\nAndreas Heusser is a founding member and Co-Director of The Institute, a project space for performing and transdisciplinary arts in Zurich. Several artists collectives from different artistic fields (film, music, dance, literature, performance) are involved and contribute with weekly events and performances to the public programme of the venue which also includes research and workshops.\n\nBetween 2011 and 2013, Andreas Heusser was the Program Director of the cultural venue \"Kaufleuten\" where he was responsible for the curation and implementation of around 200 cultural events per year, including concerts by international artists and bands, readings, podiums and cabaret events.\n\nIn 2001, Andreas Heusser founded together with Marc Rychener the interdisciplinary artist collective based in Zurich. With index he organized a number of concert series, performances and festivals (e.g. \"Festival der Künste\" 2002, 2002–2005). In 2005, he initiated the international Artist-in-Residence-program Freiraum-Stipendium which is open to artists from all disciplines. In 2005, Andreas Heusser converted his artist studio at Elisabethenstrasse 14a into a cultural venue, where he staged 3–4 events per month until 2011, including a program cinema (\"Kino im Atelier\") and the concert and performance series \"Mikro\".\n\nAfter completing the intermediate diploma in Psychology (1999–2001), Andreas Heusser studied Philosophy and German Literature at the University of Zurich, closing with a master degree in both subjects (2003). Between 2011 and 2013 he attended the Bern University of Arts and completed another master degree in Contemporary Arts Practice (Fine Arts).\n\n\n\n\n"}
{"id": "12812413", "url": "https://en.wikipedia.org/wiki?curid=12812413", "title": "Anti-Jewish legislation in prewar Nazi Germany", "text": "Anti-Jewish legislation in prewar Nazi Germany\n\nAnti-Jewish legislation in pre-war Nazi Germany comprised several laws that segregated the Jews from German society and restricted Jewish people's political, legal and civil rights. Major legislative initiatives included a series of restrictive laws passed in 1933, the Nuremberg Laws of 1935, and a final wave of legislation preceding Germany's entry into World War II.\n\nThe Enabling Act established the power of the Nazi Party to pass law by decree, bypassing the approval of parliament. It was passed on March 24, 1933, and effectively nullified the Weimar Constitution.\n\nIn April 1933, the Law for the Restoration of the Professional Civil Service, or 'Civil Service Law', as it was more commonly known when passed, established the ability of the Nazi Party to legally remove undesirables from the civil service profession, including doctors, teachers and lawyers.\n\nMany local governments also did not allow for the Jews to slaughter animals. In turn, this prevented the Jews from obeying Jewish dietary laws.\n\nThis Law created the basis for the years to come, the Nazi party saw \"racial purity [as a] condition of superior cultural creation and of the construction of a powerful state\". The Civil Service Law was used to purify Germany through excluding Jews from key areas of the German community.\n\nThe Law for the Restoration of the Professional Civil Service \"defined the three groups of undesirable civil servants and provided for their dismissal\". The first group included those who had been appointed after November 9, 1918, and could be removed if they did not have the proper training, which essentially meant anyone could fit into these standards. The Second group were those who were deemed by their past that they would not always support the national state. The third group were any \"non-Aryans\", a way of excluding the Jews without explicitly mentioning \"Jews\" in the legislation.\n\nThis act of legislation was also passed in April as a supplement to the Civil Service Law. This law specifically attacked judges and public prosecutors, and forbade any Jews from taking the bar exam which was necessary to become a lawyer.\n\nThis law affected Jewish doctors, and subsequently Jewish health care. Passed also in April, under this legislation, patients who saw a \"non-Aryan\" doctor would not be covered under the national health insurance, thus excluding Jewish doctors from German society.\n\nLooking to further enact their racial agenda, the Nazi party then looked toward curbing educational policy. On April 25, 1933, the Law Against the Over-Crowding of German schools was passed, and required an end to any Weimar teachings that discussed democracy and equality; it enforced the teaching of racial pride. Under the guise of a concern for educational over-crowding, the Nazis limited the number of Jewish students enrolled in German schools to 1.5% of the total enrollment.\n\nWith the goal of excluding Jews from having full citizenship rights an Advisory Committee for Population and Race Policy met at the ministry of the Interior to discuss a new citizenship law.\n\nWhat followed was the Denaturalization Law passed on July 14. As a result of this law, the Reich government could take away the citizenship of those who were deemed \"undesirable\", applying to anyone who had been given citizenship by the Weimar government. Those who saw the results of this law first were the \"150,000 Eastern Jews in Germany\".\n\nPassed on September 29, 1933, this law \"excluded Jews from owning farmland or engaging in agriculture\". It stated that only Germans could now be farmers. Though the law had minimal effect due to the lack of Jews involved in farming, it still displayed a central idea of the Nazi party that, \"The Reich government passes this law to secure the peasant foundations of our blood line through instituting the ancient customs of land inheritance.\"\n\nOn September 29, 1933, the power of Jewish Cultural life in Germany was transferred to Joseph Goebbels, who established chambers of culture that would regulate activity in their chamber of either film, theater, music, fine arts, literature, broadcasting, and the press.\n\nEach Chamber had the power to exclude anyone involved in any of the facets of culture, even without an \"Aryan clause\" written into the legislation.\n\nFor example, the film chamber could dismiss any Jews involved in any stages of the film-making process including the \"producer, actors and ticket collectors in the theater\". To continue involvement in the film industry one would need \"licensed permission from the chamber president\".\n\nAs a supplement to the Chambers of Culture, a journalism law went into effect on October 4, 1933, stating that to produce work for the press, journalists and editors would also need specific legal permission.\n\nAt their annual party rally held in Nuremberg in September 1935, the Nazi leaders announced a set of three new laws to further regulate and exclude Jews from German Society. These laws now known as the Nuremberg laws served also as the legality for the arrests and violence against Jews that would come to follow.\n\nThe Nuremberg Laws were created in response to Hitler's demands for broadened citizenship laws that could \"underpin the more specifically racial-biological anti-Jewish legislation\". They were made to reflect the party principles that had been outlined in the points Hitler had written in the National Socialist Program in 1920.\n\nThe first law stated that black, red, and white were the national colors, and the swastika flag was the new national flag. Per Hitler, this law was stated to \"re-pay a debt of gratitude to the movement, under whose symbol Germany regained its freedom, in that they fulfill a significant item on the program of the National Socialist Party\".\n\nThe second law established who would be granted full political and civic rights and those who would now be deprived of them. Citizenship rights were to be granted to those who were citizens of the Reich, which were only individuals classified as being of \"German or related blood\"; therefore, Jews were excluded from any and all citizenship rights, becoming \"Staatsangehörige\" or state subjects, essentially making them foreigners in their own country.\n\nThe third law forbade marriage and any relations between Jews and German citizens. The law also made any previously existing marriages or those contracted outside of Germany invalid. Furthermore, Jews were forbidden from employing German female citizens who were under 45 years old. Under this law, Jews were also forbidden to raise the German Flag.\n\nAfter the Nuremberg Laws were passed, the Nazi Party also introduced supplemental decrees to the Citizenship Law and the Law for the Defense of German Blood and Honor to specifically outline who would be considered a Jew, and who would therefore be subject to the Nuremberg Laws' exclusionary principles\n\nOn November 14, the first supplemental decree was published, and it defined a Jewish person as anyone who had at least three full Jewish grandparents, had two Jewish grandparents and were married to a Jewish spouse, belonged to the Jewish religion at the time of the law's publication, or who entered the Jewish religion later. \"Mischlinge\" or the German legal term for those who had \"Aryan\" and Jewish blood, were also clarified to determine who would be considered a Jew. Those that were three-quarters Jewish were Jewish as well as those who were half Jewish due to their choice of becoming Jewish via a Jewish spouse or through joining a Jewish community.\n\nA second decree was published on December 21, which stated that Jewish professors, teachers, physicians, lawyers, and notaries who were state employees, and had previously been exempt, would now be dismissed from their positions.\n\nIn the first decree to the Law for the Defense of German Blood and Honor, it stated which specific marriages were forbidden. These included those \"between a Jew and a \"Mischling\" with one Jewish Grandparent, between a \"Mischling\" and another each with one Jewish Grandparent, and between a \"Mischling\" with two Jewish Grandparents and a German\".\n\nIn order to prevent foreign criticism of Germany, and keep the 1936 Olympics in Berlin, and to prevent economic loss and a blow to German prestige, Hitler eased the anti-Jewish stance momentarily.\n\nOn December 3, all anti-Jewish signs near the site of the winter Olympics were to be removed. Though, normally, this would be considered a sign of good faith, it was, in actuality, only an action taken to ensure the Olympics would be held in Germany by preventing international disapproval.\n\nAfter the Nuremberg Legislations and during 1938, \"worse than total expropriation was to follow: Economic harassment and even violence would henceforward be used to force the Jews to flee the Reich or the newly annexed Austria. Within the second phase, 1938 was the fateful turning point.\"\n\nDe-certification of all Jewish physicians, who were no longer allowed to treat German patients and forced to refer to themselves as \"sick-treaters\", a degrading term.\n\nMarch 22, 1938 Jews were forbidden from owning private gardens\n\nJuly 27, 1938 A decree was enforced stating all streets in Germany needed to be re-named\n\nNovember 12, 1938 Jews were forbidden from attending movie theaters, the opera, and concerts.\n\nNovember 15, 1938 Jewish children barred from attending public school\n\nThe essential robbery of Jews became legal when Jews were forced on February 21, 1939, to turn in all jewelry of any value.\n\nIn this second wave of legislation, Jews were ostracized even further from society, with strict restrictions living under \"a German regime that practiced terror and arbitrariness through the judicial system\".\n\nKristallnacht or the night of broken glass refers to Jewish pogroms that took place on November 9 and 10 in 1938. This wave of violence took place in Germany, annexed Austria, and areas of Sudetenland that were occupied by Germany. These attacks took place against synagogues, Jewish-owned businesses, other Jewish establishments, and Jewish citizens in general. More than 100 Jewish people were killed, and thousands more were arrested during these attacks. This was also the start to organized Nazi attacks, and the mass incarceration of the Jews. There was no clear Instructions on how to execute the violence, so it caused the destroying of Jewish property and inhumane treatment of Jewish people. The rioters destroyed 267 synagogues throughout Germany, Austria, and Sudetenland. The firefighter were instructed to prevent the flames from spreading to nearby buildings, but not to put the fires out on the burning synagogues. There was also about 7,500 Jewish-owned establishments that were robbed and had their windows shattered. About 30,000 Jewish men were arrested and moved to prisons or concentration camps as well. What made things even worse is the fact that the German government placed full blame on the Jews for the destructions and imposed a fine of one billion Reichsmark on the Jewish community. The government also took away all insurance payouts that would go to the Jews whose businesses or homes got destroyed. This left the Jews responsible for the costs of all repairs.\n\nThis led to many decrees in the weeks following. These decrees were designed to deprive the Jews of their means of living. Many of these movements enforced \"Aryanization\" policy. This meant that Jewish-owned property would be transferred to \"Aryan\" ownership for a fraction of the true value. To further remove the Jews from public life, the Jews were expelled from attending German schools, and they lost the right to have a driver's license or own an automobile.\n\nKristallnacht was essentially the turning point in the Nazis persecution of the Jewish people. It expanded the efforts to remove Jews from German economic and social life. It also led to forced emigration of Jews in order to make Germany free of Jews. Kristallnacht and the events that followed it showed the Nazi regime that they could count on the nationwide support of anti-Semitism from the general public. This showed the Nazis that they could easily move forward with their plans without much opposition from within Germany. The events of Kristallnacht foreshadowed the Holocaust and mass murders of the Jewish people.\n\nDuring the pre-war Nazi Germany period (1933-1939) there were more than 400 laws, decrees and other type of regulations whose goal was to restrict Jews. There were national laws that affected all Jews, and there were state, region and city laws that only affected the Jews in those communities. These regulations limited Jews in all aspects of life, both public and private. Almost all people were involved in the support of anti-Jewish legislation in some way, whether it was passive agreement or direct involvement. These movements were all part of Hitler's intention and plan to rid the planet of all Jewish population. Hitler's plan started with stripping all Jews of their basic rights, before moving into the mass murdering of the Jewish people.\n\n1933: \n1934: \n1935:\n1936: \n1937: \n1938: \n1939: \n\n"}
{"id": "3019808", "url": "https://en.wikipedia.org/wiki?curid=3019808", "title": "Arbitrariness", "text": "Arbitrariness\n\nArbitrariness is the quality of being \"determined by chance, whim, or impulse, and not by necessity, reason, or principle\".\n\nArbitrary decisions are not necessarily the same as random decisions. For example, during the 1973 oil crisis, Americans were allowed to purchase gasoline only on odd-numbered days if their license plate was odd, and on even-numbered days if their license plate was even. The system was well-defined and not random in its restrictions; however, since license plate numbers are completely unrelated to a person's fitness to purchase gasoline, it was still an arbitrary division of people. Similarly, schoolchildren are often organized by their surname in alphabetical order, a non-random yet still arbitrary method, at least in cases where surnames are irrelevant.\n\n\"Arbitrary\" comes from the Latin \"arbitrarius\", the source of \"arbiter\"; someone who is tasked to judge some matter. An arbitrary legal judgment is a decision made at the discretion of the judge, not one that is fixed by law. In some countries, a prohibition of arbitrariness is enshrined into the constitution. Article 9 of the Swiss Federal Constitution theoretically overrides even democratic decisions in prohibiting arbitrary government action. The US Supreme Court has overturned laws for having \"no rational basis.\" A recent study of the U.S. asylum system suggests that arbitrariness in decision-making might be the cause of large disparities in outcomes between different adjudicators, a phenomenon described as refugee roulette.\n\nArticle 330 of the Russian penal code defines 'Arbitrariness' as a specific crime, but with a very broad definition encompassing any 'actions contrary to the order presented by a law'.\n\nArbitrary actions are closely related to teleology, the study of purpose. Actions lacking a \"telos\", a goal, are necessarily arbitrary. With no end to measure against, there can be no standard applied to choices, so all decisions are alike. Note that arbitrary or random methods in the standard sense of \"arbitrary\" may not qualify as arbitrary choices philosophically, if they were done in furtherance of a larger purpose; in the examples above, discipline in school and avoiding overcrowding at gas stations.\n\nNihilism is the philosophy that believes that there is no purpose in the universe, and that \"every\" choice is arbitrary. According to nihilism, the universe contains no value and is essentially meaningless. Because the universe and all of its constituents contain no higher goal for us to make subgoals from, all aspects of human life and experiences are completely arbitrary. There is no right or wrong decision, thought or practice, and whatever choice a human being makes is just as meaningless and empty as any other choice he or she could have made.\n\nMany brands of theism, the belief in a deity or deities, believe that everything has a purpose and that \"nothing\" is arbitrary. In these philosophies, God created the universe for a reason, and every event flows from that. Even seemingly random events cannot escape God's hand and purpose. This is somewhat related to the argument from design, the argument for God's existence because a purpose can be found in the universe.\n\nArbitrariness is also related to ethics, the philosophy of decision-making. Even if a person has a goal, they may choose to attempt to achieve it in ways that may be considered arbitrary. Rationalism holds that knowledge comes about through intellectual calculation and deduction; many rationalists (though not all) apply this to ethics as well. All decisions should be made through reason and logic, not via whim or how one \"feels\" what is right. Randomness may occasionally be acceptable as part of a subtask in furtherance of a larger goal, but not in general.\n\nIn semiotics, the general theory of signs, sign systems, and sign processes, Saussure introduced the notion of arbitrariness according to which there is no necessary connection between the material sign (or \"signifier\") and the entity it refers to or denotes as its meaning (or \"signified\") as a mental concept or real object. \n\nSpontaneous arbitration...\n\nIn mathematics, arbitrary normally means \"any;\" for instance, an arbitrary division of a set or an arbitrary permutation of a sequence. Its use implies generality and that a statement does not only apply to special cases – \"you may select any choice possible, and this statement will still hold.\" A simple example would be \"Given an arbitrary integer, multiplying it by two will result in an even number.\"\n\nEven further, the implication is that generality will hold even if you have an opponent choose the item in question. In some ways \"arbitrary\" is here synonymous with \"worst-case\".\n\n"}
{"id": "37568781", "url": "https://en.wikipedia.org/wiki?curid=37568781", "title": "Argument from authority", "text": "Argument from authority\n\nAn argument from authority, (\"argumentum ab auctoritate\"), also called an appeal to authority, or argumentum ad verecundiam is a form of defeasible argument in which a claimed authority's support is used as evidence for an argument's conclusion. It is well known as a fallacy, though it is used in a cogent form when all sides of a discussion agree on the reliability of the authority in the given context.\n\nHistorically, opinion on the appeal to authority has been divided: it is listed as a valid argument as often as a fallacious argument in various sources, with some holding that it is a strong argument which \"has a legitimate force\", and others that it is weak or an outright fallacy where, on a conflict of facts, \"mere appeal to authority alone had better be avoided\".\n\nIf all parties agree on the reliability of an authority in the given context it forms a valid inductive argument.\n\nScientific knowledge is best established by evidence and experiment rather than argued through authority as authority has no place in science. Carl Sagan wrote of arguments from authority: One of the great commandments of science is, \"Mistrust arguments from authority.\" ... Too many such arguments have proved too painfully wrong. Authorities must prove their contentions like everybody else.\n\nAn example of the use of the appeal to authority in science can be seen in 1923, when leading American zoologist Theophilus Painter declared, based on poor data and conflicting observations he had made, that humans had 24 pairs of chromosomes. From the 1920s to the 1950s, this continued to be held based on Painter's authority, despite subsequent counts totaling the correct number of 23. Even textbooks with photos showing 23 pairs incorrectly declared the number to be 24 based on the authority of the then-consensus of 24 pairs.\n\nThis seemingly established number created confirmation bias among researchers, and \"most cytologists, expecting to detect Painter's number, virtually always did so\". Painter's \"influence was so great that many scientists preferred to believe his count over the actual evidence\", and scientists who obtained the accurate number modified or discarded their data to agree with Painter's count.\n\nA more recent example involved the \"When contact changes minds: An experiment on transmission of support for gay equality\" paper. The paper was a fraud based on forged data, yet concerns about it were ignored in many cases due to appeals to authority. One analysis of the affair notes that \"Over and over again, throughout the scientific community and the media, LaCour’s impossible-seeming results were treated as truth, in part because of the weight [the study's co-author] Green’s name carried\". One psychologist stated his reaction to the paper was \"that’s very surprising and doesn’t fit with a huge literature of evidence. It doesn’t sound plausible to me... [then I pull it up and] I see Don Green is an author. I trust him completely, so I’m no longer doubtful\". The forger, LaCour, would use appeals to authority to defend his research: \"if his responses sometimes seemed to lack depth when he was pressed for details, his impressive connections often allayed concerns\", with one of his partners stating \"when he and I really had a disagreement, he would often rely on the kind of arguments where he’d basically invoke authority, right? He’s the one with advanced training, and his adviser is this very high-powered, very experienced person...and they know a lot more than we do\".\n\nMuch like the erroneous chromosome number taking decades to refute until microscopy made the error unmistakable, the one who would go on to debunk this paper \"was consistently told by friends and advisers to keep quiet about his concerns lest he earn a reputation as a troublemaker\", up until \"the very last moment when multiple 'smoking guns' finally appeared\", and he found that \"There was almost no encouragement for him to probe the hints of weirdness he’d uncovered\".\n\nFallacious arguments from authority are also frequently the result of citing a non-authority as an authority. An example of the fallacy of appealing to an authority in an unrelated field would be citing Albert Einstein as an authority for a determination on religion when his primary expertise was in physics. The body of attributed authorities might not even welcome their citation, such as with the \"More Doctors Smoke Camels\" ad campaign.\n\nIt is also a fallacious \"ad hominem\" argument to argue that a person presenting statements lacks authority and thus their arguments do not need to be considered. As appeals to a perceived lack of authority, these types of argument are fallacious for much the same reasons as an appeal to authority.\n\nOther related fallacious arguments assume that a person without status or authority is inherently reliable. For instance, the appeal to poverty is the fallacy of thinking that someone is more likely to be correct because they are poor. When an argument holds that a conclusion is likely to be true precisely because the one who holds or is presenting it lacks authority, it is a fallacious \"appeal to the common man\".\n\nThe argument from authority is based on the idea that a perceived authority must know better and that the person should conform to their opinion. This has its roots in psychological cognitive biases such as the Asch effect. In repeated and modified instances of the Asch conformity experiments, it was found that high-status individuals create a stronger likelihood of a subject agreeing with an obviously false conclusion, despite the subject normally being able to clearly see that the answer was incorrect.\n\nFurther, humans have been shown to feel strong emotional pressure to conform to authorities and majority positions. A repeat of the experiments by another group of researchers found that \"Participants reported considerable distress under the group pressure\", with 59% conforming at least once and agreeing with the clearly incorrect answer, whereas the incorrect answer was much more rarely given when no such pressures were present.\n\nAnother study shining light on the psychological basis of the fallacy as it relates to perceived authorities are the Milgram experiments, which demonstrated that people are more likely to go along with something when it is presented by an authority. In a variation of a study where the researchers did not wear a lab coat, thus reducing the perceived authority of the tasker, the obedience level dropped to 20% from the original rate, which had been higher than 50%. Obedience is encouraged by reminding the individual of what a perceived authority states and by showing them that their opinion goes against this authority.\n\nScholars have noted that certain environments can produce an ideal situation for these processes to take hold, giving rise to groupthink. In groupthink, individuals in a group feel inclined to minimize conflict and encourage conformity. Through an appeal to authority, a group member might present that opinion as a consensus and encourage the other group members to engage in groupthink by not disagreeing with this perceived consensus or authority. One paper about the philosophy of mathematics for example notes that, within academia, If...a person accepts our discipline, and goes through two or three years of graduate study in mathematics, he absorbs our way of thinking, and is no longer the critical outsider he once was...If the student is unable to absorb our way of thinking, we flunk him out, of course. If he gets through our obstacle course and then decides that our arguments are unclear or incorrect, we dismiss him as a crank, crackpot, or misfit.\n\nCorporate environments are similarly vulnerable to appeals to perceived authorities and experts leading to groupthink, as are governments and militaries.\n\n"}
{"id": "102883", "url": "https://en.wikipedia.org/wiki?curid=102883", "title": "Belief", "text": "Belief\n\nBelief is the state of mind in which a person thinks something to be the case with or without there being empirical evidence to prove that something is the case with factual certainty. Another way of defining belief sees it as a mental representation of an attitude positively oriented towards the likelihood of something being true. In the context of Ancient Greek thought, two related concepts were identified with regards to the concept of belief: \"pistis\" and \"doxa\". Simplified, we may say that \"pistis\" refers to \"trust\" and \"confidence\", while \"doxa\" refers to \"opinion\" and \"acceptance\". The English word \"orthodoxy\" derives from \"doxa\". Jonathan Leicester suggests that belief has the purpose of guiding action rather than indicating truth.\n\nIn epistemology, philosophers use the term \"belief\" to refer to personal attitudes\nassociated with true or false ideas and concepts. However, \"belief\" does not require active introspection and circumspection. For example, we never ponder whether or not the sun will rise. We simply assume the sun will rise. Since \"belief\" is an important aspect of mundane life, according to Eric Schwitzgebel in the \"Stanford Encyclopedia of Philosophy\", a related question asks: \"how a physical organism can have beliefs?\"\n\nEpistemology is concerned with delineating the boundary between justified belief and opinion, and involved generally with a theoretical philosophical study of knowledge. The primary problem in epistemology is to understand exactly what is needed in order for us to have knowledge. In a notion derived from Plato's dialogue \"Theaetetus\", where the epistemology of Socrates (Platon) most clearly departs from that of the sophists, who at the time of Plato seem to have defined knowledge as what is here expressed as \"justified true belief\". The tendency to translate from belief (here: doxa – common opinion) to knowledge (here: episteme), which Plato (e.g. Socrates of the dialogue) utterly dismisses, results from failing to distinguish a dispositive belief (gr. 'doxa', not 'pistis') from knowledge (episteme) when the opinion is regarded \"true\" (here: orthé), in terms of right, and juristically so (according to the premises of the dialogue), which was the task of the rhetors to prove. Plato dismisses this possibility of an affirmative relation between belief (i.e. opinion) and knowledge even when the one who opines grounds his belief on the rule, and is able to add \"justification\" (gr. logos: reasonable and necessarily plausible assertions/evidence/guidance) to it.\n\nPlato has been credited for the \"justified true belief\" theory of knowledge, even though Plato in the Theaetetus (dialogue) elegantly dismisses it, and even posits this argument of Socrates as a cause for his death penalty. Among American epistemologists, Gettier (1963) and Goldman (1967), have questioned the \"justified true belief\" definition, and challenged the \"sophists\" of their time.\n\nMainstream psychology and related disciplines have traditionally treated belief as if it were the simplest form of mental representation and therefore one of the building blocks of conscious thought. Philosophers have tended to be more abstract in their analysis, and much of the work examining the viability of the belief concept stems from philosophical analysis.\n\nThe concept of belief presumes a subject (the believer) and an object of belief (the proposition). So, like other propositional attitudes, belief implies the existence of mental states and intentionality, both of which are hotly debated topics in the philosophy of mind, whose foundations and relation to brain states are still controversial.\n\nBeliefs are sometimes divided into core beliefs (that are actively thought about) and \"dispositional beliefs\" (that may be ascribed to someone who has not thought about the issue). For example, if asked \"do you believe tigers wear pink pajamas?\" a person might answer that they do not, despite the fact they may never have thought about this situation before.\n\nThis has important implications for understanding the neuropsychology and neuroscience of belief. If the concept of belief is incoherent, then any attempt to find the underlying neural processes that support it will fail.\n\nPhilosopher Lynne Rudder Baker has outlined four main contemporary approaches to belief in her controversial book \"Saving Belief\":\nStrategic approaches make a distinction between rules, norms and beliefs as follows:\n(1) Rules. Explicit regulative processes such as policies, laws, inspection routines, or incentives. Rules function as a coercive regulator of behavior and are dependent upon the imposing entity's ability to enforce them.\n(2) Norms. Regulative mechanisms accepted by the social collective. Norms are enforced by normative mechanisms within the organization and are not strictly dependent upon law or regulation.\n(3) Beliefs. The collective perception of fundamental truths governing behavior. The adherence to accepted and shared beliefs by members of a social system will likely persist and be difficult to change over time. Strong beliefs about determinant factors (i.e., security, survival, or honor) are likely to cause a social entity or group to accept rules and norms.\n\nHistorically \"belief-in\" belonged in the realm of religious thought, \"belief-that\" instead belonged to epistemological considerations.\n\nTo \"believe in\" someone or something is a distinct concept from \"believing-that.\" There are at least these types of belief-in:\n\n\nEconomic beliefs are beliefs which are reasonably and necessarily contrary to the tenet of rational choice or instrumental rationality.\n\nStudies of the Austrian tradition of the economic thought, in the context of analysis of the influence and subsequent degree of change resulting from existing economic knowledge and belief, has contributed the most to the subsequent holistic collective analysis.\n\nInsofar as the truth of belief is expressed in sentential and propositional form we are using the sense of \"belief-that\" rather than \"belief-in\". Delusion arises when the truth value of the form is clearly nil.\n\nDelusions are defined as beliefs in psychiatric diagnostic criteria (for example in the \"Diagnostic and Statistical Manual of Mental Disorders\"). Psychiatrist and historian G.E. Berrios has challenged the view that delusions are genuine beliefs and instead labels them as \"empty speech acts,\" where affected persons are motivated to express false or bizarre belief statements due to an underlying psychological disturbance. However, the majority of mental health professionals and researchers treat delusions as if they were genuine beliefs.\n\nIn Lewis Carroll's \"Through the Looking-Glass\" the White Queen says, \"Why, sometimes I've believed as many as six impossible things before breakfast.\" This is often quoted in mockery of the common ability of people to entertain beliefs contrary to fact.\n\nPsychologists study belief formation and the relationship between beliefs and actions. Three models of belief formation and change have been proposed:\n\nWhen people are asked to estimate the likelihood that a statement is true, they search their memory for information that has implications for the validity of this statement. Once this information has been identified, they estimate a) the likelihood that the statement would be true if the information were true, and b) the likelihood that the statement would be true if the information were false. If their estimates for these two probabilities differ, people average them, weighting each by the likelihood that the information is true and false (respectively). Thus, information bears directly on beliefs of another, related statement.\n\nUnlike the previous model, this one takes into consideration the possibility of multiple factors influencing belief formation. Using regression procedures, this model predicts belief formation on the basis of several different pieces of information, with weights assigned to each piece on the basis of their relative importance.\n\nThese models address the fact that the responses people have to belief-relevant information is unlikely to be predicted from the objective basis of the information that they can recall at the time their beliefs are reported. Instead, these responses reflect the number and meaning of the thoughts that people have about the message at the time that they encounter it.\n\nSome influences on people's belief formation include:\n\nHowever, even educated people, well aware of the process by which beliefs form, still strongly cling to their beliefs, and act on those beliefs even against their own self-interest. In Anna Rowley's book, Leadership Therapy, she states \"You want your beliefs to change. It's proof that you are keeping your eyes open, living fully, and welcoming everything that the world and people around you can teach you.\" This means that peoples' beliefs should evolve as they gain new experiences.\n\nJustified true belief is a definition of knowledge that gained approval during the Enlightenment, 'justified' standing in contrast to 'revealed'. There have been attempts to trace it back to Plato and his dialogues. The concept of justified true belief states that in order to know that a given proposition is true, one must not only believe the relevant true proposition, but also have justification for doing so. In more formal terms, an agent formula_1 knows that a proposition formula_2 is true if and only if:\n\nThis theory of knowledge suffered a significant setback with the discovery of Gettier problems, situations in which the above conditions were seemingly met but that many philosophers disagree that anything is known. Robert Nozick suggested a clarification of \"justification\" which he believed eliminates the problem: the justification has to be such that were the justification false, the knowledge would be false. Bernecker and Dretske (2000) argue that \"no epistemologist since Gettier has seriously and successfully defended the traditional view.\" On the other hand, Paul Boghossian argues that the Justified True Belief account is the \"standard, widely accepted\" definition of knowledge.\n\nAn extensive amount of scientific research and philosophical discussion exists around the modification of beliefs, which is commonly referred to as belief revision.\nGenerally speaking, the process of belief revision entails the believer weighing the set of truths and/or evidence, and the dominance of a set of truths or evidence on an alternative to a held belief can lead to revision. One process of belief revision is Bayesian updating and is often referenced for its mathematical basis and conceptual simplicity. However, such a process may not be representative for individuals whose beliefs are not easily characterized as probabilistic.\n\nThere are several techniques for individuals or groups to change the beliefs of others; these methods generally fall under the umbrella of persuasion. Persuasion can take on more specific forms such as consciousness raising when considered in an activist or political context.\nBelief modification may also occur as a result of the experience of outcomes. Because goals are based, in part on beliefs, the success or failure at a particular goal may contribute to modification of beliefs that supported the original goal.\n\nWhether or not belief modification actually occurs is dependent not only on the extent of truths or evidence for the alternative belief, but also characteristics outside the specific truths or evidence. This includes, but is not limited to: the source characteristics of the message, such as credibility; social pressures; the anticipated consequences of a modification; or the ability of the individual or group to act on the modification. Therefore, individuals seeking to achieve belief modification in themselves or others need to consider all possible forms of resistance to belief revision.\n\nWithout qualification, \"belief\" normally implies a lack of doubt, especially insofar as it is a designation of a life stance. In practical everyday use however, belief is normally partial and retractable with varying degrees of certainty.\n\nA copious literature exists in multiple disciplines to accommodate this reality. In mathematics probability, fuzzy logic, fuzzy set theory, and other topics are largely directed to this.\n\nDifferent psychological models have tried to predict people's beliefs and some of them try to estimate the exact probabilities of beliefs. For example, Robert Wyer developed a model of subjective probabilities. When people rate the likelihood of a certain statement (e.g., \"It will rain tomorrow\"), this rating can be seen as a subjective probability value. The subjective probability model posits that these subjective probabilities follow the same rules as objective probabilities. For example, the law of total probability might be applied to predict a subjective probability value. Wyer found that this model produces relatively accurate predictions for probabilities of single events and for changes in these probabilities, but that the probabilities of several beliefs linked by \"and\" or \"or\" do not follow the model as well.\n\nReligious belief refers to attitudes towards mythological, supernatural, or spiritual aspects of a religion. Religious belief is distinct from religious practice and from religious behaviours – with some believers not practicing religion and some practitioners not believing religion. Religious beliefs, deriving from ideas that are exclusive to religion, often relate to the existence, characteristics and worship of a deity or deities, to the idea of divine intervention in the universe and in human life, or to the deontological explanations for the values and practices centered on the teachings of a spiritual leader or of a spiritual group. In contrast to other belief systems, religious beliefs are usually codified.\n\nA popular view holds that different religions each have identifiable and exclusive sets of beliefs or creeds, but surveys of religious belief have often found that the official doctrine and descriptions of the beliefs offered by religious authorities do not always agree with the privately held beliefs of those who identify as members of a particular religion. For a broad classification of the kinds of religious belief, see below.\n\nFirst self-applied as a term to the conservative doctrine outlined by anti-modernist Protestants in the United States of America, \"fundamentalism\" in religious terms denotes strict adherence to an interpretation of scriptures that are generally associated with theologically conservative positions or traditional understandings of the text and are distrustful of innovative readings, new revelation, or alternative interpretations. Religious fundamentalism has been identified in the media as being associated with fanatical or zealous political movements around the world that have used a strict adherence to a particular religious doctrine as a means to establish political identity and to enforce societal norms.\n\nFirst used in the context of Early Christianity, the term \"orthodoxy\" relates to religious belief that closely follows the edicts, apologies, and hermeneutics of a prevailing religious authority. In the case of Early Christianity, this authority was the communion of bishops, and is often referred to by the term \"Magisterium\". The term \"orthodox\" was applied almost as an epithet to a group of Jewish believers who held to pre-Enlightenment understanding of Judaism – now known as Orthodox Judaism. The Eastern Orthodox Church of Christianity and the Catholic Church each consider themselves to be the true heir to Early Christian belief and practice. The antonym of \"orthodox\" is \"heterodox\", and those adhering to orthodoxy often accuse the heterodox of apostasy, schism, or heresy.\n\nThe Renaissance and later the Enlightenment in Europe exhibited varying degrees of religious tolerance and intolerance towards new and old religious ideas. The \"philosophes\" took particular exception to many of the more fantastical claims of religions and directly challenged religious authority and the prevailing beliefs associated with the established churches. In response to the liberalizing political and social movements, some religious groups attempted to integrate Enlightenment ideals of rationality, equality, and individual liberty into their belief systems, especially in the nineteenth and twentieth centuries. Reform Judaism and Liberal Christianity offer two examples of such religious associations.\n\nA term signifying derogation that is used by the religious and non-religious alike, \"superstition\" refers to a deprecated belief in supernatural causation. Those who deny the existence of the supernatural generally attribute all beliefs associated with it to be superstitious, while a typical religious critique of superstition holds that it either encompasses beliefs in non-existent supernatural activity or that the supernatural activity is inappropriately feared or held in improper regard (see idolatry). Christian Churches strongly condemned occultism, animism, paganism, and other folk religions as mean forms of superstition, though such condemnation did not necessarily eliminate the beliefs among the common people, and many such religious beliefs persist .\n\nIn Buddhism, practice and progress along the spiritual path happens when one follows the system of Buddhist practice. Any religion which follows (parts of) the fundamentals of this system has, according to the teachings of Buddha, good aspects to the extent it accords with this system. Any religion which goes against (parts of) the fundamentals of this system includes bad aspects too. Any religion which does not teach certain parts of this system, is not because of this a \"bad\" religion; it just lacks those teachings and is to that extent incomplete.\n\nA question by the monk Subhadda to the Buddha:\nThe Buddha replied:\nAs a religious tradition, Hinduism has experienced many attempts at systemization. In medieval times, Shankara advocated for the Advaita system of philosophy. In recent times, Tamala Krishna Gosvami has researched the systemization of Krishna theology as expounded by Srila Prabhupada. (See Krishnology)\n\nSome believe that religion cannot be separated from other aspects of life, or believe that certain cultures did not or do not separate their religious activities from other activities in the same way that some people in modern Western cultures do.\n\nSome anthropologists report cultures in which gods are involved in every aspect of life – if a cow goes dry, a god has caused this, and must be propitiated; when the sun rises in the morning, a god has caused this, and must be thanked. Even in modern Western cultures, many people see supernatural forces behind every event, as described by Carl Sagan in his 1995 book \"The Demon-Haunted World\".\n\nPeople with such a worldview often regard the influence of Western culture as inimical. Others with this worldview resist the influence of science, and believe that science (or \"so-called science\") should be guided by religion. Still others with this worldview believe that all political decisions and laws should be guided by religion. This last belief, written into the constitutions of many Islamic nations, is shared by some fundamentalist Christians.\n\nIn addition, beliefs about the supernatural or metaphysical may not presuppose a difference between any such thing as nature and non-nature, nor between science and what most educated people believe. In the view of some historians, the pre-Socratic Athenians saw science, political tradition, culture and religion as not easily distinguishable, but as all part of the same body of knowledge and wisdom available to a community.\n\nAdherents of particular religions deal with the differing doctrines and practices espoused by other religions or by other religious denominations in a variety of ways. All strains of thought appear in different segments of all major world religions.\n\nPeople with exclusivist beliefs typically explain other beliefs either as in error, or as corruptions or counterfeits of the true faith. This approach is a fairly consistent feature among smaller new religious movements that often rely on doctrine that claims a unique revelation by the founders or leaders, and considers it a matter of faith that the \"correct\" religion has a monopoly on truth. All three major Abrahamic monotheistic religions have passages in their holy scriptures that attest to the primacy of the scriptural testimony, and indeed monotheism itself is often vouched as an innovation characterized specifically by its explicit rejection of earlier polytheistic faiths.\n\nSome exclusivist faiths incorporate a specific element of proselytization. This is a strongly-held belief in the Christian tradition which follows the doctrine of the Great Commission, and is less emphasized by the Islamic faith where the Quranic edict \"There shall be no compulsion in religion\" (2:256) is often quoted as a justification for toleration of alternative beliefs. The Jewish tradition does not actively seek out converts.\n\nExclusivism correlates with conservative, fundamentalist, and orthodox approaches of many religions, while pluralistic and syncretist approaches either explicitly downplay or reject the exclusivist tendencies within a religion.\n\nPeople with inclusivist beliefs recognize some truth in all faith systems, highlighting agreements and minimizing differences. This attitude is sometimes associated with Interfaith dialogue or with the Christian Ecumenical movement, though in principle such attempts at pluralism are not necessarily inclusivist and many actors in such interactions (for example, the Roman Catholic Church) still hold to exclusivist dogma while participating in inter-religious organizations.\n\nExplicitly inclusivist religions include many that are associated with the New Age movement, as well as modern reinterpretations of Hinduism and Buddhism. The Bahá'í Faith considers it doctrine that there is truth in all faith-systems.\n\nPeople with pluralist beliefs make no distinction between faith systems, viewing each one as valid within a particular culture. Examples include:\n\n\nPeople with syncretistic views blend the views of a variety of different religions or traditional beliefs into a unique fusion which suits their particular experiences and contexts (\"see\" eclecticism). Unitarian Universalism exemplifies a syncretistic faith.\n\nTypical reasons for adherence to religion include the following:\n\n\nPsychologist James Alcock also summarizes a number of apparent benefits which reinforce religious belief. These include prayer appearing to account for successful resolution of problems, \"a bulwark against existential anxiety and fear of annihilation,\" an increased sense of control, companionship with one's deity, a source of self-significance, and group identity.\n\nTypical reasons for rejection of religion include:\n\nA belief system is a set of mutually supportive beliefs. The beliefs of any such system can be classified as religious, philosophical, political, ideological, or a combination of these. Philosopher Jonathan Glover says that beliefs are always part of a belief system, and that tenanted belief systems are difficult for the tenants to completely revise or reject.\n\nA collective belief is referred to when people speak of what 'we' believe when this is not simply elliptical for what 'we all' believe.\n\nSociologist Émile Durkheim wrote of collective beliefs and proposed that they, like all 'social facts', 'inhered in' social groups as opposed to individual persons. Durkheim's discussion of collective belief, though suggestive, is relatively obscure.\n\nPhilosopher Margaret Gilbert has offered a related account in terms of the joint commitment of a number of persons to accept a certain belief as a body. According to this account, individuals who together collectively believe something need not personally believe it themselves. Gilbert's work on the topic has stimulated a developing literature among philosophers. One question that has arisen is whether and how philosophical accounts of belief in general need to be sensitive to the possibility of collective belief.\n\nJonathan Glover believes that he and other philosophers ought to play some role in starting dialogues between people with deeply held, opposing beliefs, especially if there is risk of violence. Glover also believes that philosophy can offer insights about beliefs that would be relevant to such dialogue.\nGlover suggests that beliefs have to be considered holistically, and that no belief exists in isolation in the mind of the believer. It always implicates and relates to other beliefs. Glover provides the example of a patient with an illness who returns to a doctor, but the doctor says that the prescribed medicine is not working. At that point, the patient has a great deal of flexibility in choosing what beliefs to keep or reject: the patient could believe that the doctor is incompetent, that the doctor's assistants made a mistake, that the patient's own body is unique in some unexpected way, that Western medicine is ineffective, or even that Western science is entirely unable to discover truths about ailments.\n\nGlover maintains that any person can continue to hold any belief if they would really like to (e.g., with help from ad hoc hypotheses). One belief can be held fixed, and other beliefs will be altered around it. Glover warns that some beliefs may not be entirely explicitly believed (e.g., some people may not realize they have racist belief systems adopted from their environment as a child). Glover believes that people tend to first realize that beliefs can change, and may be contingent on their upbringing, around age 12 or 15.\n\nGlover emphasizes that beliefs are difficult to change. He says that one may try to rebuild one's beliefs on more secure foundations (axioms), like building a new house, but warns that this may not be possible. Glover offers the example of René Descartes, saying about Descartes that \"[h]e starts off with the characteristic beliefs of a 17th-century Frenchman; he then junks the lot, he rebuilds the system, and somehow it looks a lot like the beliefs of a 17th-century Frenchman.\" To Glover, belief systems are not like houses but are instead like boats. As Glover puts it: \"Maybe the whole thing needs rebuilding, but inevitably at any point you have to keep enough of it intact to keep floating.\"\n\nGlover's final message is that if people talk about their beliefs, they may find more deep, relevant, philosophical ways in which they disagree (e.g., less obvious beliefs, or more deeply held beliefs). Glover thinks that people often manage to find agreements and consensus through philosophy. He says that at the very least, if people do not convert each other, they will hold their own beliefs more openmindedly and will be less likely to go to war over conflicting beliefs.\n\nThe British philosopher Stephen Law has described some belief systems (including belief in homeopathy, psychic powers, and alien abduction) as \"claptrap\" and said that they \"draw people in and hold them captive so they become willing slaves to victory... if you get sucked in, it can be extremely difficult to think your way clear again\".\n\n\n"}
{"id": "13307194", "url": "https://en.wikipedia.org/wiki?curid=13307194", "title": "Braden Allenby", "text": "Braden Allenby\n\nBraden R. Allenby (born 1950) is an American environmental scientist, environmental attorney and Professor of Civil and Environmental Engineering, and of Law, at Arizona State University.\n\nDr. Allenby was born in Highland Park, Illinois December 29, 1950 to Dr. Richard J. Allenby, jr.(1923-2017) and Julia T. Allenby(1925–2002). He is the oldest of three brothers, Dr. Kent Allenby(1952- ) and Peter Allenby(1957- ).\nAllenby graduated cum laude from Yale University in 1972, received his Juris Doctor from the University of Virginia Law School in 1978, his Masters in Economics from the University of Virginia in 1979, his Masters in Environmental Sciences from Rutgers University in the Spring of 1989, and his Ph.D. in Environmental Sciences from Rutgers in 1992.\n\nHe joined AT&T in 1983 as a telecommunications regulatory attorney, and was an environmental attorney and Senior Environmental Attorney for AT&T from 1984 to 1993. From 1991 to 1992 he was the J. Herbert Holloman Fellow at the National Academy of Engineering in Washington, DC. His areas of expertise include Design for Environment, industrial ecology, telework and netcentric organizations, and earth systems engineering and management. During 1992, he was the J. Herbert Holloman Fellow at the National Academy of Engineering in Washington, DC. From 1995 to 1997 he was Director for Energy and Environmental Systems at Lawrence Livermore National Laboratory, on temporary assignment from his position as Research Vice President, Technology and Environment, for AT&T. In June, 2000, he chaired the second Gordon Conference on Industrial Ecology.\n\nIn 2007 he is President of the International Society for Industrial Ecology; Chair of the AAAS Committee on Science, Engineering, and Public Policy; a Batten Fellow in Residence at the University of Virginia's Darden Graduate School of Business Administration; \nHe is a member of the Virginia Bar, and has worked as an attorney for the Civil Aeronautics Board and the Federal Communications Commission, as well as a strategic consultant on economic and technical telecommunications issues. He is a Fellow of the Royal Society for the Arts, Manufactures & Commerce. He is currently a former member of different boards.\n\nHis areas of all expertise include: design for environment, earth systems engineering and management, industrial ecology, NBIC (i.e., nanotechnology, biotechnology, information and communication technology, and cognitive science), convergence and technological evolution.\n\nHe has taught courses on industrial ecology and design for environment at the Yale University School of Forestry and Environmental Studies and at the University of Wisconsin Engineering Extension School; and has lectured widely on earth systems engineering and management, industrial ecology, and design for Environment.\n\nAllenby has authored a number of articles and book chapters on his above mentioned interests, and writes a column for the Journal of Industrial Ecology.\n\nBooks: \n\nVarious Articles:\n\n\n\n"}
{"id": "3388492", "url": "https://en.wikipedia.org/wiki?curid=3388492", "title": "Brinkler classification", "text": "Brinkler classification\n\nBrinkler Classification is the library classification system of Bartol Brinkler described in his article \"The Geographical Approach to Materials in the Library of Congress Subject Headings\". The geographical aspect of a subject may be conveyed through three types of headings labeled A, B, and C. Heading A uses a primary topical description with geographical subdivisions (e.g. Art—Paris). Type B uses a place-name for the main heading with a topical subdivision (e.g. Paris—Description). C headings use a geographical description of a phrase (e.g. Paris Literature). \n\nBrinkler explores what type of heading is more useful to a patron, and he finds that it depends on the level of familiarity a patron has with a topic and what approach they take when searching for resources on their topic. Ideally readers will either be looking for everything on a particular topic, or everything regarding a particular place. Bartol Brinkler investigates a system of classification that will best serve these two ideal types of patrons. He finds working with Type A headings will best assist a patron who is more topic oriented, while using Type B headings is preferable for those who are primarily interested in one place. \n\nHowever this is problematic in practice. One possibility is to assign Type A and Type B headings to every resource, but the cataloguing cost would be high. A system that aids readers regardless of their approach to a topic involves using cross-references (e.g. Canada—Botany, See Botany—Canada). Admitting that see and see also references would require more work on the part of librarians, Bartol Brinkler notes that librarians must keep in mind \"...readers do not have the same knowledge [of classification] and do need all the help they can get...\"\n\n\n"}
{"id": "18538028", "url": "https://en.wikipedia.org/wiki?curid=18538028", "title": "Chartered Institute of Personnel and Development", "text": "Chartered Institute of Personnel and Development\n\nThe Chartered Institute of Personnel and Development (CIPD) is a professional association for human resource management professionals. It is headquartered in Wimbledon, London, England. The organisation was founded in 1913 - it is the world's oldest association in its field and has over 145,000 members internationally working across private, public and voluntary sectors. Peter Cheese was announced in June 2012 as CIPD's new CEO from July 2012.\n\nIn the United Kingdom, factory inspectors were appointed for the first time in 1893. In 1896 to look after its women and child workers Rowntree's appointed their first inspector - a Mrs E M Wood. Edward Cadbury of Cadbury Brothers in 1909 called together employers to discuss industrial welfare work and as a result 25 employers formed an association with Mrs Wood of Rowntree's as Secretary. The work of 'welfare workers' came to public attention during a trade show in 1912 at Olympia in London.\n\nThe forerunner of the CIPD, the Welfare Workers' Association (WWA) was formed at an employers' conference in York on 6 June 1913. The meeting was chaired by Seebohm Rowntree. Alongside his company, Rowntree's around fifty other companies were present including; Boots, Cadbury and Chivers and Sons. Thirty-four of the employers present decided that the WWA be founded as...\"an association of employers interested in industrial betterment and of welfare workers engaged by them\". The outbreak of World War I in 1914 led to many women and children taking up the work of men, particularly in the larger munition factories where the appointment of welfare officers was made compulsory by legislation and was monitored by the Health of Munition Workers Committee. This led to the rapid expansion of female welfare workers. There were concerns about the training of welfare staff, and in 1917, at a gathering in Leeds of the seven welfare associations formed during the period it was agreed that they merge by forming the Central Association of Welfare Workers which to accommodate the regional associations established the beginnings of a local branch structure. The Association's position was also enhanced during the war years by nationally driven encouragement of workers to join trade unions to reduce the occurrence of industrial strife. Another development which increased the numbers of company staff dealing with labour and welfare matters occurred with the inclusion of managers, mainly men, from the North-western Area Industrial Association to assist with discipline, dismissal and industrial relations in increasingly unionised organisations. In 1918, to avoid confusion as to its purpose the Association changed its name to the Central Association of Welfare Workers (Industrial) (CAWWI).\n\nAnother important event which had a recurring impact on the activities of the CAWWI occurred in 1918 when the Rev. Robert Hyde founded the Boys’ Welfare Association soon after renamed the Industrial Welfare Society (IWS) with six employers who were concerned with the welfare of boys employed or apprenticed in the shipbuilding industry. From the outset there was a strained relationship between the two bodies which continued right through the inter-war period until resolved in 1946. Though both organisations were concerned with 'welfare at work' the CAWWI developed as an institution for practitioners and the IWS was established as a membership body for employers and there was strong disagreement on how best to bring about improvements in workplace conditions and workers' welfare. The IWS later became the Industrial Society and is now known as the Work Foundation.\n\nIn November 1919, following merger with welfare associations for men the Central Association of Welfare Workers (Industrial) was renamed the Welfare Workers' Institute and now had a membership of 700. However, the next five years saw a reduction in membership to 250 coinciding with the rapid collapse in industrial output as the government sought to reduce the national debt. Again in 1924 on incorporation the organisation changed its name to the Institute of Industrial Welfare Workers (IIWW). Minnie Louise Haskins, the author of the famous poem \"The Gate of the Year\" and a lecturer at LSE, was closely involved with the IIWW and edited its monthly bulletin. As a consequence of the activities of welfare workers during the General Strike of 1926 distrust in the welfare movement grew amongst trade unions which saw a new breed of 'labour managers' part of 'management' which was reflected in 1931 when the IIWW became the Institute of Labour Management and its magazine, rebranded \"Labour Management\". Members of the institute experienced new demands during the 1930s. Firstly, economic growth resulted in a shortfall in skilled labour as companies competed to recruit workers. Secondly, this was followed by lay-offs and industrial action as a worldwide depression took hold, particularly in the industrially-focussed north of England. This influence also had the effect of increasing the number of male members of the institute which significantly shifted the gender balance of the membership. Thirdly, new human relations practices developed in the US were finding their way into more enlightened business which invested in their employees through training and provision of such things as salary benefits, pensions and paid holidays. In 1938 the first of the institute branches opened in the Republic of Ireland. In 1939 membership stood at 760. Mirroring the First World War during World War II, the government insisted on personnel officers to be deployed in factories engaged in war-related production.\n\nDirectly after the Second World War the incoming Labour government instigated a policy of industry nationalization and launched the National Health Service, further increasing the role of personnel professions in handling public sector recruitment, retention, payroll, training, and industrial relations issues. The enlightened practices of large American corporations, some of which had adopted the ideas of human relations thinkers, such as Elton Mayo, and the Civil Service in the field of personnel management were being taken up by the larger UK companies. Both influences were reflected in a further change of name in 1946 to the Institute of Personnel Management (IPM).\n\nThe 1950s were marked by government efforts to improve productivity both through introducing more modern management practices and increase labor supply through encouraging migration of people from the British Commonwealth also known as the Windrush generation. In 1955, responding to these changes, the IPM sought to increase the professional standards and standing of its members by introducing an externally moderated examination scheme, and restricting entry to full membership to fully qualified or practising personnel officers over age 35 with several years' experience. Membership in 1956 stood at 3,979.\n\nDuring the 1960s, 1970s and 1980s influences on the UK workplace ranged from a series of technological revolutions, economic pressures from entry into the Common Market and impact of globalization, deregulation of the financial services industry (the Big Bang). Government intervention in industrial relations and the growth of health and safety, equality, collective and recruitment and employment legislation encouraged new specialisms to develop in the function. In some companies a shift was seen from reactive personnel management processes towards what became known as strategic human resource management practices, or in shorthand HRM. All these factors also influenced a steady increase in membership, which in 1987 was 31,400. The IPM underwent a period of contemplation during which it considered whether; to shift towards an HRM approach as its counterpart in the US the Society for Human Resource Management had already done, or to maintain its focus on the more traditional personnel management practices where the majority of its members still operated, or find a third way reflecting a more UK-focused approach. In 1994, a merger took place between the IPM and the Institute of Training and Development (ITD). The new organisation which had 70,000 members was named the Institute of Personnel and Development (IPD), sought to represent the range of professionals engaged in one or more elements of people management.\n\nChartered status was achieved in 2000 and the IPD was incorporated under Royal Charter from 1 July of that year to become known as the Chartered Institute of Personnel and Development (CIPD) and reported it had a membership of 120,000 practitioners. In June 2013 the CIPD commemorated its centenary year.\n\nAs of June 2017 the CIPD reported it had over 145,000 members working or studying in the UK and internationally. Following the re-designation of chartered and non-chartered membership status in 2010 the membership structure comprises:\n\n\n\nTo be eligible for Academic Membership of the CIPD you need to be an individual working in the field of teaching or research in HRM or an HRM related area.\n\n\n\nCIPD, also is a training provider of professional HR and L&D qualification as such, the non profit registered charity is incorporated with a Royal Charters as well as, its listed as an awarding body and learning provider of professional qualification which is officially recognized by the government of United Kingdom's Ofqual, CCEA and Qualification Wales. This mean the CIPD qualifications are recognized nationally and internationally. This also gives CIPD accreditation powers to choose which university meet there strict requirements for accreditation of undergraduate, postgraduate and MSC HR and L&D progammes, which is highly sought out by UK and International universities.\n\nOn an annual basis the institute conducts regular surveys on reward management, employee expectations and attitudes to pay and benefits, resourcing and talent planning, and learning and development. The CIPD's 2015 research programme includes specific projects on; the behavioural sciences and learning, people management in small and medium-sized enterprises, social media and technology impacts in the workplace, leadership and management development, valuing the impact of an organisation's people on business performance, the 'megatrends' shaping the labour market and the future workplace. Periodic research reports are published for members on the results from surveys and the findings from research projects.\n\nThe CIPD contributes on public policy issues on behalf of its members by using its in-house research team and draws on the professional experience of its members to develop responses to public policy issues including government consultations. It researches and publishes surveys and responds to media enquiries on the range of human resource issues such as labour markets, reward and employment policy. Commentating on labour market economics and trends has become an increasingly important feature of the CIPD's services to members. The institute's chief economist (currently Mark Beatson) provides economic intelligence to members via CIPD publications and events and as its key spokesperson on labour market economic analysis and forecasting is involved in promoting the institute in the national and business media.\n\nThe CIPD is represented at local level through its 52 branches in the United Kingdom, Republic of Ireland, Channel Islands, Isle of Man and Gibraltar. The branches provide learning and networking opportunities, events, information services, and membership and upgrading help for CIPD members and students.\n\nThe CIPD runs a training programme for international HR practitioners and has links with European and World Federations of HR. The CIPD hosted the 2008 WFPMA World HR Congress.\n\nBased in Dublin, CIPD Ireland has over 6,000 members spread in seven regional branches.\n\nThe Singapore Management University (SMU) has become the first university in Asia to offer a master's degree accredited by the Chartered Institute of Personnel and Development (CIPD). SMU joins prestigious universities such as the London School of Economics and Political Science (LSE) and Kings College London to deliver the CIPD's premier level accredited qualifications.\nThe CIPD website provides podcasts, an RSS news feed, blogs and a professional discussion forum.\n\nCIPD Enterprises Limited is the wholly owned subsidiary of the CIPD. Commercial services supplied by CIPD Enterprises include:\n\n\n\n"}
{"id": "3931871", "url": "https://en.wikipedia.org/wiki?curid=3931871", "title": "Code morphing", "text": "Code morphing\n\nCode morphing is an approach used in obfuscating software to protect software applications from reverse engineering, analysis, modifications, and cracking. This technology protects intermediate level code such as compiled from Java and .NET languages (Oxygene, C#, Visual Basic, etc.) rather than binary object code. Code morphing breaks up the protected code into several processor commands or small command snippets and replaces them by others, while maintaining the same end result. Thus the protector obfuscates the code at the intermediate level.\n\nCode morphing is a multilevel technology containing hundreds of unique code transformation patterns. In addition this technology transforms some intermediate layer commands into virtual machine commands (like p-code). Code morphing does not protect against runtime tracing, which can reveal the execution logic of any protected code.\n\nUnlike other code protectors, there is no concept of code decryption with this method. Protected code blocks are always in the executable state, and they are executed (interpreted) as transformed code. The original intermediate code is absent to a certain degree, but deobfuscation can still give a clear view of the original code flow.\n\nCode morphing is also used to refer to the just-in-time compilation technology used in Transmeta processors such as the Crusoe and Efficeon to implement the x86 instruction set architecture. \n\nCode morphing is often used in obfuscating the copy protection or other checks that a program makes to determine whether it is a valid, authentic installation, or an unauthorized copy, in order to make the removal of the copy-protection code more difficult than would otherwise be the case.\n\n"}
{"id": "6512", "url": "https://en.wikipedia.org/wiki?curid=6512", "title": "Coercion", "text": "Coercion\n\nCoercion () is the practice of forcing another party to act in an involuntary manner by use of threats or force. It involves a set of various types of forceful actions that violate the free will of an individual to induce a desired response, for example: a bully demanding lunch money from a student or the student gets beaten. These actions may include extortion, blackmail, torture, threats to induce favors, or even sexual assault. In law, coercion is codified as a duress crime. Such actions are used as leverage, to force the victim to act in a way contrary to their own interests. Coercion may involve the actual infliction of physical pain/injury or psychological harm in order to enhance the credibility of a threat. The threat of further harm may lead to the cooperation or obedience of the person being coerced.\n\nThe purpose of coercion is to substitute one's aims to those of the victim. For this reason, many social philosophers have considered coercion as the polar opposite to freedom.\n\nVarious forms of coercion are distinguished: first on the basis of the \"kind of injury\" threatened, second according to its \"aims\" and \"scope\", and finally according to its \"effects\", from which its legal, social, and ethical implications mostly depend.\n\nPhysical coercion is the most commonly considered form of coercion, where the content of the conditional threat is the use of force against a victim, their relatives or property. An often used example is \"putting a gun to someone's head\" (\"at gunpoint\") or putting a \"knife under the throat\" (\"at knifepoint\" or cut-throat) to compel action or the victim gets killed or injured. These are so common that they are also used as metaphors for other forms of coercion.\n\nArmed forces in many countries use firing squads to maintain discipline and intimidate the masses, or opposition, into submission or silent compliance. However, there also are nonphysical forms of coercion, where the threatened injury does not immediately imply the use of force. Byman and Waxman (2000) define coercion as \"the use of threatened force, including the limited use of actual force to back up the threat, to induce an adversary to behave differently than it otherwise would.\" Coercion does not in many cases amount to destruction of property or life since compliance is the goal.\n\nIn psychological coercion, the threatened injury regards the victim's relationships with other people. The most obvious example is \"blackmail\", where the threat consists of the dissemination of damaging information. However, many other types are possible e.g. \"emotional blackmail\", which typically involves threats of rejection from or disapproval by a peer-group, or creating feelings of guilt/obligation via a display of anger or hurt by someone whom the victim loves or respects. Another example is coercive persuasion.\n\nPsychological coercion – along with the other varieties – was extensively and systematically used by the government of the People's Republic of China during the \"Thought Reform\" campaign of 1951–1952. The process – carried out partly at \"revolutionary universities\" and partly within prisons – was investigated and reported upon by Robert Jay Lifton, then Research Professor of Psychiatry at Yale University: see Lifton (1961). The techniques used by the Chinese authorities included a technique derived from standard group psychotherapy, which was aimed at forcing the victims (who were generally intellectuals) to produce detailed and sincere ideological \"confessions\". For instance, a professor of formal logic called Chin Yueh-lin – who was then regarded as China's leading authority on his subject – was induced to write: \"The new philosophy [of Marxism-Leninism], being scientific, is the supreme truth\" [Lifton (1961) p. 545].\n\n\n"}
{"id": "1745319", "url": "https://en.wikipedia.org/wiki?curid=1745319", "title": "Community-based conservation", "text": "Community-based conservation\n\nCommunity-based conservation is a conservation movement that emerged in the 1980s through escalating protests and subsequent dialogue with local communities affected by international attempts to protect the biodiversity of the earth. Older conservation movements disregarded the interests of local inhabitants. This stems from the Western idea on which the conservation movement was founded, of nature being separate from culture. The object of community-based conservation is to incorporate improvement to the lives of local people while conserving areas through the creation of national parks or wildlife refuges. While there have been some notable successes, unfortunately community-based conservation has often been ineffective because of inadequate resources, uneven implementation, and over-wishful planning. Some critics have also complained about often unintended neocolonialist undertones involved in the particular conservation projects.\n\nThe first protected areas around the world such as Yosemite in 1864 and Yellowstone National Park in 1872 were founded by the colonial or classical conservation method. Classical conservation created protected areas to protect wilderness and wildlife areas of pristine wilderness that was untouched and uninhabited by humans. All people inhabiting these areas were removed from the land and displaced onto marginal land surrounding or near by the newly protected land. It is estimated that 20 million people were displaced from their land. This conservation strategy was used widely until the 1970s when indigenous people started to fight for their rights and land. In 1975 the International Union for Conservation of Nature (IUCN) and the World Parks Congress recognized the rights of indigenous people and to recognize their rights of the protected areas. More policy changes came about that increased the rights of indigenous people. Community-based conservation came into action from these changes.\n\nOne strategy of community-based conservation is co-management or joint management of a protected area. Co-management combines local peoples’ traditional knowledge of the environment with modern scientific knowledge of scientists. This combination of knowledge can lead to increased biodiversity and better management of the protected area.\n\n"}
{"id": "13984693", "url": "https://en.wikipedia.org/wiki?curid=13984693", "title": "Continual improvement process", "text": "Continual improvement process\n\nA continual improvement process, also often called a continuous improvement process (abbreviated as CIP or CI), is an ongoing effort to improve products, services, or processes. These efforts can seek \"incremental\" improvement over time or \"breakthrough\" improvement all at once. Delivery (customer valued) processes are constantly evaluated and improved in the light of their efficiency, effectiveness and flexibility.\n\nSome see CIPs as a meta-process for most management systems (such as business process management, quality management, project management, and program management). W. Edwards Deming, a pioneer of the field, saw it as part of the 'system' whereby feedback from the process and customer were evaluated against organisational goals. The fact that it can be called a management process does not mean that it needs to be executed by 'management'; but rather merely that it makes decisions about the implementation of the delivery process and the design of the delivery process itself.\n\nA broader definition is that of the Institute of Quality Assurance who defined \"continuous improvement as a gradual never-ending change which is: '... focused on increasing the effectiveness and/or efficiency of an organisation to fulfil its policy and objectives. It is not limited to quality initiatives. Improvement in business strategy, business results, customer, employee and supplier relationships can be subject to continual improvement. Put simply, it means ‘getting better all the time’.' \"\n\nSome successful implementations use the approach known as kaizen (the translation of kai (“change”) zen (“good”) is “improvement”). This method became famous from Imai's 1986 book \"Kaizen: The Key to Japan's Competitive Success.\"\n\nKey features of kaizen include:\n\nThe elements above are the more tactical elements of CIP. The more strategic elements include deciding how to increase the value of the delivery process output to the customer (effectiveness) and how much flexibility is valuable in the process to meet changing needs.\n\nThe CIP-concept is also used in environmental management systems (EMS), such as ISO 14000 and EMAS. The term \"continual improvement\", not \"continuous improvement\", is used in ISO 14000, and is understood to refer to an ongoing series of small or large-scale improvements which are each done discretely, i.e. in a step-wise fashion. Several differences exist between the CIP concept as it is applied in quality management and environmental management. CIP in EMS aims to improve the natural consequences of products and activities, not the products and activities as such. Secondly, there is no client-orientation in EMS-related CIP. Also, CIP in EMS is not limited to small, incremental improvements as in Kaizen, it also includes innovations of any scale.\n\nIn the late 1990s, the developers of the ISO 9001:2000 standard—which addressed quality management systems and principles—debated whether or not to update the use of the word \"continuous\" to \"continual\". ISO Technical Committee 176 and regulatory representatives ultimately decided that \"continuous was unenforceable because it meant an organization had to improve minute by minute, whereas, continual improvement meant step-wise improvement or improvement in segments\". The committee reportedly did not base the change on dictionary definitions or the standard's vocabulary. This change ran contrary to the common usage of \"continuous\" in the standard and other prior business management documentation.\n\n"}
{"id": "8398", "url": "https://en.wikipedia.org/wiki?curid=8398", "title": "Dimension", "text": "Dimension\n\nIn physics and mathematics, the dimension of a mathematical space (or object) is informally defined as the minimum number of coordinates needed to specify any point within it. Thus a line has a dimension of one because only one coordinate is needed to specify a point on itfor example, the point at 5 on a number line. A surface such as a plane or the surface of a cylinder or sphere has a dimension of two because two coordinates are needed to specify a point on itfor example, both a latitude and longitude are required to locate a point on the surface of a sphere. The inside of a cube, a cylinder or a sphere is three-dimensional because three coordinates are needed to locate a point within these spaces.\n\nIn classical mechanics, space and time are different categories and refer to absolute space and time. That conception of the world is a four-dimensional space but not the one that was found necessary to describe electromagnetism. The four dimensions of spacetime consist of events that are not absolutely defined spatially and temporally, but rather are known relative to the motion of an observer. Minkowski space first approximates the universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. Ten dimensions are used to describe string theory, eleven dimensions can describe supergravity and M-theory, and the state-space of quantum mechanics is an infinite-dimensional function space.\n\nThe concept of dimension is not restricted to physical objects. s frequently occur in mathematics and the sciences. They may be parameter spaces or configuration spaces such as in Lagrangian or Hamiltonian mechanics; these are abstract spaces, independent of the physical space we live in.\n\nIn mathematics, the dimension of an object is, roughly speaking, the number of degrees of freedom of a point that moves on this object. In other words, the dimension is the number of independent parameters or coordinates that are needed for defining the position of a point that is constrained to be on the object. For example, the dimension of a point is zero; the dimension of a line is one, as a point can move on a line in only one direction (or its opposite); the dimension of a plane is two, etc.\n\nThe dimension is an intrinsic property of an object, in the sense that it is independent of the dimension of the space in which the object is or can be embedded. For example, a curve, such as a circle is of dimension one, because the position of a point on a curve is determined by its signed distance along the curve to a fixed point on the curve. This is independent from the fact that a curve cannot be embedded in a Euclidean space of dimension lower than two, unless if it is a line.\n\nThe dimension of Euclidean -space is . When trying to generalize to other types of spaces, one is faced with the question \"what makes -dimensional?\" One answer is that to cover a fixed ball in by small balls of radius , one needs on the order of such small balls. This observation leads to the definition of the Minkowski dimension and its more sophisticated variant, the Hausdorff dimension, but there are also other answers to that question. For example, the boundary of a ball in looks locally like and this leads to the notion of the inductive dimension. While these notions agree on , they turn out to be different when one looks at more general spaces.\n\nA tesseract is an example of a four-dimensional object. Whereas outside mathematics the use of the term \"dimension\" is as in: \"A tesseract \"has four dimensions\"\", mathematicians usually express this as: \"The tesseract \"has dimension 4\"\", or: \"The dimension of the tesseract \"is\" 4\".\n\nAlthough the notion of higher dimensions goes back to René Descartes, substantial development of a higher-dimensional geometry only began in the 19th century, via the work of Arthur Cayley, William Rowan Hamilton, Ludwig Schläfli and Bernhard Riemann. Riemann's 1854 Habilitationsschrift, Schläfli's 1852 \"Theorie der vielfachen Kontinuität\", and Hamilton's discovery of the quaternions and John T. Graves' discovery of the octonions in 1843 marked the beginning of higher-dimensional geometry.\n\nThe rest of this section examines some of the more important mathematical definitions of dimension.\n\nThe dimension of a vector space is the number of vectors in any basis for the space, i.e. the number of coordinates necessary to specify any vector. This notion of dimension (the cardinality of a basis) is often referred to as the \"Hamel dimension\" or \"algebraic dimension\" to distinguish it from other notions of dimension. \n\nFor the non-free case, this generalizes to the notion of the length of a module.\n\nThe uniquely defined dimension of every connected topological manifold can be calculated. A connected topological manifold is locally homeomorphic to Euclidean -space, in which the number is the manifold's dimension.\n\nFor connected differentiable manifolds, the dimension is also the dimension of the tangent vector space at any point.\n\nIn geometric topology, the theory of manifolds is characterized by the way dimensions 1 and 2 are relatively elementary, the high-dimensional cases are simplified by having extra space in which to \"work\"; and the cases and are in some senses the most difficult. This state of affairs was highly marked in the various cases of the Poincaré conjecture, where four different proof methods are applied.\n\nThe dimension of a manifold depends on the base field with respect to which Euclidean space is defined. While analysis usually assumes a manifold to be over the real numbers, it is sometimes useful in the study of complex manifolds and algebraic varieties to work over the complex numbers instead. A complex number (\"x\" + \"iy\") has a real part \"x\" and an imaginary part \"y\", where x and y are both real numbers; hence, the complex dimension is half the real dimension. \n\nConversely, in algebraically unconstrained contexts, a single complex coordinate system may be applied to an object having two real dimensions. For example, an ordinary two-dimensional spherical surface, when given a complex metric, becomes a Riemann sphere of one complex dimension.\n\nThe dimension of an algebraic variety may be defined in various equivalent ways. The most intuitive way is probably the dimension of the tangent space at any Regular point of an algebraic variety. Another intuitive way is to define the dimension as the number of hyperplanes that are needed in order to have an intersection with the variety that is reduced to a finite number of points (dimension zero). This definition is based on the fact that the intersection of a variety with a hyperplane reduces the dimension by one unless if the hyperplane contains the variety.\n\nAn algebraic set being a finite union of algebraic varieties, its dimension is the maximum of the dimensions of its components. It is equal to the maximal length of the chains formula_1 of sub-varieties of the given algebraic set (the length of such a chain is the number of \"formula_2\").\n\nEach variety can be considered as an algebraic stack, and its dimension as variety agrees with its dimension as stack. There are however many stacks which do not correspond to varieties, and some of these have negative dimension. Specifically, if \"V\" is a variety of dimension \"m\" and \"G\" is an algebraic group of dimension \"n\" acting on \"V\", then the quotient stack [\"V\"/\"G\"] has dimension \"m\"−\"n\".\n\nThe Krull dimension of a commutative ring is the maximal length of chains of prime ideals in it, a chain of length \"n\" being a sequence formula_3 of prime ideals related by inclusion. It is strongly related to the dimension of an algebraic variety, because of the natural correspondence between sub-varieties and prime ideals of the ring of the polynomials on the variety.\n\nFor an algebra over a field, the dimension as vector space is finite if and only if its Krull dimension is 0.\n\nFor any normal topological space , the Lebesgue covering dimension of is defined to be \"n\" if \"n\" is the smallest integer for which the following holds: any open cover has an open refinement (a second open cover where each element is a subset of an element in the first cover) such that no point is included in more than elements. In this case dim . For a manifold, this coincides with the dimension mentioned above. If no such integer exists, then the dimension of is said to be infinite, and one writes dim . Moreover, has dimension −1, i.e. dim if and only if is empty. This definition of covering dimension can be extended from the class of normal spaces to all Tychonoff spaces merely by replacing the term \"open\" in the definition by the term \"functionally open\".\n\nAn inductive dimension may be defined inductively as follows. Consider a discrete set of points (such as a finite collection of points) to be 0-dimensional. By dragging a 0-dimensional object in some direction, one obtains a 1-dimensional object. By dragging a 1-dimensional object in a \"new direction\", one obtains a 2-dimensional object. In general one obtains an ()-dimensional object by dragging an -dimensional object in a \"new\" direction. The inductive dimension of a topological space may refer to the \"small inductive dimension\" or the \"large inductive dimension\", and is based on the analogy that balls have -dimensional boundaries, permitting an inductive definition based on the dimension of the boundaries of open sets.\n\nSimilarly, for the class of CW complexes, the dimension of an object is the largest for which the -skeleton is nontrivial. Intuitively, this can be described as follows: if the original space can be continuously deformed into a collection of higher-dimensional triangles joined at their faces with a complicated surface, then the dimension of the object is the dimension of those triangles.\n\nThe Hausdorff dimension is useful for studying structurally complicated sets, especially fractals. The Hausdorff dimension is defined for all metric spaces and, unlike the dimensions considered above, can also have non-integer real values. The box dimension or Minkowski dimension is a variant of the same idea. In general, there exist more definitions of fractal dimensions that work for highly irregular sets and attain non-integer positive real values. Fractals have been found useful to describe many natural objects and phenomena.\n\nEvery Hilbert space admits an orthonormal basis, and any two such bases for a particular space have the same cardinality. This cardinality is called the dimension of the Hilbert space. This dimension is finite if and only if the space's Hamel dimension is finite, and in this case the two dimensions coincide.\n\nClassical physics theories describe three physical dimensions: from a particular point in space, the basic directions in which we can move are up/down, left/right, and forward/backward. Movement in any other direction can be expressed in terms of just these three. Moving down is the same as moving up a negative distance. Moving diagonally upward and forward is just as the name of the direction implies; \"i.e.\", moving in a linear combination of up and forward. In its simplest form: a line describes one dimension, a plane describes two dimensions, and a cube describes three dimensions. (See Space and Cartesian coordinate system.)\n\nA temporal dimension is a dimension of time. Time is often referred to as the \"fourth dimension\" for this reason, but that is not to imply that it is a spatial dimension. A temporal dimension is one way to measure physical change. It is perceived differently from the three spatial dimensions in that there is only one of it, and that we cannot move freely in time but subjectively move in one direction.\n\nThe equations used in physics to model reality do not treat time in the same way that humans commonly perceive it. The equations of classical mechanics are symmetric with respect to time, and equations of quantum mechanics are typically symmetric if both time and other quantities (such as charge and parity) are reversed. In these models, the perception of time flowing in one direction is an artifact of the laws of thermodynamics (we perceive time as flowing in the direction of increasing entropy).\n\nThe best-known treatment of time as a dimension is Poincaré and Einstein's special relativity (and extended to general relativity), which treats perceived space and time as components of a four-dimensional manifold, known as spacetime, and in the special, flat case as Minkowski space.\n\nIn physics, three dimensions of space and one of time is the accepted norm. However, there are theories that attempt to unify the four fundamental forces by introducing extra dimensions. Most notably, superstring theory requires 10 spacetime dimensions, and originates from a more fundamental 11-dimensional theory tentatively called M-theory which subsumes five previously distinct superstring theories. To date, no experimental or observational evidence is available to support the existence of these extra dimensions. If extra dimensions exist, they must be hidden from us by some physical mechanism. One well-studied possibility is that the extra dimensions may be \"curled up\" at such tiny scales as to be effectively invisible to current experiments. Limits on the size and other properties of extra dimensions are set by particle experiments such as those at the Large Hadron Collider.\n\nAt the level of quantum field theory, Kaluza–Klein theory unifies gravity with gauge interactions, based on the realization that gravity propagating in small, compact extra dimensions is equivalent to gauge interactions at long distances. In particular when the geometry of the extra dimensions is trivial, it reproduces electromagnetism. However at sufficiently high energies or short distances, this setup still suffers from the same pathologies that famously obstruct direct attempts to describe quantum gravity. Therefore, these models still require a UV completion, of the kind that string theory is intended to provide. In particular, superstring theory requires six compact dimensions forming a Calabi–Yau manifold. Thus Kaluza-Klein theory may be considered either as an incomplete description on its own, or as a subset of string theory model building.\nIn addition to small and curled up extra dimensions, there may be extra dimensions that instead aren't apparent because the matter associated with our visible universe is localized on a subspace. Thus the extra dimensions need not be small and compact but may be large extra dimensions. D-branes are dynamical extended objects of various dimensionalities predicted by string theory that could play this role. They have the property that open string excitations, which are associated with gauge interactions, are confined to the brane by their endpoints, whereas the closed strings that mediate the gravitational interaction are free to propagate into the whole spacetime, or \"the bulk\". This could be related to why gravity is exponentially weaker than the other forces, as it effectively dilutes itself as it propagates into a higher-dimensional volume.\n\nSome aspects of brane physics have been applied to cosmology. For example, brane gas cosmology attempts to explain why there are three dimensions of space using topological and thermodynamic considerations. According to this idea it would be because three is the largest number of spatial dimensions where strings can generically intersect. If initially there are lots of windings of strings around compact dimensions, space could only expand to macroscopic sizes once these windings are eliminated, which requires oppositely wound strings to find each other and annihilate. But strings can only find each other to annihilate at a meaningful rate in three dimensions, so it follows that only three dimensions of space are allowed to grow large given this kind of initial configuration.\n\nExtra dimensions are said to be universal if all fields are equally free to propagate within them.\n\nSome complex networks are characterized by fractal dimensions. The concept of dimension can be generalized to include networks embedded in space. The dimension characterize their spatial constraints.\n\nScience fiction texts often mention the concept of \"dimension\" when referring to parallel or alternate universes or other imagined planes of existence. This usage is derived from the idea that to travel to parallel/alternate universes/planes of existence one must travel in a direction/dimension besides the standard ones. In effect, the other universes/planes are just a small distance away from our own, but the distance is in a fourth (or higher) spatial (or non-spatial) dimension, not the standard ones.\n\nOne of the most heralded science fiction stories regarding true geometric dimensionality, and often recommended as a starting point for those just starting to investigate such matters, is the 1884 novella \"Flatland\" by Edwin A. Abbott. Isaac Asimov, in his foreword to the Signet Classics 1984 edition, described \"Flatland\" as \"The best introduction one can find into the manner of perceiving dimensions.\"\n\nThe idea of other dimensions was incorporated into many early science fiction stories, appearing prominently, for example, in Miles J. Breuer's \"The Appendix and the Spectacles\" (1928) and Murray Leinster's \"The Fifth-Dimension Catapult\" (1931); and appeared irregularly in science fiction by the 1940s. Classic stories involving other dimensions include Robert A. Heinlein's \"—And He Built a Crooked House\" (1941), in which a California architect designs a house based on a three-dimensional projection of a tesseract; and Alan E. Nourse's \"Tiger by the Tail\" and \"The Universe Between\" (both 1951). Another reference is Madeleine L'Engle's novel \"A Wrinkle In Time\" (1962), which uses the fifth dimension as a way for \"tesseracting the universe\" or \"folding\" space in order to move across it quickly. The fourth and fifth dimensions were also a key component of the book \"The Boy Who Reversed Himself\" by William Sleator.\n\nImmanuel Kant, in 1783, wrote: \"That everywhere space (which is not itself the boundary of another space) has three dimensions and that space in general cannot have more dimensions is based on the proposition that not more than three lines can intersect at right angles in one point. This proposition cannot at all be shown from concepts, but rests immediately on intuition and indeed on pure intuition \"a priori\" because it is apodictically (demonstrably) certain.\"\n\n\"Space has Four Dimensions\" is a short story published in 1846 by German philosopher and experimental psychologist Gustav Fechner under the pseudonym \"Dr. Mises\". The protagonist in the tale is a shadow who is aware of and able to communicate with other shadows, but who is trapped on a two-dimensional surface. According to Fechner, this \"shadow-man\" would conceive of the third dimension as being one of time. The story bears a strong similarity to the \"Allegory of the Cave\" presented in Plato's \"The Republic\" (c. 380 BC).\n\nSimon Newcomb wrote an article for the \"Bulletin of the American Mathematical Society\" in 1898 entitled \"The Philosophy of Hyperspace\". Linda Dalrymple Henderson coined the term \"hyperspace philosophy\", used to describe writing that uses higher dimensions to explore metaphysical themes, in her 1983 thesis about the fourth dimension in early-twentieth-century art. Examples of \"hyperspace philosophers\" include Charles Howard Hinton, the first writer, in 1888, to use the word \"tesseract\"; and the Russian esotericist P. D. Ouspensky.\n\nZero\nOne\nTwo\nThree\nFour\nHigher dimensionsin mathematics\nInfinite\n\n"}
{"id": "638834", "url": "https://en.wikipedia.org/wiki?curid=638834", "title": "Economic model", "text": "Economic model\n\nIn economics, a model is a theoretical construct representing economic processes by a set of variables and a set of logical and/or quantitative relationships between them. The economic model is a simplified, often mathematical, framework designed to illustrate complex processes. Frequently, economic models posit structural parameters. A model may have various exogenous variables, and those variables may change to create various responses by economic variables. Methodological uses of models include investigation, theorizing, and fitting theories to the world.\n\nIn general terms, economic models have two functions: first as a simplification of and abstraction from observed data, and second as a means of selection of data based on a paradigm of econometric study.\n\n\"Simplification\" is particularly important for economics given the enormous complexity of economic processes. This complexity can be attributed to the diversity of factors that determine economic activity; these factors include: individual and cooperative decision processes, resource limitations, environmental and geographical constraints, institutional and legal requirements and purely random fluctuations. Economists therefore must make a reasoned choice of which variables and which relationships between these variables are relevant and which ways of analyzing and presenting this information are useful.\n\n\"Selection\" is important because the nature of an economic model will often determine what facts will be looked at, and how they will be compiled. For example, inflation is a general economic concept, but to measure inflation requires a model of behavior, so that an economist can differentiate between changes in relative prices and changes in price that are to be attributed to inflation.\n\nIn addition to their professional academic interest, the use of models include:\n\nA model establishes an \"argumentative framework\" for applying logic and mathematics that can be independently discussed and tested and that can be applied in various instances. Policies and arguments that rely on economic models have a clear basis for soundness, namely the validity of the supporting model.\n\nEconomic models in current use do not pretend to be \"theories of everything economic\"; any such pretensions would immediately be thwarted by computational infeasibility and the incompleteness or lack of theories for various types of economic behavior. Therefore, conclusions drawn from models will be approximate representations of economic facts. However, properly constructed models can remove extraneous information and isolate useful approximations of key relationships. In this way more can be understood about the relationships in question than by trying to understand the entire economic process.\n\nThe details of model construction vary with type of model and its application, but a generic process can be identified. Generally any modelling process has two steps: generating a model, then checking the model for accuracy (sometimes called diagnostics). The diagnostic step is important because a model is only useful to the extent that it accurately mirrors the relationships that it purports to describe. Creating and diagnosing a model is frequently an iterative process in which the model is modified (and hopefully improved) with each iteration of diagnosis and respecification. Once a satisfactory model is found, it should be double checked by applying it to a different data set.\n\nAccording to whether all the model variables are deterministic, economic models can be classified as stochastic or non-stochastic models; according to whether all the variables are quantitative, economic models are classified as discrete or continuous choice model; according to the model's intended purpose/function, it can be classified as\nquantitative or qualitative; according to the model's ambit, it can be classified as a general equilibrium model, a partial equilibrium model, or even a non-equilibrium model; according to the economic agent's characteristics, models can be classified as rational agent models, representative agent models etc.\n\n\nAt a more practical level, quantitative modelling is applied to many areas of economics and several methodologies have evolved more or less independently of each other. As a result, no overall model taxonomy is naturally available. We can nonetheless provide a few examples that illustrate some particularly relevant points of model construction.\n\n\n\nMost economic models rest on a number of assumptions that are not entirely realistic. For example, agents are often assumed to have perfect information, and markets are often assumed to clear without friction. Or, the model may omit issues that are important to the question being considered, such as externalities. Any analysis of the results of an economic model must therefore consider the extent to which these results may be compromised by inaccuracies in these assumptions, and a large literature has grown up discussing problems with economic models, or at least asserting that their results are unreliable.\n\nOne of the major problems addressed by economic models has been understanding economic growth. An early attempt to provide a technique to approach this came from the French physiocratic school in the Eighteenth century. Among these economists, François Quesnay should be noted, particularly for his development and use of tables he called \"Tableaux économiques\". These tables have in fact been interpreted in more modern terminology as a Leontiev model, see the Phillips reference below.\n\nAll through the 18th century (that is, well before the founding of modern political economy, conventionally marked by Adam Smith's 1776 Wealth of Nations) simple probabilistic models were used to understand the economics of insurance. This was a natural extrapolation of the theory of gambling, and played an important role both in the development of probability theory itself and in the development of actuarial science. Many of the giants of 18th century mathematics contributed to this field. Around 1730, De Moivre addressed some of these problems in the 3rd edition of \"The Doctrine of Chances\". Even earlier (1709), Nicolas Bernoulli studies problems related to savings and interest in the Ars Conjectandi. In 1730, Daniel Bernoulli studied \"moral probability\" in his book Mensura Sortis, where he introduced what would today be called \"logarithmic utility of money\" and applied it to gambling and insurance problems, including a solution of the paradoxical Saint Petersburg problem. All of these developments were summarized by Laplace in his Analytical Theory of Probabilities (1812). Clearly, by the time David Ricardo came along he had a lot of well-established math to draw from.\n\nIn the late 1980s the Brookings Institution compared 12 leading macroeconomic models available at the time. They compared the models' predictions for how the economy would respond to specific economic shocks (allowing the models to control for all the variability in the real world; this was a test of model vs. model, not a test against the actual outcome). Although the models simplified the world and started from a stable, known common parameters the various models gave significantly different answers. For instance, in calculating the impact of a monetary loosening on output some models estimated a 3% change in GDP after one year, and one gave almost no change, with the rest spread between.\n\nPartly as a result of such experiments, modern central bankers no longer have as much confidence that it is possible to 'fine-tune' the economy as they had in the 1960s and early 1970s. Modern policy makers tend to use a less activist approach, explicitly because they lack confidence that their models will actually predict where the economy is going, or the effect of any shock upon it. The new, more humble, approach sees danger in dramatic policy changes based on model predictions, because of several practical and theoretical limitations in current macroeconomic models; in addition to the theoretical pitfalls, (listed above) some problems specific to aggregate modelling are:\n\nComplex systems specialist and mathematician David Orrell wrote on this issue in his book Apollo's Arrow and explained that the weather, human health and economics use similar methods of prediction (mathematical models). Their systems—the atmosphere, the human body and the economy—also have similar levels of complexity. He found that forecasts fail because the models suffer from two problems : (i) they cannot capture the full detail of the underlying system, so rely on approximate equations; (ii) they are sensitive to small changes in the exact form of these equations. This is because complex systems like the economy or the climate consist of a delicate balance of opposing forces, so a slight imbalance in their representation has big effects. Thus, predictions of things like economic recessions are still highly inaccurate, despite the use of enormous models running on fast computers.\n\nEconomic and meteorological simulations may share a fundamental limit to their predictive powers: chaos. Although the modern mathematical work on chaotic systems began in the 1970s the danger of chaos had been identified and defined in \"Econometrica\" as early as 1958:\n\nIt is straightforward to design economic models susceptible to butterfly effects of initial-condition sensitivity.\n\nHowever, the econometric research program to identify which variables are chaotic (if any) has largely concluded that aggregate macroeconomic variables probably do not behave chaotically. This would mean that refinements to the models could ultimately produce reliable long-term forecasts. However the validity of this conclusion has generated two challenges:\n\nMore recently, chaos (or the butterfly effect) has been identified as less significant than previously thought to explain prediction errors. Rather, the predictive power of economics and meteorology would mostly be limited by the models themselves and the nature of their underlying systems (see Comparison with models in other sciences above).\n\nA key strand of free market economic thinking is that the market's invisible hand guides an economy to prosperity more efficiently than central planning using an economic model. One reason, emphasized by Friedrich Hayek, is the claim that many of the true forces shaping the economy can never be captured in a single plan. This is an argument that cannot be made through a conventional (mathematical) economic model, because it says that there are critical systemic-elements that will always be omitted from any top-down analysis of the economy.\n\n\n\n\n"}
{"id": "20311344", "url": "https://en.wikipedia.org/wiki?curid=20311344", "title": "European social model", "text": "European social model\n\nThe European social model is a common vision many European states have for a society that combines economic growth with high living standards and good working conditions. Historian Tony Judt has argued that the European social model \"binds Europe together\" in contrast to the 'American way of life'.\n\nEuropean states do not all use a single social model, but welfare states in Europe do share several broad characteristics. These generally include a commitment to full employment, social protections for all citizens, social inclusion, and democracy. Examples common among European countries include universal health care, free higher education, strong labor protections and regulations, and generous welfare programs in areas such as unemployment insurance, retirement pensions, and public housing. The Treaty of the European Community set out several social objectives: \"promotion of employment, improved living and working conditions ... proper social protection, dialogue between management and labour, the development of human resources with a view to lasting high employment and the combating of exclusion.\" Because different European states focus on different aspects of the model, it has been argued that there are four distinct social models in Europe: the Nordic, British, Mediterranean and the Continental.\n\nThe general outlines of a European social model emerged during the post-war boom. Tony Judt lists a number of causes: the abandonment of protectionism, the baby boom, cheap energy, and a desire to catch up with living standards enjoyed in the United States. The European social model also enjoyed a low degree of external competition as the Soviet bloc, China and India were not yet integrated into the global economy. In recent years, it has become common to question whether the European social model is sustainable in the face of low birthrates, globalisation, Europeanisation and an ageing population.\n\nSome of the European welfare states have been described as the most well developed and extensive. A unique \"European social model\" is described in contrast with the social model existing in the US. Although each European country has its own singularities, four welfare or social models are identified in Europe:\n\nAs can be seen in the graph to the right, the Nordic model holds the highest level of social insurance. Its main characteristic is its universal provision nature which is based on the principle of \"citizenship\". Therefore, there exists a more generalised access, with lower conditionability, to the social provisions.\n\nAs regards labour market, these countries are characterised by important expenditures in active labour market policies whose aim is a rapid reinsertion of the unemployed into the labour market. These countries are also characterised by a high share of public employment. Trade unions have a high membership and an important decision-making power which induces a low wage dispersion or more equitable income distribution.\n\nThe Nordic model is also characterised by a high tax wedge.\n\nThe Continental model has some similarities with the Nordic model. Nevertheless, it has a higher share of its expenditures devoted to pensions. The model is based on the principle of \"security\" and a system of subsidies which are not conditioned to employability (for example in the case of France or Belgium, there exist subsidies whose only requirement is being older than 25).\n\nAs regards the labour market, active policies are less important than in the Nordic model and in spite of a low membership rate, trade-unions have important decision-making powers in collective agreements.\n\nAnother important aspect of the Continental model is the disability pensions.\n\nThe Anglo-Saxon model features a lower level of expenditures than the previous ones. Its main particularity is its social assistance of last resort. Subsidies are directed to a higher extent to the working-age population and to a lower extent to pensions. Access to subsidies is (more) conditioned to employability (for instance, they are conditioned on having worked previously).\n\nActive labour market policies are important. Instead, trade unions have smaller decision-making powers than in the previous models, this is one of the reasons explaining their higher income dispersion and their higher number of low-wage employments.\n\nThe Mediterranean model corresponds to southern European countries who developed their welfare state later than the previous ones (during the 1970s and 1980s). It is the model with the lowest share of expenditures and is strongly based on pensions and a low level of social assistance. There exists in these countries a higher segmentation of rights and status of persons receiving subsidies which has as one of its consequences a strongly conditioned access to social provisions.\n\nThe main characteristic of labour market policies is a rigid employment protection legislation and a frequent resort to early retirement policies as a means to improve employment conditions. Trade unions tend to have an important membership which again is one of the explanations behind a lower income dispersion than in the Anglo-Saxon model.\n\nTo evaluate the different social models, we follow the criteria used in Boeri (2002) and Sapir (2005) which consider that a social model should satisfy the following:\n\nThe graph on the right shows the reduction in inequality (as measured by the Gini index) after taking account of taxes and transfers, that is, to which extent does each social model reduce poverty without taking into account the reduction in poverty provoqued by taxes and transfers. The level of social expenditures is an indicator of the capacity of each model to reduce poverty: a bigger share of expenditures is in general associated to a higher reduction in poverty. Nevertheless, another aspect that should be taken into account is the efficiency in this poverty reduction. By this is meant that with a lower share of expenditures a higher reduction in poverty may be obtained.\n\nIn this case, the graph on the right shows that the Anglosaxon and Nordic models are more efficient than the Continental or Mediterranean ones. The Continental model appears to be the least efficient. Given its high level of social expenditures, one would expect a higher poverty reduction than that attained by this model. Remark how the Anglosaxon model is found above the average line drawn whereas the Continental is found below that line.\n\nProtection against labour market risks is generally assured by two means: \n\nAs can be seen in the graph, there is a clear trade-off between these two types of labour market instruments (remark the clear negative slope between both). Once again different European countries have chosen a different position in their use of these two mechanisms of labour market protection. These differences can be summarised as follows: \n\nEvaluating the different choices is a hard task. In general there exists consensus among economists on the fact that employment protection generates inefficiencies inside firms. Instead, there is no such consensus as regards the question of whether employment protection generates a higher level of unemployment.\n\nSapir (2005) and Boeri (2002) propose looking at the employment-to-population ratio as the best way to analyse the incentives and rewards for employment in each social model. The Lisbon Strategy initiated in 2001 established that the members of the EU should attain a 70% employment rate by 2010.\n\nIn this case, the graph shows that the countries in the Nordic and Anglosaxon model are the ones with the highest employment rate whereas the Continental and Mediterranean countries have not attained the Lisbon Strategy target.\n\nSapir (2005) proposes as a general mean to evaluate the different social models, the following two criteria: \n\nAs can be seen in the graph, according to these two criteria, the best performance is achieved by the Nordic model. The Continental model should improve its efficiency whereas the Anglosaxon model its equity. The Mediterranean model under-performs in both criteria.\n\nSome economists consider that between the Continental model and the Anglo-Saxon, the latter should be preferred given its better results in employment, which make it more sustainable in the long term, whereas the equity level depends on the preferences of each country (Sapir, 2005). Other economists argue that the Continental model cannot be considered worse than the Anglosaxon given that it is also the result of the preferences of those countries that support it (Fitoussi et al., 2000; Blanchard, 2004). This last argument can be used to justify any policy.\n\n\nLocation-specific:\n\n"}
{"id": "1960343", "url": "https://en.wikipedia.org/wiki?curid=1960343", "title": "Evolutionary epistemology", "text": "Evolutionary epistemology\n\nEvolutionary epistemology refers to three distinct topics: (1) the biological evolution of cognitive mechanisms in animals and humans, (2) a theory that knowledge itself evolves by natural selection, and (3) the study of the historical discovery of new abstract entities such as abstract number or abstract value that necessarily precede the individual acquisition and usage of such abstractions.\n\n\"Evolutionary epistemology\" can refer to a branch of epistemology that applies the concepts of biological evolution to the growth of animal and human cognition. It argues that the mind is in part genetically determined and that its structure and function reflect adaptation, a nonteleological process of interaction between the organism and its environment. A cognitive trait tending to increase inclusive fitness in a given population should therefore grow more common over time, and a trait tending to prevent its carriers from passing on their genes should show up less and less frequently.\n\n\"Evolutionary epistemology\" can also refer to a theory that applies the concepts of biological evolution to the growth of human knowledge, and argues that units of knowledge themselves, particularly scientific theories, evolve according to selection. In this case, a theory—like the germ theory of disease—becomes more or less credible according to changes in the body of knowledge surrounding it.\n\nOne of the hallmarks of evolutionary epistemology is the notion that empirical testing alone does not justify the pragmatic value of scientific theories, but rather that social and methodological processes select those theories with the closest \"fit\" to a given problem. The mere fact that a theory has survived the most rigorous empirical tests available does not, in the calculus of probability, predict its ability to survive future testing. Karl Popper used Newtonian physics as an example of a body of theories so thoroughly confirmed by testing as to be considered unassailable, but which were nevertheless overturned by Einstein's insights into the nature of space-time. For the evolutionary epistemologist, all theories are true only provisionally, regardless of the degree of empirical testing they have survived.\n\n\"Evolutionary epistemology\" can also refer to the opposite of (onto)genetic epistemology, namely phylogenetic epistemology as the historical discovery and reification of abstractions that necessarily precedes the learning of such abstractions by individuals. Piaget dismissed this possibility, stating\n\nPiaget was mistaken in so quickly dismissing the study of phylogenetic epistemology, as there is much historical data available about the origins and evolution of the various notational systems that reify different kinds of abstract entity.\n\nPopper is considered by many to have given evolutionary epistemology its first comprehensive treatment, though Donald T. Campbell coined the phrase in 1974 and Piaget alluded to it in 1974 and described the concept as one of five possible theories in \"The Origins of Intelligence in Children\" (1936).\n\n\n\n"}
{"id": "14554208", "url": "https://en.wikipedia.org/wiki?curid=14554208", "title": "Gilbert model", "text": "Gilbert model\n\nThe Gilbert model was developed by Dennis Gilbert as a means of a more effective way of classifying people in a given society into social classes.\n\nKarl Marx believed that social class is determined by ownership (or non-ownership) of the \"means of economic production\" - ownership of raw materials, farm land, coal mines, factories, etc. His theory contains the idea of a struggle between two social classes - the Bourgeoisie (the capital owners) and the Proletariat (the non-owner workers).\n\nLike Marx, Max Weber agreed that social class is determined mostly on the basis of unequal distribution of economic power and hence the unequal distribution of opportunity. He also saw that honor, status and social prestige were key factors in determining what social class people belong to. \"Life-styles\" such as where a person lives and the schools they attend are very important in determining social class. \"Life-chances\" also determined social class. If a person becomes a respectable member of society it will raise their social class. Party affiliations can also influence social class.\n\nEven though Marx's and Weber's research were both taken into consideration when trying to create an effective means of social stratification, they were not weighted the same. Although the Gilbert model is based on the assumption that class structure develops out of the economic system like the Marxist theory, it is still has much more in common with Weber's more modern theory that dealt with socialism. The aspect that Marxism takes into consideration when referring to the economy is \"what a specific person owns determines their class\" - a capitalistic viewpoint. If a man owns a factory, he is going to be in a higher social class than someone who works in the factory. In Marxist theory, the middle class is not emphasized or described, but it is clearly present in the Gilbert model. The Gilbert model focuses on occupation and, more generally on the source of income (occupation for most, but also assets, and government transfers for people at the top or bottom) and when referring to how the economic system places people in classes. The occupation of a person is directly related to a person's educational preparation because better education provides for a better occupation which in turn raises their class level. [\n\nThe six social classes that provide the basis for the Gilbert model are determined based on the assumption of how class structure develops out of the economic system.\n\n\"(Typical income: $1.5 million , mostly from assets)\"\nEven though the capitalist class is a very small class of super rich capitalists at the top of the hierarchy, its impact on economy and society is far beyond their numbers. These people contribute their money to political parties and are often owners of newspapers or television stations. They have investments that affect millions of people in the labor force. They tend to only associate with other people from their own class, rarely interacting with people from an inferior class. Even their children are usually segregated attending only the most elite preparatory schools and universities.\n\n\"(typical income $200,000; for Working rich $500,000)\"\nThe upper middle class is the group in society most shaped by formal education. A college degree is usually required and graduate studies are becoming increasingly required. Most people in this class are technicians, professionals, managers, officials, and highly successfully small business owners. At the top of this class is the growing segment of working rich, affluent professionals and business owners. Children in high school strive to prepare themselves for upper middle class jobs because these type of jobs are symbols of success. Upper-middle-class people are able to purchase status symbols such as spacious homes. They are convinced that they deserve what they have achieved and are mostly satisfied that they have achieved a proper share of the American dream.\n\n\"(Typical income $85,000)\"\nTo attain a middle class job it takes at least a high school diploma. However, many in the middle class have received some form of additional post secondary training. The most educated will become semi-professionals, or have low-level managerial jobs. Sales and craft people are also included in this social class. It is estimated that really about a third of the population is middle class.\n\n\"(Typical income $40,000)\"\nThe core of this working class is made up of semi-skilled machine operators. Clerks and salespeople whose tasks are habitual and mechanized and require practically no skill beyond literacy. Brief on the job training can also be considered to be a part of this class. It is estimated that this class includes roughly a third of the population.\n\n\"(Typical Income: $25,000)\"\nThe working poor class includes unskilled laborers, people in service jobs and some of the lower-paid factory workers. Income depends on the number of workers in the family and the amount of weeks that they work. Many have not finished high school. Unable to save money and when retired the working poor depend heavily on their social security pensions to live.\n\n\"(Typcial Income $15,000)\"\nThese people are under-employed. They suffer from low education, low employability, and/or low income. Some can not work because of their age or disability. Hard times might be magnified because they belong to a minority group who suffers discrimination in the work force\n\nAlthough the social hierarchy is most obvious at the extremes, Differences between classes begin to become blurred when moving away from one of the extremes and towards the center to where the middle and working classes are. It is difficult to get a precise classification.\n\n\n"}
{"id": "33442648", "url": "https://en.wikipedia.org/wiki?curid=33442648", "title": "Global precedence", "text": "Global precedence\n\nImages and other stimuli contain both local features (details, parts) and global features (the whole). Precedence refers to the level of processing (global or local) to which attention is first directed. Global precedence occurs when an individual more readily identifies the global feature when presented with a stimulus containing both global and local features. The global aspect of an object embodies the larger, overall image as a whole, whereas the local aspect consists of the individual features that make up this larger whole. Global processing is the act of processing a visual stimulus holistically. Although global precedence is generally more prevalent than local precedence, local preference also occurs under certain circumstances and for certain individuals. Global precedence is closely related to the Gestalt principles of grouping in that the global whole is a grouping of proximal and similar objects. Within global precedence, there is also the global interference effect, which occurs when an individual is directed to identify the local characteristic, and the global characteristic subsequently interferes by slowing the reaction time.\n\nGlobal precedence was first studied using the Navon figure, where many small letters are arranged to form a larger letter that either does or does not match. Variations of the original Navon figure include both shapes and objects. \nIndividuals presented with a Navon figure will be given one of two tasks. In one type of task, participants are told before the presentation of the stimulus whether to focus on a global or local level, and their accuracy and reaction times are recorded.\n\nIn another type of task, participants are first presented with a target stimulus, and later presented with two different visuals. One of the visuals matches the target stimulus on the global level, while the other visual matches the target stimulus on the local level. In this condition, experimenters note which of the two visuals, the global or local, is chosen to match the target stimulus.\n\nIn general, reaction time for identifying the larger letter is faster than for the smaller letters that make up the shape. Navon directed participants to focus either globally or locally to stimuli that were consistent, neutral, or conflicting on the global and local levels (see figures above). Reaction time for global identification was much faster than for local identification, showing global precedence. Additionally, global interference effect, which occurs when the global aspect is automatically processed even when attention is directed locally, causes slow reaction time. Navon's study global precedence and his stimuli, or variations of it, are still used in nearly all global precedence experiments.\n\nWhen presented with a Navon figure, there is a slight local preference for Caucasians, but East Asians show an obvious global preference and are faster and more accurate at global processing. The inclination towards global precedence is also evident in second generation Asian-Australians, but the correlation is weaker than that of recent immigrants. This could stem from the physical environment of East Asian versus Western cities, as the level of visual complexity varies across these environments. The tendency of Caucasians to process information \"analytically\" and Asians \"holistically\" has also been attributed to differences in brain structure.\n\nFor some cognitive scientists, the stark contrast in cognitive processing trends across cultures and races suggests that all studies on cognitive perception should report participants’ races to ensure valid theoretical conclusions. Especially in experiments involving spatially distributed stimuli, neglected racial or cultural differences in visual perception could skew results.\n\nGlobal precedence is not a universal phenomenon.\n\nWhen Navon figure stimuli are presented to participants from a remote African culture, the Himba, local precedence is observed although the Himba show the capabilities for both global and local processing.\n\nThis difference in precedence for Navon figure stimuli can be attributed to cultural differences in occupations, or in the practice of reading and writing. This finding dispels the idea that local precedence is a consequence or symptom of disorders, since the Himba is a normally functioning society capable of both global and local processing.\n\nStimuli are either meaningful or meaningless. For example, letters and familiar objects, like a cup, are meaningful, while unidentifiable and non-geometric forms are not. In both types of stimuli, the global advantage is observed, but the global interference effect only occurs with meaningful stimuli. In other words, when the global object is meaningful, the reaction time for identification of the local feature increases.\n\nThis supports the theory that within global precedence, global advantage and global interference rely on two separate mechanisms. Global-local interference occurs as a result of automatic processing of global objects. The theory is that the global precedence effect has a sensory mechanism active in global advantage, whereas automatic and semantic processes are active in the interference effect.\n\nCognitive processing varies across different age groups, and several studies have been done using Navon-like figures to examine the correlation between precedence and age.\n\nWhen presented with a global-local task, children and adolescents exemplify a local bias. Younger children respond slower to different types of stimuli compared to older children, and thus local precedence seems more prevalent than global precedence in perceptual organization, at least until adolescence, when the transition to globally oriented visual perception begins. The ability to encode a global shape, which is necessary for efficiently recognizing and identifying objects, increases with age. However, it has also been found that there is a bias towards global information during infancy, which may be based upon high spatial frequency information, as well as limited vision. Therefore, global precedence during the early years of life may not be upwards but rather a U-shaped development.\n\nThere is a decline of global precedence in older subjects. When presented with a Navon-like figures, young adults demonstrate global precedence enhancement in that when the number of local letters forming the global letter increases, their global precedence increases. On the other hand, there is no precedence effect or enhancement for older subjects when presented with the same task. This links global precedence to the Gestalt principles of Proximity and Continuity, and suggests that Gestalt-related deficiencies, such as decline in perceptual grouping, may underlie the decline of global precedence in older subjects.\n\nGlobal precedence decline may also relate to hemispheric specialization. The spatial frequency theory proposes that global versus local information is processed through two “channels” of low (global) versus high (local) spatial frequencies. spatial frequency measures how often a stimulus moves through space. Based upon this theory, the double frequency theory links the left hemisphere with high spatial frequencies, leading to a global precedence effect, and the right hemisphere with low spatial frequencies, leading to a local precedence effect. This suggests neuropsychological factors behind global precedence decline in there may be faster aging in the right than the left hemisphere.\n\nStudies regarding mood have shown that positive and negative cues can influence global versus local attention during image-based tasks.\n\nSome studies have shown that positive priming decreases local response time, demonstrating a lessening effect of global precedence, while negative priming increases local response time. Mood dictates one's preferences for processing type.\n\nThe result that negative priming reduces flexibility correlates to the Psi theory states that negative emotion inhibits one’s access to extension memory, reducing cognitive flexibility. This also supports the theory that positive affect increases cognitive flexibility.\n\nPositive mood priming also increases cognitive flexibility when prime words do not have individualistic specificity and when primes are visual. Positive affect does not simply promote local processing, but rather improves one’s abilities in his non-preferred dimension. For example, one preferring the local aspect of stimuli would show increased performance in identifying the global aspect and vice versa. This further supports the cognitive flexibility theory.\n\nHowever, many studies regarding global processing and affect conflict with each other. One particular study showed that individuals in happy moods are more likely than those in sad moods to identify images based on global attributes rather than local ones, contrary to other studies that have been conducted. Paying attention to global features is the standard strategy for visual processing. Therefore, if positive feelings are more frequent than negative feelings, and thus positive feelings are more accessible, then positive feelings should instigate global processing more than negative feelings because the global strategy is similarly more accessible. From a more worldly application, positive affect might aid in understanding the larger meaning of stimuli like literature or art, whereas negative affect might aid understanding more minute details within those stimuli, like particularly rhythmic words or the nuance of colors.\n\nPriming with Navon figures aides the recognition of faces, a holistic task, when the response elicited from the figure matches the precedence of the figure. For example, if the stimulus has local precedence and the participant is cued to respond with the local feature identification, his accuracy in facial recognition improves. The same occurs when global responses are asked of global stimuli.\n\nWhen a facial task requires local processing for identification, participants’ facial recognition improves when they must respond to global precedence stimuli with local responses and vice versa. They are forced to show cognitive flexibility in their responses to the Navon figure primes.\n\nOne theory explains that normal facial recognition requires automatic processes, whereas special facial recognition requires controlled processes. Automatic processes are aided by correlative stimuli and responses, while controlled processes are aided by stimuli and responses that do not correlate. This indicates that facial recognition depends on type of attention, automatic or controlled, rather than focus on global or local features.\n\nWhen identifying inverted faces, those showing stronger global precedence show a more prominent Those showing a stronger global precedence also have a greater deficit in identification abilities when the faces are inverted; their identification abilities decrease more from upright identification to inverted identification than weak global precedence individuals.\n\nThis correlates to the theory that upright faces are processed holistically, or with a special mechanism. Those with stronger global precedence should perform better at holistically processing a face upright. Stronger global precedence should show a greater decrease in accuracy of identification of inverted faces because the task relies on local processing.\n\nThe degree of global precedence one demonstrates has been found to differ in relation to the variable of an individual's field dependence. \nField dependency is the amount that one relies on Gestalt laws of perceptual organization. High field dependency corresponds to a greater bias toward the global level, while field independence corresponds to a lesser dependency on the global level.\n\nThis indicates that individual characteristics have an effect on the prevalence of global precedence and that global and local processing exist on a continuum.\n\nNeuropsychological evidence based on PET scans suggests that the global aspect of visual situations activates and is processed preferentially by the right hemisphere, whereas the local aspect of visual situations activates and is processed preferentially by the left hemisphere. The classical view of Gestalt psychology also suggests the right hemisphere is involved in the perception of wholes and thus plays a stronger role in global processing, whereas the left hemisphere involves separate local elements and therefore plays a stronger role in local processing.\n\nHowever, hemispheric specialization is relative because it depends on the experimental setting as well as the individual’s “attentional set.” In addition, stimulus type may influence the neural structures underlying hemispheric specialization. Global processing is the default strategy for most individuals, but local stimuli are often more perceptually demanding to recognize and identify, showing the effect of stimuli on visual processing.\n\nThe Navon figure has been used in relating theories regarding processing to assessing cognitive learning disabilities, such as developmental dyslexia, dyscalculia, obsessive-compulsive personality disorder, and autism.\n\nWhen given a Navon figure test, people with dyslexia have difficulty automatically identifying graphemes with phonemes, but not with identifying numbers with magnitudes. On the other hand, people with dyscalculia have difficulty automatically identifying numbers with magnitudes, but not letters and with phonemes. This suggests a dissociation between subjects with dyslexia and dyscalculia. These developmental learning disabilities do not cause general problems with identifying symbols to their mental representations, but rather create specific challenges.\n\nObsessive-compulsive personality disorder (OCPD) subjects are prone to be distracted by the local aspects of stimuli when asked to identify global aspects of figures such as the Navon figure. This is likely because individuals with OCPD characteristically have sharp, detail-oriented attentions, and tend to focus more on specifics rather than the larger context.\n\nThere are correlations between global or local performance on a task and the abilities to identify emotion and canine age for autistic children. In both cases, global responses correlate to better identification. In general, autistic children demonstrate much weaker global precedence than those without the disorder. Within the group of autistic children, those who respond more globally to a discrimination task perform better on emotion and canine age tasks.\n\nOne explanation is a possible biological dysfunction in the brain region where facial processing occurs. Research indicates that global processing, facial recognition, and emotional expression recognition are all linked to the right hemisphere. A defect in that area would explain the characteristics of autism. For further information on facial recognition and processing in individuals with autism see the autism and facial recognition section of face perception.\n"}
{"id": "3364638", "url": "https://en.wikipedia.org/wiki?curid=3364638", "title": "Green-beard effect", "text": "Green-beard effect\n\nThe green-beard effect is a thought experiment used in evolutionary biology to explain selective altruism among individuals of a species. Altruistic behaviour is paradoxical when viewed in the light of old ideas of evolutionary theory that emphasised the role of competition. The evolution of altruism is better explained through the gene-centered view of evolution, which emphasizes an interpretation of natural selection from the point of view of the gene which acts as an agent that has the metaphorical \"selfish goal\" of maximizing its own propagation. A gene for (behavioral) selective altruism can be favored by (natural) selection if the altruism is primarily directed at other individuals who share the gene. Since genes are invisible, such an effect requires perceptible markers for altruistic behaviour to occur.\n\nA green-beard effect occurs when an allele, or a set of linked alleles, produce three expressed (or phenotypic) effects: \n\nThe carrier of the gene (or a specific allele) is essentially recognizing copies of the same gene (or a specific allele) in other individuals. Whereas kin selection involves altruism to related individuals who share genes in a non-specific way, green-beard alleles promote altruism toward individuals who share a gene that is expressed by a specific phenotypic trait. Some authors also note that the green-beard effects can include \"spite\" for individuals lacking the \"green-beard\" gene. This can have the effect of delineating a subset of organisms within a population that is characterized by members who show greater cooperation toward each other, this forming a \"clique\" that can be advantageous to its members who are not necessarily kin.\n\nGreen-beard effect could increase altruism on green-beard phenotypes and therefore its presence in a population even if genes assist in the increase of genes that are not exact copies; all that is required is that they express the three required characteristics. Green-beard alleles are vulnerable to mutations that produce the perceptible trait without the helping behaviour.\n\nThe idea of a green-beard allele was proposed by William D. Hamilton in his articles of 1964, and got the name from the example used by Richard Dawkins (\"I have a green beard and I will be altruistic to anyone else with green beard\") in \"The Selfish Gene\" (1976).\n\nEvolutionary biologists have debated the potential validity of green-beard genes, suggesting that it would be extraordinarily rare for a single or even a set of linked genes to produce three complex phenotypic effects. This criticism has led some to believe that they simply cannot exist or that they only can be present in less complex organisms, such as microorganisms. Several discoveries within the past ten years have illuminated the validity of this critique.\n\nThe concept remained a merely theoretical possibility under Dawkins' selfish gene model until 1998, when a green-beard allele was first found in nature, in the red imported fire ant (\"Solenopsis invicta\"). Polygyne colony queens are heterozygous (Bb) at the Gp-9 gene locus. Their worker offspring can have both heterozygous (Bb) and homozygous (BB) genotypes. The investigators discovered that homozygous dominant (BB) queens, which in the wild form produce monogyne rather than polygyne colonies, are specifically killed when introduced into polygyne colonies, most often by heterozygous (Bb) and not homozygous (BB) workers. They concluded that the allele Gp-9 is linked to a greenbeard allele which induces workers bearing this allele to kill all queens that do not have it. A final conclusion notes that the workers are able to distinguish BB queens from Bb queens based on an odor cue.\n\nThe gene csA in the slime mould \"Dictyostelium discoideum\", discovered in 2003, codes for a cell adhesion protein which binds to gp80 proteins on other cells, allowing multicellular fruiting body formation on soil. Mixtures of csA knockout cells with wild-type cells yield spores, \"born\" from the fruiting bodies, which are 82% wild-type (WT). This is because the wild-type cells are better at adhering and more effectively combine into aggregates; knockout (KO) cells are left behind. On more adhesive but less natural substances, KO cells can adhere; WT cells, still better at adhering, sort preferentially into the stalk.\n\nIn 2006, green beard-like recognition was seen in the cooperative behavior among color morphs in side-blotched lizards, although the traits appear to be encoded by multiple loci across the genome.\n\nA more recent example, found in 2008, is a gene that makes brewer's yeast clump together in response to a toxin such as alcohol. By investigating flocculation, a type of self-adherence generally present in asexual aggregations, Smukalla \"et al.\" showed that \"S. cerevisiae\" is a model for cooperative behavior evolution. When this yeast expresses FLO1 in the laboratory, flocculation is restored. Flocculation is apparently protective for the FLO1+ cells, which are shielded from certain stresses (ethanol, for example). In addition FLO1+ cells preferentially adhere to each other. The authors therefore conclude that flocculation is driven by this greenbeard allele.\n\nA mammalian example appears to be the reproductive strategy of the wood mouse, which shows cooperation among spermatozoa. Single sperms hook in each other to form sperm-trains, which are able to move faster together than single sperm would do.\n\nIt has been suggested that speciation could be possible through the manifestation of a green-beard effect.\n\nIt has also been pointed out that both the biological and cultural aspects of language are bestowed with green beard recognition systems, thus providing insights into the evolution of language.\n\n\n"}
{"id": "29930118", "url": "https://en.wikipedia.org/wiki?curid=29930118", "title": "Housing discrimination (United States)", "text": "Housing discrimination (United States)\n\nHousing discrimination is discrimination in which an individual or family is treated unequally when trying to buy, rent, lease, sell or finance a home based on certain characteristics, such as race, class, sex, religion, national origin, and familial status. This type of discrimination can lead to housing and spatial inequality and racial segregation which, in turn, can exacerbate wealth disparities between certain groups. In the United States, housing discrimination began after the abolition of slavery as part of a federally sponsored law, but has since been made illegal; however, studies show that housing discrimination still exists.\n\nAfter the end of the Civil War and the abolition of slavery, Jim Crow laws were introduced. These laws led to the discrimination of racial and ethnic minorities, especially African Americans. Fifteen state courts obeyed ordinances that enforced the denial of housing to African American and other minority groups in white-zoned areas. These ordinances were then made illegal in the 1917 Supreme Court case, \"Buchanan v. Warley\". Following this decision, however, nineteen states legally supported \"covenants,\" or agreements, between property owners to not rent or sell any homes to racial or ethnic minorities. Although the covenants, too, were made illegal in 1948, they were still allowed to be present in private deeds. It was not until the Fair Housing Act, enacted as Title VIII of the Civil Rights Act of 1968, that the federal government made its first concrete steps to deem all types of housing discrimination unconstitutional. The act explicitly prohibits housing discrimination practices common at the time, including filtering information about a home's availability, racial steering, blockbusting, and redlining.\n\nThe Fair Housing Act was passed at the urging of President Lyndon B. Johnson. Congress passed the federal Fair Housing Act (codified at 42 U.S.C. 3601-3619, penalties for violation at 42 U.S.C. 3631), Title VIII of the Civil Rights Act of 1968, only one week after the assassination of Martin Luther King, Jr.\n\nThe primary purpose of the Fair Housing Law of 1968 is to protect the buyer/renter of a dwelling from seller/landlord discrimination. Its primary prohibition makes it unlawful to refuse to sell, rent to, or negotiate with any person because of that person's inclusion in a protected class. The goal is a unitary housing market in which a person's background (as opposed to financial resources) does not arbitrarily restrict access. Calls for open housing were issued early in the twentieth century, but it was not until after World War II that concerted efforts to achieve it were undertaken.\n\nThe Fair Housing Act (Title VIII of the Civil Rights Act of 1968) introduced meaningful federal enforcement mechanisms. It outlawed:\n\n\nWhen the Fair Housing Act was first enacted, it prohibited discrimination only on the basis of race, color, religion, sex, and national origin. In 1988, disability and familial status (the presence or anticipated presence of children under 18 in a household) were added (further codified in the Americans with Disabilities Act of 1990). In certain circumstances, the law allows limited exceptions for discrimination based on sex, religion, or familial status.\n\nThe United States Department of Housing and Urban Development is the federal executive department with the statutory authority to administer and enforce the Fair Housing Act. The Secretary of Housing and Urban Development has delegated fair housing enforcement and compliance activities to HUD's Office of Fair Housing and Equal Opportunity (FHEO) and HUD's Office of General Counsel. FHEO is one of the United States' largest federal civil rights agencies. It has a staff of more than 600 people located in 54 offices around the United States. As of June 2014, the head of FHEO is Assistant Secretary for Fair Housing and Equal Opportunity Gustavo Velasquez, whose appointment was confirmed on June 19, 2014.\n\nIndividuals who believe they have experienced housing discrimination can file a complaint with FHEO at no charge. FHEO funds and has working agreements with many state and local governmental agencies where \"substantially equivalent\" fair housing laws are in place. Under these agreements, FHEO refers complaints to the state or locality where the alleged incident occurred, and those agencies investigate and process the case instead of FHEO. This is known as FHEO's Fair Housing Assistance Program (or \"FHAP\").\n\nThere is also a network of private, non-profit fair housing advocacy organizations throughout the country. Some are funded by FHEO's Fair Housing Initiatives Program (or \"FHIP\"), and some operate with private donations or grants from other sources.\n\nVictims of housing discrimination need not go through HUD or any other governmental agency to pursue their rights, however. The Fair Housing Act confers jurisdiction to hear cases on federal district courts. The United States Department of Justice also has jurisdiction to file cases on behalf of the United States where there is a pattern and practice of discrimination or where HUD has found discrimination in a case and either party elects to go to federal court instead of continuing in the HUD administrative process.\n\nThe Fair Housing Act applies to landlords renting or leasing space in their primary residence only if the residence contains living quarters occupied or intended to be occupied by three or more other families living independently of each other, such as an owner-occupied rooming house.\n\nThe Fair Housing Act has been strengthened since its adoption in 1968, but enforcement continues to be a concern among housing advocates. According to a 2010 evaluation of Analysis of Impediments (AI) reports done by the Government Accountability Office, enforcement is particularly inconsistent across local jurisdictions.\n\nThe federal government has passed other initiatives in addition to the Fair Housing Act of 1968. The Equal Credit Opportunity Act of 1974 and Community Reinvestment Act of 1977 helped with discrimination in mortgage lending and lenders' problems with credit needs. The Fair Housing Amendments Act of 1988 was passed to give the federal government the power to enforce the original Fair Housing Act to correct past problems with enforcement. The amendment established a system of administrative law judges to hear cases brought to them by the United States Department of Housing and Urban Development and to levy fines. Because of the relationship between housing discrimination cases and private agencies, the federal government passed the two initiatives. The Fair Housing Assistance Program of 1984 was passed to assist public agencies with processing complaints, and the Fair Housing Initiatives program of 1986 supported private and public fair housing agencies in their activities, such as auditing. Between 1990 and 2001 these two programs have resulted in over one thousand housing discrimination lawsuits and over $155 million in financial recovery. However, the lawsuits and financial recoveries generated from fair housing discrimination cases only scratches the surface of all instances of discrimination. Silverman and Patterson concluded that the underfunding and poor implementation of federal, state and local policies designed to address housing discrimination results in less than 1% of all instances of discrimination being addressed. Moreover, they found that local nonprofits and administrators responsible for enforcing fair housing laws had a tendency to downplay discrimination based on family status and race when designing implementation strategies.\n\nThe United States Census has shown that ethnic and racial minorities living in concentrated, high-poverty areas had actually increased following the passage of the Fair Housing Act from 1970 to 1990. African-Americans residing in these areas rose from 16 percent to 24 percent, and Hispanics living in these areas have increased from 10 percent to 15 percent. While this does not necessarily point to evidence of housing discrimination, it does mirror the phenomenon of white flight—the mass exodus during the 1970s and '80s of European-Americans from cities to the suburbs that left only one-fourth of the Anglo population still living in metropolitan areas. American sociologist Douglas Massey, in his essay, \"The New Geography of Inequality in Urban America\", argues that this new racial geography in the United States has laid the foundation for housing discrimination to occur in order to keep up the status quo.\n\nSociologists Vincent J. Roscigno, Diana L. Karafin, and Griff Tester have determined that the variety of actions that constitute housing discrimination can be classified as either exclusionary or nonexclusionary.\n\nExclusionary discrimination practices refer to \"actions and practices that exclude an individual or family from obtaining the housing of their choosing.\" These forms of housing discrimination occur at various stages of the rental or sales process. The majority of discriminatory actors in exclusionary discrimination are landlords and landowners, as they have the positional power and direct access to the individual or family and the housing being sought. Other discriminatory actors or institutions responsible for exclusion include real estate, insurance, and banking and lending agents and institutions.\n\nMost exclusionary discrimination cases entail forms of outright exclusion, such as a direct refusal to rent to a prospective tenant, direct refusal to sell or discuss the sale of a property, or the false representation of homes or apartments available for sale or rent. Although this outright exclusion is often made up of subtle actions, like lying about standards for rental qualification to disqualify certain individuals, it sometimes also comes with overt verbal abuse and slurs.\n\nOther exclusionary cases entail discriminatory terms and conditions pertaining to the sale or rental of an apartment or home with the purpose of denying access. These forms include unfair financing or loan qualifications or terms, steering or restricting the choices of people seeking homes, differential criteria to qualify to rent a home, and refusing to provide insurance, which would prevent the individual or family from acquiring a home. Consumer advocate groups conducted studies and found that many minority borrowers who were eligible for affordable, traditional loans were often steered toward incredibly high-priced subprime loans that they would never be able to repay.\n\nA study conducted by the U.S. Department of Housing and Urban Development (HUD) found that \"the greatest share of discrimination for Hispanic and African American home seekers can still be attributed to being told units are unavailable when they are available to non-Hispanic whites and being shown and told about less units than a comparable non-minority.\"\n\nNonexclusionary discrimination practices refer to \"actions and practices that occur within an already established housing arrangement, most often entailing racial harassment, differential treatment of tenants, or disparate application of contractual terms and conditions of residency.\" Individuals and families already housed experience ongoing intimidation, differential treatment, and harassment, and nonexclusionary discrimination often results in distress for victims since the victim is often legally bound to the home and usually has direct contact with the perpetrator on a regular basis. Landlords and owners are still responsible for the majority of this type of housing discrimination, but neighbors and banking and lending institutions participate more. For instance, even without institutionalized exclusionary power, residential neighbors can harass and intimidate tenants.\n\nMost nonexclusionary discrimination cases involve applying discriminatory terms and conditions within the victim's current residential setting. The majority of these cases involve terms, conditions, and privileges relating to a current rental arrangement. These cases are often seen as unfairly raising the rent of a select group or allowing certain tenants privileges, like using a facility after hours or being lenient on pet policies. Many nonexclusionary discrimination cases involve the failure to provide equal access to services and facilities, such as purposely delaying or completely forgoing fixing a broken pipe. More terms and conditions cases involve discriminatory financing, loans, and appraisals of the individual or family's property, which is when the discriminatory actor takes advantage of the victim financially.\n\nOther forms of nonexclusionary discrimination include the use of harassment, intimidation, and coercion toward victims. This includes racial slurs and threats of violence, both of which create an uneasy environment in which the victims live. These forms can cause excessive anxiety and stress for the individual or family affected. If an individual holding a position of an authority, such as the landlord, is responsible for the nonexclusionary discrimination, the victim is left with a feeling of powerlessness and lack of ability to get help.\n\nExclusionary and nonexclusionary describe the various disparate treatments that housing providers or agents can place upon renters and buyers. In addition to these types of housing discrimination, certain policies that do not discriminate on its face have also been found to cause housing discrimination in the United States. Disparate impact is a facially neutral housing policy that negatively impacts minorities or other protected groups of people. Disparate impact was not always looked at in the housing context. It was first applied in housing discrimination in 2015 when the Court found Congress had intended to include liability for disparate impact discrimination in the Fair Housing Act and its recognition is consistent with the central purpose of the Act since it was amended in 1988. After that case, HUD began recognizing many types of disparate impact treatment. For example, HUD published a statement concluding that blanket prohibitions against tenants with criminal convictions would constitute disparate impact housing discrimination because incarceration rates in the United States are disproportionate between minorities and non-minorities. Disparate impact remains controversial as some feel that their freedom in implementing policies and rules is now limited due to the fear of unintended consequences of rules that originally had no discriminatory intent.\n\nProfessor of Public Administration and Economics and expert in the field of housing discrimination studies, John Yinger, argues that discriminatory housing practices in the housing market have led to segregation and can be interpreted as forms of modern-day discrimination. One important example cited is of realtors opting to place public housing in crowded inner city minority neighborhoods instead of those with an Anglo majority due to \"public and political pressure.\" Other housing phenomena that Yinger argues encourage segregation are those of sorting and bidding in which bidders perceived to be higher-class win out on cheaper per-square-foot, larger homes farther away from inner cities. The study done by the U.S. Housing Scholars adds that school zoning has also been named a culprit for housing segregation, and may be used as a critical venue for housing discrimination. It also cites the Internet as yet another means for the perpetuation of housing discrimination that is, as of now, unrestricted by the provisions of the Fair Housing Act. While the Fair Housing Act strictly prohibits any sellers from using language that explicitly names a preference for a certain group, third-party sellers that use sites like Craigslist.com and Roommates.com to find buyers or renters are granted immunity from the FHA, and the websites are not held liable. This was cemented by the 2004 court case, \"Fair Housing Council of San Fernando Valley v. Roommates.com, LLC\", in which Roommates.com was not held liable for users with advertisements such as, 'looking for White Christian Male,' since the website did not have the resources to monitor and censor such usage.\n\nHousing discrimination focuses more on race, but recent studies have shown a growing trend toward discrimination in the housing market against those who identify themselves as gay, lesbian, or transgender. Since housing discrimination based on sexual orientation was not explicitly cited in the Fair Housing Act, as of 2007, it was banned only in 17 states. In all states, same-sex couples are frequently unable to apply to public housing as a family unit, thus decreasing their chances at being accepted into the program. For instance, in comprehensive study done by the Fair Housing Centers of Michigan in 2007, statistics showed that out of 120 paired-tests, almost 30 percent of same-sex couples were given higher rental rates and less encouragement to rent, both examples of nonexclusionary housing discrimination. An HUD study released in 2011 surveyed of 6,450 transgender and gender non-conforming persons and found that \"19 percent reported having been refused a house or an apartment because of gender identity.\"\n\nOn January 30, 2012, HUD Secretary Shaun Donovan announced new regulations that would require all housing providers that receive HUD funding to prevent housing discrimination based on sexual orientation or gender identity. These regulations went into effect on March 5, 2012.\n\nEthnic and racial minorities are impacted the most by housing discrimination. Exclusionary discrimination against African Americans most often occurs in rental markets and sales markets. Families are vulnerable to exclusion, but African American women are especially overrepresented as victims, especially single African American mothers. This discriminatory exclusion is because of stereotypes concerning race and single women. The presence of children in a minority family at times is what warrants the discrimination. African Americans are also the victims in most nonexclusionary cases, with African American women still overrepresented. Nonexclusionary forms of discrimination such as racial slurs and intimidation affect many minority victims. Some racial minorities suffer the purposeful neglect of service needs, such as a landlord fixing a white tenant's bathtub quickly but delaying fixing the bathtub of the minority tenant. Data obtained by Ohio Civil Rights Commission studied housing discrimination cases between 1988 and 2003, and of the 2,176 cases filed, 1,741 were filed by African Americans. A study by HUD released in 2005 found that more and more Hispanics are facing discrimination in their housing searches. A 2011 article by HUD asserts that one out of five times, Asian Americans and Pacific Islanders receive less favorable treatment than others when they seek housing. Some cases brought to the Department of Justice show that municipalities and other local government entities violated the Fair Housing Act of 1968 when they denied African Americans housing, permits, and zoning changes, or steered them toward neighborhoods with a predominantly minority population.\n\nQuasi-experimental audit studies, in which equally qualified individuals of different races both participate in housing searches, have found strong evidence of racial housing discrimination in the United States.\n\nAccording to the U.S. Census of Population in 1990, 25.3 percent of all Anglo-Americans in the U.S. lived in central city areas. The percentage of African Americans living in inner cities was 56.9 percent, and the percentage of inner city Hispanics was 51.5 percent. Asian Americans living in central cities totaled 46.3 percent. According to a more recent U.S. Census Bureau study in 2002, the average white person living in a metropolitan area lives in a neighborhood that is 80 percent Anglo and seven percent black, while the average African American lives in a neighborhood that is 33 percent white and more than 51 percent black. As of 2000, 75 percent of all African Americans lived in highly segregated communities, making them the most segregated group in the nation. These statistics do not necessarily point to evidence of housing discrimination, but rather to segregation based on historical reasons which have made ethnic and racial minorities more economically deprived, and thus prone to living in more poverty-stricken inner city areas.\n\nIn a comprehensive study by the HUD in 2000, paired-tests (in which two applicants of different races but the same economic status and credit scores apply to rent or buy a house) were used to determine whether or not statistics about segregation truly pointed to housing discrimination. This study reported that although adverse treatment of racial and ethnic minorities has decreased over time, roughly 25 percent of white applicants were still favored above those who were African-American or Hispanic. About 17 percent of African American applicants and 20 percent of Hispanic applicants were subjected to adverse treatment, including receiving less information about a home or being shown fewer, lower-quality units.\n\nJohn Yinger, a sociologist who has studied housing discrimination, argues that it is something perhaps most concretely evidenced by its effects: concentrated poverty. People who suffer from housing discrimination often live in lower-quality housing. Housing inequalities often reflect the unequal distribution of income. Poor areas suffer from educational disparities, and a poor education translates into earnings disparities. Those who earn less can only afford lower-quality housing. Segregation, health risks, and wealth disparities all relate to poverty.\n\nPerhaps the most unmistakable consequence of housing discrimination is residential segregation. Housing discrimination helps reinforce residential segregation through mortgage discrimination, redlining, and predatory lending practices. Racial avoidance and threats of violence also result in racial segregation. Housing discrimination can also impact minority preferences over time, as individuals or families experiencing harassment and intimidation at their home on a daily basis may transition to more accepting neighborhoods.\n\nHealth risks are a consequence of housing discrimination. Those suffering housing discrimination and people living below the poverty threshold often rent small or low-quality housing. Lead paint left over from past years and animal pests, such as rats, can be found in older housing, resulting in serious health consequences. Lead can lead to lowered intelligence in children. Asthma is also a problem that comes with lower-quality housing, since more air pollution, dust, mold, and mildew are more likely to occur.\n\nNeighborhood effects are also seen due to housing discrimination and residential segregation. The housing inequality that comes with living in lower-quality housing means that neighborhood amenities are lacking. Poorer areas offer worse education, leading to educational and employment disadvantages and a higher school dropout rate. Schools are often segregated due to the effects of housing discrimination and residential segregation, in turn hindering students' educational performance. A study conducted by the Century Foundation in Montgomery County, Md., showed that students from a low-income background enrolled in affluent schools did better than students in higher-poverty schools. Criminal activity, including gang life and drug abuse, is also more prevalent in poorer areas. The rate of teenage pregnancy has been shown to increase in these areas as well.\n\nSociologists Thomas Shapiro and Jessica Kenty-Drane state that wealth disparities are also a result of housing discrimination, as housing discrimination acts as a barrier to homeownership. Homeowners may learn management and home repair skills, and the children of homeowners are less likely to drop out of high school or to have children as teenagers. Additionally, credit constraints limit homeownership for people with low income. Housing discrimination that keeps families from affordable loans and nicer areas with increasing property values keep victims from accumulating wealth. Residential segregation also leads to generational wealth disparities. Children often inherit wealth from their parents, and if parents were forced into poor-quality housing because of housing discrimination, then there is less wealth to hand down.\n\nSociologist Douglas Massey argues that housing discrimination is a moving target. As federal legislation concerning anti-housing discrimination policies become more effective, new forms of housing discrimination have emerged to perpetuate residential segregation, and in turn, poverty.\n\nThere have been a number of solutions proposed to finally end the threat of housing discrimination and eliminate any legal loopholes in which it may operate. So far fair housing enforcement of federal legislation concerning housing discrimination has faced challenges. The main burden of enforcement falls on federal financial regulatory institutions, like the Federal Reserve Board, and the HUD. The enforcement provisions of the Fair Housing Act of 1968 were limited, and even though the act was amended in 1988, there are still problems with enforcement since housing discrimination often happens one-on-one and is not very visible, even in audits. The Fair Housing Amendment Act of 1988 did make a system of administrative law judges to hear housing discrimination cases to help against the illegal actions. Other examples of federal legislation may include increased federal legislation enforcement, scattered-site housing, or state and local enforcement on a more concentrated level. Better methods of enforcement in addition to new policies are proposed to be a help. In 2010 the Justice Department under President Barack Obama made a new fair-lending unit.\n\nInclusionary remedies to truly enforce integration are also proposed. Inclusionary housing refers to making sure that areas are integrated, and inclusionary housing increases chances for racial minorities to gain and sustain employment. Recently Montgomery County, Md., passed an ordinance to require new housing developments to consist of a percentage of moderately priced dwelling units, guaranteeing more affordable better housing for 10 years.\n\nOther proposed solutions include subsidies, such as direct subsidies, project-based subsidies, household-based subsidies, and tax reductions. As of 2001, only 15.7 percent of poor households received federal housing subsidies, meaning a majority of people in poor households did not receive that help. Household-based subsidies have been a significant source of new housing assistance as of late. HUD has handed out housing certificates to allow participants of Section 8 to move into higher-quality housing units.\n\nIt is important to prioritize policy and city planning. Planning for sustainability does not come with a one-size fits all approach; wicked problems persist without clarity or solutions. However, looking beyond urban regimes and accepting the nexus of these regimes is the first step for change that planners can take. This can be done through the notion of Equitable Development, an approach that aims to create communities of opportunity. Inequalities oppressing low-income communities composed of diverse ethnicities are not only unethical but prove to be economically and environmentally unsustainable. Partnership between government, private sectors, and community-based organizations to manipulate public policy for the promotion of social equity, as well as, economic growth and environmental sustainability are crucial for justice.\n\n\n"}
{"id": "41791120", "url": "https://en.wikipedia.org/wiki?curid=41791120", "title": "Imai – inter media art institute", "text": "Imai – inter media art institute\n\nThe imai foundation - inter media art institute (also referred to as IMAI and stylized as imai) was founded in 2006. It is an institution dedicated to the distribution and preservation of media art and associated activities. The foundation organizes workshops, conferences, exhibitions, research projects and case studies concerning the current questions of conservation and restoration of media art. It takes also part in discussions about preservation, presentation and distribution of media art.\n\nThe main purpose for IMAI's founding was to preserve the large collection of video art tapes which the former video art distributor and media art agency 235 MEDIA held in its archive. IMAI was set up with the support of the provincial capital of Düsseldorf and the Cologne media art agency 235 MEDIA. The preservation of the video art archive was funded by the Kunststiftung NRW (art foundation NRW – of the state of North Rhine Westphalia), the 'Kulturstiftung der Länder' (cultural foundation of the Federal states). The IMAI is located in the NRW Forum at the Ehrenhof complex, built by Wilhelm Kreis in 1925-26, close to Kunstakademie Düsseldorf and the museum kunst palast in Düsseldorf, the state capital of North Rhine-Westfalia. The institute is directed by Dr. Renate Buschmann.\n\nNow the IMAI archive includes approximately 3,000 artistic and documentary works from the era of the 1960s to present. The particular focus is on video art and documentaries. The collection includes US-American, Asian and European artists now considered some of the pioneers of this relatively young artistic genre, with main focus on works from the end of the 1970s and early 1980s. Audiovisual works, particularly single-channel works, are at the archive.\n\nSome of the artists represented by IMAI include: Steina and Woody Vasulka, George Barber, Dara Birnbaum, Klaus vom Bruch, Douglas Davis, Valie Export, Ken Feingold, Paul Garrin, Ulrike Rosenbach, Kirsten Geisler, Gary Hill, Dara Birnbaum, Nan Hoover, Valie Export, Jürgen Klauke, Robert Cahen, and Marcel Odenbach.\n\nDuring its ongoing its ongoing research project to restore media art installations (since 2006), IMAI uses individual case studies to test groundbreaking restoration methods which secured the long-term preservation of technology-based art. IMAI collaborates with restorers, art historians, technicians and artists. The foundation has realized six case studies so far:\n\nThe first case study was about \"Il Nuotatore (va troppo spesso ad Heidelberg)\" a work by Studio Azzurro from 1984. \"In Situ\" (1986) by Gary Hill, \"Exchange Fields\" (2000) by Bill Seaman, \"Testcuts I\" by Katharina Sieverding (2010), \"Light Composition: Documenta 8\" (1987) by Nan Hoover and \"Zweileinwandkino\" (1968/2014) by Lutz Mommartz. The results of the case studies were presented in publications and at several conferences.\n\nIMAI participated in the Düsseldorf art festival Quadriennale 2010 with \"Katharina Sieverding. Testcuts. Projected Data Images\" and participated in Quadriennale 2014 with the exhibition \"The Invisible Force Behind. Materiality in Media Art\". Media artists Holger Mader and Heike Wiermann will display a light-art projection in the Ehrenhof courtyard that will play across the windows of the Belvedere and also at times the fountain area in the inner courtyard of the Museum Kunstpalast. The exhibition \"Images against Darkness (Bilder gegen die Dunkelheit)\" at KIT – Kunst im Tunnel presented a selection of video art works from the imai archive for the first time.\n\nMembers of the Board of Trustees are Dr. Andreas Broeckmann (Director ISEA2010_Ruhr and founding Director Dortmunder U), Dr. Söke Dinkla (Director Lehmbruck Museum Duisburg), Prof. Rainer Jacobs (lawyer), Ulrich Leistner (235 MEDIA), Hans-Georg Lohe (Director of Cultural Affairs of the City of Düsseldorf), Prof. Marcel Odenbach (artist), Dr. Ingrid Stoppa-Sehlbach (Cultural Department of the State Chancellery NRW), Beat Wismer (General director of Stiftung Museum Kunstpalast), Julia Stoschek (Julia Stoschek Collection), Doris Krystof (Kunstsammlung NRW).\n\nMembers of the Board are Nicolas Maas (Foundation Schloß Benrath) and Axel Wirths (235 MEDIA).\n\nThe imai publishes exhibition catalogs and specialist works on the monographic and thematic exhibitions and case studies. A selection:\n\n\n"}
{"id": "52280151", "url": "https://en.wikipedia.org/wiki?curid=52280151", "title": "Incremental learning", "text": "Incremental learning\n\nIn computer science, incremental learning is a method of machine learning, in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.\n\nMany traditional machine learning algorithms inherently support incremental learning, other algorithms can be adapted to facilitate this. Examples of incremental algorithms include\ndecision trees\n(IDE4,\nID5R),\ndecision rules,\nartificial neural networks\n(RBF networks,\nLearn++,\nFuzzy ARTMAP,\nTopoART, and\nIGNG) or\nthe incremental SVM.\n\nThe aim of incremental learning is for the learning model to adapt to new data without forgetting its existing knowledge, it does not retrain the model. Some incremental learners have built-in some parameter or assumption that controls the relevancy of old data, while others, called stable incremental machine learning algorithms, learn representations of the training data that are not even partially forgotten over time. Fuzzy ART and TopoART are two examples for this second approach.\n\nIncremental algorithms are frequently applied to data streams or big data, addressing issues in data availability and resource scarcity respectively. Stock trend prediction and user profiling are some examples of data streams where new data becomes continuously available. Applying incremental learning to big data aims to produce faster classification or forecasting times.\n\n"}
{"id": "3993568", "url": "https://en.wikipedia.org/wiki?curid=3993568", "title": "Indigenous language", "text": "Indigenous language\n\nAn indigenous language or autochthonous language is a language that is native to a region and spoken by indigenous people. This language would be from a linguistically distinct community that has been settled in the area for many generations. Indigenous languages are not necessarily national languages, and the reverse is also true.\n\nMany indigenous peoples worldwide have stopped passing on their ancestral languages to the next generation, and have instead adopted the majority language as part of their acculturation into the majority culture. Furthermore, many indigenous languages have been subject to linguicide (language killing). Recognizing their vulnerability, the United Nations proclaimed 2019 the International Year of Indigenous Languages, \"to draw attention to the critical loss of indigenous languages and the urgent need to preserve, revitalize and promote indigenous languages.\"\n\nMany indigenous languages are disappearing as there are no longer any young people left to speak those languages, so their remaining speakers are dying out.\nIn North America, since 1600 at least 52 Native American languages have disappeared. Globally, there may be more than 7,000 languages that exist in the world today, though many of them have not been recorded because they belong to tribes in rural areas of the world or are not easily accessible. It is estimated that 6,809 \"living\" languages exist in the world today, with 90% having fewer than 100,000 speakers. This means that roughly 6,100 languages are facing a risk of eventual extinction. Some languages are very close to disappearing. \n\nForty six languages are known to have just one native speaker while 357 languages have fewer than 50 speakers. Rare languages are more likely to show evidence of decline than more common ones.\n\nOklahoma provides the backdrop for an example of language loss in the developed world. It boasts the highest density of indigenous languages in the United States. This includes languages originally spoken in the region, as well as those of Native American tribes from other areas that were forcibly relocated onto reservations there. The U.S. government drove the Yuchi from Tennessee to Oklahoma in the early 19th century. Until the early 20th century, most Yuchi tribe members spoke the language fluently. Then, government boarding schools severely punished American Indian students who were overheard speaking their own language. To avoid beatings and other punishments, Yuchi, and other Indian children abandoned their native languages in favor of English.\n\nIn 2005, only five elderly members of the Yuchi tribe were fluent in the language. These remaining speakers spoke Yuchi fluently before they went to school and have maintained the language despite strong pressure to abandon it.\n\nThis situation was not limited to Oklahoma. In the Northwest Pacific plateau there are no speakers left of the indigenous tribal languages from that area, all the way to British Columbia.\n\nOregon's Siletz reservation, established in 1855, was home to the endangered language Siletz Dee-ni. The reservation held members of 27 different Indian bands speaking many languages. In order to communicate, people adopted Chinook Jargon, a pidgin or hybrid language. Between the use of Chinook Jargon and the increased presence of English, the number of speakers of indigenous languages dwindled.\n\nOther tribes of Native Americans were also forced into government schools and reservations. They were also treated badly if they did not become \"civilized\". This meant they were to go to Christian churches and speak English. They were forced to give up their tribal religious beliefs and languages. Now, these Native Americans are trying to regain some of their lost heritage. They gather at \"Pow-wow\" to share culture, stories, remedies, dances, music, rhythms, recipes and heritage with anyone who wants to learn them.\n\nIn January 2008, in Anchorage, Alaska, friends and relatives gathered to bid their last farewell to 89 year old Marie Smith Jones, a beloved matriarch of her community. \"As they bid her farewell to her, they also bid farewell to the Eyak language as Marie was the last fluent speaker of the language.\"\n\nIn the Isle of Man, following the decline in the use of Manx during the 19th century, (The Manx Language Society) was founded in 1899. By the middle of the 20th century only a few elderly native speakers remained (the last of them, Ned Maddrell, died on 27 December 1974), but by then a scholarly revival had begun to spread and many people had learned Manx as a second language. The revival of Manx has been aided by the recording work done in the 20th century by researchers.\n\nHundreds of indigenous languages around the world are taught by traditional means, including vocabulary, grammar, readings and recordings.\n\nAbout 6,000 others can be learned to some extent by listening to recordings made for other purposes, such as religious texts, where translations are available in more widely known languages.\n\nThe term \"treasure language\" was proposed by the Rama people of Nicaragua as an alternative to Heritage language, Indigenous language and \"ethnic language\", names that are considered pejorative in the local context. The term is now also used in the context of public storytelling events.\n\nThe term \"treasure language\" references the desire of speakers to sustain the use of their mother tongue into the future:\nAccordingly, the term may be considered to be distinct from Endangered language for which objective criteria are available, or Heritage language which describes an end-state for a language where individuals are more fluent in a dominant language.\n\n\n\n"}
{"id": "2894897", "url": "https://en.wikipedia.org/wiki?curid=2894897", "title": "Internet traffic engineering", "text": "Internet traffic engineering\n\nInternet traffic engineering is defined as that aspect of Internet network engineering dealing with the issue of performance evaluation and performance optimization of operational IP networks. Traffic engineering encompasses the application of technology and scientific principles to the measurement, characterization, modeling, and control of Internet traffic [RFC-2702, AWD2].\n\nEnhancing the performance of an operational network, at both traffic and resource levels, are major objectives of Internet engineering. This is accomplished by addressing traffic performance requirements, while utilizing network economically and reliably. Traffic oriented performance includes packet transfer delay, packet delay variation, packet loss, and throughput.\n\nAn important objective of Internet traffic engineering is to facilitate reliable network operations [RFC-2702]. This can be done by providing mechanisms that network integrity and by embracing policies emphasizing survivability. This results in a minimization of the network to service outages arising from errors, faults and failures occurring within the infrastructure.\n\nThe Internet exists in order to transfer information from nodes to destination nodes. Accordingly, one of the most crucial functions performed by the Internet is the routing of traffic ingress nodes to egress nodes.\n\nUltimately, it is the performance of the network as seen by network services that is truly paramount. This crucial function should be considered throughout the development of engineering mechanisms and policies. The characteristics visible to end users are the emergent properties of the network, which are characteristics of the network when viewed as a whole. A goal of the service provider, therefore, is to enhance the properties of the network while taking economic considerations into account.\n\nThe importance of the above observation regarding the properties of networks is that special care must be taken when choosing network performance metrics to optimize. Optimizing the wrong metrics may achieve certain local objectives, but may have repercussions elsewhere.\n\n"}
{"id": "318708", "url": "https://en.wikipedia.org/wiki?curid=318708", "title": "K Foundation", "text": "K Foundation\n\nThe K Foundation was an art foundation set up by Bill Drummond and Jimmy Cauty (The KLF) in 1993, following their 'retirement' from the music industry. The Foundation served as an artistic outlet for the duo's post-retirement KLF income. Between 1993 and 1995, they spent this money in a number of ways, including on a series of Situationist-inspired press adverts and extravagant subversions in the art world, focusing in particular on the Turner Prize. Most notoriously, when their plans to use banknotes as part of a work of art fell through, they burned a million pounds in cash.\n\nThe K Foundation announced a 23-year moratorium on all projects from November 1995. They further indicated that they would not speak about the burning of the million pounds during the period of this moratorium.\n\nIn the early 1980s, British musician and artist Jimmy Cauty was the guitarist in an underachieving pop/rock band, Brilliant. Brilliant had been signed to WEA Records by A&R man Bill Drummond, formerly a member of the Liverpool group Big in Japan, the manager of The Teardrop Explodes and Echo & the Bunnymen, and co-founder of the independent record label Zoo Records. In 1986, Brilliant released their one and only album - \"Kiss The Lips Of Life\" - before splitting up. In the same year, Drummond left WEA Records to record a solo album. Whilst out walking on New Year's Day, 1987, Drummond hit upon an idea for a hip-hop record but, he said, knowing \"nothing, personally, about the technology\", he needed a collaborator. Drummond called Jimmy Cauty who agreed to join him in a new band called The Justified Ancients of Mu-Mu (The JAMs).\n\nThe JAMs' debut release, the single \"All You Need Is Love\", was released as an underground white label on 9 March 1987. By 1991, the duo—now calling themselves The KLF—had become the best-selling singles band in the world and, according to the \"Allmusic\", were \"on the verge of becoming superstars\". Instead, in May 1992 they machine-gunned a music industry audience at the BRIT Awards (albeit with blanks) and quit the music business.\n\nBy their own account, neither Drummond nor Cauty kept any of the money that they made as The KLF; it was all ploughed back into their extravagant productions. Cauty told an Australian \"Big Issue\" writer in 2003 that all the money they made as The KLF was spent, and that the royalties they accrued post-retirement amounted to approximately one million pounds:\nAlthough the duo had deleted their back catalogue in the UK with immediate effect, international licensees retained the contractual right to distribute KLF recordings for a number of years. The KLF, like any other artist, were also entitled to Performing Right Society royalties every time one of their songs was played on the radio or television. Rather than spend these earnings or invest them for personal gain, the duo decided the money would be used to fund a new art foundation - The K Foundation. \"Having created an artistic machine that created money\", said \"GQ\" Magazine, \"they [then] invented a machine for destroying it.\" Quite what the Foundation, this money-destroying machine, would do with the million pounds plus was still undecided.\n\nMusic journalist Sarah Champion pointed out (prior to the million pound fire) that, \"Being 'in the money' doesn't mean they'll ever be rich. [Drummond and Cauty will] always be skint, but their pranks will get more extravagant. If they earned £10 million, they'd blow it all by buying Jura or a fleet of K Foundation airships or a Van Gogh to be ceremonially burned.\" \"There are things we'd like to do which we haven't done.\", Drummond told a journalist in 1991. \"Totally ludicrous things. We want to buy ships, have submarines. They really are stupid things I know, but I feel confident that in the event of us selling ten million albums we would definitely go out and buy a submarine...Just to be able to say 'Look we've got a submarine and 808 State haven't'.\"\n\nThe first manifestation of the K Foundation was a series of adverts in UK national newspapers in 1993. The first adverts, in July 1993, were cryptic, referring to \"K Time\" and advising readers to \"Kick out the clocks\". There was also an advert for their single \"K Cera Cera\" which was \"Available nowhere ... no formats\" and which was not planned for release until world peace was established. The single was eventually released, but only in Israel.\n\n\"When the first in a strange series of full-page ads appeared in The Independent on July 4\", said \"The Face\", \"people started whispering. The cultish rhetoric, the unfathomable \"Divide and Kreate\" slogans, the K symbols, all suggested that the kings of cultural anarchy were back.\" Each advert cost between £5,000 and £15,000.\n\nThe 1994 K Foundation award was an award given by the K Foundation to the \"worst artist of the year\".\nThe Foundation commissioned more press adverts, instructing readers to \"Abandon all art now\" and then inviting them to vote for the worst artist of the year. The 1993 Turner Prize was being judged at the same time, and, perhaps not coincidentally, both awards had the same shortlist of four artists. The prize being offered by Drummond and Cauty was £40,000 which was double the £20,000 offered for the Turner Prize.\n\nChannel 4 Television broadcast coverage of the Turner Prize, during which three more K Foundation adverts were broadcast — these announced the \"amending of art history\". During the evening, Rachel Whiteread was announced as the winner of both the Turner Prize and the K Foundation award. Whiteread initially refused to accept the K Foundation prize, but after being told that the money would be incinerated, she reluctantly accepted, with the intention of donating £30,000 to artists in financial need and the other £10,000 to the housing charity, Shelter.\n\nDuring the buildup to the presentation of the K Foundation art award to Rachel Whiteread on 23 November 1993, the K Foundation presented their first artwork to the press. \"Nailed To A Wall\", \"the first of a series of K Foundation art installations that will also include one million pounds in a skip, one million pounds on a table and several variants on the theme of Tremendous Amounts Of Folding\", consisted of one million pounds in £50 notes, nailed to a large framed board. \"Nailed To A Wall\" had a reserve price of £500,000, half the face value of the cash used in its construction, which \"Scotland on Sunday\"'s reporter Robert Dawson Scott was \"fairly confident... really was £1 million [in cash]\". The catalogue entry for the artwork stated: \"Over the years the face value will be eroded by inflation, while the artistic value will rise and rise. The precise point at which the artistic value will overtake the face value is unknown. Deconstruct the work now and you double your money. Hang it on a wall and watch the face value erode, the market value fluctuate, and the artistic value soar. The choice is yours.\"\n\nCollectively, the K Foundation's money-as-art works were titled \"Money: A Major Body Of Cash\", \"seven pieces, all involving various amounts of cash nailed to, tied to or simply standing on inanimate objects\". \"The Face\" magazine neatly summed up the concepts behind the art project:\n\nDuring the first half of 1994, the K Foundation attempted to interest galleries in staging \"Money: A Major Body Of Cash\". However, even old friend Jayne Casey, director of the Liverpool Festival Trust, was unable to persuade a major gallery to participate. \"'The Tate, in Liverpool, wanted to be part of the 21st Century Festival I'm involved with,' says Casey. 'I suggested they put on the K Foundation exhibition; at first they were encouraging, but they seemed nervous about the personalities involved.' A curt fax from... the gallery curator, informed Casey that the K Foundation's exhibition of money had been done before and more interestingly\", leaving Drummond and Cauty obliged to pursue other options. The duo considered taking the exhibition across the former Soviet Union by train and on to the United States, but no insurer would touch the project. Then an exhibition at Dublin's Kilmainham Jail was considered. No sooner had a provisional date of August been set for the exhibition, however, when the duo changed their minds yet again. \"Jimmy said: 'Why don't we just burn it?' remembers Drummond. 'He said it in a light-hearted way, I suppose, hoping I'd say: 'No, we can't do that, let's do this...' But it seemed the most powerful thing to do.\" Cauty: \"We were just sitting in a cafe talking about what we were going to spend the money on and then we decided it would be better if we burned it. That was about six weeks before we did it. It was too long, it was a bit of a nightmare.\"\n\nOn 23 August 1994, in a boathouse on the Scottish island of Jura, Drummond and Cauty incinerated £1,000,000 in cash. The burning was witnessed by an old friend of Drummond's, freelance journalist Jim Reid, who subsequently wrote an article about the ceremony for \"The Observer\". It was filmed on Super 8 by their friend Gimpo.\n\nReid admitted to first feeling shock and guilt about the burning, which quickly turned to boredom. The money took well over an hour to burn as Drummond and Cauty fed £50 notes into the fire. Drummond later said that only about £900,000 of the money was actually burnt – the rest flew straight up the chimney. The press reported that an islander handed £1,500 into the police; the money had not been claimed and would be returned to the finder.\n\nOn 23 August 1995, exactly one year after the burning, Drummond and Cauty returned to Jura for the premier screening of the film, now known as \"Watch the K Foundation Burn a Million Quid\". The film was then toured around the UK over the next few months (plus one showing in Belgrade), with a Q&A session at the end of each screening where members of the audience asked Drummond and Cauty why they burnt the money and also offered their own interpretations.\n\nDrummond and Cauty announced a moratorium on K Foundation activities in the obscure \"The Workshop for Non-linear Architecture\" bulletin of November 1995. The duo had signed a \"contract\", agreeing to wind up the K Foundation and not to speak about the money burning for a period of 23 years. The document was signed on the bonnet of a rented car which, they claim, they then pushed over the cliffs at Cape Wrath. This was followed on 8 December 1995 by an advertisement in \"The Guardian\":\n\nIn November 1995, the BBC aired an edition of the \"Omnibus\" documentary series about The K Foundation entitled \"A Foundation Course in Art\".\n\nThe final act of the K Foundation was distributing a van load of Tennent's Super - a high-alcohol-content lager - to London's street drinkers on Christmas Day 1995. However, the Foundation discovered that their choice of location for this endeavour — near Waterloo station on the South Bank — was unusually devoid of homeless people, many of whom were in homeless shelters for the day. \"That was a pity\", said Jimmy Cauty. \"If you are down-and-out, would you rather have a bowl of soup or a can of Tennent's?\" The \"Sunday Times\" later called the scheme \"ethically dubious\".\n\nDrummond and Cauty would next work together in 1997, when they attempted to \"Fuck the Millennium\" as 2K (music) and K2 Plant Hire (conceptual art).\n\nThe only music release to bear the name of the K Foundation was \"K Cera Cera\", released as a limited edition single in Israel and Palestine in November 1993. An amalgam of \"Que Sera, Sera (Whatever Will Be, Will Be)\" and John Lennon/Yoko Ono's \"Happy Xmas (War Is Over)\", it was credited to the \"K Foundation presents The Red Army Choir\". Originally intended for release when \"world peace [is] established\" (i.e. never) and in \"no formats\", the Israeli release was made \"In acknowledgement of the recent brave steps taken by the Israeli Government and the Palestinian Liberation Organisation (PLO)\". Said Drummond: \"Our idea was to create awareness of peace in the world. Because we were worried it would be interpreted by the public as an attempt by The KLF to return to the music world on the back of a humanist gimmick, we decided to hide behind the Foundation.\"\n\nAlso made by the duo during the K Foundation's existence, reported by the \"NME\" as a K Foundation work, but officially attributed to \"The One World Orchestra featuring The Massed Pipes and Drums of the Children's Free Revolutionary Volunteer Guards\", was \"The Magnificent\", their contribution to the charity album \"Help\". The song, a drum'n'bass version of the theme tune from \"The Magnificent Seven\" with vocal samples from DJ Fleka of Serbian radio station B92, was recorded on 4 September 1995. On 5 September 1995, Drummond and Cauty claimed they would \"never make any more records\". Drummond said, \"What do you expect us to do, go and make a jungle record?\"; Cauty added \"Yeah, like a jungle novelty record with some strings on it or something. It would just be sad wouldn't it? We're too old.\" \"NME\" gleefully informed their readers, \"The K Foundation's contribution to the 'Help' LP is a jungle track.\" \"Help\" was released on 9 September 1995.\n\n"}
{"id": "6286841", "url": "https://en.wikipedia.org/wiki?curid=6286841", "title": "Li Europan lingues", "text": "Li Europan lingues\n\nLi Europan lingues is a quotation in Occidental, an international auxiliary language devised by Edgar von Wahl in \n1922. It is used in some HTML templates as a fill-in or placeholder text. \n\nOne of the most common placeholder texts is lorem ipsum. A similar text of this type is the pangram \"The quick brown fox jumps over the lazy dog\", which is often used in fill-in text and to demonstrate and compare typefaces because it contains all 26 letters of the Latin alphabet.\n\nWhen used as placeholder text, \"Li Europan lingues\" is usually one or two paragraphs and reads as follows:\n\n\nThe text of \"Li Europan lingues\" is found in many HTML templates and through copying and uploading of templates this phrase seems to have found its use in many websites.\n\nDon Harlow posted a message to the Auxlang List on 5 August 2006, mentioning its appearance in the \"CSS Cookbook\" from O'Reilly by Christopher Schmitt, and in templates of webpages which implement CSS. \n\nDon Gasper posted another message to the same list suggesting a possible source:\n\nWilliam Patterson was pleasantly surprised that evening to read about the appearance of his version of \"Lorem ipsum\" in the \"CSS Cookbook\" and quickly confessed. In June 1998, at a time when he was dabbling in Occidental, he had come across the \"Lorem ipsum\" text somewhere and learned about its use as a template. As was his wont when learning something new like that, he created a webpage about it. But while warning about the existence of \"corrupt\" versions, he offered one himself which included some Occidental text, a dash of Esperanto, some variations of his given name, and a little bit of nonsense. In September 2002 he came across a copy of his version on the Web. Curious, he searched for other instances, and found that on 10 September 2002, an AllTheWeb search for the strings \"ulliam\" and \"willum\" returned 513 hits; by 10 December 2002, 962; and by 14 October 2003, 2337. (One page which he found particularly amusing was a Swedish translation of his own webpage— Occidental, commentary, page colors, page title \"Ailanto : Lorem ipsum\" and all!) And now it's in the \"CSS Cookbook\"!\n\nAn HTML comment in the page that started it all attributes the Occidental text itself to S. W. Beer.\n\n"}
{"id": "42017954", "url": "https://en.wikipedia.org/wiki?curid=42017954", "title": "List of cosmological computation software", "text": "List of cosmological computation software\n\nThe cosmic microwave background (CMB) is the thermal radiation assumed to be left over from the \"Big Bang\" of cosmology. The CMB is a snapshot of the oldest light in our universe, imprinted on the sky when the universe was just 380,000 years old. It shows tiny temperature fluctuations that correspond to regions of slightly different densities, representing the seeds of all future structure: the stars and galaxies of today. Therefore, analysis of the small anisotropies in the CMB helps us to understand the origin and the fate of our universe. In past few decades, there has been a lot of improvement in the observations and several experiments, performed to understand the basic structure of the universe. For analyzing data of different cosmological experiments and for understanding the theoretical nature of the universe many advanced methods and computing software are developed in and used by Cosmologists for years. These software are widely used by the cosmologists across the globe.\n\nThe computational software, used in cosmology can be classified into the following major classes.\n\n\nHEALPix (sometimes written as Healpix), an acronym for Hierarchical Equal Area isoLatitude Pixelisation of a 2-sphere, can refer to either an algorithm for pixelization of the 2-sphere, an associated software package, or an associated class of map projections. Healpix is widely used for cosmological random map generation. The original motivation for devising HEALPix was one of necessity. NASA's WMAP and the European Space Agency’s mission Planck - produce multi-frequency data sets sufficient for the construction of full-sky maps of the microwave sky at an angular resolution of a few arc minutes. The principal requirements in the development of HEALPix were to create a mathematical structure that supports a suitable discretization of functions on a sphere at sufficiently high resolution, and to facilitate fast and accurate statistical and astrophysical analysis of massive full-sky data sets. The HEALPix maps are used in almost all the data processing research in cosmology.\n\nCMBFAST is a computer code, developed by Uroš Seljak and Matias Zaldarriaga (based on a Boltzmann code written by Edmund Bertschinger, Chung-Pei Ma and Paul Bode) for computing the power spectrum of the cosmic microwave background anisotropy. It is the first efficient program to do so, reducing the time taken to compute the anisotropy from several days to a few minutes by using a novel semi-analytic line-of-sight approach.\n\nCode for Anisotropies in the Microwave Background by Antony Lewis and Anthony Challinor. The code was originally based on CMBFAST. Later several developments are made to make it a faster and more accurate and compatible with the present research. The code is written in an object oriented manner to make it more user friendly.\n\nCMBEASY is a software package written by Michael Doran, Georg Robbers and Christian M. Müller. The code is based on the CMBFAST package. CMBEASY is fully object oriented C++. This considerably simplifies manipulations and extensions of the CMBFAST code. In addition, a powerful Spline class can be used to easily store and visualize data. Many features of the CMBEASY package are also accessible via a graphical user interface. This may be helpful for gaining intuition, as well as for instruction purposes.\n\nCLASS is a new Boltzmann code developed in this line. The purpose of CLASS is to simulate the evolution of linear perturbations in the universe and to compute CMB and large scale structure observables. Its name also comes from the fact that it is written in object-oriented style mimicking the notion of class. Classes are a programming feature available, e.g., in C++ and python, but these languages are known to be less vectorizable/parallelizable than plain C (or Fortran), and hence potentially slower. CLASS is written in plain C for high performances, while organizing the code in a few modules that reproduce the architecture and philosophy of C++ classes, for optimal readability and modularity.\n\nAnalizeThis is a parameter estimation package used by cosmologists. It comes with the CMBEASY package. The code is written in C++ and uses the global metropolis Algorithm for estimation of cosmological parameters. The code was developed by Michael Doran, for parameter estimation using WMAP-5 likelihood. However, the code was not updated after 2008 for the new CMB experiments. Hence this package is currently not in use by the CMB research community. The package comes up with a nice GUI.\n\nCosmoMC is a Fortran 2003 Markov chain Monte Carlo (MCMC) engine for exploring cosmological parameter space. The code does brute force (but accurate) theoretical matter power spectrum and Cl calculations using CAMB. CosmoMC uses a simple local Metropolis algorithm along with an optimized fast-slow sampling method. This fast-slow sampling method provides faster convergence for the cases with many nuisance parameters like Planck. CosmoMC package also provides subroutines for post processing and plotting of the data.\n\nCosmoMC was written by Antony Lewis in 2002 and later several versions are developed to keep the code up-to date with different cosmological experiments. It is presently the most used cosmological parameter estimation code.\n\nSCoPE/Slick Cosmological Parameter Estimator is a newly developed cosmological MCMC package written by Santanu Das in C language. Apart of standard global metropolis algorithm the code uses three unique technique named as 'delayed rejection' that increases the acceptance rate of a chain, 'pre-fetching' that helps an individual chain to run on parallel CPUs and 'inter-chain covariance update' that prevents clustering of the chains allowing faster and better mixing of the chains. The code is capable of faster computation of cosmological parameters from WMAP and Planck data.\n\n\nDifferent cosmology experiments, in particular the CMB experiments like WMAP and Planck measures the temperature fluctuations in the CMB sky and then measure the CMB power spectrum from the observed skymap. But for parameter estimation the χ² is required. Therefore, all these CMB experiments comes up with its own likelihood software.\n\n"}
{"id": "37531624", "url": "https://en.wikipedia.org/wiki?curid=37531624", "title": "Logical consequence", "text": "Logical consequence\n\nLogical consequence (also entailment) is a fundamental concept in logic, which describes the relationship between statements that hold true when one statement logically \"follows from\" one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises? All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.\n\nLogical consequence is necessary and formal, by way of examples that explain with formal proof and models of interpretation. A sentence is said to be a logical consequence of a set of sentences, for a given language, if and only if, using only logic (i.e. without regard to any \"personal\" interpretations of the sentences) the sentence must be true if every sentence in the set is true.\n\nLogicians make precise accounts of logical consequence regarding a given language formula_1, either by constructing a deductive system for formula_1 or by formal intended semantics for language formula_1. The Polish logician Alfred Tarski identified three features of an adequate characterization of entailment: (1) The logical consequence relation relies on the logical form of the sentences, (2) The relation is a priori, i.e. it can be determined with or without regard to empirical evidence (sense experience), and (3) The logical consequence relation has a modal component.\n\nThe most widely prevailing view on how to best account for logical consequence is to appeal to formality. This is to say that whether statements follow from one another logically depends on the structure or logical form of the statements without regard to the contents of that form.\n\nSyntactic accounts of logical consequence rely on schemes using inference rules. For instance, we can express the logical form of a valid argument as:\n\nThis argument is formally valid, because every instance of arguments constructed using this scheme is valid.\n\nThis is in contrast to an argument like \"Fred is Mike's brother's son. Therefore Fred is Mike's nephew.\" Since this argument depends on the meanings of the words \"brother\", \"son\", and \"nephew\", the statement \"Fred is Mike's nephew\" is a so-called material consequence of \"Fred is Mike's brother's son,\" not a formal consequence. A formal consequence must be true \"in all cases\", however this is an incomplete definition of formal consequence, since even the argument \"\"P\" is \"Qs brother's son, therefore \"P\" is \"Qs nephew\" is valid in all cases, but is not a \"formal\" argument.\n\nIf you know that formula_4 follows logically from formula_5 no information about the possible interpretations of formula_5 or formula_4 will affect that knowledge. Our knowledge that formula_4 is a logical consequence of formula_5 cannot be influenced by empirical knowledge. Deductively valid arguments can be known to be so without recourse to experience, so they must be knowable a priori. However, formality alone does not guarantee that logical consequence is not influenced by empirical knowledge. So the a priori property of logical consequence is considered to be independent of formality.\n\nThe two prevailing techniques for providing accounts of logical consequence involve expressing the concept in terms of \"proofs\" and via \"models\". The study of the syntactic consequence (of a logic) is called (its) proof theory whereas the study of (its) semantic consequence is called (its) model theory.\n\nA formula formula_10 is a syntactic consequence within some formal system formula_11 of a set formula_12 of formulas if there is a formal proof in formula_11 of formula_10 from the set formula_12.\n\nSyntactic consequence does not depend on any interpretation of the formal system.\n\nA formula formula_10 is a semantic consequence within some formal system formula_11 of a set of statements formula_12\n\nif and only if there is no model formula_21 in which all members of formula_12 are true and formula_10 is false. Or, in other words, the set of the interpretations that make all members of formula_12 true is a subset of the set of the interpretations that make formula_10 true.\n\nModal accounts of logical consequence are variations on the following basic idea:\n\nAlternatively (and, most would say, equivalently):\n\nSuch accounts are called \"modal\" because they appeal to the modal notions of logical necessity and logical possibility. 'It is necessary that' is often expressed as a universal quantifier over possible worlds, so that the accounts above translate as:\n\nConsider the modal account in terms of the argument given as an example above:\n\nThe conclusion is a logical consequence of the premises because we can't imagine a possible world where (a) all frogs are green; (b) Kermit is a frog; and (c) Kermit is not green.\n\nModal-formal accounts of logical consequence combine the modal and formal accounts above, yielding variations on the following basic idea:\n\nThe accounts considered above are all \"truth-preservational,\" in that they all assume that the characteristic feature of a good inference is that it never allows one to move from true premises to an untrue conclusion. As an alternative, some have proposed \"warrant-preservational\" accounts, according to which the characteristic feature of a good inference is that it never allows one to move from justifiably assertible premises to a conclusion that is not justifiably assertible. This is (roughly) the account favored by intuitionists such as Michael Dummett.\n\nThe accounts discussed above all yield monotonic consequence relations, i.e. ones such that if formula_10 is a consequence of formula_12, then formula_10 is a consequence of any superset of formula_12. It is also possible to specify non-monotonic consequence relations to capture the idea that, e.g., 'Tweety can fly' is a logical consequence of\n\nbut not of\n\nFor more on this, see Non-monotonic inference relation.\n\n\n"}
{"id": "8222092", "url": "https://en.wikipedia.org/wiki?curid=8222092", "title": "Masked Mystery Villain", "text": "Masked Mystery Villain\n\nA Masked Mystery Villain is a stock character in genre fiction. The Masked Mystery Villain was frequently used in the adventure stories of Pulp magazines and Movie Serials in the early twentieth century. They can also appear in Crime fiction to add to the atmosphere of suspense and suspicion. The \"Mask\" need not be literal (although it often is), referring more to the subterfuge involved.\n\nHe or she is the often main antagonist of the story, often acting behind the scenes with henchmen confronting the protagonists directly. Usually, the protagonists must discover the villain's true identity before they can be defeated. Often the Masked Mystery Villain will turn out to be either one of the protagonists themselves or a significant member of the supporting cast. The author may give the viewer or reader clues, with many red herrings, as to the villain's identity - sometime as the characters find them and sometimes for the audience alone. However, the identity is not usually revealed to the audience before it is revealed to the characters of the story. Even the villain's henchmen rarely know the truth about their master.\n\nThe concept was reversed in the serials \"The Lone Ranger\" and \"The Masked Marvel\", where the true identity of the hero is unknown and a number of characters remain possible candidates until the end.\n\nThe archetype tends to be a popular one to be parodied, often using a literal mask and breaking into a stereotypical evil laugh.\n\n\n\n\n\n\n"}
{"id": "487541", "url": "https://en.wikipedia.org/wiki?curid=487541", "title": "Matrix ring", "text": "Matrix ring\n\nIn abstract algebra, a matrix ring is any collection of matrices over some ring \"R\" that form a ring under matrix addition and matrix multiplication . The set of matrices with entries from \"R\" is a matrix ring denoted M(\"R\"), as well as some subsets of infinite matrices which form infinite matrix rings. Any subring of a matrix ring is a matrix ring.\n\nWhen \"R\" is a commutative ring, the matrix ring M(\"R\") is an associative algebra, and may be called a matrix algebra. For this case, if \"M\" is a matrix and \"r\" is in \"R\", then the matrix \"Mr\" is the matrix \"M\" with each of its entries multiplied by \"r\".\n\nThis article assumes that \"R\" is an associative ring with a unit , although matrix rings can be formed over rings without unity.\n\n\n\nand \n\n\nLet \"D\" be the set of diagonal matrices in the matrix ring M(\"R\"), that is the set of the matrices such that every nonzero entry, if any, is on the main diagonal. Then \"D\" is closed under matrix addition and matrix multiplication, and contains the identity matrix, so it is a subalgebra of \"M\"(\"R\").\n\nAs an algebra over R, \"D\" is isomorphic to the direct product of \"n\" copies of \"R\". It is a free \"R\"-module of dimension \"n\". The idempotent elements of \"D\" are the diagonal matrices such that the diagonal entries are themselves idempotent.\n\nWhen \"R\" is the field of real numbers, then the diagonal subring of M(\"R\") is isomorphic to split-complex numbers. When \"R\" is the field of complex numbers, then the diagonal subring is isomorphic to bicomplex numbers. When \"R\" = ℍ, the division ring of quaternions, then the diagonal subring is isomorphic to the ring of split-biquaternions, presented in 1873 by William K. Clifford.\n\nIn fact, \"R\" only needs to be a semiring for M(\"R\") to be defined. In this case, M(\"R\") is a semiring, called the matrix semiring. Similarly, if \"R\" is a commutative semiring, then M(\"R\") is a .\n\nFor example, if \"R\" is the Boolean semiring (the Two-element Boolean algebra \"R\" = {0,1} with 1 + 1 = 1), then M(\"R\") is the semiring of binary relations on an \"n\"-element set with union as addition, composition of relations as multiplication, the empty relation (zero matrix) as the zero, and the identity relation (identity matrix) as the unit.\n\n\n"}
{"id": "198608", "url": "https://en.wikipedia.org/wiki?curid=198608", "title": "Molecular dynamics", "text": "Molecular dynamics\n\nMolecular dynamics (MD) is a computer simulation method for studying the physical movements of atoms and molecules. The atoms and molecules are allowed to interact for a fixed period of time, giving a view of the dynamic evolution of the system. In the most common version, the trajectories of atoms and molecules are determined by numerically solving Newton's equations of motion for a system of interacting particles, where forces between the particles and their potential energies are often calculated using interatomic potentials or molecular mechanics force fields. The method was originally developed within the field of theoretical physics in the late 1950s but is applied today mostly in chemical physics, materials science and the modelling of biomolecules.\n\nBecause molecular systems typically consist of a vast number of particles, it is impossible to determine the properties of such complex systems analytically; MD simulation circumvents this problem by using numerical methods. However, long MD simulations are mathematically ill-conditioned, generating cumulative errors in numerical integration that can be minimized with proper selection of algorithms and parameters, but not eliminated entirely.\n\nFor systems which obey the ergodic hypothesis, the evolution of one molecular dynamics simulation may be used to determine macroscopic thermodynamic properties of the system: the time averages of an ergodic system correspond to microcanonical ensemble averages. MD has also been termed \"statistical mechanics by numbers\" and \"Laplace's vision of Newtonian mechanics\" of predicting the future by animating nature's forces and allowing insight into molecular motion on an atomic scale.\n\nFollowing the earlier successes of Monte Carlo simulations, the method was first developed by Fermi, Pasta, Ulam and Tsingou in the mid 50s. In 1957, Alder and Wainwright used an IBM 704 computer to simulate perfectly elastic collisions between hard spheres. In 1960, Gibson et al., simulated radiation damage of solid copper by using a Born-Mayer type of repulsive interaction along with a cohesive surface force. In 1964, Rahman published landmark simulations of liquid argon that used a Lennard-Jones potential. Calculations of system properties, such as the coefficient of self-diffusion, compared well with experimental data.\n\nEven before it became possible to simulate molecular dynamics with computers, some undertook the hard work of trying it with physical models such as macroscopic spheres. The idea was to arrange them to replicate the properties of a liquid. J.D. Bernal said, in 1962: \"\"... I took a number of rubber balls and stuck them together with rods of a selection of different lengths ranging from 2.75 to 4 inches. I tried to do this in the first place as casually as possible, working in my own office, being interrupted every five minutes or so and not remembering what I had done before the interruption.\"\n\nBeginning in theoretical physics, the method of MD gained popularity in materials science and since the 1970s also in biochemistry and biophysics. MD is frequently used to refine 3-dimensional structures of proteins and other macromolecules based on experimental constraints from X-ray crystallography or NMR spectroscopy. In physics, MD is used to examine the dynamics of atomic-level phenomena that cannot be observed directly, such as thin film growth and ion-subplantation, and also to examine the physical properties of nanotechnological devices that have not or cannot yet be created. In biophysics and structural biology, the method is frequently applied to study the motions of macromolecules such as proteins and nucleic acids, which can be useful for interpreting the results of certain biophysical experiments and for modeling interactions with other molecules, as in ligand docking. In principle MD can be used for ab initio prediction of protein structure by simulating folding of the polypeptide chain from random coil.\n\nThe results of MD simulations can be tested through comparison to experiments that measure molecular dynamics, of which a popular method is NMR spectroscopy. MD-derived structure predictions can be tested through community-wide experiments in Critical Assessment of protein Structure Prediction (CASP), although the method has historically had limited success in this area. Michael Levitt, who shared the Nobel Prize partly for the application of MD to proteins, wrote in 1999 that CASP participants usually did not use the method due to \"... a central embarrassment of molecular mechanics, namely that energy minimization or molecular dynamics generally leads to a model that is less like the experimental structure.\"\" Improvements in computational resources permitting more and longer MD trajectories, combined with modern improvements in the quality of force field parameters, have yielded some improvements in both structure prediction and homology model refinement, without reaching the point of practical utility in these areas; many identify force field parameters as a key area for further development.\n\nLimits of the method are related to the parameter sets used, and to the underlying molecular mechanics force fields. One run of an MD simulation optimizes the potential energy, rather than the free energy of the protein , meaning that all entropic contributions to thermodynamic stability of protein structure are neglected, including the conformational entropy of the polypeptide chain (the main factor that destabilizes protein structure) and hydrophobic effects (the main driving forces of protein folding). Another important factor are intramolecular hydrogen bonds, which are not explicitly included in modern force fields, but described as Coulomb interactions of atomic point charges. This is a crude approximation because hydrogen bonds have a partially quantum mechanical and chemical nature. Furthermore, electrostatic interactions are usually calculated using the dielectric constant of vacuum, although the surrounding aqueous solution has a much higher dielectric constant. Using the macroscopic dielectric constant at short interatomic distances is questionable. Finally, van der Waals interactions in MD are usually described by Lennard-Jones potentials based on the Fritz London theory that is only applicable in vacuum. However, all types of van der Waals forces are ultimately of electrostatic origin and therefore depend on dielectric properties of the environment. The direct measurement of attraction forces between different materials (as Hamaker constant) shows that \"the interaction between hydrocarbons across water is about 10% of that across vacuum\". The environment-dependence of van der Waals forces is neglected in standard simulations, but can be included by developing polarizable force fields.\n\nDesign of a molecular dynamics simulation should account for the available computational power. Simulation size (n=number of particles), timestep, and total time duration must be selected so that the calculation can finish within a reasonable time period. However, the simulations should be long enough to be relevant to the time scales of the natural processes being studied. To make statistically valid conclusions from the simulations, the time span simulated should match the kinetics of the natural process. Otherwise, it is analogous to making conclusions about how a human walks when only looking at less than one footstep. Most scientific publications about the dynamics of proteins and DNA use data from simulations spanning nanoseconds (10 s) to microseconds (10 s). To obtain these simulations, several CPU-days to CPU-years are needed. Parallel algorithms allow the load to be distributed among CPUs; an example is the spatial or force decomposition algorithm.\n\nDuring a classical MD simulation, the most CPU intensive task is the evaluation of the potential as a function of the particles' internal coordinates. Within that energy evaluation, the most expensive one is the non-bonded or non-covalent part. In Big O notation, common molecular dynamics simulations scale by formula_1 if all pair-wise electrostatic and van der Waals interactions must be accounted for explicitly. This computational cost can be reduced by employing electrostatics methods such as particle mesh Ewald summation ( formula_2 ), particle–particle-particle–mesh (P3M), or good spherical cutoff methods ( formula_3 ). \n\nAnother factor that impacts total CPU time needed by a simulation is the size of the integration timestep. This is the time length between evaluations of the potential. The timestep must be chosen small enough to avoid discretization errors (i.e., smaller than the period related to fastest vibrational frequency in the system). Typical timesteps for classical MD are in the order of 1 femtosecond (10 s). This value may be extended by using algorithms such as the SHAKE constraint algorithm, which fix the vibrations of the fastest atoms (e.g., hydrogens) into place. Multiple time scale methods have also been developed, which allow extended times between updates of slower long-range forces.\n\nFor simulating molecules in a solvent, a choice should be made between explicit and implicit solvent. Explicit solvent particles (such as the TIP3P, SPC/E and SPC-f water models) must be calculated expensively by the force field, while implicit solvents use a mean-field approach. Using an explicit solvent is computationally expensive, requiring inclusion of roughly ten times more particles in the simulation. But the granularity and viscosity of explicit solvent is essential to reproduce certain properties of the solute molecules. This is especially important to reproduce chemical kinetics.\n\nIn all kinds of molecular dynamics simulations, the simulation box size must be large enough to avoid boundary condition artifacts. Boundary conditions are often treated by choosing fixed values at the edges (which may cause artifacts), or by employing periodic boundary conditions in which one side of the simulation loops back to the opposite side, mimicking a bulk phase (which may cause artifacts too).\n\nIn the microcanonical ensemble, the system is isolated from changes in moles (N), volume (V), and energy (E). It corresponds to an adiabatic process with no heat exchange. A microcanonical molecular dynamics trajectory may be seen as an exchange of potential and kinetic energy, with total energy being conserved. For a system of N particles with coordinates formula_4 and velocities formula_5, the following pair of first order differential equations may be written in Newton's notation as\n\nThe potential energy function formula_8 of the system is a function of the particle coordinates formula_4. It is referred to simply as the \"potential\" in physics, or the \"force field\" in chemistry. The first equation comes from Newton's laws of motion; the force formula_10 acting on each particle in the system can be calculated as the negative gradient of formula_8.\n\nFor every time step, each particle's position formula_4 and velocity formula_5 may be integrated with a symplectic integrator method such as Verlet integration. The time evolution of formula_4 and formula_5 is called a trajectory. Given the initial positions (e.g., from theoretical knowledge) and velocities (e.g., randomized Gaussian), we can calculate all future (or past) positions and velocities.\n\nOne frequent source of confusion is the meaning of temperature in MD. Commonly we have experience with macroscopic temperatures, which involve a huge number of particles. But temperature is a statistical quantity. If there is a large enough number of atoms, statistical temperature can be estimated from the \"instantaneous temperature\", which is found by equating the kinetic energy of the system to nkT/2 where n is the number of degrees of freedom of the system.\n\nA temperature-related phenomenon arises due to the small number of atoms that are used in MD simulations. For example, consider simulating the growth of a copper film starting with a substrate containing 500 atoms and a deposition energy of 100 eV. In the real world, the 100 eV from the deposited atom would rapidly be transported through and shared among a large number of atoms (formula_16 or more) with no big change in temperature. When there are only 500 atoms, however, the substrate is almost immediately vaporized by the deposition. Something similar happens in biophysical simulations. The temperature of the system in NVE is naturally raised when macromolecules such as proteins undergo exothermic conformational changes and binding.\n\nIn the canonical ensemble, amount of substance (N), volume (V) and temperature (T) are conserved. It is also sometimes called constant temperature molecular dynamics (CTMD). In NVT, the energy of endothermic and exothermic processes is exchanged with a thermostat.\n\nA variety of thermostat algorithms are available to add and remove energy from the boundaries of a MD simulation in a more or less realistic way, approximating the canonical ensemble. Popular methods to control temperature include velocity rescaling, the Nosé-Hoover thermostat, Nosé-Hoover chains, the Berendsen thermostat, the Andersen thermostat and Langevin dynamics. The Berendsen thermostat might introduce the flying ice cube effect, which leads to unphysical translations and rotations of the simulated system.\n\nIt is not trivial to obtain a canonical ensemble distribution of conformations and velocities using these algorithms. How this depends on system size, thermostat choice, thermostat parameters, time step and integrator is the subject of many articles in the field.\n\nIn the isothermal–isobaric ensemble, amount of substance (N), pressure (P) and temperature (T) are conserved. In addition to a thermostat, a barostat is needed. It corresponds most closely to laboratory conditions with a flask open to ambient temperature and pressure.\n\nIn the simulation of biological membranes, isotropic pressure control is not appropriate. For lipid bilayers, pressure control occurs under constant membrane area (NPAT) or constant surface tension \"gamma\" (NPγT).\n\nThe replica exchange method is a generalized ensemble. It was originally created to deal with the slow dynamics of disordered spin systems. It is also called parallel tempering. The replica exchange MD (REMD) formulation tries to overcome the multiple-minima problem by exchanging the temperature of non-interacting replicas of the system running at several temperatures.\n\nA molecular dynamics simulation requires the definition of a potential function, or a description of the terms by which the particles in the simulation will interact. In chemistry and biology this is usually referred to as a force field and in materials physics as an interatomic potential. Potentials may be defined at many levels of physical accuracy; those most commonly used in chemistry are based on molecular mechanics and embody a classical mechanics treatment of particle-particle interactions that can reproduce structural and conformational changes but usually cannot reproduce chemical reactions.\n\nThe reduction from a fully quantum description to a classical potential entails two main approximations. The first one is the Born–Oppenheimer approximation, which states that the dynamics of electrons are so fast that they can be considered to react instantaneously to the motion of their nuclei. As a consequence, they may be treated separately. The second one treats the nuclei, which are much heavier than electrons, as point particles that follow classical Newtonian dynamics. In classical molecular dynamics, the effect of the electrons is approximated as one potential energy surface, usually representing the ground state.\n\nWhen finer levels of detail are needed, potentials based on quantum mechanics are used; some methods attempt to create hybrid classical/quantum potentials where the bulk of the system is treated classically but a small region is treated as a quantum system, usually undergoing a chemical transformation.\n\nEmpirical potentials used in chemistry are frequently called force fields, while those used in materials physics are called interatomic potentials.\n\nMost force fields in chemistry are empirical and consist of a summation of bonded forces associated with chemical bonds, bond angles, and bond dihedrals, and non-bonded forces associated with van der Waals forces and electrostatic charge. Empirical potentials represent quantum-mechanical effects in a limited way through ad-hoc functional approximations. These potentials contain free parameters such as atomic charge, van der Waals parameters reflecting estimates of atomic radius, and equilibrium bond length, angle, and dihedral; these are obtained by fitting against detailed electronic calculations (quantum chemical simulations) or experimental physical properties such as elastic constants, lattice parameters and spectroscopic measurements.\n\nBecause of the non-local nature of non-bonded interactions, they involve at least weak interactions between all particles in the system. Its calculation is normally the bottleneck in the speed of MD simulations. To lower the computational cost, force fields employ numerical approximations such as shifted cutoff radii, reaction field algorithms, particle mesh Ewald summation, or the newer particle–particle-particle–mesh (P3M).\n\nChemistry force fields commonly employ preset bonding arrangements (an exception being \"ab initio\" dynamics), and thus are unable to model the process of chemical bond breaking and reactions explicitly. On the other hand, many of the potentials used in physics, such as those based on the bond order formalism can describe several different coordinations of a system and bond breaking. Examples of such potentials include the Brenner potential for hydrocarbons and its\nfurther developments for the C-Si-H and C-O-H systems. The\nReaxFF potential can be considered a fully reactive hybrid between bond order potentials and chemistry force fields.\n\nThe potential functions representing the non-bonded energy are formulated as a sum over interactions between the particles of the system. The simplest choice, employed in many popular force fields, is the \"pair potential\", in which the total potential energy can be calculated from the sum of energy contributions between pairs of atoms. An example of such a pair potential is the non-bonded Lennard–Jones potential (also termed the 6–12 potential), used for calculating van der Waals forces.\n\nAnother example is the Born (ionic) model of the ionic lattice. The first term in the next equation is Coulomb's law for a pair of ions, the second term is the short-range repulsion explained by Pauli's exclusion principle and the final term is the dispersion interaction term. Usually, a simulation only includes the dipolar term, although sometimes the quadrupolar term is also included.(Usually termed Buckingham potential model)\n\nIn many-body potentials, the potential energy includes the effects of three or more particles interacting with each other. In simulations with pairwise potentials, global interactions in the system also exist, but they occur only through pairwise terms. In many-body potentials, the potential energy cannot be found by a sum over pairs of atoms, as these interactions are calculated explicitly as a combination of higher-order terms. In the statistical view, the dependency between the variables cannot in general be expressed using only pairwise products of the degrees of freedom. For example, the Tersoff potential, which was originally used to simulate carbon, silicon, and germanium, and has since been used for a wide range of other materials, involves a sum over groups of three atoms, with the angles between the atoms being an important factor in the potential. Other examples are the embedded-atom method (EAM), the EDIP, and the Tight-Binding Second Moment Approximation (TBSMA) potentials, where the electron density of states in the region of an atom is calculated from a sum of contributions from surrounding atoms, and the potential energy contribution is then a function of this sum.\n\nSemi-empirical potentials make use of the matrix representation from quantum mechanics. However, the values of the matrix elements are found through empirical formulae that estimate the degree of overlap of specific atomic orbitals. The matrix is then diagonalized to determine the occupancy of the different atomic orbitals, and empirical formulae are used once again to determine the energy contributions of the orbitals.\n\nThere are a wide variety of semi-empirical potentials, termed tight-binding potentials, which vary according to the atoms being modeled.\n\nMost classical force fields implicitly include the effect of polarizability, e.g., by scaling up the partial charges obtained from quantum chemical calculations. These partial charges are stationary with respect to the mass of the atom. But molecular dynamics simulations can explicitly model polarizability with the introduction of induced dipoles through different methods, such as Drude particles or fluctuating charges. This allows for a dynamic redistribution of charge between atoms which responds to the local chemical environment.\n\nFor many years, polarizable MD simulations have been touted as the next generation. For homogenous liquids such as water, increased accuracy has been achieved through the inclusion of polarizability. Some promising results have also been achieved for proteins. However, it is still uncertain how to best approximate polarizability in a simulation.\n\nIn classical molecular dynamics, one potential energy surface (usually the ground state) is represented in the force field. This is a consequence of the Born–Oppenheimer approximation. In excited states, chemical reactions or when a more accurate representation is needed, electronic behavior can be obtained from first principles by using a quantum mechanical method, such as density functional theory. This is named Ab Initio Molecular Dynamics (AIMD). Due to the cost of treating the electronic degrees of freedom, the computational cost of these simulations is far higher than classical molecular dynamics. This implies that AIMD is limited to smaller systems and shorter times.\n\n\"Ab initio\" quantum mechanical and chemical methods may be used to calculate the potential energy of a system on the fly, as needed for conformations in a trajectory. This calculation is usually made in the close neighborhood of the reaction coordinate. Although various approximations may be used, these are based on theoretical considerations, not on empirical fitting. \"Ab initio\" calculations produce a vast amount of information that is not available from empirical methods, such as density of electronic states or other electronic properties. A significant advantage of using \"ab initio\" methods is the ability to study reactions that involve breaking or formation of covalent bonds, which correspond to multiple electronic states.\n\nQM (quantum-mechanical) methods are very powerful. However, they are computationally expensive, while the MM (classical or molecular mechanics) methods are fast but suffer from several limits (require extensive parameterization; energy estimates obtained are not very accurate; cannot be used to simulate reactions where covalent bonds are broken/formed; and are limited in their abilities for providing accurate details regarding the chemical environment). A new class of method has emerged that combines the good points of QM (accuracy) and MM (speed) calculations. These methods are termed mixed or hybrid quantum-mechanical and molecular mechanics methods (hybrid QM/MM).\n\nThe most important advantage of hybrid QM/MM method is the speed. The cost of doing classical molecular dynamics (MM) in the most straightforward case scales O(n), where n is the number of atoms in the system. This is mainly due to electrostatic interactions term (every particle interacts with every other particle). However, use of cutoff radius, periodic pair-list updates and more recently the variations of the particle-mesh Ewald's (PME) method has reduced this to between O(n) to O(n). In other words, if a system with twice as many atoms is simulated then it would take between two and four times as much computing power. On the other hand, the simplest \"ab initio\" calculations typically scale O(n) or worse (restricted Hartree–Fock calculations have been suggested to scale ~O(n)). To overcome the limit, a small part of the system is treated quantum-mechanically (typically active-site of an enzyme) and the remaining system is treated classically.\n\nIn more sophisticated implementations, QM/MM methods exist to treat both light nuclei susceptible to quantum effects (such as hydrogens) and electronic states. This allows generating hydrogen wave-functions (similar to electronic wave-functions). This methodology has been useful in investigating phenomena such as hydrogen tunneling. One example where QM/MM methods have provided new discoveries is the calculation of hydride transfer in the enzyme liver alcohol dehydrogenase. In this case, quantum tunneling is important for the hydrogen, as it determines the reaction rate.\n\nAt the other end of the detail scale are coarse-grained and lattice models. Instead of explicitly representing every atom of the system, one uses \"pseudo-atoms\" to represent groups of atoms. MD simulations on very large systems may require such large computer resources that they cannot easily be studied by traditional all-atom methods. Similarly, simulations of processes on long timescales (beyond about 1 microsecond) are prohibitively expensive, because they require so many time steps. In these cases, one can sometimes tackle the problem by using reduced representations, which are also called coarse-grained models.\n\nExamples for coarse graining (CG) methods are discontinuous molecular dynamics (CG-DMD) and Go-models. Coarse-graining is done sometimes taking larger pseudo-atoms. Such united atom approximations have been used in MD simulations of biological membranes. Implementation of such approach on systems where electrical properties are of interest can be challenging owing to the difficulty of using a proper charge distribution on the pseudo-atoms. The aliphatic tails of lipids are represented by a few pseudo-atoms by gathering 2 to 4 methylene groups into each pseudo-atom.\n\nThe parameterization of these very coarse-grained models must be done empirically, by matching the behavior of the model to appropriate experimental data or all-atom simulations. Ideally, these parameters should account for both enthalpic and entropic contributions to free energy in an implicit way. When coarse-graining is done at higher levels, the accuracy of the dynamic description may be less reliable. But very coarse-grained models have been used successfully to examine a wide range of questions in structural biology, liquid crystal organization, and polymer glasses.\n\nExamples of applications of coarse-graining:\n\nThe simplest form of coarse-graining is the \"united atom\" (sometimes called \"extended atom\") and was used in most early MD simulations of proteins, lipids, and nucleic acids. For example, instead of treating all four atoms of a CH methyl group explicitly (or all three atoms of CH methylene group), one represents the whole group with one pseudo-atom. It must, of course, be properly parameterized so that its van der Waals interactions with other groups have the proper distance-dependence. Similar considerations apply to the bonds, angles, and torsions in which the pseudo-atom participates. In this kind of united atom representation, one typically eliminates all explicit hydrogen atoms except those that have the capability to participate in hydrogen bonds (\"polar hydrogens\"). An example of this is the CHARMM 19 force-field.\n\nThe polar hydrogens are usually retained in the model, because proper treatment of hydrogen bonds requires a reasonably accurate description of the directionality and the electrostatic interactions between the donor and acceptor groups. A hydroxyl group, for example, can be both a hydrogen bond donor, and a hydrogen bond acceptor, and it would be impossible to treat this with one OH pseudo-atom. About half the atoms in a protein or nucleic acid are non-polar hydrogens, so the use of united atoms can provide a substantial savings in computer time.\n\nIn many simulations of a solute-solvent system the main focus is on the behavior of the solute with little interest of the solvent behavior particularly in those solvent molecules residing in regions far from the solute molecule. Solvents may influence the dynamic behavior of solutes via random collisions and by imposing a frictional drag on the motion of the solute through the solvent. The use of non-rectangular periodic boundary conditions, stochastic boundaries and solvent shells can all help reduce the number of solvent molecules required and enable a larger proportion of the computing time to be spent instead on simulating the solute. It is also possible to incorporate the effects of a solvent without needing any explicit solvent molecules present. One example of this approach is to use a potential mean force (PMF) which describes how the free energy changes as a particular coordinate is varied. The free energy change described by PMF contains the averaged effects of the solvent.\n\nA long range interaction is an interaction in which the spatial interaction falls off no faster than formula_19 where formula_20 is the dimensionality of the system. Examples include charge-charge interactions between ions and dipole-dipole interactions between molecules. Modelling these forces presents quite a challenge as they are significant over a distance which may be larger than half the box length with simulations of many thousands of particles. Though one solution would be to significantly increase the size of the box length, this brute force approach is less than ideal as the simulation would become computationally very expensive. Spherically truncating the potential is also out of the question as unrealistic behaviour may be observed when the distance is close to the cut off distance.\n\nSteered molecular dynamics (SMD) simulations, or force probe simulations, apply forces to a protein in order to manipulate its structure by pulling it along desired degrees of freedom. These experiments can be used to reveal structural changes in a protein at the atomic level. SMD is often used to simulate events such as mechanical unfolding or stretching.\n\nThere are two typical protocols of SMD: one in which pulling velocity is held constant, and one in which applied force is constant. Typically, part of the studied system (e.g., an atom in a protein) is restrained by a harmonic potential. Forces are then applied to specific atoms at either a constant velocity or a constant force. Umbrella sampling is used to move the system along the desired reaction coordinate by varying, for example, the forces, distances, and angles manipulated in the simulation. Through umbrella sampling, all of the system's configurations—both high-energy and low-energy—are adequately sampled. Then, each configuration's change in free energy can be calculated as the potential of mean force. A popular method of computing PMF is through the weighted histogram analysis method (WHAM), which analyzes a series of umbrella sampling simulations.\n\nMolecular dynamics is used in many fields of science.\n\nThe following biophysical examples illustrate notable efforts to produce simulations of a systems of very large size (a complete virus) or very long simulation times (up to 1.112 milliseconds):\n\n\n\n\n\n\n\n\n\n"}
{"id": "22722859", "url": "https://en.wikipedia.org/wiki?curid=22722859", "title": "Murder of Ross Parker", "text": "Murder of Ross Parker\n\nRoss Andrew Parker (17 August 1984 – 21 September 2001), from Peterborough, England, was a 17-year-old\nWhite English male murdered in an unprovoked racially motivated crime. He bled to death after being stabbed, beaten with a hammer, and repeatedly kicked by a gang of British Pakistani youths. The incident occurred in Millfield, Peterborough, ten days after the September 11 attacks.\n\nIn December 2002, Shaied Nazir, Ahmed Ali Awan, and Sarfraz Ali were all, unanimously, found guilty of Parker's murder and sentenced to life imprisonment, each receiving minimum terms ranging from 16 to 18 years. A fourth defendant, Zairaff Mahrad, was cleared of murder and manslaughter.\n\nA memorial plaque for Parker is located in the Netherton area of Peterborough where a football match is played each year in his memory.\n\nRoss Parker was born in Peterborough in 1984 to Davinia and Tony Parker. His mother worked as a waitress and his father ran a car bodywork repair business; he was one of two children. Parker was an avid football player and, having completed a GNVQ in business studies at Jack Hunt School, hoped to join the police force when he was 18. He was nicknamed \"Half-Pint\" owing to his 5-foot, 5-inch height and had twice broken his leg previously. Parker lived in the Westwood area of the city and worked part-time as bar support at The Solstice, a local public house where he had met his girlfriend, Nicola Toms.\n\nParker was murdered shortly after 1:15 a.m. on Friday 21 September 2001 while walking with his girlfriend. The attack took place on a cycle path alongside Bourges Boulevard in Millfield, Peterborough, near to Russell Street. Racial tensions in the area were high as the September 11 attacks had only occurred ten days earlier.\n\nHaving finished work early, Parker and Toms were walking to visit her friend's house when they were confronted by a gang of around ten Pakistani youths, some of whom were wearing balaclavas; the 2002 trial judge concluded that they had planned to find \"a white male to attack simply because he was white\" in the context of \"hostility on the part of some of the younger white residents of the city against the Asian community\". They warned Parker he had \"better start running\", but then blocked his path and quickly sprayed him in the face with CS gas. He was punched in the stomach then stabbed three times from behind through the throat and chest with a foot-long hunting knife. The knife penetrated completely through his body on two occasions and as he was lying on the ground he was repeatedly kicked and struck with a panel beater's hammer.\n\nToms ran to a nearby petrol station for help and a man there gave her his mobile phone to call the police. While making the call, she twice heard Parker cry out in pain. By chance she spotted a passing police car. She entered the car and guided the officer to the scene of the assault. Although Toms had only been away for a few minutes, by the time she returned Parker had already bled to death and the gang had disappeared.\n\nAfter the murder, four of the gang returned to a garage which they used as their headquarters. Ahmed Ali Awan, brandishing the bloodied knife, exclaimed \"cherish the blood\". The police informed Parker's family of his death at 4:30 am; his body remained at the scene during the day while an investigation was conducted. A post-mortem revealed Parker had died as a result of stab wounds inflicted by a bladed instrument.\n\nParker's murder sparked what became one of the biggest police inquiries in the history of Peterborough. During the weekend following the attack, twelve suspects of Pakistani descent were arrested on suspicion of murder. Members of the local community posted a £1,000 reward for information leading to the arrest of the killers, later increasing to £1,500. A number of those arrested were recorded chanting \"Taliban, Osama bin Laden\" in their cells and while being transported in the police van, which they also vandalised. Detective Chief Inspector Dick Harrison, who was overseeing the case, praised the city's Muslim community for their involvement in capturing the murderers.\n\nOn 26 September 2001, Sarfraz Ali, Ahmed Ali Awan and Shaied Nazir appeared in court charged with Parker's murder. Zairaff Mahrad was charged the following day. However, by March 2002 all four defendants had been controversially released on police bail. Parker's sister, Leanne, stated \"we can't begin to comprehend why they've been allowed out of prison at this stage\". Parker's family were so concerned about the decision that they wrote a letter of complaint to the Home Secretary, David Blunkett. The Home Office refused to comment on the case and the men remained free on bail.\n\nOn 7 November 2002, Awan, Nazir, Ali and Mahrad all of Millfield, Peterborough, stood trial for Parker's murder at Northampton Crown Court, pleading not guilty.\n\nAwan, 22, ran a recruitment company and had previously attended the city's Deacon's School. He was an unofficial police informer and the court was told he thought of himself as a gangster and had a \"fantasy for knives\". Nazir, 22, was close friends with Awan. He was married in Kashmir the month prior to the murder and had a son. He was educated at Bretton Woods School, had worked in a factory and later in a takeaway restaurant with Mahrad, whom he had known since childhood. Ali, 25, had also attended Bretton Woods School and was also married, though his wife left him during the trial. He was given a reference at the trial by the Deputy Mayor of Peterborough, Raja Akhtar. and Labour Party councillor Mohammed Choudhary, with Akhtar stating he had \"known him to be caring and responsible\". Mahrad, 21, owned a takeaway restaurant business in King's Lynn.\n\nDuring the six-week trial, transcripts of covert police recordings of the suspects discussing the attack were submitted as evidence. These conversations took place in police vehicles when the suspects were arrested and were translated from Punjabi. Nazir was heard describing Parker's death as a \"bloodbath\", and how the third blow from the knife had split the whole of his neck open. Awan and Nazir were both heard discussing the statements they had given to police and the plan they had \"made up\". The court was also told of an exchange between Awan and an inmate at Bedford Prison, in which Awan described the killing of Parker in lurid detail. Contents of a letter written by Mahrad were also presented in which he stated he would \"pray to Allah for forgiveness\".\n\nThe jury heard how the murder weapons had been found in a shed at Nazir's house along with two bags of bloodied clothes. DNA and fingerprints belonging to Nazir was found on the hunting knife and Parker's blood was found on both the hammer and knife. His blood was also found on the clothes of two of the accused, along with Nazir and Mahrad's DNA and a pathologist told how marks on Parker's body matched the hammer found in Nazir's shed. Three balaclavas were also recovered from the property, again containing traces of Parker's blood, Nazir's younger brother Wyed told the court he had seen his brother cleaning the murder weapon on the night of the killing and witnessed all four defendants with blood on their clothing. Further witnesses reported Nazir admitting to beating someone up, Mahrad admitting to kicking Parker and Awan recalling stabbing him. A witness also saw all four defendants kicking Parker.\n\nIn court Nazir admitted to seeing the victim lying on the ground and attempting to spray him with CS gas and kicking him. He also acknowledged washing the murder weapon and stated Ali had hit Parker with the hammer and Awan used the knife. Nazir also alleged prosecution witnesses Zaheer Abbas and Adeel Rehman had been involved in the attack too. Mahrad had also admitted to being present at the murder scene, and claimed that the blood stains found on his trousers occurred as a result of his accidentally falling across Parker. Ali, defended by Mohammed Latif, denied being at the scene and claimed to have been asleep at the time of the murder, although a recording from a police van indicated this was a \"story\" that he had encouraged the others to \"stick to\". Awan also denied being present and claimed he had been at home playing on his PlayStation with Shokat Awan, his brother.\nNigel Rumfitt QC, defending Awan, summarised the crime by stating: \"These people were not taking the night air. Every member of the group knew what was going on. These weapons had been selected before they set off. The knife was far too big to be hidden from the others. There is no doubt there was a hunting party looking for a victim.\"\n\nOn 19 December 2002, Nazir, Awan and Ali were all found guilty of murder in unanimous verdicts. The judge, Sir Edwin Jowitt, summarised the murder during sentencing:\n\nThe judge concluded that Awan had wielded the knife, was the ringleader of the group and had intended to kill. The three received life sentences, with Awan to serve a minimum of 18 years and the others at least 16. Mahrad was cleared of murder and manslaughter.\n\nAfter the trial it was revealed that Nazir had previously been cautioned for using threatening behaviour in 1999 and fined for resisting arrest. The two Labour politicians, Akhtar and Choudhary, who provided Ali's references were later jailed themselves for forgery in relation to vote rigging.\n\nAwan and Nazir appealed against their jail terms in January 2008 but the original sentences were upheld. Judge Justice Davis said he had taken into account \"moving\" statements from Parker's family. Ali appealed his sentence in July 2009 and also had his sentence upheld. After the first two appeals Parker's father suggested that the killers of his son should never be freed. As noted by Justice Davis, had the crime occurred post-2005, then it is likely that significantly higher minimum terms would have been imposed on the perpetrators owing to reforms in the Criminal Justice Act 2003. These reforms recommend a minimum term of 30 years for racially aggravated murder.\n\nThe BBC Editorial Standards Committee in 2007 found that \"there was no evidence to suggest that the BBC had shown a specific and systemic bias in favour of cases where the victim had been black or Asian\", but accepted it had \"underplayed its coverage of the Ross Parker case\" and repeated the failings in its coverage of the Murder of Kriss Donald.\n\nYasmin Alibhai-Brown and Kelvin MacKenzie expressed similar sentiments. MacKenzie criticised newspapers including his own employer, \"The Sun\". He stated, \"if you believe you're a victim of an ethnic minority and you're white there is nowhere to go. Editors are so liberal that they are scared to be seen that they're moving to the right of their paper\". Parker's mother, Davinia, expressed similar concerns that white victims of race crime are ignored. She said \"because we are white, English, we didn't get the coverage\", adding \"\"it's as if we don't count\".\n\nIn 2006, a \"Sunday Times\" investigation by Brendan Montague examined British newspaper archives for coverage of racist crimes, finding \"an almost total boycott of stories involving the white victims of attacks\" whereas \"cases involving black and minority ethnic victims are widely reported\".\n\nOthers have noted that the lack of coverage is not simply a media issue. Peter Fahy, the spokesman on race issues for the Association of Chief Police Officers said: \"A lot of police officers and other professionals feel almost the best thing to do is to try and avoid [discussing such attacks] for fear of being criticised. This is not healthy\". Montague suggests the lack of police appeals in cases involving white victims may be a cause of the lack of media coverage. Evidence of this was seen in the Parker case, with the police initially appearing keen to dismiss the possible racist aspect of the murder, stating \"there was no reason to believe that the attack was racially motivated\".\n\nAmerican Townhall commentator Larry Provost focused on the religion of the attackers as the key factor in the murder. Writing in August 2014, he suggested the perpetrators were Islamists who had been \"motivated by religion\".\n\nThe newspaper which covered Parker's murder more extensively faced criticism. The Government Office for the East of England produced a report by Dr Roger Green examining race relations in Peterborough. The document suggested that the \"Peterborough Evening Telegraph\" had a history of insensitivity, and coverage of the case was \"possibly adding to any climate of racial and communal unrest\". The criticism of the paper was rejected by a senior police officer and an Asian community leader, both of whom praised its handling of the case.\n\nThe leader of Peterborough City Council called the report by Dr Green\nunfair.\n\nParker's murder led to increased racial tensions in Peterborough. At his former school, three Asian pupils were suspended for an attack on an Afro-Caribbean pupil; a relative of the victim then attacked an Asian teacher. A number of taxi firms stopped work early in the days after the attack in fear of reprisals. In November 2001 Home Secretary David Blunkett banned all marches in Peterborough for three months as it was feared violence would be caused by the Anti-Nazi League and National Front who both sought to hold protests on the same day.\n\nParker's death also had a major impact on his family. His mother, Davinia, was unable to work for three months after Parker's funeral and came close to attempting suicide on a number of occasions. Parker's room was left largely untouched for three years after the incident because his parents were reluctant to tidy it. They described the room as a place they \"feel close to Ross\".\n\nAs a result of the murder of Parker, local authorities set up a unity scheme, whereby gang members from different communities were trained as youth workers to ease racial tensions and reduce violence. However, problems still persisted, with racist graffiti painted by \"yobs\" and \"thugs\" near the murder scene two years after the incident reading \"no go area for whites\", \"Paki powa\" and \"death to whites\".\n\nMark Easton cited the Parker case as demonstrating how society has been forced to redefine racism and discard the erroneous definition of \"prejudice plus power\"—a definition which, in Easton's view, tended to only allow ethnic minorities to be victims and whites to be perpetrators. He also considered that \"Describing an incident as racist may say as much about a victim's mindset as the offender. How else can one explain the British Crime Survey finding that 3,100 car thefts from Asians were deemed to be racially motivated?\" Yasmin Alibhai-Brown condemned the double standards of racial equality campaigners in relation to the case, suggesting black activists should \"march and remember victims like Ross Parker ... our values are worthless unless all victims of these senseless deaths matter equally\". She went on to write that \"to treat some victims as more worthy of condemnation than others is unforgivable and a betrayal of anti-racism itself\".\n\nParker's funeral took place at Peterborough Crematorium in Marholm on 23 October 2001 and more than 400 mourners attended. His \"number 14\" Netherton United football shirt was draped over his coffin and his teammates formed a guard of honour dressed in the same strip (kit). The Reverend Geoffrey Keating described Parker as \"a beacon of light who inspired so many people\" and \"an extremely popular young man\".\n\nThe murder received little attention from politicians, although in 2003 Peterborough Member of Parliament Helen Clark made a statement in the House of Commons sending condolences to Parker's family, paying tribute to the Peterborough community and in particular Parker's former school. She described Parker's killers only as \"men older than him\" and her tributes were echoed by John Denham.\n\nA plaque was installed in Netherton in Peterborough as a memorial to Parker, and a further memorial is located at Peterborough Crematorium. His former football team mates and friends also play a match every May in his memory and formed a team called \"Ross' Rangers\".\n\nA rose bush was also planted at the Parker family home in remembrance.\n\n"}
{"id": "10872014", "url": "https://en.wikipedia.org/wiki?curid=10872014", "title": "Nordic Language Convention", "text": "Nordic Language Convention\n\nThe Nordic Language Convention is a convention of linguistic rights that came into force on 1 March 1987, under the auspices of the Nordic Council. Under the Convention, citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable to any interpretation or translation costs. The Convention covers health care, social security, tax, school, and employment authorities, the police and\ncourts. The languages included are Swedish, Danish, Norwegian, Finnish and Icelandic.\n\nThe Convention is not very well known and is mostly a recommendation. The countries have committed themselves to providing services in various languages, but citizens have no absolute rights except for criminal and court matters. The Convention does not automatically require authorities to provide services in another language but a citizen must demand an interpreter. Civil servants in official institutions are often unaware of the regulations on interpreting and translating and neglect to provide these services when requested. Furthermore, the convention excludes minority languages, like Faroese, Kalaallisut, Romany and Sami, and immigrant languages. English has also assumed an increasingly prominent role in interaction between Nordic citizens.\n\n\n\n"}
{"id": "27179756", "url": "https://en.wikipedia.org/wiki?curid=27179756", "title": "Organic form", "text": "Organic form\n\nIn romantic literature, a work has organic form if the structure has originated from the materials and subjects used by the author. Using the organic metaphor, the structure is seen to grow as a plant. It stands in contrast to a mechanical form, a work which has been produced in accordance with artificial rules. The lack of rules in Shakespeare's works led some critics to claim that they lacked form; Samuel Taylor Coleridge leapt to his defence with the concept of organic form.\n\nColeridge, an English poet, philosopher, literary critic, and founder of the Romantic movement, suggested that the concept of organic form meant that a poem or literary piece was shaped, rather than structured, from within. The use of the form allowed a piece to uniquely develop itself as it unfolded, and ultimately revealed an emphasis on the whole outcome of the piece, including the connections of each development to each other. \nIn contrast to the more mechanical processes and rules which many critics believed were necessary for the formation of poetry and works, S.T. Coleridge determined that a more subconscious approach was possible through the, ‘‘imagination of the artist’’ whereby the outcome is an organic form, where ‘‘content and form have coalesced and fused.’’\n\nColeridge's explanation can be found in Vol. 2 of \"Twentieth Century Literature in English\"\n\nIn R.A. Foake’s Introduction to \"Coleridge’s criticism of Shakespeare: a selection\", he defines Coleridge’s defense of Shakespeare’s works as ‘’an act of sympathetic imagination, to enter into the spirit of each work, to reveal its inner organizing principle, and to show how Shakespeare, properly understood, was always in control and exercising judgement,’’\n\nRegarding Shakespeare’s much criticized erratic form, Coleridge further imposed the possibilities of organic form:\n\nFollowing on from Coleridge’s 18th Century ideas on organic form, was Gerard Manley Hopkins, one of the most revered poets of the Victorian era \nHopkins introduced the terms “inscape” and “instress”. “Inscape” was the core components in individual objects, allowing him to then home in on its relation to other objects and their perception as a whole. “Instress” focussed on the assimilating the immediate apperception and the sensory processes of perception .\n\n20th-Century American poet Denise Levertov was an artist of organic form. In ‘Some notes on organic form’ she gives credit to Gerard Manley Hopkin’s influence on her own ideas and poetic form: \n\nLevertov believed that in order to achieve organic form in literature, the artist must be ‘brought to speech’ through a demanding sensory experience which renders them no choice but to put pen to paper. In her own work, Levertov concentrated on various techniques such as enjambment, popular with much of Shakespeare’s work, and ‘juxtaposition of key words’ for effect. Avenues were carefully sought and deliberated upon to create the ‘right words, the right image, the right arrangement of the lines on the page’. Many artists of organic form believed that a reader or audience were not immediately, if ever, considered during the construction of a poem or piece. Levertov, included, felt it was essential that a poem be produced from the ‘inner being of the poet’.\n\nOrganic form in literature was also theorized by Schelling. Bruce Matthews suggests that, insofar as Schelling understands life according to a schema of freedom of thinking, representations are not absolutely different, and subjects and objects are grounded in an identity that links them together. Matthews argues that Schelling’s organic philosophy strives to support an integrated framework where conceptual ideas are scrutinized, challenging ideas and concepts in hopes to create a more significant understanding of the world.\n\n\n"}
{"id": "4728741", "url": "https://en.wikipedia.org/wiki?curid=4728741", "title": "Overshooting model", "text": "Overshooting model\n\nThe overshooting model, or the exchange rate overshooting hypothesis, first developed by economist Rudi Dornbusch, is a theoretical explanation for high levels of exchange rate volatility. The key features of the model include the assumptions that goods' prices are sticky, or slow to change, in the short run, but the prices of currencies are flexible, that arbitrage in asset markets holds, via the uncovered interest parity equation, and that expectations of exchange rate changes are \"consistent\": that is, rational. The most important insight of the model is that adjustment lags in some parts of the economy can induce compensating volatility in others; specifically, when an exogenous variable changes, the short-term effect on the exchange rate can be greater than the long-run effect, so in the short term, the exchange rate \"overshoots\" its new equilibrium long-term value. \nDornbusch developed this model back when many economists held the view that ideal markets should reach equilibrium and stay there. Volatility in a market, from this perspective, could only be a consequence of imperfect or asymmetric information or adjustment obstacles in that market. Rejecting this view, Dornbusch argued that volatility is in fact a far more fundamental property than that.\n\nAccording to the model, when a change in monetary policy occurs (e.g., an unanticipated permanent increase in the money supply), the market will adjust to a new equilibrium between prices and quantities. Initially, because of the \"stickiness\" of prices of goods, the new short run equilibrium level will first be achieved through shifts in financial market prices. Then, gradually, as prices of goods \"unstick\" and shift to the new equilibrium, the foreign exchange market continuously reprices, approaching its new long-term equilibrium level. Only after this process has run its course will a new long-run equilibrium be attained in the domestic money market, the currency exchange market, and the goods market.\n\nAs a result, the foreign exchange market will initially overreact to a monetary change, achieving a new short run equilibrium. Over time, goods prices will eventually respond, allowing the foreign exchange market to dissipate its overreaction, and the economy to reach the new long run equilibrium in all markets.\n\nThat is to say, the position of the Investment Saving (IS) curve is determined by the volume of injections into the flow of income and by the competitiveness of Home country output measured by the real exchange rate.\n\nThe first assumption is essentially saying that the IS curve (demand for goods) position is in some way dependent on the real effective exchange rate Q.\n\nThat is, [IS = C + I + G +Nx(Q)]. In this case, net exports is dependent on Q (as Q goes up, foreign countries' goods are relatively more expensive, and home countries' goods are cheaper, therefore there are higher net exports).\nIf financial markets can adjust instantaneously and investors are risk neutral, it can be said the uncovered interest rate parity (UIP) holds at all times. That is, the equation r = r* + Δs holds at all times (explanation of this formula is below).\n\nIt is clear, then, that an expected depreciation/appreciation offsets any current difference in the exchange rate. If r > r*, the exchange rate (domestic price of one unit of foreign currency) is expected to increase. That is, the domestic currency depreciates relative to the foreign currency.\nIn the long run, the exchange rate(s) will equal the long run equilibrium exchange rate,(ŝ).\nFormal Notation\n\n[1] r = r* +Δse (uncovered interest rate parity - approximation)\n\n[2] Δs = θ(ŝ – s) (Expectations of market participants)\n\n[3] m - p = ky-lr (Demand/Supply on money)\n\n[4] y = h(s-p) = h(q) (demand for the home country output)\n\n[5] þ = π(yd- ŷ)(proportional change in prices with respect to time) dP/dTime\n\nFrom the above can be derived the following (using algebraic substitution)\n\n[6] p - p_hat = - lθ(ŝ - s)\n\n[7] þ = π[h(s-p) - ŷ]\n\nIn equilibrium\n\ny = ŷ (demand for output equals the long run demand for output)\n\nfrom this substitution shows that\n[8] ŷ/h = ŝ - p_hat\nThat is, in the long run, the only variable that affects the real exchange rate is growth in capacity output.\n\nAlso, \nΔs = 0 (that is, in the long run the expected change of inflection is equal to zero)\n\nSubstituting into [2] yields r = r*.\nSubstituting that into [6] shows:\n\n[9] p_hat = m -kŷ + l r*\n\ntaking [8] & [9] together:\n\n[10] ŝ = ŷ(h - k) + m +lr*\n\ncomparing [9] & [10], it is clear that the only difference between them is the intercept (that is the slope of both is the same). This reveals that given a rise in money stock pushes up the long run values of both in equally proportional measures, the real exchange rate (q) must remain at the same value as it was before the market shock. Therefore, the properties of the model at the beginning are preserved in long run equilibrium, the original equilibrium was stable.\n\nShort run disequilibrium\n\nThe standard approach is to rewrite the basic equations [6] & [7] in terms of the deviation from the long run equilibrium).\nIn equilibrium [7] implies 0 = π[h(ŝ-p_hat) - ŷ]\nSubtracting this from [7] yields\n\n[11] þ = π[h(q-q_hat) \nThe rate of exchange is positive whenever the real exchange rate is above its equilibrium level, also it is moving towards the equilibrium level] - This yields the direction and movement of the exchange rate.\n\nIn equilibrium, [9] hold, that is [6] - [9] is the difference from equilibrium.\n[12] p - p_hat = -lθ(s-ŝ)\nThis shows the line upon which the exchange rate must be moving (the line with slope -lθ).\n\nBoth [11] & [12] together demonstrates that the exchange rate will be moving towards the long run equilibrium exchange rate, whilst being in a position that implies that it was initially overshot.\nFrom the assumptions above, it is possible to derive the following situation. \nThis demonstrated the overshooting and subsequent readjustment. In the graph on the top left, So is the initial long run equilibrium, S1 is the long run equilibrium after the injection of extra money and S2 is where the exchange rate initially jumps to (thus overshooting). When this overshoot takes place, it begins to move back to the new long run equilibrium S1.\n\n\n"}
{"id": "40362343", "url": "https://en.wikipedia.org/wiki?curid=40362343", "title": "Persecution of Jews and Muslims by Manuel I of Portugal", "text": "Persecution of Jews and Muslims by Manuel I of Portugal\n\nOn 5 December 1496, King Manuel I of Portugal signed the decree of expulsion of Jews and Muslims to take effect by the end of October of the next year.\n\nUntil the 15th century, some Jews occupied prominent places in Portuguese political and economic life. For example, Isaac Abrabanel was the treasurer of King Afonso V of Portugal. Many also had an active role in Portuguese culture, and they kept their reputation of diplomats and merchants. By this time, Lisbon and Évora were home to important Jewish communities.\n\nOn 5 December 1496, because a clause of the contract of marriage between himself and Isabella, Princess of Asturias stipulated he do so in order to win her hand, King Manuel I of Portugal decreed that all Jews must convert to Catholicism or leave the country. One set of laws demonstrated the King's wish to completely and forever eradicate Judaism from Portugal. The initial edict of expulsion was turned into an edict of forced conversion in 1497: Portuguese Jews were prevented from leaving the country and forcibly converted to Christianity. \nHard times followed for the Portuguese conversos, with the massacre of 2,000 individuals in Lisbon in 1506 and the later and even more relevant establishment of the Portuguese Inquisition in 1536. The Portuguese inquisition was extinguished in 1821 by the \"General Extraordinary and Constituent Courts of the Portuguese Nation\".\n\nWhen the King allowed conversos to leave after the Lisbon massacre of 1506, many went to the Ottoman Empire (notably Thessaloniki and Constantinople and to Morocco. Smaller numbers went to Amsterdam, France, Brazil, Curaçao and the Antilles. In some of these places their presence can still be witnessed, like the use of the Ladino language by some Jewish communities in Turkey, the Portuguese based dialects of the Antilles, or the multiple synagogues built by those who became known as the Spanish and Portuguese Jews, such as the Amsterdam Esnoga.\n\nJews who converted to Christianity were known as New Christians, and were always under the constant surveillance of the Inquisition. Many of those were crypto-Jews who continued to secretly practice their religion; they eventually left the country in the centuries to come and again embraced openly their Jewish faith. Such was the case, for example, of the family of Baruch Spinoza.\nSome of the most famous descendants of Portuguese Jews who lived outside Portugal are the philosopher Baruch Spinoza (from Portuguese Bento de Espinosa), and the classical economist David Ricardo.\n\nSome Jews, very few, like the Belmonte Jews, went for a different and radical solution, practicing their faith in a strict secret isolated community. Known as the Marranos, some have survived until today (basically only the community from Belmonte, plus some more isolated families) by the practice of inmarriage and few cultural contacts with the outside world. Only recently have they re-established contact with the international Jewish community and openly practice their religion in a public synagogue with a formal rabbi.\n\nAccording to contemporary historian François Soyer, the expulsion of Muslims from Portugal has been overshadowed by the forced conversion of Jews in the country. While tolerance of Muslim minorities in Portugal was higher than in any other part of Europe, Muslims were still perceived as \"alien.\" Anti-Muslim riots were regular in neighboring Valencia during the 1460s; however, no similar acts of violence occurred in Portugal.\n\nIn December 1496, Manuel I ordered all Muslim subjects to leave without any apparent provocation. According to 15th-century Portuguese historians Damião de Góis and Jerónimo Osório, the Portuguese government originally planned to forcibly convert or execute Muslims as they had done to Jews, but fear of retaliation from Muslim kingdoms in North Africa led the king to settle on deportations instead. Manuel I's motivation behind the order is unclear, but some contemporary historians say it was part of a greater goal of Queen Isabella and King Ferdinand (known as the \"Catholic Monarchs\") to rid the peninsula of Muslims and create \"religious uniformity\" and \"monolithic Catholic Christian unity\". Other historians say it was influenced by ambitions of conquering Morocco, or by the suggestion of the Dominican confessor to the king, Friar Jorge Vogado. Some Muslims found refuge in Castile, but most fled to North Africa.\n\nIn the 19th century, some affluent families of Sephardi Jewish Portuguese origin such as the Ruah and Bensaude, resettled in Portugal from Morocco. The first synagogue to be built in Portugal since the 15th century was the Lisbon Synagogue, inaugurated in 1904.\n\nIn 2014 the Portuguese parliament changed the Portuguese nationality law in order to grant Portuguese nationality to descendants of Sephardi Jews expelled from Portugal. The law is a reaction to historical events that led to their expulsion from Portugal, but also due to increased concerns over Jewish communities throughout Europe. In order to obtain Portuguese nationality, the person must have a family surname that attests to being a direct descendant of a Sephardi of Portuguese origin or family connections in a collateral line from a former Portuguese Sephardi community. Use of expressions in Portuguese in Jewish rites or Judaeo-Portuguese or Ladino can also be considered proof.\n\nFrom 2015 several hundred Turkish Jews who were able to prove descent from Portuguese Jews expelled in 1497 emigrated to Portugal and acquired Portuguese citizenship.\n\n"}
{"id": "1101529", "url": "https://en.wikipedia.org/wiki?curid=1101529", "title": "Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States", "text": "Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States\n\nThe \"Plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States\" was a report submitted to the U.S. House of Representatives on July 13, 1790, by Secretary of State Thomas Jefferson.\n\nAt the First United States Congress, which met in 1789 when the decimal metric system had not yet been developed in France, the system of units to be used in the future USA was one point of discussion. Under the Constitution (article I, section 8), the Congress has the constitutional right to decide on a standard of weights and measures. On January 2, 1790, George Washington urged Congress to address the need for the uniform system of weights and measures, and on January 15, 1790, the House of Representatives requested Thomas Jefferson to draw up a plan.\n\nThe decimal dollar had already been agreed upon in principle in 1785, but would not be implemented until after passage of the Mint Act in 1792. In mid-1790 Jefferson proposed two systems of units. The first was evolutionary, and was based on refinement of the definitions of the units of the existing English system, as well as simplification of their relationship to each other. The second system was revolutionary, and was based on units linked by powers of ten, very similar to the decimal metric system which would be proposed in France. The base units for length, mass, and volume in Jefferson's revolutionary system (named the foot, the ounce, and the bushel, respectively) were relatively close in size to their pre-existing counterparts and bore identical names, although the manner in which they were defined was very different. \n\nJefferson's proposal was the world's first scientifically based, fully integrated, decimal system of weights and measures.\n\nIn coordination with scientists in France, Jefferson selected the seconds pendulum at 45° latitude as the basic reference. For technical reasons, he proposed using a uniform rod as the pendulum rather than a traditional pendulum. The pendulum was estimated to be 39.14912 English inches long (in the inches of that time—it wasn't until much later that the inch was defined to be 25.4 mm), or 1.5 times that for a vibrating rod (58.72368 inches).\n\nIn the evolutionary approach, the foot was to be derived from one of these lengths by a simple integer factor, which would be either three (pendulum) or five (rod), i.e. lengthening it from the traditional value by 1.04970¯6 inches to ca. 331.463 mm or shortening it by 0.255264 inches to ca. 298.317 mm. For practical purposes he wanted the rod to be 58¾ (new) inches long, an increase of less than 0.045%.\n\nJefferson's proposed decimal system preceded the adoption in France of the decimal metric system, although both developed simultaneously. In France, the metre was to be defined as the ten-millionth part of an arc between the North Pole and the equator. Jefferson's system was based on the length of a rod oscillating seconds at 45 degrees latitude, with the foot defined as one fifth of the length of such a rod. Similar to the French system, Jefferson proposed a system of units linked directly by powers of ten. However, Jefferson's system did not make use of the concept of prefixes, which were of great importance in the French system. Instead, the names of old units were carried over into the new system for decimal multiples of the base units, giving them new values. \n\nAsserting the value of such thorough reform to the existing system of weights and measures, the report stated:\n\nThe following table lists the units of the Jeffersonian decimal system, and their relationship with one another. The values of these units are based on Jefferson's proposal of a foot that was equal in length to one-fifth of a second rod, one quarter-inch shorter than the foot in use at the time, and approximately equal to 0.298461684 m.\nThe transcript of the original document gives the value of a kental as 16 stones at one point and 10 stones in another point. 10 was presumably the intended value, given the otherwise consistent decimalization.\n\nUnder the United States Constitution, Article 1 Section 8, Congress shall have power \"To coin Money, regulate the Value thereof, and of foreign Coin, and fix the Standard of Weights and Measures\". In his first annual message to Congress (what later came to be called \"State of the Union Addresses\") on January 8, 1790 (a few months before Jefferson's report to the House of Representatives), George Washington stated, \"Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.\"\n\nWashington repeated similar calls for action in his second and third annual messages (after Jefferson's report). Jefferson's decimal proposal had the support of Alexander Hamilton, James Madison, James Monroe, and George Washington. Robert Morris was a powerful opponent of the proposal. In late 1791 the Senate appointed a committee to report on the subject and make recommendations. The committee reported in April 1792, unanimously endorsing Jefferson's decimal system. The Senate was slow to act on the matter and while they delayed events in France complicated the issue. Although French scientists working on a decimal system had originally supported using the seconds pendulum as a scientific basis, and Jefferson had deliberately matched his seconds pendulum proposal to the French one, based on a measurement at the latitude of Paris, the French decided to use the length of a meridian of the Earth instead of a seconds pendulum. This and other developments changed what had promised to be an internationally developed system into a strictly French project. Jefferson wrote, \"The element of measure adopted by the National Assembly excludes, \"ipso facto\", every nation on earth from a communion of measurement with them.\"\n\nThe Senate continued to consider Jefferson's two proposals, along with a number of new proposals, for several years. In 1795 a bill titled \"An Act directing certain experiments to be made to ascertain uniform standards of weights and measures for the United States\" was passed by the House and was approved by committee in the Senate, but on the last day of the session the Senate said it would consider the bill during the next session. The bill was never taken up again. In 1795 the Northwest Indian War, which for years had prevented the surveying and sale of land in the Northwest Territory came to an end. A land rush of settlers, surveyors, squatters, and others rapidly pushed into the region and the federal government had a sudden and intense need to establish a method for surveying and selling land. On May 18, 1796 Congress passed \"an Act for the sale of land of the United States in the territory northwest of the River Ohio, and above the mouth of the Kentucky River\". This law defined a survey grid system of 6–mile–square townships divided into 1–mile–square sections, with the defining unit being the chain, specifically Gunter's chain. This was the first unit of measurement designated into law by Congress. This law and the way it defined the survey grid ended debate over Jefferson's decimal system.\n\n\n\n"}
{"id": "38998735", "url": "https://en.wikipedia.org/wiki?curid=38998735", "title": "Qanun (law)", "text": "Qanun (law)\n\nQanun is an Arabic word (, \"qānūn\"; , \"kānūn\", derived from \"kanōn\", which is also the root for the modern English word \"canon\"). It can refer to laws established by Muslim sovereigns, in particular the Ottoman sultans, in contrast to sharia, the body of law elaborated by Muslim jurists. It is thus frequently translated as \"dynastic law\". The idea of \"kanun\" first entered the Muslim World in the thirteenth century, as it was borrowed from the Mongol Empire in the aftermath of their invasions. The 10th sultan of the Ottoman Empire, Suleiman was known in the Ottoman Empire as Suleiman Kanuni (\"the Lawgiver\"), due to his code of laws. \n\nAfter the fall of the Abbasid Caliphate in 1258, a practice known to the Turks and Mongols transformed itself into qanun, which gave power to caliphs, governors, and sultans alike to \"make their own regulations for activities not addressed by the sharia.\" This became increasingly important as the Middle East started to modernize, thus running into the problems of a modern state, which were not covered by sharia. The Qanun began to unfold as early as Umar I (586–644 CE). Many of the regulations covered by qanun were based on financial matters or tax systems adapted through the law and regulations of those territories Islam conquered.\n\nThe term ḳānūn derives itself from the Greek word κανών. Originally having the less abstract meaning of “any straight rod” it then later referred to any “measure or rule” in Greek. The word was then translated into and adopted by the Arabic language after the Ottoman Empire’s conquest of Egypt under Sultan Selim I (ca. 1516). In the Ottoman empire, the term ḳānūn still carried the word’s original meanings of a system of tax regulation. However, it later came to also refer to “code of regulations” or “state law”, a well-defined secular distinction to “Muslim law” known as the s̲h̲arīʿa. The ḳānūn took on significant importance during the period of modernization in the Ottoman Empire. The ḳānūn and s̲h̲arīʿa did not contradict each other concerning administrative matters, and therefore the ḳānūn was assimilated easily into Ottoman regulatory functions. The ḳānūn promulgated by Ottoman sultans also came to be used for financial and penal law. Under Sultan Mehmed II (1451-1481) the ḳānūn continued to be strictly applied for these practices. However due to the influence of Abu ʾl-Suʿūd, grand muftī of Istanbul from 1545 to 1574 the ḳānūn was applied to deal with matters concerning property rights as well. Previously, property rights were exclusively under the jurisdiction of the s̲h̲arīʿa. Despite this seeming contradiction, due to skillful bureaucratic operations, the ḳānūn and the s̲h̲arīʿa existed harmoniously. The ḳānūn has retained its relevance in the Middle East regarding civil, commercial, administrative, and penal laws that were inspired by originally Western legislation. It also has an influence in the ways that provisions of the s̲h̲arīʿa are reproduced.\n\n"}
{"id": "2396489", "url": "https://en.wikipedia.org/wiki?curid=2396489", "title": "Raven Tales", "text": "Raven Tales\n\nRaven Tales are the traditional people and animals creation stories of the indigenous peoples of the Pacific Northwest Coast but are also found among Athabaskan-speaking peoples and others. Raven stories exist in nearly all of the First Nations throughout the region but are most prominent in the tales of the Tlingit and Tahltan people.\n\nRaven and eagle are known by many different names by many different peoples and is an important figure amongst written and verbal stories. His tales are passed down through the generations of story tellers of the people and are of cultural and historical significance. It's important to note that, from some storytellers' perspective, Native myths such as the Raven Tales, as opposed to tall tales and little stories for children, are not entertainment and are cultural property of the clan or individual that the story originates from. It is customary that others should not tell stories that are owned by another clan, especially if they do not live in the same area.\n\nWhile each culture's stories of the Raven are different, there are even those that share the same title; certain attributes of Raven remain the same. The Raven is always a magical creature able to take the form of human, animal, even inanimate objects. He is a keeper of secrets, and a trickster often focused on satisfying his own gluttony for whatever he desires. His stories tell of how worldly things came to be or offer suggestion to children on how to behave. Raven's creative nature shows itself through circumstance rather than intent, through the desire to satisfy his own needs, rather than any altruistic principles. Raven is both the protagonist among the stories of some groups, and the antagonist of others; he is a hero and an amusement.\n\nTales that feature the Raven as the hero are specific to areas in the north of the continent such as northern British Columbia and Alaska and their peoples, such as the Tsimshian and the Haida. Similar tales appear in Chukchi cultures in the north-east of Asia and it is probable that they are influenced by Native American stories.\n\nThe Haida First Nation credits Raven with finding the first humans hiding in a clam shell; he brought them berries and salmon. The Sioux tell of how a white raven used to warn buffalo of approaching hunters. Eventually an angry shaman caught the bird and threw it into a fire, turning it black.\n\nWhile Raven tales tell the origins of human beings, they do not address the origins of organized society. In tales which mirror development and organization of Native American societies, the hero is often humanity itself. Raven tales do not offer a detailed picture about the social relations and realities of life.\n\nAthabaskan is the language family of several contiguous dialects spoken by various peoples in Western Canada and the American West. They can be further subdivided into the Northern Athabaskan, Pacific Coast Athabaskan, and Southern Athabaskan subregions.\n\nThese groups lived in one of the 3 Athabaskan regions.\n\nThe Cahto are an indigenous Californian group of Native Americans. The Kato lived farthest south of all the Athapascans in California, occupying Cahto Valley and Long Valley, and in general the country south of Blue Rock and between the headwaters of the two main branches of Eel River. The Kato language is one of four Athabaskan languages that were spoken in northwestern California. Most Kato speakers were also bilingual in Northern Pomo.\n\nOne version of the Raven creation story is that of the Cahto in California.\nIn one variant, Raven is taught by his father, Kit-ka'ositiyi-qa, to be a creator, but Raven is unsatisfied with the result. He creates the world but is unable to give it light or water. On hearing that light could be found hidden in a far-off land, Raven decides to travel there and steal it. In the house of light, he finds a young woman living with her father and plays the first of many tricks. He turns himself into a small speck of dirt, slips into her drinking water, and is swallowed. The daughter becomes pregnant and she gave birth to an unusual and fussy child who cries constantly and demands to touch one of the bundles which has been stored hanging from the walls. The child is given one of the bags to quiet him, but when he tires of playing with it, he lets it go, and it floats away from him and disappears through the smoke hole. Once it reaches the sky the bundle comes undone and scatters stars across the sky. When the child cries to have it back again he is given the second bundle to play with and lets it float away through the hole in the ceiling, thus releasing the moon. It all happens again with the third and last bundle, which flies away and becomes sunlight. After bringing light to the whole world, he too flies out through the smoke hole.\n\nLocally among the Tahltan people, their customs and livelihoods varied widely as they were often widely separated and would have to endure varying conditions depending on their locality. In Tahltan culture it was believed that some of their ancestors had knowledge that others did not from times before a great flood. Some of these ancestors used that knowledge for the good of the people, while others used it for evil and to the disadvantage of others. Raven is considered to be the protagonist hero against these evil ancestors.\n\nIn Tahltan stories, Raven is referred to as Big-Crow (\"Tse'sketco\" or \"tceski'tco\", \"big raven\" - from \"tceski'a\", \"raven\").\n\nThey claim that Big-Crow was born miraculously as the youngest of many brothers in the northern Tlingit country and was raised speaking the local language. He was separated from his father at birth and his father is never spoken of in all of their stories. Raven was born, the third child of a woman whose previous two boys were killed by her uncle. Each time the woman gave birth, her uncle would offer to teach them to hunt once they were old enough; each time he would take them out on a canoe and trick them, he would instruct the boys to sit on the edge of the canoe, at which point he would rock the canoe, forcing them to fall into the water where he would leave them to drown.\n\nBut her third child was Raven who took well to carving. Just as he'd done 2 times before, her uncle asked if he could teach Raven how to hunt after he'd grown a few years. Several times she refused her uncle until Raven insisted that she allow him to go. So they went out to sea and the same scenario played out. Raven fell into the water but rather than drown, he took one of the toy canoes he had carved and made it grow into a full size canoe. He went straight back to his mother and told of what his uncle had done.\n\nTwice more the uncle tried to drown Raven in the same manner but Raven outsmarted him each time until the uncle gave up and no longer took Raven hunting but would go alone.\n\nAs Raven grew into a man, he met the uncle's wife where he tried to play with her. He tickled the girl and two birds flew out from under her arms, a bluejay and a woodpecker; and the girl died. When the uncle returned from his hunt, he saw that his wife had died and he intended to kill Raven again, but this time in rage rather than trickery. But once again, Raven escaped with his canoe carvings. But when Raven escaped this time, he did not return home and his journey began; never to return to his home.\n\nHe starting off travelling by canoe along the seashore all alone but would stop whenever he came upon a village. When he met people whom he saw take advantage of others or use their power for evil, he would kill in his efforts to deprive them of power.\n\nRaven travelled for many years along the coast of the Tlingit territory, first travelling south, having started in the north until he had gone so far south, beyond Tlingit territory until he reached the Mink people at which point he turned around and continued back the other direction. He did this north south, south north journey for several years. Not until his work along the coast was done, did he head inland along the Stikine river all the way to its source. He also traveled along the Nass, Skeena, and Taku Rivers and all of their many streams never staying in one place for very long and never traveling far off from the water ways. Through his inland journeys he met the Kaska, the Haida, and other nations to the east.\n\nLater in life, when Raven had done all the work he could do, he travelled back out to the coastal regions guided by the setting sun until he disappeared mysteriously. The only suggestion is that he may have gone to live with the Kanu'gu and other ancient gods on an island far out into the ocean where they believed weather was created from.\n\nAs the Raven stories continue after \"The Birth of Raven\", many stories follow:\n\nRaven appears in other stories not directly related to him as well. In the story of the 'Warm and Cold Wind People' it is said that someone, possibly Raven, ordained that the people send out the winds.\n\nHe appears again in the story of \"The Great Flood\", which reaccounts for the killing of the evil ancestors who used their powers to take away the sun, moon, and Dipper which were lost during the flood.\n\nThe Inuit are native to the Canadian Arctic and Greenland. Among Inuit tradition the owl, fish, and raven are of greatest prominence. Ravens are also common in the artwork of the Inuit and they have several stories that tell of Raven's birth which is often juxtaposed with the owl with whom Raven shared a deep friendship.\n\nThe Inuit say that Raven was born out of the darkness. He was weak and lost. As he began traveling aimlessly experiencing the world, he realized that he was the Raven Father, Creator of All Life. Once he realized who he was, he gathered up his strength and flew out of the darkness to a new place which he called earth, but he was still alone, so he decided to create plants. As he flew around exploring this new world, he came upon a man whom the legend claims was the first of the Inuit people. Raven fed the man and taught the man to respect the world around him. Soon after, a woman came to be and Raven taught the both of them how to cloth themselves, build shelter, and make canoes to travel the water. As the two bred and spawned children, Raven cared for their children and educated them as he had done before.\n\nRaven is known as Tulukaruq to the Yupik people and is seen as a culture hero to them who is benevolent and helps the people.\n\nThe Haida people live in British Columbia. To The Haida, Raven was the Bringer of Light and before Raven the world was nothing more than a gigantic flood. Raven was the Maker of Things, as well as the Transformer, Magician and Healer. Raven was bored of the world being nothing but water and decided to fly as the waters receded. Once Raven became hungry, land was formed so he could land and find food. It was at this point he noticed strange sounds coming from a gigantic clam shell. Confused as to the sound, Raven decided he would begin singing to the clam shell in response to its sound, hoping to calm it with his pleasant sounds. Raven did this because he was a beautiful singer. Finally, a small creature emerged from the clam shell. It had long black hair, a round head, brown smooth skin and two legs like Raven but no feathers. This was the first of the First People.\n\nWhen he got bored with them, he considered returning them to their shell, but opted instead to find female counterparts of these male beings. The raven found some female humans trapped in a chiton, freed them, and was entertained as the two sexes met and began to interact. The Raven felt responsible and very protective of them, thus many Haida myths and legends often suggest the raven as a provider to mankind and combine the roles of the creator and the trickster.\n\nThere are other versions that tells of a different creation. When the earth was only sky and water with a single reef that rose out of the water where all of the great beings lived with the greatest of them living at the highest point on the reef and the weakest of them living at the bottom. But Raven flew above them all and could never find a place to land. For that reason he decided to travel to the sky country where he met the Chief's daughter who had recently had a child. While it was dark, Raven possessed the baby and intended to take its place as Raven Child.\n\nOne ancient story told on Haida Gwaii tells about how Raven helped to bring the Sun, Moon, Stars, Fresh Water, and Fire to the world:\n\nOther Haida stories include:\nThe Heiltsuk were formerly known as the Bella Bella people and lived along the central coast of British Columbia. To the Bella Bella Raven was known as the Real Chief or He'mask.as. Raven is revered by them as a benevolent figure. He helps people, but he is also a trickster spirit whose unreflected behavior gets him into trouble.\nAnother story of the Kwakiutl or Kwakwaka'wakw of British Columbia who exposed boys' placentas to ravens to encourage future prophetic visions, thereby associating the raven with prophecy, similar to the traditions of Scandinavia.\n\nThe Miwok are segregated into three distinct groups: the Coast Miwok, the Lake Miwok, and the Interior Miwok which make up the majority of the overall population.\n\nThe Miwok territory is defined by the Maidu to their right, the Yokuts to the left, and the Washoe and Mono behind them. The Interior Miwok faction live primarily on the western side of the Sierra above the lower San Joaquin Valley. The Sierra territory of the Miwok extended from the Cosumnes River on the north to the Fresno on the\nsouth but the other boundaries that are shared with the Yokuts, Wintun, and Maidu have always been a matter of controversy.\n\nAmongst the Northern Miwok of what is now Central California the story of Raven begins with a world covered in water except for a single mountain top where people had gathered during the flooding of the world. As the waters receded the people tried to come down from the mountain but the land was so soft with mud that those that tried would sink into the ground. Wherever a person sank, a raven would come and stand on that spot. One raven at each hole. Once the ground hardened the raven turned into a person, explaining why Miwok are so dark.p101\n\nThe Nuuchahnulth are also known commonly as the Nootka.\n\nThe Ojibwe are also known by other names including Anishinaabe which is the name of their language, Odawa, or Algonquin.\n\nThe Plains Ojibwa, also known as Bûngi Indians, lived on the Long Plains Reserve in Manitoba.\n\nAnother raven story from the Puget Sound region describes the \"Raven\" as having originally lived in the land of spirits (literally \"bird land\") that existed before the world of humans. One day the Raven became so bored with \"bird land\" that he flew away, carrying a stone in his beak. When the Raven became tired of carrying the stone and dropped it, the stone fell into the ocean and expanded until it formed the firmament on which humans now live.\n\nThe Pima are in Arizona.\n\nThe Quileute are a Native American people in western Washington state in the United States, currently numbering approximately 2000. Their language belongs to the Chimakuan family of languages.\n\nThe native name for Raven among the Quileute is Báyaḳ (\"By\"-yuhk).\n\nQuileute Indians were the southernmost group along the Pacific Coast whose mythology included several stories of the Raven. Though the Quileute's primary protagonist was not the Raven, but Kweeti, whose stories can be very closely related to similar stories of the Tlingit involving the Raven.\n\nThe Raven, amongst the Quileute people, is used to tell scary stories to children of how Raven's feet look the way they do; others pursued children to be generous rather than selfish, or to be true to themselves and work hard rather than trying to take shortcuts. In their stories Raven is often punished, or must witness suffering by the people whom he cares for as a result of his trickery.\n\nIn the Quileute story of Duskeah\n\nIn the first story of \"Kweeti\" the story goes \"At Neah Bay he taught them to fish, as all men do. He traversed the whole world.\" which is compared to the Tlingit story \"Raven teaches people their mode of life\".\n\nAgain in the story of the \"Kweeti and the Wolves\", \"Finally, when the wolves had all but caught him, Kweeti urinated and made Ozetta Lake.\" which is compared to \"Raven Creates Rivers\".\n\nBut in one story, Raven and Kweeti meet and Kweed entertains Raven.\n\nThe Coeur d'Alene live in villages along the Coeur d'Alene, St. Joe, Clark Fork and Spokane Rivers; as well as sites on the shores of Lake Coeur d'Alene, Lake Pend Oreille and Hayden Lake, in what is now northern Idaho, eastern Washington and western Montana. \n\nThe Squamish see Raven to be a symbol of the Creator and even to this day is the subject of preachings.\n\nTlingit territory is in Southeast Alaska. Most of their territory is in present-day Canada.\n\nThe Tlingit call Raven Katce'de but it is disputed whether this derives from the name of another place in the region named Kate, meaning \"cedar-bark\" or from Tlingitka, signifying \"man\" or \"people\"\n\nIn Tlingit culture, there are two different raven characters which can be identified, although they are not always clearly differentiated. One is the creator raven, responsible for bringing the world into being and who is sometimes considered to be the individual who brought light to the darkness. The other is the childish raven, always selfish, sly, conniving, and hungry. When the Great Spirit created all things he kept them separate and stored in cedar boxes. The Great Spirit gifted these boxes to the animals who existed before humans. When the animals opened the boxes all the things that comprise the world came into being. The boxes held such things as mountains, fire, water, wind and seeds for all the plants. One such box, which was given to Seagull, contained all the light of the world. Seagull coveted his box and refused to open it, clutching it under his wing. All the people asked Raven to persuade Seagull to open it and release the light. Despite begging, demanding, flattering and trying to trick him into opening the box, Seagull still refused. Raven became angry and frustrated, and stuck a thorn in Seagull's foot. Raven pushed the thorn in deeper until the pain caused Seagull to drop the box. Then out of the box came the sun, moon and stars that brought light to the world and allowed the first day to begin.\n\nRaven continued using such trickery to bring water and stamp people, animals and other features in the world with certain characteristics. Many versions of Raven's theft of water are told but all center on Raven's trickery against the owner of water. In one version Raven leads its owner to believe he has soiled his bed in his sleep and threatens to shame him unless he shares his water with Raven. In another version Raven puts ash on his tongue to fool the owner to believe his extreme thirst is unquenched. Instead of drinking the water Raven collects it in a seal's bladder hidden under his clothes and flees with all of it.\n\nTo the Tsimshian, Raven, was known as Txamsem or \"ganhada\" or the Clever One and was accompanied by a brother named Lagabula or Lazy One.\n\nThe two had been born in a kelp patch and adopted by a Chief's wife and a magical being from the region of Prince Rupert Harbor. At the time of their birth things such as daylight did not yet exist; only dusk. Some records contradict this stating that they were of Gispaxloats origin, born of a Gispaxloats Chief who married a beautiful princess.\n\nAmong their journeys, they traveled to a mountain at the head of the Nass River. Txamsem, who could transform into anything, and often took the form of a human or bird, turned himself into a pine needle and was consumed by the daughter of the Chief who guarded daylight. She then gave birth to him as a baby and the baby cried incessantly to play with daylight. As soon as it was given to the baby in the form of a playful ball, he transformed back into Raven and flew away with it. He traveled back up the Nass River with daylight and released it; immediately lighting up the river and allowing it to spread all over the world.\n\nThe most prominent culture hero for many of the indigenous peoples of the Pacific Northwest is Raven. There are numerous stories, widely distributed which focus on the Raven myth and his adventures to satisfy his insatiable desire to obtain whatever he wants. The plot of most Raven tales, tell of how Raven is able to use force or trickery to obtain or motivate someone else to relinquish an object he desires. Throughout his many stories, Raven claims daylight, water, fire, the oceans waves, the olachen, salmon, the soil, and even the weather.\n\nThrough the Raven tales, people are able to explain why their surrounding environment was the way it was by linking the Raven or his companions as the cause for why various things in the world come about.\n\nSome stories account for the creation of dangerous animals which were transformed from inanimate objects(No 61. p 572), others suggest that men, animals and objects could be turned to stone.(nos89-93). While other stories suggest how names were attributed to important landmarks, and how significant geographical features came about.\n\nThere is a vague mention that Raven was the ancestor of the Raven Clan, but there are no other direct references between Raven and the ancestry of the Indian people.\n\nThe raven is not a traditional fetish of the Zuni but he, along with the Macaw play a part in the Zuni story of migration and is carved often in their artwork, typically carved from black marble though not exclusively.\n\nThe Zuni consider Raven to be a prankster but without negative characteristics which they associate with the coyote. The Raven's greatest traits are his ability to assist the people in overcoming their failures by offering gentle reminders that anything people have the courage to face, thus too do they have the power to transform.\n\nThere have been many children's and picture books that recount traditional Raven Tales. These new versions have been criticized for portraying a much \"nicer\" Raven with little left from the original greedy trickster. In some stories Raven acts for the good of people and not for himself. In other stories Raven refuses to use force, and sexual themes are edited out. Trickery is in some instances substituted for magic. These newer tales are also written in conventions of Western rather than Native American literature thus conveying the message that native storytellers' ability or style is inferior.\n\nIn 2004, The Smithsonian Institution sponsored Chris Kientz to develop a series of half-hour animated television programs targeted at school children as an entertaining way of educating kids on aboriginal folklore. The show \"Raven Tales\" was produced by New Machine Studios working with producer Winadzi James and aired for two seasons with a total of 26 episodes.\n\nIn 2010, Matt Dembicki produced an anthological graphic novel of the trickster stories, making sure to maintain the cultural integrity of the stories with the help of 21 Native American story-tellers who were paired directly with several graphic designers.\n\n\n\n"}
{"id": "40457584", "url": "https://en.wikipedia.org/wiki?curid=40457584", "title": "Refugee kidnappings in Sinai", "text": "Refugee kidnappings in Sinai\n\nBetween 2009 and 2014, there were large numbers of refugees who were kidnapped and held in Sinai. Refugees from various countries were transported to Sinai and held hostage by members of Bedouin tribes. Typically, the hostages were forced to give up phone numbers of relatives and were tortured with the relatives on the phone, in order to obtain ransoms in the range of $20,000–$40,000. If the families couldn't pay, the hostages were killed.\n\nMany of the hostages, refugees from Sudan, Ethiopia or Eritrea, paid traffickers for transport to the Israeli border, hoping to cross into that country. They were instead taken hostage by those they had paid. Others were taken by force from refugee camps in Sudan, as reported by the United Nations Refugee Agency in January 2013. Amnesty International published a report about numerous kidnappings in 2011-2013 in the Shagarab refugee camps in eastern Sudan, carried out by members of the Rashaida tribe, with victims being sold off to gangs in Sinai, where they would be brutally mistreated to extract ransoms. In 2012 Israel constructed a fence at its border to Sinai to keep out African migrants, causing the Rashaida to lose income from transporting refugees to the border; they then started to concentrate on kidnappings instead.\n\nThe phenomenon was first documented by Israeli organization Hotline for Refugees and Migrants in 2010 in a report entitled \"In the Dead of the Wilderness.\" The report was based on testimonies they collected from 60 asylum seekers, mostly Eritreans, who had been tortured for ransom in the Sinai desert. Staff and volunteers of the organisation mostly met the survivors in Israeli immigration detention centers where they were visiting recently arrived asylum seekers. Many people arrived with serious injuries, and some women arrived pregnant as a result of rape. Seriously injured people were immediately taken to hospital upon arrival in Israel.\n\nGerman journalist Michael Obert visited the region in 2013, met a victim and a torturer and talked to an activist of the New Generation Foundation for Human Rights in Al-Arish. The organization has documented hundreds of cases of mutilated corpses of Africans found in the desert. Obert reported that some Islamist militants were using force to try to stop the kidnapper gangs. He described a lawless and impoverished region where children were looking forward to grow up to become kidnappers.\n\nA 2011 CNN documentary reported on Eritrean refugees who had paid Bedouin traffickers for transport to Israel but were instead held in bondage before their organs were harvested. In 2013 the Al-Arish activist presented photos of corpses from which organs had been professionally removed and claimed to have seen mobile operation units.\n\nIn February 2011, Physicians for Human Rights-Israel reported that some 190 Eritrean and Ethiopian refugees were being held hostage in Sinai, and that numerous refugee women reported having been raped on their trip to Israel. In 2013, the same organization estimated that about 1,000 refugees were being held in Sinai hostage camps, and that in total about 7,000 refugees had been abused in these camps, resulting in more than 4,000 deaths.\n\nIn a 2013 report, This American Life reported on the experiences of journalist Meron Estefanos, who has helped publicize the kidnappings, and helped get some hostages freed.\n\nThe Egypt–Israel Peace Treaty of 1979 limits the number of Egyptian forces that can be deployed in Sinai. After the Egyptian Revolution of 2011, security forces largely abandoned the peninsula, but they returned in 2014 to fight unrelated terrorist networks, and the practice of kidnapping refugees moved to Libya and elsewhere.\n\n\n"}
{"id": "25190127", "url": "https://en.wikipedia.org/wiki?curid=25190127", "title": "Reservoir sampling", "text": "Reservoir sampling\n\nReservoir sampling is a family of randomized algorithms for randomly choosing a sample of formula_1 items from a list formula_2 containing formula_3 items, where formula_3 is either a very large or unknown number. Typically, formula_3 is too large to fit the whole list into main memory.\n\nSuppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items formula_3, then the solution is easy: select 10 distinct indices formula_7 between 1 and formula_3 with equal probability, and keep the formula_7-th elements. The problem is that we do not always know the exact formula_3 in advance. A possible solution is the following:\n\nThus,\n\nThe most common example was labelled \"Algorithm R\" by Jeffrey Vitter in his paper on the subject. This simple O(\"n\") algorithm as described in the \"Dictionary of Algorithms and Data Structures\" consists of the following steps (assuming \"k < n\" and using one-based array indexing):\n\nThe algorithm creates a \"reservoir\" array of size formula_1 and populates it with the first formula_1 items of formula_2. It then iterates through the remaining elements of formula_2 until formula_2 is exhausted. At the element of formula_2, the algorithm generates a random number formula_29 between 1 and formula_7. If formula_29 is less than or equal to formula_1, the element of the reservoir array is replaced with the element of formula_2. In effect, for all formula_7, the element of formula_2 is chosen to be included in the reservoir with probability formula_36. Similarly, at each iteration the element of the reservoir array is chosen to be replaced with probability formula_37, which simplifies to formula_38. It can be shown that when the algorithm has finished executing, each item in formula_2 has equal probability (i.e. formula_40) of being chosen for the reservoir.\n\nTo see this, consider the following proof by induction. After the round, let us assume, the probability of a number being in the reservoir array is formula_41. Since the probability of the number being replaced in the round is formula_38, the probability that it survives the round is formula_43. Thus, the probability that a given number is in the reservoir after the round is the product of these two probabilities, i.e. the probability of being in the reservoir after the round, and surviving replacement in the round. This is formula_44. Hence, the result holds for formula_7, and is therefore true by induction.\n\nA simple reservoir-based algorithm can be designed using random sort and implemented using priority queue data structure. This algorithm assigns random number as keys to each item and maintain k items with minimum value for keys. In essence, this is equivalent to assigning a random number to each item as key, sorting items using these keys and taking top k items. The worse case run time of the algorithm is formula_46 while the best case runtime is formula_47. Even though the worse case runtime is not as good as Algorithm R, this algorithm can easily be extended to weighted sampling. Note that both algorithms can operate on streams of unspecified lengths.\n\nIn many applications sampling is required to be according to the weights that are assigned to each items available in set. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. There are two ways to interpret weights assigned to each item in the set: \n\nThe following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:\nReservoirSample(S[1..?], R[1..k])\nThis algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the line how we generate the key using random number generator. The algorithm is equivalent to assigning each item a key formula_54 where is the random number and then sort items using these keys and finally select top k items for the sample.\n\nFollowing algorithm was given by M. T. Chao uses interpretation 2:\nWeightedReservoir-Chao(S[1..n], R[1..k])\n\nFor each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.\n\nIn many applications, amount of data from which a small sample is needed is too large and it is desirable to distribute sampling tasks among many machines in parallel to speed up the process. A simple approach that is often used, although less performant, is to assign a random number as key to each item and then perform a distributed sort and finally obtain a sample of desired size from top k items. If weighted sample is desired then key is computed using formula_54 where is the random number and formula_48 is the weight of an item. The inefficiency in this approach obviously arises from required distributed sort on very large amount of data.\n\nAnother more efficient approach for distributed weighted random sampling is as follows:\nThe Step 4 uses keys from Step 2 because we might have unbalanced data distribution on machines. For example, lets say k = 1, machine m1 only gets 1 item with weight 10 while machine m2 gets 2 items each with weight 100. Intuitively probability for items from m1 getting in final sample is 10/210. In Step 3, we will get 1 item from m1 as well as m2. If we recalculate keys in step 4 then the probability that item from m1 will be in final sample is 10/110 instead of required 10/210. Now observe that weighted reservoir sampling algorithm from previous section decreases max key value in priority queue as it processes more items. Therefore, items sampled from machine with larger chunk will have lower key values and thus higher chance of getting selected.\n\nSuppose one wanted to draw \"k\" random cards from a deck of playing cards (i.e., \"n=52\").\nA natural approach would be to shuffle the deck and then take the top \"k\" cards.\nIn the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the Fisher-Yates shuffle:\n\nNote that although the rest of the cards are shuffled, only the top \"k\" are important in the present context.\nTherefore, the array \"a\" need only track the cards in the top \"k\" positions while performing the shuffle, reducing the amount of memory needed.\nTruncating \"a\" to length \"k\", the algorithm is modified accordingly:\n\nSince the order of the first \"k\" cards is immaterial, the first loop can be removed and \"a\" can be initialized to be the first \"k\" items of \"S\".\nThis yields \"Algorithm R\".\n\nA fast approximation to reservoir sampling.\nUses a good-quality approximation to the sampling-gap distribution to skip over the gaps; i.e. consecutive runs of data that\nare not sampled.\n\nThe following is a simple implementation of the algorithm in Python that samples the set of English Wikipedia page titles:\nProbabilities of selection of the reservoir methods are discussed in Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to formula_60 (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).\n\nReservoir sampling makes the assumption that the desired sample fits into main memory, often implying that formula_1 is a constant independent of formula_3. In applications where we would like to select a large subset of the input list (say a third, i.e. formula_63), other methods need to be adopted. Distributed implementations for this problem have been proposed.\n\n"}
{"id": "29549", "url": "https://en.wikipedia.org/wiki?curid=29549", "title": "Self-replication", "text": "Self-replication\n\nSelf-replication is any behavior of a dynamical system that yields construction of an identical copy of itself. Biological cells, given suitable environments, reproduce by cell division. During cell division, DNA is replicated and can be transmitted to offspring during reproduction. Biological viruses can replicate, but only by commandeering the reproductive machinery of cells through a process of infection. Harmful prion proteins can replicate by converting normal proteins into rogue forms. Computer viruses reproduce using the hardware and software already present on computers. Self-replication in robotics has been an area of research and a subject of interest in science fiction. Any self-replicating mechanism which does not make a perfect copy (Mutation) will experience genetic variation and will create variants of itself. These variants will be subject to natural selection, since some will be better at surviving in their current environment than others and will out-breed them.\n\nEarly research by John von Neumann established that replicators have several parts:\n\n\nExceptions to this pattern may be possible, although none have yet been achieved. For example, scientists have come close to constructing RNA that can be copied in an \"environment\" that is a solution of RNA monomers and transcriptase. In this case, the body is the genome, and the specialized copy mechanisms are external. The requirement for an outside copy mechanism has not yet been overcome, and such systems are more accurately characterized as \"assisted replication\" than \"self-replication\".\n\nHowever, the simplest possible case is that only a genome exists. Without some specification of the self-reproducing steps, a genome-only system is probably better characterized as something like a crystal.\n\nRecent research has begun to categorize replicators, often based on the amount of support they require.\n\n\nThe design space for machine replicators is very broad. A comprehensive study to date by Robert Freitas and Ralph Merkle has identified 137 design dimensions grouped into a dozen separate categories, including: (1) Replication Control, (2) Replication Information, (3) Replication Substrate, (4) Replicator Structure, (5) Passive Parts, (6) Active Subunits, (7) Replicator Energetics, (8) Replicator Kinematics, (9) Replication Process, (10) Replicator Performance, (11) Product Structure, and (12) Evolvability.\n\nIn computer science a quine is a self-reproducing computer program that, when executed, outputs its own code. For example, a quine in the Python programming language is:\n\nA more trivial approach is to write a program that will make a copy of any stream of data that it is directed to, and then direct it at itself. In this case the program is treated as both executable code, and as data to be manipulated. This approach is common in most self-replicating systems, including biological life, and is simpler as it does not require the program to contain a complete description of itself.\n\nIn many programming languages an empty program is legal, and executes without producing errors or other output. The output is thus the same as the source code, so the program is trivially self-reproducing.\n\nIn geometry a self-replicating tiling is a tiling pattern in which several congruent tiles may be joined together to form a larger tile that is similar to the original. This is an aspect of the field of study known as tessellation. The \"sphinx\" hexiamond is the only known self-replicating pentagon. For example, four such concave pentagons can be joined together to make one with twice the dimensions. Solomon W. Golomb coined the term rep-tiles for self-replicating tilings.\n\nIn 2012, Lee Sallows identified rep-tiles as a special instance of a self-tiling tile set or setiset. A setiset of order \"n\" is a set of \"n\" shapes that can be assembled in \"n\" different ways so as to form larger replicas of themselves. Setisets in which every shape is distinct are called 'perfect'. A rep-\"n\" rep-tile is just a setiset composed of \"n\" identical pieces.\n\nIt is a long-term goal of some engineering sciences to achieve a clanking replicator, a material device that can self-replicate. The usual reason is to achieve a low cost per item while retaining the utility of a manufactured good. Many authorities say that in the limit, the cost of self-replicating items should approach the cost-per-weight of wood or other biological substances, because self-replication avoids the costs of labor, capital and distribution in conventional manufactured goods.\n\nA fully novel artificial replicator is a reasonable near-term goal.\nA NASA study recently placed the complexity of a clanking replicator at approximately that of Intel's Pentium 4 CPU. That is, the technology is achievable with a relatively small engineering group in a reasonable commercial time-scale at a reasonable cost.\n\nGiven the currently keen interest in biotechnology and the high levels of funding in that field, attempts to exploit the replicative ability of existing cells are timely, and may easily lead to significant insights and advances.\n\nA variation of self replication is of practical relevance in compiler construction, where a similar bootstrapping problem occurs as in natural self replication. A compiler (phenotype) can be applied on the compiler's own source code (genotype) producing the compiler itself. During compiler development, a modified (mutated) source is used to create the next generation of the compiler. This process differs from natural self-replication in that the process is directed by an engineer, not by the subject itself.\n\nAn activity in the field of robots is the self-replication of machines. Since all robots (at least in modern times) have a fair number of the same features, a self-replicating robot (or possibly a hive of robots) would need to do the following:\n\n\nOn a nano scale, assemblers might also be designed to self-replicate under their own power. This, in turn, has given rise to the \"grey goo\" version of Armageddon, as featured in such science fiction novels as \"Bloom\", \"Prey\", and \"Recursion\".\n\nThe Foresight Institute has published guidelines for researchers in mechanical self-replication. The guidelines recommend that researchers use several specific techniques for preventing mechanical replicators from getting out of control, such as using a broadcast architecture.\n\nFor a detailed article on mechanical reproduction as it relates to the industrial age see mass production.\n\nResearch has occurred in the following areas:\n\n\nThe goal of self-replication in space systems is to exploit large amounts of matter with a low launch mass. For example, an autotrophic self-replicating machine could cover a moon or planet with solar cells, and beam the power to the Earth using microwaves. Once in place, the same machinery that built itself could also produce raw materials or manufactured objects, including transportation systems to ship the products. Another model of self-replicating machine would copy itself through the galaxy and universe, sending information back.\n\nIn general, since these systems are autotrophic, they are the most difficult and complex known replicators. They are also thought to be the most hazardous, because they do not require any inputs from human beings in order to reproduce.\n\nA classic theoretical study of replicators in space is the 1980 NASA study of autotrophic clanking replicators, edited by Robert Freitas.\n\nMuch of the design study was concerned with a simple, flexible chemical system for processing lunar regolith, and the differences between the ratio of elements needed by the replicator, and the ratios available in regolith. The limiting element was Chlorine, an essential element to process regolith for Aluminium. Chlorine is very rare in lunar regolith, and a substantially faster rate of reproduction could be assured by importing modest amounts.\n\nThe reference design specified small computer-controlled electric carts running on rails. Each cart could have a simple hand or a small bull-dozer shovel, forming a basic robot.\n\nPower would be provided by a \"canopy\" of solar cells supported on pillars. The other machinery could run under the canopy.\n\nA \"casting robot\" would use a robotic arm with a few sculpting tools to make plaster molds. Plaster molds are easy to make, and make precise parts with good surface finishes. The robot would then cast most of the parts either from non-conductive molten rock (basalt) or purified metals. An electric oven melted the materials.\n\nA speculative, more complex \"chip factory\" was specified to produce the computer and electronic systems, but the designers also said that it might prove practical to ship the chips from Earth as if they were \"vitamins\".\n\nNanotechnologists in particular believe that their work will likely fail to reach a state of maturity until human beings design a self-replicating assembler of nanometer dimensions .\n\nThese systems are substantially simpler than autotrophic systems, because they are provided with purified feedstocks and energy. They do not have to reproduce them. This distinction is at the root of some of the controversy about whether molecular manufacturing is possible or not. Many authorities who find it impossible are clearly citing sources for complex autotrophic self-replicating systems. Many of the authorities who find it possible are clearly citing sources for much simpler self-assembling systems, which have been demonstrated. In the meantime, a Lego-built autonomous robot able to follow a pre-set track and assemble an exact copy of itself, starting from four externally provided components, was demonstrated experimentally in 2003 .\n\nMerely exploiting the replicative abilities of existing cells is insufficient, because of limitations in the process of protein biosynthesis (also see the listing for RNA).\nWhat is required is the rational design of an entirely novel replicator with a much wider range of synthesis capabilities.\n\nIn 2011, New York University scientists have developed artificial structures that can self-replicate, a process that has the potential to yield new types of materials. They have demonstrated that it is possible to replicate not just molecules like cellular DNA or RNA, but discrete structures that could in principle assume many different shapes, have many different functional features, and be associated with many different types of chemical species.\n\nFor a discussion of other chemical bases for hypothetical self-replicating systems, see alternative biochemistry.\n\n\n\n"}
{"id": "18180538", "url": "https://en.wikipedia.org/wiki?curid=18180538", "title": "Social rights (social contract theory)", "text": "Social rights (social contract theory)\n\nSocial rights are those rights arising from the social contract, in contrast to natural rights which arise from the natural law, but before the establishment of legal rights by positive law. For example, James Madison advocated that a right such as trial by jury arose neither from nature nor from a constitution of government, but from reified implications of the social contract.\n\nCecile Fabre argues that \"it is legitimate to constrain democratic majorities, by way of the constitution, to respect and promote those fundamental rights of ours that protect the secure exercise of our autonomy and enable us to achieve well-being. Insofar as, by virtue of Ch. 1, social rights are such fundamental rights, it follows that they should be constitutionalized.\"\n\nFrom a legal standpoint several approaches exercise and guarantee social rights; social rights under the constitution are rights of subjects or \"subject rights\". This assures that the public receives equal distribution of collective and private interests.\n"}
{"id": "15958803", "url": "https://en.wikipedia.org/wiki?curid=15958803", "title": "Splitting principle", "text": "Splitting principle\n\nIn mathematics, the splitting principle is a technique used to reduce questions about vector bundles to the case of line bundles.\n\nIn the theory of vector bundles, one often wishes to simplify computations, say of Chern classes. Often computations are well understood for line bundles and for direct sums of line bundles. In this case the splitting principle can be quite useful. \n\nThe theorem above holds for complex vector bundles and integer coefficients or for real vector bundles with formula_1 coefficients. In the complex case, the line bundles formula_2 or their first characteristic classes are called Chern roots. \n\nThe fact that formula_3 is injective means that any equation which holds in formula_4 (say between various Chern classes) also holds in formula_5. \n\nThe point is that these equations are easier to understand for direct sums of line bundles than for arbitrary vector bundles, so equations should be understood in formula_6 and then pushed down to formula_7.\n\nUnder the splitting principle, characteristic classes for complex vector bundles correspond to symmetric polynomials in the first Chern classes of complex line bundles; these are the Chern classes.\n\n\n"}
{"id": "26406347", "url": "https://en.wikipedia.org/wiki?curid=26406347", "title": "Tambunting Pawnshop", "text": "Tambunting Pawnshop\n\nTambunting Pawnshop is the oldest pawnshop in the Philippines, operating since 1906. The essence of business in Tambunting Pawnshop is lending money and accepting jewelry or storage (gadgets and appliances), and even mortgages of real estate and pledges of stocks as collateral. Loans are generally for a maximum period of six months, after which the collateral will be sold at auction or through their other jewelry outlets. Tambunting Pawnshop reached its centennial in 2006 and is currently celebrating its 110th anniversary this year.\n\nTambunting Pawnshop was established by Don Ildefonso Tan Bunting in 1906. He was a son of a Chinese immigrant from Fujian, China. He owned horse-drawn carriages that could be seen traveling through the streets of old Manila collecting garbage. The Government paid him just five centavos per trashcan. Over years of hard effort, he was able to accumulate real estate and other small investments. Some of this real estate was located in Divisoria, Tondo, Malabon and Navotas.\n\nAfter years of continuous expansion in his business, Don Ildefonso tried the shipping business, and later started up the original pawnshop in the town of Santa Cruz, Manila with the name \"Casa Agencia de Empeños de Ildefonso Tam Bunting\".\n\nBy the time of World War II, his pawnshop had expanded to different locations inside and outside Metro Manila. By the 1990s, the newer generation of the Tambunting family already operated more than 300 branches nationwide. As of this year, Tambunting Pawnshop already has more than 1000 branches nationwide and is still expanding.\n\n"}
{"id": "10366620", "url": "https://en.wikipedia.org/wiki?curid=10366620", "title": "United States Department of Justice Civil Rights Division", "text": "United States Department of Justice Civil Rights Division\n\nThe U.S. Department of Justice Civil Rights Division is the institution within the federal government responsible for enforcing federal statutes prohibiting discrimination on the basis of race, sex, disability, religion, and national origin. The Division was established on December 9, 1957, by order of Attorney General William P. Rogers, after the Civil Rights Act of 1957 created the office of Assistant Attorney General for Civil Rights, who has since then headed the division. The head of the Civil Rights Division is an Assistant Attorney General for Civil Rights (AAG-CR) appointed by the President of the United States. The current AAG-CR is Eric Dreiband.\n\n\nThe Division enforces\n\n\nIn addition, the Division prosecutes actions under several criminal civil rights statutes which were designed to preserve personal liberties and safety.\n\n"}
{"id": "485037", "url": "https://en.wikipedia.org/wiki?curid=485037", "title": "Urban guerrilla warfare", "text": "Urban guerrilla warfare\n\nAn urban guerrilla is someone who fights a government using unconventional warfare or domestic terrorism in an urban environment.\n\nThe urban guerrilla phenomenon is essentially one of industrialised society, resting both on the presence of large urban agglomerations where hideouts are easy to find and on a theory of alienation proper to the modern society of mass consumption.\n\nMichael Collins, a commander of the Irish Republican Army (IRA) is often considered to be the father of modern urban guerrilla warfare. In April 1919 an elite assassination unit, known as The Squad or \"Twelve Apostles\" was created in Dublin. The unit was tasked with hunting down and executing British Intelligence operatives in the city, they can be considered one of the first true urban guerrilla units.\n\nHistorically guerrilla warfare was a rural phenomenon, it was not until the 1960s that the limitations of this form were clearly demonstrated. The technique was almost entirely ineffective when used outside of the later colonial environment, as was shown by the Cuban sponsored efforts in Latin America during the 1960s culminating in the \"foco\" campaign headed by Che Guevara in Bolivia that culminated in his death. The need for the target government to be simultaneously incompetent, iniquitous, and politically isolated was rarely met.\n\nThe failure of rural insurgency forced the discontented to find new avenues for action, essentially random terrorism aimed at creating maximum publicity, provoking the targeted regimes into excessive repression and so inciting the general population to join a wider revolutionary struggle. This movement found its mentor in the leader of the ephemeral Ação Libertadora Nacional, Carlos Marighela. Before his death in 1969 he wrote the \"Minimanual of the Urban Guerrilla\" which, between the polemics, gave clear advice on strategy and was quickly adopted by others around the world.\n\n\n\n\n\n\n\n\n\n\n\n\nLeftists:\n\nFascists:\n\n\n\n\n\n\nLeftists:\n\nFascists:\n\nLeftists:\n\nFascists:\n\n\n\n\n\n\n\n\n\n\n\nNationalists\n\n\n\nHowever, not all urban political violence can be labeled as \"urban guerrilla\". The Black Panther Party might not qualify, due to its public nature, although its policy of \"self-defense\" was interchangeable with a policy of armed struggle in militarily occupied African American communities. Similarly the Italian Autonomia movement, and the German Autonomen engaged in urban political violence, but not as urban guerrillas due to their policies of public, mass and non-deadly violence.\n\nIn the 1970s BBC comedy \"Citizen Smith\" Wolfie Smith, the leader of the fictional \"Tooting Popular Front\" described himself as an Urban Guerrilla.\n\n\nSuggested readings:\n\n"}
{"id": "56419774", "url": "https://en.wikipedia.org/wiki?curid=56419774", "title": "Whisper network", "text": "Whisper network\n\nA whisper network describes a chain of information privately passed between people, typically a list of powerful people in an industry alleged as being sexual harassers or abusers. The information is often shared between women by word of mouth, online in private communities, in forums, via spreadsheets, and sometimes using crowd-sourced documents. The stated purpose of maintaining these lists is to warn potential victims of \"people to avoid\" in their industry. Whisper networks also purportedly help victims realize they are not alone so they can find each other and come forward together about a serial abuser. The term \"whisper network\" was newly popularized during the #MeToo movement after several private lists were published outside private networks, for example the Shitty Media Men list, the California State Capitol list, and the Harvey Weinstein Google doc. Karen Kelsky created a less controversial list called \"Sexual Harassment In the Academy: A Crowdsourced Survey\" which had grown to over 2000 entries by the end of 2017, and includes stories without actually naming the accusing and accused parties. Kelsky said she hoped the list would help demonstrate the scope of sexual misconduct in the academic field, and it has resulted in the investigation of twelve men at the University of Michigan.\n\nPublishing whisper networks to the public has been widely criticized for spreading unsubstantiated rumors which can damage reputations, though there continues to be debate on the best alternatives to anonymous sharing for women who have been punished or ignored by official channels yet would still like to warn other women. It has been noted that certain vulnerable groups rarely get access to these private lists, for example women who are young and women of color. As a result, these groups rarely receive any protection from whisper networks unless they are published. The main problem with trying to protect more potential victims by publishing whisper networks is determining the best mechanism to verify allegations. Some suggestions have included strengthening unions in vulnerable industries so workers can report directly to the union, maintaining industry hotlines which have the power to trigger third-party investigations, and creating public systems that allow anonymous reporting with the ability to connect victims who report the same perpetrator. Several apps have been developed which offer various ways for women to report sexual misconduct, and some of these apps have the ability to connect victims with each other. Sex workers regularly share “bad date lists” and St. James Infirmary Clinic (which offers health and safety services for sex workers), created a “Bad Date” app that allows sex workers to anonymously log incidents with clients who have threatened, extorted, robbed, or been violent, potentially warning other sex workers in the future.\n"}
