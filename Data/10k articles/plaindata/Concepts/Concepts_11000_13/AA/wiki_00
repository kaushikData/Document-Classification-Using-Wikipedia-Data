{"id": "36727085", "url": "https://en.wikipedia.org/wiki?curid=36727085", "title": "Active redundancy", "text": "Active redundancy\n\nActive redundancy is a design concept that increases operational availability and that reduces operating cost by automating most critical maintenance actions.\n\nThis concept is related to condition-based maintenance and fault reporting.\n\nThe initial requirement began with military combat systems during World War I. The approach used for survivability was to install thick armor plate to resist gun fire and install multiple guns.\n\nThis became unaffordable and impractical during the Cold War when aircraft and missile systems became common.\n\nThe new approach was to build distributed systems that continue to work when components are damaged. This depends upon very crude forms of artificial intelligence that perform reconfiguration by obeying specific rules. An example of this approach is the AN/UYK-43 computer.\n\nFormal design philosophies involving active redundancy are required for critical systems where corrective labor is undesirable or impractical to correct failure during normal operation.\n\nCommercial aircraft are required to have multiple redundant computing systems, hydraulic systems, and propulsion systems so that a single in-flight equipment failure will not cause loss of life.\n\nA more recent outcome of this work is the Internet, which relies on a backbone of routers that provide the ability to automatically re-routre communication without human intervention when failures occur.\n\nSatellites placed into orbit around the earth must include massive active redundancy to ensure operation will continue for a decade or longer despite failures induced by normal failure, radiation-induced failure, and thermal shock.\n\nThis strategy now dominates space systems, aircraft, and missile systems.\n\nMaintenance requires three actions, which usually involve down time and high priority labor costs:\n\nActive redundancy eliminates down time and reduces manpower requirements by automating all three actions. This requires some amount of automated artificial intelligence.\n\n\"N\" stands for needed equipment. The amount of excess capacity affects overall system reliability by limiting the effects of failure.\n\nFor example, if it takes two generators to power a city, then \"N+1\" would be three generators to allow a single failure. Similarly, \"N+2\" would be four generators, which would allow one generator to fail while a second generator has already failed.\n\nActive redundancy improves operational availability as follows.\n\nActive redundancy in passive components requires redundant components that share the burden when failure occurs, like in cabling and piping. \n\nThis allows forces to be redistributed across a bridge to prevent failure if a vehicle ruptures a cable.\n\nThis allows water flow to be redistributed through pipes when a limited number of valves are shut or pumps shut down.\n\nActive redundancy in active components requires reconfiguration when failure occurs. Computer programming must recognize the failure and automatically reconfigure to restore operation. \n\nAll modern computers provide the following when an existing feature is enabled via fault reporting.\n\nMechanical devices must reconfigure, such as transmission settings on hybrid vehicles that have redundant propulsion systems. The petroleum engine will start up when battery power fails.\n\nElectrical power systems must perform two actions to prevent total system failure when smaller failures occur, such as when a tree falls across a power line. Power systems incorporate communication, switching, and automatic scheduling that allows these actions to be automated.\n\nThis is the only known strategy that can achieve high availability.\n\nThis maintenance philosophy requires custom development with extra components.\n\n"}
{"id": "2449978", "url": "https://en.wikipedia.org/wiki?curid=2449978", "title": "Balance (metaphysics)", "text": "Balance (metaphysics)\n\nIn the metaphysical or conceptual sense, balance is used to mean a point between two opposite forces that is desirable over purely one state or the other, such as a balance between the metaphysical Law and Chaos — law by itself being overly controlling, chaos being overly unmanageable, balance being the point that minimizes the negatives of both.\n\nMore recently, the term \"balance\" has come to refer to a balance of power between multiple opposing forces. Lack of balance (of power) is generally considered to cause aggression by stronger forces towards weaker forces less capable of defending themselves. In the real world, unbalanced stronger forces tend to portray themselves as balanced, and use media controls to downplay this, as well as prevent weaker forces from coming together to achieve a new balance of power. In constructed worlds, such as in video gaming, where nearly all-powerful corporate interests strive to maintain a balance of power among players, players tend to be extremely vocal about what they see as unbalanced mechanics, providing the unbalance negatively affects them. Though the strong and unbalanced (or \"overpowered\") players commonly are vigorous in denial of any lack of balance, the comparative media equality among all player brings change quickly, to further a sense of balance.\n\nThe twentieth century saw the development of both law and chaos in art (and art-music) to the point that the end product became unintelligible at an instinctive/emotional level. Many composers saw one or other of these controlling trends as superior to the other. The truth may lie in a fundamental acceptance of balance as the controlling force in art. In time, we may even come to accept balance between structural and emotional as the essence of beauty.\n\nIn philosophy, the concept of moral balance exists in various forms, one of them is the golden mean, which has virtue being between the extreme and the lacking. Greek philosophers—such as Plato, Aristotle, and the Pythagoreans (who related moral excellence with mathematical perfection)—applied the principle to ethics as well as politics. \"Nothing in excess\" was one of the three phrases carved into the Temple of Apollo at Delphi.\n\nIn Buddhism, this concept is known as the middle way, or \"samatā\", which stated that the way to nirvana led between bodily sexual indulgence and self-mortification and asceticism.\n\nConfucian teachings contain the similar doctrine of Zhōngyōng; the middle.\n\n\n"}
{"id": "20324344", "url": "https://en.wikipedia.org/wiki?curid=20324344", "title": "Constructed language", "text": "Constructed language\n\nA constructed language (sometimes called a conlang) is a language whose phonology, grammar, and vocabulary are, instead of having developed naturally, consciously devised for communication between intelligent beings, most commonly for use by humanoids. Constructed languages may also be referred to as artificial, planned or invented languages and in some cases fictional languages. There are many possible reasons to create a constructed language, such as to ease human communication (see international auxiliary language and code), to give fiction or an associated constructed setting an added layer of realism, for experimentation in the fields of linguistics, cognitive science, and machine learning, for artistic creation, and for language games. \n\nThe expression \"planned language\" is sometimes used to indicate international auxiliary languages and other languages designed for actual use in human communication. Some prefer it to the adjective \"artificial\", as this term may be perceived as pejorative. Outside Esperanto culture, the term language planning means the prescriptions given to a natural language to standardize it; in this regard, even a \"natural language\" may be artificial in some respects, meaning some of its words have been crafted by conscious decision. Prescriptive grammars, which date to ancient times for classical languages such as Latin and Sanskrit, are rule-based codifications of natural languages, such codifications being a middle ground between naïve natural selection and development of language and its explicit construction. The term \"glossopoeia\" is also used to mean language construction, particularly construction of artistic languages.\n\nConlang speakers are rare. For example, the Hungarian census of 2001 found 4570 speakers of Esperanto, 10 of Romanid, two each of Interlingua and Ido and one each of Idiom Neutral and Mundolinco. The Russian census of 2010 found 992 speakers of Esperanto, nine of Ido, one of Edo and no speakers of Slovio or Interlingua.\n\nThe terms \"planned\", \"constructed\", and \"artificial\" are used differently in some traditions. For example, few speakers of Interlingua consider their language artificial, since they assert that it has no invented content: Interlingua's vocabulary is taken from a small set of natural languages, and its grammar is based closely on these source languages, even including some degree of irregularity; its proponents prefer to describe its vocabulary and grammar as standardized rather than artificial or constructed. Similarly, Latino sine flexione (LsF) is a simplification of Latin from which the inflections have been removed. As with Interlingua, some prefer to describe its development as \"planning\" rather than \"constructing\". Some speakers of Esperanto and Esperantidos also avoid the term \"artificial language\" because they deny that there is anything \"unnatural\" about the use of their language in human communication.\n\nBy contrast, some philosophers have argued that all human languages are conventional or artificial. François Rabelais, for instance, stated: \"C'est abus de dire que nous avons une langue naturelle; les langues sont par institution arbitraires et conventions des peuples.\" (\"It's an abuse to say that we have a natural language; languages are arbitrary and conventions of peoples by institution.\")\n\nAn artificial language can also refer to a language that emerges naturally out of experimental studies within the framework of artificial language evolution.\n\nFurthermore, fictional and experimental languages can be naturalistic in that they are meant to sound natural and have realistic amounts of irregularity. If a naturalistic conlang is derived \"a posteriori\" from a real-world natural language, a real-world reconstructed proto-language (such as Vulgar Latin or Proto-Indo-European) or a fictional proto-language, it should imitate natural processes of phonological, lexical, and grammatical change. In contrast with Interlingua, they are not usually intended for easy learning or communication, and most artlangers do not consider Interlingua to be naturalistic in the sense in which this term is used in artlang criticism. Thus, a naturalistic fictional language tends to be more difficult and complex. While Interlingua has simpler grammar, syntax, and orthography than its source languages (though more complex and irregular than Esperanto or its descendants), naturalistic fictional languages typically mimic behaviors of natural languages like irregular verbs and nouns and complicated phonological processes.\n\nIn terms of purpose, most constructed languages can broadly be divided into:\n\n\nThe boundaries between these categories are by no means clear. A constructed language could easily fall into more than one of the above categories. A logical language created for aesthetic reasons would also be classifiable as an artistic language, which might be created by someone with philosophical motives intending for said conlang to be used as an auxiliary language. There are no rules, either inherent in the process of language construction or externally imposed, that would limit a constructed language to fitting only one of the above categories.\n\nA constructed language can have native speakers if young children learn it from parents who speak it fluently. According to \"Ethnologue\", there are \"200–2000 who speak Esperanto as a first language\". A member of the Klingon Language Institute, d'Armond Speers, attempted to raise his son as a native (bilingual with English) Klingon speaker.\n\nAs soon as a constructed language has a community of fluent speakers, especially if it has numerous native speakers, it begins to evolve and hence loses its constructed status. For example, Modern Hebrew and its pronunciation norms were developed from existing traditions of Hebrew, such as Mishnaic Hebrew and Biblical Hebrew following a general Sephardic pronunciation, rather than engineered from scratch, and has undergone considerable changes since the state of Israel was founded in 1948 (Hetzron 1990:693). However, linguist Ghil'ad Zuckermann argues that Modern Hebrew, which he terms \"Israeli\", is a Semito-European hybrid based not only on Hebrew but also on Yiddish and other languages spoken by revivalists. Zuckermann therefore endorses the translation of the Hebrew Bible into what he calls \"Israeli\". Esperanto as a living spoken language has evolved significantly from the prescriptive blueprint published in 1887, so that modern editions of the \"Fundamenta Krestomatio\", a 1903 collection of early texts in the language, require many footnotes on the syntactic and lexical differences between early and modern Esperanto.\n\nProponents of constructed languages often have many reasons for using them. The famous but disputed Sapir–Whorf hypothesis is sometimes cited; this claims that the language one speaks influences the way one thinks. Thus, a \"better\" language should allow the speaker to think more clearly or intelligently or to encompass more points of view; this was the intention of Suzette Haden Elgin in creating Láadan, a feminist language embodied in her feminist science fiction series \"Native Tongue\". A constructed language could also be used to \"restrict\" thought, as in George Orwell's Newspeak, or to \"simplify\" thought, as in Toki Pona. In contrast, linguists such as Steven Pinker argue that ideas exist independently of language. For example, in the book \"The Language Instinct\", Pinker states that children spontaneously re-invent slang and even grammar with each generation. These linguists argue that attempts to control the range of human thought through the reform of language would fail, as concepts like \"freedom\" will reappear in new words if the old words vanish.\n\nProponents claim a particular language makes it easier to express and understand concepts in one area, and more difficult in others. An example can be taken from the way various programming languages make it easier to write certain kinds of programs and harder to write others.\n\nAnother reason cited for using a constructed language is the telescope rule, which claims that it takes less time to first learn a simple constructed language and then a natural language, than to learn only a natural language. Thus, if someone wants to learn English, some suggest learning Basic English first. Constructed languages like Esperanto and Interlingua are in fact often simpler due to the typical lack of irregular verbs and other grammatical quirks. Some studies have found that learning Esperanto helps in learning a non-constructed language later (see propaedeutic value of Esperanto).\n\nCodes for constructed languages include the ISO 639-2 \"art\" for conlangs; however, some constructed languages have their own ISO 639 language codes (e.g. \"eo\" and \"epo\" for Esperanto, \"jbo\" for Lojban, \"ia\" and \"ina\" for Interlingua, \"tlh\" for Klingon and \"io\" and \"ido\" for Ido).\n\nAn \"a priori\" constructed language is one whose features (including vocabulary, grammar, etc.) are not based on an existing language, and an \"a posteriori\" language is the opposite. This categorization, however, is not absolute, as many constructed languages may be both \"a priori\" and \"a posteriori\" depending on which linguistic factors of them are observed.\n\nAn \"a priori\" language (from Latin \"a priori\", \"from the former\") is any constructed language of which all or a number of features are not based on existing languages, but rather invented or elaborated as to work in a different way or to allude different purposes. Some \"a priori\" languages are designed to be international auxiliary languages that remove what could be considered an unfair learning advantage for native speakers of a source language that would otherwise exist for \"a posteriori\" languages. Others, known as philosophical or taxonomic languages, try to categorize their vocabulary, either to express an underlying philosophy or to make it easier to recognize new vocabulary.\n\nThere have been many languages constructed to test linguistic hypotheses (such as the Sapir–Whorf hypothesis) and/or to explore innovative or invented linguistic features. They are therefore necessarily designed with \"a priori\" features. Examples include Ithkuil, Kotava, Lojban (and its predecessor Loglan, both of which are also schematic), and even the fictional-setting Láadan.\n\nMost artistic languages, i.e. those created for amusement or to serve as natural languages of fictional worlds, are largely \"a priori\" in both vocabulary and grammar. Among classical \"fictional natural languages\" are Klingon from the science-fiction franchise \"Star Trek\" and the languages created by fantasy writer J. R. R. Tolkien. Other examples include Dothraki and Valyrian from \"Game of Thrones', 'Trigedasleng from The 100, Atlantean from \"\", Kēlen by Sylvia Sotomayor and aUI by W. John Weilgart. Láadan is both experimental and artistic.\n\nAn \"a posteriori\" language (from Latin \"a posteriori\", \"from the latter\"), according to Louis Couturat, is any constructed language whose elements are borrowed from or based on existing languages. The term can also be extended to controlled versions of existing languages, and is most commonly used to refer to vocabulary despite other features. In distinguishing whether the language is \"a priori\" or \"a posteriori\", the prevalence and distribution of respectable traits is often the key. They may be controlled, naturalistic or schematic.\n\nA controlled language is a version of a natural language with improvements intended to make it easier to learn and to use, such as reduced vocabulary, simplified grammar, etc. These include:\n\nNaturalistic languages are constructed languages which largely imitate the grammar and vocabulary of a particular group of related languages within a family, thus being potentially zonal, i.e. auxiliary between speakers of these languages.\n\nA constructed language is considered schematic when it features a more original grammar, which is usually precise and regular to avoid ambiguities and other obstacles to an easy study, and a vocabulary coming from languages of different families, to make it as recognizable as possible for potential international auxiliary purposes. For that they can be described as half \"a priori\", half \"a posteriori\". There have been many attempts of developing languages with these characteristics, including:\n\nGrammatical speculation dates from Classical Antiquity, appearing for instance in Plato's \"Cratylus\" in Hermogenes's contention that words are not inherently linked to what they refer to; that people apply \"a piece of their own voice ... to the thing\". Athenaeus of Naucratis, in Book III of Deipnosophistae, tells the story of two figures: Dionysius of Sicily and Alexarchus. Dionysius of Sicily created neologisms like \"menandros\" \"virgin\" (from \"menei\" \"waiting\" and \"andra\" \"husband\"), \"menekratēs\" \"pillar\" (from \"menei\" \"it remains in one place\" and \"kratei\" \"it is strong\"), and \"ballantion\" \"javelin\" (from \"balletai enantion\" \"thrown against someone\"). Incidentally, the more common Greek words for those three are \"parthenos\", \"stulos\", and \"akon\". Alexarchus of Macedon, the brother of King Cassander of Macedon, was the founder of the city of Ouranopolis. Athenaeus recounts a story told by Heracleides of Lembos that Alexarchus \"introduced a peculiar vocabulary, referring to a rooster as a \"dawn-crier,\" a barber as a \"mortal-shaver,\" a drachma as \"worked silver\"...and a herald as an \"aputēs\" [from \"ēputa\" \"loud-voiced\"]. \"He once wrote something...to the public authorities in Casandreia...As for what this letter says, in my opinion not even the Pythian god could make sense of it.\" While the mechanisms of grammar suggested by classical philosophers were designed to explain existing languages (Latin, Greek, Sanskrit), they were not used to construct new grammars. Roughly contemporary to Plato, in his descriptive grammar of Sanskrit, Pāṇini constructed a set of rules for explaining language, so that the text of his grammar may be considered a mixture of natural and constructed language.\n\nA legend recorded in the seventh-century Irish work Auraicept na n-Éces claims that Fénius Farsaid visited Shinar after the confusion of tongues, and he and his scholars studied the various languages for ten years, taking the best features of each to create \"in Bérla tóbaide\" (\"the selected language\"), which he named \"Goídelc\"—the Irish language. This appears to be the first mention of the concept of a constructed language in literature.\n\nThe earliest non-natural languages were considered less \"constructed\" than \"super-natural\", mystical, or divinely inspired. The Lingua Ignota, recorded in the 12th century by St. Hildegard of Bingen is an example, and apparently the first entirely artificial language. It is a form of private mystical cant (see also language of angels). An important example from Middle-Eastern culture is Balaibalan, invented in the 16th century. Kabbalistic grammatical speculation was directed at recovering the original language spoken by Adam and Eve in Paradise, lost in the confusion of tongues. The first Christian project for an ideal language is outlined in Dante Alighieri's \"De vulgari eloquentia\", where he searches for the ideal Italian vernacular suited for literature. Ramon Llull's \"Ars Magna\" was a project of a perfect language with which the infidels could be convinced of the truth of the Christian faith. It was basically an application of combinatorics on a given set of concepts. During the Renaissance, Lullian and Kabbalistic ideas were drawn upon in a magical context, resulting in cryptographic applications. The Voynich manuscript may be an example of this.\n\nRenaissance interest in Ancient Egypt, notably the discovery of the \"Hieroglyphica\" of Horapollo, and first encounters with the Chinese script directed efforts towards a perfect written language. Johannes Trithemius, in \"Steganographia\" and \"Polygraphia\", attempted to show how all languages can be reduced to one. In the 17th century, interest in magical languages was continued by the Rosicrucians and Alchemists (like John Dee and his Enochian). Jakob Boehme in 1623 spoke of a \"natural language\" (\"Natursprache\") of the senses.\n\nMusical languages from the Renaissance were tied up with mysticism, magic and alchemy, sometimes also referred to as the language of the birds. The Solresol project of 1817 re-invented the concept in a more pragmatic context.\n\nThe 17th century saw the rise of projects for \"philosophical\" or \"a priori\" languages, such as:\n\n\nThese early taxonomic conlangs produced systems of hierarchical classification that were intended to result in both spoken and written expression. Leibniz had a similar purpose for his \"lingua generalis\" of 1678, aiming at a lexicon of characters upon which the user might perform calculations that would yield true propositions automatically, as a side-effect developing binary calculus. These projects were not only occupied with reducing or modelling grammar, but also with the arrangement of all human knowledge into \"characters\" or hierarchies, an idea that with the Enlightenment would ultimately lead to the \"Encyclopédie\". Many of these 17th−18th centuries conlangs were pasigraphies, or purely written languages with no spoken form or a spoken form that would vary greatly according to the native language of the reader.\n\nLeibniz and the encyclopedists realized that it is impossible to organize human knowledge unequivocally in a tree diagram, and consequently to construct an \"a priori\" language based on such a classification of concepts. Under the entry \"Charactère\", D'Alembert critically reviewed the projects of philosophical languages of the preceding century. After the \"Encyclopédie\", projects for \"a priori\" languages moved more and more to the lunatic fringe. Individual authors, typically unaware of the history of the idea, continued to propose taxonomic philosophical languages until the early 20th century (e.g. Ro), but most recent engineered languages have had more modest goals; some are limited to a specific field, like mathematical formalism or calculus (e.g. Lincos and programming languages), others are designed for eliminating syntactical ambiguity (e.g., Loglan and Lojban) or maximizing conciseness (e.g., Ithkuil).\n\nAlready in the \"Encyclopédie\" attention began to focus on \"a posteriori\" auxiliary languages. Joachim Faiguet de Villeneuve in the article on \"Langue\" wrote a short proposition of a \"laconic\" or regularized grammar of French. During the 19th century, a bewildering variety of such International Auxiliary Languages (IALs) were proposed, so that Louis Couturat and Léopold Leau in \"Histoire de la langue universelle\" (1903) reviewed 38 projects.\n\nThe first of these that made any international impact was Volapük, proposed in 1879 by Johann Martin Schleyer; within a decade, 283 Volapükist clubs were counted all over the globe. However, disagreements between Schleyer and some prominent users of the language led to schism, and by the mid-1890s it fell into obscurity, making way for Esperanto, proposed in 1887 by Ludwik Lejzer Zamenhof, and its descendants. Interlingua, the most recent auxlang to gain a significant number of speakers, emerged in 1951, when the International Auxiliary Language Association published its Interlingua–English Dictionary and an accompanying . The success of Esperanto did not stop others from trying to construct new auxiliary languages, such as Leslie Jones' Eurolengo, which mixes elements of English and Spanish.\n\nLoglan (1955) and its descendants constitute a pragmatic return to the aims of the \"a priori\" languages, tempered by the requirement of usability of an auxiliary language. Thus far, these modern \"a priori\" languages have garnered only small groups of speakers.\n\nRobot Interaction Language (2010) is a spoken language that is optimized for communication between machines and humans. The major goals of ROILA are that it should be easily learnable by the human user, and optimized for efficient recognition by computer speech recognition algorithms.\n\nLanguage can be artistic to the extent that artists use language as a source of creativity in art, poetry, calligraphy or as a metaphor to address themes as cultural diversity and the vulnerability of the individual in a globalizing world. \n\nSome people prefer however to take pleasure in constructing, crafting a language by a conscious decision for reasons of literary enjoyment or aesthetic reasons without any claim of usefulness. Such artistic languages begin to appear in Early Modern literature (in Pantagruel, and in Utopian contexts), but they only seem to gain notability as serious projects beginning in the 20th century. \"A Princess of Mars\" (1912) by Edgar Rice Burroughs was possibly the first fiction of that century to feature a constructed language. J. R. R. Tolkien developed a family of related fictional languages and discussed artistic languages publicly, giving a lecture entitled \"A Secret Vice\" in 1931 at a congress. (Orwell's Newspeak is considered a satire of an IAL rather than an artistic language proper.)\n\nBy the beginning of the first decade of the 21st century, it had become common for science-fiction and fantasy works set in other worlds to feature constructed languages, or more commonly, an extremely limited but defined vocabulary which \"suggests\" the existence of a complete language, and constructed languages are a regular part of the genre, appearing in \"Star Wars\", \"Star Trek\", \"Lord of the Rings (Elvish)\", \"Stargate SG-1\", \"\", \"Game of Thrones (Dothraki language and Valyrian languages)\", \"Avatar\", \"Dune\" and the \"Myst\" series of computer adventure games.\n\nVarious paper zines on constructed languages were published from the 1970s through the 1990s, such as \"Glossopoeic Quarterly\", \"Taboo Jadoo\", and \"The Journal of Planned Languages\".\nThe Conlang Mailing List was founded in 1991, and later split off an AUXLANG mailing list dedicated to international auxiliary languages. In the early to mid-1990s a few conlang-related zines were published as email or websites, such as \"Vortpunoj\" and \"Model Languages\". The Conlang mailing list has developed a community of conlangers with its own customs, such as translation challenges and translation relays, and its own terminology. Sarah Higley reports from results of her surveys that the demographics of the Conlang list are primarily men from North America and western Europe, with a smaller number from Oceania, Asia, the Middle East, and South America, with an age range from thirteen to over sixty; the number of women participating has increased over time.\n\nMore recently founded online communities include the Zompist Bulletin Board (ZBB; since 2001) and the Conlanger Bulletin Board. Discussion on these forums includes presentation of members' conlangs and feedback from other members, discussion of natural languages, whether particular conlang features have natural language precedents, and how interesting features of natural languages can be repurposed for conlangs, posting of interesting short texts as translation challenges, and meta-discussion about the philosophy of conlanging, conlangers' purposes, and whether conlanging is an art or a hobby. Another 2001 survey by Patrick Jarrett showed an average age of 30.65, with the average time since starting to invent languages 11.83 years. A more recent thread on the ZBB showed that many conlangers spend a relatively small amount of time on any one conlang, moving from one project to another; about a third spend years on developing the same language.\n\n"}
{"id": "28014782", "url": "https://en.wikipedia.org/wiki?curid=28014782", "title": "Criticism of advertising", "text": "Criticism of advertising\n\nAdvertising is a form of communication intended to persuade an audience to purchase products, ideals or services. While advertising can be seen as necessary for economic growth, it is not without social costs. Unsolicited commercial email and other forms of spam have become so prevalent that they are a major nuisance to internet users, as well as being a financial burden on internet service providers. Advertising increasingly invades public spaces, such as schools, which some critics argue is a form of child exploitation. Advertising frequently uses psychological pressure (for example, appealing to feelings of inadequacy) on the intended consumer, which may be harmful. As a result of these criticisms, the advertising industry has seen low approval rates in surveys and negative cultural portrayals.\n\nCriticism of advertising is closely linked with criticism of media and often interchangeable. Critics can refer to advertising's\n\nAs advertising has become prevalent in modern society, it is increasingly being criticized. Advertising occupies public space and more and more invades the private sphere of people. According to Georg Franck, \"It is becoming harder to escape from advertising and the media. Public space is increasingly turning into a gigantic billboard for products of all kind. The aesthetical and political consequences cannot yet be foreseen.\" Hanno Rauterberg in the German newspaper \"Die Zeit\" calls advertising a new kind of dictatorship that cannot be escaped.\n\nAd creep says, \"There are ads in schools, airport lounges, doctors offices, movie theaters, hospitals, gas stations, elevators, convenience stores, on the Internet, on fruit, on ATMs, on garbage cans and countless other places. There are ads on beach sand and restroom walls.\" \"One of the ironies of advertising in our times is that as commercialism increases, it makes it that much more difficult for any particular advertiser to succeed, hence pushing the advertiser to even greater efforts.\" Within a decade advertising in radios climbed to nearly 18 or 19 minutes per hour, on prime-time television the standard until 1982 was no more than 9.5 minutes of advertising per hour, today it is between 14 and 17 minutes. With the introduction of the shorter 15-second-spot the total amount of ads increased even more. Ads are not only placed in breaks but also into sports telecasts during the game itself. They flood the Internet, a growing market.\n\nOther growing markets are product placements in entertainment programming and movies where it has become standard practice and virtual advertising where products get placed retroactively into rerun shows. Product billboards are virtually inserted into Major League Baseball broadcasts and in the same manner, virtual street banners or logos are projected on an entry canopy or sidewalks, for example during the arrival of celebrities at the 2001 Grammy Awards. Advertising precedes the showing of films at cinemas including lavish 'film shorts' produced by companies such as Microsoft or DaimlerChrysler. \"The largest advertising agencies have begun working to co-produce programming in conjunction with the largest media firms\", creating Infomercials resembling entertainment programming.\n\nOpponents equate the growing amount of advertising with a \"tidal wave\" and restrictions with \"damming\" the flood. Kalle Lasn, one of the most outspoken critics of advertising, considers advertising \"the most prevalent and toxic of the mental pollutants. From the moment your radio alarm sounds in the morning to the wee hours of late-night TV microjolts of commercial pollution flood into your brain at the rate of around 3,000 marketing messages per day. Every day an estimated 12 billion display ads, 3 million radio commercials and more than 200,000 television commercials are dumped into North America's collective unconscious\". In the course of their life, the average American watches three years of advertising on television.\n\nVideo games incorporate products into their content. Special commercial patient channels in hospitals and public figures sporting temporary tattoos. A method unrecognisable as advertising is so-called \"guerrilla marketing\" which is spreading 'buzz' about a new product in target audiences. Cash-strapped U.S. cities offer police cars for advertising. Companies buy the names of sports stadiums for advertising. The Hamburg soccer Volkspark stadium first became the AOL Arena and then the HSH Nordbank Arena. The Stuttgart Neckarstadion became the Mercedes-Benz Arena, the Dortmund Westfalenstadion is the Signal Iduna Park. The former SkyDome in Toronto was renamed Rogers Centre.\n\nWhole subway stations in Berlin are redesigned into product halls and exclusively leased to a company. Düsseldorf has \"multi-sensorial\" adventure transit stops equipped with loudspeakers and systems that spread the smell of a detergent. Swatch used beamers to project messages on the Berlin TV-tower and Victory column, which was fined because it was done without a permit. The illegality was part of the scheme and added promotion. Christopher Lasch states that advertising leads to an overall increase in consumption in society; \"Advertising serves not so much to advertise products as to promote consumption as a way of life.\"\n\nIn the US, advertising is equated with constitutionally guaranteed freedom of opinion and speech.\n\nAn example for this debate is advertising for tobacco or alcohol but also advertising by mail or fliers (clogged mail boxes), advertising on the phone, in the Internet and advertising for children. Various legal restrictions concerning spamming, advertising on mobile phones, when addressing children, tobacco, alcohol have been introduced by the US, the EU and other countries.\n\nMcChesney argues, that the government deserves constant vigilance when it comes to such regulations, but that it is certainly not \"the only antidemocratic force in our society. Corporations and the wealthy enjoy a power every bit as immense as that enjoyed by the lords and royalty of feudal times\" and \"markets are not value-free or neutral; they not only tend to work to the advantage of those with the most money, but they also by their very nature emphasize profit over all else. Hence, today the debate is over whether advertising or food labelling, or campaign contributions are speech... if the rights to be protected by the First Amendment can only be effectively employed by a fraction of the citizenry, and their exercise of these rights gives them undue political power and undermines the ability of the balance of the citizenry to exercise the same rights and/or constitutional rights, then it is not necessarily legitimately protected by the First Amendment\". \"Those with the capacity to engage in free press are in a position to determine who can speak to the great mass of citizens and who cannot\".\n\nGeorg Franck at Vienna University of Technology, says that advertising is part of what he calls \"mental capitalism\", taking up a term (mental) which has been used by groups concerned with the mental environment, such as Adbusters. Franck blends the \"Economy of Attention\" with Christopher Lasch's culture of narcissism into the mental capitalism: In his essay \"Advertising at the Edge of the Apocalypse\", Sut Jhally writes: \"20th century advertising is the most powerful and sustained system of propaganda in human history and its cumulative cultural effects, unless quickly checked, will be responsible for destroying the world as we know it.\"\n\nAdvertising has developed into a multibillion-dollar business. In 2014, 537 billion US dollars were spent worldwide for advertising. In 2013, TV accounted for 40.1% of ad spending, compared to a combined 18.1% for internet, 16.9% for newspapers, 7.9% for magazines, 7% for outdoor, 6.9% for radio, 2.7% for mobile and 0.5% for cinema as a share of ad spending by medium. Advertising is considered to raise consumption.\n\nAttention and attentiveness have become a new commodity for which a market developed. \"The amount of attention that is absorbed by the media and redistributed in the competition for quotas and reach is not identical with the amount of attention, that is available in society. The total amount circulating in society is made up of the attention exchanged among the people themselves and the attention given to media information. Only the latter is homogenised by quantitative measuring and only the latter takes on the character of an anonymous currency.\" According to Franck, any surface of presentation that can guarantee a certain degree of attentiveness works as magnet for attention, for example, media which are actually meant for information and entertainment, culture and the arts, public space etc. It is this attraction which is sold to the advertising business. In Germany, the advertising industry contributes 1.5% of the gross national income. The German Advertising Association stated that in 2007, 30.78 billion Euros were spent on advertising in Germany, 26% in newspapers, 21% on television, 15% by mail and 15% in magazines. In 2002 there were 360,000 people employed in the advertising business. The Internet revenues for advertising doubled to almost 1 billion Euros from 2006 to 2007, giving it the highest growth rates.\n\nFew consumers are aware of the fact that they are the ones paying for every cent spent for public relations, advertisements, rebates, packaging etc., since they ordinarily get included in the price calculation.\n\nThe most important element of advertising is not information but suggestion - more or less making use of associations, emotions and drives in the subconscious, such as sex drive, herd instinct, desires such as happiness, health, fitness, appearance, self-esteem, reputation, belonging, social status, identity, adventure, distraction, reward, fears such as illness, weaknesses, loneliness, need, uncertainty, security or of prejudices, learned opinions and comforts. \"All human needs, relationships, and fears - the deepest recesses of the human psyche - become mere means for the expansion of the commodity universe under the force of modern marketing. With the rise to prominence of modern marketing, commercialism - the translation of human relations into commodity relations - although a phenomenon intrinsic to capitalism, has expanded exponentially.\" Cause-related marketing in which advertisers link their product to some worthy social cause has boomed over the past decade.\n\nAdvertising uses the model role of celebrities or popular figures and makes deliberate use of humor as well as of associations with color, tunes, certain names and terms. These are factors of how one perceives themself and one's self-worth. In his description of 'mental capitalism' Franck says, \"the promise of consumption making someone irresistible is the ideal way of objects and symbols into a person's subjective experience. Evidently, in a society in which revenue of attention moves to the fore, consumption is drawn by one's self-esteem. As a result, consumption becomes 'work' on a person's attraction. From the subjective point of view, this 'work' opens fields of unexpected dimensions for advertising. Advertising takes on the role of a life councillor in matters of attraction. The cult around one's own attraction is what Christopher Lasch described as 'Culture of Narcissism'.\"\n\nFor advertising critics another serious problem is that, \"the long standing notion of separation between advertising and editorial/creative sides of media is rapidly crumbling\" and advertising is increasingly hard to tell apart from news, information or entertainment. The boundaries between advertising and programming are becoming blurred. According to the media firms all this commercial involvement has no influence over actual media content, but as McChesney puts it, \"this claim fails to pass even the most basic giggle test, it is so preposterous.\"\n\nAdvertising draws \"heavily on psychological theories about how to create subjects, enabling advertising and marketing to take on a 'more clearly psychological tinge'. Increasingly, the emphasis in advertising has switched from providing 'factual' information to the symbolic connotations of commodities, since the crucial cultural premise of advertising is that the material object being sold is never in itself enough. Even those commodities providing for the most mundane necessities of daily life must be imbued with symbolic qualities and culturally endowed meanings via the 'magic system' of advertising. In this way and by altering the context in which advertisements appear, things 'can be made to mean 'just about anything' and the 'same' things can be endowed with different intended meanings for different individuals and groups of people, thereby offering mass produced visions of individualism.\"\n\nBefore advertising is done, market research institutions need to know and describe the target group to exactly plan and implement the advertising campaign and to achieve the best possible results. A whole array of sciences directly deal with advertising and marketing or are used to improve its effects. Focus groups, psychologists and cultural anthropologists are de rigueur in marketing research\". Vast amounts of data on persons and their shopping habits are collected, accumulated, aggregated and analysed with the aid of credit cards, bonus cards, raffles and internet surveying. With increasing accuracy this supplies a picture of behaviour, wishes and weaknesses of certain sections of a population with which advertisement can be employed more selectively and effectively.\n\nThe efficiency of advertising is improved through advertising research. Universities, of course supported by business and in co-operation with other disciplines (s. above), mainly Psychiatry, Anthropology, Neurology and behavioural sciences, are constantly in search for ever more refined, sophisticated, subtle and crafty methods to make advertising more effective. \"Neuromarketing is a controversial new field of marketing which uses medical technologies such as functional Magnetic Resonance Imaging (fMRI)—not to heal, but to sell products. Advertising and marketing firms have long used the insights and research methods of psychology in order to sell products, of course. But today these practices are reaching epidemic levels, and with a complicity on the part of the psychological profession that exceeds that of the past. The result is an enormous advertising and marketing onslaught that comprises, arguably, the largest single psychological project ever undertaken. Yet, this great undertaking remains largely ignored by the American Psychological Association.\" Robert McChesney calls it \"the greatest concerted attempt at psychological manipulation in all of human history.\"\n\nAlmost all mass media are advertising media and many of them are exclusively advertising media and, with the exception of public service broadcasting, are in the private sector. Their income is predominantly generated through advertising; in the case of newspapers and magazines from 50 to 80%. Public service broadcasting in some countries can also heavily depend on advertising as a source of income (up to 40%). In the view of critics no media that spreads advertisements can be independent and the higher the proportion of advertising, the higher the dependency. This dependency has \"distinct implications for the nature of media content... In the business press, the media are often referred to in exactly the way they present themselves in their candid moments: as a branch of the advertising industry.\"\n\nIn addition, the private media are increasingly subject to mergers and concentration with property situations often becoming entangled and opaque. This development, which Henry A. Giroux calls an \"ongoing threat to democratic culture\", by itself should suffice to sound all alarms in a democracy. Five or six advertising agencies dominate this 400 billion U.S. dollar global industry.\n\n\"Journalists have long faced pressure to shape stories to suit advertisers and owners ... the vast majority of TV station executives found their news departments 'cooperative' in shaping the news to assist in 'non-traditional revenue development.\"\n\nNegative and undesired reporting can be prevented or influenced when advertisers threaten to cancel orders or simply when there is a danger of such a cancellation. Media dependency and such a threat become very real when there is only one dominant or very few large advertisers. The influence of advertisers is not only in regard to news or information on their own products or services but expands to articles or shows not directly linked to them. In order to secure their advertising revenues the media have to create the best possible 'advertising environment'.\n\nAnother problem considered censorship by critics is the refusal of media to accept advertisements that are not in their interest. A striking example of this is the refusal of TV stations to broadcast ads by Adbusters. Groups try to place advertisements and are refused by networks.\n\nIt is principally the viewing rates which decide upon the programme in the private radio and television business. \"Their business is to absorb as much attention as possible. The viewing rate measures the attention the media trades for the information offered. The service of this attraction is sold to the advertising business\" and the viewing rates determine the price that can be demanded for advertising.\n\n\"Advertising companies determining the contents of shows has been part of daily life in the USA since 1933. Procter & Gamble (P&G) ... offered a radio station a history-making trade (today known as \"bartering\"): the company would produce an own show for \"free\" and save the radio station the high expenses for producing contents. Therefore, the company would want its commercials spread and, of course, its products placed in the show. Thus, the series 'Ma Perkins' was created, which P&G skilfully used to promote Oxydol, the leading detergent brand in those years and the Soap opera was born ...\"\n\nWhile critics basically worry about the subtle influence of the economy on the media, there are also examples of blunt exertion of influence. The US company Chrysler, before it merged with Daimler Benz had its agency (PentaCom) send out a letter to numerous magazines, demanding that they send an overview of all the topics before the next issue was published, to \"avoid potential conflict\". Chrysler most of all wanted to know if there would be articles with \"sexual, political or social\" content, or which could be seen as \"provocative or offensive\". PentaCom executive David Martin said: \"Our reasoning is, that anyone looking at a 22.000 $ product would want it surrounded by positive things. There is nothing positive about an article on child pornography.\" In another example, the USA Network held top-level‚ off-the-record meetings with advertisers in 2000 to let them tell the network what type of programming content they wanted in order for USA to get their advertising.\"\nTelevision shows are created to accommodate the needs of advertising, e.g. splitting them up in suitable sections. Their dramaturgy is typically designed to end in suspense or leave an unanswered question in order to keep the viewer attached.\n\nThe movie system, at one time outside the direct influence of the broader marketing system, is now fully integrated into it through the strategies of licensing, tie-ins and product placements. The prime function of many Hollywood films today is to aid in the selling of the immense collection of commodities.\nThe press called the 2002 Bond film 'Die Another Day' featuring 24 major promotional partners an 'ad-venture' and noted that James Bond \"now has been 'licensed to sell'\" As it has become standard practice to place products in motion pictures, it \"has self-evident implications for what types of films will attract product placements and what types of films will therefore be more likely to get made\".\n\nAdvertising and information are increasingly hard to distinguish from each other. \"The borders between advertising and media ... become more and more blurred... What August Fischer, chairman of the board of Axel Springer publishing company considers to be a 'proven partnership between the media and advertising business' critics regard as nothing but the infiltration of journalistic duties and freedoms\". According to RTL Group former executive Helmut Thoma \"private stations shall not and cannot serve any mission but only the goal of the company which is the 'acceptance by the advertising business and the viewer'. The setting of priorities in this order actually says everything about the 'design of the programmes' by private television.\"\nPatrick Le Lay, former managing director of TF1, a private French television channel with a market share of 25 to 35%, said: \"There are many ways to talk about television. But from the business point of view, let's be realistic: basically, the job of TF1 is, e. g. to help Coca Cola sell its product. (...) For an advertising message to be perceived the brain of the viewer must be at our disposal. The job of our programmes is to make it available, that is to say, to distract it, to relax it and get it ready between two messages. It is disposable human brain time that we sell to Coca Cola.\"\n\nBecause of these dependencies, a widespread and fundamental public debate about advertising and its influence on information and freedom of speech is difficult to obtain, at least through the usual media channels: it would saw off the branch it was sitting on. \"The notion that the commercial basis of media, journalism, and communication could have troubling implications for democracy is excluded from the range of legitimate debate\" just as \"capitalism is off-limits as a topic of legitimate debate in US political culture\".\n\nAn early critic of the structural basis of US journalism was Upton Sinclair with his novel The Brass Check in which he stresses the influence of owners, advertisers, public relations, and economic interests on the media. In his book \"Our Master's Voice - Advertising\" the social ecologist James Rorty (1890-1973) wrote:\n\"The gargoyle's mouth is a loudspeaker, powered by the vested interest of a two-billion dollar industry, and back of that the vested interests of business as a whole, of industry, of finance. It is never silent, it drowns out all other voices, and it suffers no rebuke, for it is not the voice of America? That is its claim and to some extent it is a just claim...\"\n\nIt has taught us how to live, what to be afraid of, what to be proud of, how to be beautiful, how to be loved, how to be envied, how to be successful.. Is it any wonder that the American population tends increasingly to speak, think, feel in terms of this jabberwocky? That the stimuli of art, science, religion are progressively expelled to the periphery of American life to become marginal values, cultivated by marginal people on marginal time?\"\n\nPerformances, exhibitions, shows, concerts, conventions and most other events can hardly take place without sponsoring. Artists are graded and paid according to their art's value for commercial purposes.\nCorporations promote renowned artists, thereby getting exclusive rights in global advertising campaigns. Broadway shows like 'La Bohème' featured commercial props in their sets.\n\nAdvertising itself is extensively considered to be a contribution to culture. Advertising is integrated into fashion. On many pieces of clothing the company logo is the only design or is an important part of it. There is only a little room left outside the consumption economy, in which culture and art can develop independently and where alternative values can be expressed. A last important sphere, the universities, is under strong pressure to open up for business and its interests.\nCompetitive sports have become unthinkable without sponsoring and there is a mutual dependency. High income with advertising is only possible with a comparable number of spectators or viewers. On the other hand, the poor performance of a team or a sportsman results in less advertising revenues. Jürgen Hüther and Hans-Jörg Stiehler talk about a 'Sports/Media Complex which is a complicated mix of media, agencies, managers, sports promoters, advertising etc. with partially common and partially diverging interests but in any case with common commercial interests. The media presumably is at centre stage because it can supply the other parties involved with a rare commodity, namely (potential) public attention. In sports \"the media are able to generate enormous sales in both circulation and advertising.\"\n\n\"Sports sponsorship is acknowledged by the tobacco industry to be valuable advertising. A Tobacco Industry journal in 1994 described the Formula One car as 'The most powerful advertising space in the world'. ... In a cohort study carried out in 22 secondary schools in England in 1994 and 1995 boys whose favourite television sport was motor racing had a 12.8% risk of becoming regular smokers compared to 7.0% of boys who did not follow motor racing.\"\n\nNot the sale of tickets but transmission rights, sponsoring and merchandising in the meantime make up the largest part of sports association's and sports club's revenues with the IOC (International Olympic Committee) taking the lead. The influence of the media brought many changes in sports including the admittance of new 'trend sports' into the Olympic Games, the alteration of competition distances, changes of rules, animation of spectators, changes of sports facilities, the cult of sports heroes who quickly establish themselves in the advertising and entertaining business because of their media value and last but not least, the naming and renaming of sport stadiums after big companies.\n\n\"In sports adjustment into the logic of the media can contribute to the erosion of values such as equal chances or fairness, to excessive demands on athletes through public pressure and multiple exploitation or to deceit (doping, manipulation of results ...). It is in the very interest of the media and sports to counter this danger because media sports can only work as long as sport exists.\n\nEvery visually perceptible place has potential for advertising, especially urban areas with their structures but also landscapes in sight of thoroughfares are more and more turning into media for advertisements. Signs, posters, billboards, flags have become decisive factors in the urban appearance and their numbers are still on the increase. \"Outdoor advertising has become unavoidable. Traditional billboards and transit shelters have cleared the way for more pervasive methods such as wrapped vehicles, sides of buildings, electronic signs, kiosks, taxis, posters, sides of buses, and more. Digital technologies are used on buildings to sport 'urban wall displays'. In urban areas commercial content is placed in our sight and into our consciousness every moment we are in public space. The German Newspaper 'Zeit' called it a new kind of 'dictatorship that one cannot escape'.\n\nOver time, this domination of the surroundings has become the \"natural\" state. Through long-term commercial saturation, it has become implicitly understood by the public that advertising has the right to own, occupy and control every inch of available space. The steady normalization of invasive advertising dulls the public's perception of their surroundings, re-enforcing a general attitude of powerlessness toward creativity and change, thus a cycle develops enabling advertisers to slowly and consistently increase the saturation of advertising with little or no public outcry.\"\n\nThe massive optical orientation toward advertising changes the function of public spaces which are utilised by brands. Urban landmarks are turned into trademarks. The highest pressure is exerted on renown and highly frequented public spaces which are also important for the identity of a city (e.g. Piccadilly Circus, Times Square, Alexanderplatz).\nUrban spaces are public commodities and in this capacity they are subject to \"aesthetical environment protection\", mainly through building regulations, heritage protection and landscape protection. \"It is in this capacity that these spaces are now being privatised. They are peppered with billboards and signs, they are remodelled into media for advertising.\"\n\n\"Advertising has an \"agenda setting function\" which is the ability, with huge sums of money, to put consumption as the only item on the agenda. In the battle for a share of the public conscience this amounts to non-treatment (ignorance) of whatever is not commercial and whatever is not advertised for\n\nWith increasing force, advertising makes itself comfortable in the private sphere so that the voice of commerce becomes the dominant way of expression in society.\"\nAdvertising critics see advertising as the leading light in our culture. Sut Jhally and James Twitchell go beyond considering advertising as kind of religion and that advertising even replaces religion as a key institution.\n\n\"Corporate advertising (or commercial media) is the largest single psychological project ever undertaken by the human race. Yet for all of that, its impact on us remains unknown and largely ignored. When I think of the media's influence over years, over decades, I think of those brainwashing experiments conducted by Dr. Ewen Cameron in a Montreal psychiatric hospital in the 1950s (see MKULTRA). The idea of the CIA-sponsored \"depatterning\" experiments was to outfit conscious, unconscious or semiconscious subjects with headphones, and flood their brains with thousands of repetitive \"driving\" messages that would alter their behaviour over time...Advertising aims to do the same thing.\"\n\nAdvertising is especially aimed at young people and children and it increasingly reduces young people to consumers.\nFor Sut Jhally it is not \"surprising that something this central and with so much being expended on it should become an important presence in social life. Indeed, commercial interests intent on maximizing the consumption of the immense collection of commodities have colonized more and more of the spaces of our culture. For instance, almost the entire media system (television and print) has been developed as a delivery system for marketers, and its prime function is to produce audiences for sale to advertisers. Both the advertisements it carries and the editorial matter that acts as a support for it celebrate the consumer society. The movie system, at one time outside the direct influence of the broader marketing system, is now fully integrated into it through the strategies of licensing, tie-ins and product placements.\n\nThe prime function of many Hollywood films today is to aid in the selling of the immense collection of commodities. As public funds are drained from the non-commercial cultural sector, art galleries, museums and symphonies bid for corporate sponsorship.\" In the same way effected is the education system and advertising is increasingly penetrating schools and universities. Cities, such as New York, accept sponsors for public playgrounds. \"Even the pope has been commercialized ... The pope's 4-day visit to Mexico in ...1999 was sponsored by Frito-Lay and PepsiCo.\nThe industry is accused of being one of the engines powering a convoluted economic mass production system which promotes consumption.\nAs far as social effects are concerned it does not matter whether advertising fuels consumption but which values, patterns of behaviour and assignments of meaning it propagates.\n\nAdvertising is accused of hijacking the language and means of pop culture, of protest movements and even of subversive criticism and does not shy away from scandalizing and breaking taboos (e.g. Benneton). This in turn incites counter action, what Kalle Lasn in 2001 called \"Jamming the Jam of the Jammers\". Anything goes. \"It is a central social-scientific question what people can be made to do by suitable design of conditions and of great practical importance. For example, from a great number of experimental psychological experiments it can be assumed, that people can be made to do anything they are capable of, when the according social condition can be created.\"\n\nAdvertising often uses stereotype gender specific roles of men and women reinforcing existing clichés and it has been criticized as \"inadvertently or even intentionally promoting sexism, racism, heterosexualism, ableism, ageism, et cetera... At very least, advertising often reinforces stereotypes by drawing on recognizable \"types\" in order to tell stories in a single image or 30 second time frame.\" Activities are depicted as typical male or female (stereotyping). In addition, people are reduced to their sexuality or equated with commodities and gender specific qualities are exaggerated. Sexualized female bodies, but increasingly also males, serve as eye-catchers.\n\nIn advertising, it is usually a woman that is depicted as\n\nA large portion of advertising deals with the promotion of products in a way that defines an \"ideal\" body image. This objectification greatly affects women; however, men are also affected. Women and men in advertising are frequently portrayed in unrealistic and distorted images that set a standard for what is considered \"beautiful,\" \"attractive\" or \"desirable.\" Such imagery does not allow for what is found to be beautiful in various cultures or to the individual. It is exclusionary, rather than inclusive, and consequently, these advertisements promote a negative message about body image to the average person. Because of this form of media, girls, boys, women and men may feel under high pressure to maintain an unrealistic and often unhealthy body weight or even to alter their physical appearance cosmetically or surgically in minor to drastic ways.\n\nThe EU parliament passed a resolution in 2008 that advertising may not be discriminating and degrading. This shows that politicians are increasingly concerned about the negative impacts of advertising. However, the benefits of promoting overall health and fitness are often overlooked. Men are also negatively portrayed as incompetent and the butt of every joke in advertising.\n\nBusiness is interested in children and adolescents because of their buying power and because of their influence on the shopping habits of their parents. As they are easier to influence they are especially targeted by the advertising business.\n\nChildren \"represent three distinct markets:\nKids will carry forward brand expectations, whether positive, negative, or indifferent. Kids are already accustomed to being catered to as consumers. The long term prize: Loyalty of the kid translates into a brand loyal adult customer\"\n\n\"Kids represent an important demographic to marketers because they have their own purchasing power, they influence their parents' buying decisions and they're the adult consumers of the future.\" Advertising for other products preferably uses media with which they can also reach the next generation of consumers. \"Key advertising messages exploit the emerging independence of young people\".\n\nThe children's market, where resistance to advertising is weakest, is the \"pioneer for ad creep\". One example is product placement. \"Product placements show up everywhere, and children aren't exempt. Far from it. The animated film, Foodfight, had 'thousands of products and character icons from the familiar (items) in a grocery store.' Children's books also feature branded items and characters, and millions of them have snack foods as lead characters.\"\n\nThe average Canadian child sees 350,000 TV commercials before graduating from high school, spends nearly as much time watching TV as attending classes. In 1980 the Canadian province of Quebec banned advertising for children under age 13. \"In upholding the constitutional validity of the Quebec Consumer Protection Act restrictions on advertising to children under age 13 (in the case of a challenge by a toy company) the Court held: '...advertising directed at young children is per se manipulative. Such advertising aims to promote products by convincing those who will always believe.'\"\nNorway (ads directed at children under age 12), and Sweden (television ads aimed at children under age 12) also have legislated broad bans on advertising to children, during child programmes any kind of advertising is forbidden in Sweden, Denmark, Austria and Flemish Belgium. In Greece there is no advertising for kids products from 7 to 22 h. An attempt to restrict advertising directed at children in the US failed with reference to the First Amendment. In Spain bans are also considered undemocratic.\n\nWeb sites targeted to children may also display advertisements, though there are fewer ads on non-profit web sites than on for-profit sites and those ads were less likely to contain enticements. However, even ads on non-profit sites may link to sites that collect personal information.\n\nSweets, ice cream, and breakfast food makers often aim their promotion at children and adolescents. For example, an ad for a breakfast cereal on a channel aimed at adults will have music that is a soft ballad, whereas on a channel aimed at children, the same ad will use a catchy rock jingle of the same song to aim at kids.\n\"The marketing industry is facing increased pressure over claimed links between exposure to food advertising and a range of social problems, especially growing obesity levels.\" \"Fast food chains spend more than 3 billion dollars a year on advertising, much of it aimed at children... Restaurants offer incentives such as playgrounds, contests, clubs, games, and free toys and other merchandise related to movies, TV shows and even sports leagues.\"\nThese businesses are constantly reaping the benefits of this child manipulation.\nIn 2006, forty-four of the largest U.S. food industries spent about 2 billion dollars on advertising their products, which mainly consisted of unhealthy, sugary and fatty foods. Such massive advertising has a detrimental effect on children and it heavily influences their diets. Extensive research proves that most of the food consumed between ages of 2-18 is low in nutrients. Facing a lot of pressure from health industries and laws, such as the Children's Food and Beverage Advertising initiative, food marketers were forced to tweak and limit their advertising strategies. Despite regulations, a 2009 report shows that three quarters of all food advertising during children's television programs were outside of the law's boundaries. Government attempts to put a heavy burden on food marketers in order to prevent the issue, but food marketers enjoy the benefits of the First Amendment which limits government's power to prevent advertising against children. The Federal Trade Commission states that children between the ages of 2-11 on average see 15 food based commercials on television daily. Most of these commercial involve high-sugar and high-fat foods, which adds to the problem of childhood obesity. An experiment that took place in a summer camp, where researches showed food advertisements to children between ages 5–8 for two weeks. The outcome-what kids chose to eat at a cafeteria were the ads they saw on TV over the two weeks.\n\nIn advertisements, cigarettes \"are used as a fashion accessory and appeal to young women. Other influences on young people include the linking of sporting heroes and smoking through sports sponsorship, the use of cigarettes by popular characters in television programmes and cigarette promotions. Research suggests that young people are aware of the most heavily advertised cigarette brands.\" Alcohol is portrayed in advertising similarly to smoking, \"Alcohol ads continue to appeal to children and portrayals of alcohol use in the entertainment media are extensive\". The consumption of alcohol is glamorized and shown without consequences in advertisements, music, magazines, television, film, etc. The advertisements include alcoholic beverages with colorful packaging and sweet tasting flavors, catering to the interests and likes of children and teens. The alcohol industry has a big financial stake in underage drinking, hoping to gain lifelong customers. Therefore, the media are overrun with alcohol ads which appeal to children, involving animal characters, popular music, and comedy.\n\n\"Kids are among the most sophisticated observers of ads. They can sing the jingles and identify the logos, and they often have strong feelings about products. What they generally don't understand, however, are the issues that underlie how advertising works. Mass media are used not only to sell goods but also ideas: how we should behave, what rules are important, who we should respect and what we should value.\"\n\nAccording to critics, the total commercialization of all fields of society, the privatization of public space, the acceleration of consumption and waste of resources including the negative influence on lifestyles and on the environment has not been noticed to the necessary extent. The \"hyper-commercialization of the culture is recognized and roundly detested by the citizenry, although the topic scarcely receives a whiff of attention in the media or political culture.\" \"The greatest damage done by advertising is precisely that it incessantly demonstrates the prostitution of men and women who lend their intellects, their voices, their artistic skills to purposes in which they themselves do not believe, and ... that it helps to shatter and ultimately destroy our most precious non-material possessions: the confidence in the existence of meaningful purposes of human activity and respect for the integrity of man.\" \"The struggle against advertising is therefore essential if we are to overcome the pervasive alienation from all genuine human needs that currently plays such a corrosive role in our society. But in resisting this type of hyper-commercialism we should not be under any illusions. Advertising may seem at times to be an almost trivial if omnipresent aspect of our economic system. Yet, as economist A. C. Pigou pointed out, it could only be 'removed altogether' if 'conditions of monopolistic competition' inherent to corporate capitalism were removed. To resist it is to resist the inner logic of capitalism itself, of which it is the pure expression.\"\n\n\"Visual pollution, much of it in the form of advertising, is an issue in all the world's large cities. But what is pollution to some is a vibrant part of a city's fabric to others. New York City without Times Square's huge digital billboards or Tokyo without the Ginza's commercial panorama is unthinkable. Piccadilly Circus would be just a London roundabout without its signage. Still, other cities, like Moscow, have reached their limit and have begun to crack down on over-the-top outdoor advertising.\"\n\n\"Many communities have chosen to regulate billboards to protect and enhance their scenic character. The following is by no means a complete list of such communities. Scenic America estimates the nationwide total of cities and communities prohibiting the construction of new billboards to be at least 1500.\n\nA number of states in the US prohibit all billboards:\n\nIn 2006, the city of São Paulo, Brazil ordered the downsizing or removal of all billboards and most other forms of commercial advertising in the city.\" In 2015, Grenoble, France similarly banned all billboards and public advertising.\n\nTechnical appliances, such as Spam filters, TV-Zappers, ad blockers for TVs and stickers on mail boxes—\"No Advertising\"—and an increasing number of court cases indicate a growing interest of people to restrict or rid themselves of unwelcome advertising.\n\nConsumer protection associations, environment protection groups, globalization opponents, consumption critics, sociologists, media critics, scientists and many others deal with the negative aspects of advertising. \"Antipub\" in France, \"subvertising\", culture jamming and adbusting have become established terms in the anti-advertising community. On the international level globalization critics such as Naomi Klein and Noam Chomsky are also renowned media and advertising critics. These groups criticize the complete occupation of public spaces, surfaces, the airwaves, the media, schools etc. and the constant exposure of almost all senses to advertising messages, the invasion of privacy, and that only few consumers are aware that they themselves are bearing the costs for this to the very last penny. Some of these groups, such as the Billboard Liberation Front Creative Group in San Francisco or Adbusters in Vancouver, Canada, have manifestos. Grassroots organizations campaign against advertising or certain aspects of it in various forms and strategies and quite often have different roots. Adbusters, for example contests and challenges the intended meanings of advertising by subverting them and creating unintended meanings instead. Other groups, like Illegal Signs Canada, try to stem the flood of billboards by detecting and reporting ones that have been put up without permit. Examples for various groups and organizations in different countries are L'association in France, where media critic Jean Baudrillard is a renowned author. The Anti Advertising Agency works with parody and humour to raise awareness about advertising, and Commercial Alert campaigns for the protection of children, family values, community, environmental integrity and democracy.\n\nMedia literacy organisations aim at training people, especially children, in the workings of the media and advertising in their programmes. In the US, for example, the Media Education Foundation produces and distributes documentary films and other educational resources.\nMediaWatch, a Canadian non-profit women's organization, works to educate consumers about how they can register their concerns with advertisers and regulators. The Canadian 'Media Awareness Network/Réseau éducation médias' offers one of the world's most comprehensive collections of media education and Internet literacy resources. Its member organizations represent the public, non-profit but also private sectors. Although it stresses its independence, it accepts financial support from Bell Canada, CTVglobemedia, Canwest, Telus and S-VOX.\n\nTo counter the increasing criticism of advertising aiming at children media literacy organizations are also initiated and funded by corporations and the advertising business themselves. In the US the Advertising Educational Foundation was created in 1983 supported by ad agencies, advertisers and media companies. It is the \"advertising industry's provider and distributor of educational content to enrich the understanding of advertising and its role in culture, society and the economy,\" sponsored for example by American Airlines, Anheuser-Busch, Campbell Soup, Coca-Cola, Colgate-Palmolive, Disney, Ford, General Foods, General Mills, Gillette, Heinz, Johnson & Johnson, Kellogg's, Kraft, Nabisco, Nestlé, Philip Morris, Quaker Oats, Schering, Sterling, Unilever, Warner Lambert, advertising agencies like Saatchi & Saatchi, and media companies like ABC, CBS, Capital Cities Communications, Cox Enterprises, Forbes, Hearst, Meredith, The New York Times, RCA/NBC, Reader's Digest, Time, and The Washington Post, just to mention a few.\n\nCanadian businesses established Concerned Children's Advertisers in 1990 \"to instill confidence in all relevant publics by actively demonstrating our commitment, concern, responsibility and respect for children.\" Members are CanWest, Corus, CTV, General Mills, Hasbro, Hershey's, Kellogg's, Loblaw, Kraft, Mattel, McDonald's, Nestle, Pepsi, Walt Disney, and Weston, as well as almost 50 private broadcast partners and others. Concerned Children's Advertisers was an example for similar organizations in other countries, like 'Media smart' in the United Kingdom, with offspring in Germany, France, the Netherlands and Sweden. New Zealand has a similar business-funded programme called Willie Munchright. \"While such interventions are claimed to be designed to encourage children to be critical of commercial messages in general, critics of the marketing industry suggest that the motivation is simply to be seen to address a problem created by the industry itself, that is, the negative social impacts to which marketing activity has contributed... By contributing media literacy education resources, the marketing industry is positioning itself as being part of the solution to these problems, thereby seeking to avoid wide restrictions or outright bans on marketing communication, particularly for food products deemed to have little nutritional value directed at children... The need to be seen to be taking positive action primarily to avert potential restrictions on advertising is openly acknowledged by some sectors of the industry itself... Furthermore, Hobbs (1998) suggests that such programs are also in the interest of media organizations that support the interventions to reduce criticism of the potential negative effects of the media themselves.\"\n\nThere has also been movement that began in Paris, France, called \"POP_DOWN PROJECT\" in which they equate street advertising to the annoying pop-up ads on the internet. Their goal is \"symbolically restoring everyone's right to non-exposure\". They achieve their goal by using stickers of the \"Close Window\" buttons used to close pop-up ads.\n\nPublic interest groups suggest that \"access to the mental space targeted by advertisers should be taxed, in that at the present moment that space is being freely taken advantage of by advertisers with no compensation paid to the members of the public who are thus being intruded upon. This kind of tax would be a Pigovian tax in that it would act to reduce what is now increasingly seen as a public nuisance. Efforts to that end are gathering more momentum, with Arkansas and Maine considering bills to implement such a taxation. Florida enacted such a tax in 1987 but was forced to repeal it after six months, as a result of a concerted effort by national commercial interests, which withdrew planned conventions, causing major losses to the tourism industry, and cancelled advertising, causing a loss of 12 million dollars to the broadcast industry alone\".\n\nIn the US, for example, advertising is tax deductible and suggestions for possible limits to the advertising tax deduction are met with fierce opposition from the business sector, not to mention suggestions for a special taxation. In other countries, advertising at least is taxed in the same manner services are taxed and in some advertising is subject to special taxation although on a very low level. In many cases the taxation refers especially to media with advertising (e.g. Austria, Italy, Greece, Netherlands, Turkey, Estonia). Tax on advertising in European countries:\nIn his book, \"When Corporations Rule the World\", US author and globalization critic David Korten even advocates a 50% tax on advertising to counterattack what he calls \"an active propaganda machinery controlled by the world's largest corporations\" which \"constantly reassures us that consumerism is the path to happiness, governmental restraint of market excess is the cause of our distress, and economic globalization is both a historical inevitability and a boon to the human species.\"\n\n"}
{"id": "5498706", "url": "https://en.wikipedia.org/wiki?curid=5498706", "title": "Current density", "text": "Current density\n\nIn electromagnetism, current density is the electric current per unit area of cross section. The current density vector is defined as a vector whose magnitude is the electric current per cross-sectional area at a given point in space, its direction being that of the motion of the charges at this point. In SI units, the electric current density is measured in amperes per square metre.\n\nAssume that \"A\" (SI unit: m) is a small surface centred at a given point \"M\" and orthogonal to the motion of the charges at \"M\". If \"I\" (SI unit: A) is the electric current flowing through \"A\", then electric current density \"J\" at \"M\" is given by the limit:\n\nwith surface \"A\" remaining centred at \"M\" and orthogonal to the motion of the charges during the limit process.\n\nThe current density vector J is the vector whose magnitude is the electric current density, and whose direction is the same as the motion of the charges at \"M\".\n\nAt a given time \"t\", if v is the speed of the charges at \"M\", and \"dA\" is an infinitesimal surface centred at \"M\" and orthogonal to v, then during an amount of time \"dt\", only the charge contained in the volume formed by \"dA\" and will flow through \"dA\". This charge is equal to , where \"ρ\" is the charge density at \"M\", and the electric current at \"M\" is . It follows that the current density vector can be expressed as:\n\nThe surface integral of J over a surface \"S\", followed by an integral over the time duration \"t\" to \"t\", gives the total amount of charge flowing through the surface in that time ():\n\nMore concisely, this is the integral of the flux of J across \"S\" between \"t\" and \"t\".\n\nThe area required to calculate the flux is real or imaginary, flat or curved, either as a cross-sectional area or a surface. For example, for charge carriers passing through an electrical conductor, the area is the cross-section of the conductor, at the section considered.\n\nThe vector area is a combination of the magnitude of the area through which the charge carriers pass, \"A\", and a unit vector normal to the area, formula_4. The relation is formula_5.\n\nIf the current density J passes through the area at an angle \"θ\" to the area normal formula_4, then\n\nwhere ⋅ is the dot product of the unit vectors. That is, the component of current density passing through the surface (i.e. normal to it) is , while the component of current density passing tangential to the area is , but there is \"no\" current density actually passing \"through\" the area in the tangential direction. The \"only\" component of current density passing normal to the area is the cosine component.\n\nCurrent density is important to the design of electrical and electronic systems.\n\nCircuit performance depends strongly upon the designed current level, and the current density then is determined by the dimensions of the conducting elements. For example, as integrated circuits are reduced in size, despite the lower current demanded by smaller devices, there is a trend toward higher current densities to achieve higher device numbers in ever smaller chip areas. See Moore's law.\n\nAt high frequencies, the conducting region in a wire becomes confined near its surface which increases the current density in this region. This is known as the skin effect. \n\nHigh current densities have undesirable consequences. Most electrical conductors have a finite, positive resistance, making them dissipate power in the form of heat. The current density must be kept sufficiently low to prevent the conductor from melting or burning up, the insulating material failing, or the desired electrical properties changing. At high current densities the material forming the interconnections actually moves, a phenomenon called \"electromigration\". In superconductors excessive current density may generate a strong enough magnetic field to cause spontaneous loss of the superconductive property.\n\nThe analysis and observation of current density also is used to probe the physics underlying the nature of solids, including not only metals, but also semiconductors and insulators. An elaborate theoretical formalism has developed to explain many fundamental observations.\n\nThe current density is an important parameter in Ampère's circuital law (one of Maxwell's equations), which relates current density to magnetic field.\n\nIn special relativity theory, charge and current are combined into a 4-vector.\n\nCharge carriers which are free to move constitute a free current density, which are given by expressions such as those in this section.\n\nElectric current is a coarse, average quantity that tells what is happening in an entire wire. At position r at time \"t\", the \"distribution\" of charge flowing is described by the current density:\n\nwhere J(r, \"t\") is the current density vector, v(r, \"t\") is the particles' average drift velocity (SI unit: m∙s), and\n\nis the charge density (SI unit: coulombs per cubic metre), in which \"n\"(r, \"t\") is the number of particles per unit volume (\"number density\") (SI unit: m), \"q\" is the charge of the individual particles with density \"n\" (SI unit: coulombs).\n\nA common approximation to the current density assumes the current simply is proportional to the electric field, as expressed by:\n\nwhere E is the electric field and \"σ\" is the electrical conductivity.\n\nConductivity \"σ\" is the reciprocal (inverse) of electrical resistivity and has the SI units of siemens per metre (S⋅m), and E has the SI units of newtons per coulomb (N⋅C) or, equivalently, volts per metre (V⋅m).\n\nA more fundamental approach to calculation of current density is based upon:\n\nindicating the lag in response by the time dependence of \"σ\", and the non-local nature of response to the field by the spatial dependence of \"σ\", both calculated in principle from an underlying microscopic analysis, for example, in the case of small enough fields, the linear response function for the conductive behaviour in the material. See, for example, Giuliani or Rammer. The integral extends over the entire past history up to the present time.\n\nThe above conductivity and its associated current density reflect the fundamental mechanisms underlying charge transport in the medium, both in time and over distance.\n\nA Fourier transform in space and time then results in:\n\nwhere \"σ\"(k, \"ω\") is now a complex function.\n\nIn many materials, for example, in crystalline materials, the conductivity is a tensor, and the current is not necessarily in the same direction as the applied field. Aside from the material properties themselves, the application of magnetic fields can alter conductive behaviour.\nCurrents arise in materials when there is a non-uniform distribution of charge.\nIn dielectric materials, there is a current density corresponding to the net movement of electric dipole moments per unit volume, i.e. the polarization P:\n\nSimilarly with magnetic materials, circulations of the magnetic dipole moments per unit volume, i.e. the magnetization M, lead to magnetization currents:\n\nTogether, these terms add up to form the bound current density in the material (resultant current due to movements of electric and magnetic dipole moments per unit volume):\n\nThe total current is simply the sum of the free and bound currents:\n\nThere is also a displacement current corresponding to the time-varying electric displacement field D:\n\nwhich is an important term in Ampere's circuital law, one of Maxwell's equations, since absence of this term would not predict electromagnetic waves to propagate, or the time evolution of electric fields in general.\n\nSince charge is conserved, current density must satisfy a continuity equation. Here is a derivation from first principles.\n\nThe net flow out of some volume \"V\" (which can have an arbitrary shape but fixed for the calculation) must equal the net change in charge held inside the volume:\n\nwhere \"ρ\" is the charge density, and \"dA\" is a surface element of the surface \"S\" enclosing the volume \"V\". The surface integral on the left expresses the current \"outflow\" from the volume, and the negatively signed volume integral on the right expresses the \"decrease\" in the total charge inside the volume. From the divergence theorem:\n\nHence:\n\nThis relation is valid for any volume, independent of size or location, which implies that:\n\nand this relation is called the continuity equation.\n\nIn electrical wiring, the maximum current density can vary from 4 A⋅mm for a wire with no air circulation around it, to 6 A⋅mm for a wire in free air. Regulations for building wiring list the maximum allowed current of each size of cable in differing conditions. For compact designs, such as windings of SMPS transformers, the value might be as low as 2 A⋅mm. If the wire is carrying high frequency currents, the skin effect may affect the distribution of the current across the section by concentrating the current on the surface of the conductor. In transformers designed for high frequencies, loss is reduced if Litz wire is used for the windings. This is made of multiple isolated wires in parallel with a diameter twice the skin depth. The isolated strands are twisted together to increase the total skin area and to reduce the resistance due to skin effects.\n\nFor the top and bottom layers of printed circuit boards, the maximum current density can be as high as 35 A⋅mm with a copper thickness of 35 μm. Inner layers cannot dissipate as much heat as outer layers; designers of circuit boards avoid putting high-current traces on inner layers.\n\nIn the semiconductors field, the maximum current densities for different elements are given by the manufacturer. Exceeding those limits raises the following problems:\n\nThe following table gives an idea of the maximum current density for various materials.\n\nEven if manufacturers add some margin to their numbers, it is recommended to, at least, double the calculated section to improve the reliability, especially for high quality electronics. One can also notice the importance to keep electronic devices cool to avoid them to be exposed to electromigration and slow diffusion.\n\nIn biological organisms, ion channels regulate the flow of ions (for example, sodium, calcium, potassium) across the membrane in all cells. Current density is measured in pA⋅pF (picoamperes per picofarad), that is, current divided by capacitance, a de facto measure of membrane area.\n\nIn gas discharge lamps, such as flashlamps, current density plays an important role in the output spectrum produced. Low current densities produce spectral line emission and tend to favour longer wavelengths. High current densities produce continuum emission and tend to favour shorter wavelengths. Low current densities for flash lamps are generally around 10 A⋅mm. High current densities can be more than 40 A⋅mm.\n"}
{"id": "5304481", "url": "https://en.wikipedia.org/wiki?curid=5304481", "title": "Design competition", "text": "Design competition\n\nA design competition is a competition in which an entity solicits design proposals from the public for a specified purpose.\n\nAn architectural design competition solicits architects to submit design proposals for a building, bridge, or other structure. Such competitions may be \"open\", receiving bids internationally, domestically, or regionally. The competition may occur in a single stage, or involve two stages, the first of which eliminates non-viable candidates.\n\nFamous early examples of design competitions were for the Acropolis of Athens in 448 BCE, and the dome of the Florence Cathedral in 1418.\n\nCoin and stamp design contests solicit designs to appear on the face of stamps and usually the obverse of coins. In 1998, the Royal Canadian Mint held the Millennium Coin Design Contest, a competition for the design of 24 quarters, one for each month of 1999 and 2000.\n\nThe design of artistic objects and monuments is a common subject in design competitions. A well-known example is the Vietnam Veterans Memorial in Washington D.C. designed by Maya Lin.\n\nUrban and landscape projects may solicit design proposals in a competition. Among them are projects for urban parks, streetscapes, and rehabilitation of natural areas.\n\nA student design competition is a student competition to introduce students to real-world engineering practices and design.\n"}
{"id": "40129720", "url": "https://en.wikipedia.org/wiki?curid=40129720", "title": "Devex algorithm", "text": "Devex algorithm\n\nIn applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris. It identifies the steepest-edge approximately in its search for the optimal solution.\n"}
{"id": "3731424", "url": "https://en.wikipedia.org/wiki?curid=3731424", "title": "Difference (philosophy)", "text": "Difference (philosophy)\n\nDifference is a key concept of philosophy, denoting the process or set of properties by which one entity is distinguished from another within a relational field or a given conceptual system. In the Western philosophical system, difference is traditionally viewed as being opposed to identity, following the Principles of Leibniz, and in particular, his Law of the identity of indiscernibles. In structuralist and poststructuralist accounts, however, difference is understood to be \"constitutive\" of both meaning and identity. In other words, because identity (particularly, personal identity) is viewed in non-essentialist terms as a construct, and because constructs only produce meaning through the interplay of differences (see below), it is the case that for both structuralism and poststructuralism, identity cannot be said to exist without difference.\n\nGottfried Leibniz's Principle of the identity of indiscernibles states that two things are identical if and only if they share the same and only the same properties. This is a principle which defines identity rather than difference, although it established the tradition in logic and analytical philosophy of conceiving of identity and difference as oppositional.\n\nIn his \"Critique of Pure Reason\", Immanuel Kant argues that it is necessary to distinguish between the thing in itself and its appearance. Even if two objects have completely the same properties, if they are at two different places at the same time, they are numerically different:\nStructural linguistics, and subsequently structuralism proper, are founded on the idea that meaning can only be produced differentially in signifying systems (such as language). This concept first came to prominence in the structuralist writings of Swiss linguist Ferdinand de Saussure and was developed for the analysis of social and mental structures by French anthropologist Claude Lévi-Strauss.\n\nThe former was concerned to question the prevailing view of meaning \"inhering\" in words, or the idea that language is a nomenclature bearing a one-to-one correspondence to the real. Instead, Saussure argues that meaning arises through differentiation of one sign from another, or even of one phoneme from another:\nIn language there are only differences. Even more important: a difference generally implies positive terms between which the difference is set up; but in language there are only differences without positive terms. Whether we take the signified or the signifier, language has neither ideas nor sounds that existed before the linguistic system, but only conceptual and phonic differences that have issued from the system. The idea or phonic substance that a sign contains is of less importance than the other signs that surround it. [...] A linguistic system is a series of differences of sound combined with a series of differences of ideas; but the pairing of a certain number of acoustical signs with as many cuts made from the mass thought engenders a system of values.\nIn his \"Structural Anthropology\", Claude Lévi-Strauss applied this concept to the anthropological study of mental structures, kinship and belief systems, examining the way in which social meaning emerges through a series of structural oppositions between paired/opposed kinship groups, for example, or between basic oppositional categories (such as friend and enemy, life and death, or in a later volume, the raw and the cooked).\n\nThe French philosopher Jacques Derrida both extended and profoundly critiqued structuralist thought on the processes by which meaning is produced through the interplay of difference in language, and in particular, writing. Whereas structuralist linguistics had recognized that meaning is differential, much structuralist thought, such as narratology, had become too focused on identifying and producing a typology of the fixed differential structures and binary oppositions at work in any given system. In his work, Derrida sought to show how the differences on which any signifying system depends are not fixed, but get caught up and entangled with each other. Writing itself becomes the prototype of this process of entanglement, and in \"Of Grammatology\" (1967) and \"\" (in \"Margins of Philosophy\", 1972) Derrida shows how the concept of writing (as the paradoxical absence or de-presencing of the living voice) has been subordinated to the desired \"full presence\" of speech within the Western philosophical tradition. His early thought on the relationship between writing and difference is collected in his book of essays entitled \"Writing and Difference\" (1967).\n\nElsewhere, Derrida coined the term (a deliberate misspelling of ) in order to provide a conceptual hook for his thinking on the meaning processes at work within writing/language. This neologism is a play on the two meanings of the French word : to differ and to defer. Derrida thereby argues that meaning does not arise out of fixed differences between static elements in a structure, but that the meanings produced in language and other signifying systems are always partial, provisional and infinitely deferred along a chain of differing/deferring signifiers. At the same time, the word itself \"performs\" this entanglement and confusion of differential meanings, for it depends on a minimal difference (the substitution of the letter \"a\" for the letter \"e\") which cannot be apprehended in oral speech, since the suffixes \"-ance\" and \"-ence\" have the same pronunciation in French. The \"phonemic\" (non-)difference between and can only be observed in writing, hence producing differential meaning only in a partial, deferred and entangled manner.\n\nIn a similar vein, Gilles Deleuze's \"Difference and Repetition\" (1968) was an attempt to think difference as having an ontological privilege over identity, inverting the traditional relationship between those two concepts and implying that identities are only produced through processes of differentiation.\n\n"}
{"id": "35969423", "url": "https://en.wikipedia.org/wiki?curid=35969423", "title": "Domestic Abuse Restraining Order", "text": "Domestic Abuse Restraining Order\n\nA Domestic Abuse Restraining Order (DARO) is a form of restraining order or order of protection used under the domestic abuse laws of the state of Wisconsin and enforceable nationwide under invocation of the Full Faith and Credit Clause in the Violence Against Women Act (). It is a legal intervention in which one person (the respondent) who is deemed to be hurting, threatening or stalking another person (the petitioner) is ordered to stop—and often cease all direct and indirect contact—with the goal of reducing risk of further threat or harm to the petitioner. The petitioner and respondent will generally be in certain specific relationships such as a spousal or sexual relationship. If the petitioner is in an unwanted stalking relationship with the respondent, however, a closely related form of injunction, a Harassment Restraining Order (HRO) may be more appropriate.\n\nPursuant to Wis. Stat. § 813.12, domestic abuse for the purposes of obtaining either type of restraining order is defined to include intentional infliction of physical pain, physical injury or illness; intentional impairment of physical condition; sexual intercourse under Wis. Stat. § 940.225; sexual contact under Wis. Stat. § 940.225; stalking under Wis. Stat. § 940.32; damage to property under Wis. Stat. § 943.01; or a threat to do any of the above. A petitioner whose has experienced domestic abuse, under this definition, can file for either type of order for free. Pursuant to Wis. Stat. § 813.125, other forms of harassment for the purposes of obtaining a harassment restraining order include striking, shoving, kicking, or other physical abuse; or repeated intimidating acts. Petitioning for a harassment restraining order that does not qualify as domestic abuse requires the payment of a fee.\n\nIn Wisconsin there are similar processes for obtaining a domestic violence or harassment restraining order. The first step is ordinarily for the woman—the Wisconsin Coalition Against Domestic Violence generally refers to petitioners as female as most are women—to file an initial petition with the court. It is free of charge to file a petition for a domestic abuse restraining order. The harassment restraining order sometimes requires a fee but this is waived if the abuse also qualifies as domestic abuse, which it often does. The woman often begins the process under extreme stress and overwhelmed with emotion. She must provide a detailed written statement of the facts supporting the granting of the order. She may request that a temporary restraining order be granted for a period of up to 14 days. The temporary restraining order may be granted ex parte—without the abuser having the opportunity to appear in court. Unlike in some states, in Wisconsin an ex parte order can be granted on the basis of past abuse, with no further evidence of the likelihood of future abuse being needed.\n\nThe woman will also request that a full hearing—at which the abuser will be present—be held to determine whether to grant a permanent restraining order. Wisconsin courts will have jurisdiction to hear the case if the petitioner resides in Wisconsin--even if the respondent does not also reside in Wisconsin.\n\nThe temporary restraining order does not go into effect until it is served to the person being restrained. Serving the restraining order is the responsibility of the petitioner. Service can be carried out by the Sheriff's Department of the county where the harasser lives or works, or by any adult who is not a party named in the case. Judges are assigned to restraining order hearings on a rotating basis, with each judge handling restraining order hearings for one week every few months. At the full hearing, petitioners are seated on one side of the courtroom, and respondents on the other side. The hearing usually lasts about 15–30 minutes. At the hearing, both parties will have an opportunity to testify and present evidence, and the judge will make a decision. The permanent restraining order, if granted, may be in effect for up to four years, and the judge must, if granting it, grant it for as long as the petitioner requests up to four years. The order may also be granted, or extended, up to ten years if there is a substantial risk that the respondent may commit homicide or sexual assault against the petitioner. Respondents are supposed to be held by the bailiff for 15 minutes after the hearing ends to allow petitioners to leave safely, but this rule isn't always followed.\n\nAn HRO or DARO can require the harasser to stop harassing the victim, and/or avoid the victim's residence or temporary place of accommodation. Like a DARO, subsequent to 17 April 2014, an HRO may also require that the harasser avoid all contact with the victim.\n\nWisconsin is considering requiring GPS monitoring for all persons subject to a harassment or domestic abuse restraining order. At the present time, a pilot program will provide funds for testing such a program in some counties before it is implemented statewide. Once implemented, the program would make Wisconsin the only state in the country to order GPS monitoring for those who are under, but who have not violated, a restraining order. The program has not yet been implemented by the Wisconsin Department of Justice (DOJ), however. The DOJ claims that current law does not permit judges to order GPS tracking for those who have not violated a restraining order. The governor's office has pledged to tweak the language in the next budget so the program can go ahead.\n\nLaw enforcement must make an arrest if they have probable cause to believe that the abuser has violated a HRO or DARO. Upon conviction, the penalty is a fine of up to $10,000 and/or a prison term of up to 9 months. Violators may be subject to global positioning system tracking based upon a risk assessment by the department of corrections. The victim may be referred to a domestic violence or sexual assault victim service provider. An exclusion zone will be created which the violator is not permitted to enter under GPS tracking.\n\nThe HRO or DARO will generally be registered in the Protection Order File of the National Crime Information Center. At the state level, a domestic abuse restraining order will automatically trigger a restriction on owning or possessing firearms. For a harassment restraining order, such restrictions are at the discretion of the judge granting the order and are not automatic.\n\nFederal restrictions on firearms may also apply. The NCIC entry will include a \"Brady indicator\" indicating whether the restrained person is prohibited from owning firearms under federal law, with a \"Y\" indicating yes (the restrictions do apply), \"N\" for no, and \"U\" for unknown. Generally the Brady indicator will be set to \"Y\" only if certain relationships exist between the parties, such as a sexual/romantic relationship or a parent/child relationship. If the restraining order does not specify the relationship, an attempt is made to determine the relationship from other available data before setting it to \"U\". The federal Brady indicator restrictions, which are automatic if, and only if, certain conditions apply are distinct from possible state restrictions on HRO or DARO respondents possessing firearms. For an HRO, a state restriction on the respondent possessing firearms will depend on whether the judge, at his or her discretion, feels the respondent is a risk to use a firearm to harm others, but it is not automatic.\n\nThe domestic abuse and harassment restraining orders are among the several types of restraining orders used in Wisconsin. The DARO is similar to the HRO but requires that certain specific relationships exist or did exist between the parties. The child abuse restraining order is also similar but is used where the victim of the abuse is a minor. Unlike the HRO, both the domestic abuse and child abuse restraining orders carry an automatic requirement not to possess firearms under state law. The individual at risk restraining order is a restraining order designed to protect adults with significant impairment in their ability to care for themselves.\n\nThe petitioner will sometimes have a choice as to whether to file a domestic abuse or harassment restraining order. The differences between the two orders have grown less over the years. In the past the penalties were less for violating a harassment restraining order, and a full no contact order could not be issued in a harassment restraining order hearing. A petitioner whose request for a temporary restraining order was denied was formerly entitled to a later injunction hearing only for domestic abuse restraining orders. If the respondent and petitioner shared a residence, the respondent could, in the past, not be ordered to avoid that residence under a harassment restraining order. These differences have been significantly reduced. Harassment and domestic abuse restraining orders now have the same penalties for violation, and a full no contact order can now be made in either case. The respondent can now be ordered to avoid the petitioner's residence in either case. A full hearing will now be granted in either case even if the temporary order is denied provided that, in the case of a harassment restraining order, the harassment also meets the definition of domestic abuse. The significant difference between the two types of orders that remains is the fact that the domestic abuse order carries an automatic firearms ban, while the harassment order does not. Sometimes petitioners who are eligible for both will opt for the harassment order because they don't want to force the respondents to give up their firearms. This can carry significant risks for petitioners, as guns are the most common cause of domestic abuse homicides in Wisconsin. Between 2000 and 2010, there were 213 domestic abuse murders in Wisconsin by guns, more than the 194 murders by knives, other weapons, or other means combined. As such, a petitioner stipulating to a harassment restraining order rather than a domestic abuse order must be questioned to the judge as to whether she entered into the stipulation voluntarily and understands the differences between the two types of orders. The petitioner must be fully informed of the consequences of opting for a harassment restraining order.\n\nOut of state orders, including from any state, the District of Columbia, Puerto Rico, the United States Virgin Islands, a tribal court, or a province or territory of Canada, may be enforced in Wisconsin with similar penalties for violation as if the out of state order was actually a Wisconsin domestic abuse restraining order.\n\nA 2006 Wisconsin study showed that female petitioners were successful about 62 percent of the time in seeking restraining orders at the full hearing where the respondents had the chance to be present. The success rate appeared to be higher if the petitioners were represented by advocates or attorneys.\n\nExperts disagree on whether restraining orders are effective in preventing further harassment. A 2010 analysis published in the \"Journal of the American Academy of Psychiatry and the Law\" reviewed 15 U.S. studies of restraining order effectiveness, and concluded that restraining orders \"can serve a useful role in threat management.\" However, a 2002 analysis of 32 U.S. studies found that restraining orders are violated an average of 40 per cent of the time and are perceived as being \"followed by worse events\" almost 21 per cent of the time, and concluded that \"evidence of [restraining orders'] relative efficacy is lacking,\" and that they may pose some degree of risk. A large America-wide telephone survey conducted in 1998 found that, of stalking victims who obtained a restraining order, more than 68 per cent reported it being violated by their stalker. \n\nThreat management experts are often suspicious of restraining orders, believing they may escalate or enrage stalkers. In his 1997 book \"The Gift Of Fear\", well-known American security specialist Gavin de Becker characterized restraining orders as \"homework assignments police give to women to prove they're really committed to getting away from their pursuers,\" and said they \"clearly serve police and prosecutors,\" but \"they do not always serve victims.\" De Becker also observed that restraining orders are most effective when the emotional involvement is lowest—for example, when used following a brief, unsatisfactory, dating relationship as opposed to with an ex-spouse. In the case of stalking, de Becker advised that restraining orders are most effective if the woman rejects once, and then obtains the restraining order immediately following any further unwanted contact. If she continues to allow contact for an extended period after an initial firm rejection, any eventual restraining order may be less effective.\n"}
{"id": "9905000", "url": "https://en.wikipedia.org/wiki?curid=9905000", "title": "Electronic Language International Festival", "text": "Electronic Language International Festival\n\nThe Festival Internacional de Linguagem Eletrônica (FILE; English: Electronic Language International Festival) is a New media art festival that usually takes place in three different cities of Brazil: São Paulo, Rio de Janeiro and Porto Alegre and it has also participated in other events around the world. It is the biggest art & technology festival in Brazil, and it serves as a lead indicator of the plurality of the work created in the interactive art field not only nationally but also internationally.\n\nFILE is organized by a non-profit group whose purpose is to disseminate and to develop culture, arts, technology and scientific research. Its first edition was in 2000. The 17th edition in 2016 brought together 330 works by international artists working in the field of art and technology. Admission to the FILE exhibition is free.\n\nThe FILE festival is split in different areas of interest, although some overlap and sub-categorization occurs. The main groups of interest are:\n\n\n\n"}
{"id": "29150377", "url": "https://en.wikipedia.org/wiki?curid=29150377", "title": "Empirical theory of perception", "text": "Empirical theory of perception\n\nAn empirical theory of perception is a kind of explanation for how percepts arise. These theories hold that sensory systems incorporate information about the statistical properties of the natural world into their design and relate incoming stimuli to this information, rather than analyzing sensory stimulation into its components or features.\n\nVisual perception is initiated when objects in the world reflect light rays towards the eye. Most empirical theories of visual perception begin with the observation that stimulation of the retina is fundamentally ambiguous. In empirical accounts, the most commonly proposed mechanism for circumventing this ambiguity is \"unconscious inference,\" a term that dates back to Helmholtz.\n\nAccording to Hatfield, Alhazen was the first to propose that higher-level cognitive processes (\"judgments\") could supplement sense perception to lead to veridical perception of distance, suggesting that these \"judgments\" are formally equivalent to syllogisms. Descartes extended and refined this account. Berkeley departed from this tradition, putting forth the new idea that sensory systems, rather than performing logical operations on stimuli to reach veridical conclusions (i.e. these light rays come with certain orientations relative to each other, therefore their source is at a certain distance), make \"associations\", so that for instance if certain co-occurring sensory attributes are usually present when an object is at a given distance an observer would see an object with those attributes as being at that distance. For Helmholtz Berkeleyan associations form the premises for inductive \"judgements,\" in Alhazen's sense of the term. Helmholtz was one of the first thinkers on the subject to augment his reasoning with detailed knowledge of the anatomy of sensory mechanisms.\n\nIn current work Helmholtz's use of the term is construed as referring to some mechanism that augments sense impressions with acquired knowledge or through application of heuristics. In general, contemporary empirical theories of perception seek to describe and/or explain the physiological underpinnings of this \"unconscious inference,\" particularly in terms of how sensory systems acquire information about general statistical features of their environments (see natural scene statistics) and apply this information to sensory data in order to shape perception. A recurring theme in these theories is that stimulus ambiguity is rectified by \"a priori\" knowledge about the natural world.\n\nThe wholly empirical approach to perception, developed by Dale Purves and his colleagues, holds that percepts are determined solely by evolutionary and individual experience with sensory impressions and the objects from which they derive. The success or failure of behavior in response to these sensory impressions tends to increase the prevalence of neural structures that support some ways of interpreting sensory input while decreasing the prevalence of neural structures that support other ways of interpreting sensory input.\n\nOn the wholly empirical account, this strategy determines qualities of perception in all visual domains and sensory modalities. Accumulating evidence suggests that the perception of color, contrast, distance, size, length, line orientation and angles, and motion, as well as pitch and consonance in music, may be determined by empirically derived associations between the sensory patterns humans have always experienced and the relative success of behavior in response to those patterns.\nThe wholly empirical theory of perception departs from many other empirical theories by recognizing the seriousness of the optical inverse problem. To illustrate this problem, imagine that three hoses are used to fill a bucket with water. If how much water each hose has contributed is known, it is straightforward to calculate how much water is in the bucket. These kinds of problems are known as “forward” problems, and scientists like them because they are easy to solve. But if instead, all that is known is the amount of water in the bucket, it is impossible to figure out, on this basis alone, how much water came from each hose: it is impossible to work “backwards” from the bucket to the hoses. This is a simple example of an inverse problem. Solutions to these problems are rarely possible, although they can sometimes be approximated by imposing assumption-based constraints on the “solution space”.\n\nNavigating the world on the basis of sensory stimulation alone represents an inverse problem in the realm of biology. Consider, for example, the case of the distance and line length. When light reflected from a linear object falls on the retina, the object in 3-D space is transformed into a two-dimensional line. Note, however, that a distant line can form the same image on the retina as a shorter but close line. All the eyes receive is an image, which is analogous to the bucket of water. It is impossible to go backward to know the real distance, length, and orientation of the source of the projected line, analogous to the amounts of water that came from each hose. Despite this fact, percipients usually manage to behave effectively in response to sensory stimulation.\n\nThe inverse optics problem presents a quandary for traditional approaches to perception. For example, advocates of feature detection or, in more current terms, neural filtering, propose that the visual system performs logical computations on retinal inputs to determine higher-level aspects of a perceptual scene such as contrast, contour, shape and color percepts. However, given the inverse problem, it is hard to imagine how these computations, if they were actually performed, would be useful, since they would have little or nothing to do with properties of the real world. Empirical approaches to perception take a different tack, arguing that the only way for organisms to successfully overcome the inverse problem is to exploit their long and varied past experience with the real world.\nThe wholly empirical approach holds that this experience is the sole determinant of perceptual qualities. The reason percipients see an object as dark or light, the argument goes, is that in both our own past and the past of the species it paid off to see it that particular way. Returning to the bucket analogy, imagine that each of the three hoses pumps out water of a different color: one pumps out black water, one pumps out gray water, and one pumps out clear water. All one sees is the water in the bucket, which can be clear, gray, black, or any shade in between. As expected, it is impossible to perform some calculation on the color of the water in the bucket to find out how much water came out of each hose. Now imagine that it is your job to bet on how much water came out of the gray hose. The output ratios of the hoses are not random, but co-vary in all kinds of complicated ways based on the time of day, how long it takes to fill up the bucket, etc. At first your behavior in response to the color of the bucket might not be so good, but over time this would gradually improve as different shades and behaviors in response became associated by trial and error. The key is that in order to improve you have to know whether or not your behaviors worked by interacting with the world.\n\nOn the wholly empirical view, the retinal image is like the bucket and what you see is determined by past behaviors that have succeeded. Although this example is simplistic, it illustrates the general strategy that visual system uses to work around the inverse problem. Over millions of years individuals whose visual systems more successfully linked sensory stimulation with successful behavior won out. In this view the inverse problem is not actually solved—which would be analogous to computing the outputs of all three hoses—the result might be close enough to behave appropriately in response to stimuli.\n\nColor vision is dependent on activation of three cone cell types in the human retina, each of which is primarily responsive to a different spectrum of light frequencies. While these retinal mechanisms enable subsequent color processing, their properties alone cannot account for the full range of color perception phenomena. In part this is due to the fact that illuminance (the amount of light shining on an object), reflectance (the amount of light an object is predisposed to reflect), and transmittance (the extent to which the light medium distorts the light as it travels) are conflated in the retinal image. This is problematic because, if color vision is to be useful, it must somehow guide behavior in line with these properties. Even so, the visual system only has access to retinal input, which does not distinguish the relative contributions of each of these factors to the final light spectra that stimulate the retina.\n\nAccording to the empirical framework, the visual system solves this problem by drawing on species and individual experience with retinal images that have signified different combinations of illuminance, reflectance, and transmittance in the past. Only those associations that led to appropriate behavior were retained through evolution and development, leading to a repertoire of neural associations and predispositions that ground color perception in the world.\n\nOne way to test this idea is to see whether the frequency of co-occurrence of light spectra predicts simultaneous color contrast effects (see side image, Fig. 1). Long and Purves showed that by sampling thousands of natural images, analysis of associations between target colors and the colors of their surrounds could explain perceptual effects like those seen on the right. Rather than explaining the diverging color percepts as unfortunate byproducts of a normally veridical color perception mechanism, according to this work the different colors humans see are simply the byproducts of our species and individual exposure to the distribution of color spectra in the world.\n\n’’Brightness’’ refers to a subjective sense that the object considered is emitting light. Whereas the perceptual correlates of color are the frequencies of light that compose the light spectrum, the perceptual correlate of brightness is luminance, or the intensity of light emitted by an object. While it may seem obvious that the sensation of brightness is straightforwardly related to the amount or intensity of light coming to the eyes, perception researchers have long known that brightness is not caused solely by the luminance incident on the retina. A common example is simultaneous brightness contrast (shown to the right), in which the two identical target diamonds appear differently bright.\n\nIn the empirical account, the same general framework used to rationalize simultaneous color contrast applies to simultaneous brightness contrast. Because the three factors that determine luminance emissions—transmittance, reflectance, and illuminance—are blended in the retinal image of the object, operations on the luminance returns as such cannot in principle yield percepts that are good guides to behavior. The visual system solves this problem by associating luminance values and their given contexts with the success or failure of ensuing behavior, leading to percepts that often (but only incidentally) reflect properties of objects rather than their associated images.\n\nThe image on the right (Fig. 2.) strongly supports this view of how brightness perception works. Although other frameworks have either no explanation for this effect or explanations that are highly inconsistent with their explanations for similar effects, the empirical framework makes the case that the perceived brightness differences are due to empirical associations between the targets and their respective contexts. In this case, because the “lighter” targets would typically have been shadowed, humans perceive them in a way that is consistent with their having a higher reflectance despite their presumably low levels of illuminance. Note that this approach is considerably different from computational “context”-driven approaches, since in this case the target/context relationships are contingent and world-based, and therefore cannot be generalized to other cases in any meaningful way.\n\nPerception of line length is confounded by another optical inverse problem: the further away a line in the world, the smaller the projected line will be on the retina. Different orientations of a line relative to the observer may obscure true line length as well. It is well known that straight lines are erroneously reported as longer or shorter as a function of their angular orientation, as is evident in Fig. 3. While no generally accepted explanations of this phenomenon have been offered previously, the empirical approach has had some success in explaining the effect as a function of the distribution of lines in natural scenes. \nHowe and Purves (2002) analyzed natural scene photographs to find projected lines that corresponded to straight line sources. They found that the ratios of the actual length of the lines to the projected lines on the retina, when classified by their respective orientations on the retina, almost perfectly matched subjective estimation of line length as a function of angle relative to the observer. For example, horizontal lines on the retinal image would typically have turned out to issue from relatively short physical sources, while lines at about 60 degrees relative to the observer would typically have signified longer physical sources, which explains why individuals tend to see the 60° line in Fig. 3 as longer than the 0° (horizontal) line. While there is no way for the visual system to know this \"a priori\", the fact that it seems to take this knowledge for granted in its construction of length estimation percepts strongly supports the wholly empirical view of perception.\n\nPerception of motion is also confounded by an inverse problem: movement in three-dimensional space does not map perfectly onto movement on the retinal plane. A distant object moving at a given speed will translate more slowly on the retina than a nearby object moving at the same speed, and as mentioned previously size, distance and orientation are also ambiguous given only the retinal image. As with other aspects of perception, empirical theorists propose that this problem is solved by trial-and-error experience with moving stimuli, their associated retinal images and the consequences of behavior. \nOne way to test this hypothesis is by seeing whether it can explain the flash lag illusion, a visual effect in which a flash superimposed on a moving bar is falsely seen to lag behind the bar. The task for empirical theorists is to explain why individuals perceive the flash in this way, and further, why the perceived lag increases with the speed of the moving bar. To investigate this question, Wojtach et al. (2008) simulated a three-dimensional environment full of moving virtual particles. They modeled the transformation from three dimensions to the two-dimensional image plane and tallied up the frequency of occurrence of particle speeds, particle distances, image speeds, and image distances (image meaning the path projected across the computer-modeled “retina”). The probability distributions they obtained in this way predicted the magnitude of the bar-flash disparity quite well. The authors concluded that the flash-lag effect was a signature of the way brains evolve and develop to behave appropriately in response to moving retinal images.\n"}
{"id": "38637693", "url": "https://en.wikipedia.org/wiki?curid=38637693", "title": "Energy hierarchy", "text": "Energy hierarchy\n\nThe Energy Hierarchy is a classification of energy options, prioritised to assist progress towards a more sustainable energy system. It is a similar approach to the waste hierarchy for minimising resource depletion, and adopts a parallel sequence.\n\nThe highest priorities cover the prevention of unnecessary energy usage both through eliminating waste and improving energy efficiency. The sustainable production of energy resources is the next priority. Depletive and waste-producing energy generation options are the lowest priority.\n\nFor an energy system to be sustainable: the resources applied to producing the energy must be capable of lasting indefinitely; energy conversion should produce no harmful by-products, including net emissions, nor wastes which cannot be fully recycled; and it must be capable of meeting reasonable energy demands.\n\nThe top priority under the Energy Hierarchy is energy conservation or the prevention of unnecessary use of energy. This category includes eliminating waste by turning off unneeded lights and appliances and by avoiding unnecessary journeys. Heat loss from buildings is a major source of energy wastage, so improvements to building insulation and air-tightness can make a significant contribution to energy conservation.\n\nMany countries have agencies to encourage energy saving.\n\nThe second priority under the energy hierarchy is to ensure that energy that is used is produced and consumed efficiently. Energy efficiency has two main aspects.\n\nEnergy efficiency is the ratio of the productive output of a device to the energy it consumes.\n\nEnergy efficiency was a lower priority when energy was cheap and awareness of its environmental impact was low. In 1975 the average fuel economy of a car in the US was under 15 miles per gallon Incandescent light bulbs, which were the most common type until the late 20th century, waste 90% of their energy as heat, with only 10% converted to useful light.\n\nMore recently, energy efficiency has become a priority. The last reported average fuel efficiency of US cars had almost doubled from the 1975 level; LED lighting is now being promoted which are between five and ten times more efficient than incandescents. Many household appliances are now required to display labels to show their energy efficiency.\n\nLosses are incurred when energy is harvested from the natural resource from which it is derived, such as fossil fuels, radioactive materials, solar radiation or other sources. Most electricity production is in thermal power stations, where much of the source energy is lost as heat. The average efficiency of world electricity production in 2009 was c.37%.\n\nA priority in the Energy Hierarchy is to improve the efficiency of energy conversion, whether in traditional power stations or by improving the performance ratio of Photovoltaic power stations and other energy sources.\n\nOverall efficiency and sustainability can also be improved by capacity- or fuel-switching from less efficient, less sustainable resources to better ones; but this is mainly covered under the fourth level of the hierarchy.\n\nRenewable energy describes naturally occurring, theoretically inexhaustible sources of energy. These sources are treated as being inexhaustible, or naturally replenished, and fall into two classes.\n\nThe first class of renewables derive from climatic or elemental sources, such as sunlight, wind, waves, tides or rainfall (hydropower). Geothermal energy from the heat of the earth's core also falls in this category.\n\nThese are treated as being inexhaustible because most derive ultimately from energy emanating from the sun, which has an estimated life of 6.5 billion years.\n\nThe other main class of renewables, bioenergy, derives from biomass, where the relatively short growing cycle means that usage is replenished by new growth. Bioenergy is usually converted by combustion, and therefore gives rise to carbon emissions. It is treated as carbon neutral overall, because an equivalent amount of carbon dioxide will have been extracted from the atmosphere during the growing cycle.\n\nBioenergy sources can be solid, such as wood and energy crops; liquid, such as biofuels; or gaseous, such as biomethane from anaerobic digestion.\n\nThe next priority in the hierarchy covers energy sources that are not entirely sustainable, but have a low environmental impact. These include the use of fossil fuels with carbon capture and storage.\n\nNuclear energy is sometimes treated as a low impact source, because it has low carbon emissions.\n\nThe lowest priority under the energy hierarchy is energy production using unsustainables sources, such as unabated fossil fuels. Some also place nuclear energy in this category, rather than the one above, because of the required management/storage of highly hazardous radioactive waste over extremely long (hundreds of thousands of years or more) timeframes and depletion of uranium resources.\n\nThere is a consensus that the share of such energy sources must decline.\n\nWithin this tier, there are possibilities for limiting adverse impacts by switching from the most damaging fuel sources, such as coal, to less emissive sources, such as gas.\n\nMany suggest that when such high impact energy usage has been minimised, the effects of any unavoidable residual usage should be counterbalanced by emissions offsetting.\n\nThe Energy Hierarchy was first proposed in 2005 by Philip Wolfe, when he was Director General of the Renewable Energy Association. This first version had three levels; energy efficiency, renewables and traditional energy production. It was endorsed and adopted in 2006 by a consortium of institutions, associations and other bodies in the Sustainable Energy Manifesto. Subsequently, the concept has been adopted and refined by others in the energy industry and in government.\n\n"}
{"id": "11064", "url": "https://en.wikipedia.org/wiki?curid=11064", "title": "Faith", "text": "Faith\n\nIn the context of religion, one can define faith as confidence or trust in a particular system of religious belief,\nwithin which faith may equate to confidence based on some perceived degree of warrant,\nin contrast to a definition of faith as being belief without evidence.\n\nThe English word \"faith\" is thought to date from 1200–1250, from the Middle English \"feith\", via Anglo-French \"fed\", Old French \"feid\", \"feit\" from Latin \"fidem\", accusative of \"fidēs\" (trust), akin to \"fīdere\" (to trust).\n\nJames W. Fowler (1940–2015) proposes a series of stages of faith-development (or spiritual development) across the human life-span. His stages relate closely to the work of Piaget, Erikson, and Kohlberg regarding aspects of psychological development in children and adults. Fowler defines faith as an activity of trusting, committing, and relating to the world based on a set of assumptions of how one is related to others and the world.\n\n\nNo hard-and-fast rule requires individuals pursuing faith to go through all six stages. There is a high probability for individuals to be content and fixed in a particular stage for a lifetime; stages from 2-5 are such stages. Stage 6 is the summit of faith development. This state is often considered as \"not fully\" attainable.\n\nIn the Bahá'í Faith, faith is meant, first, conscious knowledge, and second, the practice of good deeds, ultimately the acceptance of the divine authority of the Manifestations of God. In the religion's view, faith and knowledge are both required for spiritual growth. Faith involves more than outward obedience to this authority, but also must be based on a deep personal understanding of religious teachings.\n\nFaith in Buddhism (', ') refers to a serene commitment in the practice of the Buddha's teaching and trust in enlightened or highly developed beings, such as Buddhas or \"bodhisattvas\" (those aiming to become a Buddha). Buddhists usually recognize multiple objects of faith, but many are especially devoted to one particular object of faith, such as one particular Buddha.\n\nIn early Buddhism, faith was focused on the Triple Gem, that is, Gautama Buddha, his teaching (the Dhamma), and the community of spiritually developed followers, or the monastic community seeking enlightenment (the Sangha). Although offerings to the monastic community were valued highest, early Buddhism did not morally condemn peaceful offerings to deities. A faithful devotee was called \"upāsaka\" or \"upāsika\", for which no formal declaration was required. In early Buddhism, personal verification was valued highest in attaining the truth, and sacred scriptures, reason or faith in a teacher were considered less valuable sources of authority. As important as faith was, it was a mere initial step to the path to wisdom and enlightenment, and was obsolete or redefined at the final stage of that path.\n\nWhile faith in Buddhism does not imply \"blind faith\", Buddhist practice nevertheless requires a degree of trust, primarily in the spiritual attainment of Gautama Buddha. Faith in Buddhism centers on the understanding that the Buddha is an Awakened being, on his superior role as teacher, in the truth of his Dharma (spiritual teachings), and in his Sangha (community of spiritually developed followers). Faith in Buddhism can be summarised as faith in the Three Jewels: the Buddha, Dharma and Sangha. It is intended to lead to the goal of enlightenment, or bodhi, and Nirvana. Volitionally, faith implies a resolute and courageous act of will. It combines the steadfast resolution that one will do a thing with the self-confidence that one can do it.\n\nIn the later stratum of Buddhist history, especially Mahāyāna Buddhism, faith was given a much more important role. The concept of the Buddha Nature was developed, as devotion to Buddhas and \"bodhisattvas\" residing in Pure Lands became commonplace. With the arising of the cult of the Lotus Sūtra, faith gained a central role in Buddhist practice, which was further amplified with the development of devotion to the Amitabha Buddha in Pure Land Buddhism. In the Japanese form of Pure Land Buddhism, under the teachers Hōnen and Shinran, only entrusting faith toward the Amitabha Buddha was believed to be a fruitful form of practice, as the practice of celibacy, morality and other Buddhist disciplines were dismissed as no longer effective in this day and age, or contradicting the virtue of faith. Faith was defined as a state similar to enlightenment, with a sense of self-negation and humility.\n\nThus, the role of faith increased throughout Buddhist history. However, from the nineteenth century onward, Buddhist modernism in countries like Sri Lanka and Japan, and also in the West, has downplayed and criticized the role of faith in Buddhism. Faith in Buddhism still has a role in modern Asia or the West, but is understood and defined differently from traditional interpretations. Within the Dalit Buddhist Movement communities, taking refuge is defined not only as a religious, but also a political choice.\n\nThe word translated as \"faith\" in the New Testament is the Greek word \"πίστις\" (\"pístis\") which can also be translated \"belief\", \"faithfulness\", and \"trust\". There are various views in Christianity regarding the nature of faith. Some see faith as being persuaded or convinced that something is true. In this view, a person believes something when they are presented with adequate evidence that it is true. Theologian Thomas Aquinas did not hold that faith is mere opinion: on the contrary, he held that it is a mean (understood in the Platonic sense) between excessive reliance on science (i.e. demonstration) and excessive reliance on opinion.\n\nThen there are numerous views regarding the results of faith. Some believe that true faith results in good works, while others believe that while faith in Jesus brings eternal life, it does not necessarily result in good works.\n\nRegardless of which approach to faith a Christian takes, all agree that the Christian faith is aligned with the ideals and the example of the life of Jesus. The Christian sees the mystery of God and his grace and seeks to know and become obedient to God. To a Christian, faith is not static but causes one to learn more of God and to grow; Christian faith has its origin in God.\n\nThe definition of faith given by the apostle Paul at Hebrews 11:1 carries particular weight with Christians that respect the Bible as the source of divine truth. There the apostle writes:\n\n\"Now faith is the substance of things hoped for, the evidence of things not seen.\" — King James Version\n\n\"Now faith is the assurance that what we hope for will come about and the certainty that what we cannot see exists.\" — International Standard Version\n\n“The naive or inexperienced person[is easily misled and believes every word he hears, but the prudent man is discreet and astute.” (Proverbs 14:15, Amplified Bible) The Christian apostle Paul wrote: “Test everything that is said to be sure it is true, and if it is, then accept it.” (1 Thessalonians 5:21, Living Bible)\n\nIn Christianity, faith causes change as it seeks a greater understanding of God. Faith is not only fideism or simple obedience to a set of rules or statements. Before Christians have faith, they must understand in whom and in what they have faith. Without understanding, there cannot be true faith, and that understanding is built on the foundation of the community of believers, the scriptures and traditions and on the personal experiences of the believer. In English translations of the New Testament, the word \"faith\" generally corresponds to the Greek noun πίστις (\"pistis\") or to the Greek verb πιστεύω (\"pisteuo\"), meaning \"to trust, to have confidence, faithfulness, to be reliable, to assure\".\n\nIn contrast to noted atheist Richard Dawkins' view of faith as \"blind trust, in the absence of evidence, even in the teeth of evidence\", Alister McGrath quotes the Oxford Anglican theologian W. H. Griffith-Thomas (1861–1924), who states that faith is \"not blind, but intelligent\" and that it \"commences with the conviction of the mind based on adequate evidence...\", which McGrath sees as \"a good and reliable definition, synthesizing the core elements of the characteristic Christian understanding of faith\".\n\nAmerican biblical scholar Archibald Thomas Robertson stated that the Greek word \"pistis\" used for faith in the New Testament (over two hundred forty times), and rendered \"assurance\" in Acts 17:31 (KJV), is \"an old verb meaning \"to furnish\", used regularly by Demosthenes for bringing forward evidence.\" Tom Price (Oxford Centre for Christian Apologetics) affirms that when the New Testament talks about faith positively it only uses words derived from the Greek root [pistis] which means \"to be persuaded\".\n\nBritish Christian apologist John Lennox argues that \"faith conceived as belief that lacks warrant is very different from faith conceived as belief that has warrant\". He states that \"the use of the adjective 'blind' to describe 'faith' indicates that faith is not necessarily, or always, or indeed normally, blind\". \"The validity, or warrant, of faith or belief depends on the strength of the evidence on which the belief is based.\" \"We all know how to distinguish between blind faith and evidence-based faith. We are well aware that faith is only justified if there is evidence to back it up.\" \"Evidence-based faith is the normal concept on which we base our everyday lives.\"\n\nPeter S Williams holds that \"the classic Christian tradition has always valued rationality, and does not hold that faith involves the complete abandonment of reason while believing in the teeth of evidence.\" Quoting Moreland, faith is defined as \"a trust in and commitment to what we have reason to believe is true.\"\n\nRegarding doubting Thomas in John 20:24-31, Williams points out that \"Thomas wasn't asked to believe without evidence\". He was asked to believe on the basis of the other disciples' testimony. Thomas initially lacked the first-hand experience of the evidence that had convinced them... Moreover, the reason John gives for recounting these events is that what he saw is evidence... Jesus did many other miraculous signs in the presence of his disciples...But these are written that you may believe that Jesus is the Christ, the son of God, and that believing ye might have life in his name. John 20:30,31.\n\nConcerning doubting Thomas, Michael R. Allen wrote, \"Thomas's definition of faith implies adherence to conceptual propositions for the sake of personal knowledge, knowledge of and about a person \"qua\" person\".\n\nKenneth Boa and Robert M. Bowman Jr. describe a classic understanding of faith that is referred to as \"evidentialism\", and which is part of a larger epistemological tradition called \"classical foundationalism\", which is accompanied by \"deontologism\", which holds that humans have an obligation to regulate their beliefs in accordance with evidentialist structures.\n\nThey show how this can go too far, and Alvin Plantinga deals with it. While Plantinga upholds that faith may be the result of evidence testifying to the reliability of the source (of the truth claims), yet he sees having faith as being the result of hearing the truth of the gospel with the internal persuasion by the Holy Spirit moving and enabling him to believe. \"Christian belief is produced in the believer by the internal instigation of the Holy Spirit, endorsing the teachings of Scripture, which is itself divinely inspired by the Holy Spirit. The result of the work of the Holy Spirit is faith.\"\n\nThe four-part \"Catechism of the Catholic Church\" (CCC) gives Part One to \"The Profession of Faith\". This section describes the content of faith. It elaborates and expands particularly upon the Apostles' Creed. CCC 144 initiates a section on the \"Obedience of Faith\".\n\nIn the theology of Pope John Paul II, faith is understood in personal terms as a trusting commitment of person to person and thus involves Christian commitment to the divine person of Jesus Christ.\n\nSome alternative, yet impactful, ideas regarding the nature of faith were presented in a collection of sermons now presented as \n\n\nAhimsa, also referred to as nonviolence, is the fundamental tenet of Hinduism which advocates harmonious and peaceful co-existence and evolutionary growth in grace and wisdom for all humankind unconditionally.\n\nIn Hinduism, most of the Vedic prayers begins with the chants of Om. Om is the Sanskrit symbol that amazingly resonates the peacefulness ensconced within one's higher self. Om is considered to have a profound effect on the body and mind of the one who chants and also creates a calmness, serenity, healing, strength of its own to prevail within and also in the surrounding environment.\n\nIn Islam, a believer's faith in the metaphysical aspects of Islam is called \"Iman\" (), which is complete submission to the will of God, not unquestionable or blind belief. A man must build his faith on well-grounded convictions beyond any reasonable doubt and above uncertainty. According to the Quran, Iman must be accompanied by righteous deeds and the two together are necessary for entry into Paradise. In the Hadith of Gabriel, \"Iman\" in addition to \"Islam\" and \"Ihsan\" form the three dimensions of the Islamic religion.\n\nMuhammad referred to the six articles of faith in the Hadith of Gabriel: \"Iman is that you believe in God and His Angels and His Books and His Messengers and the Hereafter and the good and evil fate [ordained by your God].\" The first five are mentioned together in the Qur'an The Quran states that faith can grow with remembrance of God. The Qur'an also states that nothing in this world should be dearer to a true believer than faith.\n\nFaith itself is not a religious concept in Judaism. The \"faith in God\" is mentioned in the Book of Genesis Chapter 15 verse 6 and in the Book of Exodus Chapter 4 verse 31 and in the Book of Isaiah, Chapter 43 verse 10, in the 24 books of the Jewish Bible. The word translated as \"faith\" here is the Hebrew word אָמַן which can also be translated \"believe\", \"reliable\", and \"trustworthy\". In the Book of Isaiah, Chapter 43 verse 10, the commandment to know God is followed by the commandments to believe and to understand, thus denoting descending importance.\n\nHowever, Judaism does recognize the positive value of \"Emunah\" (generally translated as faith, trust in God) and the negative status of the \"Apikorus\" (heretic), but faith is not as stressed or as central as it is in other religions, especially compared with Christianity and Islam. It could be a necessary means for being a practicing religious Jew, but the emphasis is placed on true knowledge, true prophecy and practice rather than on faith itself. Very rarely does it relate to any teaching that must be believed. Judaism does not require one to explicitly identify God (a key tenet of Christian faith, which is called Avodah Zarah in Judaism, a minor form of idol worship, a big sin and strictly forbidden to Jews). Rather, in Judaism, one is to honour a (personal) idea of God, supported by the many principles quoted in the Talmud to define Judaism, mostly by what it is not. Thus there is no established formulation of Jewish principles of faith which are mandatory for all (observant) Jews.\n\nIn the Jewish scriptures trust in God – \"Emunah\" – refers to how God acts toward his people and how they are to respond to him; it is rooted in the everlasting covenant established in the Torah, notably Deuteronomy 7:9:\n\nThe specific tenets that compose required belief and their application to the times have been disputed throughout Jewish history. Today many, but not all, Orthodox Jews have accepted Maimonides' Thirteen Principles of Belief.\n\nA traditional example of \"Emunah\" as seen in the Jewish annals is found in the person of Abraham. On a number of occasions, Abraham both accepts statements from God that seem impossible and offers obedient actions in response to direction from God to do things that seem implausible (see Genesis 12-15).\n\"The Talmud describes how a thief also believes in G‑d: On the brink of his forced entry, as he is about to risk his life—and the life of his victim—he cries out with all sincerity, 'G‑d help me!' The thief has faith that there is a G‑d who hears his cries, yet it escapes him that this G‑d may be able to provide for him without requiring that he abrogate G‑d’s will by stealing from others. For \"emunah\" to affect him in this way he needs study and contemplation.\"\n\nFaith itself is not a religious concept in Sikhism. However, the five Sikh symbols, known as Kakaars or Five Ks (in Punjabi known as pañj kakkē or pañj kakār), are sometimes referred to as the \"Five articles of Faith\". The articles include \"kēs\" (uncut hair), \"kaṅghā\" (small wooden comb), \"kaṛā\" (circular steel or iron bracelet), \"kirpān\" (sword/dagger), and \"kacchera\" (special undergarment). Baptised Sikhs are bound to wear those five articles of faith, at all times, to save them from bad company and keep them close to God.\n\nThere is a wide spectrum of opinion with respect to the epistemological validity of faith - that is, whether it is a reliable way to acquire true beliefs.\n\nFideism is an epistemological theory which maintains that faith is independent of reason, or that reason and faith are hostile to each other and faith is superior at arriving at particular truths (see natural theology). Fideism is not a synonym for religious belief, but describes a particular philosophical proposition in regard to the relationship between faith's appropriate jurisdiction at arriving at truths, contrasted against reason. It states that faith is needed to determine some philosophical and religious truths, and it questions the ability of reason to arrive at all truth. The word and concept had its origin in the mid- to late-19th century by way of Catholic thought, in a movement called Traditionalism. The Roman Catholic Magisterium has, however, repeatedly condemned fideism.\n\nReligious epistemologists have formulated and defended reasons for the rationality of accepting belief in God without the support of an argument. Some religious epistemologists hold that belief in God is more analogous to belief in a person than belief in a scientific hypothesis. Human relations demand trust and commitment. If belief in God is more like belief in other persons, then the trust that is appropriate to persons will be appropriate to God. American psychologist and philosopher William James offers a similar argument in his lecture \"The Will to Believe.\" Foundationalism is a view about the structure of justification or knowledge. Foundationalism holds that all knowledge and justified belief are ultimately based upon what are called properly basic beliefs. This position is intended to resolve the infinite regress problem in epistemology. According to foundationalism, a belief is epistemically justified only if it is justified by properly basic beliefs. One of the significant developments in foundationalism is the rise of reformed epistemology.\n\nReformed epistemology is a view about the epistemology of religious belief, which holds that belief in God can be properly basic. Analytic philosophers Alvin Plantinga and Nicholas Wolterstorff develop this view. Plantinga holds that an individual may rationally believe in God even though the individual does not possess sufficient evidence to convince an agnostic. One difference between reformed epistemology and fideism is that the former requires defence against known objections, whereas the latter might dismiss such objections as irrelevant. Plantinga has developed reformed epistemology in \"Warranted Christian Belief\" as a form of externalism that holds that the justification conferring factors for a belief may include external factors. Some theistic philosophers have defended theism by granting evidentialism but supporting theism through deductive arguments whose premises are considered justifiable. Some of these arguments are probabilistic, either in the sense of having weight but being inconclusive, or in the sense of having a mathematical probability assigned to them. Notable in this regard are the cumulative arguments presented by British philosopher Basil Mitchell and analytic philosopher Richard Swinburne, whose arguments are based on Bayesian probability. In a notable exposition of his arguments, Swinburne appeals to an inference for the best explanation.\n\nProfessor of Mathematics and philosopher of science at University of Oxford John Lennox has stated, \"Faith is not a leap in the dark; it’s the exact opposite. It’s a commitment based on evidence… It is irrational to reduce all faith to blind faith and then subject it to ridicule. That provides a very anti-intellectual and convenient way of avoiding intelligent discussion.” He criticises Richard Dawkins as a famous proponent of asserting that faith equates to holding a belief without evidence, thus that it is possible to hold belief without evidence, for failing to provide evidence for this assertion.\n\nBertrand Russell wrote:\n\nEvolutionary biologist Richard Dawkins criticizes all faith by generalizing from specific faith in propositions that conflict directly with scientific evidence. He describes faith as belief without evidence; a process of active non-thinking. He states that it is a practice that only degrades our understanding of the natural world by allowing anyone to make a claim about nature that is based solely on their personal thoughts, and possibly distorted perceptions, that does not require testing against nature, has no ability to make reliable and consistent predictions, and is not subject to peer review.\n\nPhilosophy professor Peter Boghossian argues that reason and evidence are the only way to determine which \"claims about the world are likely true\". Different religious traditions make different religious claims, and Boghossian asserts that faith alone cannot resolve conflicts between these without evidence. He gives as an example of the belief held by that Muslims that Muhammad (who died in the year 632) was the last prophet, and the contradictory belief held by Mormons that Joseph Smith (born in 1805) was a prophet. Boghossian asserts that faith has no \"built-in corrective mechanism\". For factual claims, he gives the example of the belief that the Earth is 4,000 years old. With only faith and no reason or evidence, he argues, there is no way to correct this claim if it is inaccurate. Boghossian advocates thinking of faith either as \"belief without evidence\" or \"pretending to know things you don't know\".\n\n\n\n\n\n\n"}
{"id": "292136", "url": "https://en.wikipedia.org/wiki?curid=292136", "title": "Feud", "text": "Feud\n\nA feud , referred to in more extreme cases as a blood feud, vendetta, faida, beef, clan war, gang war, or private war, is a long-running argument or fight, often between social groups of people, especially families or clans. Feuds begin because one party (correctly or incorrectly) perceives itself to have been attacked, insulted or wronged by another. Intense feelings of resentment trigger the initial retribution, which causes the other party to feel equally aggrieved and vengeful. The dispute is subsequently fuelled by a long-running cycle of retaliatory violence. This continual cycle of provocation and retaliation makes it extremely difficult to end the feud peacefully. Feuds frequently involve the original parties' family members or associates, can last for generations, and may result in extreme acts of violence. They can be interpreted as an extreme outgrowth of social relations based in family honor.\n\nUntil the early modern period, feuds were considered legitimate legal instruments and were regulated to some degree. For example, Serb culture calls this \"krvna osveta\", meaning \"blood revenge\", which had unspoken but highly valued rules. In tribal societies, the blood feud, coupled with the practice of blood wealth, functioned as an effective form of social control for limiting and ending conflicts between individuals and groups who are related by kinship, as described by anthropologist Max Gluckman in his article \"The Peace in the Feud\" in 1955.\n\nA blood feud is a feud with a cycle of retaliatory violence, with the relatives of someone who has been killed or otherwise wronged or dishonored seeking vengeance by killing or otherwise physically punishing the culprits or their relatives. In the English-speaking world, the Italian word vendetta is used to mean a blood feud, but in reality it means (personal) \"vengeance\" or \"revenge\", originating from the Latin \"vindicta\" (vengeance), while the word \"faida\" would be more appropriate for a blood feud. In the English-speaking world, \"vendetta\" is sometimes extended to mean any other long-standing feud, not necessarily involving bloodshed. Sometimes, it is not mutual, but rather refers to a prolonged series of hostile acts waged by one person against another without reciprocation.\n\nBlood feuds were common in societies with a weak rule of law (or where the state does not consider itself responsible for mediating this kind of dispute), where family and kinship ties are the main source of authority. An entire family is considered responsible for the actions of any of its members. Sometimes two separate branches of the same family have even come to blows, or worse, over some dispute.\nThe practice has mostly disappeared with more centralized societies where law enforcement and criminal law take responsibility for punishing lawbreakers.\n\nIn Homeric ancient Greece, the practice of personal vengeance against wrongdoers was considered natural and customary: \"Embedded in the Greek morality of retaliation is the right of vengeance... Feud is a war, just as war is an indefinite series of revenges; and such acts of vengeance are sanctioned by the gods\".\n\nIn the ancient Hebraic context, it was considered the duty of the individual and family to avenge evil on behalf of God. The executor of the law of blood-revenge who personally put the initial killer to death was given a special designation: \"go'el haddam\", the blood-avenger or blood-redeemer (Book of Numbers 35: 19, etc.). Six Cities of Refuge were established to provide protection and due process for any unintentional manslayers. The avenger was forbidden from harming the unintentional killer if the killer took refuge in one of these cities. As the \"Oxford Companion to the Bible\" states: \"Since life was viewed as sacred (Genesis 9.6), no amount of blood money could be given as recompense for the loss of the life of an innocent person; it had to be 'life for life' (Exodus 21.23; Deuteronomy 19.21)\".\n\nAccording to historian Marc Bloch:\n\nRita of Cascia, a popular 15th-century Italian saint, was canonized by the Catholic Church due mainly to her great effort to end a feud in which her family was involved and which claimed the life of her husband.\n\nThe blood feud has certain similarities to the ritualized warfare found in many pre-industrial tribes. Thus, for instance, more than a third of Ya̧nomamö males, on average, died from warfare. The accounts of missionaries to the area have recounted constant infighting in the tribes for women or prestige, and evidence of continuous warfare for the enslavement of neighboring tribes such as the Macu before the arrival of European settlers and government.\n\nIn Japan's feudal past, the samurai class upheld the honor of their family, clan, and their lord by \"katakiuchi\" (), or revenge killings. These killings could also involve the relatives of an offender. While some vendettas were punished by the government, such as that of the Forty-seven Ronin, others were given official permission with specific targets.\n\nAt the Holy Roman Empire's \"Reichstag\" at Worms in 1495 AD, the right of waging feuds was abolished. The Imperial Reform proclaimed an \"eternal public peace\" (\"Ewiger Landfriede\") to put an end to the abounding feuds and the anarchy of the robber barons, and it defined a new standing imperial army to enforce that peace. However, it took a few more decades until the new regulation was universally accepted. In 1506, for example, knight Jan Kopidlansky killed a family rival in Prague, and the town councillors sentenced him to death and had him executed. His brother, Jiri Kopidlansky, revenged Jan by continuing atrocities. Another case was the Nuremberg-Schott feud, in which Maximilian was forced to step in to halt the damages done by robber knight Schott.\n\nIn Greece, the custom of blood feud is found in several parts of the country, for instance in Crete and Mani. Throughout history, the Maniots have been regarded by their neighbors and their enemies as fearless warriors who practice blood feuds, known in the Maniot dialect of Greek as \"Γδικιωμός\" (Gdikiomos). Many vendettas went on for months, some for years. The families involved would lock themselves in their towers and, when they got the chance, would murder members of the opposing family. The Maniot vendetta is considered the most vicious and ruthless; it has led to entire family lines being wiped out. The last vendetta on record required the Greek Army with artillery support to force it to a stop. Regardless of this, the Maniot Greeks still practice vendettas even today. Maniots in America, Australia, Canada and Corsica still have on-going vendettas which have led to the creation of Mafia families known as \"Γδικιωμέοι\" (Gdikiomeoi).\n\nIn Corsica, vendetta was a social code that required Corsicans to kill anyone who wronged the family honor. Between 1821 and 1852, no less than 4,300 murders were perpetrated in Corsica.\n\nIn the Late Middle Ages, the Basque Country was ravaged by the War of the Bands, bitter partisan wars between local ruling families. In Navarre, these conflicts became polarised in a violent struggle between the Agramont and Beaumont parties. In Biscay, the two major warring factions were named Oinaz and Gamboa. (\"Cf.\" the Guelphs and Ghibellines in Italy). High defensive structures (\"towers\") built by local noble families, few of which survive today, were frequently razed by fires, and sometimes by royal decree.\n\nLeontiy Lyulye, an expert on conditions in the Caucasus, wrote in the mid-19th century: \"Among the mountain people the blood feud is not an uncontrollable permanent feeling such as the vendetta is among the Corsicans. It is more like an obligation imposed by the public opinion.\" In the Dagestani \"aul\" of Kadar, one such blood feud between two antagonistic clans lasted for nearly 260 years, from the 17th century till the 1860s.\n\nThe Celtic phenomenon of the \"blood feud\" demanded \"an eye for an eye,\" and usually descended into murder. Disagreements between clans might last for generations in Scotland and Ireland.\n\nDue to the Celtic heritage of many people living in Appalachia, a series of prolonged violent engagements in late nineteenth-century Kentucky and West Virginia were referred to commonly as feuds, a tendency that was partly due to the nineteenth-century popularity of William Shakespeare and Sir Walter Scott, both of whom had written semihistorical accounts of blood feuds. These incidents, the most famous of which was the Hatfield–McCoy feud, were regularly featured in the newspapers of the eastern U.S. between the Reconstruction Era and the early twentieth century, and are seen by some as linked to a Southern culture of honor with its roots in the Scots-Irish forebears of the residents of the area. Another prominent example is the Regulator–Moderator War, which took place between rival factions in the Republic of Texas. It is sometimes considered the largest blood feud in American history.\n\nAn alternative to feud was \"blood money\" (or \"weregild\" in the Norse culture), which demanded payment of some kind from those responsible for a wrongful death, even an accidental one. If these payments were not made, or were refused by the offended party, a blood feud would ensue.\n\nBlood feuds are still practised in some areas in:\n\nBlood feuds within Russian communities do exist (mostly related to criminal gangs), but are neither as common nor as pervasive as they are in the Caucasus. In the United States, blood feuds are also not as pervasive or common, but do exist within the African-American and Chicano communities (sometimes gang-related, but not necessarily). \nGang warfare also often takes the form of blood feuds. African-American, Cambodian, Cuban Marielito, Dominican, Guatemalan, Haitian, Hmong, Sino-Vietnamese Hoa, Jamaican, Korean, Laotian, Puerto Rican, Salvadoran and Vietnamese gang fights in the United States, as well as Colombian, Mexican and Brazilian gang and paramilitary wars, Cape Coloured turf wars in South Africa, Dutch Antillean, Surinamese and Moluccan gang fights in the Netherlands, and Scottish, White British, Black and Mixed British criminal feuds in the UK, very often have taken the form of blood feuds where a family member in the gang is killed and a relative takes revenge by killing the murderer as well as other members of the rival gang. This has resulted in gun violence and murders in cities like Chicago, Detroit, Los Angeles, Miami, Ciudad Juarez, Medellin, Rio de Janeiro, Cape Town, Amsterdam, London, Liverpool, and Glasgow, to name just a few.\n\nBlood feuds also have a long history within the White Southern population of the U.S., where it is called the \"culture of honor\", and still exist to the present day.\n\nIn Albania, \"gjakmarrja\" (blood feuding) is a tradition. They have returned in rural areas after more than 40 years of being abolished by Albanian communists led by Enver Hoxha, and more than 3000 Albanian families are currently engaged in them. There are now more than 1,600 families who live under an ever-present death sentence because of them, and since 1992, at least 10,000 Albanians have been killed in them. Blood feuds in Albania trace back to the Kanun, this custom is also practiced among the Albanians of Kosovo\n\nA feud may develop into a vicious circle of further killings, retaliation, counterattacks, and all-out warfare that can end in the mutual extinction of both families. Often the original cause is forgotten, and feuds continue simply because it is perceived that there has always been a feud.\n\nBlood feuds have also been part of a centuries-old tradition in Kosovo, tracing back to the Kanun, a 15th-century codification of Albanian customary rules. In the early 1990s, most cases of blood feuds were reconciled in the course of a large-scale reconciliation movement to end blood feuds led by Anton Çetta. The largest reconciliation gathering took place at Verrat e Llukës on 1 May 1990, which had between 100,000 and 500,000 participants. By 1992 the reconciliation campaign ended at least 1,200 deadly blood feuds, and in 1993 not a single homicide occurred in Kosovo.\n\nCriminal gang feuds also exist in Dublin, Ireland and in the Republic's third-largest city, Limerick. Traveller feuds are also common in towns across the country. Feuds can be due to personal issues, money, or disrespect, and grudges can last generations. Since 2001, over 300 people have been killed in feuds between different drugs gangs, dissident republicans, and Traveller families.\n\nFamily and clan feuds, known locally as \"rido\", are characterized by sporadic outbursts of retaliatory violence between families and kinship groups, as well as between communities. It can occur in areas where the government or a central authority is weak, as well as in areas where there is a perceived lack of justice and security. \"Rido\" is a Maranao term commonly used in Mindanao to refer to clan feuds. It is considered one of the major problems in Mindanao because, apart from numerous casualties, \"rido\" has caused destruction of property, crippled local economies, and displaced families.\n\nLocated in the southern Philippines, Mindanao is home to a majority of the country’s Muslim community, and includes the Autonomous Region in Muslim Mindanao. Mindanao \"is a region suffering from poor infrastructure, high poverty, and violence that has claimed the lives of more than 120,000 in the last three decades.\" There is a widely held stereotype that the violence is perpetrated by armed groups that resort to terrorism to further their political goals, but the actual situation is far more complex. While the Muslim-Christian conflict and the state-rebel conflicts dominate popular perceptions and media attention, a survey commissioned by The Asia Foundation in 2002 — and further verified by a recent Social Weather Stations survey — revealed that citizens are more concerned about the prevalence of \"rido\" and its negative impact on their communities than the conflict between the state and rebel groups. The unfortunate interaction and subsequent confusion of \"rido\"-based violence with secessionism, communist insurgency, banditry, military involvement and other forms of armed violence shows that violence in Mindanao is more complicated than what is commonly believed.\n\n\"Rido\" has wider implications for conflict in Mindanao, primarily because it tends to interact in unfortunate ways with separatist conflict and other forms of armed violence. Many armed confrontations in the past involving insurgent groups and the military were triggered by a local \"rido\". The studies cited above investigated the dynamics of \"rido\" with the intention of helping design strategic interventions to address such conflicts.\n\nThe causes of \"rido\" are varied and may be further complicated by a society's concept of honor and shame, an integral aspect of the social rules that determine accepted practices in the affected communities. The triggers for conflicts range from petty offenses, such as theft and jesting, to more serious crimes, like homicide. These are further aggravated by land disputes and political rivalries, the most common causes of \"rido\". Proliferation of firearms, lack of law enforcement and credible mediators in conflict-prone areas, and an inefficient justice system further contribute to instances of \"rido\".\n\nStudies on \"rido\" have documented a total of 1,266 \"rido\" cases between the 1930s and 2005, which have killed over 5,500 people and displaced thousands. The four provinces with the highest numbers of \"rido\" incidences are: Lanao del Sur (377), Maguindanao (218), Lanao del Norte (164), and Sulu (145). Incidences in these four provinces account for 71% of the total documented cases. The findings also show a steady rise in \"rido\" conflicts in the eleven provinces surveyed from the 1980s to 2004. According to the studies, during 2002–2004, 50% (637 cases) of total \"rido\" incidences occurred, equaling about 127 new \"rido\" cases per year. Out of the total number of \"rido\" cases documented, 64% remain unresolved.\n\n\"Rido\" conflicts are either resolved, unresolved, or reoccurring. Although the majority of these cases remain unresolved, there have been many resolutions through different conflict-resolving bodies and mechanisms. These cases can utilize the formal procedures of the Philippine government or the various indigenous systems. Formal methods may involve official courts, local government officials, police, and the military. Indigenous methods to resolve conflicts usually involve elder leaders who use local knowledge, beliefs, and practices, as well as their own personal influence, to help repair and restore damaged relationships. Some cases using this approach involve the payment of blood money to resolve the conflict. Hybrid mechanisms include the collaboration of government, religious, and traditional leaders in resolving conflicts through the formation of collaborative groups. Furthermore, the institutionalization of traditional conflict resolution processes into laws and ordinances has been successful with the hybrid method approach. Other conflict-resolution methods include the establishment of ceasefires and the intervention of youth organizations.\n\n\n\n\n"}
{"id": "438098", "url": "https://en.wikipedia.org/wiki?curid=438098", "title": "First-sale doctrine", "text": "First-sale doctrine\n\nThe first-sale doctrine is a legal concept playing an important role in U.S. copyright and trademark law by limiting certain rights of a copyright or trademark owner. The doctrine enables the distribution chain of copyrighted products, library lending, giving, video rentals and secondary markets for copyrighted works (for example, enabling individuals to sell their legally purchased books or CDs to others). In trademark law, this same doctrine enables reselling of trademarked products after the trademark holder put the products on the market. The doctrine is also referred to as the \"right of first sale,\" \"first sale rule,\" or \"exhaustion rule.\"\n\nThe first-sale doctrine is one of the limitations and exceptions to copyright.\n\nCopyright law grants a copyright owner an exclusive right \"to distribute copies or phonorecords of the copyrighted work to the public by sale or other transfer of ownership, or by rental, lease, or lending.\" 17 U.S.C. 106(3). This is called a \"distribution right\" and differs from the copyright owner's \"reproduction right\" which involves making copies of the copyrighted works. Rather than the right to copy, the distribution right involves the right to transfer physical copies or phonorecords (i.e., recorded music) of the copyrighted work. For example, the distribution right could be infringed when a retailer acquires and sells to the public unlawfully made audio or video tapes. Although the retailer may not have copied the work in any way and may not have known that the tapes were made unlawfully, he nevertheless infringes the distribution right by the sale. The distribution right allows the copyright owner to seek redress from any member in the chain of distribution.\n\nThe first-sale doctrine creates a basic exception to the copyright holder's distribution right. Once the work is lawfully sold or even transferred gratuitously, the copyright owner's interest in the material object in which the copyrighted work is embodied is exhausted. The owner of the material object can then dispose of it as he sees fit. Thus, one who buys a copy of a book is entitled to resell it, rent it, give it away, or destroy it. However, the owner of the copy of the book will not be able to make new copies of the book because the first-sale doctrine does not limit copyright owner's reproduction right. The rationale of the doctrine is to prevent the copyright owner from restraining the free alienability of goods. Without the doctrine, a possessor of a copy of a copyrighted work would have to negotiate with the copyright owner every time he wished to dispose of his copy. After the initial transfer of ownership of a legal copy of a copyrighted work, the first-sale doctrine exhausts copyright holder's right to control how ownership of that copy can be disposed of. For this reason, this doctrine is also referred to as the \"exhaustion rule.\"\n\nThe doctrine was first recognized by the Supreme Court of the United States in 1908 (see \"Bobbs-Merrill Co. v. Straus\") and subsequently codified in the Copyright Act of 1976, 17 U.S.C. § 109. In the \"Bobbs-Merrill\" case, the publisher, Bobbs-Merrill, had inserted a notice in its books that any retail sale at a price under $1.00 would constitute an infringement of its copyright. The defendants, who owned Macy's department store, disregarded the notice and sold the books at a lower price without Bobbs-Merrill's consent. The Supreme Court held that the exclusive statutory right to \"vend\" applied only to the first sale of the copyrighted work.\n\nSection 109(a) provides:\n\"Notwithstanding the provisions of section 106 (3), the owner of a particular copy or phonorecord lawfully made under this title, or any person authorized by such owner, is entitled, without the authority of the copyright owner, to sell or otherwise dispose of the possession of that copy or phonorecord.\" The elements of the first sale doctrine can be summarized as follows: (1) the copy was lawfully made with the authorization of the copyright owner; (2) ownership of the copy was initially transferred under the copyright owner's authority; (3) the defendant is a lawful owner of the copy in question; and (4) the defendant's use implicates the distribution right only; not the reproduction or some other right given to the copyright owner.\n\n17 U.S.C. §109(c) creates a limited exception to a copyright owner's public display right. Owners of a lawful copy of a copyrighted work can, without permission from the copyright owner, display that copy to viewers present at the place where the copy is located. \n\nAn amicus brief in \"Kirtsaeng v. John Wiley & Sons, Inc.\" argued that Section 109 was a key provision for US art museums:\n\nMost U.S. art museums have permanent collections that were acquired through purchases, gifts, and bequests, and on which they draw for exhibitions to the public. Museums also present special exhibitions, largely made up of works not in their collections, through loans from private collectors, galleries, and other institutions. For all these activities museums depend on the protections afforded by Section 109. Section 109(c) provides that the owner of a particular copy \"lawfully made under this title\" is entitled to display that copy publicly without the copyright owner's permission. Section 109(a) similarly allows museums to buy, borrow, loan, and sell such \"lawfully made\" artworks.\n\nThe first sale doctrine only limits the distribution rights of copyright holders. This principle sometimes clashes with the holder's other rights, such as the right of reproduction & derivative work rights. For example, in \"Lee v. A.R.T. Co.\", the defendant bought plaintiff's artworks in the form of notecards and then mounted them on ceramic tiles, covering the artworks with transparent epoxy resin. Despite plaintiff's assertion of violation of his right to prepare derivative works, the 7th Circuit held that the derivative work right was not violated and that defendant's sale of the tiles was protected under the first sale doctrine. However, based on very similar facts, the 9th Circuit in \"Mirage Editions, Inc. v. Albuquerque A.R.T. Company\" held that plaintiff's right to prepare derivative works was infringed and that the first sale doctrine did not protect the defendant under such circumstances.\n\nThe first-sale doctrine does not neatly fit transfers of copies of digital works because an actual transfer does not actually happen – instead, the recipient receives a new copy of the work while, at the same time, the sender has the original copy (unless that copy is deleted, either automatically or manually). For example, this exact issue played out in Capitol Records, LLC v. ReDigi Inc., a case involving online marketplace for pre-owned digital music.\n\nE-books have the same issue. Because the first sale doctrine does not apply to electronic books, libraries cannot freely lend e-books indefinitely after purchase. Instead, electronic book publishers came up with business models to sell the subscriptions to the license of the text. This results in e-book publishers placing restrictions on the number of times an e-book can circulate and/or the amount of time a book is within a collection before a library's license expires, then the book no longer belongs to them.\n\nThe question is whether the first-sale doctrine should be retooled to reflect the realities of the digital age. Physical copies degrade over time, whereas digital information may not. Works in digital format can be reproduced without any flaws and can be disseminated worldwide without much difficulty. Thus, applying the first sale doctrine to digital copies affects the market for the original to a greater degree than transfers of physical copies. The U.S. Copyright Office stated that \"[t]he tangible nature of a copy is a defining element of the first sale doctrine and critical to its rationale.\"\n\nIn Europe, the European Court of Justice ruled, on July 3, 2012, that it is indeed permissible to resell software licenses even if the digital good has been downloaded directly from the Internet, and that the first sale doctrine applied whenever software was originally sold to a customer for an unlimited amount of time, as such sale involves a transfer of ownership, thus prohibiting any software maker from preventing the resale of their software by any of their legitimate owners. The court requires that the previous owner must no longer be able to use the licensed software after the resale, but finds that the practical difficulties in enforcing this clause should not be an obstacle to authorizing resale, as they are also present for software which can be installed from physical supports, where the first-sale doctrine is in force. The ruling applies to the European Union, but could indirectly find its way to North America; moreover the situation could entice publishers to offer platforms for a secondary market.\n\nFor the first sale doctrine to apply, lawful \"ownership\" of the copy or phonorecord is required. As §109(d) prescribes, first sale doctrine does not apply if the possession of the copy is \"by rental, lease, loan, or otherwise without acquiring ownership of it.\"\n\nSome software and digital content publishers claim in their end-user license agreements (EULA) that their software or content is licensed, not sold, and thus the first sale doctrine does not apply to their works. These publishers have had some success in contracting around first sale doctrine through various clickwrap, shrink wrap, and other license agreements. For example, if someone buys MP3 songs from Amazon.com, the MP3 files are merely licensed to them and hence they may not be able to resell those MP3 files. However, MP3 songs bought through iTunes Store may be characterized as \"sales\" because of Apple's language in its EULA and hence they may be resell-able, if other requirements of first sale doctrine are met.\n\nCourts have struggled and taken dramatically different approaches to sort out when only a license was granted to the end user as compared to ownership. Most of these cases involved software-licensing agreements. In general, courts look beneath the surface of the agreements to conclude whether the agreements create a licensing relationship or if they amount to, in substance, sales subject to first sale doctrine under §109(a). Thus, specifying that the agreement grants only a \"license\" is necessary to create the licensing relationship, but not sufficient. Other terms of the agreement should be consistent with such a licensing relationship.\n\nIn \"Vernor v. Autodesk, Inc.\" the 9th Circuit created a three-factor test to decide whether a particular software licensing agreement is successful in creating a licensing relationship with the end user. The factors include: 1) whether a copyright owner specifies that a user is granted a license; 2) whether the copyright owner significantly restricts the user's ability to transfer the software to others; and 3) whether the copyright owner imposes notable use restrictions on the software. In \"Vernor\", Autodesk's license agreement specified that it retains title to the software and the user is only granted a non-exclusive license. The agreement also had restrictions against modifying, translating, or reverse-engineering the software, or removing any proprietary marks from the software packaging or documentation. The agreement also specified that software could not be transferred or leased without Autodesk's written consent, and could not be transferred outside the Western Hemisphere. Based on these facts, the 9th Circuit held that the user is only a licensee of Autodesk's software, not an owner and hence the user could not resell the software on eBay without Autodesk's permission.\n\nHowever, the same 9th Circuit panel that decided \"Vernor v. Autodesk\", refused to apply \"Vernor's\" three-factor test in \"UMG v. Augusto\" to a purported licensing agreement created when UMG sent unsolicited promotional CDs to music critics. The promotional CDs' packaging contained the language: \"This CD is the property of the record company and is licensed to the intended recipient for personal use only. Acceptance of this CD shall constitute an agreement to comply with the terms of the license. Resale or transfer of possession is not allowed and may be punishable under federal and state laws.\" Augusto tried to sell these CDs on eBay and UMG argued that first sale doctrine did not apply since the CDs were not sold and only a licensing relationship was created. However the court held that first sale doctrine applies when a copy is given away and that recipients of the promotional CDs did not accept the terms of the license agreement by merely not sending back the unsolicited CDs.\n\nIn the case \"UsedSoft v Oracle\", the European Court of Justice ruled that the sale of a software product, either through a physical support or download, constituted a transfer of ownership in EU law, thus the first sale doctrine applies; the ruling thereby breaks the \"licensed, not sold\" legal theory, but leaves open numerous questions.\n\nSection 602(a)(1) of the Copyright statute states that \"importation into the United States, without the authority of the owner of copyright under this title, of copies or phonorecords of a work that have been acquired outside the United States is an infringement of the exclusive right to distribute copies or phonorecords.\" This provision provides copyright owner an opportunity to stop goods from entering the United States market altogether.\n\nApplication of this provision created difficult legal issues in the context of gray market products. Gray market dealers buy the genuine goods in foreign countries at a significant discount from U.S. prices. They then import these genuine goods into the U.S. and sell them at discount prices, undercutting the authorized U.S. dealers. The gray market exists where the price for goods outside the US is lower than the price inside.\n\nOn the surface, §602(a), barring unauthorized importation, would seem to clash with the first-sale doctrine, which permits the resale of lawfully made copies. The issue comes down to whether §602(a) creates an affirmative right to bar all unauthorized importation, or does the first-sale doctrine limit the reach of §602(a), thus permitting the resale of at least some lawfully made imported copies.\n\nIn 1998, the U.S. Supreme Court in \"Quality King v. L'Anza\" found that first-sale doctrine applied to imported goods at least where the imported goods are first lawfully made in the United States, shipped abroad for resale, and later reenter the United States. That case involved importation of hair care products bearing copyrighted labels. A unanimous Supreme Court found that the first-sale doctrine does apply to importation into the US of copyrighted works (the labels), which were made in the US and then exported.\n\nHowever, the Supreme Court did not decide the issue where gray-market products are initially manufactured abroad and then imported into the US. The Court indicated that importation of goods made outside the US could perhaps be barred under §602(a), since such goods would not be \"lawfully made under this title.\" Such products might be lawfully made, either by the copyright owner or a licensee, but they would not be lawfully made under US copyright law. Rather, they would be lawfully made under the copyright laws of the other country; and the first-sale doctrine would therefore not limit the §602 importation restriction.\n\nThe 2008 \"Omega v. Costco\" case involved this exact unresolved issue, where the defendant Costco obtained authentic Omega watches, which feature a copyrighted design on the back of the watches, through the gray market and resold them in its stores in the US. Omega manufactured these watches outside the US and did not authorize their importation into the US. Based on the \"Quality King\" case, the 9th Circuit held that \"application of first-sale doctrine to foreign-made copies would impermissibly apply\" the Copyright Act extraterritorially. However, the court stated that first-sale doctrine might still apply to a foreign manufactured copy if it was imported \"with the authority of the U.S. copyright owner.\" The Supreme Court granted certiorari to \"Omega v. Costco\", and affirmed 4-4. However, as an evenly split decision, it set precedent only in the 9th Circuit, not nationwide.\n\nHowever, in \"Kirtsaeng v. John Wiley & Sons, Inc.\", in 2013, the United States Supreme Court held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to charge vastly different prices in different markets due to ease of arbitrage. The decision removes the incentive to US manufacturers of shifting manufacturing abroad purely in an attempt to circumvent the first-sale doctrine.\n\nThe Record Rental Amendment of 1984, codified in 17 USC §109(b) prohibits an owner of a phonorecord that embodies a sound recording or musical work from renting it to the public for direct or indirect commercial advantage. This exception was designed to prevent music stores from renting records and thereby facilitating home copying.\n\nSection 109(b) is an exception to the first sale doctrine, but it is limited in several ways. It applies only to rentals, and not to resale or other transfers. It is also limited to a subset of sound recordings—only those sound recordings that contain only a musical work. It does not apply to sound recordings that contain other content, such as commentaries or dialog soundtrack, or to non-musical sound recordings, for example audiobooks. Lastly, libraries and educational institutions are exempt from this restriction, and may rent or loan musical sound recordings.\n\nThe Copyright Software Rental Amendments Act of 1990 amended §109(b) further to prohibit rentals of computer software for direct or indirect commercial advantage. The exception does not apply to lending of a copy by a nonprofit library for nonprofit purposes, provided the library affixes an appropriate warning. The amendment also specifically excluded:\n\nWith reference to trade in tangible merchandise, such as the retailing of goods bearing a trademark, the first sale doctrine serves to immunize a reseller from infringement liability. Such protection to the reseller extends to the point where said goods have not been altered so as to be materially different from those originating from the trademark owner.\n\n\nSpecific citations:\n\nGeneral references:\n"}
{"id": "27086745", "url": "https://en.wikipedia.org/wiki?curid=27086745", "title": "Frame-dragging", "text": "Frame-dragging\n\nFrame-dragging is an effect on spacetime, predicted by Einstein's general theory of relativity, that is due to non-static stationary distributions of mass–energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static, rotating for instance. More generally, the subject of effects caused by mass–energy currents is known as gravitomagnetism, in analogy with classical electromagnetism. \n\nThe first frame-dragging effect was derived in 1918, in the framework of general relativity, by the Austrian physicists Josef Lense and Hans Thirring, and is also known as the Lense–Thirring effect. They predicted that the rotation of a massive object would distort the spacetime metric, making the orbit of a nearby test particle precess. This does not happen in Newtonian mechanics for which the gravitational field of a body depends only on its mass, not on its rotation. The Lense–Thirring effect is very small—about one part in a few trillion. To detect it, it is necessary to examine a very massive object, or build an instrument that is very sensitive.\n\nIn 2015, new general-relativistic extensions of Newtonian rotation laws were formulated to describe geometric dragging of frames which incorporates a newly discovered antidragging effect.\n\nRotational frame-dragging (the Lense–Thirring effect) appears in the general principle of relativity and similar theories in the vicinity of rotating massive objects. Under the Lense–Thirring effect, the frame of reference in which a clock ticks the fastest is one which is revolving around the object as viewed by a distant observer. This also means that light traveling in the direction of rotation of the object will move past the massive object faster than light moving against the rotation, as seen by a distant observer. It is now the best known frame-dragging effect, partly thanks to the Gravity Probe B experiment. Qualitatively, frame-dragging can be viewed as the gravitational analog of electromagnetic induction.\n\nAlso, an inner region is dragged more than an outer region. This produces interesting locally rotating frames. For example, imagine that a north-south–oriented ice skater, in orbit over the equator of a black hole and rotationally at rest with respect to the stars, extends her arms. The arm extended toward the black hole will be \"torqued\" spinward due to gravitomagnetic induction (\"torqued\" is in quotes because gravitational effects are not considered \"forces\" under GR). Likewise the arm extended away from the black hole will be torqued anti-spinward. She will therefore be rotationally sped up, in a counter-rotating sense to the black hole. This is the opposite of what happens in everyday experience. There exists a particular rotation rate that, should she be initially rotating at that rate when she extends her arms, inertial effects and frame-dragging effects will balance and her rate of rotation will not change. Due to the equivalence principle, gravitational effects are locally indistinguishable from inertial effects, so this rotation rate, at which when she extends her arms nothing happens, is her local reference for non-rotation. This frame is rotating with respect to the fixed stars and counter-rotating with respect to the black hole. This effect is analogous to the hyperfine structure in atomic spectra due to nuclear spin. A useful metaphor is a planetary gear system with the black hole being the sun gear, the ice skater being a planetary gear and the outside universe being the ring gear. See Mach's principle.\n\nAnother interesting consequence is that, for an object constrained in an equatorial orbit, but not in freefall, it weighs more if orbiting anti-spinward, and less if orbiting spinward. For example, in a suspended equatorial bowling alley, a bowling ball rolled anti-spinward would weigh more than the same ball rolled in a spinward direction. Note, frame dragging will neither accelerate or slow down the bowling ball in either direction. It is not a \"viscosity\". Similarly, a stationary plumb-bob suspended over the rotating object will not list. It will hang vertically. If it starts to fall, induction will push it in the spinward direction.\n\nLinear frame dragging is the similarly inevitable result of the general principle of relativity, applied to linear momentum. Although it arguably has equal theoretical legitimacy to the \"rotational\" effect, the difficulty of obtaining an experimental verification of the effect means that it receives much less discussion and is often omitted from articles on frame-dragging (but see Einstein, 1921).\n\nStatic mass increase is a third effect noted by Einstein in the same paper. The effect is an increase in inertia of a body when other masses are placed nearby. While not strictly a frame dragging effect (the term frame dragging is not used by Einstein), it is demonstrated by Einstein that it derives from the same equation of general relativity. It is also a tiny effect that is difficult to confirm experimentally.\n\nIn 1976 Van Patten and Everitt proposed to implement a dedicated mission aimed to measure the Lense–Thirring node precession of a pair of counter-orbiting spacecraft to be placed in terrestrial polar orbits with drag-free apparatus. A somewhat equivalent, cheaper version of such an idea was put forth in 1986 by Ciufolini who proposed to launch a passive, geodetic satellite in an orbit identical to that of the LAGEOS satellite, launched in 1976, apart from the orbital planes which should have been displaced by 180 deg apart: the so-called butterfly configuration. The measurable quantity was, in this case, the sum of the nodes of LAGEOS and of the new spacecraft, later named LAGEOS III, LARES, WEBER-SAT.\n\nLimiting the scope to the scenarios involving existing orbiting bodies, the first proposal to use the LAGEOS satellite and the Satellite Laser Ranging (SLR) technique to measure the Lense–Thirring effect dates back to 1977–1978. Tests have started to be effectively performed by using the LAGEOS and LAGEOS II satellites in 1996, according to a strategy involving the use of a suitable combination of the nodes of both satellites and the perigee of LAGEOS II. The latest tests with the LAGEOS satellites have been performed in 2004–2006 by discarding the perigee of LAGEOS II and using a linear combination. Recently, a comprehensive overview of the attempts to measure the Lense-Thirring effect with artificial satellites was published in the literature. The overall accuracy reached in the tests with the LAGEOS satellites is subject to some controversy.\n\nThe Gravity Probe B experiment was a satellite-based mission by a Stanford group and NASA, used to experimentally measure another gravitomagnetic effect, the Schiff precession of a gyroscope, to an expected 1% accuracy or better. Unfortunately such accuracy was not achieved. The first preliminary results released in April 2007 pointed towards an accuracy of 256–128%, with the hope of reaching about 13% in December 2007.\nIn 2008 the Senior Review Report of the NASA Astrophysics Division Operating Missions stated that it was unlikely that Gravity Probe B team will be able to reduce the errors to the level necessary to produce a convincing test of currently untested aspects of General Relativity (including frame-dragging).\nOn May 4, 2011, the Stanford-based analysis group and NASA announced the final report, and in it the data from GP-B demonstrated the frame-dragging effect with an error of about 19 percent, and Einstein's predicted value was at the center of the confidence interval.\n\nIn the case of stars orbiting close to a spinning, supermassive black hole, frame dragging should cause the star's orbital plane to precess about the black hole spin axis. This effect should be detectable within the next few years via astrometric monitoring of stars at the center of the Milky Way galaxy.\nBy comparing the rate of orbital precession of two stars on different orbits, it is possible in principle to test the no-hair theorems of general relativity, in addition to measuring the spin of the black hole.\n\nRelativistic jets may provide evidence for the reality of frame-dragging. Gravitomagnetic forces produced by the Lense–Thirring effect (frame dragging) within the ergosphere of rotating black holes combined with the energy extraction mechanism by Penrose have been used to explain the observed properties of relativistic jets. The gravitomagnetic model developed by Reva Kay Williams predicts the observed high energy particles (~GeV) emitted by quasars and active galactic nuclei; the extraction of X-rays, γ-rays, and relativistic e–e pairs; the collimated jets about the polar axis; and the asymmetrical formation of jets (relative to the orbital plane).\n\nFrame-dragging may be illustrated most readily using the Kerr metric, which describes the geometry of spacetime in the vicinity of a mass \"M\" rotating with angular momentum \"J\", and Boyer–Lindquist coordinates, where an unphysical, but mathematically more elegant radial coordinate \"r\" (see the link for the transformation) is used:\n\nwhere \"r\" is the Schwarzschild radius\n\nand where the following shorthand variables have been introduced for brevity\n\nIn the non-relativistic limit where \"M\" (or, equivalently, \"r\") goes to zero, the Kerr metric becomes the orthogonal metric for the oblate spheroidal coordinates\n\nWe may rewrite the Kerr metric in the following form\n\nThis metric is equivalent to a co-rotating reference frame that is rotating with angular speed Ω that depends on both the radius \"r\" and the colatitude θ\n\nIn the plane of the equator this simplifies to:\n\nThus, an inertial reference frame is entrained by the rotating central mass to participate in the latter's rotation; this is frame-dragging.\n\nAn extreme version of frame dragging occurs within the ergosphere of a rotating black hole. The Kerr metric has two surfaces on which it appears to be singular. The inner surface corresponds to a spherical event horizon similar to that observed in the Schwarzschild metric; this occurs at\n\nwhere the purely radial component \"g\" of the metric goes to infinity. The outer surface can be approximated by an oblate spheroid with lower spin parameters, and resembles a pumpkin-shape with higher spin parameters. It touches the inner surface at the poles of the rotation axis, where the colatitude θ equals 0 or π; its radius in Boyer-Lindquist coordinates is defined by the formula\n\nwhere the purely temporal component \"g\" of the metric changes sign from positive to negative. The space between these two surfaces is called the ergosphere. A moving particle experiences a positive proper time along its worldline, its path through spacetime. However, this is impossible within the ergosphere, where \"g\" is negative, unless the particle is co-rotating with the interior mass \"M\" with an angular speed at least of Ω. However, as seen above, frame-dragging occurs about every rotating mass and at every radius \"r\" and colatitude θ, not only within the ergosphere.\n\nThe Lense–Thirring effect inside a rotating shell was taken by Albert Einstein as not just support for, but a vindication of Mach's principle, in a letter he wrote to Ernst Mach in 1913 (five years before Lense and Thirring's work, and two years before he had attained the final form of general relativity). A reproduction of the letter can be found in Misner, Thorne, Wheeler. The general effect scaled up to cosmological distances, is still used as a support for Mach's principle.\n\nInside a rotating spherical shell the acceleration due to the Lense–Thirring effect would be\n\nwhere the coefficients are\n\nfor \"MG\" ≪ \"Rc\" or more precisely,\n\nThe spacetime inside the rotating spherical shell will not be flat. A flat spacetime inside a rotating mass shell is possible if the shell is allowed to deviate from a precisely spherical shape and the mass density inside the shell is allowed to vary.\n\n\n\n"}
{"id": "514231", "url": "https://en.wikipedia.org/wiki?curid=514231", "title": "Frequency-dependent selection", "text": "Frequency-dependent selection\n\nFrequency-dependent selection is an evolutionary process by which the fitness of a phenotype depends on its frequency relative to other phenotypes in a given population.\n\n\nFrequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where \"α\" parameters in Lotka-Volterra competition equations are non-zero).\n\nThe first explicit statement of frequency-dependent selection appears to have been by Edward Bagnall Poulton in 1884, on the way that predators could maintain color polymorphisms in their prey.\n\nPerhaps the best known early modern statement of the principle is Bryan Clarke's 1962 paper on apostatic selection (a synonym of negative frequency-dependent selection). Clarke discussed predator attacks on polymorphic British snails, citing Luuk Tinbergen's classic work on searching images as support that predators such as birds tended to specialize in common forms of palatable species. Clarke later argued that frequency-dependent balancing selection could explain molecular polymorphisms (often in the absence of heterosis) in opposition to the neutral theory of molecular evolution.\n\nAnother example is plant self-incompatibility alleles. When two plants share the same incompatibility allele, they are unable to mate. Thus, a plant with a new (and therefore, rare) allele has more success at mating, and its allele spreads quickly through the population .\n\nIn human pathogens, such as the flu virus, once a particular strain has become common, most individuals have developed an immune response to that strain. But a rare, novel strain of the flu virus is able to spread quickly to almost any individual, causing continual evolution of viral strains.\n\nThe major histocompatibility complex (MHC) is involved in the recognition of foreign antigens and cells. Frequency-dependent selection may explain the high degree of polymorphism in the MHC.\n\nIn behavioral ecology, negative frequency-dependent selection often maintains multiple behavioral strategies within a species. A classic example is the Hawk-Dove model of interactions among individuals in a population. In a population with two traits A and B, being one form is better when most members are the other form. As another example, male common side-blotched lizards have three morphs, which either defend large territories and maintain large harems of females, defend smaller territories and keep one female, or mimic females in order to sneak matings from the other two morphs. These three morphs participate in a rock paper scissors sort of interaction such that no one morph completely outcompetes the other two. Another example occurs in the scaly-breasted munia, where certain individuals become scroungers and others become producers.\n\nPositive frequency-dependent selection gives an advantage to common phenotypes. A good example is warning coloration in aposematic species. Predators are more likely to remember a common color pattern that they have already encountered frequently than one that is rare. This means that new mutants or migrants that have color patterns other than the common type are eliminated from the population by differential predation. Positive frequency-dependent selection provides the basis for Müllerian mimicry, as described by Fritz Müller , because all species involved are aposematic and share the benefit of a common, honest signal to potential predators.\n\nAnother, rather complicated example occurs in the Batesian mimicry complex between a harmless mimic, the scarlet kingsnake (\"Lampropeltis elapsoides\"), and the model, the eastern coral snake (\"Micrurus fulvius\"), in locations where the model and mimic were in deep sympatry, the phenotype of the scarlet kingsnake was quite variable due to relaxed selection. But where the pattern was rare, the predator population was not 'educated', so the pattern brought no benefit. The scarlet kingsnake was much less variable on the allopatry/sympatry border of the model and mimic, most probably due to increased selection since the eastern coral snake is rare, but present, on this border. Therefore, the coloration is only advantageous once it has become common.\n\n\n"}
{"id": "31441114", "url": "https://en.wikipedia.org/wiki?curid=31441114", "title": "Genetypes", "text": "Genetypes\n\nGenetypes is a taxonomic concept proposed in 2010 to describe any genetic sequences from type specimens. This nomenclature integrates molecular systematics and terms used in biological taxonomy. This nomenclature is designed to label, or flag, genetic sequences that were sampled from type specimens. The nomenclature of genetypes proposes that genetic sequences from a holotype should be referred to as a “hologenetype” (from: holotype and genetype), sequences from a topotype will be a “topogenetype,” and so forth. In addition, the genetic marker(s) used should also be incorporated into the nomenclature (e.g., paragenetype ND2).\n\nThe genetypes nomenclatural system could be used to flag “gold standard” sequences that due to their direct link to type specimens will be more credible than standard sequences whose species identification may be problematic. Misidentifications plague many sequences on GenBank and having some sequences that are linked to type specimens will help locate and manage misidentifications and to create positively identified \"gold standard\" sequences available for comparison. It is suggested that this nomenclature be used in publications and databases that display or discuss sequences from type specimens.\n\nExamples of genetypes include:\nThe genetypes concept was superseded by the GenSeq concept, proposed in 2013 due to some confusion among researchers that genetypes were equivalent to name-bearing types.\n\n"}
{"id": "18562346", "url": "https://en.wikipedia.org/wiki?curid=18562346", "title": "Gossen's laws", "text": "Gossen's laws\n\nGossen's laws, named for Hermann Heinrich Gossen (1810 – 1858), are three laws of economics:\n\nThe citation referenced is the translation by Nicholas Georgescu-Roegen in which the traslator names only two laws: 1) ”If an enjoyment is experienced uninterruptedly, the corresponding intensity of pleasure decreases continuously until satiety is ultimately reached, at which point the intensity becomes nil.\" and, 2) \"A similar decrease of the intensity of pleasure takes place if a previous enjoyment of the same kind of pleasure is repeated. Not only does the initial intensity of pleasure become smaller but also the duration of the enjoyment becomes shorter, so that satiety is reached sooner. Moreover, the sooner the repetition, the smaller becomes the initial intensity as well as the duration of the enjoyment.\" (p.lxxx)\n\n\n"}
{"id": "39516424", "url": "https://en.wikipedia.org/wiki?curid=39516424", "title": "Grey box model", "text": "Grey box model\n\nIn mathematics, statistics, and computational modelling, a grey box model combines a partial theoretical structure with data to complete the model. The theoretical structure may vary from information on the smoothness of results, to models that need only parameter values from data or existing literature. Thus, almost all models are grey box models as opposed to black box where no model form is assumed or white box models that are purely theoretical. Some models assume a special form such as a linear regression or neural network. These have special analysis methods. In particular linear regression techniques are much more efficient than most non-linear techniques. The model can be deterministic or stochastic (i.e. containing random components) depending on its planned use.\n\nThe general case is a non-linear model with a partial theoretical structure and some unknown parts derived from data. Models with unlike theoretical structures need to be evaluated individually, possibly using simulated annealing or genetic algorithms.\n\nWithin a particular model structure, parameters or variable parameter relations may need to be found. For a particular structure it is arbitrarily assumed that the data consists of sets of feed vectors f, product vectors p, and operating condition vectors c. Typically c will contain values extracted from f, as well as other values. In many cases a model can be converted to a function of the form:\nwhere the vector function m gives the errors between the data p, and the model predictions. The vector q gives some variable parameters that are the model's unknown parts.\n\nThe parameters q vary with the operating conditions c in a manner to be determined. This relation can be specified as q = Ac where A is a matrix of unknown coefficients, and c as in linear regression includes a constant term and possibly transformed values of the original operating conditions to obtain non-linear relations between the original operating conditions and q. It is then a matter of selecting which terms in A are non-zero and assigning their values. The model completion becomes an optimisation problem to determine the non-zero values in A that minimizes the error terms m(f,p,Ac) over the data.\n\nOnce a selection of non-zero values is made, the remaining coefficients in A can be determined by minimizing \"m\"(\"f\",\"p\",\"Ac\") over the data with respect to the nonzero values in A, typically by non-linear least squares. Selection of the nonzero terms can be done by optimization methods such as simulated annealing and evolutionary algorithms. Also the non-linear least squares can provide accuracy estimates for the elements of A that can be used to determine if they are significantly different from zero, thus providing a method of term selection.\n\nIt is sometimes possible to calculate values of q for each data set, directly or by non-linear least squares. Then the more efficient linear regression can be used to predict q using c thus selecting the non-zero values in A and estimating their values. Once the non-zero values are located non-linear least squares can be used on the original model m(f,p,Ac) to refine these values .\n\nA third method is model inversion, which converts the non-linear m(f,p,Ac) into an approximate linear form in the elements of A, that can be examined using efficient term selection and evaluation of the linear regression. For the simple case of a single q value (q = ac) and an estimate q* of q. Putting dq = ac − q* gives\n\nso that a is now in a linear position with all other terms known, and thus can be analyzed by linear regression techniques. For more than one parameter the method extends in a direct manner. After checking that the model has been improved this process can be repeated until convergence. This approach has the advantages that it does not need the parameters q to be able to be determined from an individual data set and the linear regression is on the original error terms\n\nWhere sufficient data is available, division of the data into a separate model construction set and one or two evaluation sets is recommended. This can be repeated using multiple selections of the construction set and the resulting models averaged or used to evaluate prediction differences.\n\nA statistical test such as chi-squared on the residuals is not particularly useful. The chi squared test requires known standard deviations which are seldom available, and failed tests give no indication of how to improve the model\n\nAn attempt to predict the residuals m(, ) with the operating conditions c using linear regression will show if the residuals can be predicted. Residuals that cannot be predicted offer little prospect of improving the model using the current operating conditions. Terms that do predict the residuals are prospective terms to incorporate into the model to improve its performance.\n\nThe model inversion technique above can be used as a method of determining whether a model can be improved. In this case selection of nonzero terms is not so important and linear prediction can be done using the significant eigenvectors of the regression matrix. The values in A determined in this manner need to be substituted into the nonlinear model to assess improvements in the model errors. The absence of a significant improvement indicates the available data is not able to improve the current model form using the defined parameters. Extra parameters can be inserted into the model to make this test more comprehensive.\n"}
{"id": "42998585", "url": "https://en.wikipedia.org/wiki?curid=42998585", "title": "Henry James Richter", "text": "Henry James Richter\n\nHenry James Richter (1772–1857), artist and philosopher, was born in Middlesex, possibly at 40 Great Newport Street, Soho, on 8 March 1772 and baptised at St Anne's Church, Soho, on 5 April at that same year.\n\nHenry James was the second son (of five children) of John Augustus Richter and Mary Haig. John was originally from Dresden, Germany and was an artist, engraver, and scagliolist, well known for his works in imitation of marble. John Augustus Richter was a partner with Domenico Bartoli another scagliolist in London beginning in 1767 and continuing through the 1777 or 1778. Bartoli emigrated from the port city of Livorn, Italy (often referred to in English as Leghorn) and, after working for almost 10 years with Richter, moved on to Ireland. One notable item the elder Richter produced was a child's building blocks set called Richter's Anchor Blocks (an example of which is currently in the collection of the UK National Trust Museum of Childhood.) Henry's older brother, John Richter (1769?–1830), was a prominent radical politician, and member of the London Corresponding Society. He was later arrested for high treason and incarcerated briefly in the Tower of London. In 1794, the Richter brothers compiled and printed John Milton's book of poems called Paradise Lost which included 13 illustrations done by Henry James. The book was dedicated and presented to His Royal Highness George, Prince of Wales while John Richter was on trial for treason. Another brother, Thomas Richter, was a director of the Phoenix Life Insurance Company and remained with them until nearly the mid century. He had a sister, Elizabeth Richter, who married Rev. James Stuart Freeman around July 1808. Richter was educated at Dr Barrow's school, Soho and St Martin's Library School, London. In about 1787, he began instruction under the artist Thomas Stothard, with whom he remained a close friend. He also began an association with William Blake. In 1788 he produced his first illustrations (to Shakespeare's plays), and first exhibited paintings, showing two landscapes at the Royal Academy, where he exhibited for many years. He became a student of the Royal Academy Schools in 1790, and reportedly studied anatomy.\n\nThroughout the 1790s Richter worked mainly as an illustrator, demonstrating skills as both draftsman and engraver; projects included editions of Samuel Richardson's \"Sir Charles Grandison\" and \"Clarissa Harlowe\" (with others, both 1793), Samuel Johnson's \"Lives of the English Poets\" (with others, 1797), and J.-H. Bernardin de Saint-Pierre's Paul and Virginia (1799). In 1809 Richter began to exhibit at the Associated Artists in Water Colours, Bond Street, becoming a member in 1810, and president in 1811–12. His most popular work at this stage was the genre subject A Brute of a Husband, though he also established himself as a literary painter, with depictions of such characters as Don Quixote and Falstaff, and became one of the few notable artists to paint historical subjects in watercolour. With the dissolution of the Associated Artists in Water Colours in 1812 he was elected a member of the Society of Painters in Oil and Water Colours, though he resigned his membership in December of the same year, and until 1820 was represented on its walls only as an exhibitor. (In that year the society returned to its original form as the Society of Painters in Water Colours.)\n\nRichter was a pioneer in painting from nature, in both practice and theory. In 1812 he painted the oil \"Christ Giving Sight to the Blind\", in bright sunlight on the roof of his house in Newman Street; the work was purchased by the trustees of the British Institution for 500 guineas and later presented to the New Church, Greenwich, completed in 1825, where it was installed as an altarpiece. A second version, attempting to improve on its truth to nature, was exhibited four years later. Then, in 1817, he published the pamphlet Daylight: a recent discovery in the art of painting, with hints on the philosophy of the fine arts, and on that of the human mind, as first dissected by Immanuel Kant. His more general ideas, including his approach to color and use of models of his compositions in clay or wax, influenced other painters, notably the miniature painter James Holmes. Daylight combined his artistic interests with his study of metaphysical philosophy. Richter also wrote the article 'Metaphysics' in the Encyclopaedia Londonensis and a paper, German Transcendentalism (1855).\n\nIn 1821 Richter was again elected to the Society of Painters in Water Colours, though his membership and the frequency of his exhibits varied through the decade. But from 1829 until his death he was both a member and a frequent exhibitor. The subjects of his most ambitious paintings of this later phase were taken from Shakespeare. He was also known for drawings and engravings of characters of Sir Walter Scott.\n\nHis work became highly popular through reproductive engravings and, from 1828, through the illustrations he produced for annuals such as the Forget-me-Not and Fisher's Drawing Room Scrapbook for 1833; The painting, \"The School in an Uproar\", was reproduced in several variations and even printed on pocket handkerchiefs. He reportedly produced as many as 150 works in the form of drawings, engravings and paintings. Some examples of his work are in the British Museum and in private collections. The painting, \"The Love Letter\" was shows two characters taken from Sir Walter Scott's The Antiquary and was painted between 1816–1833. A poem of the same name was authored by Letitia Elizabeth Landon who was noted for creating literary pieces based on drawings and paintings and well known during that period in the UK. The painting depicts the Mrs Mailsetter in an admonishing pose and Jenny Caxon the recipient of a love letter from her older, male beau, Lieutenant Richard Taffril, who was away at sea. An engraving by Charles Rolls of the painting and the poem were published side-by-side in Fisher's Drawing Room Scrapbook for 1833 (London, UK Fisher Jackson). Rolls contributed by engraving a number of drawings and paintings done by Richter. It is believed that the painting may have been based on the drawing and subsequent engraving of a different view of the scene. This engraving has been referenced by the titles \"One Peep Was Enough\" and \"At the Post Office\" showing Mrs Mailsetter, Jenny and two other characters; Mrs Heukbane and Mrs Shortcake.\n\nRichter was married twice: first, at Marylebone on 9 July 1808, to Elizabeth née Smith; and second, at Marylebone on 2 May 1818, to Charlotte Sophia née Edson (d. 1862). He had at least two sons and two daughters. On 8 April 1857, Henry James Richter died at his home at 101 Lisson Grove, London. He was 85. At the time of his death, he was translating a metaphysical work by J. S. Beck, a former student of Kant. His daughter Henrietta Sophia Richter (1813–1896) was a successful amateur portrait painter, who exhibited at the Royal Academy from 1842 to 1849. His son Henry Constantine Richter (1821–1902) became a well recognised draftsman and lithographer, noted for his illustrations of birds and mammals which were reproduced in the works of the English ornithologist and bird artist, John Gould.\n"}
{"id": "182970", "url": "https://en.wikipedia.org/wiki?curid=182970", "title": "Immorality", "text": "Immorality\n\nImmorality is the violation of moral laws, norms or standards. Immorality is normally applied to people or actions, or in a broader sense, it can be applied to groups or corporate bodies, beliefs, religions, and works of art.\n\nAristotle saw many vices as excesses or deficits in relation to some virtue, as cowardice and rashness relate to courage. Some attitudes and actionssuch as envy, murder, and thefthe saw as wrong in themselves, with no question of a deficit/excess in relation to the mean.\n\nImmorality is often closely linked with both religion and sexuality. Max Weber saw rational articulated religions as engaged in a long-term struggle with more physical forms of religious experience linked to dance, intoxication and sexual activity. Durkheim pointed out how many primitive rites culminated in abandoning the distinction between licit and immoral behavior.\n\nFreud's dour conclusion was that \"In every age immorality has found no less support in religion than morality has\".\n\nCoding of sexual behavior has historically been a feature of all human societies, as too; has been the policing of breaches of its moressexual immoralityby means of formal and informal social control. Interdictions and taboos among primitive societies were arguably no less severe than in traditional agrarian societies. In the latter, the degree of control might vary from time to time and region to region, being least in urban settlements; however, only the last three centuries of intense urbanisation, commercialisation and modernisation have broken with the restrictions of the pre-modern world, in favor of a successor society of fractured and competing sexual codes and subcultures, where sexual expression is integrated into the workings of the commercial world.\n\nNevertheless, while the meaning of sexual immorality has been drastically redefined in recent times, arguably the boundaries of what is acceptable remain publicly policed and as highly charged as ever, as the decades-long debates in the US over reproductive rights after \"Roe v. Wade\", or 21st-century controversy over child images on Wikipedia and Amazon would tend to suggest.\n\nMichel Foucault considered that the modern world was unable to put forward a coherent moralityan inability underpinned philosophically by emotivism. Nevertheless, modernism has often been accompanied by a cult of immorality, as for example when John Ciardi acclaimed Naked Lunch as \"a monumentally moral descent into the hell of narcotic addiction\".\n\nPsychoanalysis received much early criticism for being the unsavory product of an immoral townVienna; psychoanalysts for being both unscrupulous and dirty-minded.\n\nFreud himself however was of the opinion that \"anyone who has succeeded in educating himself to truth about himself is permanently defended against the danger of immorality, even though his standard of morality may differ\".\n\n\n"}
{"id": "1659824", "url": "https://en.wikipedia.org/wiki?curid=1659824", "title": "Impartiality", "text": "Impartiality\n\nImpartiality (also called evenhandedness or fair-mindedness) is a principle of justice holding that decisions should be based on objective criteria, rather than on the basis of bias, prejudice, or preferring the benefit to one person over another for improper reasons.\n\nEuropean Union law refers in the Charter of Fundamental Rights of the European Union to:\n\n\n\n\n\n\n\n\n"}
{"id": "50355", "url": "https://en.wikipedia.org/wiki?curid=50355", "title": "Informed consent", "text": "Informed consent\n\nInformed consent is a process for getting permission before conducting a healthcare intervention on a person, or for disclosing personal information. A health care provider may ask a patient to consent to receive therapy before providing it, or a clinical researcher may ask a research participant before enrolling that person into a clinical trial. Informed consent is collected according to guidelines from the fields of medical ethics and research ethics.\n\nAn informed consent can be said to have been given based upon a clear appreciation and understanding of the facts, implications, and consequences of an action. Adequate informed consent is rooted in respecting a person’s dignity. To give informed consent, the individual concerned must have adequate reasoning faculties and be in possession of all relevant facts. Impairments to reasoning and judgment that may prevent informed consent include basic intellectual or emotional immaturity, high levels of stress such as posttraumatic stress disorder (PTSD) or a severe intellectual disability, severe mental disorder, intoxication, severe sleep deprivation, Alzheimer's disease, or being in a coma.\n\nObtaining informed consent is not always required. If an individual is considered unable to give informed consent, another person is generally authorized to give consent on his behalf, e.g., parents or legal guardians of a child (though in this circumstance the child may be required to provide informed assent) and conservators for the mentally disordered, or consent can be assumed through the doctrine of implied consent, e.g., when an unconscious person will die without immediate medical treatment.\n\nIn cases where an individual is provided insufficient information to form a reasoned decision, serious ethical issues arise. Such cases in a clinical trial in medical research are anticipated and prevented by an ethics committee or Institutional Review Board.\n\nInformed Consent Form Templates can be found on the World Health Organization Website for practical use.\n\nInformed consent can be complex to evaluate, because neither expressions of consent, nor expressions of understanding of implications, necessarily mean that full adult consent was in fact given, nor that full comprehension of relevant issues is internally digested. Consent may be implied within the usual subtleties of human communication, rather than explicitly negotiated verbally or in writing. In some cases consent cannot legally be possible, even if the person protests he does indeed understand and wish. There are also structured instruments for evaluating capacity to give informed consent, although no ideal instrument presently exists.\n\nThus, there is always a degree to which informed consent must be assumed or inferred based upon observation, or knowledge, or legal reliance. This especially is the case in sexual or relational issues. In medical or formal circumstances, explicit agreement by means of signature—normally relied on legally—regardless of actual consent, is the norm. This is the case with certain procedures, such as a \"do not resuscitate\" directive that a patient signed before onset of their illness. \n\nBrief examples of each of the above:\n\nFor an individual to give valid informed consent, three components must be present: disclosure, capacity and voluntariness.\n\n\nWaiver of the consent requirement may be applied in certain circumstances where no foreseeable harm is expected to result from the study or when permitted by law, federal regulations, or if an ethical review committee has approved the non-disclosure of certain information.\n\nBesides studies with minimal risk, waivers of consent may be obtained in a military setting. According to 10 USC 980, the United States Code for the Armed Forces, Limitations on the Use of Humans as Experimental Subjects, a waiver of advanced informed consent may be granted by the Secretary of Defense if a research project would:\n\nWhile informed consent is a basic right and should be carried out effectively, if a patient is incapacitated due to injury or illness, it is still important that patients benefit from emergency experimentation. The Food and Drug Administration (FDA) and the Department of Health and Human Services (DHHS) joined together to create federal guidelines to permit emergency research, without informed consent. However, they can only proceed with the research if they obtain a waiver of informed consent (WIC) or an emergency exception from informed consent (EFIC).\n\n\"Informed consent\" is a technical term first used by attorney, Paul G. Gebhard, in a medical malpractice United States court case in 1957. In tracing its history, some scholars have suggested tracing the history of checking for any of these practices:\nThese practices are part of what constitutes informed consent, and their history is the history of informed consent. They combine to form the modern concept of informed consent—which rose in response to particular incidents in modern research. Whereas various cultures in various places practiced informed consent, the modern concept of informed consent was developed by people who drew influence from Western tradition.\n\nHistorians cite a series of medical guidelines to trace the history of informed consent in medical practice.\n\nThe Hippocratic Oath, a 500 BC Greek text, was the first set of Western writings giving guidelines for the conduct of medical professionals. It advises that physicians conceal most information from patients to give the patients the best care. The rationale is a beneficence model for care—the doctor knows better than the patient, and therefore should direct the patient's care, because the patient is not likely to have better ideas than the doctor.\n\nHenri de Mondeville, a French surgeon who in the 14th century, wrote about medical practice. He traced his ideas to the Hippocratic Oath. Among his recommendations were that doctors \"promise a cure to every patient\" in hopes that the good prognosis would inspire a good outcome to treatment. Mondeville never mentioned getting consent, but did emphasize the need for the patient to have confidence in the doctor. He also advised that when deciding therapeutically unimportant details the doctor should meet the patients' requests \"so far as they do not interfere with treatment\".\n\nBenjamin Rush was an 18th-century United States physician who was influenced by the Age of Enlightenment cultural movement. Because of this, he advised that doctors ought to share as much information as possible with patients. He recommended that doctors educate the public and respect a patient's informed decision to accept therapy. There is no evidence that he supported seeking a consent from patients. In a lecture titled \"On the duties of patients to their physicians\", he stated that patients should be strictly obedient to the physician's orders; this was representative of much of his writings. John Gregory, Rush's teacher, wrote similar views that a doctor could best practice beneficence by making decisions for the patients without their consent.\n\nThomas Percival was a British physician who published a book called \"Medical Ethics\" in 1803. Percival was a student of the works of Gregory and various earlier Hippocratic physicians. Like all previous works, Percival's \"Medical Ethics\" makes no mention of soliciting for the consent of patients or respecting their decisions. Percival said that patients have a right to truth, but when the physician could provide better treatment by lying or withholding information, he advised that the physician do as he thought best.\n\nWhen the American Medical Association was founded they in 1847 produced a work called the first edition of the \"American Medical Association Code of Medical Ethics\". Many sections of this book are verbatim copies of passages from Percival's \"Medical Ethics\". A new concept in this book was the idea that physicians should fully disclose all patient details truthfully when talking to other physicians, but the text does not also apply this idea to disclosing information to patients. Through this text, Percival's ideas became pervasive guidelines throughout the United States as other texts were derived from them.\n\nWorthington Hooker was an American physician who in 1849 published \"Physician and Patient\". This medical ethics book was radical demonstrating understanding of the AMA's guidelines and Percival's philosophy and soundly rejecting all directives that a doctor should lie to patients. In Hooker's view, benevolent deception is not fair to the patient, and he lectured widely on this topic. Hooker's ideas were not broadly influential.\n\nHistorians cite a series of human subject research experiments to trace the history of informed consent in research.\n\nThe U.S. Army Yellow Fever Commission “is considered the first research group in history to use consent forms.” In 1900, Major Walter Reed was appointed head of the four man U.S. Army Yellow Fever Commission in Cuba that determined mosquitoes were the vector for yellow fever transmission. His earliest experiments were probably done without formal documentation of informed consent. In later experiments he obtained support from appropriate military and administrative authorities. He then drafted what is now “one of the oldest series of extant informed consent documents.” The three surviving examples are in Spanish with English translations; two have an individual’s signature and one is marked with an X.\n\n\"Tearoom Trade\" is the name of a book by American psychologist Laud Humphreys. In it he describes his research into male homosexual acts. In conducting this research he never sought consent from his research subjects and other researchers raised concerns that he violated the right to privacy for research participants.\n\nThe Milgram experiment is the name of a 1961 experiment conducted by American psychologist Stanley Milgram. In the experiment Milgram had an authority figure order research participants to commit a disturbing act of harming another person. After the experiment he would reveal that he had deceived the participants and that they had not hurt anyone, but the research participants were upset at the experience of having participated in the research. The experiment raised broad discussion on the ethics of recruiting participants for research without giving them full information about the nature of the research.\n\nChester M. Southam used HeLa cells to inject into cancer patients and Ohio State Penitentiary inmates without informed consent to determine if people could become immune to cancer and if cancer could be transmitted.\n\nThe doctrine of informed consent relates to professional negligence and establishes a breach of the duty of care owed to the patient (see duty of care, breach of the duty, and respect for persons). The doctrine of informed consent also has significant implications for medical trials of medications, devices, or procedures.\n\nUntil 2015 in the United Kingdom and in countries such as Malaysia and Singapore, informed consent in medical procedures requires proof as to the standard of care to expect as a recognised standard of acceptable professional practice (the Bolam Test), that is, what risks would a medical professional usually disclose in the circumstances (see Loss of right in English law). Arguably, this is \"sufficient consent\" rather than \"informed consent.\" The UK has since departed from the Bolam test for judging standards of informed consent, due to the landmark ruling in \"Montgomery v Lanarkshire Health Board\". This moves away from the concept of a reasonable physician and instead uses the standard of a reasonable patient, and what risks an individual would attach significance to.\n\nMedicine in the United States, Australia, and Canada also takes this patient-centric approach to \"informed consent.\" Informed consent in these jurisdictions requires healthcare providers to disclose significant risks, as well as risks of particular importance to that patient. This approach combines an objective (a hypothetical reasonable patient) and subjective (this particular patient) approach.\n\nThe doctrine of informed consent should be contrasted with the general doctrine of medical consent, which applies to assault or battery. The consent standard here is only that the person understands, in general terms, the nature of and purpose of the intended intervention. As the higher standard of informed consent applies to negligence, not battery, the other elements of negligence must be made out. Significantly, causation must be shown: That had the individual been made aware of the risk he would not have proceeded with the operation (or perhaps with that surgeon).\n\nOptimal establishment of an informed consent requires adaptation to cultural or other individual factors of the patient. For example, people from Mediterranean and Arab appear to rely more on the context of the delivery of the information, with the information being carried more by who is saying it and where, when, and how it's being said, rather than \"what\" is said, which is of relatively more importance in typical \"Western\" countries.\n\nThe informed consent doctrine is generally implemented through good healthcare practice: pre-operation discussions with patients and the use of medical consent forms in hospitals. However, reliance on a signed form should not undermine the basis of the doctrine in giving the patient an opportunity to weigh and respond to the risk. In one British case, a doctor performing routine surgery on a woman noticed that she had cancerous tissue in her womb. He took the initiative to remove the woman's womb; however, as she had not given informed consent for this operation, the doctor was judged by the General Medical Council to have acted negligently. The council stated that the woman should have been informed of her condition, and allowed to make her own decision.\n\nTo capture and manage informed consents, hospital management systems typically use paper-based consent forms which are scanned and stored in a document handling system after obtaining the necessary signatures. Hospital systems and research organizations are adopting an electronic way of capturing informed consents to enable indexing, to improve comprehension, search and retrieval of consent data, thus enhancing the ability to honor to patient intent and identify willing research participants. More recently, Health Sciences South Carolina, a statewide research collaborative focused on transforming healthcare quality, health information systems and patient outcomes, developed an open-source system called Research Permissions Management System (RPMS).\n\nThe ability to give informed consent is governed by a general requirement of competency. In common law jurisdictions, adults are presumed competent to consent. This presumption can be rebutted, for instance, in circumstances of mental illness or other incompetence. This may be prescribed in legislation or based on a common-law standard of inability to understand the nature of the procedure. In cases of incompetent adults, a health care proxy makes medical decisions. In the absence of a proxy, the medical practitioner is expected to act in the patient's best interests until a proxy can be found.\n\nBy contrast, 'minors' (which may be defined differently in different jurisdictions) are generally presumed incompetent to consent, but depending on their age and other factors may be required to provide Informed assent. In some jurisdictions (e.g. much of the U.S.), this is a strict standard. In other jurisdictions (e.g. England, Australia, Canada), this presumption may be rebutted through proof that the minor is ‘mature’ (the ‘Gillick standard’). In cases of incompetent minors, informed consent is usually required from the parent (rather than the 'best interests standard') although a parens patriae order may apply, allowing the court to dispense with parental consent in cases of refusal.\n\nResearch involving deception is controversial given the requirement for informed consent. Deception typically arises in social psychology, when researching a particular psychological process requires that investigators deceive subjects. For example, in the Milgram experiment, researchers wanted to determine the willingness of participants to obey authority figures despite their personal conscientious objections. They had authority figures demand that participants deliver what they thought was an electric shock to another research participant. For the study to succeed, it was necessary to deceive the participants so they believed that the subject was a peer and that their electric shocks caused the peer actual pain.\n\nNonetheless, research involving deception prevents subjects from exercising their basic right of autonomous informed decision-making and conflicts with the ethical principle of respect for persons.\n\nThe Ethical Principles of Psychologists and Code of Conduct set by the American Psychological Association says that psychologists may conduct research that includes a deceptive compartment only if they can both justify the act by the value and importance of the study's results and show they could not obtain the results by some other way. Moreover, the research should bear no potential harm to the subject as an outcome of deception, either physical pain or emotional distress. Finally, the code requires a debriefing session in which the experimenter both tells the subject about the deception and gives subject the option of withdrawing the data.\n\nIn some U.S. states, informed consent laws (sometimes called \"right to know\" laws) require that a woman seeking an elective abortion receive information from the abortion provider about her legal rights, alternatives to abortion (such as adoption), available public and private assistance, and other information specified in the law, before the abortion is performed. Other countries with such laws (e.g. Germany) require that the information giver be properly certified to make sure that no abortion is carried out for the financial gain of the abortion provider and to ensure that the decision to have an abortion is not swayed by any form of incentive.\n\nSome informed consent laws have been criticized for allegedly using \"loaded language in an apparently deliberate attempt to 'personify' the fetus,\" but those critics acknowledge that \"most of the information in the [legally mandated] materials about abortion comports with recent scientific findings and the principles of informed consent\", although \"some content is either misleading or altogether incorrect.\"\n\nAs children often lack the decision making ability or legal power (competence) to provide true informed consent for medical decisions, it often falls on parents or legal guardians to provide \"informed permission\" for medical decisions. This \"consent by proxy\" usually works reasonably well, but can lead to ethical dilemmas when the judgment of the parents or guardians and the medical professional differ with regard to what constitutes appropriate decisions \"in the best interest of the child\". Children who are legally emancipated, and certain situations such as decisions regarding sexually transmitted diseases or pregnancy, or for unemancipated minors who are deemed to have medical decision making capacity, may be able to provide consent without the need for parental permission depending on the laws of the jurisdiction the child lives in. The American Academy of Pediatrics encourages medical professionals also to seek the assent of older children and adolescents by providing age appropriate information to these children to help empower them in the decision making process.\n\nResearch on children has benefited society in many ways. The only effective way to establish normal patterns of growth and metabolism is to do research on infants and young children. When addressing the issue of informed consent with children, the primary response is parental consent. This is valid, although only legal guardians are able to consent for a child, not adult siblings. Additionally, parents may not order the termination of a treatment that is required to keep a child alive, even if they feel it is in the best interest. Guardians are typically involved in the consent of children, however a number of doctrines have developed that allow children to receive health treatments without parental consent. For example, emancipated minors may consent to medical treatment, and minors can also consent in an emergency.\n\nInformed consent is part of the ethical clinical research as well, in which a human subject voluntarily confirms his or her willingness to participate in a particular clinical trial, after having been informed of all aspects of the trial that are relevant to the subject's decision to participate. Informed consent is documented by means of a written, signed, and dated informed consent form.\nIn medical research, the Nuremberg Code set a base international standard in 1947, which continued to develop, for example in response to the ethical violation in the Holocaust. Nowadays, medical research is overseen by an ethics committee that also oversees the informed consent process.\n\nAs the medical guidelines established in the Nuremberg Code were imported into the ethical guidelines for the social sciences, informed consent became a common part of the research procedure. However, while informed consent is the default in medical settings, it is not always required in the social science. Here, research often involves low or no risk for participants, unlike in many medical experiments. Second, the mere knowledge that they participate in a study can cause people to alter their behavior, as in the Hawthorne Effect: \"In the typical lab experiment, subjects enter an environment in which they are keenly aware that their behavior is being monitored, recorded, and subsequently scrutinized.\"\nIn such cases, seeking informed consent directly interferes with the ability to conduct the research, because the very act of revealing that a study is being conducted is likely to alter the behavior studied. List exemplifies the potential dilemma that can result: \"if one were interested in exploring whether, and to what extent, race or gender influences the prices that buyers pay for used cars, it would be difficult to measure accurately the degree of discrimination among used car dealers who know that they are taking part in an experiment.\" In cases where such interference is likely, and after careful consideration, a researcher may forgo the informed consent process. This is commonly done after weighting the risk to study participants versus the benefit to society and whether participants are present in the study out of their own wish and treated fairly. Researchers often consult with an ethics committee or institutional review board to render a decision.\n\nThe birth of new online media, such as social media, has complicated the idea of informed consent. In an online environment people pay little attention to Terms of Use agreements and can subject themselves to research without thorough knowledge. This issue came to the public light following a study conducted by Facebook Inc. in 2014, and published by that company and Cornell University. Facebook conducted a study where they altered the Facebook News Feeds of roughly 700,000 users to reduce either the amount of positive or negative posts they saw for a week. The study then analyzed if the users status updates changed during the different conditions. The study was published in the Proceedings of the National Academy of Sciences.\n\nThe lack of informed consent led to outrage among many researchers and users. Many believed that by potentially altering the mood of users by altering what posts they see, Facebook put at-risk individuals at higher dangers for depression and suicide. However, supports of Facebook claim that Facebook details that they have the right to use information for research in their terms of use. Others say the experiment is just a part of Facebook’s current work, which alters News Feeds algorithms continually to keep people interested and coming back to the site. Others pointed out that this specific study is not along but that news organizations constantly try out different headlines using algorithms to elicit emotions and garner clicks or Facebook shares. They say this Facebook study is no different than things people already accept. Still, others say that Facebook broke the law when conducting the experiment on user that didn’t give informed consent.\n\nThe Facebook study controversy raises numerous questions about informed consent and the differences in the ethical review process between publicly and privately funded research. Some say Facebook was within its limits and others see the need for more informed consent and/or the establishment of in-house private review boards.\n\nOther, long-standing controversies underscore the role for conflicts of interest among medical school faculty and researchers. For example, coverage of University of California (UC) medical school faculty members has included news of ongoing corporate payments to researchers and practitioners from companies that market and produce the very devices and treatments they recommend to patients. Robert Pedowitz, the former chairman of UCLA’s orthopedic surgery department, reported concern that his colleague’s financial conflicts of interest could negatively affect patient care or research into new treatments. In a subsequent lawsuit about whistleblower retaliation, the University provided a $10 million settlement to Pedowitz while acknowledging no wrongdoing. Consumer Watchdog, an oversight group, observed that University of CA policies were “either inadequate or unenforced…Patients in UC hospitals deserve the most reliable surgical devices and medication…and they shouldn’t be treated as subjects in expensive experiments.” Other UC incidents include taking the eggs of women for implantation into other women without consent and injecting live bacteria into human brains, resulting in potentially premature deaths.\n"}
{"id": "50427403", "url": "https://en.wikipedia.org/wiki?curid=50427403", "title": "International date line in Judaism", "text": "International date line in Judaism\n\nThe international date line in Judaism is used to demarcate the change of one calendar day to the next in the Jewish calendar. The Jewish calendar defines days as running from sundown to sundown rather than midnight to midnight. So in the context of Judaism, an international date line demarcates when the line of sundown moving across the Earth's surface stops being the sundown ending and starting one day and starts being the sundown ending and starting the following day.\n\nHowever, the conventional International Date Line is a relatively recent geographic and political construct whose exact location has moved from time to time depending on the needs of different interested parties. While it is well-understood why the conventional date line is located in the Pacific Ocean, there are not really objective criteria for its exact placement within the Pacific. In that light, it cannot be taken for granted that the conventional International Date Line can (or should) be used as a date line under Jewish law. In practice, within Judaism the \"halakhic\" date line is similar to, but not necessarily identical with, the conventional Date Line, and the differences can have consequences under religious law.\n\nMany of the opinions about the \"halakhic\" date line are structured as a response to the question of what days someone should observe as Shabbat and Jewish holidays. Shabbat occurs every seven days at any location on earth. It is normally thought to occur on Saturday—or more precisely, from Friday at sundown to Saturday at nightfall. But if the \"halakhic\" date line is not identical to the conventional Date Line, it is possible that what is Saturday with respect to the conventional Date Line is not Saturday with respect to the \"halakhic\" date line, at least in some places.\n\nThere are several opinions regarding where exactly the \"halakhic\" date line should be according to Jewish law, and at least one opinion that says that no \"halakhic\" date line really exists.\n\n1. \"90 degrees east of Jerusalem.\" The concept of a \"halakhic\" date line is mentioned in the \"Baal HaMeor,\" a 12th-century Talmudic commentary, which seems to indicate that the day changes in an area where the time is six hours ahead of Jerusalem (90 degrees east of Jerusalem, about 125.2°E, a line now known to run through Australia, the Philippines, China and Russia). This line, which he refers to as the \"K'tzai Hamizrach\" (the easternmost line), is used to calculate the day of Rosh Hashanah, the Jewish New Year. According to some sources it is alluded to in both the Babylonian Talmud (Rosh Hashanah and Eruvin) and in the Jerusalem Talmud. The \"Kuzari\" of Yehuda Halevi, also a 12th-century work, seems to agree with this ruling.\n\nLater decisors like the \"Chazon Ish\" (twentieth century) fundamentally agree with this ruling. However, they recognize practical issues associated with the pure use of a line of longitude for this purpose. As an example, 125.2°E passes directly through Dongfeng Street in Changchun, China. If this line of longitude were used strictly, people could simply avoid Shabbat altogether by crossing the street. To prevent that, the Chazon Ish rules that the contiguous land masses to the east of that line of longitude are considered secondary (\"tafel\") to the land masses west of that line. As a result, he rules that the date line runs along 125.2°E when over water, but curves around the eastern coast of mainland Asia and Australia. By this view, Russia, China and Australia are west of the date line and observe Shabbat on local Saturday. Japan, New Zealand and Tasmania are east of the date line and should observe Shabbat on local Sunday, as defined by the conventional International Date Line. By this view, the Philippines and Indonesia would have portions west of the line and portions east of the line.\n\n2. \"180 degrees east of Jerusalem.\" Rabbi Yechiel Michel Tucazinsky ruled that the International Date Line is 180 degrees east of Jerusalem. That would mean that the date line, rather than being near 180°, would be at 144.8°W. By this view, places east of the conventional International Date Line but west of 144.8°W—Alaska, Hawaii and a variety of archipelagos in the Pacific—would observe Shabbat on the local Friday instead of the local Saturday.\n\nIt is possible (but not certain) under this view to apply the principal of \"tafel\" described above as well. In that event, mainland Alaska would be east of the date line, but the Aleutian Islands would be west of the date line.\n\n3. \"Mid-Pacific.\" A variety of decisors rule that the date line runs in the middle of the Pacific Ocean, close to (but not necessarily the same as) the conventional International Date Line. According to this point of view, all of the major populated areas of the Pacific (such as New Zealand, Japan, Alaska and Hawaii) observe Shabbat on local Saturday (that is, consistent with the conventional International Date Line). Only certain Pacific islands, generally having few or no permanent Jewish residents, might not observe Shabbat on local Saturday.\n\n4. \"Following local custom/There is no fixed date line.\" According to Rabbi Menachem Mendel Kasher, there is no clear tradition or Talmudic source dictating any of the preceding opinions as binding. For that reason, and consistent with a responsum of the Radbaz, Rabbi Kasher starts with the default law that a Jew not knowing the proper day for Shabbat should count days from the last time s/he observed Shabbat, and that every seven days is Shabbat. In his view, established Jewish communities are presumed to have fixed their calendars according to this principle. Therefore, Shabbat in an established community is whatever day the community has established. Rabbi Isser Zalman Meltzer and Rabbi Zvi Pesach Frank apparently agree with this position. This position does not in and of itself require a formal date line to be established, and Rabbi Kasher does not seem to think that it is necessary to do so. But the \"de facto\" result of this position is consistent with the conventional International Date Line, at least anywhere there is an established Jewish community.\n\nIn practice, the conventional International Date Line (or another mid-Pacific line near it) is the \"de facto\" date line under Jewish law, at least for established Jewish communities. The communities of Japan, New Zealand, Hawaii, and French Polynesia all observe Shabbat on local Saturday \"(i.e.,\" Friday night until Saturday night). No known Jewish community observes Shabbat on a day other than local Saturday. However, that practical conclusion is reached in two different ways, resulting in somewhat different practice patterns in each case.\n\nFollowing local custom/There is no fixed date line. As noted above, according to this point of view, Shabbat is simply observed on the date previously established as Shabbat by the local community—uniformly, local Saturday—without any need for any further observance. This appears to be the default practice for residents of such places as Japan, New Zealand and Hawaii. At minimum, it is difficult to find evidence of other practices by residents of those areas.\n\nEstablishment of a date line by a majority among three halakhic positions. The travelers' guide of the Star-K kosher supervision service, compiled according to the rulings of its rabbinic administrator, Rabbi Moshe Heinemann, uses the following approach, which is also cited by others. According to this approach, the first three numbered sections above constitute three valid, parallel, \"halakhic\" rulings. Shabbat is consequently fully observed on whichever day is consistent with the majority view among those opinions (two out of three). However, out of respect to the minority view of the third ruling, and with an eye toward not desecrating Shabbat, Torah-level prohibitions are to be avoided on the day consistent with the minority view, although that day is otherwise considered a weekday. According to this rule, practice is as follows: \nThe Star-K's international kosher supervision staff follows this approach, and there is evidence that some other travelers also do. Authorities suggesting this approach often advise travelers to avoid the zone of doubt entirely near weekends, or to consult with a competent rabbinical authority directly.\n\nThe issues discussed in the previous section apply \"per se\" to individuals or communities in fixed locations. However, the act of crossing the date line (wherever it may be drawn) introduces a number of additional issues under Jewish law. Questions potentially affected include:\n\n\nIn some cases, crossing the date line (wherever it may be drawn) has a specific impact on practice or prohibitions under Jewish law. In others, an individual's count of days (by the experience of sunset and sunrise) is the determining factor, regardless of the crossing of the date line. Details around specific questions, cases and rulings of Jewish law are beyond the scope of this article.\n\nBefore Israeli astronaut Ilan Ramon flew on the Space Shuttle \"Columbia\" in 2003, he decided (after consultation with rabbis) to observe Shabbat according to time in his last residence, Cape Canaveral, since he would be crossing the date line and observing sunset many times per day. Judith Resnik, the first American Jewish astronaut in space, lit (electronic) Shabbat candles according to the time in Houston, TX, her home and the location of Mission Control.\n\n"}
{"id": "41702608", "url": "https://en.wikipedia.org/wiki?curid=41702608", "title": "Jivatva", "text": "Jivatva\n\nJivatva (Sanskrit: जीवत्व) means – the state of life or the state of the individual soul. Jivatva is the state of life of the Jiva (transmigratory individual soul), the living entity, which is a particular manifestation of Atman, the embodied being limited to psycho-physical states, and the source of avidya that suffers (repeated) transmigration as result of its actions. Until ignorance ceases the Jiva remains caught in experience of the results of actions bringing merit and demerit, and in the state of individuality (\"jivatva\") (Brahma Sutra I.iv.6), and so long as the connection with the intellect as conditioning adjunct lasts, so long the individuality and transmigration of soul lasts (Brahma Sutra II.iii.30).\n\nThe Jivatva-bhavana is the feeling of limitation induced by the body, mind and intellect. The nature of Jivatva is adventitious, dependent on external factors; Jivatva is accidental and not an essential nature of Brahman. It is illusorily superimposed on Brahman. The Atman is the witness (saksin) of the activities of antahkarana (inward intellect) composed of buddhi (intellect), ahankara (I-faculty) and manas (mind). Viraj has one Jiva and Hiranyagarbha another, because it is commonly known that when bodies are different the Jivas are different, but it is possible to have one Jiva for the past and future bodies, a difference in the bodies does not indicate difference in respect to the Jiva.\n\nAccording to Dvaitadvaita (dualism) Brahman and Jiva are different entities; that God, Soul and the Universe are three separate entities with the former governing the latter two. The Jiva (individual soul that lives in the world) is one of the three categories of realities, the other two being Jagat (the Universe or the world) and Brahman (the Universal Soul and the substratum behind the Jagat and the Jivas).The soul can migrate to the heaven and live with God. According to Vishishtadvaita (qualified dualism), God alone exists who is formless. The Jagat and the Jivas form his body and the soul becomes liberated when it realizes that it is a part of God. According to Advaita Vedanta (non-dualism), Brahman is the ultimate supreme sole Reality beyond names and forms. Brahman, which is Truth, Consciousness and Bliss, and the soul are non-different, identical, unchangeable and eternal.\n\nAccording to the dvaita schools, jiva is essentially an eternal spiritual entity (ajada-dravya) whose essence is constituted by knowledge (jnana). As jnana-svarupa it is a sentient being (cetna) and self-revealing (svayam-prakasa), as the knower or subject of knowledge (jnata) and as the agent of action (karta) it experiences both pleasure and pain (bhokta). It is monadic in character (anu) unlike Ishvara who is all-pervasive (vibhu). Jiva is a part (amsa) of Paramatman, it is supported (adheya) by, controlled (niyamya) and dependent (sesa) on the Paramatman. It is the finite individual who experiences the waking (jagarita), sleep (svapna) and dreamless sleep (susupti) states of mind which can be successive, but rarely the Fourth (turiya) which is being-in-Brahman. On account of the adhyasa, Jiva interacts with the objects and other Jivas with a sense of doer-ship etc.; and experiences samsara; liberation from samsara is called moksha.\n\nIn the Katha Upanishad the Jiva, the individual soul, and Brahman, the Universal Soul, are regarded as being at par with each other as enjoying equally the fruits of their action. In the Mundaka Upanishad only the individual soul is described as tasting of the fruits of action, the Universal Soul described simply as the on-looker. In the Svetasvatra Upanishad the individual soul enjoys the unborn Prakrti consisting of three qualities, which the Universal Soul leaves off. This is with regard to the relation of the Two Souls first dealt with by the Rig Veda in mantra I.164.17 which reads:-\n\nAnd, adapting which ideation and imagery the sage of the Mundaka Upanishad (III.1.2) proceeds to tell us that:-\n\nJiva is limited by the inner sense organ, being limited it is distinct from the substratum consciousness of objects which is the all-pervasive consciousness (saksi). It always feels its distinction from God, because Brahman is not the object of ordinary knowledge. Jiva is the locus (asraya) of avidya. The identification between Jiva, a false entity, and Brahman, occurs only when the Jiva aspect of the Self is totally eliminated by true knowledge of the real nature of the Self dispelling the primordial avidya. \n\nSvatantryavada is the doctrine of the absolute sovereignty and freedom of the Divine Will to express and manifest itself in any way it likes; Svatantrya makes the moveable and the immoveable objects appear as separate though in essence they are not separate from samvit (Universal Consciousness) and which does not conceal the nature of the Supreme. From the point of its manifestation, it is known as Abhasavada. Abhasavada is the theory of appearance, the creation theory of the Saiva and the Sakta schools according to which theory the universe consists of appearances which are all real as the aspects of the ultimate reality; the world is an abhasa (prakasa or light) of Shiva, it is not Maya. In the Advaita Vedanta this is the theory according to which the Jiva is the illusory appearance of Brahma-consciousness.\n\nThe theory of Abhasavada finds its roots in Brahma Sutra II.iii.50 which reads – \n\nThis theory was advocated by Suresvara in which the Jivas are as real as Brahman, they being primary appearances in and through avidya, while the objects of the world are unreal, they being secondary appearances, mere reflections of the primary appearances. The Vivarna School upholds the theory of Bimba-pratibimbavada or theory of reflection, in which the Absolute Reality, reflected in upadhis, appears as numerous selves due to the intervening adjuncts and their cause Avidya. The Bhamati School of Vacaspati Misra upholds the Avacchedavada, the theory of limitation, in which the jivas are Brahman Itself but appearing as though limited by adjuncts such as the mind-body-complex.\n\nAccording to Sankara, the Jivatva of the Jiva is a consequence of the Jiva’s false sense of identity i.e. one-ness, with the body, making Jiva an empirical individual. The sense of bodied being-hood (sasariratvam) of the Jiva is entirely due to false conviction (mithyapratiyayanimittantvat), the truly enlightened person is bodyless even when living in this life and in this body. Avidya or ignorance consists in the wrong identification of the Self with the psycho-physical complex called the individual’s body and in the development of the self-sense (atmabhimana) in the bodied being; this is the upadhi cutting into the very nature of the Jiva. The apparent modification is an epistemic fact, and the totality of cosmic plurality is also an epistemic fact. The apparently substantial Jivatva is an offshoot of avidya sustained and nourished by mithyajnana. Jivatva, the phenomenal individuality, although beginning-less, is terminable (santa) in the case of one getting release and gaining Brahmatva. The Jivatva of the Jiva is Jiva’s limitedness. The Jivahood of the atman (the individualization of the soul) is unreal, it is merely an imagination caused by the delusion of buddhi, and vanishes with the annulment of the delusion that comes about by the realization of one’s real nature. \n"}
{"id": "1212980", "url": "https://en.wikipedia.org/wiki?curid=1212980", "title": "John Roseboro", "text": "John Roseboro\n\nJohn Junior Roseboro (May 13, 1933 – August 16, 2002) was an American professional baseball player and coach. He played as a catcher in Major League Baseball from 1957 until 1970, most notably for the Los Angeles Dodgers. Roseboro was a four-time All-Star player and won two Gold Glove Awards for his defensive skills. He was the Dodgers' starting catcher in four World Series with the Dodgers winning three of those. He is considered one of the best defensive catchers of the 1960s. Roseboro was known for his role in one of the most violent incidents in baseball history when Juan Marichal struck him in the head with a bat during a game in .\n\nRoseboro was born in Ashland, Ohio and enrolled at Central State University. He was signed by the Brooklyn Dodgers as an amateur free agent prior to the 1952 season and, began his professional baseball career with the Sheboygan Indians of the Wisconsin State League. He posted a .365 batting average with Sheboygan in 1952 to finish second in the league batting championship. Roseboro missed the 1954 season due to military service but, after five years in the minor leagues, he was promoted to the major leagues in June 1957 at the age of 24.\n\nDuring his first season, Roseboro served as backup catcher for the Dodgers' perennial All-Star catcher, Roy Campanella, and was being groomed to be Campanella's replacement. However, he was promoted to the starting catcher's position ahead of schedule when Campanella had a tragic car accident in January 1958 that ended his career. In his first full season, with the team having moved to Los Angeles, Roseboro hit for a .271 batting average along with 14 home runs and 43 runs batted in. He was also named as a reserve player for the National League in the 1958 All-Star game. In 1959, Roseboro led the league's catchers in putouts and in baserunners caught stealing, helping the Dodgers win the National League Pennant. The Dodgers went on to win the 1959 World Series, defeating the Chicago White Sox in six games.\n\nAfter having a below par season in 1960, Roseboro rebounded in 1961 posting career highs with 18 home runs and 59 runs batted in. He also led the National League catchers in putouts and double plays and finished second in fielding percentage and in assists to earn his first Gold Glove Award as, the Dodgers finished the season in second place behind the Cincinnati Reds. He also earned his second All-Star team berth as a reserve player in the 1961 All-Star game. He won his third All-Star berth as a reserve in the 1962 All-Star game. The Dodgers battled the San Francisco Giants in a tight pennant race during the 1962 season with the two teams ending the season tied for first place and met in the 1962 National League tie-breaker series. The Giants won the three-game series to clinch the National League championship.\n\nRoseboro helped guide the Dodgers' pitching staff to a league leading 2.85 earned run average in 1963 as the Dodgers clinched the National League Pennant by 6 games over the St. Louis Cardinals. Roseboro made his presence felt in the 1963 World Series against the New York Yankees when he hit a three-run home run off Whitey Ford to win the first game of the series. The Dodgers went on to win the series by defeating the Yankees in four straight games. The Dodgers dropped to seventh place in the 1964 season, however Roseboro hit for a career high .287 batting average and led the league's catchers with a 60.4% caught stealing percentage.\n\nRoseboro was involved in a major altercation with Juan Marichal during a game between the Dodgers and San Francisco Giants at Candlestick Park on August 22, . The Giants and the Dodgers had nurtured a heated rivalry with each other dating back to their days together in the New York City market. As the 1965 season neared its climax, the Dodgers were involved in a tight pennant race, entering the game leading the Milwaukee Braves by half a game and the Giants by one and a half games. The incident occurred in the aftermath of the Watts riots near Roseboro's Los Angeles home and while the Dominican Civil War raged in Marichal's home country, so emotions were raw.\n\nMaury Wills led off the game with a bunt single off Marichal, and eventually scored a run when Ron Fairly hit a double. Marichal, a fierce competitor, viewed the bunt as a cheap way to get on base and took umbrage with Wills. When Wills came up to bat in the second inning, Marichal threw a pitch directly at Wills sending him sprawling to the ground. Willie Mays then led off the bottom of the second inning for the Giants and Dodgers' pitcher Sandy Koufax threw a pitch over Mays' head as a token form of retaliation. In the top of the third inning with two outs, Marichal threw a fastball that came close to hitting Fairly, prompting him to dive to the ground. Marichal's act angered the Dodgers sitting in the dugout and home plate umpire Shag Crawford then warned both teams that any further retaliations would not be tolerated.\n\nMarichal came to bat in the third inning expecting Koufax to take further retaliation against him but instead, he was startled when Roseboro's return throw to Koufax after the second pitch either brushed his ear or came close enough for him to feel the breeze off the ball. When Marichal confronted Roseboro about the proximity of his throw, Roseboro came out of his crouch with his fists clenched. Marichal afterwards stated that he thought Roseboro was about to attack him and raised his bat, striking Roseboro at least twice over the head with his bat, opening a two-inch gash that sent blood flowing down the catcher's face that required 14 stitches. Koufax raced in from the mound to attempt to separate them and was joined by the umpires, players and coaches from both teams.\n\nA 14-minute brawl ensued on the field before Koufax, Giants captain Willie Mays and other peacemakers restored order. Marichal was ejected from the game and afterwards, National League president Warren Giles suspended him for eight games (two starts), fined him a then-NL record US$1,750 (), and also forbade him from traveling to Dodger Stadium for the final, crucial two-game series of the season. Roseboro filed a $110,000 damage suit against Marichal one week after the incident, but eventually settled out of court for $7,500.\n\nYears later, Roseboro stated that he was retaliating for Marichal having thrown at Wills. He explained that Koufax would not throw at batters for fear of hurting them due to the velocity of his pitches. He further stated that his throwing close to Marichal's ear was, \"standard operating procedure\", as a form of retribution. Marichal didn't face the Dodgers again until spring training in April 3, . In his first at bat against Marichal after the incident, Roseboro hit a three-run home run. San Francisco General Manager Chub Feeney approached Dodgers General Manager Buzzy Bavasi to attempt to arrange a handshake between Marichal and Roseboro, but Roseboro declined the offer.\n\nDodger fans remained angry with Marichal for several years after the altercation, and reacted unfavorably when he was signed by the Dodgers in 1975. By then, however, Roseboro had forgiven Marichal, and personally appealed to fans to do the same.\n\nThe Dodgers went on to win the 1965 National League Pennant by two games over the Giants. Even though the Giants won the two games against the Dodgers during which Marichal had been suspended, the outcome of the season may have been different without the Giants' pitcher's suspension, as he finished the season with a 22-13 win-loss record. Roseboro once again guided the Dodgers' pitching staff to a league-leading 2.81 earned run average. He caught for two twenty game winners in 1965 with Koufax winning 26 games, while Don Drysdale won 23 games. In his book, \"The Bill James Historical Baseball Abstract\", baseball historian Bill James said the decision to give Joe Torre a National League Gold Glove Award was absurd, stating that he was given the award because of his offensive statistics and that either Roseboro or Tom Haller was more deserving of the award. In the 1965 World Series against the Minnesota Twins, Roseboro contributed a two-run single to win Game 3 of the series as, the Dodgers went on to win the world championship in seven games.\n\nThe Dodgers' pitching staff continued to lead the league in earned run average in 1966 as they battled with the San Francisco Giants and the Pittsburgh Pirates in a tight pennant race. The Dodgers eventually prevailed to win the National League Pennant for a second consecutive year. Roseboro led the league with a career-high 903 putouts and finished second to Joe Torre in fielding percentage to win his second Gold Glove Award. The Dodgers would eventually lose the 1966 World Series, getting swept in four games by the Baltimore Orioles.\n\nThe Dodgers would fall to 8th place in the 1967 season and Roseboro would be traded to the Minnesota Twins after the season with Bob Miller and Ron Perranoski for Mudcat Grant and Zoilo Versalles. While with the Twins, he would be named to his fourth and final All-Star team when he was named as a reserve for the American League team in the 1969 All-Star game. After the season, Roseboro was released by the Twins. He signed as a free agent with the Washington Senators on December 31, 1969 but appeared in only 46 games for the last place Senators. He played in his final major league game on August 11, 1970 at the age of 37.\n\nIn an fourteen-year major league career, Roseboro played in 1,585 games, accumulating 1,206 hits in 4,847 at bats for a .249 career batting average along with 104 home runs, 548 runs batted in and an on-base percentage of .326. He had a .989 career fielding percentage as a catcher. Roseboro caught 112 shutouts during his career, ranking him 19th all-time among major league catchers. He was the catcher for two of Sandy Koufax's four no-hitters and caught more than 100 games in 11 of his 14 major league seasons. Baseball historian Bill James ranked Roseboro 27th all-time among major league catchers.\n\nAfter completing his playing career with Washington, Roseboro coached for the Senators (1971) and California Angels (1972–74). Later, he served as a minor league batting instructor (1977) and catching instructor (1987) for the Dodgers. Roseboro and his wife, Barbara Fouch Roseboro, also owned a Beverly Hills public relations firm.\n\nRoseboro appeared as a plainclothes officer in the 1966 \"Dragnet\" TV movie. He also appeared as himself in the 1962 film \"Experiment in Terror\" along with Don Drysdale and Wally Moon, and as himself in the 1963 \"Mister Ed\" episode \"Leo Durocher Meets Mister Ed.\"\n\nChevrolet was one of the sponsors of the Dodgers' radio coverage in the mid-1960s. The song \"See the USA in Your Chevrolet\", made famous by Dinah Shore in the '50s, was sung in Chevrolet commercials by Roseboro and other Dodger players. Dodger announcer Jerry Doggett once joked that Roseboro was a singer \"whose singing career was destined to go absolutely nowhere.\"\n\nAfter several years of bitterness over their famous altercation, Roseboro and Marichal became friends in the 1980s. Roseboro personally appealed to the Baseball Writers' Association of America not to hold the incident against Marichal after he was passed over for election to the Hall of Fame in his first two years of eligibility. Marichal was elected in 1983, and thanked Roseboro during his induction speech. \"There were no hard feelings on my part,\" Roseboro said, \"and I thought if that was made public, people would believe that this was really over with. So I saw him at a Dodger old-timers' game, and we posed for pictures together, and I actually visited him in the Dominican. The next year, he was in the Hall of Fame. Hey, over the years, you learn to forget things.\"\n\nRoseboro died of heart disease on August 16, 2002 in Los Angeles at age 69. Marichal served as an honorary pallbearer at his funeral. \"Johnny's forgiving me was one of the best things that happened in my life,\" he said, at the service. \"I wish I could have had John Roseboro as my catcher.\"\n\n\n"}
{"id": "416853", "url": "https://en.wikipedia.org/wiki?curid=416853", "title": "Magnate", "text": "Magnate\n\nMagnate, from the Late Latin \"magnas\", a great man, itself from Latin \"magnus\", 'great', designates a noble or other man in a high social position, by birth, wealth or other qualities. In reference to the Middle Ages, the term is often used to distinguish higher territorial landowners and warlords such as counts, earls, dukes, and territorial-princes from the baronage.\n\nIn England, the magnate class went through a change in the later Middle Ages. It had previously consisted of all tenants-in-chief of the crown, a group of more than a hundred families. The emergence of Parliament led to the establishment of a parliamentary peerage that received personal summons, rarely more than sixty families. A similar class in the Gaelic world were the Flatha. In the Middle Ages a bishop sometimes held territory as a magnate, collecting the revenue of the manors and the associated knights' fees.\n\nIn the Tudor period, after Henry VII defeated Richard III at Bosworth Field, Henry made a point of executing or neutralizing as many magnates as possible. Henry VII would make parliament attaint undesirable nobles and magnates, thereby stripping them of their wealth, protection from torture, and power. Henry VII also used the Court of the Star Chamber to have powerful nobles executed. Henry VIII continued this approach in his reign; he inherited a survivalistic mistrust of nobles from his father. Henry VIII ennobled very few men and the ones he did were all \"new men\": novi homines, greatly indebted to him and having very limited power.\n\nThe term was specifically applied to the members (equivalent to British Peers) of the Upper House in the Apostolic Kingdom of Hungary, the \"Főrendiház\" or House of Magnates.\n\nMagnates were a social class of wealthy and influential nobility in the Crown of the Kingdom of Poland and Grand Duchy of Lithuania, and later the Polish–Lithuanian Commonwealth.\n\n \nThe magnates, \"velikaši\", of Serbia in the Middle Ages were noted by their higher titles in relation to those held by the lesser nobles. In the Early and High Middle Ages the highest (most prestigious) title was \"vojvoda\", a military rank and title of governors. During the Serbian Empire (1345–71) the higher court members held titles such as \"despot\", \"sevastokrator\" and \"kesar\". During foreign rule, under the Ottoman Empire, Habsburg Monarchy, Republic of Venice, and later in the Revolution, and Principality the magnates were influential voivodes (though all voivodes were not considered magnates).\n\nIn Spain, since the late Middle Ages the highest class of nobility hold the appellation of Grandee of Spain.\n\nIn Sweden, the wealthiest medieval lords were known as \"storman\" (plural \"stormän\"), \"great men\", a similar description and meaning as the English term magnate.\n\n\n"}
{"id": "4643400", "url": "https://en.wikipedia.org/wiki?curid=4643400", "title": "Majorana fermion", "text": "Majorana fermion\n\nA Majorana fermion (), also referred to as a Majorana particle, is a fermion that is its own antiparticle. They were hypothesized by Ettore Majorana in 1937. The term is sometimes used in opposition to a Dirac fermion, which describes fermions that are not their own antiparticles.\n\nWith the exception of the neutrino, all of the Standard Model fermions are known to behave as Dirac fermions at low energy (after electroweak symmetry breaking), and none are Majorana fermions. The nature of the neutrinos is not settled – they may be either Dirac or Majorana fermions.\n\nIn condensed matter physics, bound Majorana fermions can appear as quasiparticle excitations – the collective movement of several individual particles, not a single one, and they are governed by non-abelian statistics.\n\nThe concept goes back to Majorana's suggestion in 1937 that neutral spin- particles can be described by a real wave equation (the Majorana equation), and would therefore be identical to their antiparticle (because the wave functions of particle and antiparticle are related by complex conjugation).\n\nThe difference between Majorana fermions and Dirac fermions can be expressed mathematically in terms of the creation and annihilation operators of second quantization: The creation operator formula_1 creates a fermion in quantum state formula_2 (described by a \"real\" wave function), whereas the annihilation operator formula_3 annihilates it (or, equivalently, creates the corresponding antiparticle). For a Dirac fermion the operators formula_1 and formula_3 are distinct, whereas for a Majorana fermion they are identical. The ordinary fermionic annihilation and creation operators formula_6 and formula_7 can be written in terms of two Majorana operators formula_8 and formula_9 by\n\nIn supersymmetry models, neutralinos — superpartners of gauge bosons and Higgs bosons — are Majorana.\n\nAnother common convention for the normalization of the Majorana fermion operator is\nThis convention has the advantage that the Majorana operator squares to the identity.\n\nUsing this convention, a collection of Majorana fermions formula_14 (formula_15) obey the following commutation identities\nwhere formula_18 and formula_19 are antisymmetric matrices.\n\nBecause particles and antiparticles have opposite conserved charges, Majorana fermions have zero charge. All of the elementary fermions of the Standard Model have gauge charges, so they cannot have fundamental Majorana masses.\n\nHowever, the right-handed sterile neutrinos introduced to explain neutrino oscillation could have Majorana masses. If they do, then at low energy (after electroweak symmetry breaking), by the seesaw mechanism, the neutrino fields would naturally behave as six Majorana fields, with three of them expected to have very high masses (comparable to the GUT scale) and the other three expected to have very low masses (below 1 eV). If right-handed neutrinos exist but do not have a Majorana mass, the neutrinos would instead behave as three Dirac fermions and their antiparticles with masses coming directly from the Higgs interaction, like the other Standard Model fermions.\nThe seesaw mechanism is appealing because it would naturally explain why the observed neutrino masses are so small. However, if the neutrinos are Majorana then they violate the conservation of lepton number and even of B − L.\n\nNeutrinoless double beta decay, which has not (yet) been observed. It can be viewed as two ordinary beta decay events whose resultant antineutrinos immediately annihilate with each other, and is only possible if neutrinos are their own antiparticles.\n\nThe high-energy analog of the neutrinoless double beta decay process is the production of same-sign charged lepton pairs in hadron colliders; it is being searched for by both the ATLAS and CMS experiments at the Large Hadron Collider. In theories based on left–right symmetry, there is a deep connection between these processes. In the currently most-favored explanation of the smallness of neutrino mass, the seesaw mechanism, the neutrino is “naturally” a Majorana fermion.\n\nMajorana fermions cannot possess intrinsic electric or magnetic moments, only toroidal moments. Such minimal interaction with electromagnetic fields makes them potential candidates for cold dark matter.\n\nIn superconducting materials, Majorana fermions can emerge as (non-fundamental) quasiparticles (more commonly referred to as Bogoliubov quasiparticles in condensed matter physics). This becomes possible because a quasiparticle in a superconductor is its own antiparticle.\n\nMathematically, the superconductor imposes electron hole \"symmetry\" on the quasiparticle excitations, relating the creation operator formula_20 at energy formula_21 to the annihilation operator formula_22 at energy formula_23. Majorana fermions can be bound to a defect at zero energy, and then the combined objects are called Majorana bound states or Majorana zero modes. This name is more appropriate than Majorana fermion (although the distinction is not always made in the literature), because the statistics of these objects is no longer fermionic. Instead, the Majorana bound states are an example of non-abelian anyons: interchanging them changes the state of the system in a way that depends only on the order in which the exchange was performed. The non-abelian statistics that Majorana bound states possess allows them to be used as a building block for a topological quantum computer.\n\nA quantum vortex in certain superconductors or superfluids can trap midgap states, so this is one source of Majorana bound states. Shockley states at the end points of superconducting wires or line defects are an alternative, purely electrical, source. An altogether different source uses the fractional quantum Hall effect as a substitute for the superconductor.\n\nIn 2008, Fu and Kane provided a groundbreaking development by theoretically predicting that Majorana bound states can appear at the interface between topological insulators and superconductors. Many proposals of a similar spirit soon followed, where it was shown that Majorana bound states can appear even without any topological insulator. An intense search to provide experimental evidence of Majorana bound states in superconductors first produced some positive results in 2012. A team from the Kavli Institute of Nanoscience at Delft University of Technology in the Netherlands reported an experiment involving indium antimonide nanowires connected to a circuit with a gold contact at one end and a slice of superconductor at the other. When exposed to a moderately strong magnetic field the apparatus showed a peak electrical conductance at zero voltage that is consistent with the formation of a pair of Majorana bound states, one at either end of the region of the nanowire in contact with the superconductor.. Simultaneously, a group from Purdue University and University of Notre Dame reported observation of fractional Josephson effect (decrease of the Josephson frequency by a factor of 2) in indium antimonide nanowires connected to two superconducting contacts and subjected to a moderate magnetic field, another signature of Majorana bound states. Bound state with zero energy was soon detected by several other groups in similar hybrid devices, and fractional Josephson effect was observed in topological insulator HgTe with superconducting contacts\n\nThe aforementioned experiments marks a possible verification of independent 2010 theoretical proposals from two groups predicting the solid state manifestation of Majorana bound states in semiconducting wires. However, it was also pointed out that some other trivial non-topological bounded states could highly mimic the zero voltage conductance peak of Majorana bound state. The subtle relation between those trivial bound states and Majorana bound states was reported by the researchers in Niels Bohr Institute, who can directly \"watch\" coalescing Andreev bound states evolving into Majorana bound states, thanks to a much cleaner semiconductor-superconductor hybrid system.\n\nIn 2014, evidence of Majorana bound states was also observed using a low-temperature scanning tunneling microscope, by scientists at Princeton University. It was suggested that Majorana bound states appeared at the edges of a chain of iron atoms formed on the surface of superconducting lead. The detection was not decisive because of possible alternative explanations.\n\nMajorana fermions may also emerge as quasiparticles in quantum spin liquids, and were observed by researchers at Oak Ridge National Laboratory, working in collaboration with Max Planck Institute and University of Cambridge on 4 April 2016.\n\nChiral Majorana fermions were detected in 2017, in a quantum anomalous Hall effect/superconductor hybrid device. In this system, Majorana fermions edge mode will give a rise to a formula_24 conductance edge current.\n\nOn 16 August 2018, a strong evidence for the existence of Majorana bound states (or Majorana anyons) in an iron-based superconductor, which many alternative trivial explanations cannot account for, was reported by researchers in Prof. Gao Hong-jun's team and Prof. Ding Hong's team at Institute of Physics, Chinese Academy of Sciences and University of Chinese Academy of Sciences, when they used scanning tunneling spectroscopy on the superconducting Dirac surface state of the iron-based superconductor. It was the first time that Majorana particles were observed in a bulk of pure substance.\n\nMajorana bound states can also be realized in quantum error correcting codes. This is done by creating so called 'twist defects' in codes such as the Toric code which carry unpaired Majorana modes. The braiding of Majoranas realized in such a way forms a projective representation of the braid group.\n\nSuch a realization of Majoranas would allow them to be used to store and process quantum information within a quantum computation. Though the codes typically have no Hamiltonian to provide suppression of errors, fault-tolerance would be provided by the underlying quantum error correcting code.\n"}
{"id": "33323747", "url": "https://en.wikipedia.org/wiki?curid=33323747", "title": "Manav Gupta", "text": "Manav Gupta\n\nManav Gupta(born 29 December 1967) is an artist from India who has pioneered collaborative art as performances and mega murals.He has co-opted his art practices in paintings, poetry, music and sound to create one-minute films on climate change, sustainable development, ecosystems and alternate energy for public service messages commissioned by the Ministry of Environment & Forests, Government of India (2005–2006,2011).\n\nHe has won international acclaim for his first of its kind six floor high 5000 sq ft in facade and 10000 sq ft of total painted surface, commissioned mega mural at the headquarters of leading telecom corporate giant in Gurgaon near Delhi,India in 2010, where he painted Live for three months while involving thousands of employees’ brush strokes in his creation \"The Tree of Life\".\nThe Tree of Life is the tallest and largest three-dimensional indoor staircase mural by artist Manav Gupta. It covers approximately 5,000 sq ft of visible frontage through a glass façade and 10,000 sq ft of total painted surface. Its significance lies in the methodology of creation as a work of contemporary art, with the first of its kind simultaneous use of four different art practices in conceptual, site specific, collaborative and performance art.\n\nWhen commissioned by Bharti Airtel Ltd to create a \"staircase artwork\" at their headquarters, the artist introduced a unique sustainable development process in corporate entity, by establishing that art can contribute to the business environment as well, by refreshing the intangible quality of its soft power among employees. \nAdding a collaborative dimension, he conceptualised the mural as a public art project by allowing thousands of employees the experience of putting brush strokes. In the process, a series of role plays of teaching and motivating employees took effect as performance art. In the second half of the project, a three dimensional site specific composition took shape. With Gupta working solo all through the day,live in front of 3500 employees and almost as many visitors at the corporate campus site, the mural took three months to complete as an evolving storyline of five elements in Nature. Keeping in mind the visibility of the staircase all over the campus through a 60 ft high glass facade, the site specific intervention amalgamated the background wall and the front face of the staircase perspectives of five floors into a single canvas merging surrounding sides and roof within one composition.\nHe has pioneered co-creation with his \"Jugalbandis\" (Collaborations) with leading musicians,poets,dancers including Dr.L.Subramanium, Shubha Mudgal, Anup Jalota, Rahul Sharma where he translates a performing artist's oveure LIVE on stage on canvas(2003–2011).He has co authored a book of poems and paintings with former President of India Dr A.P.J. Abdul Kalam,published by Penguin India.(2002–2005).\n\nGupta has been listed by \"Financial Times\" among the top ten contemporaryIndian artists whose works would fetch good returns. Gupta's works have been sold by Christie's, Bonhams, Philip de Pury and are in leading permanent public collections around the world including the Parliament of India, the Rashtrapati Bhawan, the Royal family of Oman, Indian embassies abroad, Chitrakala Parishad and Birla Academy museums, etc.\n\nOne of the youngest members to have been nominated on the Expert Committee of India's National Republic Day celebrations for consecutive years, he has been invited for many an advisory role including the formative process of the Museum of Natural History, New Delhi.\n\nWorking on a 360-degree platform of canvases,video installations and performances he collaborated with dance troupes and audiences in public art projects besides his performances and exhibitions at different venues along his three-month travelling trilogy across United States and Europe in 2010 including New York,Amherst,Des Moines, San Francisco, Berlin, and London.He also delivered guest lectures on art,environment and collaborative public art practices at the San Jose State University and the ICD,Berlin in 2010.\nIn 2011 he delineated Bhutan–India relations on a public mural commissioned by the Government of India and mounted in Bhutan.The monumental work consists of a suite of eighteen feet and twelve feet high canvases mapping the political, socio-cultural, spiritual and natural archival history of friendship between the two countries.\n\nGupta creates single edition functional sculptures and public installations with varied media including iron, steel, wood, discarded roots of trees, glass, recycled scrap metal and clay for interior and exterior corporate and private spaces.\n\nTrained in Kolkata at the Academy of Fine Arts under Rathin Maitra and under his guru Vasant Pandit, the artist currently works in New Delhi.\n\nCategory:Indian male contemporary artists\n"}
{"id": "55886410", "url": "https://en.wikipedia.org/wiki?curid=55886410", "title": "Marfa Inofuentes Pérez", "text": "Marfa Inofuentes Pérez\n\nMarfa Inofuentes Pérez (1969–2015) was an Afro-Bolivian activist involved in the Constitutional reform movement to recognize black Bolivians as an ethnic minority in the country. After achieving the goal for Afro-Bolivians to be protected under the law, she served as the head the Ministry of Gender and was appointed deputy mayor of the Peripheral Macrodistrict of the Municipality of La Paz.\n\nMarfa Inofuentes Pérez was born in 1969 in La Paz, Bolivia, one of three children of Benjamín Inofuentes. Her father was born in Tocaña, a village in the Bolivian Yungas Region, where the majority of the country's of Afro-Bolivians originated. After completion of her secondary studies, Inofuentes enrolled in the Universidad Mayor de San Andrés, studying sociology and law.\n\nIn 1990 Inofuentes joined the Afro-Bolivian Saya Cultural Movement (), a group organized to preserve the cultural traditions of black Bolivians, particularly the artform of \"saya\", by increasing their visibility and acceptance in the wider society. She participated in a public performance of saya in Tocaña in 1990, one of the first times that the public was allowed to witness the ceremonies. \n\nIn 2001, after the World Conference against Racism, Inofuentes and Jorge Medina, co-founded the Afro-Bolivian Center for Community Development () to advocate for government recognition of Bolivia's black population. According to a 1997 poll by the Inter-American Development Bank, which included Mónica Rey Gutiérrez, another Afro-Bolivian activist, the population numbered around 20,000 people, but the previous census in 2001 had no category except \"other\" to survey the actual size of the ethnic group. She believed that lack of state recognition as a minority, and inability to determine the size of the population, furthered marginalization of Afro-Bolivians, as there were no protections in law for discrimination or racial violence.\n\nTraveling widely, Inofuentes represented Bolivia's black women at meetings of the Organization of Ibero-American States in Brazil, Colombia, Ecuador, Panama, Peru, and the United States. She was also a member of the feminist organization the Network of Caribbean Latin American and African Diaspora Women (). \n\nIn 2006, after the election of President Evo Morales, lawmakers met in Sucre to rewrite the Constitution. Inofuentes and other black activists lobbied all of the political parties, pressing for recognition as an ethnic group. As one of the main activists involved in the Bolivian Constitutional Assembly, Inofuentes argued for the addition of articles to protect the civil rights of Afro-Bolivians, including language that recognized the population and protected their culture with the same provisions afforded to indigenous people and other intercultural minorities. Gaining the concessions desired, in 2009, Afro-Bolivians gained constitutional protection and recognition, after the Bolivian constitutional referendum passed. Her activism led to her being appointed to head the Ministry of Gender. In 2010, she was appointed as Deputy Mayor of the Peripheral Macrodistrict of the Municipality of La Paz, but after one year, she developed health problems and entered into a coma from which she did not recover.\n\nInofuentes died on 4 March 2015 at the Obrero Hospital in La Paz and is most remembered for her activism to gain recognition for the cultural traditions and identity of Afro-Bolivians.\n"}
{"id": "31883778", "url": "https://en.wikipedia.org/wiki?curid=31883778", "title": "Mass shooting", "text": "Mass shooting\n\nA mass shooting is an incident involving multiple participants of firearms-related violence. The United States' Congressional Research Service acknowledges that there is not a broadly accepted definition, and defines a \"public mass shooting\" as one in which four or more people select someone indiscriminately, and kill them, echoing the FBI definition of the term \"mass murder\". However, according to the Investigative Assistance for Violent Crimes Act of 2012, signed into law in January 2013, a mass killing is defined as a killing with at least three deaths, excluding the perpetrator. Another unofficial definition of a mass shooting is an event involving the shooting (not necessarily resulting in death) of five or more people (sometimes four) with no cooling-off period. Related terms include school shooting and massacre.\n\nA mass shooting may be committed by individuals or organizations in public or non-public places. Terrorist groups in recent times have used the tactic of mass shootings to fulfill their political aims. Individuals who commit mass shootings may fall into any of a number of categories, including killers of family, of coworkers, of students, and of random strangers. Individuals' motives for shooting vary.\n\nResponses to mass shootings take a variety of forms, depending on the context: number of casualties, the country, political climate, and other factors. The media cover mass shootings extensively and often sensationally, and the effect of that coverage has been examined. Countries such as the United Kingdom and Australia have changed their gun laws in the wake of mass shootings. In contrast, the United States' constitution prohibits laws which disallow firearm ownership outright and owns about half of the world's guns.\n\nThe characterization of an event as a mass shooting depends upon definition and definitions vary. Under U.S. federal law the Attorney General may on a request from a state assist in investigating \"mass killings\", rather than mass shootings. The term was originally defined as the murder of four or more people with no cooling-off period but redefined by Congress in 2013 as being murder of three or more people. In \"Behind the Bloodshed\", a report by \"USA Today\", a mass killing is defined as any incident in which four or more were killed and also includes family killings. A crowdsourced data site cited by CNN, MSNBC, \"The New York Times\", \"The Washington Post\", \"The Economist\", the BBC, etc., Mass Shooting Tracker, defines a mass shooting as any incident in which four or more people are shot, whether injured or killed. A noteworthy connection has been reported in the U.S. between mass shootings and domestic or family violence, with a current or former intimate partner or family member killed in 76 of 133 cases (57%), and a perpetrator having previously been charged with domestic violence in 21. The lack of a single definition can lead to alarmism in the news media, with some reports conflating categories of crimes.\n\nIn Australia, a 2006 paper defined a mass shooting as \"one in which ⩾5 firearm‐related homicides are committed by one or two perpetrators in proximate events in a civilian setting, not counting any perpetrators\".\n\nCrime violence research group Gun Violence Archive, whose research is used by all major American media outlets defines Mass Shooting as \"FOUR or more shot and/or killed in a single event [incident], at the same general time and location not including the shooter\" differentiating between Mass Shooting and Mass Murder [Killing] and not counting shooters as victims.\n\nAn act is typically defined as terrorist if it \"appears to have been intended\" to intimidate or to coerce people; a mass shooting is not, in itself, an act of terrorism. A U.S. Congressional Research Service report explicitly excluded from its definition of public mass shootings those in which the violence is a means to an end, for example where the gunmen \"pursue criminal profit or kill in the name of terrorist ideologies\".\n\nMass shootings have occurred on the African continent, including the 2015 Sousse attacks, the 2015 Bamako hotel attack, the 2013 Westgate shopping mall attack in Nairobi, Kenya, and the 1994 Kampala wedding massacre. Most mass shootings in Africa have stemmed from terrorism, with tourists and diplomats frequently being the targets. Workplace violence and prejudice against ethnic minorities have less-frequently been involved in such spontaneous acts of mass violence.\n\nSeveral mass shootings have occurred in Asia, including the 1938 Tsuyama massacre, the 1948 Babrra massacre, the 1983 Pashupatinath Temple shooting, the 1993 Chongqing shooting, and the 1994 Tian Mingjian incident.\n\nThe single deadliest event was the 2008 Mumbai attacks in which 164 people were killed and a further 308 people were wounded by terrorists.\n\nSouth Korea has suffered multiple mass shootings in the South Korean Army, mainly due to soldier's stress and conflicts from its violence and detention from society.\n\nJapan has as few as two gun-related homicides per year. These numbers include all homicides in the country, not just mass shootings.\n\nThere have been in Israel such as the 1972 Lod Airport Massacre, which killed 26 and injured 80, the 2002 Bat Mitzvah massacre and the June 2016, massacre at the popular Sarona center complex. These were all planned or executed by Palestinian or Arab terrorists.\n\nIn addition there have been two mass shootings by Jews in Israel. In 1991, Ami Popper was convicted of murdering seven Palestinian men in a mass shooting carried out in 1990. In 1994 Baruch Goldstein murdered 29 Muslims worshipping and injuring a further 125 in Hebron. Also known as the Cave of the Patriarchs massacre.\n\nOther shootings include the 2013 Meet al-Attar shooting in Egypt.\n\nSeveral mass shootings have occurred in Europe, including the November 2015 Paris attacks, the 2012 Toulouse and Montauban shootings, the 2011 Norway attacks, the 2009 Winnenden school shooting, the 2007 Jokela school shooting, the 2008 Kauhajoki school shooting, the 2001 Zug massacre, the 2002 Erfurt massacre, the 1987 Hungerford massacre, the 1990 Puerto Hurraco massacre, the 1993 Greysteel massacre, the 2010 Cumbria shootings and the 1996 Dunblane massacre.\n\nNotable mass shootings include the 1992 Tatarstan shooting, the 2002 Yaroslavsky shooting, the 2002 Moscow theater hostage crisis, the 2004 Beslan school siege, the 2012 Moscow shooting, the 2013 Belgorod shooting, and the 2014 Moscow school shooting.\n\nNotable mass shootings in Canada include the 1989 École Polytechnique massacre, the 1992 Concordia University massacre, the 2012 Danzig Street shooting, the 2014 Edmonton killings, the 2017 Quebec City mosque shooting, and the 2018 Danforth shooting.\n\nNotable mass shootings in Mexico include the 2010 Chihuahua shootings.\n\nThe U.S. has more mass shootings than any other country.\n\nHowever, when adjusting for different population sizes, analysing data between 2009 and 2015 (therefore excluding shootings like the 2016 Orlando nightclub shooting and the 2017 Las Vegas shooting), the US falls to 12th in a comparison between the US and Europe.\n\nIn one study by criminologist Adam Lankford, it was estimated that 31% of public mass shootings occur in the U.S., although it has only 5% of the world's population. The study concludes that \"The United States and other nations with high firearm ownership rates may be particularly susceptible to future public mass shootings, even if they are relatively peaceful or mentally healthy according to other national indicators.\" Criminologist Gary Kleck criticized Adam's findings stating the study fails to provide evidence that gun ownership increases mass shootings and that Lankford has been unwilling to share a list of his cases, provide a list of the number of attacks per country, or even list his sources so that others can check his numbers. Mass shootings have also been observed to be followed by an increase in the purchase of weapons, but this phenomenon seems to be driven by a minority since neither gun owners nor non-owners report an increased feeling of needing guns.\n\nNotable mass shootings in Argentina include the 2004 Carmen de Patagones school shooting.\n\nNotable mass shootings in Brazil include the 2011 Realengo massacre.\n\nNotable mass shootings in Australia include the 1987 Hoddle Street massacre and the 1996 Port Arthur Massacre. There were 13 mass shootings with five or more deaths between 1979 and 1996, and none thereafter, following stricter gun control laws. Australia has had two mass shootings with 5 or more deaths since 1996, however these shootings involved family members.\n\nNotable mass shootings in New Zealand include the 1990 Aramoana massacre.\n\nAfter mass shootings, some survivors have written about their experiences and their experiences have been covered by journalists. A survivor of the Knoxville Unitarian Universalist church shooting wrote about his reaction to other mass shooting incidents. The father of a victim in a mass shooting at a movie theater in Aurora, Colorado, wrote about witnessing other mass shootings after the loss of his son. The survivors of the 2011 Norway attacks recounted their experience to GQ. In addition, one paper studied Swedish police officers' reactions to a mass shooting.\n\nSurvivors of mass shootings can suffer from posttraumatic stress disorder.\n\nNotable mass shooters from outside the United States include Anders Behring Breivik (Norway, 2011), Robert Steinhauser and Tim Kretschmer (Germany, 2002 and 2009), William Unek (Africa, 1954 and 1957), Marc Lépine and Valery Fabrikant, (Canada, 1989 and 1992), Pekka-Eric Auvinen and Matti Juhani Saari (Finland, 2007 and 2008), Genildo Ferreira de França (Brazil, 1997), Friedrich Leibacher (Switzerland, 2001), Ľubomír Harman (Slovakia, 2010), Tristan van der Vlis (Netherlands, 2011), Richard Komakech (Uganda, 1994), Omar Abdul Razeq Abdullah Rifai (Egypt, 2013), Farda Gadirov (Azerbaijan, 2009), Martin Bryant (Australia, 1996), Michael Robert Ryan and Derrick Bird (England, 1987 and 2010), Thomas Hamilton (Scotland, 1996) Ljubiša Bogdanović (Serbia, 2013) and Woo Bum-kon (South Korea, 1982).\n\nNotable perpetrators of massacres in the U.S. include Edward Charles Allaway, James Edward Pough, Carl Robert Brown, Omar Mateen, Robert A. Hawkins, James Oliver Huberty, Nathan Dunlap, George Hennard, Dylann Roof, Adam Lanza, Nidal Malik Hasan, Charles Whitman, Jeff Weise, Gang Lu, Patrick Sherrill, Barry Loukaitis, Esteban Santiago, Christopher Harper-Mercer, Gian Luigi Ferri, Mark Essex, Scott Evans Dekraai, Steven Kazmierczak, Jennifer San Marco, James Eagan Holmes, Anthony F. Barbaro, Michael McLendon, Rodrick Shonte Dantzler, Jared Lee Loughner, Seung-Hui Cho, Elliot Rodger, Charles Carl Roberts IV, Rizwan Farook and Tashfeen Malik, Robert Lewis Dear, Mitchell Johnson and Andrew Golden, Aaron Alexis, Wade Michael Page, Eric Harris and Dylan Klebold, Patrick Edward Purdy, Gavin Eugene Long, Micah Xavier Johnson, Kyle Aaron Huff, One L. Goh, Stephen Paddock, Devin Patrick Kelley, Nikolas Cruz, Dimitrios Pagourtzis, David Katz, Robert Bowers, and Ian Long. U.S. mass shooters are overwhelmingly males. According to a database compiled by \"Mother Jones\" magazine, the race of the shooters is approximately proportionate to the overall U.S. population, although Asians are overrepresented and Latinos underrepresented. Criminologist James Allen Fox said that most mass murderers do not have a criminal record, or involuntary incarceration at a mental health center, but an article in the New York Times in December 2015 about 15 recent mass shootings found that six perpetrators had had run-ins with law enforcement, and six had mental health issues.\n\nMass shootings can be motivated by misanthropy and terrorism and caused by mental illness, inceldom and extensive bullying among other reasons. Forensic psychologist Stephen Ross says that extreme anger and the thought shooters are working for a cause, rather than mental illness, is most often the explanation. A study by Vanderbilt University researchers found that \"fewer than 5% of the 120,000 gun-related killings in the United States between 2001 and 2010 were perpetrated by people diagnosed with mental illness\". John Roman of the Urban Institute argues that, while better access to mental health care, restricting high powered weapons, and creating a defensive infrastructure to combat terrorism are constructive, they don't address the greater issue, which is \"we have a lot of really angry young men in our country and in the world.\"\n\nAuthor Dave Cullen described killer Eric Harris as an \"injustice collector\" in his 2009 book \"Columbine\". He expanded on the concept in a 2015 \"New Republic\" essay on injustice collectors, identifying several notorious killers as fitting the category, including Christopher Dorner, Elliot Rodger, Vester Flanagan, and Andrew Kehoe. Likewise, mass shooting expert and former FBI profiler Mary O'Toole also uses the phrase \"injustice collector\" in characterizing motives of some mass shooting perpetrators. In relation, criminologist James Alan Fox contends that mass murderers are \"enabled by social isolation\" and typically experience \"years of disappointment and failure that produce a mix of profound hopelessness and deep-seated resentment.\" Jillian Peterson, an assistant professor of criminology at Hamline University who is participating in the construction of a database on mass shooters, noted that two phenomena surface repeatedly in the statistics: hopelessness and a need for notoriety in life or in death.\nNotoriety was first suggested as a possible motive and researched by Justin Nutt. Nutt stated in a 2013 article, \"those who feel nameless and as though no one will care or remember them when they are gone may feel doing something such as a school shooting will make sure they are remembered and listed in the history books.\"\n\nIn considering the frequency of mass shootings in the United States, criminologist Peter Squires says that the individualistic culture in the United States puts the country at greater risk for mass shootings than other countries, noting that \"many other countries where gun ownership is high, such as Norway, Finland, Switzerland and Israel . . . tend to have more tight-knit societies where a strong social bond supports people through crises, and mass killings are fewer.\" He is an advocate of gun control, but contends there is more to mass shootings than the prevalence of guns.\n\nAccording to Michael Cook and Carolyn Moynihan of Mercatornet, an angle that is missed by mainstream media is the findings of important social scientists such as eminent Harvard sociologist Robert J. Sampson who wrote: “Family structure is one of the strongest, if not the strongest, predictor of variations in urban violence across cities in the United States. The close empirical connection between family breakdown and crime suggests that increased spending on crime-fighting, imprisonment, and criminal justice in the United States over the last 40 years is largely the direct or indirect consequence of marital breakdown.” His views are echoed by the eminent criminologists Michael Gottfredson and Travis Hirschi, who have written that “such family measures as the percentage of the population divorced, the percentage of households headed by women, and the percentage of unattached individuals in the community are among the most powerful predictors of crime rates.”\n\nBased on the research of another social scientist who was himself raised by a single mother, Bradford Wilcox, \"boys living in single mother homes are almost twice as likely to end up delinquent compared to boys who enjoy good relationships with their father.\"\n\nMoynihan said that \"almost all school shooters come from families where the parents are either divorced or alienated\", and Cook argued that \"perhaps they wouldn’t need more gun control if they had better divorce control.”\n\nSome people have considered whether media attention revolving around the perpetrators of mass shootings is a factor in sparking further incidents. In response to this, some in law enforcement have decided against naming mass shooting suspects in media-related events to avoid giving them notoriety.\n\nThe effects of messages used in the coverage of mass shootings has been studied. Researchers studied the role the coverage plays in shaping attitudes toward persons with serious mental illness and public support for gun control policies.\n\nIn 2015 a paper written by a physicist and statistician, Sherry Towers, along with four colleagues was published, which proved that there is indeed mass shooting contagion using mathematical modeling. However in 2017 Towers said in an interview that she prefers self-regulation to censorship to address this issue, just like years ago major news outlets successfully prevent copycat suicide.\n\nIn 2016 the American Psychological Association published a press release, claiming that mass shooting contagion does exist and news media and social media enthusiasts should withhold the name(s) and face(s) of the victimizer(s) when reporting a mass shooting to deny the fame the shooter(s) want to curb contagion.\n\nSome news media have weighed in on the gun control debate. After the 2015 San Bernardino attack, the \"New York Daily News\" front-page headline, \"God isn't fixing this\", was accompanied by \"images of tweets from leading Republicans who shared their 'thoughts' and 'prayers' for the shooting victims\". Since the 2014 Isla Vista killings, satirical news website \"The Onion\" has republished the story \"'No Way To Prevent This', Says Only Nation Where This Regularly Happens\" with minor edits after major mass shootings, to satirise the popular consensus that there is a lack of political power in the United States to prevent mass shootings.\n\nResponses to mass shootings take a variety of forms, depending on the country and political climate.\n\nAfter the 1996 Port Arthur massacre in Australia, the government changed gun laws in Australia. As in the United States, figures vary according to the definition of \"mass shooting\"; a 2006 paper used a definition \"one in which ⩾5 firearm‐related homicides are committed by one or two perpetrators in proximate events in a civilian setting, not counting any perpetrators\", compared to the usual U.S. definition of an indiscriminate rampage in public places resulting in four or more victims killed. Between 1981 and the passing of the law in 1996 there were 13 mass shootings with five or more deaths; in the following decade, while the new law was in place, there were no such mass shootings. Overall gun deaths have continued to decline for two decades since the law was passed, however there have been several shootings with three or more deaths since 1996 where the victims were related to the shooter.\n\nThere were five significant shootings, though not meeting the \"mass shooting\" definition of the 2006 paper, between 1996 :\n\nAs a result of the 1987 Hungerford massacre and 1996 Dunblane school massacre mass shootings, the United Kingdom enacted tough gun laws and a buyback program to remove guns from private ownership. There have been two mass shootings since the laws were restricted, the Cumbria shootings in 2010 which killed 13 people and the Moss Side mass shooting in 2018, in which no-one was killed.\n\nIn the United States, support for gun law reform varies considerably by political party, with Democrats generally more supportive and Republicans generally more opposed. Some in the U.S. believe that tightening gun laws would prevent future mass shootings. Some politicians in the U.S. introduced legislation to reform the background check system for purchasing a gun. A vast majority of Americans support tighter background checks. \"According to a poll [Made by CNN] by Quinnipiac University in Connecticut, 93 percent of registered voters said they would support universal background checks for all gun buyers.\"\n\nOthers contend that mass shootings should not be the main focus in the gun law reform debate because these shootings account for less than one percent of the U.S. homicide rate and believe that these shootings are hard to stop. They often argue that civilians with concealed guns will be able to stop shootings.\n\nGun control policies may cause a lot of controversy due to divided opinions on who should be able to carry a weapon. An opinion survey was conducted by the firm GfK Knowledge Networks to differentiate between the different attitudes towards gun control. There was a gun policy survey and a mental illness survey. Studies showed that over 85% of those questioned supported national background checks into the mental health records of citizens attempting to purchase a gun. More than 50% of people felt that those suffering with mental health issues were more deviant and threatening than those who had good mental health. The study also proved that there is large interest in contributing to mental health awareness as well as simply prohibiting those suffering from purchasing guns. Nearly two thirds of respondents supported greater government spending on mental health, with more than 60% of people believing this would reduce gun violence in the USA. (Colleen L. Barry, 2013)\n\nAs of June 2016, U.S. President Barack Obama had spoken in the aftermath of fourteen mass shootings during his nearly eight-year presidency, repeatedly calling for more gun safety laws in the United States. After the Charleston church shooting, U.S. President Barack Obama said, \"At some point, we as a country will have to reckon with the fact that this type of mass violence does not happen in other advanced countries. It doesn't happen in other places with this kind of frequency.\" After the December 2015 San Bernardino attack, Obama renewed his call for reforming gun-safety laws and also said that the frequency of mass shootings in the United States has \"no parallel in the world\". After the February 2018 attack at Florida's Parkland school shooting at Stoneman Douglas High School, the school's student survivors, teachers, and parents became strong leaders in the effort to ban assault weapon sales and easy accessibility to military weapons.\n\n\n"}
{"id": "7260624", "url": "https://en.wikipedia.org/wiki?curid=7260624", "title": "Michael Denton", "text": "Michael Denton\n\nMichael John Denton (born 25 August 1943) is a British-Australian author and biochemist. He is a Senior Fellow at the Discovery Institute's Center for Science and Culture. Denton’s most prominent book, \"\", inspired intelligent design proponents Phillip Johnson and Michael Behe.\n\nDenton gained a medical degree from Bristol University in 1969 and a PhD in biochemistry from King's College London in 1974. He was a senior research fellow in the Biochemistry Department at the University of Otago, Dunedin, New Zealand from 1990 to 2005. He later became a scientific researcher in the field of genetic eye diseases. He has spoken worldwide on genetics, evolution and the anthropic argument for design. Denton's current interests include defending the \"anti-Darwinian evolutionary position\" and the design hypothesis formulated in his book \"Nature’s Destiny\". Denton describes himself as an agnostic. He is currently a senior fellow at the Discovery Institute's Center for Science and Culture.\n\nIn 1985 Denton wrote the book \"Evolution: A Theory in Crisis\", presenting a systematic critique of neo-Darwinism ranging from paleontology, fossils, homology, molecular biology, genetics and biochemistry, and argued that evidence of design exists in nature.\nSome of book reviews criticized his arguments. He describes himself as an evolutionist and he has rejected biblical creationism. The book influenced both Phillip E. Johnson, the father of intelligent design, Michael Behe, a proponent of irreducible complexity, and George Gilder, co-founder of the Discovery Institute, the hub of the intelligent design movement. Since writing the book Denton has changed many of his views on evolution, however he still believes that the existence of life is a matter of design.\n\nDenton still accepts design and embraces a non-Darwinian evolutionary theory. He denies that randomness accounts for the biology of organisms; he has proposed an evolutionary theory which is a \"directed evolution\" in his book \"Nature's Destiny\" (1998). Life, according to Denton, did not exist until the initial conditions of the universe were fine-tuned (see Fine-tuned Universe). Denton was influenced by Lawrence Joseph Henderson (1878-1942), Paul Davies and John D. Barrow who argued for an anthropic principle in the cosmos (Denton 1998, v, Denton 2005). His second book \"Nature's Destiny\" (1998) is his biological contribution to the anthropic principle debate, dominated by physicists. He argues for a law-like evolutionary unfolding of life.\n\n\n"}
{"id": "39408762", "url": "https://en.wikipedia.org/wiki?curid=39408762", "title": "Miles Christianus", "text": "Miles Christianus\n\nThe miles Christianus (Christian soldier) or \"miles Christi\" (soldier of Christ) is a Christian allegory based on New Testament military metaphors, especially the Armor of God metaphor of military equipment standing for Christian virtues and on certain passages of the Old Testament from the Latin Vulgate.\n\nBy the fifth century, the Church had started to develop doctrines that allowed for Christian participation in battle, though this was limited by a requirement that the fighting must be undertaken to convert infidels or spread the glory of Christ. Christians were not to fight for conquest or personal glory.\n\nThe concepts of \"miles Christi\" and \"militia Christi\" can be traced back to the first century AD. The phrase \"miles Christi\", derived from a letter from Paul the Apostle and much employed by Pope Gregory VII, also appeared in the \"Gesta Francorum\" in reference to the young Prince Tancred, Bohemond, Godfrey and Count Raymond of Toulouse, each of whom were Christian leaders in the First Crusade.\n\nThe metaphor has its origins in early Christianity of the Roman Empire, and gave rise to the contrasting term \"paganus\" (hitherto used in the sense of \"civilian\", \"one lacking discipline\") for its opposite, i.e. one who was not a soldier of Christ.\n\nChivalry as the idealized image of knighthood was a common moral allegory in early Christian literature. Knighthood emerged as a concept during the time of Charlemagne. During the Saxon Wars, Charlemagne's Christian knights attended mass, surrounded by relics, before battles.\n\nFragments from 15th c. Polish chronicler Jan Długosz describe the sanctification of weapons and a concept of knighthood that was grounded in religion. It became a theme in art during the High Middle Ages, with depictions of a knight with his various pieces of equipment identified with various virtues. This parallels the development of the understanding in medieval Christendom of the armed nobility as defenders of the faith, first emphasized by Gregory VII in the context of the Investiture controversy and later made even more explicit with the actual military expeditions of the crusades. \nDepictions of the \"miles christianus\" with the emblematic Armour of God however remained very rare in the medieval period and only became prominent after the Protestant Reformation.\n\nIn the early modern period, the understanding of the term again became more metaphorical, but it survives in various Christian orders or confessions; it is especially pronounced among the Jesuits and in the Salvation Army, and it is the central theme of the 18th century hymn \"Soldiers of Christ, Arise\" and the 19th century hymn \"Onward, Christian Soldiers.\"\n\n\n\n"}
{"id": "50189312", "url": "https://en.wikipedia.org/wiki?curid=50189312", "title": "Minnie M. Kenny", "text": "Minnie M. Kenny\n\nMinnie M. Kenny (1929–August 17, 2005) was a cryptanalyst, educator and equal opportunity activist who worked at the National Security Agency (NSA). She served as Deputy Commandant at the National Cryptologic School and was responsible for creating scholarships for NSA employees. The recipient of numerous awards, including the Meritorious and the Exceptional Civilian Service Awards, the presidential Meritorious Executive Award and Distinguished Service Award of the CIA, Kenny was inducted into the Cryptologic Hall of Honor in 2009.\n\nMinnie McNeal was born in 1929 in Philadelphia. After graduating from the Philadelphia High School for Girls, she worked for the Philadelphia Commerce Department and the United States Census Bureau in Washington, D. C., before being hired in 1951 to work at Arlington Hall for the Army Security Agency. She was part of the first group of African Americans who were allowed to work upstairs in the Operations Division, out of the basement, and was assigned to the 'U' Street School for training. After her training and the change of the organization from military footing to the National Security Agency in 1952, McNeal was assigned as a communications clerk to the ALLO (All Other (non-Soviet)) linguistics unit. She was particularly well known for her expertise in cryptanalysis, language and traffic analysis and worked with an elite \"think tank\" at NSA, to develop programs for cryptanalysis and language, serving as chair of the agency's Language Panel. In 1972, McNeal married Herbert Cornelius Kenny, one of the singers of The Ink Spots, with whom she had a daughter Daphne.\n\nIn 1973, Kenny became the founding editor of NSA's Group B journal \"Dragon Seeds\" and pressed for each cryptanalyst to have their own personal computer as a necessary tool. From 1975 to 1981, she served in the Office of Techniques and Standards of the NSA, as the chief of the language and linguistics. In 1980, she was the recipient of the Meritorious Civilian Service Award. Beginning in 1982, Kenny served as Deputy Commandant at the National Cryptologic School, and worked with traditional Black colleges and universities to increase diversity. She introduced computer assisted teaching techniques and founded the Computer Assisted Learning and Instruction Consortium (CALICO) to bring professional language teachers and those who use language together in an international association. In 1984, Kenny was awarded a second civilian honor, the Exceptional Civilian Service Award by the NSA and both presidents Ronald Reagan and George H. W. Bush awarded her with the Meritorious Executive Award. After a decade working to improve the school, Kenny served on the Department of Defense Congressional Task Force on Women, Minorities, and the Handicapped in Science and Technology and as the NSA Director for Equal Employment Opportunity, before her 1993 retirement. She was a recipient of the Central Intelligence Agency's Distinguished Service Award.\n\nKenny then served as a Congressional Fellow on the staff of Congressman Louis Stokes and worked on the drafting of the Underground Railroad Act of 1997, which was designed to preserve and protect the history of significant buildings associated with the historic organization.\n\nKenny died August 17, 2005, in Columbia, Maryland, and was buried at St. John's Cemetery in Ellicott City, Maryland. Posthumously, in 2009, she was inducted into the Cryptologic Hall of Honor.\n\n"}
{"id": "3558732", "url": "https://en.wikipedia.org/wiki?curid=3558732", "title": "Negative and positive rights", "text": "Negative and positive rights\n\nNegative and positive rights are rights that oblige either action (\"positive rights\") or inaction (\"negative rights\"). These obligations may be of either a legal or moral character. The notion of positive and negative rights may also be applied to liberty rights.\n\nTo take an example involving two parties in a court of law: Adrian has a \"negative right to x\" against Clay if and only if Clay is \"prohibited\" from acting upon Adrian in some way regarding \"x\". In contrast, Adrian has a \"positive right to x\" against Clay if and only if Clay is obliged to act upon Adrian in some way regarding \"x\". A case in point, if Adrian has a \"negative right to life\" against Clay, then Clay is required to refrain from killing Adrian; while if Adrian has a \"positive right to life\" against Clay, then Clay is required to act as necessary to preserve the life of Adrian.\n\nRights considered \"negative rights\" may include civil and political rights such as freedom of speech, life, private property, freedom from violent crime, freedom of religion, \"habeas corpus\", a fair trial, and freedom from slavery.\n\nRights considered \"positive rights\", as initially proposed in 1979 by the Czech jurist Karel Vasak, may include other civil and political rights such as police protection of person and property and the right to counsel, as well as economic, social and cultural rights such as food, housing, public education, employment, national security, military, health care, social security, internet access, and a minimum standard of living. In the \"three generations\" account of human rights, negative rights are often associated with the first generation of rights, while positive rights are associated with the second and third generations.\n\nSome philosophers (see criticisms) disagree that the negative-positive rights distinction is useful or valid.\n\nUnder the theory of positive and negative rights, a negative right is a right \"not to be\" subjected to an action of another person or group—a government, for example—usually in the form of abuse or coercion. As such, negative rights exist unless someone acts to \"negate\" them. A positive right is a right \"to be\" subjected to an action of another person or group. In other words, for a positive right to be exercised, someone else's actions must be \"added\" to the equation. In theory, a negative right forbids others from acting against the right holder, while a positive right obligates others to act with respect to the right holder. In the framework of the Kantian categorical imperative, negative rights can be associated with perfect duties while positive rights can be connected to imperfect duties.\n\nBelief in a distinction between positive and negative rights is usually maintained, or emphasized, by libertarians, who believe that positive rights do not exist until they are created by contract. The United Nations Universal Declaration of Human Rights lists both positive and negative rights (but does not identify them as such). The constitutions of most liberal democracies guarantee negative rights, but not all include positive rights. Nevertheless, positive rights are often guaranteed by other laws, and the majority of liberal democracies provide their citizens with publicly funded education, health care, social security and unemployment benefits.\n\nRights are often spoken of as inalienable and sometimes even absolute. However, in practice this is often taken as graded absolutism; rights are ranked by degree of importance, and violations of lesser ones are accepted in the course of preventing violations of greater ones. Thus, even if the right not to be killed is inalienable, the corresponding obligation on others to refrain from killing is generally understood to have at least one exception: self-defense. Certain widely accepted negative obligations (such as the obligations to refrain from theft, murder, etc.) are often considered prima facie, meaning that the legitimacy of the obligation is accepted \"on its face\"; but even if not questioned, such obligations may still be ranked for ethical analysis.\n\nThus a thief may have a negative obligation not to steal, and a police officer may have a negative obligation not to tackle people—but a police officer tackling the thief easily meets the burden of proof that he acted justifiably, since his was a breach of a lesser obligation and negated the breach of a greater obligation. Likewise a shopkeeper or other passerby may also meet this burden of proof when tackling the thief. But if any of those individuals pulled a gun and shot the (unarmed) thief for stealing, most modern societies would not accept that the burden of proof had been met. The obligation not to kill—being universally regarded as one of the highest, if not the highest obligation—is so much greater than the obligation not to steal that a breach of the latter does not justify a breach of the former. Most modern societies insist that other, very serious ethical questions need come into play before stealing could justify killing.\n\nPositive obligations confer duty. But as we see with the police officer, exercising a duty may violate negative obligations (e.g. not to overreact and kill). For this reason, in ethics positive obligations are almost never considered \"prima facie\". The greatest negative obligation may have just one exception—one higher obligation of self-defense—but even the greatest positive obligations generally require more complex ethical analysis. For example, one could easily justify failing to help, not just one, but a great many injured children quite ethically in the case of triage after a disaster. This consideration has led ethicists to agree in a general way that positive obligations are usually junior to negative obligations because they are not reliably \"prima facie\". Some critics of positive rights implicitly suggest that because positive obligations are not reliably \"prima facie\" they must always be agreed to through contract.\n\nNineteenth-century philosopher Frédéric Bastiat summarized the conflict between these negative and positive rights by saying:\nAccording to Jan Narveson, the view of some that there is no distinction between negative and positive rights on the ground that negative rights require police and courts for their enforcement is \"mistaken\". He says that the question between what one has a right to do and who if anybody enforces it are separate issues. If rights are only negative then it simply means no one has a duty to enforce them, although individuals have a right to use any non-forcible means to gain the cooperation of others in protecting those rights. Therefore, he says \"the distinction between negative and positive is quite robust.\" Libertarians hold that positive rights, which would include a right to be protected, do not exist until they are created by contract. However, those who hold this view do not mean that police, for example, are not obligated to protect the rights of citizens. Since they contract with their employers to defend citizens from violence, then they have created that obligation to their employer. A negative right to life allows an individual to defend his life from others trying to kill him, or obtain voluntary assistance from others to defend his life—but he may not force others to defend him, because he has no natural right to be provided with defense. To force a person to defend one's own negative rights, or the negative rights of a third party, would be to violate that person's negative rights.\n\nOther advocates of the view that there is a distinction between negative and positive rights argue that the presence of a police force or army is not due to any positive right to these services that citizens claim, but rather because they are natural monopolies or public goods—features of any human society that arise naturally, even while adhering to the concept of negative rights only. Robert Nozick discusses this idea at length in his book \"Anarchy, State, and Utopia\".\n\nIn the field of medicine, positive rights of patients often conflict with negative rights of physicians. In controversial areas such as abortion and assisted suicide, medical professionals may not wish to offer certain services for moral or philosophical reasons. If enough practitioners opt out as a result of conscience, a right granted by conscience clause statutes in many jurisdictions, patients may not have any means of having their own positive rights fulfilled. Such was the case of Janet Murdock, a Montana woman who could not find any physician to assist her suicide in 2009. This controversy over positive and negative rights in medicine has become a focal point in the ongoing public debate between conservative ethicist Wesley J. Smith and bioethicist Jacob M. Appel. In discussing \"Baxter v. Montana\", Appel has written:\nSmith replies that this is \"taking the duty to die and transforming it into a duty to kill\", which he argues \"reflects a profound misunderstanding of the government’s role\".\n\nPresumably, if a person has positive rights it implies that other people have positive duties (to take certain actions); whereas negative rights imply that others have negative duties (to avoid certain other actions). Philosopher Henry Shue is skeptical; he believes that all rights (regardless of whether they seem more \"negative\" or \"positive\") requires both kinds of duties at once. In other words, Shue says that honouring a right will require avoidance (a \"negative\" duty) but also protective or reparative actions (\"positive\" duties). The negative positive distinction may be a matter of emphasis; it is therefore unhelpful to describe \"any right\" as though it requires only one of the two types of duties.\n\nTo Shue, rights can always be understood as confronting \"standard threats\" against humanity. Dealing with standard threats requires all kinds of duties, which may be divided across time (e.g. \"if avoiding the harmful behaviour fails, begin to repair the damages\"), but also divided across people. The point is that every right provokes all 3 types of behaviour (avoidance, protection, repair) to some degree. Dealing with a threat like murder, for instance, will require one individual to practice avoidance (e.g. the potential murderer must stay calm), others to protect (e.g. the police officer, who must stop the attack, or the bystander, who may be obligated to call the police), and others to repair (e.g. the doctor who must resuscitate a person who has been attacked). Thus, even the negative right not to be killed can only be guaranteed with the help of some positive duties. Shue goes further, and maintains that the negative and positive rights distinction can be harmful, because it may result in the neglect of necessary duties.\n\nJames P. Sterba makes similar criticisms. He holds that any right can be made to appear either positive or negative depending on the language used to define it. He writes:\n\nSterba has rephrased the traditional \"positive right\" to provisions, and put it in the form of a sort of \"negative right\" \"not to be prevented\" from taking the resources on their own.. Thus, all rights may not only require both \"positive\" and \"negative\" duties, but it seems that rights that do not involve forced labor can be phrased positively or negatively at will. The distinction between positive and negative may not be very useful, or justified, as rights requiring the provision of labor can be rephrased from \"right to education\" or \"right to health care\" to \"right to take surplus money to pay teachers\" or \"right to take surplus money to pay doctors\".\n\n\n\n"}
{"id": "1088262", "url": "https://en.wikipedia.org/wiki?curid=1088262", "title": "Organizational behavior", "text": "Organizational behavior\n\nOrganizational behavior (OB) or organisational behaviour is \"the study of human behavior in organizational settings, the interface between human behavior and the organization, and the organization itself\". OB research can be categorized in at least three ways, including the study of:\n\n\nChester Barnard recognized that individuals behave differently when acting in their organizational role than when acting separately from the organization. Organizational behavior researchers study the behavior of individuals primarily in their organizational roles. One of the main goals of organizational behavior is \"to revitalize organizational theory and develop a better conceptualization of organizational life\".\n\nMiner (2006) mentioned that \"there is a certain arbitrariness\" in identifying a \"point at which organizational behavior became established as a distinct discipline\" (p. 56), suggesting that it could have emerged in the 1940s or 1950s. He also underlined the fact that the industrial psychology division of the American Psychological Association did not add \"organizational\" to its name until 1970, \"long after organizational behavior had clearly come into existence\" (p. 56), noting that a similar situation arose in sociology. Although there are similarities and differences between the two disciplines, there is still confusion around differentiating organizational behavior and organizational psychology.\n\nAs a multi-disciplinary , organizational behavior has been influenced by developments in a number of related disciplines including: Sociology, industrial/organizational psychology, and economics.\n\nThe Industrial Revolution is a period from the 1760s where new technologies resulted in the adoption of new manufacturing techniques and increased mechanization. In his famous iron cage metaphor, Max Weber raised concerns over the reduction in religious and vocational work experiences. Weber claimed that the Industrial Revolution's focus on efficiency constrained the worker to a kind of \"prison\" and \"stripped a worker of their individuality\". The significant social and cultural changes caused by the Industrial Revolution also gave rise to new forms of organization. Weber analyzed one of these organizations and came to the conclusion that bureaucracy was \"an organization that rested on rational-legal principles and maximized technical efficiency.\"\n\nA number of OB practitioners documented their ideas about management and organisation. The best known theories today originate from Henri Fayol, Chester Barnard, and Mary Parker Follet. All three of them drew from their experience to develop a model of effective organizational management, and each of their theories independently shared a focus on human behavior and motivation. One of the first management consultants, Frederick Taylor, was a 19th-century engineer who applied an approach known as the scientific management. Taylor advocated for maximizing task efficiency through the scientific method. The scientific method was further refined by Lillian and Frank Gilbreth, who utilized time and motion study to further improve worker efficiency. In the early 20th century the idea of Fordism emerged. Named after automobile mogul Henry Ford, the method relied on the standardization of production through the use of assembly lines. This allowed unskilled workers to produce complex products efficiently. Sorenson later clarified that Fordism developed independently of Taylor. Fordism can be explained as the application of bureaucratic and scientific management principles to whole manufacturing process. The success of the scientific method and Fordism resulted in the widespread adoption of these methods.\n\nIn the 1920s, the Hawthorne Works Western Electric factory commissioned the first of what was to become known as the Hawthorne Studies. These studies initially adhered to the traditional scientific method, but also investigated whether workers would be more productive with higher or lower lighting levels. The results showed that regardless of lighting levels, when workers were being studied, productivity increased, but when the studies ended, worker productivity would return to normal. In following experiments, Elton Mayo concluded that job performance and the so-called Hawthorne Effect was strongly correlated to social relationships and job content. Following the Hawthorne Studies motivation became a focal point in the OB community. A range of theories emerged in the 1950s and 1960s and include theories from notable OB researchers such as: Frederick Herzberg, Abraham Maslow, David McClelland, Victor Vroom, and Douglas McGregor. These theories underline employee motivation, work performance, and job satisfaction.\n\nHerbert Simon's \"Administrative Behavior\" introduced a number of important OB concepts, most notably decision-making. Simon, along with Chester Barnard, argued that people make decisions differently inside an organization when compared to their decisions outside of an organization. While classical economic theories assume that people are rational decision-makers, Simon argued a contrary point. He argued that cognition is limited because of bounded rationality For example, decision-makers often employ satisficing, the process of utilizing the first marginally acceptable solution rather than the most optimal solution. Simon was awarded the Nobel Prize in Economics for his work on organizational decision-making. In the 1960s and 1970s, the field started to become more quantitative and resource dependent. This gave rise to contingency theory, institutional theory, and organizational ecology. Starting in the 1980s, cultural explanations of organizations and organizational change became areas of study, in concert with fields such as anthropology, psychology and sociology.\n\nResearch in and the teaching of OB primarily takes place in university management departments in colleges of business. Sometimes OB topics are taught in industrial and organizational psychology graduate programs.\nThere have been additional developments in OB research and practice. Anthropology has become increasingly influential, and led to the idea that one can understand firms as communities, by introducing concepts such as organizational culture, organizational rituals, and symbolic acts. Leadership studies have also become part of OB. OB researchers have shown increased interest in ethics and its importance in an organization. Some OB researchers have become interested in the aesthetic sphere of organizations.\n\nA variety of methods are used in organizational behavior, many of which are found in other social sciences.\n\nStatistical methods used in OB research commonly include correlation, analysis of variance, meta-analysis, multilevel modeling, multiple regression, structural equation modeling, and time series analysis\n\nComputer simulation is a prominent method in organizational behavior. While there are many uses for computer simulation, most OB researchers have used computer simulation to understand how organizations or firms operate. More recently, however, researchers have also started to apply computer simulation to understand individual behavior at a micro-level, focusing on individual and interpersonal cognition and behavior such as the thought processes and behaviors that make up teamwork.\n\nQualitative research consists of a number of methods of inquiry that generally do not involve the quantification of variables. Qualitative methods can range from the content analysis of interviews or written material to written narratives of observations. Common methods include ethnography, case studies, historical methods, and interviews.\n\nConsultants use principles developed in OB research to assess clients' organizational problems and provide high quality services.\n\nCounterproductive work behavior is employee behavior that harms or intends to harm an organization.\n\nMany OB researchers embrace the rational planning model. Decision-making research often focuses on how decisions are ordinarily made (normative decision-making), how thinkers arrive at a particular judgement (descriptive decision-making), and how to improve this decision-making (prescriptive decision-making).\n\nThere are several types of mistreatment that employees endure in organizations including: Abusive supervision, bullying, incivility, and sexual harassment.\n\nAbusive supervision is the extent to which a supervisor engages in a pattern of behavior that harms subordinates.\n\nAlthough definitions of workplace bullying vary, it involves a repeated pattern of harmful behaviors directed towards an individual. In order for a behavior to be termed bullying, the individual or individuals doing the harm have to possesses (either singly or jointly) more power than the victim.\n\nWorkplace incivility consists of low-intensity discourteous and rude behavior and is characterized by an ambiguous intent to harm, and the violation of social norms governing appropriate workplace behavior.\n\nSexual harassment is behavior that denigrates or mistreats an individual due to his or her gender, often creating an offensive workplace that interferes with job performance.\n\nOrganizational behavior deals with employee attitudes and feelings, including job satisfaction, organizational commitment, and emotional labor. Job satisfaction reflects the feelings an employee has about his or her job or facets of the job, such as pay or supervision. Organizational commitment represents the extent to which employees feel attached to their organization. Emotional labor concerns the requirement that an employee display certain emotions, such smiling at customers, even when the employee does not feel the emotion he or she is required to display.\n\nThere have been a number of theories that concern leadership. Early theories focused on characteristics of leaders, while later theories focused on leader behavior, and conditions under which leaders can be effective. Among these approaches are contingency theory, the consideration and initiating structure model, leader-member exchange or LMX theory, path-goal theory, and transformational leadership theory.\n\nContingency theory indicates that good leadership depends on characteristics of the leader and the situation. The Ohio State Leadership Studies identified dimensions of leadership known as consideration (showing concern and respect for subordinates) and initiating structure (assigning tasks and setting performance goals). LMX theory focuses on exchange relationships between individual supervisor-subordinate pairs. Path-goal theory is a contingency theory linking appropriate leader style to organizational conditions and subordinate personality. Transformational leadership theory concerns the behaviors leaders engage in that inspire high levels of motivation and performance in followers. The idea of charismatic leadership is part of transformational leadership theory.\n\nIn the late 1960s Henry Mintzberg, a graduate student at MIT, carefully studied the activities of five executives. On the basis of his observations, Mintzberg arrived at three categories that subsume managerial roles: interpersonal roles, decisional roles, and informational roles.\n\nBaron and Greenberg (2008) wrote that motivation involves \"the set of processes that arouse, direct, and maintain human behavior toward attaining some goal.\" There are several different theories of motivation relevant to OB, including equity theory, expectancy theory, Maslow's hierarchy of needs, incentive theory, organizational justice theory, Herzberg's two-factor theory, and Theory X and Theory Y.\n\nNational culture is thought to affect the behavior of individuals in organizations. This idea is exemplified by Hofstede's cultural dimensions theory. Hofstede surveyed a large number of cultures and identified six dimensions of national cultures that influence the behavior of individuals in organizations. These dimensions include power distance, individualism vs. collectivism, uncertainty avoidance, masculinity vs. femininity, long-term orientation vs. short term orientation, and indulgence vs. restraint.\n\nOrganizational citizenship behavior is behavior that goes beyond assigned tasks and contributes to the well-being of organizations.\n\nOrganizational culture reflects the values and behaviors that are commonly observed in an organization. Investigators who pursue this line of research assume that organizations can be characterized by cultural dimensions such as beliefs, values, rituals, symbols, and so forth. Researchers have developed models for understanding an organization's culture or developed typologies of organizational culture. Edgar Schein developed a model for understanding organizational culture. He identified three levels of organizational culture: (a) artifacts and behaviors, (b) espoused values, and (c) shared basic assumptions. Specific cultures have been related to organizational performance and effectiveness.\n\nPersonality concerns consistent patterns of behavior, cognition, and emotion in individuals. The study of personality in organizations has generally focused on the relation of specific traits to employee performance. There has been a particular focus on the Big Five personality traits, which refers to five overarching personality traits.\n\nThere are number of ways to characterize occupational stress. One way of characterizing it is to term it an imbalance between job demands (aspects of the job that require mental or physical effort) and resources that help manage the demands.\n\nChester Barnard recognized that individuals behave differently when acting in their work role than when acting in roles outside their work role. Work-family conflict occurs when the demands of family and work roles are incompatible, and the demands of at least one role interfere with the discharge of the demands of the other.\n\nOrganization theory is concerned with explaining the workings of an organization as a whole or of many organizations. The focus of organizational theory is to understand the structure and processes of organizations and how organizations interact with each other and the larger society.\n\nMax Weber argued that bureaucracy involved the application of rational-legal authority to the organization of work, making bureaucracy the most technically efficient form of organization. Weber enumerated a number of principles of bureaucratic organization including: a formal organizational hierarchy, management by rules, organization by functional specialty, selecting people based on their skills and technical qualifications, an \"up-focused\" (to organization's board or shareholders) or \"in-focused\" (to the organization itself) mission, and a purposefully impersonal environment (e.g., applying the same rules and structures to all members of the organization). These rules reflect Weberian \"ideal types,\" and how they are enacted in organizations varies according to local conditions. Charles Perrow extended Weber's work, arguing that all organizations can be understood in terms of bureaucracy and that organizational failures are more often a result of insufficient application of bureaucratic principles.\n\nAt least three theories are relevant here, theory of the firm, transaction cost economics, and agency theory.\n\nTheories pertaining to organizational structures and dynamics include complexity theory, French and Raven's five bases of power, hybrid organization theory, informal organizational theory, resource dependence theory, and Mintzberg's organigraph.\n\nThe systems framework is also fundamental to organizational theory. Organizations are complex, goal-oriented entities. Alexander Bogdanov, an early thinker in the field, developed his tectology, a theory widely considered a precursor of Bertalanffy's general systems theory. One of the aims of general systems theory was to model human organizations. Kurt Lewin, a social psychologist, was influential in developing a systems perspective with regard to organizations. He coined the term \"systems of ideology,\" partly based on his frustration with behaviorist psychology, which he believed to be an obstacle to sustainable work in psychology. Niklas Luhmann, a sociologist, developed a sociological systems theory.\n\nOrganizational ecology models apply concepts from evolutionary theory to the study of populations of organisations, focusing on birth (founding), growth and change, and death (firm mortality). In this view, organizations are 'selected' based on their fit with their operating environment.\n\nScientific management refers to an approach to management based on principles of engineering. It focuses on incentives and other practices empirically shown to improve productivity.\n\n\n"}
{"id": "728487", "url": "https://en.wikipedia.org/wiki?curid=728487", "title": "Pie chart", "text": "Pie chart\n\nA pie chart (or a circle chart) is a circular statistical graphic, which is divided into slices to illustrate numerical proportion. In a pie chart, the arc length of each slice (and consequently its central angle and area), is proportional to the quantity it represents. While it is named for its resemblance to a pie which has been sliced, there are variations on the way it can be presented. The earliest known pie chart is generally credited to William Playfair's \"Statistical Breviary\" of 1801.\n\nPie charts are very widely used in the business world and the mass media. However, they have been criticized, and many experts recommend avoiding them, pointing out that research has shown it is difficult to compare different sections of a given pie chart, or to compare data across different pie charts. Pie charts can be replaced in most cases by other plots such as the bar chart, box plot or dot plots.\n\nThe earliest known pie chart is generally credited to William Playfair's \"Statistical Breviary\" of 1801, in which two such graphs are used. Playfair presented an illustration, which contained a series of pie charts. One of those charts depicting the proportions of the Turkish Empire located in Asia, Europe and Africa before 1789. This invention was not widely used at first;\n\nThe French engineer Charles Joseph Minard was one of the first to use pie charts in 1858, in particular in maps. Minard's map, 1858 used pie charts to represent the cattle sent from all around France for consumption in Paris (1858).\n\nPlayfair thought that pie charts were in need of a third dimension to add additional information. It has been said that Florence Nightingale invented it, though in fact she just popularised it and she was later assumed to have created it due to the obscurity of Playfair's creation.\n\nA 3d pie chart, or perspective pie chart, is used to give the chart a 3D look. Often used for aesthetic reasons, the third dimension does not improve the reading of the data; on the contrary, these plots are difficult to interpret because of the distorted effect of perspective associated with the third dimension. The use of superfluous dimensions not used to display the data of interest is discouraged for charts in general, not only for pie charts.\n\nA doughnut chart (also spelled donut) is a variant of the pie chart, with a blank center allowing for additional information about the data as a whole to be included.\n\nA chart with one or more sectors separated from the rest of the disk is known as an \"exploded pie chart\". This effect is used to either highlight a sector, or to highlight smaller segments of the chart with small proportions.\n\nThe polar area diagram is similar to a usual pie chart, except sectors have equal angles and differ rather in how far each sector extends from the center of the circle.\nThe polar area diagram is used to plot cyclic phenomena (e.g., counts of deaths by month).\nFor example, if the counts of deaths in each month for a year are to be plotted then there will be 12 sectors (one per month) all with the same angle of 30 degrees each. The radius of each sector would be proportional to the square root of the death count for the month, so the area of a sector represents the number of deaths in a month.\nIf the death count in each month is subdivided by cause of death, it is possible to make multiple comparisons on one diagram, as is seen in the polar area diagram famously developed by Florence Nightingale.\n\nThe first known use of polar area diagrams was by André-Michel Guerry, which he called , in an 1829 paper showing seasonal and daily variation in wind direction over the year and births and deaths by hour of the day. Léon Lalanne later used a polar diagram to show the frequency of wind directions around compass points in 1843. The wind rose is still used by meteorologists. Nightingale published her rose diagram in 1858. Although the name \"coxcomb\" has come to be associated with this type of diagram, Nightingale originally used the term to refer to the publication in which this diagram first appeared--an attention-getting book of charts and tables--rather than to this specific type of diagram.\n\nA ring chart, also known as a sunburst chart or a multilevel pie chart, is used to visualize hierarchical data, depicted by concentric circles. The circle in the center represents the root node, with the hierarchy moving outward from the center. A segment of the inner circle bears a hierarchical relationship to those segments of the outer circle which lie within the angular sweep of the parent segment.\n\nA variant of the polar area chart is the spie chart designed by Dror Feitelson.\nThis superimposes a normal pie chart with a modified polar area chart to permit the comparison of two sets of related data. \nThe base pie chart represents the first data set in the usual way, with different slice sizes. The second set is represented by the superimposed polar area chart, using the same angles as the base, and adjusting the radii to fit the data. For example, the base pie chart could show the distribution of age and gender groups in a population, and the overlay their representation among road casualties. Age and gender groups that are especially susceptible to being involved in accidents then stand out as slices that extend beyond the original pie chart.\n\nSquare charts, also called Waffle Charts, are a form of pie charts that use squares instead of circles to represent percentages. Similar to basic circular pie charts, square pie charts take each percentage out of a total 100%.They are usually 10x10 grids, where each cell represents 1%. Despite the name, circles, pictograms (such as of people), and other shapes may be used instead of squares. The benefit to these is that it is easier to depict smaller percentages that would be hard to see on traditional pie charts. \n\nThe following example chart is based on preliminary results of the election for the European Parliament in 2004. The table lists the number of seats allocated to each party group, along with the derived percentage of the total that they each make up. The values in the last column, the derived central angle of each sector, Is found by multiplying the percentage by 360°.\n<nowiki>*</nowiki>Because of rounding, these totals do not add up to 100 and 360.\n\nThe size of each central angle is proportional to the size of the corresponding quantity, here the number of seats. Since the sum of the central angles has to be 360°, the central angle for a quantity that is a fraction \"Q\" of the total is 360\"Q\" degrees.\nIn the example, the central angle for the largest group (European People's Party (EPP)) is 135.7° because 0.377 times 360, rounded to one decimal place, equals 135.7.\n\nAn obvious flaw exhibited by pie charts is that they cannot show more than a few values without separating the visual encoding (the “slices”) from the data they represent (typically percentages). When slices become too small, pie charts have to rely on colors, textures or arrows so the reader can understand them. This makes them unsuitable for use with larger amounts of data. Pie charts also take up a larger amount of space on the page compared to the more flexible bar charts, which do not need to have separate legends, and can display other values such as averages or targets at the same time.\n\nStatisticians generally regard pie charts as a poor method of displaying information, and they are uncommon in scientific literature. One reason is that it is more difficult for comparisons to be made between the size of items in a chart when area is used instead of length and when different items are shown as different shapes.\nFurther, in research performed at AT&T Bell Laboratories, it was shown that comparison by angle was less accurate than comparison by length. This can be illustrated with the adjacent diagram, showing three pie charts, and, below each of them, the corresponding bar chart representing the same data. Most subjects have difficulty ordering the slices in the pie chart by size; when the bar chart is used the comparison is much easier. Similarly, comparisons between data sets are easier using the bar chart. However, if the goal is to compare a given category (a slice of the pie) with the total (the whole pie) in a single chart and the multiple is close to 25 or 50 percent, then a pie chart can often be more effective than a bar graph.\nThe example (left) is of a pie chart with 18 values, having to separate the data from its representation. Note also that several values are represented with the same color, making interpretation difficult.\n\nSeveral studies presented at the \"European Visualization Conference\" analyzed the relative accuracy of several pie chart formats, reaching the conclusion that pie charts and doughnut charts produce similar error levels when reading them, and square pie charts provide the most accurate reading.\n"}
{"id": "42557085", "url": "https://en.wikipedia.org/wiki?curid=42557085", "title": "Quasi-commutative property", "text": "Quasi-commutative property\n\nIn mathematics, the quasi-commutative property is an extension or generalization of the general commutative property. This property is used in specific applications with various definitions.\n\nTwo matrices \"p\" and \"q\" are said to have the commutative property whenever\n\nThe quasi-commutative property in matrices is defined as follows. Given two non-commutable matrices \"x\" and \"y\"\nsatisfy the quasi-commutative property whenever \"z\" satisfies the following properties:\n\nAn example is found in the matrix mechanics introduced by Heisenberg as a version of quantum mechanics. In this mechanics, \"p\" and \"q\" are infinite matrices corresponding respectively to the momentum and position variables of a particle. These matrices are written out at Matrix mechanics#Harmonic oscillator, and z = iħ times the infinite unit matrix, where ħ is the reduced Planck constant.\n\nA function \"f\", defined as follows:\nis said to be quasi-commutative if for all formula_6 and for all formula_7,\n\n"}
{"id": "6112194", "url": "https://en.wikipedia.org/wiki?curid=6112194", "title": "Real freedom", "text": "Real freedom\n\nReal freedom is a term coined by the political philosopher and economist Philippe Van Parijs. It expands upon notions of negative freedom by incorporating not simply institutional or other constraints on a person's choices, but also the requirements of physical reality, resources and personal capacity. To have real freedom, according to Van Parjis, an individual must:\n\nUnder this conception, a moral agent could be \"negatively\" free to take a holiday in Miami, because no-one is forcing them not to (condition 1 is met); but not \"really\" free to do so, because they cannot afford the flight (condition 2 is not met). Similarly, someone could be negatively free to swim across the English Channel; but not really free, because they are not a good enough swimmer and would not be able to succeed in the task. Real freedom is, then, a matter of \"degree\" — one is more or less really free, not just either really free or not; and no-one has \"complete\" real freedom — no-one is currently really free to teleport to Mars, for instance.\n\nReal freedom expands on negative freedom by adding the idea of actually being able to exercise a capacity or resource in the absence of constraint; but does not go as far as some ideas of positive freedom, by refraining from appeal to self-government by a real, best, or higher self.\n\nVan Parijs uses the concept of real freedom as part of his influential argument for a universal basic income.\n\n\n"}
{"id": "7996042", "url": "https://en.wikipedia.org/wiki?curid=7996042", "title": "Rigo 23", "text": "Rigo 23\n\nRigo 23 (born Ricardo Gouveia, 1966) is a Portuguese muralist, painter, and political artist residing in San Francisco, California. He is known in the San Francisco community for having painted a number of large, graphic \"sign\" murals including: \"One Tree\" next to the U.S. Route 101 on-ramp at 10th and Bryant Street, \"Innercity Home\" on a large public housing structure, \"Sky/Ground\" on a tall abandoned building at 3rd and Mission Street, and \"Extinct\" over a Shell gas station.\nRigo was born and raised on the island of Madeira. He later established himself as an artist in San Francisco, earning a BFA from San Francisco Art Institute in 1991 and an MFA from Stanford University in 1997. \nFrom 1984-2002, Rigo used the last two digits of the current year as part of his name, finally settling upon \"23\" in 2003.\n\nThe bulk of Rigo's work more literally highlights world politics and political prisoners from the Black Panthers and the Angola Three to Mumia Abu-Jamal, whose conviction for the murder of a policeman is contested, and the American Indian Movement's Leonard Peltier. Rigo create a controversial statue of Peltier that was removed from the grounds of the American University in January 2017.\n\nIn 2005, he created a statue based on the 1968 Olympics Black Power salute titled \"Victory Salute\", a twenty-two foot tall monument of two men: Tommie Smith and John Carlos. In the 1968 Olympic Games in Mexico City, these men each raised a black-gloved fist for human rights. Their simple gesture of the hand is considered as one of the most controversial statements of political and social activism in Olympic history. \"Victory Salute\" is a monument of that moment which was specifically built on the San Jose State University campus because Smith and Carlos were both student-athletes at the college.\n\nRigo is one of the founding members of Clarion Alley Mural Project collective and is still an active member, as of 2006, as well as an occasional professor at The San Francisco Art Institute.\nHe has also designed several installations as part of the 2006 Liverpool Biennial. He is considered by some art critics and curators to be part of the first generation of the San Francisco Mission School art movement. His work is in the collection of di Rosa.\n\n\n"}
{"id": "43678079", "url": "https://en.wikipedia.org/wiki?curid=43678079", "title": "Sex Industry Network", "text": "Sex Industry Network\n\nSex Industry Network (also known as SIN) is a peer-based, not for profit organisation, funded by SA Health, a South Australia government organisation. Its aim is to maintain low rates of sexually transmitted infections (STIs), blood borne viruses (BBVs) and HIV among sex workers and their clients in South Australia.\n\nSIN provides safer sex supplies such as condoms and lube and delivers outreach to brothels, parlours, private and street based sex workers.\n\nSIN was formed out of the Prostitutes Association of South Australia (PASA), a group formed in 1986 by sex workers.\n\nDuring this time community groups from priority populations (sex workers, men who have sex with men and injecting drug users) were being funded to deliver HIV prevention information and education within communities as part of Australia’s partnership approach to HIV/AIDS. In 1987, PASA received a grant from the SA Health Commission to conduct a three-month HIV/AIDS education project with sex workers called the ‘Travelling Parlour Show’. Several sex workers received training to become ‘peer educators’ and joined with a nurse from the sexually transmitted infections clinic and took their education and information sessions to the workplaces of sex workers across the metropolitan area of Adelaide.\n\nIn 1989, PASA and the AIDS council of South Australia (ACSA) joined forces to develop an ongoing HIV/AIDS education project with the sex workers of South Australia. This project has had several name changes including ‘the PASA project’ and ‘SWIPE’ but has been known as the SA Sex Industry Network or SIN since 1994.\n\nSIN was a founding member and continues to be an active member of Scarlet Alliance. In 2013, ACSA became insolvent and SIN was temporarily closed. Scarlet Alliance supported a group of volunteer sex workers in South Australia to keep limited services going while new arrangements were negotiated with the funding bodies. The SA health department agreed to fund Scarlet Alliance to complete ACSA’s original contract.\n\nSIN has supported Labor MP Steph Key's attempts to decriminalize prostitution in South Australia. In 2012, the decriminalisation bill was defeated by one vote. \nIn 2017, an improved bill which had support of sex workers, the working women's centre and unions was introduced into the upper house. The decriminalisation of sex work bill passed the upper house 13 votes to 8. . In August 2017, the lower house was set to debate the bill, however time ran out, resulting in a further delay to any progress.\n\n"}
{"id": "3469049", "url": "https://en.wikipedia.org/wiki?curid=3469049", "title": "Straight and Crooked Thinking", "text": "Straight and Crooked Thinking\n\nStraight and Crooked Thinking, first published in 1930 and revised in 1953, is a book by Robert H. Thouless which describes, assesses and critically analyses flaws in reasoning and argument. Thouless describes it as a practical manual, rather than a theoretical one.\n\nThirty-eight fallacies are discussed in the book. Among them are:\n\n\n"}
{"id": "213682", "url": "https://en.wikipedia.org/wiki?curid=213682", "title": "Thermal depolymerization", "text": "Thermal depolymerization\n\nThermal depolymerization (TDP) is a depolymerization process using hydrous pyrolysis for the reduction of complex organic materials (usually waste products of various sorts, often biomass and plastic) into light crude oil. It mimics the natural geological processes thought to be involved in the production of fossil fuels. Under pressure and heat, long chain polymers of hydrogen, oxygen, and carbon decompose into short-chain petroleum hydrocarbons with a maximum length of around 18 carbons.\n\nThermal depolymerisation is similar to other processes which use superheated water as a major step to produce fuels, such as direct Hydrothermal Liquefaction.\nThese are distinct from processes using dry materials to depolymerize, such as pyrolysis. The term Thermochemical Conversion (TCC) has also been used for conversion of biomass to oils, using superheated water, although it is more usually applied to fuel production via pyrolysis.\nOther commercial scale processes include the \"SlurryCarb\" process operated by EnerTech, which uses similar technology to decarboxylate wet solid biowaste, which can then be physically dewatered and used as a solid fuel called E-Fuel. The plant in Rialto, California was designed to process 683 tons of waste per day. However, it failed to perform to design standards and was closed down. The Rialto facility defaulted on its bond payments and is in the process of being liquidated. \nThe Hydro Thermal Upgrading (HTU) process uses superheated water to produce oil from domestic waste.\nA demonstration plant is due to start up in The Netherlands said to be capable of processing 64 tons of biomass (dry basis) per day into oil. Thermal depolymerisation differs in that it contains a hydrous process followed by an anhydrous cracking / distillation process.\n\nThermal depolymerization is similar to the geological processes that produced the fossil fuels used today, except that the technological process occurs in a timeframe measured in hours. Until recently, the human-designed processes were not efficient enough to serve as a practical source of fuel—more energy was required than was produced.\n\nThe first industrial process to obtain gas, diesel fuels and other petroleum products through pyrolysis of coal, tar or biomass was designed and patented in the late 1920s by Fischer-Tropsch. In U. S. patent 2,177,557, issued in 1939, Bergstrom and Cederquist discuss a method for obtaining oil from wood in which the wood is heated under pressure in water with a significant amount of calcium hydroxide added to the mixture. In the early 1970s Herbert R. Appell and coworkers worked with hydrous pyrolysis methods, as exemplified by U. S. patent 3,733,255 (issued in 1973), which discusses the production of oil from sewer sludge and municipal refuse by heating the material in water, under pressure, and in the presence of carbon monoxide.\n\nAn approach that exceeded break-even was developed by Illinois microbiologist Paul Baskis in the 1980s and refined over the next 15 years (see U. S. patent 5,269,947, issued in 1993). The technology was finally developed for commercial use in 1996 by Changing World Technologies (CWT). Brian S. Appel (CEO of CWT) took the technology in 2001 and expanded and changed it into what is now referred to as TCP (Thermal Conversion Process), and has applied for and obtained several patents (see, for example, published patent 8,003,833, issued August 23, 2011). A Thermal Depolymerization demonstration plant was completed in 1999 in Philadelphia by Thermal Depolymerization, LLC, and the first full-scale commercial plant was constructed in Carthage, Missouri, about from ConAgra Foods' massive Butterball turkey plant, where it is expected to process about 200 tons of turkey waste into of oil per day.\n\nIn the method used by CWT, the water improves the heating process and contributes hydrogen to the reactions.\n\nIn the Changing World Technologies (CWT) process, the feedstock material is first ground into small chunks, and mixed with water if it is especially dry. It is then fed into a pressure vessel reaction chamber where it is heated at constant volume to around 250 °C. Similar to a pressure cooker (except at much higher pressure), steam naturally raises the pressure to 600 psi (4 MPa) (near the point of saturated water). These conditions are held for approximately 15 minutes to fully heat the mixture, after which the pressure is rapidly released to boil off most of the water (see: Flash evaporation). The result is a mix of crude hydrocarbons and solid minerals. The minerals are removed, and the hydrocarbons are sent to a second-stage reactor where they are heated to 500 °C, further breaking down the longer hydrocarbon chains. The hydrocarbons are then sorted by fractional distillation, in a process similar to conventional oil refining.\n\nThe CWT company claims that 15 to 20% of feedstock energy is used to provide energy for the plant. The remaining energy is available in the converted product. Working with turkey offal as the feedstock, the process proved to have yield efficiencies of approximately 85%; in other words, the energy contained in the end products of the process is 85% of the energy contained in the inputs to the process (most notably the energy content of the feedstock, but also including electricity for pumps and natural gas or woodgas for heating). If one considers the energy content of the feedstock to be free (i.e., waste material from some other process), then 85 units of energy are made available for every 15 units of energy consumed in process heat and electricity. This means the \"Energy Returned on Energy Invested\" (EROEI) is (6.67), which is comparable to other energy harvesting processes. Higher efficiencies may be possible with drier and more carbon-rich feedstocks, such as waste plastic.\n\nBy comparison, the current processes used to produce ethanol and biodiesel from agricultural sources have EROEI in the 4.2 range, when the energy used to produce the feedstocks is accounted for (in this case, usually sugar cane, corn, soybeans and the like). These EROEI values are not directly comparable, because these EROEI calculations include the energy cost to produce the feedstock, whereas the above EROEI calculation for thermal depolymerization process (TDP) does not.\n\nThe process breaks down almost all materials that are fed into it. TDP even efficiently breaks down many types of hazardous materials, such as poisons and difficult-to-destroy biological agents such as prions.\n\n\"(Note: Paper/cellulose contains at least 1% minerals, which was probably grouped under carbon solids.)\"\n\nAs reported on 04/02/2006 by Discover Magazine, a Carthage, Missouri plant was producing of oil made from 270 tons of turkey entrails and 20 tons of hog lard. This represents an oil yield of 22.3 percent. The Carthage plant produces API 40+, a high value crude oil. It contains light and heavy naphthas, a kerosene, and a gas oil fraction, with essentially no heavy fuel oils, tars, asphaltenes or waxes. It can be further refined to produce No. 2 and No. 4 fuel oils.\n\nThe fixed carbon solids produced by the TDP process have multiple uses as a filter, a fuel source and a fertilizer. It can be used as activated carbon in wastewater treatment, as a fertilizer, or as a fuel similar to coal.\n\nThe process can break down organic poisons, due to breaking chemical bonds and destroying the molecular shape needed for the poison's activity. It is likely to be highly effective at killing pathogens, including prions. It can also safely remove heavy metals from the samples by converting them from their ionized or organometallic forms to their stable oxides which can be safely separated from the other products.\n\nAlong with similar processes, it is a method of recycling the energy content of organic materials without first removing the water. It can produce liquid fuel, which separates from the water physically without need for drying. Other methods to recover energy often require pre-drying (e.g. burning, pyrolysis) or produce gaseous products (e.g. anaerobic digestion).\n\nThe United States Environmental Protection Agency estimates that in 2006 there were 251 million tons of municipal solid waste, or 4.6 pounds generated per day per person in the USA. Much of this mass is considered unsuitable for oil conversion.\n\nThe process only breaks long molecular chains into shorter ones, so small molecules such as carbon dioxide or methane cannot be converted to oil through this process. However, the methane in the feedstock is recovered and burned to heat the water that is an essential part of the process. In addition, the gas can be burned in a combined heat and power plant, consisting of a gas turbine which drives a generator to create electricity, and a heat exchanger to heat the process input water from the exhaust gas. The electricity can be sold to the power grid, for example under a feed-in tariff scheme. This also increases the overall efficiency of the process (already said to be over 85% of feedstock energy content).\n\nAnother option is to sell the methane product as biogas. For example, biogas can be compressed, much like natural gas, and used to power motor vehicles.\n\nMany agricultural and animal wastes could be processed, but many of these are already used as fertilizer, animal feed, and, in some cases, as feedstocks for paper mills or as boiler fuel. Energy crops constitute another potentially large feedstock for thermal depolymerization.\n\nReports in 2004 claimed that the Carthage facility was selling products at 10% below the price of equivalent oil, but its production costs were low enough that it produced a profit. At the time it was paying for turkey waste (see also below).\n\nThe plant then consumed 270 tons of turkey offal (the full output of the turkey processing plant) and 20 tons of egg production waste daily. In February 2005, the Carthage plant was producing about of crude oil.\n\nIn April 2005 the plant was reported to be running at a loss. Further 2005 reports summarized some economic setbacks which the Carthage plant encountered since its planning stages. It was thought that concern over mad cow disease would prevent the use of turkey waste and other animal products as cattle feed, and thus this waste would be free. As it turned out, turkey waste may still be used as feed in the United States, so that the facility must purchase that feed stock at a cost of $30 to $40 per ton, adding $15 to $20 per barrel to the cost of the oil. Final cost, as of January 2005, was $80/barrel ($1.90/gal).\n\nThe above cost of production also excludes the operating cost of the thermal oxidizer and scrubber added in May 2005 in response to odor complaints (see below).\n\nA biofuel tax credit of roughly $1 per US gallon (26 ¢/L) on production costs was not available because the oil produced did not meet the definition of \"biodiesel\" according to the relevant American tax legislation. The Energy Policy Act of 2005 specifically added thermal depolymerization to a $1 renewable diesel credit, which became effective at the end of 2005, allowing a profit of $4/barrel of output oil.\n\nThe company has explored expansion in California, Pennsylvania, and Virginia, and is presently examining projects in Europe, where animal products cannot be used as cattle feed. TDP is also being considered as an alternative means for sewage treatment in the United States.\n\nThe pilot plant in Carthage was temporarily shut down due to smell complaints. It was soon restarted when it was discovered that few of the odors were generated by the plant. Furthermore, the plant agreed to install an enhanced thermal oxidizer and to upgrade its air scrubber system under a court order. Since the plant is located only four blocks from the tourist-attracting town center, this has strained relations with the mayor and citizens of Carthage.\n\nAccording to a company spokeswoman, the plant has received complaints even on days when it is not operating. She also contended that the odors may not have been produced by their facility, which is located near several other agricultural processing plants.\n\nOn December 29, 2005, the plant was ordered by the state governor to shut down once again over allegations of foul odors as reported by MSNBC.\n\nAs of March 7, 2006, the plant has begun limited test runs to validate it has resolved the odor issue.\n\nAs of August 24, 2006, the last lawsuit connected with the odor issue has been dismissed and the problem is acknowledged as fixed. In late November, however, another complaint was filed over bad smells. This complaint was closed on January 11 of 2007 with no fines assessed.\n\nA May 2003 article in Discover magazine stated, \"Appel has lined up federal grant money to help build demonstration plants to process chicken offal and manure in Alabama and crop residuals and grease in Nevada. Also in the works are plants to process turkey waste and manure in Colorado and pork and cheese waste in Italy. He says the first generation of depolymerization centers will be up and running in 2005. By then it should be clear whether the technology is as miraculous as its backers claim.\"\n\nHowever, as of August 2008, the only operational plant listed at the company's website is the initial one in Carthage, Missouri.\n\nChanging World Technology applied for an IPO on August 12; 2008, hoping to raise $100 million.\n\nThe unusual Dutch Auction type IPO failed possibly because CWT has lost nearly $20 million with very little revenue.\n\nCWT, the parent company of Renewable Energy Solutions, filed for Chapter 11 bankruptcy. No details on plans for the Carthage plant have been released.\n\nIn April 2013, CWT was acquired by a Canadian firm, Ridgeline Energy Services, based in Calgary.\n\n\n\n"}
{"id": "32120", "url": "https://en.wikipedia.org/wiki?curid=32120", "title": "Universal (metaphysics)", "text": "Universal (metaphysics)\n\nIn metaphysics, a universal is what particular things have in common, namely characteristics or qualities. In other words, universals are repeatable or recurrent entities that can be instantiated or exemplified by many particular things. For example, suppose there are two chairs in a room, each of which is green. These two chairs both share the quality of \"chairness\", as well as greenness or the quality of being green; in other words, they share a \"universal\". There are three major kinds of qualities or characteristics: types or kinds (e.g. mammal), properties (e.g. short, strong), and relations (e.g. father of, next to). These are all different types of universals.\n\nParadigmatically, universals are \"abstract\" (e.g. humanity), whereas particulars are \"concrete\" (e.g. the personhood of Socrates). However, universals are not necessarily abstract and particulars are not necessarily concrete. For example, one might hold that numbers are particular yet abstract objects. Likewise, some philosophers, such as D. M. Armstrong, consider universals to be concrete.\n\nMost do not consider classes to be universals, although some prominent philosophers do, such as John Bigelow.\n\n\"The problem of universals\" is an ancient problem in metaphysics about whether universals exist. The problem arises from attempts to account for the phenomenon of similarity or attribute agreement among things. For example, grass and Granny Smith apples are similar or agree in attribute, namely in having the attribute of greenness. The issue is how to account for this sort of agreement in attribute among things.\n\nThere are many philosophical positions regarding universals. Taking \"beauty\" as an example, three positions are:\n\nTaking a broader view, the main positions are generally considered classifiable as: realism, nominalism, and idealism (sometimes simply named \"anti-realism\" with regard to universals). Realists posit the existence of independent, abstract universals to account for attribute agreement. Nominalists deny that universals exist, claiming that they are not necessary to explain attribute agreement. Conceptualists posit that universals exist only in the mind, or when conceptualized, denying the independent existence of universals. Complications which arise include the implications of language use and the complexity of relating language to ontology.\n\nA universal may have instances, known as its \"particulars\". For example, the type \"dog\" (or \"doghood\") is a universal, as are the property \"red\" (or \"redness\") and the relation \"betweenness\" (or \"being between\"). Any particular dog, red thing, or object that is between other things is not a universal, however, but is an \"instance\" of a universal. That is, a universal type (\"doghood\"), property (\"redness\"), or relation (\"betweenness\") \"inheres\" in a particular object (a specific dog, red thing, or object between other things).\n\nPlatonic realism holds universals to be the referents of general terms, such as the \"abstract\", nonphysical, non-mental entities to which words such as \"sameness\", \"circularity\", and \"beauty\" refer. Particulars are the referents of proper names, such as \"Phaedo,\" or of definite descriptions that identify single objects, such as the phrase, \"that bed over there\". Other metaphysical theories may use the terminology of universals to describe physical entities.\n\nPlato's examples of what we might today call universals included mathematical and geometrical ideas such as a circle and natural numbers as universals. Plato's views on universals did, however, vary across several different discussions. In some cases, Plato spoke as if the perfect circle functioned as the form or blueprint for all copies and for the word definition of \"circle\". In other discussions, Plato describes particulars as \"participating\" in the associated universal.\n\nContemporary realists agree with the thesis that universals are multiply-exemplifiable entities. Examples include by D. M. Armstrong, Nicholas Wolterstorff, Reinhardt Grossmann, Michael Loux.\n\nNominalists hold that universals are not real mind-independent entities but either merely concepts (sometimes called \"conceptualism\") or merely names. Nominalists typically argue that properties are abstract particulars (like tropes) rather than universals. JP Moreland distinguishes between \"extreme\" and \"moderate\" nominalism. Examples of nominalists include the medieval philosophers Roscelin of Compiègne and William of Ockham and contemporary philosophers W. V. O. Quine, Wilfred Sellars, D. C. Williams, and Keith Campbell.\n\nThe ness-ity-hood principle is used mainly by English-speaking philosophers to generate convenient, concise names for universals or properties. According to the Ness-Ity-Hood Principle, a name for any universal may be formed that is distinctive, \"of left-handers\" may be formed by taking the predicate \"left-handed\" and adding \"ness\", which yields the name \"left-handedness\". The principle is most helpful in cases where there is not an established or standard name of the universal in ordinary English usage: What is the name of the universal distinctive of chairs? \"Chair\" in English is used not only as a subject (as in \"The chair is broken\"), but also as a predicate (as in \"That is a chair\"). So to generate a name for the universal distinctive of chairs, take the predicate \"chair\" and add \"ness\", which yields \"chairness\".\n\n\n\n\n"}
{"id": "1007662", "url": "https://en.wikipedia.org/wiki?curid=1007662", "title": "Ussuri black bear", "text": "Ussuri black bear\n\nThe Ussuri black bear (\"Ursus thibetanus ussuricus\"), also known as the Manchurian black bear, is a large subspecies of the Asian black bear native to the Far East, including the Korean Peninsula.\n\nThe subspecies is named after the Ussuri River.\n\nSympatric predators include the Ussuri brown bear and tiger.\n"}
{"id": "2615272", "url": "https://en.wikipedia.org/wiki?curid=2615272", "title": "V. M. Tarkunde", "text": "V. M. Tarkunde\n\nVithal Mahadeo Tarkunde (3 July 1909 in Saswad – 22 March 2004 in Delhi), was a prominent Indian lawyer, civil rights activist, and humanist leader and has been referred to as the \"Father of the Civil Liberties movement\" in India and a former judge of the Bombay High Court The Supreme Court of India also praised him as \"undoubtedly the most distinguished judge of the post-Chagla 1957 period\" in the Bombay High Court.\n\nVithal Mahadeo Tarkunde was born in Saswad, Pune District, Maharashtra on 3 July 1909. He was the 2nd of the five children of Mahadeo Rajaram Tarkunde, a popular lawyer and social reformer at Saswad,then headquarters of Purandar taluka adjoining Pune. His father, a Brahmin by caste, had fought against the practice of untouchability.\n\nIn 1920 he migrated from Saswad to Pune and joined the New English School, Pune. In the Matriculation examination of 1925 held by the Bombay University, he stood first in the erstwhile Bombay Presidency. He also secured the prestigious Jagannath Shankersheth Scholarship for Sanskrit. He then joined the Fergusson College for BA which he completed in 1929, subsequently moving to London, where he attended the Lincoln's Inn and qualified as a Barrister-at-Law in 1931. He also attended lectures in economics, political science and social anthropology at the London School of Economics(LSE) as an external student. He returned to India the next year in December and commenced his legal practice in Pune.\n\nTarkunde started practice at Pune soon after he returned to India in 1933. He continued there till 1942 when he gave up his practice to become a full-time member of the Radical Democratic Party. He resumed his legal practice in the Bombay High Court in 1948 after Independence and was elevated to the bench as a Judge of the Bombay High Court in September 1957. He stepped down voluntarily as Judge of the Bombay High Court in 1969 and set up practice in the Supreme Court of India where he continued till his resignation in 1977 at the age of 68. He was chiefly concerned with Public Interest Litigations and constitutional cases, most of which he conducted with little or no fees.\n\nIn 1933, he joined the Congress Socialist Party(CSP) and the Indian National Congress but later left the CSP disillusioned with their vote against Subhas Chandra Bose in the January 1939 Tripuri session of Congress. He then joined the League of Radical Congressmen led by his mentor M. N. Roy in April 1939.\n\nIn 1940, Roy and Tarkunde, along with several others, left the Congress after dissenting on the question of participation in the Second World War. Roy advocated participation in the war against the Axis powers, while simultaneously striving for Indian independence, and founded the Radical Democratic Party to further this cause.In 1942, Tarkunde gave up his legal practice to become a full-time member of the Radical Democratic Party and was elected General Secretary of the RDP in 1944, thereby migrating to Delhi. By 1946 Roy formulated the philosophy of New Humanism. By 1948 he and Roy decided that political parties were an inadequate instrument for promoting freedom of the people and so dissolved the RDP in December 1948. He returned to legal practice the same year.\n\nIn 1969, Tarkunde founded the Indian Radical Humanist Association as an organisation for radical humanists. He also began editing the \"Radical Humanist\" (founded in 1937 by Roy as \"Independent India\") in April 1970, supporting it initially with his own income. In 1973 he was one of the signers of the Humanist Manifesto.\n\nDuring the emergency, he worked closely with Jayaprakash Narayan, providing leadership to the NGOs Citizens for Democracy and People's Union for Civil Liberties, of which he was the founding president. He also worked on the Citizen's Justice Committee and played a principal part in resisting and investigating the excesses of the period, including the 1984 Anti-Sikh riots, and human rights violations in the Punjab, Kashmir, and the North-East. His refusal to consider kashmiri pandits who had fled valley in 1990 as human right victims caused much controversy and led to his dubbing as \" Terrorists' defender in chief\" as he regularly attacked Indian army for fake encounters and extra judicial killings.In 1995, he departed from his earlier stand of considering firing by police as human rights violation and defended UP government in Muzaffarnagar police firing and rape on Uttarakhand state demand activists on 2 October 1994 in Supreme Court.His volte face was noted by honourable bench with humour and he won the case with court ruling that there was not adequate evidence of wilful human rights violation by State government.But it led to his breaking ranks with radical humanists.\n\nTarkunde was a board member for the International Humanist and Ethical Union(IHEU), the world union of Humanist organisations for over 40 years.\n\nAt the 1978 London Congress of the IHEU, VM Tarkunde received the International Humanist Award 1978. The Government of India awarded him the civilian honour of the Padma Bhushan in 1998.\n\n\n"}
{"id": "43382131", "url": "https://en.wikipedia.org/wiki?curid=43382131", "title": "Vidya (Knowledge)", "text": "Vidya (Knowledge)\n\nThe Sanskrit word, Vidya, figures prominently in all texts pertaining to Indian philosophy - to mean science, learning, knowledge and scholarship; most importantly, it refers to valid knowledge which cannot be contradicted and true knowledge which is the knowledge of the Self intuitively gained. \"Vidya\" is not mere intellectual knowledge, for the Vedas demand understanding.\n\nVidya or \"vidyā\" (Sanskrit: विद्या) primarily means science, learning, philosophy, knowledge, scholarship, any knowledge whether true or false. Its root is \"Vid\" (Sanskrit: विद्) which means - to reason upon, knower, finding, knowing, acquiring or understanding.\n\nIn Hindu philosophy, \" Vidyā\" refers to the knowledge of the soul or spiritual knowledge; it refers to the study of the six schools of Hindu philosophy – Nyaya, Yoga, Vaisheshika, Samkhya, Purvamimamsa and Uttaramimamsa. The process of gaining the knowledge of the Atman cannot commence unless one has explored the \"Prānavidya\" or \"Agnividya\" to the full in all its numerous phase; through \"vidyā\" or \" upasana \" to \"jnana \" was always the eternal order indicated by the Upanishads. \"Jnāna\" dawns after the completion and perfection of the being through the \"vidyās\"; then, one crosses over beyond birth and death having already destroyed the bonds of death.\n\nDuring the Vedic period, \"Vidyādāna\" or the gift for the sake of education was considered to be the best of gifts, possessing a higher religious efficacy than even the gift of land. \"Vidyā\" comes from the root \"Vid\" ('to know'), it therefore means knowledge, science, learning, lore, scholarship and philosophy. There are basically four \"Vidyas\" – 1) \"Trayi\" (triple) which is the study of the Vedas and their auxiliary texts, 2) \"Anviksiki\" which is logic and metaphysics, 3) \"Dandaniti\" which is the science of government, and 4) \"Varum\", the practical arts such as agriculture, commerce, medicine etc. \"Vidyā\" gives insight, in the spiritual sphere it leads to salvation, in the mundane sphere it leads to progress and prosperity. \"Vidyā\" illuminates the mind and shatters illusions, increases intelligence, power and efficiency; develops the intellect and makes it more re-fined; it effects a complete transformation as the root of all happiness and as the source of illumination and power. The word, \"Vidyā\", does not occur in the Rig Veda, it occurs in the Atharvaveda and in the Brahmana portions of the Yajurveda and in the Upanishads.\n\nIn Hinduism, Goddesses are personifications of the deepest level of power and energy. The concept of Shakti, in its most abstract terms, relates to the energetic principle of Ultimate Reality, the dynamic aspect of the divine. This concept surfaces in the Kena Upanishad as Goddess Umā bestowing \"Brahma-vidya\" on Indra; when linked with \"shakti\" and \"maya\", she embodies the power of illusion (\"maya\"), encompassing ignorance (\"avidya\") and knowledge (\"vidyā\") and thereby presented with a dual personality. According to the Saktas, Māyā is basically a positive, creative, magical energy of the Goddess that brings forth the universe. The ten \"Mahāvidyās\" are bestowers or personifications of transcendent and liberating religious knowledge; the term \"Vidyā\" in this context refers to power, essence of reality and the \"mantras\". The gentle and motherly forms of Goddess Sri Vidyā are 'right-handed'. When the awareness of the 'exterior' (Shiva) combined with the \"I\" encompasses the entire space as \"I\" it is called \"sada-siva-tattva\". When later, discarding the abstraction of the Self and the exterior, clear identification with the insentient space takes place, it is called \"isvara-tattva\"; the investigation of these two last steps is pure \"vidyā\" (knowledge). \"Māyā\", which has been identified with \"Prakrti\" in the Shvetashvatara Upanishad represents its three \"gunas\"; also identified with \"avidyā\", which term primarily means the dark abyss of non-being and secondarily the mysterious darkness of the unmanifest state, \"Māyā\" binds through \"avidyā\" and releases through \"vidyā\" .\n\nIn Mahayana texts, the female divinities are designated grammatically feminine terms \"Dhārāni\" and \"Vidyā\". \"Dharani\" refers to mantras, the sounds that carry the essence or energy of a deity, which enable contact with the goddess on her plane of reality because the mantras invoke all deities. \"Vidyā\" is also synonymous with mantra and refers to the mantric invocation of female deities. In Tibet, the word, \"rigpa\" , meaning \"vidyā\", refers to the non-dualistic awareness or intrinsic awareness. In Theravada Buddhism, \"vidyā\" means 'non-dual awareness' of three marks of existence.\n\n\"Agni Vidyā\" or the science of Fire is said to be the greatest discovery of the ancient Indians who gained direct experience of divine fire through continuous research, contemplation, observation and experimentation; their experience led them to discover ways of using this knowledge to heal and nurture the outer and the inner worlds. To them fire is sacred, and because of the pervasive nature of fire all things are sacred. Body and mind which are extensions of the fire that the soul spontaneously emits are also sacred. Within the body the most significant centres of fire are more subtle than those of the sense organs. They are called the \"chakras\" which are seven fields of sacred fire. The understanding of the role of fire without and within gives proper self-understanding which understanding is gained through yogic practices. The performance of yajnas is the \"karma-kānda\" aspect of \"agni vidyā\". All rituals follow set rules and conditions. The main function of the fire ritual is to make an offering to nature’s finest forces and divinities that fill the space of inner consciousness; fire carries oblations to these forces and divinities. The fire has seven tongues all having unique qualities. The gods, goddesses, divinities and nature’s forces are grouped in seven main categories which match with the qualities of the seven tongues of fire.\n\n\"Atmaikatva\" or the absolute oneness of the self is the theme of entire Advaita Vedanta which distinguishes six \"pramanas\" or means of valid knowledge, but this \"vidyā\" or knowledge of Brahman is \"guhahita\", \"gahavareshta\" i.e. set in the secret place and hidden in its depth, unattainable except through \"adhyātma-yoga\", the meditation centering upon the nature of the self. Vedanta literature is only preparatory to it, it dispels ignorance and makes the mind receptive but does not reveal the truth therefore it is an indirect means of knowledge. The oneness of the self, which is self-established and self-shining, is called \"vidyā\" in cosmic reference which reveals the true nature of Brahman, the self-shining pure consciousness which is not a \"visaya\" ('object matter or content') but the one subject, transcendent of all conventional subjects and objects. The Self or the Atman is to be sought, the Self is to be enquired into, known and understood.\n\nThe sage of the Mundaka Upanishad(Verse I.1.4), more in the context of the ritualistic than of epistemological concerns, states that there are two kinds of knowledge (\"vidyā\") to be attained, the higher (\"para\") and the lower (\"apara\"). \"Para vidyā\", the higher knowledge, is knowledge of the Absolute (Brahman, Atman); \"Apara\", the lower knowledge, is knowledge of the world – of objects, events, means, ends, virtues and vices. \"Para vidyā\" has Reality as its content; \"Apara vidyā\", the phenomenal world. According to Advaita Vedanta, \"Para vidyā\", by the nature of its content, possesses a unique quality of ultimacy that annuls any supposed ultimacy that might be attached to any other or form of knowledge, and is intuitively gained as self-certifying. Once Brahman is realized all other modes of knowledge are seen to be touched by \"avidyā\", the root of ignorance. In this context, \"Vidyā\" means true knowledge.\nHowever, it is argued that the Advaita Vedanta interpretation does not answer the final question: what is the reality or truth-value of \"avidyā\" or what is the substratum that is the basis or cause of \"avidyā\"?\n\nThe Upanishads teach us that the knowledge of difference is \"avidyā\" or ignorance, and the knowledge of identity is true knowledge or \"vidyā\" or valid knowledge, which leads to life eternal. For the Cārvākas, perception is the only means of valid knowledge (\"pramana\"). Vadi Deva Suri of the Jaina school defines valid knowledge as determinate cognition which apprehends itself and an object and which is capable of prompting activity which attains a desirable object or rejects an undesirable object; the result of valid knowledge is cessation of ignorance. Vaisheshikas recognized four kinds of valid knowledge – Perception, Inference, Recollection and Intuition. The Mimamsa schools introduced the concept of intrinsic validity of knowledge (\"svatahpramanya\") and extrinsic validity of knowledge (\"parastah-apramana\") but agreed that the validity of knowledge cannot be determined by the knowledge of any special excellence in its cause or the knowledge of its harmony with the real nature of its object or the knowledge of a fruitful action. Sankara accepted perception, inference, scriptural testimony, comparison, presumption and non-apprehension as the six sources of knowledge and concluded that the knowledge which corresponds with the real nature of its object is valid. The Atman is the reality in the empirical self as the ever present foundational subject-objectless universal consciousness which sustains the empirical self.\n\nIn \"upāsanā\" the movement starts from the outer extremities and gradually penetrates into the inmost recesses of the soul, and the whole investigation is conducted in two spheres, in the subject as well as in the object, in the individual as well as in the world, in the \"aham\" as also in the \" idam \", in the \"adhyātma\" and also in \"adhidaiva\" spheres and conducted synthetically as well as analytically, through \"apti\" as well as \"samrddhi\", which the Bhagavad Gita calls \" yoga \" and \" vibhooti \". The \"vidyās\" do not rest content in knowing the reality simply as a whole but proceed further to comprehend it in all its infinite details too. The higher includes the lower grades and adds something more to it and never rejects it; the lower has its fulfilment in the higher and finds its consummation there but never faces extinction. All forms of contemplation have only one aim: to lead to the Supreme Knowledge and hence they are termed as \"vidyās\"; through \"vidyā\", which is \"amrta\", one attains immortality (Shvetashvatara Upanishad Verse V.1). \"Dahara Vidyā\", \"Udgitha Vidyā\" and \"Madhu Vidyā\" are the synthetic way whereas the analytic way is signified by the Sleeping man of the \"Garga-Ajātsatru\" episode and by the Five Sheaths, which ways show that the world and the individual spring from the same eternal source.\n"}
{"id": "3995195", "url": "https://en.wikipedia.org/wiki?curid=3995195", "title": "Vopěnka's principle", "text": "Vopěnka's principle\n\nIn mathematics, Vopěnka's principle is a large cardinal axiom. \nThe intuition behind the axiom is that the set-theoretical universe is so large that in every proper class, some members are similar to others, with this similarity formalized through elementary embeddings.\n\nVopěnka's principle was first introduced by Petr Vopěnka and independently considered by H. Jerome Keisler, and was written up by .\nAccording to , Vopěnka's principle was originally intended as a joke: Vopěnka was apparently unenthusiastic about large cardinals and introduced his principle as a bogus large cardinal property, planning to show later that it was not consistent. However, before publishing his inconsistency proof he found a flaw in it.\n\nVopěnka's principle asserts that for every proper class of binary relations (each with set-sized domain), there is one elementarily embeddable into another. This cannot be stated as a single sentence of ZFC as it involves a quantification over classes. A cardinal κ is called a Vopěnka cardinal if it is inaccessible and Vopěnka's principle holds in the rank \"V\" (allowing arbitrary \"S\" ⊂ \"V\" as \"classes\").\nMany equivalent formulations are possible.\nFor example, Vopěnka's principle is equivalent to each of the following statements. \n\nEven when restricted to predicates and proper classes definable in first order set theory, the principle implies existence of Σ correct extendible cardinals for every \"n\".\n\nIf κ is an almost huge cardinal, then a strong form of Vopěnka's principle holds in \"V\":\n\n\n gives a number of equivalent definitions of Vopěnka's principle.\n"}
{"id": "418179", "url": "https://en.wikipedia.org/wiki?curid=418179", "title": "Whyte notation", "text": "Whyte notation\n\nThe Whyte notation for classifying steam locomotives by wheel arrangement was devised by Frederick Methvan Whyte, and came into use in the early twentieth century following a December 1900 editorial in \"American Engineer and Railroad Journal\". The notation counts the number of leading wheels, then the number of driving wheels, and finally the number of trailing wheels, groups of numbers being separated by dashes. Other classification schemes, like UIC classification and the French, Turkish and Swiss systems for steam locomotives, count axles rather than wheels.\n\nIn the notation a locomotive with two leading axles (four wheels) in front, then three driving axles (six wheels) and then one trailing axle (two wheels) is classified as 4-6-2, and is commonly known as a \"Pacific\".\n\nArticulated locomotives such as Garratts, which are effectively two locomotives joined by a common boiler, have a + between the arrangements of each engine. Thus a \"double Pacific\" type Garratt is a 4-6-2+2-6-4. For Garratt locomotives the + sign is used even when there are no intermediate unpowered wheels, e.g. the LMS Garratt 2-6-0+0-6-2. This is because the two engine units are more than just power bogies. They are complete engines, carrying fuel and water tanks. The + sign represents the bridge (carrying the boiler) that links the two engines.\n\nSimpler articulated types such as Mallets, have a jointed frame under a common boiler where there are no unpowered wheels between the sets of powered wheels. Typically, the forward frame is free to swing, whereas the rear frame is rigid with the boiler. Thus a Union Pacific Big Boy is a 4-8-8-4; four leading wheels, one group of eight driving wheels, another group of eight driving wheels, and then four trailing wheels.\n\nThis numbering system is shared by duplex locomotives, which have powered wheel sets sharing a rigid frame.\n\nNo suffix means a tender locomotive.\n\nT indicates a tank locomotive: in European practice, this is sometimes extended to indicate the type of tank locomotive: T means side tank, PT pannier tank, ST saddle tank, WT well tank. T+T means a tank locomotive that also has a tender.\n\nIn Europe, the suffix R can signify rack (0-6-0RT) or reversible (0-6-0TR), the latter being \"Bi-cabine\" locomotives used in France.\n\nThe suffix F indicates a fireless locomotive (0-4-0F). This locomotive has no tender.\n\nOther suffixes have been used, including ng for narrow-gauge (less than ) and CA or ca for compressed air (running on compressed air from a tank instead of steam from a boiler).\n\nIn Britain, small diesel and petrol locomotives are usually classified in the same way as steam locomotives, e.g. 0-4-0, 0-6-0, 0-8-0. This may be followed by D for diesel or P for petrol, and another letter describing the transmission: E for electric, H hydraulic, M mechanical. Thus, 0-6-0DE denotes a six-wheel diesel locomotive with electric transmission. Where the axles are coupled by chains or shafts (rather than side rods) or are individually driven, the terms 4w, 6w or 8w are generally used. Thus, 4wPE indicates a four-wheel petrol locomotive with electric transmission. For large diesel locomotives the UIC classification is used.\n\nThe main limitation of Whyte Notation is that it does not cover non-standard types such as Shay locomotives, which use geared trucks rather than driving wheels. The most commonly used system in Europe outside the United Kingdom is UIC classification, based on German practice, which can define the exact layout of a locomotive.\n\nIn American (and to a lesser extent British) practice, most wheel arrangements in common use were given names, sometimes from the name of the first such locomotive built. For example, the 2-2-0 type arrangement is named \"Planet\", after the 1830 locomotive on which it was first used. (This naming convention is similar to the naming of warship classes.)\n\nThe most common wheel arrangements are listed below. In the diagrams, the front of the locomotive is to the left.\n\n"}
