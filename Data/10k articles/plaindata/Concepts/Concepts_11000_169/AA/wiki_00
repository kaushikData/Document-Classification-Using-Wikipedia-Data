{"id": "32617243", "url": "https://en.wikipedia.org/wiki?curid=32617243", "title": "20-sim", "text": "20-sim\n\n20-sim is commercial modeling and simulation program for multidomain dynamic systems, which is developed by Controllab. With 20-sim models can be entered as equations, block diagrams, bond graphs and physical components. 20-sim is widely used for modeling complex multi-domain systems and the development of control systems.\n\n20-sim supports four methods for modeling dynamic systems: iconic diagrams, block diagrams, bond graphs and equations and allows all of these methods to be used in one model. The package has advanced support for bond graph modeling, making it well known in bond graph communities. \n\nFor modeling physical systems the package provides libraries for electrical systems, mechanical systems, hydraulics systems and thermal systems. For block diagrams, libraries comparable to those of Simulink, are provided. A feature of the software is the option to create models with differential equations and package them as block diagram elements or physical components.\n\n20-sim models can be simulated using state of the art numerical integration methods. After checking and processing, models are directly converted into machine code, resulting in high speed simulations. Unlike Simulink, simulation results are shown in 20-sim in a separate window called the Simulator. The simulator is versatile: plots can be displayed horizontally and vertically as time and frequency based plots and 3D animations.\n\n20-sim is self containing, i.e. no additional software is required and all toolboxes are included. Toolboxes are available for model building, time domain analysis, frequency domain analysis and controller design. To enable scripting it is necessary to install either Matlab, GNU Octave, or Python. The last is included as an optional feature in the 20-sim installer.\n\nBecause of its extended support of bond graph modeling 20-sim is highly rated in the bond graph community. According to Borutzky only \n\"20-sim, MS1 and Symbols can be categorized as a fully integrated (multi-formalism) modeling and simulation environments especially supporting bond graphs\".\n\nRoddeck compares several modeling and simulation tools like Simulink, Labview and 20-sim. Roddeck acknowledges the market leadership of Simulink but states that the advantage of 20-sim is the direct input of bond graphs in 20-sim and the availability of built-in tools for FFT-analysis and 3D mechanical modeling. \nThe book of J. Ledin gives practical guidelines for modeling and simulation of dynamic systems. An entire chapter is spent on simulation tools. According to Ledin, \"20-sim differs from other simulation tools like Simulink and VisSim in that it supports four methods for modeling dynamic systems: iconic diagrams, block diagrams, bond graphs and equations. This allows for example, the construction of electrical circuit simulations using standard symbols to represent components, such as op-amps and capacitors.\" A weak point, according to Ledin is the missing capability for distributed simulation in 20-sim.\n\n20-sim offers tight integration with 20-sim 4C. Any 20-sim model can be exported as C-code to 20-sim 4C where it can be used for deployment on hardware. Typical use is the development of controllers for embedded software and the creation of \"virtual plants\" for use in hardware-in-the-loop simulators.\n20-sim can be controlled by scripting, allowing task automation and scenario building. Scripting is supported in Matlab or GNU Octave, and in Python (since v4.6).\n\nNext to scripting, 20-sim has a tight connection with Matlab, GNU Octave and Simulink allowing the import and export of data on many levels. The export of 20-sim models as M-files or S-functions is the most prominent example.\n\n20-sim is the redevelopment of the simulation software package TUTSIM, which was developed at the Control Laboratory of the University of Twente. While TUTSIM was sold in the late 70's, research into modeling and simulation continued at the laboratory. A new program was developed as part of the Ph.D. project of Jan Broenink. The program was equipped with a graphical user interface and allowed the creation of models by bond graphs. The prototype name for this modeling and simulation package was CAMAS. While CAMAS was all built around bond graphs, a new prototype package called MAX was developed to investigated object oriented modeling techniques and modeling by iconic diagrams. After extensive testing, in August 1995 version 1.0 of the software was commercially released under the trade name 20-sim (Twente Sim). The trade name refers to the origin (University of Twente) of the package and the region (Twente) where it was made. The company Controllab Products was established to further develop and distribute the package.\n\n\n\n"}
{"id": "884040", "url": "https://en.wikipedia.org/wiki?curid=884040", "title": "Bivector", "text": "Bivector\n\nIn mathematics, a bivector or 2-vector is a quantity in exterior algebra or geometric algebra that extends the idea of scalars and vectors. If a scalar is considered an order zero quantity, and a vector is an order one quantity, then a bivector can be thought of as being of order two. Bivectors have applications in many areas of mathematics and physics. They are related to complex numbers in two dimensions and to both pseudovectors and quaternions in three dimensions. They can be used to generate rotations in any number of dimensions, and are a useful tool for classifying such rotations. They also are used in physics, tying together a number of otherwise unrelated quantities.\n\nBivectors are generated by the exterior product on vectors: given two vectors a and b, their exterior product is a bivector, as is the sum of any bivectors. Not all bivectors can be generated as a single exterior product. More precisely, a bivector that can be expressed as an exterior product is called \"simple\"; in up to three dimensions all bivectors are simple, but in higher dimensions this is not the case. The exterior product of two vectors is anticommutative and alternating, so is the negation of the bivector , producing the opposite orientation, and is the zero bivector.\n\nGeometrically, a simple bivector can be interpreted as an oriented plane segment, much as vectors can be thought of as directed line segments. The bivector has a \"magnitude\" equal to the area of the parallelogram with edges a and b, has the \"attitude\" of the plane spanned by a and b, and has \"orientation\" being the sense of the rotation that would align a with b. \nIn layman terms, any surface is the same bivector, if it has the same area, same orientation, and is parallel to the same plane (see figure).\n\nThe bivector was first defined in 1844 by German mathematician Hermann Grassmann in exterior algebra as the result of the exterior product of two vectors. Just the previous year, in Ireland, William Rowan Hamilton had discovered quaternions. It was not until English mathematician William Kingdon Clifford in 1888 added the geometric product to Grassmann's algebra, incorporating the ideas of both Hamilton and Grassmann, and founded Clifford algebra, that the bivector as it is known today was fully understood.\n\nAround this time Josiah Willard Gibbs and Oliver Heaviside developed vector calculus, which included separate cross product and dot products that were derived from quaternion multiplication. The success of vector calculus, and of the book \"Vector Analysis\" by Gibbs and Wilson, had the effect that the insights of Hamilton and Clifford were overlooked for a long time, since much of 20th century mathematics and physics was formulated in vector terms. Gibbs used vectors to fill the role of bivectors in three dimensions, and used \"bivector\" to describe an unrelated quantity, a use that has sometimes been copied.\nToday the bivector is largely studied as a topic in geometric algebra, a Clifford algebra over real or complex vector spaces with a nondegenerate quadratic form. Its resurgence was led by David Hestenes who, along with others, applied geometric algebra to a range of new applications in physics.\n\nFor this article the bivector will be considered only in real geometric algebras. This in practice is not much of a restriction, as all useful applications are drawn from such algebras. Also unless otherwise stated, all examples have a Euclidean metric and so a positive-definite quadratic form.\n\nThe bivector arises from the definition of the geometric product over a vector space. For vectors a, b and c, the geometric product on vectors is defined as follows:\n\n\nFrom associativity , a scalar times b. When b is not parallel to and hence not a scalar multiple of a, ab cannot be a scalar. But\n\nis a sum of scalars and so a scalar. From the law of cosines on the triangle formed by the vectors its value is |a||b|cos\"θ\", where \"θ\" is the angle between the vectors. It is therefore identical to the interior product between two vectors, and is written the same way,\n\nIt is symmetric, scalar valued, and can be used to determine the angle between two vectors: in particular if a and b are orthogonal the product is zero.\n\nJust as the interior product can be formulated as the symmetric part of the geometric product another quantity, the exterior product can be formulated as its antisymmetric part:\n\nIt is antisymmetric in a and b\n\nand by addition:\n\nThat is, the geometric product is the sum of the symmetric interior product and antisymmetric exterior product.\n\nTo examine the nature of , consider the formula\n\nwhich using the Pythagorean trigonometric identity gives the value of \n\nWith a negative square it cannot be a scalar or vector quantity, so it is a new sort of object, a bivector. It has magnitude , where \"θ\" is the angle between the vectors, and so is zero for parallel vectors.\n\nTo distinguish them from vectors, bivectors are written here with bold capitals, for example:\n\nalthough other conventions are used, in particular as vectors and bivectors are both elements of the geometric algebra.\n\nThe algebra generated by the geometric product is the geometric algebra over the vector space. For a Euclidean vector space it is written formula_13 or \"C\"ℓ(ℝ), where \"n\" is the dimension of the vector space ℝ. \"C\"ℓ is both a vector space and an algebra, generated by all the products between vectors in ℝ, so it contains all vectors and bivectors. More precisely as a vector space it contains the vectors and bivectors as linear subspaces, though not subalgebras (since the geometric product of two vectors is not generally another vector). The space of all bivectors is written Λℝ.\n\nThe subalgebra generated by the bivectors is the \"even subalgebra\" of the geometric algebra, written . This algebra results from considering all products of scalars and bivectors generated by the geometric product. It has dimension , and contains Λℝ as a linear subspace with dimension (a triangular number). In two and three dimensions the even subalgebra contains only scalars and bivectors, and each is of particular interest. In two dimensions the even subalgebra is isomorphic to the complex numbers, ℂ, while in three it is isomorphic to the quaternions, ℍ. More generally the even subalgebra can be used to generate rotations in any dimension, and can be generated by bivectors in the algebra.\n\nAs noted in the previous section the magnitude of a simple bivector, that is one that is the exterior product of two vectors a and b, is |a||b|sin \"θ\", where \"θ\" is the angle between the vectors. It is written |B|, where B is the bivector.\n\nFor general bivectors the magnitude can be calculated by taking the norm of the bivector considered as a vector in the space Λℝ. If the magnitude is zero then all the bivector's components are zero, and the bivector is the zero bivector which as an element of the geometric algebra equals the scalar zero.\n\nA unit bivector is one with unit magnitude. It can be derived from any non-zero bivector by dividing the bivector by its magnitude, that is\n\nOf particular interest are the unit bivectors formed from the products of the standard basis. If e and e are distinct basis vectors then the product is a bivector. As the vectors are orthogonal this is just ee, written e, with unit magnitude as the vectors are unit vectors. The set of all such bivectors form a basis for Λℝ. For instance in four dimensions the basis for Λℝ is (ee, ee, ee, ee, ee, ee) or (e, e, e, e, e, e).\n\nThe exterior product of two vectors is a bivector, but not all bivectors are exterior products of two vectors. For example, in four dimensions the bivector\n\ncannot be written as the exterior product of two vectors. A bivector that can be written as the exterior product of two vectors is simple. In two and three dimensions all bivectors are simple, but not in four or more dimensions; in four dimensions every bivector is the sum of at most two exterior products. A bivector has a real square if and only if it is simple, and only simple bivectors can be represented geometrically by an oriented plane area.\n\nThe geometric product of two bivectors, A and B, is\n\nThe quantity is the scalar valued interior product, while is the grade 4 exterior product that arises in four or more dimensions. The quantity is the bivector valued commutator product, given by\n\nThe space of bivectors Λℝ are a Lie algebra over ℝ, with the commutator product as the Lie bracket. The full geometric product of bivectors generates the even subalgebra.\n\nOf particular interest is the product of a bivector with itself. As the commutator product is antisymmetric the product simplifies to\n\nIf the bivector is \"simple\" the last term is zero and the product is the scalar valued , which can be used as a check for simplicity. In particular the exterior product of bivectors only exists in four or more dimensions, so all bivectors in two and three dimensions are simple.\n\nWhen working with coordinates in geometric algebra it is usual to write the basis vectors as (e, e, ...), a convention that will be used here.\n\nA vector in real two-dimensional space ℝ can be written , where \"a\" and \"a\" are real numbers, e and e are orthonormal basis vectors. The geometric product of two such vectors is\n\nThis can be split into the symmetric, scalar valued, interior product and an antisymmetric, bivector valued exterior product:\n\nAll bivectors in two dimensions are of this form, that is multiples of the bivector ee, written e to emphasise it is a bivector rather than a vector. The magnitude of e is 1, with\n\nso it is called the unit bivector. The term unit bivector can be used in other dimensions but it is only uniquely defined (up to a sign) in two dimensions and all bivectors are multiples of e. As the highest grade element of the algebra e is also the pseudoscalar which is given the symbol \"i\".\n\nWith the properties of negative square and unit magnitude, the unit bivector can be identified with the imaginary unit from complex numbers. The bivectors and scalars together form the even subalgebra of the geometric algebra, which is isomorphic to the complex numbers ℂ. The even subalgebra has basis (1, e), the whole algebra has basis (1, e, e, e).\n\nThe complex numbers are usually identified with the coordinate axes and two-dimensional vectors, which would mean associating them with the vector elements of the geometric algebra. There is no contradiction in this, as to get from a general vector to a complex number an axis needs to be identified as the real axis, e say. This multiplies by all vectors to generate the elements of even subalgebra.\n\nAll the properties of complex numbers can be derived from bivectors, but two are of particular interest. First as with complex numbers products of bivectors and so the even subalgebra are commutative. This is only true in two dimensions, so properties of the bivector in two dimensions that depend on commutativity do not usually generalise to higher dimensions.\n\nSecond a general bivector can be written\n\nwhere \"θ\" is a real number. Putting this into the Taylor series for the exponential map and using the property e = −1 results in a bivector version of Euler's formula,\n\nwhich when multiplied by any vector rotates it through an angle \"θ\" about the origin:\n\nThe product of a vector with a bivector in two dimensions is anticommutative, so the following products all generate the same rotation\n\nOf these the last product is the one that generalises into higher dimensions. The quantity needed is called a rotor and is given the symbol \"R\", so in two dimensions a rotor that rotates through angle \"θ\" can be written\n\nand the rotation it generates is\n\nIn three dimensions the geometric product of two vectors is\n\nThis can be split into the symmetric, scalar valued, interior product and the antisymmetric, bivector valued, exterior product:\n\nIn three dimensions all bivectors are simple and so the result of an exterior product. The unit bivectors e, e and e form a basis for the space of bivectors Λℝ, which is itself a three-dimensional linear space. So if a general bivector is:\n\nthey can be added like vectors\n\nwhile when multiplied they produce the following\n\nwhich can be split into symmetric scalar and antisymmetric bivector parts as follows\n\nThe exterior product of two bivectors in three dimensions is zero.\n\nA bivector B can be written as the product of its magnitude and a unit bivector, so writing \"β\" for |B| and using the Taylor series for the exponential map it can be shown that\n\nThis is another version of Euler's formula, but with a general bivector in three dimensions. Unlike in two dimensions bivectors are not commutative so properties that depend on commutativity do not apply in three dimensions. For example, in general in three (or more) dimensions.\nThe full geometric algebra in three dimensions, \"C\"ℓ(ℝ), has basis (1, e, e, e, e, e, e, e). The element e is a trivector and the pseudoscalar for the geometry. Bivectors in three dimensions are sometimes identified with pseudovectors to which they are related, as discussed below.\n\nBivectors are not closed under the geometric product, but the even subalgebra is. In three dimensions it consists of all scalar and bivector elements of the geometric algebra, so a general element can be written for example \"a\" + A, where \"a\" is the scalar part and A is the bivector part. It is written and has basis (1, e, e, e). The product of two general elements of the even subalgebra is\n\nThe even subalgebra, that is the algebra consisting of scalars and bivectors, is isomorphic to the quaternions, ℍ. This can be seen by comparing the basis to the quaternion basis, or from the above product which is identical to the quaternion product, except for a change of sign which relates to the negative products in the bivector interior product . Other quaternion properties can be similarly related to or derived from geometric algebra.\n\nThis suggests that the usual split of a quaternion into scalar and vector parts would be better represented as a split into scalar and bivector parts; if this is done the quaternion product is merely the geometric product. It also relates quaternions in three dimensions to complex numbers in two, as each is isomorphic to the even subalgebra for the dimension, a relationship that generalises to higher dimensions.\n\nThe rotation vector, from the axis-angle representation of rotations, is a compact way of representing rotations in three dimensions. In its most compact form, it consists of a vector, the product of a unit vector \"ω\" that is the axis of rotation with the (signed) angle of rotation \"θ\", so that the magnitude of the overall rotation vector \"θω\" equals the (unsigned) rotation angle.\n\nThe quaternion associated with the rotation is\n\nIn geometric algebra the rotation is represented by a bivector. This can be seen in its relation to quaternions. Let \"Ω\" be a unit bivector in the plane of rotation, and let \"θ\" be the angle of rotation. Then the rotation bivector is \"Ωθ\". The quaternion closely corresponds to the exponential of half of the bivector \"Ωθ\". That is, the components of the quaternion correspond to the scalar and bivector parts of the following expression:\n\nformula_37\n\nThe exponential can be defined in terms of its power series, and easily evaluated using the fact that \"Ω\" squared is -1.\n\nSo rotations can be represented by bivectors. Just as quaternions are elements of the geometric algebra, they are related by the exponential map in that algebra.\n\nThe bivector Ω\"θ\" generates a rotation through the exponential map. The even elements generated rotate a general vector in three dimensions in the same way as quaternions:\n\nAs to two dimensions the quantity \"e\" is called a rotor and written \"R\". The quantity \"e\" is then \"R\", and they generate rotations as follows\n\nThis is identical to two dimensions, except here rotors are four-dimensional objects isomorphic to the quaternions. This can be generalised to all dimensions, with rotors, elements of the even subalgebra with unit magnitude, being generated by the exponential map from bivectors. They form a double cover over the rotation group, so the rotors \"R\" and −\"R\" represent the same rotation.\n\nBivectors are isomorphic to skew-symmetric matrices; the general bivector maps to the matrix\n\nThis multiplied by vectors on both sides gives the same vector as the product of a vector and bivector minus the outerproduct; an example is the angular velocity tensor.\n\nSkew symmetric matrices generate orthogonal matrices with determinant 1 through the exponential map. In particular the exponent of a bivector associated with a rotation is a rotation matrix, that is the rotation matrix \"M\" given by the above skew-symmetric matrix is\n\nThe rotation described by \"M\" is the same as that described by the rotor \"R\" given by\n\nand the matrix \"M\" can be also calculated directly from rotor \"R\":\n\nBivectors are related to the eigenvalues of a rotation matrix. Given a rotation matrix \"M\" the eigenvalues can calculated by solving the characteristic equation for that matrix . By the fundamental theorem of algebra this has three roots, but only one real root as there is only one eigenvector, the axis of rotation. The other roots must be a complex conjugate pair. They have unit magnitude so purely imaginary logarithms, equal to the magnitude of the bivector associated with the rotation, which is also the angle of rotation. The eigenvectors associated with the complex eigenvalues are in the plane of the bivector, so the exterior product of two non-parallel eigenvectors result in the bivector, or at least a multiple of it.\n\nThe rotation vector is an example of an axial vector. Axial vectors, or pseudovectors, are vectors with the special feature that their coordinates undergo a sign change relative to the usual vectors (also called \"polar vectors\") under inversion through the origin, reflection in a plane, or other orientation-reversing linear transformation. Examples include quantities like torque, angular momentum and vector magnetic fields. Quantities that would use axial vectors in vector algebra are properly represented by bivectors in geometric algebra. More precisely, if an underlying orientation is chosen, the axial vectors are naturally identified with the usual vectors; the Hodge dual then gives the isomorphism between axial vectors and bivectors, so each axial vector is associated with a bivector and vice versa; that is\n\nwhere ∗ indicates the Hodge dual. Note that if the underlying orientation is reversed by inversion through the origin, both the identification of the axial vectors with the usual vectors and the Hodge dual change sign, but the bivectors don't budge. Alternately, using the unit pseudoscalar in \"C\"ℓ(ℝ), \"i\" = eee gives\n\nThis is easier to use as the product is just the geometric product. But it is antisymmetric because (as in two dimensions) the unit pseudoscalar \"i\" squares to −1, so a negative is needed in one of the products.\n\nThis relationship extends to operations like the vector valued cross product and bivector valued exterior product, as when written as determinants they are calculated in the same way:\n\nso are related by the Hodge dual:\n\nBivectors have a number of advantages over axial vectors. They better disambiguate axial and polar vectors, that is the quantities represented by them, so it is clearer which operations are allowed and what their results are. For example, the inner product of a polar vector and an axial vector resulting from the cross product in the triple product should result in a pseudoscalar, a result which is more obvious if the calculation is framed as the exterior product of a vector and bivector. They generalises to other dimensions; in particular bivectors can be used to describe quantities like torque and angular momentum in two as well as three dimensions. Also, they closely match geometric intuition in a number of ways, as seen in the next section.\n\nAs suggested by their name and that of the algebra, one of the attractions of bivectors is that they have a natural geometric interpretation. This can be described in any dimension but is best done in three where parallels can be drawn with more familiar objects, before being applied to higher dimensions. In two dimensions the geometric interpretation is trivial, as the space is two-dimensional so has only one plane, and all bivectors are associated with it differing only by a scale factor.\n\nAll bivectors can be interpreted as planes, or more precisely as directed plane segments. In three dimensions there are three properties of a bivector that can be interpreted geometrically:\n\nIn three dimensions all bivectors can be generated by the exterior product of two vectors. If the bivector then the magnitude of B is\n\nwhere \"θ\" is the angle between the vectors. This is the area of the parallelogram with edges a and b, as shown in the diagram. One interpretation is that the area is swept out by b as it moves along a. The exterior product is antisymmetric, so reversing the order of a and b to make a move along b results in a bivector with the opposite direction that is the negative of the first. The plane of bivector contains both a and b so they are both parallel to the plane.\n\nBivectors and axial vectors are related by Hodge dual. In a real vector space the Hodge dual relates a subspace to its orthogonal complement, so if a bivector is represented by a plane then the axial vector associated with it is simply the plane's surface normal. The plane has two normals, one on each side, giving the two possible orientations for the plane and bivector.\n\nThis relates the cross product to the exterior product. It can also be used to represent physical quantities, like torque and angular momentum. In vector algebra they are usually represented by vectors, perpendicular to the plane of the force, linear momentum or displacement that they are calculated from. But if a bivector is used instead the plane is the plane of the bivector, so is a more natural way to represent the quantities and the way they act. It also unlike the vector representation generalises into other dimensions.\n\nThe product of two bivectors has a geometric interpretation. For non-zero bivectors A and B the product can be split into symmetric and antisymmetric parts as follows:\n\nLike vectors these have magnitudes and , where \"θ\" is the angle between the planes. In three dimensions it is the same as the angle between the normal vectors dual to the planes, and it generalises to some extent in higher dimensions.\n\nBivectors can be added together as areas. Given two non-zero bivectors B and C in three dimensions it is always possible to find a vector that is contained in both, a say, so the bivectors can be written as exterior products involving a:\n\nThis can be interpreted geometrically as seen in the diagram: the two areas sum to give a third, with the three areas forming faces of a prism with a, b, c and as edges. This corresponds to the two ways of calculating the area using the distributivity of the exterior product:\n\nThis only works in three dimensions as it is the only dimension where a vector parallel to both bivectors must exist. In higher dimensions bivectors generally are not associated with a single plane, or if they are (simple bivectors) two bivectors may have no vector in common, and so sum to a non-simple bivector.\n\nIn four dimensions the basis elements for the space Λℝ of bivectors are (e, e, e, e, e, e), so a general bivector is of the form\n\nIn four dimensions the Hodge dual of a bivector is a bivector, and the space Λℝ is dual to itself. Normal vectors are not unique, instead every plane is orthogonal to all the vectors in its Hodge dual space. This can be used to partition the bivectors into two 'halves', in the following way. We have three pairs of orthogonal bivectors: (e, e), (e, e) and (e, e). There are four distinct ways of picking one bivector from each of the first two pairs, and once these first two are picked their sum yields the third bivector from the other pair. For example, (e, e, e) and (e, e, e).\n\nIn four dimensions bivectors are generated by the exterior product of vectors in ℝ, but with one important difference from ℝ and ℝ. In four dimensions not all bivectors are simple. There are bivectors such as that cannot be generated by the exterior product of two vectors. This also means they do not have a real, that is scalar, square. In this case\n\nThe element e is the pseudoscalar in \"C\"ℓ, distinct from the scalar, so the square is non-scalar.\n\nAll bivectors in four dimensions can be generated using at most two exterior products and four vectors. The above bivector can be written as\n\nSimilarly, every bivector can be written as the sum of two simple bivectors. It is useful to choose two orthogonal bivectors for this, and this is always possible to do. Moreover, for a generic bivector the choice of simple bivectors is unique, that is, there is only one way to decompose into orthogonal bivectors; the only exception is when the two orthogonal bivectors have equal magnitudes (as in the above example): in this case the decomposition is not unique. The decomposition is always unique in the case of simple bivectors, with the added bonus that one of the orthogonal parts is zero.\n\nAs in three dimensions bivectors in four dimension generate rotations through the exponential map, and all rotations can be generated this way. As in three dimensions if B is a bivector then the rotor \"R\" is \"e\" and rotations are generated in the same way:\n\nThe rotations generated are more complex though. They can be categorised as follows:\n\nThese are generated by bivectors in a straightforward way. Simple rotations are generated by simple bivectors, with the fixed plane the dual or orthogonal to the plane of the bivector. The rotation can be said to take place about that plane, in the plane of the bivector. All other bivectors generate double rotations, with the two angles of the rotation equalling the magnitudes of the two simple bivectors the non-simple bivector is composed of. Isoclinic rotations arise when these magnitudes are equal, in which case the decomposition into two simple bivectors is not unique.\n\nBivectors in general do not commute, but one exception is orthogonal bivectors and exponents of them. So if the bivector , where B and B are orthogonal simple bivectors, is used to generate a rotation it decomposes into two simple rotations that commute as follows:\n\nIt is always possible to do this as all bivectors can be expressed as sums of orthogonal bivectors.\n\nSpacetime is a mathematical model for our universe used in special relativity. It consists of three space dimensions and one time dimension combined into a single four-dimensional space. It is naturally described using geometric algebra and bivectors, with the Euclidean metric replaced by a Minkowski metric. That algebra is identical to that of Euclidean space, except the signature is changed, so\n\n(Note the order and indices above are not universal – here e is the time-like dimension). The geometric algebra is \"C\"ℓ(ℝ), and the subspace of bivectors is Λℝ.\n\nThe simple bivectors are of two types. The simple bivectors e, e and e have negative squares and span the bivectors of the three-dimensional subspace corresponding to Euclidean space, ℝ. These bivectors generate ordinary rotations in ℝ.\n\nThe simple bivectors e, e and e have positive squares and as planes span a space dimension and the time dimension. These also generate rotations through the exponential map, but instead of trigonometric functions, hyperbolic functions are needed, which generates a rotor as follows:\n\nwhere \"Ω\" is the bivector (e, etc.), identified via the metric with an antisymmetric linear transformation of ℝ. These are Lorentz boosts, expressed in a particularly compact way, using the same kind of algebra as in ℝ and ℝ.\n\nIn general all spacetime rotations are generated from bivectors through the exponential map, that is, a general rotor generated by bivector A is of the form\n\nThe set of all rotations in spacetime form the Lorentz group, and from them most of the consequences of special relativity can be deduced. More generally this show how transformations in Euclidean space and spacetime can all be described using the same kind of algebra.\n\nMaxwell's equations are used in physics to describe the relationship between electric and magnetic fields. Normally given as four differential equations they have a particularly compact form when the fields are expressed as a spacetime bivector from Λℝ. If the electric and magnetic fields in ℝ are and then the \"electromagnetic bivector\" is\n\nwhere e is again the basis vector for the time-like dimension and \"c\" is the speed of light. The product e yields the bivector that is Hodge dual to in three dimensions, as discussed above, while e as a product of orthogonal vectors is also bivector valued. As a whole it is the electromagnetic tensor expressed more compactly as a bivector, and is used as follows. First it is related to the 4-current J, a vector quantity given by\n\nwhere is current density and \"ρ\" is charge density. They are related by a differential operator ∂, which is\n\nThe operator ∇ is a differential operator in geometric algebra, acting on the space dimensions and given by . When applied to vectors ∇·M is the divergence and ∇∧M is the curl but with a bivector rather than vector result, that is dual in three dimensions to the curl. For general quantity M they act as grade lowering and raising differential operators. In particular if M is a scalar then this operator is just the gradient, and it can be thought of as a geometric algebraic del operator.\n\nTogether these can be used to give a particularly compact form for Maxwell's equations in a vacuum:\n\nThis when decomposed according to geometric algebra, using geometric products which have both grade raising and grade lowering effects, is equivalent to Maxwell's four equations. This is the form in a vacuum, but the general form is only a little more complex. It is also related to the electromagnetic four-potential, a vector A given by\n\nwhere is the vector magnetic potential and \"V\" is the electric potential. It is related to the electromagnetic bivector as follows\n\nusing the same differential operator ∂.\n\nAs has been suggested in earlier sections much of geometric algebra generalises well into higher dimensions. The geometric algebra for the real space ℝ is \"C\"ℓ(ℝ), and the subspace of bivectors is Λℝ.\n\nThe number of simple bivectors needed to form a general bivector rises with the dimension, so for \"n\" odd it is , for \"n\" even it is . So for four and five dimensions only two simple bivectors are needed but three are required for six and seven dimensions. For example, in six dimensions with standard basis (e, e, e, e, e, e) the bivector\n\nis the sum of three simple bivectors but no less. As in four dimensions it is always possible to find orthogonal simple bivectors for this sum.\n\nAs in three and four dimensions rotors are generated by the exponential map, so\n\nis the rotor generated by bivector B. Simple rotations, that take place in a plane of rotation around a fixed blade of dimension are generated by simple bivectors, while other bivectors generate more complex rotations which can be described in terms of the simple bivectors they are sums of, each related to a plane of rotation. All bivectors can be expressed as the sum of orthogonal and commutative simple bivectors, so rotations can always be decomposed into a set of commutative rotations about the planes associated with these bivectors. The group of the rotors in \"n\" dimensions is the spin group, Spin(\"n\").\n\nOne notable feature, related to the number of simple bivectors and so rotation planes, is that in odd dimensions every rotation has a fixed axis – it is misleading to call it an axis of rotation as in higher dimensions rotations are taking place in multiple planes orthogonal to it. This is related to bivectors, as bivectors in odd dimensions decompose into the same number of bivectors as the even dimension below, so have the same number of planes, but one extra dimension. As each plane generates rotations in two dimensions in odd dimensions there must be one dimension, that is an axis, that is not being rotated.\n\nBivectors are also related to the rotation matrix in \"n\" dimensions. As in three dimensions the characteristic equation of the matrix can be solved to find the eigenvalues. In odd dimensions this has one real root, with eigenvector the fixed axis, and in even dimensions it has no real roots, so either all or all but one of the roots are complex conjugate pairs. Each pair is associated with a simple component of the bivector associated with the rotation. In particular the log of each pair is ± the magnitude, while eigenvectors generated from the roots are parallel to and so can be used to generate the bivector. In general the eigenvalues and bivectors are unique, and the set of eigenvalues gives the full decomposition into simple bivectors; if roots are repeated then the decomposition of the bivector into simple bivectors is not unique.\n\nGeometric algebra can be applied to projective geometry in a straightforward way. The geometric algebra used is , the algebra of the real vector space ℝ. This is used to describe objects in the real projective space ℝℙ. The non-zero vectors in \"C\"ℓ(ℝ) or ℝ are associated with points in the projective space so vectors that differ only by a scale factor, so their exterior product is zero, map to the same point. Non-zero simple bivectors in Λℝ represent lines in ℝℙ, with bivectors differing only by a (positive or negative) scale factor representing the same line.\n\nA description of the projective geometry can be constructed in the geometric algebra using basic operations. For example, given two distinct points in ℝℙ represented by vectors a and b the line between them is given by (or ). Two lines intersect in a point if for their bivectors A and B. This point is given by the vector\n\nThe operation \"⋁\" is the meet, which can be defined as above in terms of the join, for non-zero . Using these operations projective geometry can be formulated in terms of geometric algebra. For example, given a third (non-zero) bivector C the point p lies on the line given by C if and only if\n\nSo the condition for the lines given by A, B and C to be collinear is\n\nwhich in \"C\"ℓ(ℝ) and ℝℙ simplifies to\n\nwhere the angle brackets denote the scalar part of the geometric product. In the same way all projective space operations can be written in terms of geometric algebra, with bivectors representing general lines in projective space, so the whole geometry can be developed using geometric algebra.\n\nAs noted above a bivector can be written as a skew-symmetric matrix, which through the exponential map generates a rotation matrix that describes the same rotation as the rotor, also generated by the exponential map but applied to the vector. But it is also used with other bivectors such as the angular velocity tensor and the electromagnetic tensor, respectively a 3×3 and 4×4 skew-symmetric matrix or tensor.\n\nReal bivectors in Λℝ are isomorphic to \"n\"×\"n\" skew-symmetric matrices, or alternately to antisymmetric tensors of order 2 on ℝ. While bivectors are isomorphic to vectors (via the dual) in three dimensions they can be represented by skew-symmetric matrices in any dimension. This is useful for relating bivectors to problems described by matrices, so they can be re-cast in terms of bivectors, given a geometric interpretation, then often solved more easily or related geometrically to other bivector problems.\n\nMore generally every real geometric algebra is isomorphic to a matrix algebra. These contain bivectors as a subspace, though often in a way which is not especially useful. These matrices are mainly of interest as a way of classifying Clifford algebras.\n\n\n"}
{"id": "4473270", "url": "https://en.wikipedia.org/wiki?curid=4473270", "title": "Cai Yuan and Jian Jun Xi", "text": "Cai Yuan and Jian Jun Xi\n\nCai Yuan and Jian Jun Xi are two Chinese-born artists, based in Britain, who work together under the name Mad For Real. They have enacted (unofficial) events at the Venice Biennale and the Turner Prize, where, in 1999, they jumped onto Tracey Emin's \"My Bed\" installation. Originally finding fame as performance artists specialising in art intervention, they have since diversified, engaging in numerous works in both Asia and Europe.\n\nBorn in China, Cai Yuan (1956) and JJ Xi (1962), have been based in the UK since the 1980s. Cai Yuan trained in oil painting at Nanjing College of Art, and gained a BA from Chelsea College of Art and Design in 1989, and his MA from the Royal College of Art, London in 1991. JJ Xi trained at the Central Academy of Applied Arts in Beijing and later at Goldsmiths College.\n\nThey started working as a performance duo in 1999 with \"Two Artists Jump on Tracey Emin’s Bed\" (1999).\n\nTheir best-known performance occurred at 12.58 p.m. on 25 October 1999, when they jumped on Tracey Emin's installation \"My Bed\", a work incorporating memorabilia on and around an unmade bed, in the Turner Prize at Tate Britain. They called their performance \"Two Naked Men Jump into Tracey's Bed\" (although in fact they kept their trousers on). They had in mind including some \"critical sex\" as they considered \"a sexual act was necessary to fully respond to Tracey's piece\", although this part of their intention was not fulfilled. A visitor reported, \"Everyone at the exhibition started clapping as they thought it was part of the show. At first, the security people didn't know what to do.\" It was not clear to some whether the action was part of Emin's display or even a protest against the current visit of Chinese President Jiang Zemin. \n\nAnother visitor commented, \"After a few minutes of hopping about and shouting I think they ran out of things to do. If they had tried to wreck it, or stolen the vodka or her knickers, I might have felt differently. It made my weekend.\" The men only had time to start a pillow fight and attempt a swig from one of the empty vodka bottles next to the bed, before they were apprehended. The police and security guards were booed when they took the pair away. Cai and Xi were arrested for their action, but no charges were pressed, since \"neither the gallery nor the artist had any desire to bring the matter further\". \n\nCai considered that, although Emin's work was strong, it was nevertheless institutionalised and said, \"We want to push the idea further. Our action will make the public think about what is good art or bad art. We didn't have time to do a proper performance. I thought I should touch the bed and smell the bed.\" He had various words written in Chinese and English on his body, such as \"Internationalism\", \"Freedom\" and \"Idealism\". Xi said that the work was not interesting enough and also that he wanted to push it further, increasing its significance and sensationalism. Words written on his body included \"Anarchism\", \"Idealism\" and \"Optimism\". \n\nOne of the words prominent along the length of Cai's torso was \"Anti-Stuckism\". This was surprising as the Stuckists had themselves been critical of Emin's art. However, Cai and Xi's explanation is that they were not anti Emin's type of work (which they merely wanted to \"improve\"—\"We are simply trying to react to the work and the self-promotion implicit in it\"), but were opposed to the Stuckists, who are anti-performance art. According to Fiachra Gibbons of \"The Guardian\", the event \"will go down in art history as the defining moment of the new and previously unheard of Anti-Stuckist Movement.\"\n\nThe Tate's official pronouncement was \"The work has now been restored and the exhibition will open to the public as usual at 10 a.m.\", but they would not be drawn on the nature of the restoration.\n\nIn 1997, they erected fake street signs in an attempt to mislead high-profile visitors to the Venice Biennale. \n\nAt Goldsmiths College in London they scattered £1,200 around a room to point to the commercialism and greed of the art market. The audience scrambled on the floor to pick up the money.\n\nIn spring 2000, the artists returned to the Tatespecifically, to the Tate Modernin an attempt to urinate into a 1964 artist-authorized replica of Marcel Duchamp's \"Fountain\", a urinal laid on its back and signed \"R. Mutt\". The Tate denied that the attempt succeeded. The sculpture is now enclosed in a transparent box.\n\n\n"}
{"id": "72356", "url": "https://en.wikipedia.org/wiki?curid=72356", "title": "Cathedral floorplan", "text": "Cathedral floorplan\n\nIn Western ecclesiastical architecture, a cathedral diagram is a floor plan showing the sections of walls and piers, giving an idea of the profiles of their columns and ribbing. Light double lines in perimeter walls indicate glazed windows. Dashed lines show the ribs of the vaulting overhead. By convention, ecclesiastical floorplans are shown map-fashion, with north to the top and the liturgical east end to the right.\n\nMany abbey churches have floorplans that are comparable to cathedrals, though sometimes with more emphasis on the sanctuary and choir spaces that are reserved for the religious community. Smaller churches are similarly planned, with simplifications.\n\nCathedral floorplans are designed to provide for the liturgical rites of the church. Before the legalization of Christianity by Emperor Constantine, Christians worshiped in private homes or in secretive locations. Once legally able to publicly worship, the local churches adapted the available Roman designs to their needs. Unlike the Roman and Greek religions, where priests performed rituals without public participation, Christian worship involved the believers. Thus, the limited spaces used in pagan temples were not suitable to Christian worship.\nRoman civic buildings were designed for the participation of the citizens of the city, and thus the Roman Basilica was adopted for Christian purposes. This included an entry on one end of a long narrow, covered space with a raised dais at the other end. Upon the dais, public officials would hear legal cases, or expound on some matter of public interest. Christians adopted the long hall of the basilica for the public liturgy of the Mass.\n\n\n\n"}
{"id": "9765502", "url": "https://en.wikipedia.org/wiki?curid=9765502", "title": "Cloth modeling", "text": "Cloth modeling\n\nCloth modeling is the term used for simulating cloth within a computer program; usually in the context of 3D computer graphics. The main approaches used for this may be classified into three basic types: geometric, physical, and particle/energy.\n\nMost models of cloth are based on \"particles\" of mass connected in some manner of mesh. Newtonian Physics is used to model each particle through the use of a \"black box\" called a physics engine. This involves using the basic law of motion (Newton's Second Law):\n\nIn all of these models, the goal is to find the position and shape of a piece of fabric using this basic equation and several other methods.\n\nWeil pioneered the first of these, the geometric technique, in 1986. His work was focused on approximating the look of cloth by treating cloth like a collection of cables and using Hyperbolic cosine (catenary) curves. Because of this, it is not suitable for dynamic models but works very well for stationary or single-frame renders. This technique creates an underlying shape out of single points; then, it parses through each set of three of these points and maps a catenary curve to the set. It then takes the lowest out of each overlapping set and uses it for the render.\n\nThe second technique treats cloth like a grid work of particles connected to each other by springs. Whereas the geometric approach accounted for none of the inherent stretch of a woven material, this physical model accounts for stretch (tension), stiffness, and weight:\n\n\nNow we apply the basic principle of mechanical equilibrium in which all bodies seek lowest energy by differentiating this equation to find the minimum energy.\n\nThe last method is more complex than the first two. The particle technique takes the physical methods a step further and supposes that we have a network of particles interacting directly. Rather than springs, the energy interactions of the particles are used to determine the cloth’s shape. An energy equation that adds onto the following is used:\n\n\nTerms for energy added by any source can be added to this equation, then derive and find minima, which generalizes our model. This allows for modelling cloth behavior under any circumstance, and since the cloth is treated as a collection of particles its behavior can be described with the dynamics provided in our physics engine.\n\n\n"}
{"id": "6978", "url": "https://en.wikipedia.org/wiki?curid=6978", "title": "Concept", "text": "Concept\n\nConcepts are mental representations, abstract objects or abilities that make up the fundamental building blocks of thoughts and beliefs. They play an important role in all aspects of cognition.\n\nIn contemporary philosophy, there are at least three prevailing ways to understand what a concept is:\n\n\nConcepts can be organized into a hierarchy, higher levels of which are termed \"superordinate\" and lower levels termed \"subordinate\". Additionally, there is the \"basic\" or \"middle\" level at which people will most readily categorize a concept. For example, a basic-level concept would be \"chair\", with its superordinate, \"furniture\", and its subordinate, \"easy chair\".\n\nA concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.\n\nConcepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word \"concept\" often just means any idea.\n\nWithin the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called \"mental representations\" (colloquially understood as \"ideas in the mind\"). Mental representations, in turn, are the building blocks of what are called \"propositional attitudes\" (colloquially understood as the stances or perspectives we take towards ideas, be it \"believing\", \"doubting\", \"wondering\", \"accepting\", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.\n\nA central question in the study of concepts is the question of what concepts \"are\". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.\n\nPlatonist views of the mind construe concepts as abstract objects,\n\nThere is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept \"dog\" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called \"lexical concepts\".\n\nStudy of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.\n\nIn the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word \"moon\" (a concept) is not the large, bright, shape-changing object up in the sky, but only \"represents\" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.\n\nKant declared that human minds possess pure or \"a priori\" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things \"in general\", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an \"a priori\" concept can relate to individual phenomena, in a manner analogous to an \"a posteriori\" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction \"a posteriori concepts\" (meaning concepts that arise out of experience). An empirical or an \"a posteriori\" concept is a general representation (\"Vorstellung\") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)\n\nA concept is a common feature or characteristic. Kant investigated the way that empirical \"a posteriori\" concepts are created.\nIn cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or \"recollections\", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.\n\nPlato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.\n\nGottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).\n\nAccording to Carl Benjamin Boyer, in the introduction to his \"The History of the Calculus and its Conceptual Development\", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.\n\nIn a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.\n\nConcepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. (\"Sort\" is itself another word for concept, and \"sorting\" thus means to organise into concepts.)\n\nThe classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both \"necessary\" and \"sufficient\" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example \"bachelor\" is said to be defined by \"unmarried\" and \"man\". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the \"law of the excluded middle\", which means that there are no partial members of a class, you are either in or out.\n\nThe classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic \"Time Without Change\" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.\n\nGiven that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:\n\nPrototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as \"family resemblances\". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.\n\nTheory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.\n\nAccording to the theory of ideasthesia (or \"sensing concepts\"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.\n\nThere is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.\n\nThe term \"concept\" is traced back to 1554–60 (Latin \"\" – \"something conceived\").\n\n"}
{"id": "13078153", "url": "https://en.wikipedia.org/wiki?curid=13078153", "title": "Conceptual photography", "text": "Conceptual photography\n\nConceptual photography is a type of photography that illustrates an idea. There has been illustrative photographs made since the medium's invention, for example in the earliest staged photographs, such as Hippolyte Bayard's \"Self Portrait as a Drowned Man\" (1840). However, the term Conceptual Photography derives from Conceptual Art a movement of the late 1960s. Today the term is used to describe either a methodology or a genre.\n\nAs a methodology conceptual photography is a type of photography that is staged to represent an idea. The 'concept' is both preconceived and, if successful, understandable in the completed image. It is most often seen in advertising and illustration where the picture may reiterate a headline or catchphrase that accompanies it. Photographic advertising and illustration commonly derive from Stock photography, which is often produced in response to current trends in image usage as determined by the research of picture agencies like Getty Images or Corbis. These photographs are therefore produced to visualize a predetermined concept. The advent of picture editing software like Adobe Photoshop has allowed the greater manipulation of images to seamlessly combine elements that previously it would only have been possible to combine in graphic illustration.\n\nThe term 'conceptual photography' used to describe a genre may refer to the use of photography in Conceptual Art or in contemporary art photography. In either case, the term is not widely used or consistently applied. \n\nConceptual art of the late 1960s and early 1970s often involved photography to document performances, ephemeral sculpture or actions. The artists did not describe themselves as photographers, for example Edward Ruscha said \"Photography's just a playground for me. I'm not a photographer at all.\" These artists are sometimes referred to as conceptual photographers but those who used photography extensively such as John Hilliard and John Baldessari and Pedram Mousavi are more often described as photoconceptualists or \"artists using photography\".\n\nSince the 1970s artists using photography like Cindy Sherman and latterly Thomas Ruff and Thomas Demand have been described as conceptual. Although their work does not generally resemble the lo-fi aesthetic of 1960s conceptual art they may use certain methods in common such as documenting performance (Sherman), typological or serial imagery (Ruff) or the restaging of events (Demand). In fact the indebtedness to these and other approaches from Conceptual Art is so widespread in contemporary Fine-art photography that almost any work might be described as conceptual. The term has perhaps been used most specifically in a negative sense to distinguish some contemporary art photography from documentary photography or Photojournalism. This distinction has been made in the coverage of the Deutsche Börse Photography Prize. Conceptual photography is often used interchangeably with Fine-Art Photography, and there has been some dispute about whether there is a difference between the two. However, the central school of thought is that conceptual photography is a type of fine-art photography. Fine art Photography is inclusive of conceptual photography. While all conceptual photography is fine art, not all fine art is conceptual. \n\n"}
{"id": "867970", "url": "https://en.wikipedia.org/wiki?curid=867970", "title": "Country club", "text": "Country club\n\nA country club is a privately owned club, often with a membership quota and admittance by invitation or sponsorship, that generally offers both a variety of recreational sports and facilities for dining and entertaining. Typical athletic offerings are golf, tennis, and swimming. A country club is most commonly located in city outskirts or suburbs, and is distinguished from an urban athletic club by having substantial grounds for outdoor activities and a major focus on golf.\n\nCountry clubs originated in Scotland and first appeared in the US in the early 1880s. Country clubs had a profound effect on expanding suburbanization and are considered to be the precursor to gated community development.\n\nCountry clubs can be exclusive organizations. In small towns, membership in the country club is often not as exclusive or expensive as in larger cities where there is competition for a limited number of memberships. In addition to the fees, some clubs have additional requirements to join. For example, membership can be limited to those who reside in a particular housing community.\n\nCountry clubs were founded by upper-class elites between 1880 and 1930. By 1907, country clubs were claimed to be “the very essence of American upper-class.” The number of country clubs increased exponentially with industrialization, the rise in incomes, and suburbanization in the 1920s. During the 1920s, country clubs acted as community social centers. When people lost most of their income and net worth during the Great Depression, the number of country clubs decreased drastically for lack of membership funding.\n\nHistorically, many country clubs refused to admit members of minority racial groups as well those of specific faiths, such as Jews and Catholics. In a 1990 landmark ruling at Shoal Creek Golf and Country Club, the PGA refused to hold tournaments at private clubs that practiced racial discrimination. This new regulation led to the admittance of blacks at private clubs. The incident at Shoal Creek is comparable to the 1966 NCAA Basketball Tournament, which led to the end of racial discrimination in college basketball.\n\nBeginning in the 1960s civil rights lawsuits forced clubs to drop exclusionary policies, but \"de facto\" discrimination still occurs in cases until protest or legal remedies are brought to bear.\n\nThe Philadelphia Cricket Club is the oldest country club in the United States devoted to playing games, Country Club Of Salisbury is also one of the oldest country clubs in the U.S.\n\nIn the United Kingdom, most exclusive country clubs are simply golf clubs, and play a smaller role in their communities than American country clubs; gentlemen's clubs in Britain—many of which admit women while remaining socially exclusive—fill many roles of the United States' country clubs.\n\n Country clubs exist in multiple forms, including athletic-based clubs and golf clubs. Examples are the Breakfast Point Country Club and Cumberland Grove Country Club in Sydney, the Castle Hill Country Club, the Gold Coast Polo & Country Club, Elanora Country Club, and the Sanctuary Cove's Country Club.\n\nIn Japan, almost all golf clubs are called \"Country Clubs\" by their owners. See Japan Golf Tour.\n\n"}
{"id": "1425449", "url": "https://en.wikipedia.org/wiki?curid=1425449", "title": "Coupling (computer programming)", "text": "Coupling (computer programming)\n\nIn software engineering, coupling is the degree of interdependence between software modules; a measure of how closely connected two routines or modules are; the strength of the relationships between modules.\n\nCoupling is usually contrasted with cohesion. Low coupling often correlates with high cohesion, and vice versa. Low coupling is often a sign of a well-structured computer system and a good design, and when combined with high cohesion, supports the general goals of high readability and maintainability.\n\nThe software quality metrics of coupling and cohesion were invented by Larry Constantine in the late 1960s as part of a structured design, based on characteristics of “good” programming practices that reduced maintenance and modification costs. Structured design, including cohesion and coupling, were published in the article and the book , and the latter subsequently became standard terms.\n\nCoupling can be \"low\" (also \"loose\" and \"weak\") or \"high\" (also \"tight\" and \"strong\"). Some types of coupling, in order of highest to lowest coupling, are as follows:\n\nA module here refers to a subroutine of any kind, i.e. a set of one or more statements having a name and preferably its own set of variable names.\n\n\n\n\nIn recent work various other coupling concepts have been investigated and used as indicators for different modularization principles used in practice.\n\nTightly coupled systems tend to exhibit the following developmental characteristics, which are often seen as disadvantages:\n\n\nWhether loosely or tightly coupled, a system's performance is often reduced by message and parameter creation, transmission, translation (e.g. marshaling) and message interpretation (which might be a reference to a string, array or data structure), which require less overhead than creating a complicated message such as a SOAP message. Longer messages require more CPU and memory to produce. To optimize runtime performance, message length must be minimized and message meaning must be maximized.\n\n\n\nOne approach to decreasing coupling is functional design, which seeks to limit the responsibilities of modules along functionality. Coupling increases between two classes A and B if:\n\n\nLow coupling refers to a relationship in which one module interacts with another module through a simple and stable interface and does not need to be concerned with the other module's internal implementation (see Information Hiding).\n\nSystems such as CORBA or COM allow objects to communicate with each other without having to know anything about the other object's implementation. Both of these systems even allow for objects to communicate with objects written in other languages.\n\nCoupling and cohesion are terms which occur together very frequently. Coupling refers to the interdependencies between modules, while cohesion describes how related the functions within a single module are. Low cohesion implies that a given module performs tasks which are not very related to each other and hence can create problems as the module becomes large.\n\nCoupling in Software Engineering describes a version of metrics associated with this concept.\n\nFor data and control flow coupling:\n\n\nFor global coupling:\n\n\nFor environmental coupling:\n\n\nformula_1\n\ncodice_1 makes the value larger the more coupled the module is. This number ranges from approximately 0.67 (low coupling) to 1.0 (highly coupled)\n\nFor example, if a module has only a single input and output data parameter\n\nformula_2\n\nIf a module has 5 input and output data parameters, an equal number of control parameters, and accesses 10 items of global data, with a fan-in of 3 and a fan-out of 4,\n\nformula_3\n\n\n"}
{"id": "1830721", "url": "https://en.wikipedia.org/wiki?curid=1830721", "title": "Duty to retreat", "text": "Duty to retreat\n\nIn law, the duty to retreat is a legal requirement in some jurisdictions that a threatened person cannot harm another in self-defense (especially lethal force) when it is possible to instead retreat to a place of safety. This requirement contrasts with right in some other jurisdictions to \"stand one's ground\", meaning being allowed to defend one's self instead of retreating.\n\nIt is a specific component which sometimes appears in the criminal defense of self-defense, and which must be addressed if criminal defendants are to prove that their conduct was justified. \n\nSome U.S. jurisdictions require that a person retreat from an attack, and allow the use of deadly force in self-defense only when retreat is not possible or when retreat poses a danger to the person under attack. The duty to retreat is not universal, however. For example, police officers are not required to retreat when acting in the line of duty. \n\nOther states apply what is known as the castle doctrine, whereby a threatened person need not retreat within his or her own dwelling or place of work. Sometimes this has been the result of court rulings that one need not retreat in a place where one has an especial right to be. In other states, this has been accomplished by statute, such as that suggested by the Model Penal Code.\n\nStill other states have passed stand your ground laws that do not require an individual to retreat and allow one to match force for force, deadly force for deadly force. \n\nMost state legal systems began by importing English Common Law such as Acts of Parliament of 2 Ed. III (Statute of Northampton), and 5 Rich. II of 1381 (Forcible Entry Act 1381)—which imposed criminal sanctions intending to discourage the resort to self-help. This required a threatened party to retreat, whenever property was \"involved\" and resolve the issue by civil means.\n\nToday, the majority of American states have construed their statutes of forcible entry, both penal and civil, in such a manner as to abrogate (i.e. abolish) the common law privilege to use force in the recovery of possession of land.\n\nIn \"Erwin v. State\" (1876), the Supreme Court of Ohio wrote that a \"true man\", one without fault, would not retreat. In \"Runyan v. State\" (1877), the Indiana court rejected a duty to retreat, implying it was un-American, writing of a referring to the distinct American mind, \"the tendency of the American mind seems to be very strongly against\" a duty to retreat. The court went further in saying that no statutory law could require a duty to retreat, because the right to stand one's ground is \"founded on the law of nature; and is not, nor can be, superseded by any law of society.\"\n\nIn English law the focus of the test is whether the defendant is acting reasonably in the particular situation. There is no specific requirement that a person must retreat in anticipation of an attack. Although some withdrawal would be useful evidence to prove that the defendant did not want to fight, not every defendant is able to escape. In \"R v Bird\" the defendant was physically attacked, and reacted instinctively and immediately without having the opportunity to retreat. Had there been a delay in the response, the reaction might have appeared more revenge than self-defense.\n\nAs to carrying weapons in anticipation of an attack, \"Evans v Hughes\" held that for a defendant to justify his possession of a metal bar on a public highway, he had to show that there was an imminent particular threat affecting the particular circumstances in which the weapon was carried. Similarly, in \"Taylor v Mucklow\" a building owner was held to be using an unreasonable degree of force in carrying a loaded airgun against a builder who was demolishing a new extension because his bills were unpaid. More dramatically, in \"AG's Reference (No 2 of 1983)\" Lord Lane held that a defendant who manufactured ten petrol bombs to defend his shop during the Toxteth riots could set up the defense of showing that he possessed an explosive substance \"for a lawful purpose\" if he could establish that he was acting in self-defense to protect himself or his family or property against an imminent and apprehended attack by means which he believed to be no more than reasonably necessary to meet the attack.\n\n\n"}
{"id": "301500", "url": "https://en.wikipedia.org/wiki?curid=301500", "title": "Ecological footprint", "text": "Ecological footprint\n\nThe ecological footprint measures human demand on nature, i.e., the quantity of nature it takes to support people or an economy. It tracks this demand through an ecological accounting system. The accounts contrast the biologically productive area people use for their consumption to the biologically productive area available within a region or the world (biocapacity, the productive area that can regenerate what people demand from nature). In short, it is a measure of human impact on Earth's ecosystem and reveals the dependence of the human economy on natural capital.\n\nThe ecological footprint is defined as the biologically productive area needed to provide for everything people use: fruits and vegetables, fish, wood, fibers, absorption of carbon dioxide from fossil fuel use, and space for buildings and roads.\n\nFootprint and biocapacity can be compared at the individual, regional, national or global scale. Both footprint and biocapacity change every year with number of people, per person consumption, efficiency of production, and productivity of ecosystems. At a global scale, footprint assessments show how big humanity's demand is compared to what planet Earth can renew. Since 2003, Global Footprint Network has calculated the ecological footprint from UN data sources for the world as a whole and for over 200 nations (known as the National Footprint Accounts). Every year the calculations are updated with the newest data. The time series are recalculated with every update since UN statistics also change historical data sets. As shown in Lin et al (2018) the time trends for countries and the world have stayed consistent despite data updates. Also, a recent study by the Swiss Ministry of Environment independently recalculated the Swiss trends and reproduced them within 1-4% for the time period that they studied (1996-2015). Global Footprint Network estimates that, as of 2014, humanity has been using natural capital 1.7 times as fast as Earth can renew it. This means humanity's ecological footprint corresponds to 1.7 planet Earths.\n\nEcological footprint analysis is widely used around the Earth in support of sustainability assessments. It enables people to measure and manage the use of resources throughout the economy and explore the sustainability of individual lifestyles, goods and services, organizations, industry sectors, neighborhoods, cities, regions and nations. Since 2006, a first set of ecological footprint standards exist that detail both communication and calculation procedures. The latest version are the updated standards from 2009.\n\n For 2014, Global Footprint Network estimated humanity's ecological footprint as 1.7 planet Earths. This means that, according to their calculations, humanity's demands were 1.7 times faster than what the planet's ecosystems renewed.\n\nEcological footprints can be calculated at any scale: for an activity, a person, a community, a city, a town, a region, a nation, or humanity as a whole. Cities, due to their population concentration, have large ecological footprints and have become ground zero for footprint reduction.\n\nThe ecological footprint accounting method at the national level is described on the web page of Global Footprint Network or in greater detail in academic papers, including Borucke et al. \n\nThe National Accounts Review Committee has also published a research agenda on how to improve the accounts.\n\nThe first academic publication about ecological footprints was by William Rees in 1992. The ecological footprint concept and calculation method was developed as the PhD dissertation of Mathis Wackernagel, under Rees' supervision at the University of British Columbia in Vancouver, Canada, from 1990–1994. Originally, Wackernagel and Rees called the concept \"appropriated carrying capacity\". To make the idea more accessible, Rees came up with the term \"ecological footprint\", inspired by a computer technician who praised his new computer's \"small footprint on the desk\". In early 1996, Wackernagel and Rees published the book \"Our Ecological Footprint: Reducing Human Impact on the Earth\" with illustrations by Phil Testemale.\n\nFootprint values at the end of a survey are categorized for Carbon, Food, Housing, and Goods and Services as well as the total footprint number of Earths needed to sustain the world's population at that level of consumption. This approach can also be applied to an activity such as the manufacturing of a product or driving of a car. This resource accounting is similar to life-cycle analysis wherein the consumption of energy, biomass (food, fiber), building material, water and other resources are converted into a normalized measure of land area called global hectares (gha).\n\nPer capita ecological footprint (EF), or ecological footprint analysis (EFA), is a means of comparing consumption and lifestyles, and checking this against nature's ability to provide for this consumption. The tool can inform policy by examining to what extent a nation uses more (or less) than is available within its territory, or to what extent the nation's lifestyle would be replicable worldwide. The footprint can also be a useful tool to educate people about carrying capacity and overconsumption, with the aim of altering personal behavior. Ecological footprints may be used to argue that many current lifestyles are not sustainable. Such a global comparison also clearly shows the inequalities of resource use on this planet at the beginning of the twenty-first century.\n\nIn 2007, the average biologically productive area per person worldwide was approximately 1.8 global hectares (gha) per capita. The U.S. footprint per capita was 9.0 gha, and that of Switzerland was 5.6 gha, while China's was 1.8 gha. The WWF claims that the human footprint has exceeded the biocapacity (the available supply of natural resources) of the planet by 20%. Wackernagel and Rees originally estimated that the available biological capacity for the 6 billion people on Earth at that time was about 1.3 hectares per person, which is smaller than the 1.8 global hectares published for 2006, because the initial studies neither used global hectares nor included bioproductive marine areas.\n\nA number of NGOs offer ecological footprint calculators (\"see\" Footprint Calculator, below).\n\nAccording to the 2018 edition of the National Footprint Accounts, humanity’s total ecological footprint has exhibited an increasing trend since 1961, growing an average of 2.1% per year (SD= 1.9). Humanity’s ecological footprint was 7.0 billion gha in 1961 and increased to 20.6 billion gha in 2014. The world-average ecological footprint in 2014 was 2.8 global hectares per person. The carbon footprint is the fastest growing part of the ecological footprint and accounts currently for about 60% of humanity’s total ecological footprint.\n\nThe Earth’s biocapacity has not increased at the same rate as the ecological footprint. The increase of biocapacity averaged at only 0.5% per year (SD = 0.7). Because of agricultural intensification, biocapacity was at 9.6 billion gha in 1961 and grew to 12.2 billion gha in 2014. \n\nTherefore, the Earth has been in ecological overshoot (where humanity is using more resources and generating waste at a pace that the ecosystem can’t renew) since the 1970’s. In 2018, Earth Overshoot Day, the date where humanity has used more from nature then the planet can renew in the entire year, was estimated to be August 1. Now more than 85% of humanity live in countries that run an ecological deficit. This means their ecological footprint for consumption exceeds the biocapacity of that country. \n\nThe UK's average ecological footprint is 5.45 global hectares per capita (gha) with variations between regions ranging from 4.80 gha (Wales) to 5.56 gha (East England).\n\nTwo recent studies have examined relatively low-impact small communities. BedZED, a 96-home mixed-income housing development in South London, was designed by Bill Dunster Architects and sustainability consultants BioRegional for the Peabody Trust. Despite being populated by relatively \"mainstream\" home-buyers, BedZED was found to have a footprint of 3.20 gha due to on-site renewable energy production, energy-efficient architecture, and an extensive green lifestyles program that included on-site London's first carsharing club. The report did not measure the added footprint of the 15,000 visitors who have toured BedZED since its completion in 2002. Findhorn Ecovillage, a rural intentional community in Moray, Scotland, had a total footprint of 2.56 gha, including both the many guests and visitors who travel to the community to undertake residential courses there and the nearby campus of Cluny Hill College. However, the residents alone have a footprint of 2.71 gha, a little over half the UK national average and one of the lowest ecological footprints of any community measured so far in the industrialized world. Keveral Farm, an organic farming community in Cornwall, was found to have a footprint of 2.4 gha, though with substantial differences in footprints among community members.\n\nIn a 2012 study of consumers acting \"green\" vs. \"brown\" (where green people are «expected to have significantly lower ecological impact than “brown” consumers»), the conclusion was \"the research found no significant difference between the carbon footprints of green and brown consumers\". A 2013 study concluded the same.\n\nA 2017 study published in \"Environmental Research Letters\" posited that the most significant way individuals could reduce their own carbon footprint is to have fewer children, followed by living without a vehicle, forgoing air travel and adopting a plant-based diet.\n\nEarly criticism was published by van den Bergh and Verbruggen in 1999, which was updated in 2014. Another criticism was published in 2008. A more complete review commissioned by the Directorate-General for the Environment (European Commission) was published in June 2008. The review found Ecological Footprint \"a useful indicator for assessing progress on the EU’s Resource Strategy\" the authors noted that Ecological Footprint analysis was unique \"in its ability to relate resource use to the concept of carrying capacity.\" The review noted that further improvements in data quality, methodologies and assumptions were needed.\n\nA recent critique of the concept is due to Blomqvist et al., 2013a, with a reply from Rees and Wackernagel, 2013, and a rejoinder by Blomqvist et al., 2013b.\n\nAn additional strand of critique is due to Giampietro and Saltelli (2014a), with a reply from Goldfinger et al., 2014, a rejoinder by Giampietro and Saltelli (2014a), and additional comments from van den Bergh and Grazi (2015).\n\nA number of countries have engaged in research collaborations to test the validity of the method. This includes Switzerland, Germany, United Arab Emirates, and Belgium.\n\nGrazi et al. (2007) have performed a systematic comparison of the ecological footprint method with spatial welfare analysis that includes environmental externalities, agglomeration effects and trade advantages. They find that the two methods can lead to very distinct, and even opposite, rankings of different spatial patterns of economic activity. However this should not be surprising, since the two methods address different research questions.\n\nNewman (2006) has argued that the ecological footprint concept may have an anti-urban bias, as it does not consider the opportunities created by urban growth. Calculating the ecological footprint for densely populated areas, such as a city or small country with a comparatively large population — e.g. New York and Singapore respectively — may lead to the perception of these populations as \"parasitic\". This is because these communities have little intrinsic biocapacity, and instead must rely upon large \"hinterlands\". Critics argue that this is a dubious characterization since mechanized rural farmers in developed nations may easily consume more resources than urban inhabitants, due to transportation requirements and the unavailability of economies of scale. Furthermore, such moral conclusions seem to be an argument for autarky. Some even take this train of thought a step further, claiming that the Footprint denies the benefits of trade. Therefore, the critics argue that the Footprint can only be applied globally.\nThe method seems to reward the replacement of original ecosystems with high-productivity agricultural monocultures by assigning a higher biocapacity to such regions. For example, replacing ancient woodlands or tropical forests with monoculture forests or plantations may improve the ecological footprint. Similarly, if organic farming yields were lower than those of conventional methods, this could result in the former being \"penalized\" with a larger ecological footprint. Of course, this insight, while valid, stems from the idea of using the footprint as one's only metric. If the use of ecological footprints are complemented with other indicators, such as one for biodiversity, the problem might be solved. Indeed, WWF's Living Planet Report complements the biennial Footprint calculations with the Living Planet Index of biodiversity. Manfred Lenzen and Shauna Murray have created a modified Ecological Footprint that takes biodiversity into account for use in Australia.\n\nAlthough the ecological footprint model prior to 2008 treated nuclear power in the same manner as coal power, the actual real world effects of the two are radically different. A life cycle analysis centered on the Swedish Forsmark Nuclear Power Plant estimated carbon dioxide emissions at 3.10 g/kW⋅h and 5.05 g/kW⋅h in 2002 for the Torness Nuclear Power Station. This compares to 11 g/kW⋅h for hydroelectric power, 950 g/kW⋅h for installed coal, 900 g/kW⋅h for oil and 600 g/kW⋅h for natural gas generation in the United States in 1999. Figures released by Mark Hertsgaard, however, show that because of the delays in building nuclear plants and the costs involved, investments in energy efficiency and renewable energies have seven times the return on investment of investments in nuclear energy.\n\nThe Swedish utility Vattenfall did a study of full life-cycle greenhouse-gas emissions of energy sources the utility uses to produce electricity, namely: Nuclear, Hydro, Coal, Gas, Solar Cell, Peat and Wind. The net result of the study was that nuclear power produced 3.3 grams of carbon dioxide per kW⋅h of produced power. This compares to 400 for natural gas and 700 for coal (according to this study). The study also concluded that nuclear power produced the smallest amount of CO of any of their electricity sources.\n\nClaims exist that the problems of nuclear waste do not come anywhere close to approaching the problems of fossil fuel waste. A 2004 article from the BBC states: \"The World Health Organization (WHO) says 3 million people are killed worldwide by outdoor air pollution annually from vehicles and industrial emissions, and 1.6 million indoors through using solid fuel.\" In the U.S. alone, fossil fuel waste kills 20,000 people each year. A coal power plant releases 100 times as much radiation as a nuclear power plant of the same wattage. It is estimated that during 1982, US coal burning released 155 times as much radioactivity into the atmosphere as the Three Mile Island incident. In addition, fossil fuel waste causes global warming, which leads to increased deaths from hurricanes, flooding, and other weather events. The World Nuclear Association provides a comparison of deaths due to accidents among different forms of energy production. In their comparison, deaths per TW-yr of electricity produced (in UK and USA) from 1970 to 1992 are quoted as 885 for hydropower, 342 for coal, 85 for natural gas, and 8 for nuclear.\n\nThe Western Australian government State of the Environment Report included an Ecological Footprint measure for the average Western Australian seven times the average footprint per person on the planet in 2007, a total of about 15 hectares.\n\nThe world-average ecological footprint in 2013 was 2.8 global hectares per person. The average per country ranges from over 10 to under 1 global hectares per person. There is also a high variation within countries, based on individual lifestyle and economic possibilities.\n\nThe GHG footprint or the more narrow carbon footprint are a component of the ecological footprint. Often, when only the carbon footprint is reported, it is expressed in weight of CO2 (or CO2e representing GHG warming potential (GGWP)), but it can also be expressed in land areas like ecological footprints. Both can be applied to products, people or whole societies.\n\nSince the 1950s, a new geological epoch called the Anthropocene has been proposed to distinguish the period of major human impact.\n\n\n"}
{"id": "4402239", "url": "https://en.wikipedia.org/wiki?curid=4402239", "title": "Emptiness", "text": "Emptiness\n\nEmptiness as a human condition is a sense of generalized boredom, social alienation and apathy. Feelings of emptiness often accompany dysthymia, depression, loneliness, anhedonia,\ndespair, or other mental/emotional disorders, including schizoid personality disorder, post trauma, attention deficit hyperactivity disorder, schizotypal personality disorder and borderline personality disorder. A sense of emptiness is also part of a natural process of grief, as resulting of separation, death of a loved one, or other significant changes. However, the particular meanings of “emptiness” vary with the particular context and the religious or cultural tradition in which it is used.\n\nWhile Christianity and Western sociologists and psychologists view a state of emptiness as a negative, unwanted condition, in some Eastern philosophies such as Buddhist philosophy and Taoism, emptiness (Śūnyatā) represents seeing through the illusion of independent self-nature.\n\nIn the West, feeling \"empty\" is often viewed as a negative condition. Psychologist Clive Hazell, for example, attributes feelings of emptiness to problematic family backgrounds with abusive relationships and mistreatment. He claims that some people who are facing a sense of emptiness try to resolve their painful feelings by becoming addicted to a drug or obsessive activity (be it compulsive sex, gambling or work) or engaging in \"frenzied action\" or violence. In sociology, a sense of emptiness is associated with social alienation of the individual. This sense of alienation may be suppressed while working, due to the routine nature of work tasks, but during leisure hours or during the weekend, people may feel a sense of \"existential vacuum\" and emptiness.\n\nIn political philosophy, emptiness is associated with nihilism. Literary critic Georg Lukács (born in 1885) argued against the \"spiritual emptiness and moral inadequacy of capitalism\", and argued in favour of communism as an \"entirely new type of civilization, one that promised a fresh start and an opportunity to lead a meaningful and purposeful life.\"\n\nThe concept of \"emptiness\" was important to a \"certain type of existentialist philosophy and some forms of the Death of God movement\". Existentialism, the \"philosophic movement that gives voice to the sense of alienation and despair\", comes from \"man’s recognition of his fundamental aloneness in an indifferent universe\". People whose response to the sense of emptiness and aloneness is to give excuses live in bad faith; \"people who face the emptiness and accept responsibility aim to live 'authentic' lives\". Existentialists argue that \"man lives in alienation from God, from nature, from other men, from his own true self.\" Crowded into cities, working in mindless jobs, and entertained by light mass media, we \"live on the surface of life\", so that even \"people who seemingly have 'everything' feel empty, uneasy, discontented.\"\n\nIn cultures where a sense of emptiness is seen as a negative psychological condition, it is often associated with depression. As such, many of the same treatments are proposed: psychotherapy, group therapy, or other types of counselling. As well, people who feel empty may be advised to keep busy and maintain a regular schedule of work and social activities. Other solutions which have been proposed to reduce a sense of emptiness are getting a pet or trying Animal-Assisted Therapy; getting involved in spirituality such as meditation or religious rituals and service; volunteering to fill time and bring social contact; doing social interactions, such as community activities, clubs, or outings; or finding a hobby or recreational activity to regain their interest in life.\n\nIn Austrian philosopher/educator Rudolf Steiner's (1861–1925) thinking, spiritual emptiness was a major problem in the educated European middle class. In his 1919 lectures he argued that European culture became \"empty of spirit\" and \"ignorant of the needs, the conditions, that are essential for the life of the spirit\". People experienced a \"spiritual emptiness\" and their thinking became marked by a \"lazy passivity\" due to the \"absence of will from the life of thought\". In modern Europe, Steiner claimed that people would \"allow their thoughts to take possession of them\", and these thoughts were increasingly filled with abstraction and \"pure, natural scientific thinking\". The educated middle classes began to think in a way that was \"devoid of spirit\", with their minds becoming \"dimmer and darker\", and increasing empty of spirit.\n\nLouis Dupré, a Professor of Philosophy at Yale University, argues that the \"spiritual emptiness of our time is a symptom of its religious poverty\". He claims that \"many people never experience any emptiness: they are too busy to feel much absence of any kind\"; they only realize their spiritual emptiness if \"painful personal experiences -- the death of a loved one, the collapse of a marriage, the alienation of a child, the failure of a business\" shock them into reassessing their sense of meaning.\n\nSpiritual emptiness has been associated with juvenile violence. In John C. Thomas' 1999 book \"How Juvenile Violence Begins: Spiritual Emptiness\", he argues that youth in impoverished indigenous communities who feel empty may turn to fighting and aggressive crime to fill their sense of meaninglessness. In Cornell University professor James Garbarino's 1999 book \"Lost Boys: Why Our Sons Turn Violent and How We Can Save Them\", he argues that \"neglect, shame, spiritual emptiness, alienation, anger and access to guns are a few of the elements common to violent boys\". A professor of human development, Garbarino claims that violent boys have an \"alienation from positive role models\" and \"a spiritual emptiness that spawns despair\". These youth are seduced by the violent fantasy of the US gun culture, which provides negative role models of tough, aggressive men who use power to get what they want. He claims that boys can be helped by giving them \"a sense of purpose\" and \"spiritual anchors\" that can \"anchor boys in empathy and socially engaged moral thinking\".\n\nSpiritual emptiness is often connected with addiction, especially by Christian-influenced addiction organizations and counsellors. Bill Wilson, the founder of Alcoholics Anonymous, argued that one of the impacts of alcoholism was causing a spiritual emptiness in heavy drinkers. In Abraham J. Twerski's 1997 book \"Addictive Thinking: Understanding Self-Deception\", he argues that when people feel spiritually empty, they often turn to addictive behaviors to fill the inner void. In contrast to having an empty stomach, which is a clear feeling, having spiritual emptiness is hard to identify, so it fills humans with a \"vague unrest\". While people may try to resolve this emptiness by obsessively having sex, overeating, or taking drugs or alcohol, these addictions only give temporary satisfaction. When a person facing a crisis due to feeling spiritually empty is able to stop one addiction, such as compulsive sex, they often just trade it in for another addictive behaviour, such as gambling or overeating.\n\nA number of novelists and filmmakers have depicted emptiness. The concept of \"emptiness\" was important to a \"good deal of 19th–20th century Western imaginative literature\". Novelist Franz Kafka depicted a meaningless bizarre world in \"The Trial\" and the existentialist French authors sketched a world cut off from purpose or reason in Jean-Paul Sartre's \"La Nausée\" and Albert Camus' \"L'étranger\". Existentialism influenced 20th century poet T.S. Eliot, whose poem “The Love Song of J. Alfred Prufrock” describes an \"anti-hero or alienated soul, running away from or confronting the emptiness of his or her existence\". Professor Gordon Bigelow argues that the existentialist theme of \"spiritual barrenness is commonplace in literature of the 20th century\", which in addition to Eliot includes Ernest Hemingway, Faulkner, Steinbeck and Anderson.\n\nFilm adaptations of a number of existentialist novels capture the bleak sense of emptiness espoused by Sartre and Camus. This theme of emptiness has also been used in modern screenplays. Mark Romanek's 1985 film \"Static\" tells the surreal story of a struggling inventor and crucifix factory worker named Ernie who feels spiritually empty because he is saddened by his parents' death in an accident. Screenwriter Michael Tolkin's 1994 film \"The New Age\" examines \"cultural hipness and spiritual emptiness\", creating a \"dark, ambitious, unsettling\" film that depicts a fashionable LA couple who \"are miserable in the midst of their sterile plenty\", and whose souls are stunted by their lives of empty sex, consumption, and distractions. The 1999 film \"American Beauty\" examines the spiritual emptiness of life in the US suburbs. In Wes Anderson's 2007 film \"The Darjeeling Limited\", three brothers who \"... suffer from spiritual emptiness\" and then \"self-medicate themselves through sex, social withdrawal, and drugs.\"The 2008 film \"The Informers\" is a Hollywood drama film written by Bret Easton Ellis and Nicholas Jarecki and directed by Gregor Jordan. The film is based on Ellis' 1994 collection of short stories of the same name. The film, which is set amidst the decadence of the early 1980s, depicts an assortment of socially alienated, mainly well-off characters who numb their sense of emptiness with casual sex, alcohol, and drugs. \n\nContemporary architecture critic Herbert Muschamp argues that \"horror vacui\" (which is Latin for \"fear of emptiness\") is a key principle of design. He claims that it has become an obsessive quality that is the \"driving force in contemporary American taste\". Muschamp states that \"along with the commercial interests that exploit this interest, it is the major factor now shaping attitudes toward public spaces, urban spaces, and even suburban sprawl.\"\n\nFilms that depict nothingness, shadows and vagueness, either in a visual sense or a moral sense are appreciated in genres such as film noir. As well, travellers and artists are often intrigued by and attracted to vast empty spaces, such as open deserts, barren wastelands or salt flats, and the open sea.\n\nIn visual arts emptiness and absence were recognized as phenomena that characterize not only particular works of art (e.g. Yves Klein) but also as a more general tendency within the history of modern art and aesthetics. Following Davor Džalto's argument on the modern concept of art, the gradual elimination of particular elements that traditionally characterized visual arts, which results in emptiness, is the most important phenomenon within the history and theory of art over the past two hundred years.\n\nThe Buddhist term \"emptiness\" (Skt. \"śūnyatā\") refers specifically to the idea that everything is dependently originated, including the causes and conditions themselves, and even the principle of causality itself. It is not nihilism, nor is it meditating on nothingness. Instead, it refers to the absence (emptiness) of inherent existence. Buddhapalita says:\nIn an interview, the Dalai Lama stated that tantric meditation can be used for \"heightening your own realization of emptiness or mind of enlightenment\". In Buddhist philosophy, attaining a realization of emptiness of inherent existence is key to the permanent cessation of suffering, i.e. liberation.\n\nThe Dalai Lama argues that tantric yoga trainees need to realize the emptiness of inherent existence before they can go on to the \"highest yoga tantra initiation\"; realizing the emptiness of inherent existence of the mind is the \"fundamental innate mind of clear light, which is the subtlest level of the mind\", where all \"energy and mental processes are withdrawn or dissolved\", so that all that appears to the mind is \"pure emptiness\". As well, emptiness is \"linked to the creative Void, meaning that it is a state of complete receptivity and perfect enlightenment\", the merging of the \"ego with its own essence\", which Buddhists call the \"clear light\".\n\nIn Ven. Thubten Chodron's 2005 interview with Lama Zopa Rinpoche, the lama noted that we \"...ordinary beings who haven't realized emptiness don't see things as similar to illusions\", and we do not \"realize that things are merely labeled by mind and exist by mere name\". He argues that \"when we meditate on emptiness, we drop an atom bomb on this [sense of a] truly existent I\" and we realize that \"what appears true... isn't true\". By this, the lama is claiming that what we think is real—our thoughts and feelings about people and things—\"exists by being merely labeled\". He argues that meditators who attain knowledge of a state of emptiness are able to realize that their thoughts are merely illusions from labelling by the mind.\n\nIn Taoism, attaining a state of emptiness is viewed as a state of stillness and placidity which is the \"mirror of the universe\" and the \"pure mind\". The Tao Te Ching claims that emptiness is related to the \"Tao, the Great Principle, the Creator and Sustainer of everything in the universe\". It is argued that it is the \"state of mind of the Taoist disciple who follows the Tao\", who has successfully emptied the mind \"of all wishes and ideas not fitted with the Tao's Movement\". For a person who attains a state of emptiness, the \"still mind of the sage is the mirror of heaven and earth, the glass of all things\", a state of \"vacancy, stillness, placidity, tastelessness, quietude, silence, and non-action\" which is the \"perfection of the Tao and its characteristics, the \"mirror of the universe\" and the \"pure mind\". In this significance, emptiness is the central theme of the artworks by the contemporary Australian artist David Booker.\n\n\n"}
{"id": "43964845", "url": "https://en.wikipedia.org/wiki?curid=43964845", "title": "Ethnic plastic surgery", "text": "Ethnic plastic surgery\n\nEthnic plastic surgery or ethnic modification, is plastic surgery intended to change an individual's appearance to look more or less like a particular race or ethnicity.\n\nRhinoplasty (nose surgery) and blepharoplasty (eyelid surgery) are also popular procedures.\n\nMichael Jackson's plastic surgery has been discussed in the context of ethnic plastic surgery. In her book \"Venus Envy: A History of Cosmetic Surgery\", Elizabeth Haiken devotes a chapter to \"The Michael Jackson Factor\" presenting \"black, Asian and Jewish women who seek WASP noses and Playboy breasts. They are caught in the vexed immigrants' dilemma of struggling not only to keep up with the Joneses but to look like them, too.\"\n\nPlastic surgeons Chuma J. Chike-Obi, M.D., Kofi Boahene, M.D., and Anthony E. Brissett, M.D., F.A.C.S. distinguish between motivations of aesthetics and racial transformation for patients of African descent seeking plastic surgery. In their opinion, \"Patients whose desired surgical outcomes result in racial transformation should be educated about the potential risks of this objective, and these requests should generally be discouraged.\"\n\nFeminist scholars have split views on the subject. Christine Overall, professor of philosophy at Queens University, has written that personal racial transformation, or as she puts it \"transracialism\", belongs to a larger class of personal surgical interventions. This larger class includes transsexual identity change, body art, cosmetic surgery, Munchhausen syndrome and voluntary female genital cutting. Her basic thesis is that the arguments against the ethical nature of racial transformation (e.g. \"it's not possible\", \"betrayal of group identity\", \"reinforces oppression\", etc.) stand or fall with the ethical arguments related to transsexual change. Basically, if it's OK for persons to change their gender, it's OK for persons to change their race. Cressida Heyes, professor of Philosophy of Gender and Sexuality at the University of Alberta, disagrees with Overall's schema. Heyes feels that racial transformation is fundamentally different from gender transformation since race is also determined by ancestry, personal cultural history and societal definitions. Hence ethical considerations of transracial surgery are different from ethical considerations in transsexual surgery.\n\n"}
{"id": "14712915", "url": "https://en.wikipedia.org/wiki?curid=14712915", "title": "Gift wrapping", "text": "Gift wrapping\n\nGift wrapping is the act of enclosing a gift in some sort of material. Wrapping paper is a kind of paper designed for gift wrapping. An alternative to gift wrapping is using a gift box or bag. A wrapped or boxed gift may be held closed with ribbon and topped with a decorative bow (an ornamental knot made of ribbon).\n\nThe use of wrapping paper is first documented in ancient China, where paper was invented in 2nd century BC. In the Southern Song dynasty, monetary gifts were wrapped with paper, forming an envelope known as a \"chih pao\". The wrapped gifts were distributed by the Chinese court to government officials. In the Chinese text \"Thien Kung Khai Wu\", Sung Ying-Hsing states that the coarsest wrapping paper is manufactured with rice straws and bamboo fiber. \n\nAlthough the Hall brothers Rollie and Joyce Hall, founders of Hallmark Cards, did not invent gift wrapping, their innovations led to the development of modern gift wrapping. They helped to popularize the idea of decorative gift wrapping in the 20th century, and according to Joyce Hall, \"the decorative gift-wrapping business was born the day Rollie placed those French envelope linings on top of that showcase.\"\n\nGift wrapping has been shown to positively influence the recipient who are more likely to rate their gifts positively if they had traditional gift wrapping.\n\nIn Western culture, gifts are often wrapped in wrapping paper and accompanied by a gift note which may note the occasion, the receiver's name and the giver's name.\n\nModern patterned wrapping paper was introduced to the American market by the Hall Brothers in 1917. The Kansas City stationery store had run out of traditional white, red, and green monocolor tissue papers, and started selling colorful envelope liners from France. Proving popular, the company promoted the new designs in the subsequent decades, adding ribbons in the 1930s, and Hallmark remains one of the largest American producers of gift wrap.\n\nIn the United States, an additional five million tons of waste are generated over the Christmas gift-giving period; four million tons of this is wrapping paper and shopping bags. Some people attempt to avoid this by unwrapping gifts with care to hopefully allow the paper to be reused, while others use decorated cloth gift sacks that can be easily reused many times; both of these concepts are part of the Green Gifting trend that encourages recycling. Additionally, some people use old newspapers instead of wrapping paper.\n\nIn Chinese culture, red wrapping denotes luck.\n\nIn Japanese culture, wrapping paper and boxes are common. However, the traditional cloth wrapping called \"furoshiki\" is increasing in popularity, particularly as an ecologically friendly alternative to wrapping paper.\n\nIn Korean culture, \"bojagi\" are sometimes used for gift wrapping. A \"yedanbo\" is a ceremonial gift bojagi used to wrap wedding gifts from the bride's family to the members of the groom's.\n"}
{"id": "54657635", "url": "https://en.wikipedia.org/wiki?curid=54657635", "title": "Guess the Correlation", "text": "Guess the Correlation\n\nReleased in 2016, Guess the Correlation is a minimalistic browser-based game with a purpose developed by Omar Wagih at the European Bioinformatics Institute. The game was developed to study human perception in scatter plots. Players are presented with a stream of scatter plots depicting the relationship between two random variables and are asked to guess how positively correlated they are. Guesses closer to the real correlation, are rewarded more points. The game features both single and two-player modes and has a retro 8-bit design and sound effects.\n\nCollected guesses are used to better understand how humans perceive correlations in scatter plots by identifying features within scatter plots, such as outliers, that cause players to over or under estimate the true correlation. \n\nBy 1 February 2016, over 2 million guesses had been collected from 100,000 participants. \n\nIn the single-player mode, players are presented with a stream of scatter plots depicting the relationship between two random variables. The aim is to guess the true Pearson correlation coefficient, where the guess can range from 0 (no correlation) to 1 (perfect positive correlation). Players start out with three lives and no points. Guesses made within 0.05 of the true correlation are awarded a life and five points. Guesses made within 0.10 are awarded one point, and guesses over 0.10 are not awarded any points and a life is deducted. The game ends when the player has run out of lives.\n\nIn the two player mode, opponents challenge each other at guessing the true correlation. Once a session has been initiated between two players, both players are presented with the same scatter plot. The player with the closest guess to true correlation is awarded a point. In the event of a draw, no points are awarded to either player. The first player to reach 10 points is declared the winner.\n\n"}
{"id": "1670504", "url": "https://en.wikipedia.org/wiki?curid=1670504", "title": "Hakham Bashi", "text": "Hakham Bashi\n\nHakham Bashi (, , ) is the Turkish name for the Chief Rabbi of the nation's Jewish community. In the time of the Ottoman Empire it was also used for the chief rabbi of a particular region of the empire, such as Syria or Iraq, though the Hakham Bashi of Constantinople was considered overall head of the Jews of the Empire.\n\"Hakham\" is Hebrew for \"wise man\" (or \"scholar\"), while \"başı\" is Turkish for \"head\".\nThe institution of the \"Hakham Bashi\" was established by the Ottoman Sultan Mehmet II, as part of his policy of governing his exceedingly diverse subjects according to their own laws and authorities wherever possible. Religion was considered as primordial aspect of a communities 'national' identity, so the term Ethnarch has been applied to such religious leaders, especially the (Greek Orthodox) Ecumenical Patriarch of Constantinople (i.e. in the Sultan's imperial capital, renamed Istanbul in 1930 but replaced by Ankara as republican capital in 1923). As Islam was the official religion of both court and state, the Chief Mufti in Istanbul had a much higher status, even of cabinet rank.\n\nBecause of the size and nature of the Ottoman state, containing a far greater part of the diaspora than any other, the position of Hakham Bashi has been compared to that of the Jewish Exilarch.\n\nIn the Ottoman Empire, and as such, the \"Hakham Bashi\" was the closest thing to an overall Exilarchal authority among Jewry everywhere in the Middle East in early modern times. They held broad powers to legislate, judge and enforce the laws among the Jews in the Ottoman Empire and often sat on the Sultan's divan.\n\nThe office also maintained considerable influence outside the Ottoman Empire, especially after the forced migration of numerous Jewish communities and individuals out of Spain (after the fall of Granada in 1492) and Italy.\n\nThe Chief Rabbi of the modern, secular Republic of Turkey is still known as \"Hahambaşı\".\n\nThe term \"Hakham Bashi\" was also used for the official Government-appointed Chief Rabbi of other important cities in the Ottoman Empire, such as Damascus and Baghdad. In particular, the position of Hakham Bashi of Palestine was the precursor of that of Sephardic Chief Rabbi of Israel.\n\n\n"}
{"id": "193965", "url": "https://en.wikipedia.org/wiki?curid=193965", "title": "Hermetica", "text": "Hermetica\n\nThe Hermetica are Egyptian-Greek wisdom texts from the 2nd century AD and later, which are mostly presented as dialogues in which a teacher, generally identified as Hermes Trismegistus (\"thrice-greatest Hermes\"), enlightens a disciple. The texts form the basis of Hermeticism. They discuss the divine, the cosmos, mind, and nature. Some touch upon alchemy, astrology, and related concepts.\n\nThe term particularly applies to the Corpus Hermeticum, Marsilio Ficino's Latin translation in fourteen tracts, of which eight early printed editions appeared before 1500 and a further twenty-two by 1641. This collection, which includes \"Poimandres\" and some addresses of Hermes to disciples Tat, Ammon and Asclepius, was said to have originated in the school of Ammonius Saccas and to have passed through the keeping of Michael Psellus: it is preserved in fourteenth century manuscripts. The last three tracts in modern editions were translated independently from another manuscript by Ficino's contemporary Lodovico Lazzarelli (1447–1500) and first printed in 1507. Extensive quotes of similar material are found in classical authors such as Joannes Stobaeus.\n\nParts of the \"Hermetica\" appeared in the 4th-century Gnostic library found in Nag Hammadi. Other works in Syriac, Arabic, Coptic and other languages may also be termed \"Hermetica\" — another famous tract is the \"Emerald Tablet\", which teaches the doctrine \"as above, so below\".\n\nAll these are themselves remnants of a more extensive literature, part of the syncretic, intellectualized paganism of their era, a cultural movement that also included the Neoplatonic philosophy of the Greco-Roman mysteries and late Orphic and Pythagorean literature and influenced Gnostic forms of the Abrahamic religions. There are significant differences: the Hermetica are little concerned with Greek mythology or the technical minutiae of metaphysical Neoplatonism. The Hermetica are heavily influenced by Judaism and explicitly refer to Genesis 1:28. However most of these schools do agree in attributing the creation of the world to a Demiurge rather than the supreme being and in accepting reincarnation. Although Neoplatonic philosophers, who quote apocryphal works of Orpheus, Zoroaster, Pythagoras and other figures, almost never cite Hermes Trismegistus, the tracts were still popular enough in the 5th century to be argued against by Augustine of Hippo in the \"City of God\".\n\nThe extant Egyptian-Greek texts dwell upon the oneness and goodness of God, urge purification of the soul, and defend pagan religious practices, such as the veneration of images. Their concerns are practical in nature, their end is a spiritual rebirth through the enlightenment of the mind:\nSeeing within myself an immaterial vision that came from the mercy of God, I went out of myself into an immortal body, and now I am not what I was before. I have been born in mind!\n\nWhile they are difficult to date with precision, the texts of the \"Corpus\" were likely redacted between the 1st and 3rd centuries AD. During the Renaissance these texts were believed to be of ancient Egyptian origin and even today some readers believe them to date from Pharaonic Egypt. Since Plato's Timaeus dwelt upon the great antiquity of the Egyptian teachings upon which the philosopher purported to draw, scholars were willing to accept that these texts were the sources of Greek ideas. However the classical scholar Isaac Casaubon (1559–1614) successfully argued that some, mainly those dealing with philosophy, betrayed too recent a vocabulary. Hellenisms in the language itself point to a Greek-era origin. However, flaws in this dating were discerned by the 17th century scholar Ralph Cudworth, who argued that Casaubon's allegation of forgery could only be applied to three of the seventeen treatises contained within the Corpus Hermeticum. Moreover, Cudworth noted Casaubon's failure to acknowledge the codification of these treatises as a late formulation of a pre-existing oral tradition. According to Cudworth, the texts must be viewed as a terminus ad quem and not a quo. Lost Greek texts, and many of the surviving vulgate books, contained discussions of alchemy clothed in philosophical metaphor. And one text, the \"Asclepius\", lost in Greek but partially preserved in Latin, contained a bloody prophecy of the end of Roman rule in Egypt and the resurgence of pagan Egyptian power. Thus, it would be fair to assess the \"Corpus Hermeticum\" as intellectually eclectic.\n\nMore recent research, while affirming the late dating in a period of syncretic cultural ferment in Roman Egypt, suggests more continuity with the culture of Pharaonic Egypt than had previously been believed. There are many parallels with Egyptian prophecies and hymns to the gods but the closest comparisons can be found in Egyptian wisdom literature, which is characteristically couched in words of advice from a \"father\" to a \"son\". Demotic (late Egyptian) papyri contain substantial sections of a dialogue of Hermetic type between Thoth and a disciple. Egyptologist Sir William Flinders Petrie states that some texts in the Hermetic corpus date back to the 6th century BC during the Persian period. Some similarities between the Demotic texts and Platonic philosophy could be the result of Plato and his followers' having drawn on Egyptian sources.\n\nMany hermetic texts were lost to Western culture during the Middle Ages but rediscovered in Byzantine copies and popularized in Italy during the Renaissance. The impetus for this revival came from the Latin translation by Marsilio Ficino, a member of the de' Medici court, who published a collection of thirteen tractates in 1471, as \"De potestate et sapientia Dei\". The \"Hermetica\" provided a seminal impetus in the development of Renaissance thought and culture, having a profound impact on alchemy and modern magic as well as influencing philosophers such as Giordano Bruno and Pico della Mirandola, Ficino's student. This influence continued as late as the 17th century with authors such as Sir Thomas Browne.\n\nAlthough the most famous examples of Hermetic literature were products of Greek-speakers under Roman rule, the genre did not suddenly stop with the fall of the Empire but continued to be produced in Coptic, Syriac, Arabic, Armenian and Byzantine Greek. The most famous example of this later Hermetica is the \"Emerald Tablet\", known from medieval Latin and Arabic manuscripts with a possible Syriac source. Little else of this rich literature is easily accessible to non-specialists. The mostly gnostic Nag Hammadi Library, discovered in 1945, also contained one previously unknown hermetic text called \"The Ogdoad and the Ennead\", a description of a hermetic initiation into gnosis that has led to new perspectives on the nature of Hermetism as a whole, particularly due to the research of Jean-Pierre Mahé.\n\nJohn Everard's historically important 1650 translation into English of the \"Corpus Hermeticum\", entitled \"The Divine Pymander in XVII books\" (London, 1650) was from Ficino's Latin translation; it is no longer considered reliable by scholars. The modern standard editions are the Budé edition by A. D. Nock and A.-J. Festugière (Greek and French, 1946, repr. 1991) and Brian P. Copenhaver (English, 1992).\n\nThe following are the titles given to the eighteen tracts, as translated by G.R.S. Mead:\n\nThe following are the titles given by John Everard:\n\n\n\n"}
{"id": "18672144", "url": "https://en.wikipedia.org/wiki?curid=18672144", "title": "Hollow Earth Expedition", "text": "Hollow Earth Expedition\n\nHollow Earth Expedition is a pulp fiction role-playing game set in the fictitious Hollow Earth (see Hollow Earth theory), published by Exile Game Studio. The game has been nominated for several Origins and ENnie awards since its release in 2006. The main rule book is \"Hollow Earth Expedition\".\n\n\"Hollow Earth Expedition\" uses the Ubiquity rules, which were also created by Exile Game Studio. The main feature of the system is its use of binary dice pool rolls to determine success. Under the default rules, each even number rolled counts as a success, while odd numbers count as nothing. Odd 'nothings' do not cancel out even 'successes', making dice rolling quick and simple. This binary system has also led to the development of special Ubiquity Dice, which are not detailed below.\n\nHollow Earth Expedition is set in the 1930s. On the surface, everything appears as it does in our real world history books, but if you dig a little deeper you encounter secret societies and villainous organizations, many of whom have a vested interest in the Hollow Earth.\n\nTwo organizations are detailed in the main rulebook: the Terra Arcana and the Thule Society. Further organizations are listed in the first expansion, Secrets of the Surface World.\n\nThe Terra Arcanum is an esoteric society dedicated to keeping the Hollow Earth a secret. They have their foundations deep in the mists of time, and only a few of their current members realize their true origins.\n\nThe Thule Society are based on an actual organization associated with the Nazi Party during World War II. In HEX they have a deep interest in getting into the Hollow Earth and the powerful ancient weapons they believe are kept there.\n\nThe Hollow Earth boasts a wide selection of beasts and humans. Dinosaurs and other extinct creatures still live and hunt and are hunted by the various human groups who have become stranded in the Hollow Earth over the centuries. Not a great deal of detail is provided about the Hollow Earth in the main book, allowing Game Masters to customize the terrain to suit their campaign.\n\nMysteries of the Hollow Earth expands a great deal on the denizens of Hollow Earth, including rules for beastmen of various sorts, and provides details such as a map and gazetteer for those GMs who want a more fleshed-out world, rather than the more nebulous \"land of mysteries\" from the core book.\n\nRevelations of Mars expands the setting to include the dying and dangerous Red Planet.\n\nExplore a desolate world filled with strange aliens, bizarre creatures, and ancient artifacts buried beneath its shifting sands. Discover xenophobic nomads roaming the wastelands, sky pirates prowling the air in their great flying vessels, and power-hungry warlords fighting over dwindling resources. Take shelter within one of the great walled city-states, and rub shoulders with haughty nobles, devout priests, and greedy merchants who plot and scheme therein.\n\n\nHollow Earth Expedition\n\nENnies:\n\nOrigins:\n\nHollow Earth Expedition GM Screen\n\nENnies:\n\nOrigins:\n\nSecrets of the Surface World\n\nENnies:\n\nMysteries of The Hollow Earth\n\nENnies:\n\nPerils of the Surface World\n\nENnies:\n\n"}
{"id": "57651406", "url": "https://en.wikipedia.org/wiki?curid=57651406", "title": "Hygroreception", "text": "Hygroreception\n\nHygroreception is the ability to detect changes in the moisture and humidity content of an environment. It is a sense that is not present in humans. Some insects have this sense. The structure responsible for this sense is a hygroreceptor.\n"}
{"id": "4995922", "url": "https://en.wikipedia.org/wiki?curid=4995922", "title": "Ideal point", "text": "Ideal point\n\nIn hyperbolic geometry, an ideal point, omega point or point at infinity is a well defined point outside the hyperbolic plane or space.\nGiven a line \"l\" and a point \"P\" not on \"l\", right- and left-limiting parallels to \"l\" through \"P\" converge to \"l\" at \"ideal points\".\n\nUnlike the projective case, ideal points form a boundary, not a submanifold. So, these lines do not \"intersect\" at an ideal point and such points, although well defined, do not belong to the hyperbolic space itself.\n\nThe ideal points together form the Cayley absolute or boundary of a hyperbolic geometry. \nFor instance, the unit circle forms the Cayley absolute of the Poincaré disk model and the Klein disk model.\nWhile the real line forms the Cayley absolute of the Poincaré half-plane model .\n\nPasch's axiom and the exterior angle theorem still hold for an omega triangle, defined by two points in hyperbolic space and an omega point.\n\n\nif all vertices of a triangle are ideal points the triangle is an ideal triangle.\n\nIdeal triangles have a number of interesting properties:\n\n\nif all vertices of a quadrilateral are ideal points the quadrilateral is an ideal quadrilateral.\n\nWhile all ideal triangles are congruent, not all quadrilaterals are, the diagonals can make different angles with each other resulting in noncongruent quadrilaterals\nhaving said this:\n\n\nThe ideal quadrilateral where the two diagonals are perpendicular to each other form an ideal square.\n\nIt was use by Ferdinand Karl Schweikart in his memorandum on what he called \"astral geometry\", one of the first publications acknowledging the possibility of hyperbolic geometry.\n\nAn ideal \"n\"-gon can be subdivided into ideal triangles, with area times the area of an ideal triangle.\n\nIn the Klein disk model and the Poincaré disk model of the hyperbolic plane. In both disk models the ideal points are on the unit circle (hyperbolic plane) or unit sphere (higher dimensions) which is the unreachable boundary of the hyperbolic plane. \nWhen projecting the same hyperbolic line to the Klein disk model and the Poincaré disk model both lines go through the same two ideal points.(the ideal points in both models are on the same spot).\n\nGiven two distinct points \"p\" and \"q\" in the open unit disk the unique straight line connecting them intersects the unit circle in two ideal points, \"a\" and \"b\", labeled so that the points are, in order, \"a\", \"p\", \"q\", \"b\" so that |aq| > |ap| and |pb| > |qb|. Then the hyperbolic distance between \"p\" and \"q\" is expressed as\n\nGiven two distinct points \"p\" and \"q\" in the open unit disk then the unique circle arc orthogonal to the boundary connecting them intersects the unit circle in two ideal points, \"a\" and \"b\", labeled so that the points are, in order, \"a\", \"p\", \"q\", \"b\" so that |aq| > |ap| and |pb| > |qb|. Then the hyperbolic distance between \"p\" and \"q\" is expressed as\n\nWhere the distances are measured along the (straight line) segments aq, ap, pb and qb.\n\nIn the Poincaré half-plane model the ideal points are the points on the boundary axis. There is also another ideal point that is not represented in the half-plane model (but rays parallel to the positive y-axis approach it).\n\nIn the hyperboloid model there are no ideal points.\n\n"}
{"id": "52409583", "url": "https://en.wikipedia.org/wiki?curid=52409583", "title": "Jaina seven-valued logic", "text": "Jaina seven-valued logic\n\nJaina seven-valued logic is system of argumentation developed by Jaina philosophers and thinkers to support and substantiate their theory of pluralism. This argumentation system has seven distinct semantic predicates which may be thought of as seven different truth values. Traditionally, in the Jaina and other Indian literature dealing with topics in Jain philosophy, this system of argumentation is referred to as \"Saptabhangivada\" or \"Syadvada\". The earliest reference to \"Syadvada\" occurs is the writings of Bhadrabahu (c. 433–357 BCE). There is mention of \"Syadvada\" in the \"Nyayavatara\" of Siddhasens Divakara (about 480–550 CE). Samantabhadra (about 600 CE) gave a full exposition of the seven parts of \"Syadvada\" or \"Saptabhanginyaya\" in his \"Aptamimamsa\". The \"Syadvadamanjari\" of Mallisena (1292 CE) is a separate treatise on the same theory. There are, of course, still later works and a large number of modern commentaries. The interpretation of Saptabhangivada as a seven-valued logic was attempted by Harvard University philosophy professor George Bosworth Burch (1902–1973) in a paper published in International Philosophical Quarterly in the year 1964. P. C. Mahalanobis, an Indian applied statistician, has given a probabilistic interpretation of the \"Saptabhangivada\".\n\nThe \"Saptabhangivada\", the seven predicate theory may be summarized as follows:\n\nThe seven predicate theory consists in the use of seven claims about sentences, each preceded by \"arguably\" or \"conditionally\" (syat), concerning a single object and its particular properties, composed of assertions and denials, either simultaneously or successively, and without contradiction. These seven claims are the following.\n\n\nThere are three basic truth values, namely, true (t), false (f) and unassertible (u). These are combined to produce four more truth values, namely, tf, tu, fu, and tfu. Though, superficially, it appears that there are only three distinct truth values a deeper analysis of the Jaina system reveals that the seven truth values are indeed distinct. This is a consequence of the conditionalising operator \"arguably\" denoted in Sanskrit by the word \"syat\". This Sanskrit word has the literal meaning of \"perhaps it is\", and it is used to mean \"from a certain standpoint\" or \"within a particular philosophical perspective\".\n\nIn this discussion the term \"standpoint\" has been used in a technical sense. Consider a situation in which a globally inconsistent set of propositions, the totality of philosophical discourse, is divided into sub-sets, each of which is internally consistent. Any proposition might be supported by others from within the same sub-set. At the same time, the negation of that proposition might occur in a distinct, though possibly overlapping subset, and be supported by other propositions within it. Each such consistent sub-set of a globally inconsistent discourse, is what the Jainas call a \"standpoint\" (naya). A standpoint corresponds to a particular philosophical perspective.\n\nIn this terminology, it can be seen that the seven predicates get translated to the following seven possibilities. Each proposition \"p\" has the following seven states:\n\n\n"}
{"id": "43929588", "url": "https://en.wikipedia.org/wiki?curid=43929588", "title": "Kindness UK", "text": "Kindness UK\n\nKindness UK is an independent London-based not-for-profit organisation that promotes kindness in United Kingdom.\n\nKindness UK was founded by social entrepreneur David Jamilly in 2011 with the aim of undertaking initiatives to enhance the value and profile of kindness in society. Jamilly previously founded Pod Children's Charity in 1977, the Good Deeds Organisation in 2007 and co-founded Kindness Day UK in 2010.\n\nIn 2016 Kindness UK in collaboration with Coolabi Group developed a month-long campaign \"Clangers for Kindness\", designed to promote simple acts of kindness among kids and their parents.\n\nIn partnership with the University of Sussex the Kindness UK Doctoral Conference was launched in 2015. The\nKindness UK Doctoral Conference Award is open to all academic disciplines and is dedicated to the kindness and its effect on people and communities.\n\nInterdisciplinary University of Sussex Kindness UK Symposium was held in 2016.\n\nIn 2014 on Kindness Day UK, Kindness UK distributed 10,000 chocolate bars at London underground stations as a random act of kindness.\n\nKindness Day UK encourages the public to recognize the value of kindness and perform at least one act of kindness on the day. Kindness UK is a lead UK organisation that promotes and celebrates Kindness Day UK.\n\nKindness UK were asked to guest judge Nissan’s CARED4 competition which was aimed to search for and reward kind people in the community.\n\n"}
{"id": "32248891", "url": "https://en.wikipedia.org/wiki?curid=32248891", "title": "Livestock Keepers' Rights", "text": "Livestock Keepers' Rights\n\nLivestock Keepers' Rights are a bundle of rights that would support the survival of small-scale livestock keepers such as pastoralists, smallholders and family farms in a general policy environment that favours large-scale industrial modes of livestock production.\n\nThe term \"Livestock Keepers' Rights\" was coined during the World Food Summit in 2002 by civil society attending the Forum for Food Sovereignty to flag the role of livestock keepers in animal genetic resource management. It alluded to \"Farmers' Rights\" as known from the International Treaty on Plant Genetic Resources for Food and Agriculture that had been recently concluded.\n\nBetween . 2003 and 2007, a large number of grassroots consultations were carried out by and with livestock keeping communities to define the term more closely. These consultations took place in Kenya \"Karen Comittment\"), India, Italy (\"Bellagio Brief\") and Ethiopia (\"Addis Résumé\") and involved about 500 representatives of livestock keeping communities from Africa, Asia, Latin America and Europe. They identified 7 cornerstones of \"Livestock Keepers' Rights\" that would enable livestock keepers to continue playing their role as guardians of biological diversity.\n\nDuring this process, Livestock Keepers’ Rights were elaborated into a much more comprehensive concept than Farmers’ Rights. Rather than representing legal rights, they correspond to development principles that would help livestock keepers continue to conserve biodiversity and animal genetic resources.\n\nDuring a workshop with legal experts held in Kalk Bay, South Africa in December 2008, the rights were further refined and subdivided into principles and rights:\n\nPrinciple 1:\n\"Livestock Keepers are creators of breeds and custodians of animal genetic resources for food and agriculture. \"\n\nPrinciple 2:\n\"Livestock Keepers and the sustainable use of traditional breeds are dependent on the conservation of their respective ecosystems. \"\n\nPrinciple 3:\n\"Traditional breeds represent collective property, products of indigenous knowledge and cultural expression of Livestock Keepers.\"\n\nBased on these principles articulated and implicit in existing legal instruments and international agreements, Livestock Keepers from traditional livestock keeping communities and/or adhering to ecological principles of animal production, shall be given the following Livestock Keepers' Rights:\n\n\nThe workshop also resulted in a Declaration on Livestock Keepers Rights that references the individual principles and rights to existing international legal frameworks such as the UN Convention on Biological Diversity, the United Nations Convention to Combat Desertification, the Global Plan of Action for Animal Genetic Resources and the Interlaken Declaration on Animal Genetic Resources, as well as the Universal Declaration of Human Rights, the International Covenant on Economic, Social and Cultural Rights, the United Nations Declaration on the Rights of Indigenous Peoples, the Convention on the Protection and Promotion of the Diversity of Cultural Expressions, the Convention (No. 169) concerning Indigenous and Tribal Peoples in Independent Countries, the Declaration on the Rights of Persons Belonging to National or Ethnic, Religious and Linguistic Minorities, and other pertinent legal agreements.\n\nThe Declaration on Livestock Keepers’ Rights was signed by a large number of individuals and organizations. At an International Technical Expert Workshop on Access and Benefit Sharing in Animal Genetic Resources for Food and Agriculture, that was held in Wageningen in the Netherlands from 8–10 December 2010, it was decided that Livestock Keepers’ Rights should be addressed. At another workshop on rights to animal genetic resources that was held in Bern (Switzerland) on 20 June 2011, Livestock Keepers’ Rights were contrasted with Animal Breeders Rights.In July 2011,at a side-event during the 13th session of the Commission on Genetic Resources for Food and Agriculture (CGRFA) held at FAO in Rome, the concept of Livestock Keepers' Rights was introduced and supported by government officials.\n\nKöhler-Rollefson, I., E. Mathias and H.S. Rathore. 2008. Local breeds, livelihoods, and livestock keepers’ rights in South Asia. Tropical Animal Health and Production, published on line 22 November 2008\n\nKöhler-Rollefson, I., Evelyn Mathias, Hanwant Singh, P. Vivekanandan and Jacob Wanyama. 2010. Livestock Keepers’ Rights: The State of Discussion. Animal Genetic Resources: 47, 1–5.\n\nKöhler-Rollefson, I., P. Vivekanandan and HS Rathore. 2010. Livestock Keepers Rights and Biocultural Protocols : Tools for Protecting Biodiversity and the Livelihoods of the Poor. LEISA India 12(1):35-36\n\nKöhler-Rollefson, I. and E. Mathias. 2010. Livestock Keepers‘Rights: a Rights-based approach towards invoking justice for pastoralists and biodiversity conserving livestock keepers. Policy Matters.\n\n"}
{"id": "55988690", "url": "https://en.wikipedia.org/wiki?curid=55988690", "title": "Materiality (social sciences and humanities)", "text": "Materiality (social sciences and humanities)\n\nIn the social sciences, materiality is the notion that the physical properties of a cultural artifact have consequences for how the object is used. Some scholars expand this definition to encompass a broader range of actions, such as the process of making art, and the power of organizations and institutions to orient activity around themselves. The concept of materiality is used across many disciplines within the social sciences to focus attention on the impact of material or physical factors. Scholars working in science and technology studies, anthropology, organization studies, or communication studies may incorporate materiality as a dimension of their investigations. Central figures in the social scientific study of materiality are Harold Innis and Marshall McLuhan.\nCommunication studies investigated by scholars include the particular impacts of media as it emerged and spread. In particular, the growth of newspapers, radio, television, personal computing and the Internet have been examined. Collectively, these are termed “media effects” studies. Some communication scholars make use of the concept of concordances: the idea that the features of a particular technology may encourage certain behaviours from the technology's users. Other scholars explore how technologies and the communities that use them may be mutually determining (the way users respond to technology tends to drive both features and cultural norms among users of that technology) or they may behave as co-creators (the abilities and limitations of a technology may make it a part of the works created using that technology).\n\nAlthough science and technology studies (STS) are typically associated with a social constructivist viewpoint, some STS scholars (e.g. Langdon Winner) incorporate materiality into their studies of technology and explore how the affordances of technology may shape or even control their use. Actor-network theory, or ANT, is an example of an STS theory which incorporates both social and material interactions.\n\nThe Toronto School view of materiality, also known as the ‘medium’ view, includes the intellectual legacies of Innis and McLuhan, who focused attention on the consequences of the medium, on what authors communicate and on what audiences experience. Innis explored the broad historical consequences of time-bound media (e.g. transportable but fragile media such as papyrus) and space-bound media (e.g. hard-to-transport but longer-lasting media such as stone tablets). Different cultures have used various media to store information and its availability and transportability through time impacts its use. In an extension of Innis' ideas, McLuhan wrote, “The medium is the message”, that is, the way people transmit ideas is consequential in and of itself. The influence of the medium can be invisible and difficult to characterize.\n"}
{"id": "40283", "url": "https://en.wikipedia.org/wiki?curid=40283", "title": "Melting point", "text": "Melting point\n\nThe melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.\n\nWhen considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the \"characteristic freezing point\" of a substance is determined, in fact the actual methodology is almost always \"the principle of observing the disappearance rather than the formation of ice\", that is, the melting point.\n\nFor most substances, melting and freezing points are approximately equal. For example, the melting point \"and\" freezing point of mercury is 234.32 kelvins (−38.83 °C or −37.89 °F). However, certain substances possess differing solid-liquid transition temperatures. For example, agar melts at 85 °C (185 °F) and solidifies from ; such direction dependence is known as hysteresis. The melting point of ice at 1 atmosphere of pressure is very close to ; this is also known as the ice point. In the presence of nucleating substances, the freezing point of water is not always the same as the melting point. In the absence of nucleators water can exist as a supercooled liquid down to −48.3 °C (−55 °F, 224.8 K) before freezing.\n\nThe chemical element with the highest melting point is tungsten, at ; this property makes tungsten excellent for use as filaments in light bulbs. The often-cited carbon does not melt at ambient pressure but sublimes at about ; a liquid phase only exists above pressures of and estimated (see ). Tantalum hafnium carbide (TaHfC) is a refractory compound with a very high melting point of 4215 K (3942 °C, 7128 °F). At the other end of the scale, helium does not freeze at all at normal pressure even at temperatures close to absolute zero; a pressure of more than twenty times normal atmospheric pressure is necessary.\n\nMany laboratory techniques exist for the determination of melting points.\nA Kofler bench is a metal strip with a temperature gradient (range from room temperature to 300 °C). Any substance can be placed on a section of the strip, revealing its thermal behaviour at the temperature at that point. Differential scanning calorimetry gives information on melting point together with its enthalpy of fusion.\nA basic melting point apparatus for the analysis of crystalline solids consists of an oil bath with a transparent window (most basic design: a Thiele tube) and a simple magnifier. The several grains of a solid are placed in a thin glass tube and partially immersed in the oil bath. The oil bath is heated (and stirred) and with the aid of the magnifier (and external light source) melting of the individual crystals at a certain temperature can be observed. In large/small devices, the sample is placed in a heating block, and optical detection is automated.\n\nThe measurement can also be made continuously with an operating process. For instance, oil refineries measure the freeze point of diesel fuel online, meaning that the sample is taken from the process and measured automatically. This allows for more frequent measurements as the sample does not have to be manually collected and taken to a remote laboratory.\n\nFor refractory materials (e.g. platinum, tungsten, tantalum, some carbides and nitrides, etc.) the extremely high melting point (typically considered to be above, say, 1800 °C) may be determined by heating the material in a black body furnace and measuring the black-body temperature with an optical pyrometer. For the highest melting materials, this may require extrapolation by several hundred degrees. The spectral radiance from an incandescent body is known to be a function of its temperature. An optical pyrometer matches the radiance of a body under study to the radiance of a source that has been previously calibrated as a function of temperature. In this way, the measurement of the absolute magnitude of the intensity of radiation is unnecessary. However, known temperatures must be used to determine the calibration of the pyrometer. For temperatures above the calibration range of the source, an extrapolation technique must be employed. This extrapolation is accomplished by using Planck's law of radiation. The constants in this equation are not known with sufficient accuracy, causing errors in the extrapolation to become larger at higher temperatures. However, standard techniques have been developed to perform this extrapolation.\n\nConsider the case of using gold as the source (mp = 1063 °C). In this technique, the current through the filament of the pyrometer is adjusted until the light intensity of the filament matches that of a black-body at the melting point of gold. This establishes the primary calibration temperature and can be expressed in terms of current through the pyrometer lamp. With the same current setting, the pyrometer is sighted on another black-body at a higher temperature. An absorbing medium of known transmission is inserted between the pyrometer and this black-body. The temperature of the black-body is then adjusted until a match exists between its intensity and that of the pyrometer filament. The true higher temperature of the black-body is then determined from Planck's Law. The absorbing medium is then removed and the current through the filament is adjusted to match the filament intensity to that of the black-body. This establishes a second calibration point for the pyrometer. This step is repeated to carry the calibration to higher temperatures. Now, temperatures and their corresponding pyrometer filament currents are known and a curve of temperature versus current can be drawn. This curve can then be extrapolated to very high temperatures.\n\nIn determining melting points of a refractory substance by this method, it is necessary to either have black body conditions or to know the emissivity of the material being measured. The containment of the high melting material in the liquid state may introduce experimental difficulties. Melting temperatures of some refractory metals have thus been measured by observing the radiation from a black body cavity in solid metal specimens that were much longer than they were wide. To form such a cavity, a hole is drilled perpendicular to the long axis at the center of a rod of the material. These rods are then heated by passing a very large current through them, and the radiation emitted from the hole is observed with an optical pyrometer. The point of melting is indicated by the darkening of the hole when the liquid phase appears, destroying the black body conditions. Today, containerless laser heating techniques, combined with fast pyrometers and spectro-pyrometers, are employed to allow for precise control of the time for which the sample is kept at extreme temperatures. Such experiments of sub-second duration address several of the challenges associated with more traditional melting point measurements made at very high temperatures, such as sample vaporization and reaction with the container.\n\nFor a solid to melt, heat is required to raise its temperature to the melting point. However, further heat needs to be supplied for the melting to take place: this is called the heat of fusion, and is an example of latent heat.\n\nFrom a thermodynamics point of view, at the melting point the change in Gibbs free energy (ΔG) of the material is zero, but the enthalpy (\"H\") and the entropy (\"S\") of the material are increasing (ΔH, ΔS > 0). Melting phenomenon happens when the Gibbs free energy of the liquid becomes lower than the solid for that material. At various pressures this happens at a specific temperature. It can also be shown that:\n\nHere \"T\", \"ΔS\" and \"ΔH\" are respectively the temperature at the melting point, change of entropy of melting and the change of enthalpy of melting.\n\nThe melting point is sensitive to extremely large changes in pressure, but generally this sensitivity is orders of magnitude less than that for the boiling point, because the solid-liquid transition represents only a small change in volume. If, as observed in most cases, a substance is more dense in the solid than in the liquid state, the melting point will increase with increases in pressure. Otherwise the reverse behavior occurs. Notably, this is the case of water, as illustrated graphically to the right, but also of Si, Ge, Ga, Bi. With extremely large changes in pressure, substantial changes to the melting point are observed. For example, the melting point of silicon at ambient pressure (0.1 MPa) is 1415 °C, but at pressures in excess of 10 GPa it decreases to 1000 °C.\n\nMelting points are often used to characterize organic and inorganic compounds and to ascertain their purity. The melting point of a pure substance is always higher and has a smaller range than the melting point of an impure substance or, more generally, of mixtures. The higher the quantity of other components, the lower the melting point and the broader will be the melting point range, often referred to as the \"pasty range\". The temperature at which melting begins for a mixture is known as the \"solidus\" while the temperature where melting is complete is called the \"liquidus\". Eutectics are special types of mixtures that behave like single phases. They melt sharply at a constant temperature to form a liquid of the same composition. Alternatively, on cooling a liquid with the eutectic composition will solidify as uniformly dispersed, small (fine-grained) mixed crystals with the same composition.\n\nIn contrast to crystalline solids, glasses do not possess a melting point;\non heating they undergo a smooth glass transition into a viscous liquid.\nUpon further heating, they gradually soften, which can be characterized by certain softening points.\n\nThe freezing point of a solvent is depressed when another compound is added, meaning that a solution has a lower freezing point than a pure solvent. This phenomenon is used in technical applications to avoid freezing, for instance by adding salt or ethylene glycol to water.\n\nIn organic chemistry, Carnelley's rule, established in 1882 by Thomas Carnelley, states that \"high molecular symmetry is associated with high melting point\". Carnelley based his rule on examination of 15,000 chemical compounds. For example, for three structural isomers with molecular formula CH the melting point increases in the series isopentane −160 °C (113 K) n-pentane −129.8 °C (143 K) and neopentane −16.4 °C (256.8 K). Likewise in xylenes and also dichlorobenzenes the melting point increases in the order meta, ortho and then para. Pyridine has a lower symmetry than benzene hence its lower melting point but the melting point again increases with diazine and triazines. Many cage-like compounds like adamantane and cubane with high symmetry have relatively high melting points.\n\nA high melting point results from a high heat of fusion, a low entropy of fusion, or a combination of both. In highly symmetrical molecules the crystal phase is densely packed with many efficient intermolecular interactions resulting in a higher enthalpy change on melting.\n\nAn attempt to predict the bulk melting point of crystalline materials was first made in 1910 by Frederick Lindemann. The idea behind the theory was the observation that the average amplitude of thermal vibrations increases with increasing temperature. Melting initiates when the amplitude of vibration becomes large enough for adjacent atoms to partly occupy the same space. The Lindemann criterion states that melting is expected when the vibration root mean square amplitude exceeds a threshold value.\n\nAssuming that all atoms in a crystal vibrate with the same frequency \"ν\", the average thermal energy can be estimated using the equipartition theorem as\nwhere \"m\" is the atomic mass, \"ν\" is the frequency, \"u\" is the average vibration amplitude, \"k\" is the Boltzmann constant, and \"T\" is the absolute temperature. If the threshold value of \"u\" is \"ca\" where \"c\" is the Lindemann constant and \"a\" is the atomic spacing, then the melting point is estimated as\nSeveral other expressions for the estimated melting temperature can be obtained depending on the estimate of the average thermal energy. Another commonly used expression for the Lindemann criterion is\nFrom the expression for the Debye frequency for \"ν\", we have\nwhere \"θ\" is the Debye temperature and \"h\" is the Planck constant. Values of \"c\" range from 0.15–0.3 for most materials.\n\nIn February 2011, Alfa Aesar released over 10,000 melting points of compounds from their catalog as open data. This dataset has been used to create a random forest model for melting point prediction which is now freely available. Open melting point data are also available from \"Nature Precedings\". High quality data mined from patents and also models developed with these data were published by Tetko \"et al\".\n\n\n"}
{"id": "6331719", "url": "https://en.wikipedia.org/wiki?curid=6331719", "title": "Mind-wandering", "text": "Mind-wandering\n\nMind-wandering (sometimes referred to as task unrelated thought) is the experience of thoughts not remaining on a single topic for a long period of time, particularly when people are engaged in an attention-demanding task.\n\nMind-wandering tends to occur during driving, reading and other activities where vigilance may be low. In these situations, people do not remember what happened in the surrounding environment because they are preoccupied with their thoughts. This is known as the decoupling hypothesis. Studies using event-related potentials (ERPs) have quantified the extent that mind-wandering reduces the cortical processing of the external environment. When thoughts are unrelated to the task at hand, the brain processes both task-relevant and unrelated sensory information in a less detailed manner.\n\nMind-wandering appears to be a stable trait of people and a transient state. Studies have linked performance problems in the laboratory and in daily life. Mind-wandering has been associated with possible car accidents. Mind-wandering is also intimately linked to states of affect. Studies indicate that task-unrelated thoughts are common in people with low or depressed mood. Mind-wandering also occurs when a person is intoxicated via the consumption of alcohol.\n\nStudies have demonstrated a prospective bias to spontaneous thought because individuals tend to engage in more future than past related thoughts during mind-wandering. The default mode network is thought to be involved in mind-wandering and internally directed thought, although recent work has challenged this assumption \n\nThe history of mind-wandering research dates back to 18th century England. British philosophers struggled to determine whether mind-wandering occurred in the mind or if an outside source caused it. In 1921, Varendonck published \"The Psychology of Day-Dreams\", in which he traced his \"'trains of thoughts' to identify their origins, most often irrelevant external influences\". Wallas (1926) considered mind-wandering as an important aspect of his second stage of creative thought – incubation. It wasn't until the 1960s that the first documented studies were conducted on mind-wandering. John Antrobus and Jerome Singer developed a questionnaire and discussed the experience of mind-wandering. This questionnaire, known as the Imaginal Processes Inventory (IPI), provides a trait measure of mind-wandering and it assesses the experience on three dimensions: how vivid the person's thoughts are, how many of those thoughts are guilt- or fear-based, and how deep into the thought a person goes. As technology continues to develop, psychologists are starting to use functional magnetic resonance imaging to observe mind-wandering in the brain and reduce psychologists' reliance on verbal reports.\n\nJonathan Smallwood and colleagues popularized mind-wandering using thought sampling and questionnaires. Mind-wandering is studied using experience sampling either online or retrospectively. One common paradigm within which to study mind-wandering is the SART (sustained attention to response) task. In a SART task there are two categories of words. One of the categories are the target words. In each block of the task a word appears for about 300 ms, there will be a pause and then another word. When a target word appears the participant hits a designated key. About 60% of the time after a target word a thought probe will appear to gauge whether thoughts were on task. If participants were not engaged in the task they were experiencing task-unrelated thoughts (TUTs), signifying mind-wandering. Another task to judge TUTs is the experience sampling method (ESM). Participants carry around a personal digital assistant (PDA) that signals several times a day. At the signal a questionnaire is provided. The questionnaire questions vary but can include: (a) whether or not their minds had wandered at the time of the (b) what state of control they had over their thoughts and (c) about the content of their thoughts. Questions about context are also asked to measure the level of attention necessary for the task. One process used was to give participants something to focus on and then at different times ask them what they were thinking about. Those who were not thinking about what was given to them were considered \"wandering\". Another process was to have participants keep a diary of their mind-wandering. Participants are asked to write a brief description of their mind-wandering and the time in which it happened. These methodologies are improvements on past methods that were inconclusive.\n\nMind-wandering is important in understanding how the brain produces what William James called the train of thought and the stream of consciousness. This aspect of mind-wandering research is focused on understanding how the brain generates the spontaneous and relatively unconstrained thoughts that are experienced when the mind wanders. One candidate neural mechanism for generating this aspect of experience is a network of regions in the frontal and parietal cortex known as the default network. This network of regions is highly active even when participants are resting with their eyes closed suggesting a role in generating spontaneous internal thoughts. One relatively controversial result is that periods of mind-wandering are associated with increased activation in both the default and executive system a result that implies that mind-wandering may often be goal oriented.\n\nIt is commonly assumed that the default mode network is known to be involved during mind-wandering. The default mode network is active when a person is not focused on the outside world and the brain is at wakeful rest because experiences such as during mind-wandering and daydreaming are common in this state. However, it is also active when the individual is thinking about others, thinking about themselves, remembering the past, and planning for the future. However, recent studies show that signals in the default mode network provide information regarding patterns of detailed experience in active tasks states. These data suggest that the relationship between the default mode network and mind-wandering remains a matter of conjecture.\n\nIn addition to neural models, computational models of consciousness based on Bernard Baars' Global Workspace theory suggest that mind-wandering, or \"spontaneous thought\" may involve competition between internally and externally generated activities attempting to gain access to a limited capacity central network.\n\nThere are individual differences in some aspects of mind-wandering between older and younger adults. Although older adults reported less mind-wandering, these older participants showed the same amount of mind-wandering as younger adults. There were also differences in how participants responded to an error. After an error, older adults took longer to return focus back to the task when compared with younger adults. It is possible that older adults reflect more about an error due to conscientiousness. Research has shown that older adults tend to be more conscientious than young adults. Personality can also affect mind-wandering. People that are more conscientious are less prone to mind-wandering. Being more conscientious allows people to stay focused on the task better which causes fewer instances of mind-wandering. Differences in mind-wandering between young and older adults may be limited because of this personality difference.\n\nMental disorders, such as ADHD, also go along with changes in different aspects of mind-wandering. In many disorders it is the regulation of the overall amount of mind-wandering that is disturbed, leading to increased distractibility when performing tasks. Additionally, the contents of mind-wandering is changed; thoughts can be more negative and past-oriented, particularly unstable or self-centered.\n\nOne important question facing the study of mind-wandering is how it relates to working memory capacity. Recent research has studied the relationship between mind-wandering and working memory capacity. This relationship requires more research to understand how they influence one another. It is possible that mind-wandering causes lower performance on working memory capacity tasks or that lower working memory capacity causes more instances of mind-wandering. Although only this last one has actually been proven. Also, reports of task-unrelated thoughts are less frequent when performing tasks that do not demand continuous use of working memory than tasks which do. Moreover, individual difference studies demonstrate that when tasks are non-demanding, high levels of working memory capacity are associated with more frequent reports of task-unrelated thinking especially when it is focused on the future. By contrast, when performing tasks that demand continuous attention, high levels of working memory capacity are associated with fewer reports of task-unrelated thoughts. Together these data are consistent with the claim that working memory capacity helps sustain a train of thought whether it is generated in response to a perceptual event or is self-generated by the individual. Therefore, under certain circumstances, the experience of mind-wandering is supported by working memory resources. Working memory capacity variation in individuals has been proven to be a good predictor of the natural tendency for mind-wandering to occur during cognitively demanding tasks and various activities in daily life.\nMind-wandering sometimes occurs as a result of saccades. In an antisaccade task, for example, subjects with higher working memory capacity scores resisted looking at the flashing visual cue better than participants with lower working memory capacity. Higher working memory capacity is associated with fewer saccades toward environmental cues. \nMind-wandering has been shown to be related to goal orientation; people with higher working memory capacity keep their goals more accessible than those who have lower working memory capacity, thus allowing these goals to better guide their behavior and keep them on task.\n\nAnother study compared differences in speed of processing information between people of different ages. The task they used was a go/no go task where participants responded if a white arrow moved in a specific direction but did not respond if the arrow moved in the other direction or was a different color. In this task, children and young adults showed similar speed of processing but older adults were significantly slower. Speed of processing information affects how much information can be processed in working memory. People with faster speed of processing can encode information into memory better than people that have slower speed of processing. This can lead to memory of more items because more things can be encoded.\n\nMind-wandering affects retention where working memory capacity is directly related to reading comprehension levels. Participants with lower working memory capacity perform worse on comprehension-based tests. \nWhen investigating how mind-wandering affects retention of information, experiments are conducted where participants are asked a variety of questions about factual information, or deducible information while reading a detective novel. Participants are also asked about the state of their mind before the questions are asked. Throughout the reading itself, the author provides important cues to identify the villain, known as inference critical episodes (ICEs). The questions are asked randomly and before critical episodes are reached. It was found that episodes of mind-wandering, especially early on in the text led to decreased identification of the villain and worse results on both factual and deducible questions. Therefore, when mind-wandering occurs during reading, the text is not processed well enough to remember key information about the story. Furthermore, both the timing and the frequency of mind-wandering helps determine how much information is retained from the narrative.\n\nReading comprehension must also be investigated in terms of text difficulty. To assess this, researchers provide an easy and hard version of a reading task. During this task, participants are interrupted and asked whether their thoughts at the time of interruption had been related or unrelated to the task. What is found is that mind-wandering has a negative effect on text comprehension in more difficult readings. This supports the executive-resource hypothesis which describes that both task related and task-unrelated thoughts (TUT) compete for executive function resources. Therefore, when the primary task is difficult, little resources are available for mind-wandering, whereas when the task is simple, the possibility for mind-wandering is abundant because it takes little executive control to focus on simple tasks. However, mind-wandering tends to occur more frequently in harder readings as opposed to easier readings. Therefore, it is possible that similar to retention, mind-wandering increases when readers have difficulty constructing a model of the story.\n\nAs part of his doctoral research at Harvard University, Matthew Killingsworth used an iPhone app that captured a user's feelings in real time. The tool alerts the user at random times and asks: \"How are you feeling right now?\" and \"What are you doing right now?\" Killingsworth and Gilbert's analysis suggested that mind-wandering was much more typical in daily activities than in laboratory settings. They also describe that people were less happy when their minds were wandering than when they were otherwise occupied. This effect was somewhat counteracted by people's tendency to mind-wander to happy topics, but unhappy mind-wandering was more likely to be rated as more unpleasant than other activities. The authors note that unhappy moods can also cause mind-wandering, but the time-lags between mind-wandering and mood suggests that mind-wandering itself can also lead to negative moods. Furthermore, research suggests that regardless of working memory capacity, participants participating in mind-wandering experiments report more mind-wandering when bored, stressed, unhappy.\n\nExecutive functions (EFs) are cognitive processes that make a person pay attention or concentrate on a task. Three executive functions that relate to memory are inhibiting, updating and shifting. Inhibiting controls a person's attention and thoughts when distractions are abundant. Updating reviews old information and replaces it with new information in the working memory. Shifting controls the ability to go between multiple tasks. All three EFs have a relationship to mind-wandering.\n\nExecutive functions have roles in attention problems, attention control, thought control, and working memory capacity. Attention problems relate to behavioral problems such as inattention, impulsivity and hyperactivity. These behaviors make staying on task difficult leading to more mind-wandering. Higher inhibiting and updating abilities correlates to lower levels of attention problems in adolescence. The inhibiting executive function controls attention and thought. The failure of cognitive inhibition is a direct cause of mind-wandering. Mind-wandering is also connected to working memory capacity (WMC). People with higher WMC mind-wander less on high concentration tasks no matter their boredom levels. People with low WMC are better at staying on task for low concentration tasks, but once the task increases in difficulty they had a hard time keeping their thoughts focused on task. Updating takes place in the working memory, therefore those with low WMC have a lower updating executive function ability. That means a low performing updating executive function can be an indicator of high mind-wandering. Working memory relies on executive functions, with mind-wandering as an indicator of their failure. task-unrelated thoughts (TUTs) are empirical behavioral manifestations of mind-wandering in a person. The longer a task is performed the more TUTs reported. Mind-wandering is an indication of an executive control failure that is characterized by TUTs.\n\nPeople can make decisions by mind-wandering, for example, looking at a set of art posters until the most desirable comes to mind. In contrast to people's belief that the choices they make while mind-wandering will be worse than the choices made though more careful deliberation over choice options, people appear to be as happy with choices made by mind-wandering as with choices made by engaging in more careful deliberation. Giblin, Morewedge, and Norton (2013) conducted two experiments in which participants judged the value of a poster chosen through conscious deliberation, mind-wandering, or random assignment. Forecasters in a first experiment predicted that participants would like and value the art poster they chose in the deliberation condition most, that they would like and value the poster chose by mind-wandering less, and they would like and value the poster to which they were randomly assigned least of the three methods. By contrast, in an incentive-compatible second experiment, participants who chose a poster by one of these three methods (via random assignment) liked and valued the poster they chose as much in the deciding by mind-wandering condition as in the deciding by deliberation condition. Participants in the random assignment condition liked and valued their poster least.\n\nPaul Seli and colleagues have shown that spontaneous mind-wandering is associated with increased fidgeting; by contrast, interest, attention and visual engagement lead to Non-Instrumental Movement Inhibition. One possible application for this phenomenon is that detection of non-instrumental movements may be an indicator of attention or boredom in computer aided learning. Traditionally teachers and students have viewed fidgeting as a sign of diminished attention, which is summarized by the statement, “Concentration of consciousness, and concentration of movements, diffusion of ideas and diffusion of movements go together.” However, James Farley and colleagues have proposed that fidgeting is not only an indicator of spontaneous mind-wandering, but is also a subconscious attempt to increase arousal in order to improve attention and thus reduce mind-wandering.\n\n\n"}
{"id": "2019798", "url": "https://en.wikipedia.org/wiki?curid=2019798", "title": "Normalcy bias", "text": "Normalcy bias\n\nThe normalcy bias, or normality bias, is a belief people hold when facing a disaster. It causes people to underestimate both the likelihood of a disaster and its possible effects, because people believe that things will always function the way things normally have functioned. This may result in situations where people fail to adequately prepare themselves for disasters, and on a larger scale, the failure of governments to include the populace in its disaster preparations. About 70% of people reportedly display normalcy bias in disasters.\n\nJournalist Amanda Ripley identified common response patterns of people in disasters and found that there are three phases of response: Denial, Deliberation and the Decisive Moment. The faster people can get through the Denial and Deliberation phase, the quicker they will reach the Decisive Moment and begin to take action.\n\nThe normalcy bias can manifest itself in various disasters, ranging from car crashes to world-historical events. It is hypothesized that the normalcy bias may be caused by the way the brain processes new information. Stress slows information processing, and when the brain cannot find an acceptable response to a situation, it fixates on a single and sometimes default solution. This single resolution can result in unnecessary injury or death in disaster situations. The lack of preparation for disasters often leads to inadequate shelter, supplies, and evacuation plans. Thus, normalcy bias can cause people to drastically underestimate the effects of the disaster and assume that everything will be all right. The negative effects of normalcy bias can be combatted through the four stages of disaster response: preparation, warning, impact, and aftermath.\n\nNormalcy bias has also been called \"analysis paralysis\", \"the ostrich effect\", and by first responders, \"the negative panic\". The opposite of normalcy bias is overreaction, or worst-case scenario bias, in which small deviations from normality are dealt with as signals of an impending catastrophe.\n\nAmanda Ripley, author of \"The Unthinkable: Who Survives When Disaster Strikes – and Why\", identifies common response patterns of people in disasters and explains that there are three phases of response: Denial, Deliberation and the Decisive Moment. With regard to the first phase, described as Denial, Ripley found that people were likely to deny that a disaster was happening. It takes time for the brain to process information and recognize that a disaster is a threat. In the Deliberation phase, people have to decide what to do. If the person does not have a plan in place, this creates a serious problem because the effects of life-threatening stress on the body (e.g. tunnel vision, audio exclusion, time dilations, out-of-body experiences, or reduced motor skills) limit an individual's ability to perceive information and make plans. Ripley asserts that in the third and final phase, described as the Decisive Moment, a person must act quickly and decisively. Failure to do so can result in injury or death. She explains that the faster someone can get through the Denial and Deliberation phase, the quicker they will reach the Decisive Moment and begin to take action.\n\n\"Normalcy bias flows into the brain no matter the scale of the problem,\" journalist David McRaney has written. \"It will appear whether you have days and plenty of warning or are blindsided with only seconds between life and death.\" It can manifest itself in phenomena such as car crashes. Car crashes occur very frequently, but the average individual experiences them only rarely, if ever. It also manifests itself in connection with events in world history. According to a 2001 study by sociologist Thomas Drabek, when people are asked to leave in anticipation of a disaster, most check with four or more sources of information before deciding what to do. The process of checking in, known as milling, is common in disasters.\n\nAs for events in world history, the normalcy bias explains why, when the volcano Vesuvius erupted, the residents of Pompeii watched for hours without evacuating. It explains why thousands of people refused to leave New Orleans as Hurricane Katrina approached and why at least 70% of 9/11 survivors spoke with others before leaving. Officials at the White Star Line made insufficient preparations to evacuate passengers on the \"Titanic\" and people refused evacuation orders because they underestimated the odds of a worst-case scenario and minimized its potential impact. Similarly, experts connected with the Fukushima nuclear power plant were strongly convinced that a multiple reactor meltdown could never occur.\n\nA website for police officers has noted that members of that profession have \"all seen videos of officers who were injured or killed while dealing with an ambiguous situation, like the old one of a father with his young daughter on a traffic stop\". In a video referred to, \"the officer misses multiple threat cues...because the assailant talks lovingly about his daughter and jokes about how packed his minivan is. The officer only seems to react to the positive interactions, while seeming to ignore the negative signals. It's almost as if the officer is thinking, 'Well I've never been brutally assaulted before so it certainly won't happen now.' No one is surprised at the end of the video when the officer is violently attacked, unable to put up an effective defense.\" This professional failure, notes the website, is a consequence of normalcy bias.\n\nNormalcy bias, David McRaney has written, \"is often factored into fatality predictions in everything from ship sinkings to stadium evacuations.\" Disaster movies, he adds, \"get it all wrong. When you and others are warned of danger, you don’t evacuate immediately while screaming and flailing your arms.\" McRaney notes that in the book \"Big Weather\", tornado chaser Mark Svenvold discusses \"how contagious normalcy bias can be. He recalled how people often tried to convince him to chill out while fleeing from impending doom. Even when tornado warnings were issued, people assumed it was someone else's problem. Stake-holding peers, he said, would try to shame him into denial so they could remain calm. They didn't want him deflating their attempts at feeling normal\".\n\nPeople who promote conspiracy theories or apocalyptic future scenarios have cited the normalcy bias as a prime reason why others scoff at their pronouncements. For example, survivalists who fear that the U.S. will soon descend into totalitarianism cite normalcy bias as the reason why most Americans do not share their worries. Similarly, fundamentalist Christians use the normalcy bias to explain why others scoff at their beliefs about the \"End Time\". One fundamentalist website writes: \"May we not get blinded by the 'normalcy bias' but rather live with the knowledge that the Lord’s coming is near.\"\n\nThe normalcy bias may be caused in part by the way the brain processes new data. Research suggests that even when the brain is calm, it takes 8–10 seconds to process new information. Stress slows the process, and when the brain cannot find an acceptable response to a situation, it fixates on a single and sometimes default solution that may or may not be correct. An evolutionary reason for this response could be that paralysis gives an animal a better chance of surviving an attack and predators are less likely to see prey that is not moving.\n\nAbout 70% of people reportedly display normalcy bias in disasters. Normalcy bias has been described as \"one of the most dangerous biases we have\". The lack of preparation for disasters often leads to inadequate shelter, supplies, and evacuation plans. Even when all these things are in place, individuals with a normalcy bias often refuse to leave their homes.\n\nNormalcy bias can cause people to drastically underestimate the effects of the disaster. Therefore, people think that they will be safe even though information from the radio, television, or neighbors gives them reasons to believe there is a risk. The normalcy bias creates a cognitive dissonance that people then must work to eliminate. Some manage to eliminate it by refusing to believe new warnings coming in and refusing to evacuate (maintaining the normalcy bias), while others eliminate the dissonance by escaping the danger. The possibility that some people may refuse to evacuate causes significant problems in disaster planning.\n\nThe negative effects of normalcy bias can be combatted through the four stages of disaster response:\n\nThe opposite of normalcy bias is overreaction bias. Noting the effect regression to the mean, most deviations from normalcy do not lead to catastrophe, despite regular predictions of doomsday. Both underreaction (normalcy bias) and overreaction (worst-case thinking) are cognitive flaws and may extend to patterns of cognitive distortions.\n"}
{"id": "32194239", "url": "https://en.wikipedia.org/wiki?curid=32194239", "title": "North–South model", "text": "North–South model\n\nThe North–South model, developed largely by Columbia University economics professor Ronald Findlay, is a model in developmental economics that explains the growth of a less developed \"South\" or \"periphery\" economy that interacts through trade with a more developed \"North\" or \"core\" economy. The North–South model is used by dependencia theorists as a theoretical economic justification for dependency theory.\n\nThe model makes a few critical assumptions about the North and the South, as well as the relationship between the two.\n\n\nThe North–South model begins by defining the relevant equations for the economies of each country, and concludes that the growth rate of the South is locked by the growth rate of the North. This conclusion relies heavily on an analysis of the terms of trade between the two countries; i.e., the price ratio between manufactures and primary products. The terms of trade, formula_1, are defined as\n\nformula_2\n\nTo determine equilibrium, we need only to look at the market for one of the goods, as per Walras' law. We consider the market for the South's goods: primary products. The demand for imports, M, from the South is a positive function of per capita consumption in the North and a negative function of the terms of trade, formula_1, (higher formula_1 means relative price of primary products is high and less will be demanded). The supply side comes from export of primary products by the South, X, and is a positive function of the terms of trade and the South’s aggregate consumption of primary products.\n\nThis graph makes it clear that the real terms of trade decreases when the growth rate is higher in the South than in the North (because, thanks to unity in elasticity of demand, the export line would shift to the right faster than the import line). The resultant decrease in the terms of trade, however, means a lower growth rate for the South. This creates a negative feedback cycle in which the growth rate of the South is exogenously determined by that of the North. Note that the growth rate of the north, g, is equal to n + m, where n is population growth and m is growth of labor-augmenting technical progress, as per the Solow-Swan model.\n\nThe conclusion, which fits in with dependency theory, is that the South can never grow faster than the North, and thus will never catch up.\n\nEconomic theories such as the North–South model have been used to justify arguments for import substitution. Under this theory, less developed countries should use barriers to trade such as protective tariffs to shelter their industries from foreign competition and allow them to grow to the point where they will be able to compete globally.\n\nIt is important to note, however, that the North–South model only applies to countries that are completely specialized; that is, they are not competing with foreign markets – they are the only ones producing whichever good they are producing. The way around the terms of trade trap predicted by the North–South model is to produce goods that do compete with foreign goods. For example, the Asian Tigers are famous for pursuing development strategies that involved using their comparative advantage in labor to produce labor-intensive goods like textiles more efficiently than the United States and Europe.\n\n"}
{"id": "42657043", "url": "https://en.wikipedia.org/wiki?curid=42657043", "title": "Part-time job terrorism", "text": "Part-time job terrorism\n\nJapanese culture is often associated with long hours, some individuals working as many as 60 hours a week. Such working hours are associated with mental and physiological health complications, with Japanese newspapers citing rare cases of spontaneous death, informally known as Karōshi. To complicate matters further, Japanese offices are often noisy, smoke-filled (due to lack of smoking laws) environments and this may have further adverse effects on the health and well-being of employees. Part-time job terrorism might in this context be a form of escapism, disenchanted employees gaining support from their peers outside of work to compensate for the praise or reward their working life lacks.\n\nThe social phenomenon emerged around the summer of 2013, when internet-based Japanese news agencies such as Yukan-news recorded such an incident. with more traditional news agencies later following suit. In Japanese the social phenomenon is termed \"baito tero\". The name stems from the Japanese word \"baito\", which means \"part-time job\" and is a loan-word originating from the German \"arbeit\", meaning \"work\". \n\nJapanese employers are nonplussed by such shenanigans, and penalties and punishment can range from administrative tongue-lashings to termination of employment; employees can also, in some circumstances, be held financially accountable for loss of business due to the negative publicity.\n\nhttp://www.justlanded.com/english/Japan/Japan-Guide/Jobs/Working-in-Japan\nhttp://www.oxforddictionaries.com/definition/english/karoshi\nhttp://thisjapaneselife.org/2013/02/27/on-smoking-cigarettes-in-japan/\nhttp://www.hindawi.com/journals/bmri/2014/303917/\n\n"}
{"id": "47156854", "url": "https://en.wikipedia.org/wiki?curid=47156854", "title": "Paul Zarifopol", "text": "Paul Zarifopol\n\nPaul Zarifopol (November 30, 1874 – May 1, 1934) was a Romanian literary and social critic, essayist, and literary historian. The scion of an aristocratic family, formally trained in both philology and the sociology of literature, he emerged in the 1910s as a rebel, highly distinctive, voice among the Romanian press and book reviewers. He was a confidant and publisher of the Romanian writer Ion Luca Caragiale, building his theories on Caragiale's already trenchant appraisals of Romanian society and culture. Zarifopol defended art for art's sake even against the Marxism of his father-in-law, Constantin Dobrogeanu-Gherea, and the Poporanism of his friend, Garabet Ibrăileanu. He was also a noted censurer of neoclassical trends, of philistinism, and of inauthentic customs, advocating renewal, but not revolution. A skeptic reviewer of modernist literature, he reemerged during the interwar as its dedicated promoter, but his preference for literary entertainment over substance and many of his literary bets were shortly dismissed by other experts of the day.\n\nZarifopol endures in cultural memory as an eccentric—not just because he tackled and derided the literary establishment, but also because he refused to publish most of his work in book form, or take up employment in academia. Having lost a considerable fortune, he lived withdrawn from the public eye, surviving on his revenues as a literary columnist, mostly for Ibrăileanu's \"Viața Românească\". Shortly before his death, he set up his own successful magazine, \"Revista Fundațiilor Regale\". In such venues, Zarifopol defended his cosmopolitan philosophy against other philologists, but also against the emerging neotraditionalists at \"Gândirea\" journal. Zarifopol viewed modern traditionalism as a fabrication and, with his essays, came out as a non-traditionalist and anti-totalitarian conservative thinker.\n\nThe future critic was born in Iași to Paul (Pavel) Zarifopol, or \"Zarifopoulo\", and his wife Elena (\"née\" Culiano). His paternal family had attested Greek, and more generically \"southern\" or Balkan, roots. Originally horse traders and assignees for Ottoman Turks settled in Moldavia, they were elevated into the Moldavian boyar nobility after 1850.\n\nOn his mother's Culiano side, Paul was related to prestigious literary and political figures. One of Elena's brothers was Nicolae Culianu, the boyar, astronomer, and doyen of \"Junimea\" literary society, great-grandfather of the religious scholar-novelist Ioan Petru Culianu. His other maternal uncle, who remained closest to Elena, was jurist Ștefan \"Nei\" Stamatiu-Culianu, also a \"Junimea\" man. A sister of theirs, Maria Nanu, was the wife of landowner Gheorghe Nanu—making Paul Jr a first cousin of poet D. Nanu.\n\nThe elder Zarifopol managed the estates of Moldavian Prince Mihail Sturdza in Cristești. It was there that he and Elena met. According to scholar Elena Vulcănescu, it is possible that Paul Jr was not his natural son, but born to Elena from a liaison with General Grigore Sturdza, heir to the Sturdza estate. Zarifopol eventually bought for himself the baroque manor and Sturdza property at Cârligi, near Roman, then a townhouse in Iași, where he and Stamatiu-Culianu managed Borta Rece tavern. His brothers George and Ștefan Zarifopol both had careers in local politics—the former, a Paris-trained agronomist, as a Prefect, the latter as a delegate to Chamber. Paul Sr was the great-uncle of poet Dimitrie Anghel; another of his nephews, Alexandru Zarifopol, was the adoptive father of writer Alexandru Paleologu.\n\nPaul Sr died in 1881, leaving Stamatiu in charge of family affairs. Their family fortune helped finance both sides of the family, and the Culianu children were all educated abroad. Paul Jr himself graduated from \"Junimea\"s \"Institutele-Unite\" private high school, followed by the literature faculty of Iași University, from 1892 to 1898. He made his published debut in Alexandru Dimitrie Xenopol's \"Arhiva\" in 1897, with a review of a historiographical work by Marie Henri d'Arbois de Jubainville. In 1899, he wrote a short story, \"Povestea Moșului\" (\"The Tale of the Old Man\"). Well-liked by his literary friends Gheorghe T. Kirileanu and Paul Bujor, it was published in \"Carmen Sylva\" magazine under the pen name of \"Z.\".\n\nIn 1902, Zarifopol's professor, Alexandru Philippide, took him on as an assistant and considered him for a likely successor; Zarifopol excused himself, and recommended the Jewish intellectual Heimann Hariton Tiktin for that position. As he noted, Tiktin was more qualified, despite being targeted by a \"rather violent antisemitic current\". Instead, Zarifopol left for Germany to specialize in philology (under Hermann Suchier), and philosophy, as a student of Alois Riehl's objectivist worldview. He took a doctorate at the University of Halle in 1904, with a dissertation on trouvère Richard de Fournival.\n\nOn April 25, 1903, at Berlin, Zarifopol married Ștefania (Fany) Dobrogeanu-Gherea, the daughter of Constantin Dobrogeanu-Gherea. Zarifopol was an atheist; Fany was Jewish, and, like Zarifopol, a religious nonconformist. The couple turned up at the marriage registrar in street clothes and neither held a church wedding nor baptized their children; all this outraged his mother, a strict Romanian Orthodox. Dobrogeanu-Gherea, a literary critic, and his son Alexandru were also Marxist doctrinaires; Alexandru's daughter, also called Fany, was married to Ion Luca Caragiale's son Luca (Luki). While visiting his in-laws, Zarifopol met various figures of international socialism, including revolutionist Karl Radek.\n\nThe Zarifopols had two children: daughter Sonia (born 1904), and son Paul (born 1905). Selling their Cârligi home in 1906, the new family settled in Leipzig, where Paul developed a friendship with Caragiale the elder, who frequently visited from Berlin; with Dimitrie Gusti and Panait Cerna, he is described by Caragiale biographers as the comedic writer's closest Romanian friends in old age, or even, with an affinity in \"sneering spirits\", as his one true friend. The relationship proved decisive in Zarifopol's evolution as a critic, but also left a trace on Caragiale's style: Zarifopol introduced him to the work of Anatole France. Until Caragiale's death in 1912, he and Zarifopol pursued a steady correspondence. Sometimes involving kitsch postcards they each collected for their involuntary humor, such exchanges were noted for witticisms and ridicule of traditional writers. They also shed light on Caragiale's intellectual, psychological, and artistic evolution, making Zarifopol a prime reference in that field.\nFrom 1908 to 1911, Zarifopol contributed to the Munich-based \"Süddeutsche Monatshefte\". Also a correspondent for the Romanian literary press, his writing broke with the older canons of Romanian literary criticism and brought him to the attention of Dobrogeanu-Gherea disciple Garabet Ibrăileanu. Co-opted by the latter to write for \"Viața Românească\", Zarifopol made himself known for sarcastic comments about modernist literature, describing Proust, Gide, and Cocteau as difficult \"boys and children\". Instead, he persuaded Ibrăileanu to publish there a novel written in collaborations by two of his brothers-in-law, Luki and Ionel Gherea. Together with the latter, he also supervised the debut of a female novelist, Lucia Demetrius.\n\nIn 1915, Zarifopol and his family returned to Romania. By then, World War I had broken out, with Romania settled in uneasy neutrality until 1916. For much of that interval, Zarifopol was a literary contributor for Tudor Arghezi's \"Cronica\", but did not necessarily share the magazine's radical stances, nor its anti-war \"Germanophilia\". During the campaigns of 1916–1917, Zarifopol and his family remained in Iași; all of southern Romania fell to the Germans. For a while, accepting Ibrăileanu's offer, he returned to his alma mater as a substitute professor, replacing C. Fedeleș (presumed missing in action). Upon Fedeleș's unexpected return, Zarifopol was sacked. In late 1916, the family took refuge in Russia, settling for a while in Moscow, but were driven out by the October Revolution.\n\nDescribed by historian Lucian Nastasă as a withdrawn, \"neurasthenic\" and \"very impractical\" person, Zarifopol was financially ruined in the monetary devaluation that took place after the war. He supported himself by turning to regular journalism, but still had trouble making ends meet (though he did not admit to it), and made efforts to keep away from the centers of culture, living mostly in provincial Sinaia.\n\nZarifopol, sometimes using the pen names \"PZ\" and \"Anton Gherman\", returned as one of the main columnists at \"Viața Românească\" and its satellite, \"Adevărul Literar și Artistic\". Despite his material difficulties, Zarifopol categorically refused Ibrăileanu and Petre Andrei's offers to take a professorate at Iași. In early 1920, he complained that the \"endless infirmities of my children, my wife, and myself\" prevented him from publishing his cultural journalism as an academic volume, which would have qualified him for the office. In 1924, Zarifopol informed his protectors that he now had \"a holy terror of officialdom\", and that he resented Iași for its support for the National-Christian Defense League, a form of \"nationalist imbecility and charlatanry\". As he noted: \"all things considered, I can make a living from journalism alone.\"\n\nA guest writer at Camil Petrescu's \"Săptămâna Intelectuală și Artistică\" in 1924, and, in 1925, at \"Cuvântul Liber\", Zarifopol became more deeply involved in the cultural debates of Greater Romania. This was the time of his explicit emancipation from Dobrogeanu-Gherea's left-wing didacticism and from Ibrăileanu's Poporanism. Didactic and social art, Zarifopol contended, had no real artistic value, and politics were irrelevant in assessing the quality of artistic endeavor. Revising some of his earlier pronouncements on modernism, he now believed in art for art's sake, illustrated by such qualities as \"pleasure\", \"amusement\", \"life\", \"drama\", \"color\", \"strange sensibilities\", or \"childish delirium\". His refusal of didactic art aimed higher, showing up in his celebrated essay on Tolstoy's \"Kreutzer Sonata\". Working from Sinaia, Zarifopol translated and prefaced an anthology of fantasy short stories, published in 1924 as \"Vedenii\" (\"Visions\"). Part of his essays were published as \"Din registrul ideilor gingașe\" (\"A Register of Tender Ideas\", 1926), \"Despre stil\" (\"On Style\", 1928), \"Artiști și idei literare române\" (\"Artists and Ideas in Romanian Literature\", 1930), and \"Încercări de precizie literară\" (\"Essays in Literary Precision\", 1931).\n\nZarifopol's dissidence was admonished by other \"Viața Românească\" veterans Ibrăileanu and Mihai Ralea. Ibrăileanu took in some of Zarifopol's criticism, but argued that some of Poporanism's theories were rehabilitated in psychologism or social determinism, without which \"an artwork can never hope to be fully understood\". The magazine's literary columnists even accused Zarifopol of committing a \"crime\" against taste at \"Adevărul Literar și Artistic\", where Zarifopol was introducing aestheticist guidelines, albeit with contributions that remained \"interesting and profound\". Nonetheless, Zarifopol's derision of traditionalism and mysticism showed lasting similarities with Ralea's philosophical stances. In 1928, Ralea, Zarifopol, D. I. Suchianu, Felix Aderca and other literati were lumped together as the \"irresponsible malcontents\", in a neotraditionalist pamphlet put out by Petre Pandrea and \"Gândirea\" magazine.\n\nIn his replies, Zarifopol noted that he did not reject mysticism as a cultural phenomenon, only objecting to the \"career mysticism\" of traditionalist ideologues. Contested by young and old critics alike, Zarifopol found himself a follower with Mihail Sebastian, who honored him as \"a lucid man in a time of visionaries\", \"a sober teetotaler during a raging drinking bout\". Zarifopol's cultural role, Sebastian wrote, was that of a \"policeman\", a \"reactionary\", who would curb the excesses of mysticism, \"Trăirism\", and nationalism.\n\nZarifopol eventually moved to Bucharest in 1928, taking up residence on Strada Spătarului, Moșilor. Slowly discarding social and literary criticism in favor of philology, he worked on a critical edition of Caragiale's works; he put out the first three volumes (1930, 1931, 1932), winning a prize from the Romanian Writers' Society. In March 1929, together with Gala Galaction, Nicolae L. Lupu and others, he put out the monthly magazine \"Hanul Samariteanului\", which only published a single issue. Later that year, he became a writer for Isac Ludo's mainly Jewish review, \"Adam\", and, in \"Viața Românească\", published his review of Immanuel Kant's aesthetics (\"Kant și estetica\"). His other contributions appeared in various new magazines and newspapers, including \"Adevărul\" (which in 1927 hosted his humorous memoir of a meeting with Radek), \"Dreptatea\", \"Kalende\", \"Lamura\", \"Gazeta Fălticenilor\", and \"Ancheta\".\n\nHe carried on with his lampoons of traditionalism, publishing, in 1932, an especially mordant portrait of historian Vasile Pârvan, \"Plicticoase fantome\" (\"Tedious Apparitions\"). He was also involved with \"Criterion\", a debate club for political and cultural factions, one of the \"old men\" who were called upon as both arbiters and active participants. He and Ralea were thus present when the debates about Gide, Lenin, Sigmund Freud, and Charlie Chaplin either degenerated into squabbles or were broken up by the far-right Defense League.\n\nIn 1933, Zarifopol was named editor-in-chief of \"Revista Fundațiilor Regale\", the official literary magazine, which was largely conceived by him. In February 1934, he was involved with \"Convorbiri Literare\" in round-table talks that were supposed to revive \"Criterion\". These projects ended abruptly when Zarifopol died of a heart attack on May 1, at 12 AM, allegedly while visiting his mistress, musicologist Lisette Georgescu. His body was cremated on May 3, with Camil Petrescu succeeding him at \"Revista Fundațiilor Regale\".\n\nHis most important work as a literary critic appeared posthumously in 1934; called \"Pentru arta literară\" (\"For Literary Art\"), it was praised by Sebastian as \"a model of precise understanding of values and of their order\". In 1935, Șerban Cioculescu put out a critical edition of the Caragiale–Zarifopol correspondence, while editing new volumes from the Caragiale corpus, which Zarifopol had begun publishing.\n\nOnce settled in his role as art for art's sake advocate, Zarifopol created himself a confrontational niche, earning both respect and bewilderment from his readers. A traditionalist adversary, Nicolae Iorga, recognized Zarifopol as a \"refined and daring thinker\", while his \"Viața Românească\" partner Ralea called him \"charming and irritating\". Ralea identified in him \"a freethinker\" with \"the courage of looking truth in the face\", but essentially a \"freezing intelligence\" of \"destructive anarchism\", a man \"alone within his sarcasm\". A more virulent review came from classicist George Călinescu, who proposed that Zarifopol's one original note was \"continuous and systematic persiflage, to the point of annoyance\". He attributed such traits to Zarifopol's familiarity with \"two sophistic races\", Greeks and Jews, his claim in turn criticized by Ralea and philosopher Mircea Florian for its racialist undertones. Florian also discussed the constructive side of Zarifopol's work, arguing that accusations of \"bourgeois anarchism\" and \"iconoclasm\" were prejudiced. Another sympathetic reviewer, Andreea Grinea Mironescu, sees Călinescu's pronouncements as \"minimizing and petty.\"\n\nEugen Lovinescu, the modernist literary theorist, shared Zarifopol's overall aesthetic goals, but not his methods: Zarifopol, he writes, was an unlikely follower of Titu Maiorescu's non-didactic school of \"aesthetic autonomy\" and authenticity, which had emerged at \"Junimea\" in the 1860s, and had also influenced Caragiale. In 1941, Nicolae Bagdasar identified in Zarifopol \"a vastly cultured critic, of a rare subtlety and fine irony\", lamenting that his work \"remains scattered in so many magazines\". However, as argued by colleague Pompiliu Constantinescu, this improvidence was a fundamental trait and shortcoming of Zarifopol's literary contribution: his was a \"newspaperman's critique\" of \"spontaneous impression, quick analysis, and incomplete assessment, meaning that he could never embrace a creator in all their complexity.\" The same was noted by Călinescu, who argues that Zarifopol's \"journalistic method\" relied on explicit appeals to popularity and spurious accuracy.\n\nLovinescu assessed that Zarifopol's criticism \"lingers in the paradox\", always placing itself \"at the antipodes of common sense\": denying merit to prestigious figures such as Renan or Maupassant, but praising Ion Minulescu as an outstanding novelist. His \"practical\" verdicts, Lovinescu notes, remain \"disoriented\", \"regrettable\". He acknowledges Zarifopol's \"stylistic rigor\", but concludes that his is \"an equation in which the unknown is better left unknown\", something \"virtually alien to the rhythms of our literary movement.\" The same was argued decades later by Lovinescu disciple Nicolae Manolescu.\n\nAccording to Ralea, Zarifopol should be read as a Romanian counterpart of anti-populist \"lone travelers\", from Barbey d'Aurevilly and Edgar Allan Poe to Hanns Heinz Ewers, often applauding causes that were \"at odds with the establishment\". Thus, responding to the established literary canons, Zarifopol fashioned himself an alternative one, comprising Caragiale, Minulescu, Proust and Cocteau, but also Joseph Delteil, Henri de Régnier, Adrian Maniu, Păstorel Teodoreanu, and Dragoș Protopopescu. Călinescu believes that Zarifopol was most \"intelligent\" in his essays on Proust and Gustave Flaubert, where he overcame his usual \"journalistic banality\".\n\nZarifopol's rejection of neoclassicism, from Goethe to Dimitrie Bolintineanu, but also his friend Panait Cerna, had to do with both its \"mechanic\" use of poetic imagery and its communication of \"bland truths\". He preferred archaic Moldavian forms, that he found resonating in the poetry of Dosoftei and Vasile Alecsandri. Călinescu was especially critical of Zarifopol literal and \"negativist\" reading of Alexandru Vlahuță's \"Din prag\", which ridiculed the poet's presentment of death eternal.\n\nSuch a sustained attack, equated by comparatist Nicolae Balotă with \"a holocaust of poetry\", nevertheless impressed Lovinescu. The latter declared himself in agreement with the thesis that ancient models were perishable, and that traditionalist art was implicitly false. Seen by Alexandru Paleologu as a more radical anti-classicist than Zarifopol ever was, Lovinescu noted in 1943: \"as modernism or synchronism, I have been supporting those same ideas these past twenty years.\" Zarifopol's revolt was more contextual, and bound by his own debt to the classics: Ralea sees him as a classical rationalist in the manner of Voltaire, Sainte-Beuve, and Anatole France, and Călinescu as a \"cultured academic, for all his freethinking airs\", copying his style from Caragiale and Tudor Arghezi, without \"a sense of the sublime\". Balotă also finds Zarifopol a \"suppressed scholar\", \"in denial of his formative background\".\n\nCălinescu sees Zarifopol a man of little classical culture, his \"obviously German method\" being more akin to the sociology of literature. Such verdicts were nuanced by another literary historian, Alexandru Dima, who suggests that Zarifopol did have contribution to the scholarly study of aesthetics, one which \"imposes itself even against his own wish.\" Zarifopol's rejection of scientism and historicism had deep roots in \"Junimism\" and Neo-Kantianism, but Zarifopol also criticized Kantian assumptions about the sublime, finding them too indebted to ethical imperatives. As noted by Dima, his attachment to phenomenology was \"at the very least formal\". According to Balotă, his applications of the art for art's sake principle show that, despite his own claims to the contrary, Zarifopol borrowed his poetics from Henri Brémond, Paul Valéry, and Stéphane Mallarmé.\n\nRevisiting Zarifopol's moralizing essays in 2007, critic Henri Zalis found him to be \"chaste and meticulous, and in this unmatched.\" \"Din registrul ideilor gingașe\", is, according to Lovinescu, an \"interesting intellectual spectacle of deliberate originality\". This is one of several essays containing Zarifopol's \"Junimea\"-like satire of inauthentic \"mofturi\" (\"trifles\" or \"coquetries\", a term echoing Caragiale), including wholesale borrowings of foreign customs that respond to bourgeois tastes. The eponymous \"tender ideas\" were defined by Zarifopol as \"those which should be familiar to any man wishing to pass for cultured, and also those that said man must be careful to speak about just so, lest he insult the views of society\"; the book was addressed to those skeptics who, rejecting cultural crazes, \"still value consistency\". As Ralea notes, Zarifopol's anti-ideological critique, continuing the work of Caragiale, was specifically deriding philosophers and philosophies that were the height of fashion: mystical, Bergsonian, Japonist; other such hobbyhorses were Nietzscheism and psychoanalysis.\n\nCălinescu was unimpressed by Zarifopol's \"rather belated\" satire of bourgeois mores, since \"the bourgeois is no longer that ridiculous conservative figure\". Yet, Zarifopol was not entirely anti-middle class: he believed his type of \"cold lucidity\" was primarily an in-built antidote to the decay of the \"colossal civilization\" that was liberal society. He issued what Florian calls a \"passionate call to order of the bourgeoisie, [which is] still a bearer of cultural values.\" Himself a conservative, Zarifopol expressed his nostalgia for old-regime social differentiation and division of labor, against \"the political type\", and for the nuclear family of patriarchy, against the \"neutralization\" of fathers in modern society. He criticizes both Francophilia and Germanophilia, noting that, although competing, they each supported deindividuation: the former, through corporatism; the latter, through militarism. While targeting philistinism, \"Din registrul...\" is itself an antiintellectualist manifesto. Zarifopol contended that intellectuals were an illusory social class (no economic interest bound together \"a lawyer with a novelist\"), but still collectively responsible for the failures of a society such as Romania's.\n\nZarifopol took distance from more radical antiintelectualist stances, communist as well as Christian; but also noted that natural disunity between intellectuals meant that communist terrorism was itself an intellectuals' affair. Both \"Din registrul...\" an other writings show him as an anti-Soviet in line with Nikolai Berdyaev, and believed that Leninism was a somewhat worrisome, but generally puerile, non-philosophy. Like fascism and, historically, Bonapartism, it stood for a \"simplistic autocratic drive\" and \"stupefied blindness\". Moreover, Zarifopol rejected Marxist literary criticism, with its discourse of base and superstructure, seeing it as the source of modernist kitsch.\n\nIn 2014, posthumously reviewing Zarifiopol's anticommunist notes, scholar Vladimir Tismăneanu described him as a diagnostician of \"totalitarian reflexes\", displaying \"urbanity, civility, moderation and firmness\". Evidence also exists that, beyond this public persona, Zarifopol was more illiberal. Prolonging his antihumanist tendencies, he expressed in private his reserves for the \"deplorable\" literature of \"the oppressed\", including Jews, social climbers, and especially women; according to Nastasă, he was an antifeminist, and perhaps also a misogynist. In 1932, writer Barbu Brezianu suggested that Zarifopol was on the \"far-right\" of Romanian literature, in the \"grand conservative party\" of D. Nanu, Cincinat Pavelescu, Mihail Sadoveanu, and Al. T. Stamatiad.\n\nFany Zarifopol, born in 1876, lived until 1945. Paul Jr spent seventeen years as a political prisoner under the communist regime, being released with a general amnesty in 1964. Officially designating him a \"bourgeois idealist\", communist censorship prevented Zarifopol's essays from being either reprinted or quoted before the mid 1960s liberalization. When they were finally returned to circulation, large parts of his work were still being bracketed out.\n\nSonia Zarifopol, who never married, was a lover of literature and, in the 1930s, a discreet presence at Lovinescu's \"Sburătorul\" society. She kept her father's entire collection of manuscripts and documents, now housed at the Museum of Romanian Literature. She died in 1981, and her brother three years later; neither had children of their own. George Zarifopol's son, Constantin Radu \"Dinu\", was a published novelist. His daughter Ilinca, marginalized at home for her aristocratic descent, emigrated to the United States in 1977. A linguist and comparatist, she taught at Indiana University Bloomington, marrying Anglicist Kenneth R. Johnston. She was joined there by her sister Christina Zarifopol-Illias, who organized the Bloomington Romanian Studies Program.\n\nThe critical reappraisal of Zarifopol, begun by Eugen Simion in 1956, was taken up in the 1980s by Paleologu and Marin Mincu. In underground culture, his memory was cultivated by the essayist Nicolae Steinhardt. Although a fervent Orthodox, he viewed the non-believers Zarifopol and Ralea as intellectual standards, praising their \"quick wit\". From 1971, Al. Săndulescu was allowed to publish selections of Zarifopol's literary prose, and, in 1987, Zarifopol's correspondence (at Editura Minerva, with samples in \"Manuscriptum\" magazine). A more thorough recovery came after the Romanian Revolution of 1989, when more complete anthologies first emerged, and Zarifopol monographs were published by Alex. Cistelecan. In 1992, his formerly censored critique of socialism was issued by Editura Albatros as a standalone book, \"Marxism amuzant\" (\"Amusing Marxism\"). His Bucharest home was not fitted with a memorial plaque—the new owners having reportedly refused to allow it.\n\n"}
{"id": "1443820", "url": "https://en.wikipedia.org/wiki?curid=1443820", "title": "Penguin diagram", "text": "Penguin diagram\n\nIn quantum field theory, penguin diagrams are a class of Feynman diagrams which are important for understanding CP violating processes in the standard model. They refer to one-loop processes in which a quark temporarily changes flavor (via a W or Z loop), and the flavor-changed quark engages in some tree interaction, typically a strong one. For the interactions where some quark flavors (e.g. very heavy ones) have much higher interaction amplitudes than others, such as CP-violating or Higgs interactions, these penguin processes may have amplitudes comparable to or even greater than those of the direct tree processes. A similar diagram can be drawn for leptonic decays.\n\nThey were first isolated and studied by Mikhail Shifman, Arkady Vainshtein, and Valentin Zakharov. The processes which they describe were first directly observed in 1991 and 1994 by the CLEO collaboration.\n\nJohn Ellis was the first to refer to a certain class of Feynman diagrams as \"penguin diagrams\", due in part to their shape, and in part to a legendary bar-room bet with Melissa Franklin. According to John Ellis:\n"}
{"id": "14236625", "url": "https://en.wikipedia.org/wiki?curid=14236625", "title": "Personality systematics", "text": "Personality systematics\n\nPersonality systematics is a contribution to the psychology of personality and to psychotherapy summarized by Jeffrey J. Magnavita in 2006 and 2009. It is the study of the interrelationships among subsystems of personality as they are embedded in the entire ecological system. The model falls into the category of complex, biopsychosocial approaches to personality. The term personality systematics was originally coined by William Grant Dahlstrom in 1972.\n\nSystems psychology has emerged here as a new approach in which groups and individuals, are considered as systems in homeostasis. Within open systems they have an active method of remaining stable through the dynamic relationship between parts. A classic example of this homeostatic dynamic is the \"problem behavior\" of a bed wetting child having a stabilizing function of holding a troubled marriage together because the attention of the parents is drawn away from their conflict towards the \"problem\" child.\n\nMore recent developments in systems psychology have challenged this understanding of homeostasis as being too focused on causal understanding of systems. This change in thought from 1st order cybernetics to 2nd order cybernetics involved a postmodern shift in understanding of reality as objective to being socially and linguistically constructed.\n\nFamily systems therapy received an important boost in the mid-1950s through the work of anthropologist Gregory Bateson and colleagues – Jay Haley, Donald D. Jackson, John Weakland, William Fry, and later, Virginia Satir, Paul Watzlawick and others – at Palo Alto in the US, who introduced ideas from cybernetics and general systems theory into social psychology and psychotherapy, focusing in particular on the role of communication. This approach eschewed the traditional focus on individual psychology and historical factors – that involve so-called linear causation and content – and emphasized instead feedback and homeostatic mechanisms and “rules” in here-and-now interactions – so-called circular causation and process – that were thought to maintain or exacerbate problems, whatever the original cause(s).\n\nRelational psychoanalysis began in the 1980s as an attempt to integrate interpersonal psychoanalysis's emphasis on the detailed exploration of interpersonal interactions with British object relations theory's sophisticated ideas about the psychological importance of internalized relationships with other people. Relationalists argue that personality emerges from the matrix of early formative relationships with parents and other figures. Philosophically, relational psychoanalysis is closely allied with social constructionism.\n\nPersonality systematics seeks to establish the underlying processes within the domains of the larger system. The domains range from what can be observed from the microlevel to the macrolevel domains.\n\nPersonality can be conceptualized as an emergent phenomena of the convergent forces and their expressions which can be compared to a holographic representation. Personality systematics is based on a holistic model of functioning which considers part-whole relationships as being essential to understanding complex self-organizing systems.\n\nThe model describes four levels of the personality system:\n\nThis model has been used in treating complex clinical syndromes, personality dysfunction, and relational disturbances. To enhance efficacy and range, new methods are used, such as audiovisual recording and physiological measurements, galvanic skin response, heart rate variability, and other forms of bio- and neuro-feedback.\n\n"}
{"id": "6257801", "url": "https://en.wikipedia.org/wiki?curid=6257801", "title": "Power harassment", "text": "Power harassment\n\nPower harassment is harassment or unwelcome attention of a political nature, often occurring in the environment of a workplace including hospitals, schools and universities. It includes a range of behavior from mild irritation and annoyances to serious abuses which can even involve forced activity beyond the boundaries of the job description. Power harassment is considered a form of illegal discrimination and is a form of political and psychological abuse, and bullying.\n\nAlthough the phenomenon power harassment is not uniquely Japanese, since it has occurred in many environments, the term is a Japanese coinage. It has received significant attention in Japan in recent years as a workplace problem. For many Japanese, the only kind of management style they have ever experienced is one in which subordinates are treated harshly and no complaints are tolerated. This makes the nightmare boss seem normal, and those who have never had a different role model for management style find themselves falling back on the old methods. And indeed, many Japanese admire authoritarian bosses, finding them to be strong and admiring their \"passion\" that may be expressed in angry outbursts. The term itself was coined by Yasuko Okada in 2002 and is used mainly in Japan. It is analogous to \"abuse of authority\" or rankism in the workplace. Yuichiro Makiguchi is one of the researchers of power harassment at Tokoha Gakuen Junior College in Japan.\n\nThe topic of power harassment is known in South Korea as Gabjil, and it has been recently increasingly discussed in Korean media and scholarly works.\n\nMany workers are forced by their superiors to perform tasks outside of their job description and working hours. It is common for workers to be fired or suffer severe repercussions if they do not satisfy their superior's orders, despite there being no justifiable basis for such orders. \nSituations exist where employees are treated in a manner that far oversteps the bounds of what is proper between a boss and his or her workers. Someone in a position of power should never be allowed to exercise the power in a bullying or discriminatory fashion. This can create an unhappy and unsafe work environment not just for those being harassed but for the entire work force. \nTypical examples of power harassment include: \n\n"}
{"id": "51508407", "url": "https://en.wikipedia.org/wiki?curid=51508407", "title": "Pseudo-ring", "text": "Pseudo-ring\n\nIn mathematics, and more specifically in abstract algebra, a pseudo-ring is one of the following variants of a ring:\n\n\nNo two of these definitions are equivalent, so it is best to avoid the term \"pseudo-ring\" or to clarify which meaning is intended.\n\n"}
{"id": "4816797", "url": "https://en.wikipedia.org/wiki?curid=4816797", "title": "Race and society", "text": "Race and society\n\nSocial interpretations of race regard the common categorizations of people into different races, often with biologist tagging of particular \"racial\" attributes beyond mere anatomy, as more socially and culturally determined than based upon biology. Some interpretations are often deconstructionist and poststructuralist in that they critically analyze the historical construction and development of racial categories.\n\nMarks (1995) argued that even as the idea of \"race\" was becoming a powerful organizing principle in many societies, the shortcomings of the concept were apparent. In the Old World, the gradual transition in appearances from one racial group to adjacent racial groups emphasized that \"one variety of mankind does so sensibly pass into the other, that you cannot mark out the limits between them,\" as Blumenbach observed in his writings on human variation. In parts of the Americas, the situation was somewhat different. The immigrants to the New World came largely from widely separated regions of the Old World—western and northern Europe, western Africa, and, later, eastern Asia and southern and eastern Europe. In the Americas, the immigrant populations began to mix among themselves and with the indigenous inhabitants of the continent. In the United States, for example, most people who self-identify as African American have some European ancestors—in one analysis of genetic markers that have differing frequencies between continents, European ancestry ranged from an estimated 7% for a sample of Jamaicans to ∼23% for a sample of African Americans from New Orleans. In a survey of college students who self-identified as white in a northeastern U.S. university, the west African and Native American genetic contribution were 0.7% and 3.2%.\n\nIn the United States, social and legal conventions developed over time that forced individuals of mixed ancestry into simplified racial categories. An example is the \"one-drop rule\" implemented in some state laws that treated anyone with a single known African American ancestor as black. The decennial censuses conducted since 1790 in the United States also created an incentive to establish racial categories and fit people into those categories. In other countries in the Americas, where mixing among groups was more extensive, social non racial categories have tended to be more numerous and fluid, with people moving into or out of categories on the basis of a combination of socioeconomic status, social class, ancestry.\n\nEfforts to sort the increasingly mixed population of the United States into discrete racial categories generated many difficulties. Additionally, efforts to track mixing between census racial groups led to a proliferation of categories (such as mulatto and octoroon) and \"blood quantum\" distinctions that became increasingly untethered from self-reported ancestry. A person's racial identity can change over time. One study found differences between self-ascribed race and Veterans Affairs administrative data.\n\nThe notion of a biological basis for race originally emerged through speculations surrounding the \"blood purity\" of Jews during the Spanish Inquisition, eventually translating to a general association of one's biology with their social and personal characteristics. In the 19th century, this recurring ideology was intensified in the development of the racial sciences, eugenics and ethnology, which meant to further categorize groups of humans in terms of biological superiority or inferiority. While the field of racial sciences, also known as scientific racism, has expired in history, these antiquated conceptions of race have persisted throughout the 21st century. (See also: Historical origins of racial classification)\n\nContrary to popular belief that the division of the human species based on physical variations is natural, there exists no clear, reliable distinctions that bind people to such groupings. According to the American Anthropological Association, \"Evidence from the analysis of genetics (e.g., DNA) indicates that most physical variation, about 94%, lies within so-called racial groups. Conventional geographic \"racial\" groupings differ from one another only in about 6% of their genes.\" While there is a biological basis for differences in human phenotypes, most notably in skin color, the genetic variability of humans is found not amongst, but rather within racial groups – meaning the perceived level of dissimilarity amongst the species has virtually no biological basis. Genetic diversity has characterized human survival, rendering the idea of a \"pure\" ancestry as obsolete. Under this interpretation, race is conceptualized through a lens of artificiality, rather than through the skeleton of a scientific discovery. As a result, scholars have begun to broaden discourses of race by defining it as a social construct and exploring the historical contexts that led to its inception and persistence in contemporary society.\n\nMost historians, anthropologists, and sociologists describe human races as a social construct, preferring instead the term \"population\" or \"ancestry\", which can be given a clear operational definition. Even those who reject the formal concept of race, however, still use the word \"race\" in day-to-day speech. This may either be a matter of semantics, or an effect of an underlying cultural significance of race in racist societies. Regardless of the name, a working concept of sub-species grouping can be useful, because in the absence of cheap and widespread genetic tests, various race-linked gene mutations (see Cystic fibrosis, Lactose intolerance, Tay–Sachs disease and Sickle cell anemia) are difficult to address without recourse to a category between \"individual\" and \"species\". As genetic tests for such conditions become cheaper, and as detailed haplotype maps and SNP databases become available, identifiers of race should diminish. Also, increasing interracial marriage is reducing the predictive power of race. For example, babies born with Tay-Sachs disease in North America are not only or primarily Ashkenazi Jews, despite stereotypes to contrary; French Canadians, Louisiana Cajuns, and Irish-Americans also see high rates of the disease.\n\nExperts in the fields of genetics, law, and sociology have offered their opinions on the subject. Audrey Smedley and Brian D. Smedley of Virginia Commonwealth University Institute of Medicine discuss the anthropological and historical perspectives on ethnicity, culture, and race. They define culture as the habits acquired by a society. Smedley states \"Ethnicity and culture are related phenomena and bear no intrinsic connection to human biological variations or race\" (Smedley 17). The authors state using physical characteristics to define an ethnic identity is inaccurate. The variation of humans has actually decreased over time since, as the author states, \"Immigration, intermating, intermarriage, and reproduction have led to increasing physical heterogeneity of peoples in many areas of the world\" (Smedley 18). They referred to other experts and their research, pointing out that humans are 99% alike. That one percent is caused by natural genetic variation, and has nothing to do with the ethnic group of the subject. Racial classification in the United States started in the 1700s with three ethnically distinct groups. These groups were the white Europeans, Native Americans, and Africans. The concept of race was skewed around these times because of the social implications of belonging to one group or another. The view that one race is biologically different from another rose out of society's grasp for power and authority over other ethnic groups. This did not only happen in the United States but around the world as well. Society created race to create hierarchies in which the majority would prosper most.\nAnother group of experts in sociology has written on this topic. Guang Guo, Yilan Fu, Yi Li, Kathleen Mullan Harris of the University of North Carolina department of sociology as well as Hedwig Lee (University of Washington Seattle), Tianji Cai (University of Macau) comment on remarks made by one expert. The debate is over DNA differences, or lack thereof, between different races. The research in the original article they are referring to uses different methods of DNA testing between distinct ethnic groups and compares them to other groups. Small differences were found, but those were not based on race. They were from biological differences caused from the region in which the people live. They describe that the small differences cannot be fully explained because the understanding of migration, intermarriage, and ancestry is unreliable at the individual level. Race cannot be related to ancestry based on the research on which they are commenting. They conclude that the idea of \"races as biologically distinct peoples with differential abilities and behaviors has long been discredited by the scientific community\" (2338).\n\nOne more expert in the field has given her opinion. Ann Morning of the New York University Department of Sociology, and member of the American Sociological Association, discusses the role of biology in the social construction of race. She examines the relationship between genes and race and the social construction of social race clusters. Morning states that everyone is assigned to a racial group because of their physical characteristics. She identifies through her research the existence of DNA population clusters. She states that society would want to characterize these clusters as races. Society characterizes race as a set of physical characteristics. The clusters though have an overlap in physical characteristics and thus cannot be counted as a race by society or by science. Morning concludes that \"Not only can constructivist theory accommodate or explain the occasional alignment of social classifications and genetic estimates that Shiao et al.'s model hypothesizes, but empirical research on human genetics is far from claiming—let alone demonstrating—that statistically inferred clusters are the equivalent of races\" (Morning 203). Only using ethnic groups to map a genome is entirely inaccurate, instead every individual must be viewed as having their own wholly unique genome (unique in the 1%, not the 99% all humans share).\n\nIan Haney López, the John H. Boalt Professor of Law at the University of California, Berkeley explains ways race is a social construct. He uses examples from history of how race was socially constructed and interpreted. One such example was of the Hudgins v. Wright case. A slave woman sued for her freedom and the freedom of her two children on the basis that her grandmother was Native American. The race of the Wright had to be socially proven, and neither side could present enough evidence. Since the slave owner Hudgins bore the burden of proof, Wright and her children gained their freedom. López uses this example to show the power of race in society. Human fate, he argues, still depends upon ancestry and appearance. Race is a powerful force in everyday life. These races are not determined by biology though, they are created by society to keep power with the majority. He describes that there are not any genetic characteristics that all blacks have that non-whites do not possess and vice versa. He uses the example of Mexican. It truly is a nationality, yet it has become a catch-all for all Hispanic nationalities. This simplification is wrong, López argues, for it is not only inaccurate but it tends to treat all \"Mexicans\" as below fervent Americans. He describes that \"More recently, genetic testing has made it clear the close connections all humans share, as well as the futility of explaining those differences that do exist in terms of racially relevant gene codes\" (Lopez 199–200). Those differences clearly have no basis in ethnicity, so race is completely socially constructed.\n\nThrough this small sampling of experts, it is clear that race as a social construction is a common theory. All of the experts in this sampling say that biological race is non-existent. Race therefore must have been created by societies. They were created to do what humans do, to serve the purposes of the majority. The hierarchies created by race have kept the majority \"race\" in control of everything from public policy to the workforce to law enforcement. They benefit from this construction of race. Yet, the minorities, who are just the same genetically, suffer under this system. Most of the points made by the experts expose this issue, yet none truly suggest a way to fix the problem. Bill Nye weighs in on the issue on the same side as the experts in the sample. He says that humans are humans, we are all one species. We have to fix it. If society created the problem, society has to take it on itself to fix it.\n\nSome argue it is preferable when considering biological relations to think in terms of populations, and when considering cultural relations to think in terms of ethnicity, rather than of race.\n\nThese developments had important consequences. For example, some scientists developed the notion of \"population\" to take the place of race. It is argued that this substitution is not simply a matter of exchanging one word for another.\n\nThis view does not deny that there are physical differences among peoples; it simply claims that the historical conceptions of \"race\" are not particularly useful in accounting for these differences scientifically. In particular, it is claimed that:\n\n\nNeven Sesardic has argued that such arguments are unsupported by empirical evidence and politically motivated. Arguing that races are not completely discrete biologically is a straw man argument. He argues \"racial recognition is not actually based on a single trait (like skin color) but rather on a number of characteristics that are to a certain extent concordant and that jointly make the classification not only possible but fairly reliable as well\". Forensic anthropologists can classify a person's race with an accuracy close to 100% using only skeletal remains if they take into consideration several characteristics at the same time. A.W.F. Edwards has argued similarly regarding genetic differences in \"\".\n\nResearchers have reported significant differences in the average IQ test scores of various racial groups. The interpretation and causes of these differences are controversial, as researchers disagree about whether this gap is caused by genetic differences. The social interpretations of the race concept is incompatible with the idea that the IQ gap between racial groups is caused by genetic factors, and those who see race as a social construction posit purely environmental and sociological explanations for the gap. Such explanations include different access to education for different racial groups, different social attitudes towards test-taking, stereotype threat, lack of effort optimism due to low social status and many other proposed explanations. For example, psychologist Jefferson Fish argues that race is a social construction and argues that for this reason the question of racial differences in intelligence is not scientific, though his opinion has been repeatedly disproven. For example, one might want to compare black-white IQ differences in Brazil with those in the United States. Since many people who are considered black in the U. S. would not be considered black in Brazil, and since many people who are considered white in Brazil would not be considered white in the U. S., such a comparison is not possible. \n\nRichard Lynn, however, in his book \"Race Differences in Intelligence\" does not define races based on current social classification but on ancestral populations. Many current ethnic groups would be mixtures of several races in this classification. Arthur Jensen and J. Philippe Rushton have also defined race based on ancestral home, although somewhat differently from Lynn, when speaking of Black–White–East Asian IQ differences in the US. \"Blacks (Africans, Negroids) are those who have most of their ancestors from sub-Saharan Africa; Whites (Europeans, Caucasoids) have most of their ancestors from Europe; and East Asians (Orientals, Mongoloids) have most of their ancestors from Pacific Rim countries.\"\n\nThere is an active debate among biomedical researchers about the meaning and importance of race in their research. The primary impetus for considering race in biomedical research is the possibility of improving the prevention and treatment of diseases by predicting hard-to-ascertain factors on the basis of more easily ascertained characteristics. The most well-known examples of genetically determined disorders that vary in incidence between ethnic groups would be sickle cell disease and thalassemia among black and Mediterranean populations respectively and Tay–Sachs disease among people of Ashkenazi Jewish descent. Some fear that the use of racial labels in biomedical research runs the risk of unintentionally exacerbating health disparities, so they suggest alternatives to the use of racial taxonomies.\n\nIn the United States since its early history, Native Americans, African-Americans and European-Americans were classified as belonging to different races. For nearly three centuries, the criteria for membership in these groups were similar, comprising a person's appearance, his fraction of known non-White ancestry, and his social circle. But the criteria for membership in these races diverged in the late 19th century. During Reconstruction, increasing numbers of Americans began to consider anyone with \"one drop\" of \"Black blood\" to be Black. By the early 20th century, this notion of invisible blackness was made statutory in many states and widely adopted nationwide. In contrast, Amerindians continue to be defined by a certain percentage of \"Indian blood\" (called \"blood quantum\") due in large part to American slavery ethics.\n\nThe concept of race as used by the Census Bureau reflects self-identification by people according to the race or races with which they most closely identify. These categories are sociopolitical constructs and should not be interpreted as being scientific or anthropological in nature. They change from one census to another, and the racial categories include both racial and national-origin groups.\n\nCompared to 19th-century United States, 20th-century Brazil was characterized by a relative absence of sharply defined racial groups. This pattern reflects a different history and different social relations. Basically, race in Brazil was recognized as the difference between ancestry (which determines genotype) and phenotypic differences. Racial identity was not governed by a rigid descent rule. A Brazilian child was never automatically identified with the racial type of one or both parents, nor were there only two categories to choose from. Over a dozen racial categories are recognized in conformity with the combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and no one category stands significantly isolated from the rest. That is, race referred to appearance, not heredity.\n\nThrough this system of racial identification, parents and children and even brothers and sisters were frequently accepted as representatives of opposite racial types. In a fishing village in the state of Bahia, an investigator showed 100 people pictures of three sisters and they were asked to identify the races of each. In only six responses were the sisters identified by the same racial term. Fourteen responses used a different term for each sister. In another experiment nine portraits were shown to a hundred people. Forty different racial types were elicited. It was found, in addition, that a given Brazilian might be called by as many as thirteen different terms by other members of the community. These terms are spread out across practically the entire spectrum of theoretical racial types. A further consequence of the absence of a descent rule was that Brazilians apparently not only disagreed about the racial identity of specific individuals, but they also seemed to be in disagreement about the abstract meaning of the racial terms as defined by words and phrases. For example, 40% of a sample ranked moreno claro as a lighter type than mulato claro, while 60% reversed this order. A further note of confusion is that one person might employ different racial terms to describe the same person over a short time span. The choice of which racial description to use may vary according to both the personal relationships and moods of the individuals involved. The Brazilian census lists one's race according to the preference of the person being interviewed. As a consequence, hundreds of races appeared in the census results, ranging from blue (which is blacker than the usual black) to pink (which is whiter than the usual white).\n\nHowever, Brazilians are not so naïve to ignore one's racial origins just because of his (or her) better social status. An interesting example of this phenomenon has occurred recently, when the famous football (soccer) player Ronaldo declared publicly that he considered himself as White, thus linking racism to a form or another of class conflict. This caused a series of ironic notes on newspapers, which pointed out that he should have been proud of his African origin (which is obviously noticeable), a fact that must have made life for him (and for his ancestors) more difficult, so, being a successful personality was, in spite of that, a victory for him. What occurs in Brazil that differentiates it largely from the US or South Africa, for example, is that black or mixed-race people are, in fact, more accepted in social circles if they have more education, or have a successful life (a euphemism for \"having a better salary\"). As a consequence, inter-racial marriages are more common, and more accepted, among highly educated Afro-Brazilians than lower-educated ones.\n\nSo, although the identification of a person by race is far more fluid and flexible in Brazil than in the U.S., there still are racial stereotypes and prejudices. African features have been considered less desirable; Blacks have been considered socially inferior, and Whites superior. These white supremacist values were a legacy of European colonization and the slave-based plantation system. The complexity of racial classifications in Brazil is reflective of the extent of miscegenation in Brazilian society, which remains highly, but not strictly, stratified along color lines. Henceforth, Brazil's desired image as a perfect \"post-racist\" country, composed of the \"cosmic race\" celebrated in 1925 by José Vasconcelos, must be met with caution, as sociologist Gilberto Freyre demonstrated in 1933 in \"Casa Grande e Senzala\".\n\nMichel Foucault argued the popular historical and political use of a non-essentialist notion of \"race\" used in the \"race struggle\" discourse during the 1688 Glorious Revolution and under Louis XIV's end of reign. In Foucault's view, this discourse developed in two different directions: Marxism, which seized the notion and transformed it into \"class struggle\" discourse, and racists, biologists and eugenicists, who paved the way for 20th century \"state racism\".\n\nDuring the Enlightenment, racial classifications were used to justify enslavement of those deemed to be of \"inferior\", non-White races, and thus supposedly best fitted for lives of toil under White supervision. These classifications made the distance between races seem nearly as broad as that between species, easing unsettling questions about the appropriateness of such treatment of humans. The practice was at the time generally accepted by both scientific and lay communities.\n\nArthur Gobineau's \"An Essay on the Inequality of the Human Races\" (1853–1855) was one of the milestones in the new racist discourse, along with Vacher de Lapouge's \"anthroposociology\" and Johann Gottfried Herder (1744–1803), who applied race to nationalist theory to develop militant ethnic nationalism. They posited the historical existence of national races such as German and French, branching from basal races supposed to have existed for millennia, such as the Aryan race, and believed political boundaries should mirror these supposed racial ones.\n\nLater, one of Hitler's favorite sayings was, \"Politics is applied biology\". Hitler's ideas of racial purity led to unprecedented atrocities in Europe. Since then, ethnic cleansing has occurred in Cambodia, the Balkans, Sudan, and Rwanda. In one sense, \"ethnic cleansing\" is another name for the tribal warfare and mass murder that has afflicted human society for ages.\n\nRacial inequality has been a concern of United States politicians and legislators since the country's founding. In the 19th century most White Americans (including abolitionists) explained racial inequality as an inevitable consequence of biological differences. Since the mid-20th century, political and civic leaders as well as scientists have debated to what extent racial inequality is cultural in origin. Some argue that current inequalities between Blacks and Whites are primarily cultural and historical, the result of past and present racism, slavery and segregation, and could be redressed through such programs as affirmative action and Head Start. Others work to reduce tax funding of remedial programs for minorities. They have based their advocacy on aptitude test data that, according to them, shows that racial ability differences are biological in origin and cannot be leveled even by intensive educational efforts. In electoral politics, many more ethnic minorities have won important offices in Western nations than in earlier times, although the highest offices tend to remain in the hands of Whites.\n\nIn his famous \"Letter from Birmingham Jail\", Martin Luther King Jr. observed:\n\nKing's hope, expressed in his I Have a Dream speech, was that the civil rights struggle would one day produce a society where people were not \"judged by the color of their skin, but by the content of their character\".\n\nBecause of the identification of the concept of race with political oppression, many natural and social scientists today are wary of using the word \"race\" to refer to human variation, but instead use less emotive words such as \"population\" and \"ethnicity\". Some, however, argue that the concept of race, whatever the term used, is nevertheless of continuing utility and validity in scientific research. Science and politics frequently take opposite sides in debates that relate to human intelligence and biomedicine.\n\nIn an attempt to provide general descriptions that may facilitate the job of law enforcement officers seeking to apprehend suspects, the United States FBI employs the term \"race\" to summarize the general appearance (skin color, hair texture, eye shape, and other such easily noticed characteristics) of individuals whom they are attempting to apprehend. From the perspective of law enforcement officers, a description needs to capture the features that stand out most clearly in the perception within the given society.\n\nThus, in the UK, Scotland Yard use a classification based on the ethnic composition of British society: W1 (White British), W2 (White Irish), W9 (Other White); M1 (White and black Caribbean), M2 (White and black African), M3 (White and Asian), M9 (Any other mixed background); A1 (Asian-Indian), A2 (Asian-Pakistani), A3 (Asian-Bangladeshi), A9 (Any other Asian background); B1 (Black Caribbean), B2 (Black African), B3 (Any other black background); O1 (Chinese), O9 (Any other).\n\nIn the United States, the practice of racial profiling has been ruled to be both unconstitutional and also to constitute a violation of civil rights. There also an ongoing debate on the relationship between race and crime regarding the disproportional representation of certain minorities in all stages of the criminal justice system.\n\nStudies in racial taxonomy based on DNA cluster analysis (See Lewontin's Fallacy) has led law enforcement to pursue suspects based on their racial classification as derived from their DNA evidence left at the crime scene. DNA analysis has been successful in helping police determine the race of both victims and perpetrators.\nThis classification is called \"biogeographical ancestry\".\n\n"}
{"id": "33112023", "url": "https://en.wikipedia.org/wiki?curid=33112023", "title": "Rationes seminales", "text": "Rationes seminales\n\nRationes seminales (Latin, from the Greek \"λόγοι σπερματικοὶ\" or \"logoi spermatikoi\"), translated variously as germinal or causal principles, primordial reasons, original factors, seminal reasons or virtues, or seedlike principles, is a theological theory on the origin of species. It is the doctrine that God created the world in seed form, with certain potentialities, which then developed or unfolded accordingly over time; what appears to be change is simply the realization of the preexisting potentialities. The theory is a metaphor of the growth of a plant: much like a planted seed eventually develops into a tree, so when God created the world he planted \"rationes seminales\", from which all life sprung. It is intended to reconcile the belief that God created all things, with the evident fact that new things are constantly developing.\n\nThe roots of this idea can be found within the Greek philosophy of the Stoics and Neoplatonism\nThe idea was incorporated into Christian thought through the writings of authors such as Athenagoras of Athens, Tertullian, Gregory of Nyssa, Augustine of Hippo, Bonaventure, Albertus Magnus, and Roger Bacon, until mostly rejected in the modern period. Evolution, though now it is seen to be compatible with evolution theories (cf \"Man incarnate spirit\" by Ramon Lucas Lucas). The idea of \"rationes seminales\" was also used as an explanation for spontaneous generation.\n\n"}
{"id": "3345862", "url": "https://en.wikipedia.org/wiki?curid=3345862", "title": "Recklessness (psychology)", "text": "Recklessness (psychology)\n\nRecklessness (also called unchariness) is disregard for or indifference to the dangers of a situation or for the consequences of one's actions, as in deciding to act without stopping to think beforehand. Aristotle considered such rashness as one end (excessive) of a continuum, with courage as the mean, cowardice as the deficit vice. Recklessness has been linked to antisocial personality disorder.\n\n\"Reck\" is a regard or reckoning, particularly of a situation. A reckless individual would engage in an activity without concern for its after-effects. It can in certain cases be seen as heroic—for example, the soldier fearlessly charging into battle, with no care for his own safety, has a revered status among some. However, recklessness is more commonly regarded as a vice—this same soldier may be a liability to his own side, or get himself killed for no benefit – and may be the product of a death wish.\n\nThe driving-force behind recklessness may be a need to test fate - an attempt to bolster a sense of omnipotence or of special privileges.\n\nOr it may be due to a loss of the feeling of anxiety, to a denial of it, or to an attempt to overcompensate for it.\n\nSimilarly dare-devils may overcompensate for an inhibited aggressiveness, while narcissists may enjoy a feeling that nothing can happen to them, similar to what Aristotle termed the maniac.\n\nRecklessness should not be confused with bravery. Although the two could sometimes be connected, the latter is usually applied to cases where a person displays a more reasonable reckoning of the inherent danger, rather than none at all.\n"}
{"id": "24933673", "url": "https://en.wikipedia.org/wiki?curid=24933673", "title": "Robert W. Spike", "text": "Robert W. Spike\n\nRobert Warren Spike (November 13, 1923 – October 17, 1966) was an American clergyman, theologian, and civil rights leader.\n\nSpike was born in Buffalo, New York and educated at Denison University, Union Theological Seminary, Columbia University, and Colgate-Rochester Divinity School. He began his career as pastor at the mainline Protestant Judson Memorial Church on Washington Square in Greenwich Village in 1949, reviving the social activism of this famous urban church. During his tenure there neighborhood kids played basketball in the church’s ramshackle gym and an interracial, international residence for students was established. Spike also helped to create an art gallery where artists such as Claes Oldenburg, Allen Kaprow and Jim Dine could exhibit their, then unconventional, work.\n\nIn 1958 Spike left his parish ministry to take on a national role as General Secretary of the United Church Board For Homeland Ministries. In 1963 he was appointed the Executive Director of the National Council of Churches’ Commission on Religion and Race, which became an important arm of the Civil Rights Movement. Anna Arnold Hedgeman joined his staff there as a Coordinator of Special Events. Through Spike’s efforts Protestant churches participated significantly in the March on Washington in August 1963. Spike worked with Robert Parris Moses to set up the Freedom Summer project.\n\nIn January 1966 Spike took a position as Professor of Ministry and Director of the Doctor of Ministry Program in the Divinity School at the University of Chicago. Less than a year after assuming his post in Chicago, Spike was bludgeoned to death at Ohio State University in Columbus on October 17, 1966. No one was ever tried for his murder; after a systematic review some church sources believe that he was assassinated. Police investigations attempted to link Spike's murder with his bisexuality.\n\nUpon learning of Rev. Spike's death, Martin Luther King Jr. was quoted as stating, \"He was one of those rare individuals who sought at every point to make religion relevant to the social issues of our time. He lifted religion from the stagnant arena of pious irrelevancies and sanctimonious trivialities. His brilliant and dedicated work will be an inspiration to generals yet unborn. We will always remember his unswerving devotion to the legitimate aspirations of oppressed people for freedom and human dignity. It was my personal pleasure and sacred privilege to work closely with him in various undertakings.\"\n\nSpike's son, Paul Robert Spike is an American author, editor and journalist best known as the author of the 1973 memoir \"Photographs of My Father\" about the murder of his father, in 1966.\n\n\n\n"}
{"id": "11153041", "url": "https://en.wikipedia.org/wiki?curid=11153041", "title": "Saint-Venant's principle", "text": "Saint-Venant's principle\n\nSaint-Venant's principle, named after Adhémar Jean Claude Barré de Saint-Venant, a French elasticity theorist, may be expressed as follows:\n\nThe original statement was published in French by Saint-Venant in 1855. Although this informal statement of the principle is well known among structural and mechanical engineers, more recent mathematical literature gives a rigorous interpretation in the context of partial differential equations. An early such interpretation was made by von Mises in 1945.\n\nThe Saint-Venant's principle allows elasticians to replace complicated stress distributions or weak boundary conditions with ones that are easier to solve, as long as that boundary is geometrically short. Quite analogous to the electrostatics, where the electric field due to the \"i\"-th moment of the load (with 0th being the net charge, 1st the dipole, 2nd the quadrupole) decays as formula_1 over space, Saint-Venant's principle states that high order momentum of mechanical load (moment with order higher than torque) decays so fast that they never need to be considered for regions far from the short boundary. Therefore, the Saint-Venant's principle can be regarded as a statement on the asymptotic behavior of the Green's function by a point-load.\n\n"}
{"id": "4148239", "url": "https://en.wikipedia.org/wiki?curid=4148239", "title": "Science and Rationalists' Association of India", "text": "Science and Rationalists' Association of India\n\nThe Science and Rationalists' Association of India () is a rationalist group based in Kolkata, India.\n\nInspired by Sri Lankan rationalists, Dr. Abraham Kovoor, Bengali rationalists established an organisation called 'Bharater Yuktibadi Samity' on March 1, 1985, the international rationalists' day. Two years later, in 1987, it was renamed as 'Bharatiya Bigyan O Yuktibadi Samiti'. Prabir Ghosh, the author of 'Aloukik Noy Loukik' series is the founder secretary and Dr. Dhirendranath Gangopadhyay was the first president of this organisation. Eminent science communicators Amit Chakraborty, Aparajito Basu, Jugalkanti Ray, Shankar Chakraborty and others were also associated during its formation. In 1986, Ghosh published the first book of the 'Aloukik Noy Loukik' series, debunking various superstitious beliefs. It received wide circulation among Bengali readers of both West Bengal and Bangladesh, and the Rationalists' Association gained popularity.\n\nThe main goal of the organisation is to advocate against pseudoscience, astrology and mysticism.\n\n"}
{"id": "23533161", "url": "https://en.wikipedia.org/wiki?curid=23533161", "title": "Standard Business Reporting", "text": "Standard Business Reporting\n\nStandard Business Reporting is a group of international programs instigated by a number of governments to reduce the regulatory burden for business. The concept is to make business the centre when it comes to managing business-to-government reporting obligations.* Businesses conduct their own financial administration; the facts they record and decisions they make should drive their reporting. The government should be able to receive and process this information without imposing undue constraints on how businesses administer their finances.\n\nThe method used to achieve this goal is to define a \"common language\" (or taxonomy) using appropriate standards such as XBRL, XML and JSON, then provide systems to process information classified under the taxonomy.\n\nThe Dutch Taxonomy Project (Nederlandse Taxonomie Project) or NTP began in 2004 as part of the Dutch cabinet’s objectives to reduce the administrative burdens on businesses. The project was sponsored jointly by the Dutch Ministries of Finance and Justice. The NTP created an XBRL taxonomy that \"enables businesses to generate the required reporting information directly from their own records and the government to then process this information efficiently and effectively\".\n\nThe Dutch approach was adopted by the Australian government in 2006, which established the Standard Business Reporting (SBR) Program. In addition to Australia, other countries (including New Zealand) are also planning to apply this approach. This approach has since been internationally designated as \"Standard Business Reporting\".\n\nIn December 2008, the Dutch government decided to rename the NTP to the Standard Business Reporting (SBR) Programme, thus adopting the name introduced by Australia. The Dutch SBR programme has been tasked with deepening and embedding the results obtained so far and broadening the scope to other domains and applications.\n\nIn March 2017, the Data Foundation and PwC published a research report explaining how the adoption of Standard Business Reporting (SBR) in the United States would reduce costs for both companies and agencies. The report, \"Standard Business Reporting: Open Data to Cut Compliance Costs\", defines SBR as multiple regulatory agencies adopting a common open data structure for the information they collect. \n\nAs of the first of January 2007, businesses and intermediaries can report their financial data to the government using the Dutch XBRL taxonomy.\n\nThe 2006 report of the \"Taskforce on Reducing Regulatory Burdens on Business\", \"Rethinking Regulation\" (the Banks report), recognised that government reporting requirements impose a significant burden on Australian business.\n\nThe objective of the SBR Program in Australia is to reduce the cost of reporting for business by A$800 million over six years at a cost of A$320 million over the same period.\n\nThe key activity of the SBR Program is to work across agencies and jurisdictions to standardise the reporting approach and language – developing the taxonomy. As well as the reporting language, SBR is developing a new e-channel for business which will include a single sign-on to on-line services across the agencies that are in scope.\n\nThe agencies in scope are:\n\nMore than 75 government forms are in scope to be rationalised and replaced by electronic lodgments.\n\nThe Australian SBR solution is planned to be developed as follows:\n\nThe SBR program also created the SBR Business Advisory Forum as a way to provide ongoing consultation to the project. It is made up of 18 representatives drawn from industry groups (e.g. Council of Small Business Organisations of Australia, professional associations (e.g. CPA Australia) and the SBR program itself.\n\nIn May 2008 the New Zealand Ministry of Economic Development published a business case for adopting an SBR program. The business case states that success relies on a high take-up rate by intermediaries such as accountants and lawyers. This is because many owner-operators and small businesses (68% and 21% of businesses, respectively) conduct their reporting via these third parties.\n\nThe business case states that \"SBR will deliver compliance cost reductions to business by reducing the need for them to submit information to multiple agencies, standardising data definitions and implementing a standard communication language.\".\n"}
{"id": "29375525", "url": "https://en.wikipedia.org/wiki?curid=29375525", "title": "StopWatch (campaign)", "text": "StopWatch (campaign)\n\nStopWatch is a joint venture between a range of civil society organisations, activist and human rights groups, academics and campaigners. StopWatch was established to address concerns about the use of Stop and Search powers by police in the UK with regards to law, community relations and civil rights. Its primary target is addressing the significant ethnic dis-proportionality in the use of stop and search; however, it also aims to review the use of powers which do not require reasonable suspicion to order a stop and search such as section 60 and to ensure effective monitoring and accountability are employed in conjunction with Stop and Search powers. It also aims to promote more effective methods of policing that do not have the same impact upon civil liberties and community relations.\n\nThe StopWatch Campaign involves: Equanomics UK; Federation of Student Islamic Societies (FOSIS); Mannheim Centre for Criminology, LSE; Muslim Safety Forum; NACRO; Not Another Drop;Open Society Justice Initiative; Release;The Runnymede Trust; School of Law, King’s College London; Second Wave; and Turning Point.\n\nStopWatch formed in the summer of 2010 in response to concerns about the use of a variety of stop and search powers. Following a ruling by the European Court of Human Rights that the power to search people without suspicion was illegal, amendments to Section 44 powers were proposed by the new Home Secretary Theresa May. Previously police officers were, in certain defined areas, able to use anti-terrorism legislation to stop people without requiring reasonable suspicion that they were actually involved in terrorism. Under the new proposals police officers would no longer have this license. However, StopWatch has drawn attention to changes to the Police and Criminal Evidence Act 1984 which would remove requirements for the collection of key data such as name, and whether any injury or damage resulted from the incident.\n\nStopWatch was officially launched on the 18 October 2010 by the Reverend Jesse Jackson at Kings College London. Reverend Jackson's has previously addressed similar issues in the US with his Rainbow PUSH coalition in New York.\n\nOne of StopWatch’s key targets is a 50% reduction in dis-proportionality in stop and search figures. Since StopWatch formed statistics were released indicating that Black people were 26 times more likely to be stopped and searched.\n\nMembers of StopWatch have stated that their aim is not to eliminate stop and search powers per se, rather to help create a more responsible, and measured approach to its use and to Police officers’ interaction with the public generally.\n\n"}
{"id": "36750373", "url": "https://en.wikipedia.org/wiki?curid=36750373", "title": "Strikes! Labor History Encyclopedia for the Pacific Northwest", "text": "Strikes! Labor History Encyclopedia for the Pacific Northwest\n\nStrikes! Labor History Encyclopedia of the Pacific Northwest is a clearinghouse of information on the labor history of the region developed by the University of Washington and Professor James N. Gregory as part of the Pacific Northwest Labor and Civil Rights History Projects. The Encyclopedia covers the major industries of Washington State, major unions and worker struggles, civil rights activism among many ethnic communities, radical organizations, the Great Depression and the New Deal and the region's rich history of labor and radical newspapers.\n\nThe site is organized into thematic categories, focusing on the core industries and movements that have shaped the social history of the region. These include the timber, waterfront, aerospace, construction, farm industries. Outside of unions and strikes, other aspects of workers and their struggles are examined, from everyday life in the Great Depression to labor culture and art. A particular focus is also given to the intertwined history of radical organizations with labor struggles, including the Industrial Workers of the World, the Communist Party and the United Construction Workers Association. Civil rights organizations and the struggles of black, Asian-American, women, and Latino workers for jobs, housing and equal rights is also cataloged. Much of the content is housed by the individual projects of the Pacific Northwest Labor and Civil Rights History initiative, which has gathered thousands of oral histories, rare photos, vintage newspapers and original academic research.\n\n"}
{"id": "1460183", "url": "https://en.wikipedia.org/wiki?curid=1460183", "title": "Swale (landform)", "text": "Swale (landform)\n\nA swale is a shallow channel with gently sloping sides. A swale may be either natural or human created. Artificial swales are often infiltration basins, designed to manage water runoff, filter pollutants, and increase rainwater infiltration.\n\nThe swale concept has also been popularized as a rainwater harvesting and soil conservation strategy by Bill Mollison, Geoff Lawton and other advocates of permaculture. In this context it is usually a water-harvesting ditch on contour, also called a \"contour bund\".\nSwales as used in permaculture are designed to slow and capture runoff by spreading it horizontally across the landscape (along an elevation contour line), facilitating runoff infiltration into the soil. This type of swale is created by digging a ditch on contour and piling the dirt on the downhill side of the ditch to create a berm. \n\nIn arid climates, vegetation (existing or planted) along the swale can benefit from the concentration of runoff. Trees and shrubs along the swale can provide shade and mulch which decrease evaporation.\n\nThe term swale or \"beach swale\" is also used to describe long, narrow, usually shallow troughs between ridges or sandbars on a beach, that run parallel to the shoreline.\n\n\n"}
{"id": "53885535", "url": "https://en.wikipedia.org/wiki?curid=53885535", "title": "The Academy of Experts", "text": "The Academy of Experts\n\nThe Academy of Experts (TAE; formerly the British Academy of Experts) is a UK legal institute for expert witnesses. It was founded in 1987 with the objective of providing a professional body for experts to establish and promote high objective standards.\n\nAlthough there is representation on the Academy’s Council from the legal profession the majority of the officers, including the Chairman, are practising Experts.\n\nThe President of The Academy is currently Lord Saville of Newdigate. Past Presidents include the UK politician and former Chancellor of the Exchequer Geoffrey Howe and Gordon Slynn, Baron Slynn of Hadley.\n\nAs a multi-disciplinary body TAE works with professional bodies around the world advising and supporting their Expert Witness practices. In partnership with the Institute of Chartered Accountants in England and Wales TAE publishes the Register of Accredited Accountant Expert Witnesses.\n\nIn 2005 the Code of Practice for Experts was endorsed for by the Master of the Rolls, Lord Phillips of Worth Matravers. The code is cited in the Northern Ireland Rules on Expert Evidence PD 1 of 2015.\n"}
{"id": "113031", "url": "https://en.wikipedia.org/wiki?curid=113031", "title": "Total depravity", "text": "Total depravity\n\nTotal depravity (also called radical corruption or pervasive depravity) is a Christian theological doctrine derived from the concept of original sin. It is the teaching that, as a consequence of the Fall of Man, every person born into the world is enslaved to the service of sin as a result of their fallen nature and, apart from the efficacious or prevenient grace of God, is utterly unable to choose to follow God, refrain from evil, or accept the gift of salvation as it is offered.\n\nIt is advocated to various degrees by many Protestant confessions of faith and catechisms, including those of some Lutheran synods, and Calvinism. Arminians, such as Methodists, believe and teach total depravity, but with distinct differences. The key distinction between the total depravity embraced by Calvin and the total depravity taught by Arminius is the distinction between irresistible grace and prevenient grace.\n\nIn opposition to Pelagius, who believed that after the fall people are able to choose not to sin, Augustine of Hippo argued that, since the fall, all humanity is in self-imposed bondage to sin. All people are inescapably predisposed to evil prior to making any actual choice, and are unable to not sin. Free will is not taken away in the sense of the ability to choose between alternatives, but people are unable to make these choices in service to God rather than self. Thomas Aquinas also taught that people are not able to avoid sin after the fall, and that this entailed a loss of original righteousness or sinlessness, as well as concupiscence or selfish desire. Duns Scotus, however, modified this interpretation and only believed that sin entailed a lack of original righteousness. During the Protestant Reformation, the Reformers took Scotus's position to be the Catholic position and argued that it made sin only a defect or privation of righteousness rather than an inclination toward evil. Martin Luther, John Calvin and other Reformers used the term \"total depravity\" to articulate what they claimed to be the Augustinian view that sin corrupts the entire human nature. This did not, however, mean the loss of the \"imago Dei\" (image of God). The only theologian who argued that the \"imago Dei\" itself was taken away and that the very substance of fallen humanity was sin was Matthias Flacius Illyricus, and this view was repudiated in the Formula of Concord.\n\nJohn Calvin used terms like \"total depravity\" to mean that, despite the ability of people to outwardly uphold the law, there remained an inward distortion which makes all human actions displeasing to God, whether or not they are outwardly good or bad. Even after regeneration, every human action is mixed with evil. Later Calvinist theologians were agreed on this, but the language of the Canons of Dort as well as the 17th-century Reformed theologians which followed it did not repeat the language of \"total depravity\", and arguably offer a more moderate view on the state of fallen humanity than Calvin.\n\nArminianism also accepts a doctrine of total depravity, although not identical to the Calvinist position. Total depravity was affirmed by the Five articles of Remonstrance, by Jacobus Arminius himself, and by John Wesley, who strongly identified with Arminius through publication of his periodical \"The Arminian\" and also advocated a strong doctrine of inability. \"The Methodist Quarterly Review\" states that\n\nSome Reformed theologians have mistakenly used the term \"Arminianism\" to include some who hold the Semipelagian doctrine of limited depravity, which allows for an \"island of righteousness\" in human hearts that is uncorrupted by sin and able to accept God's offer of salvation without a special dispensation of grace. Although Arminius and Wesley both vehemently rejected this view, it has sometimes inaccurately been lumped together with theirs (particularly by Calvinists) because of other similarities between their respective systems such as conditional election, unlimited atonement, and prevenient grace. In particular, prevenient grace is viewed by some as giving humans back the freedom to follow God in one way or another.\n\nThe term \"total depravity\", as understood in colloquial English, obscures the theological issues involved. One cannot simply look at the two words and conjecture upon the extent of the depravity of humanity. For example, Reformed and Lutheran theologians have never considered humans to be absent of goodness or unable to do good outwardly as a result of the fall. People retain the \"imago Dei\", though it has been distorted. \n\nTotal depravity is the fallen state of human beings as a result of original sin. The doctrine of total depravity asserts that people are, as a result of the fall, not inclined or even able to love God wholly with heart, mind, and strength, but rather are inclined by nature to serve their own will and desires and reject His rule. Even religion and philanthropy are wicked to God because they originate from a selfish human desire and are not done to the glory of God. Therefore, in Reformed theology, if God is to save anyone, He must predestine, call, or elect individuals to salvation since fallen man does not want to, and is indeed incapable of, choosing Him. However, in Arminian theology prevenient grace (or \"enabling grace\") does reach through total depravity to enable people to respond to the salvation offered by God in Jesus Christ.\n\nTotal depravity does not mean that people have lost part of their humanity or are ontologically deteriorated. Just as Adam and Eve were created with the ability to not sin, people retain that essential ability to either sin or not sin, even though some properties of their humanity are corrupted. It also does not mean that people are as evil as possible. Rather, it means that even the good which a person may intend is faulty in its premise, false in its motive, and weak in its implementation; and there is no mere refinement of natural capacities that can correct this condition. Thus, even acts of generosity and altruism are in fact egoist acts in disguise. All good, consequently, is derived from God alone, and in no way through humanity.\n\nThe total reach of sin taught with the doctrine of total depravity highlights people's dire need for God. No part of the person is not in need of grace, and all people are in need of grace, no matter how outwardly pious.\n\nIt is important to understand the scope of the \"total depravity\" of humanity in order to understand the Calvinist-Arminian debate. As noted, both views embrace total depravity; it is a question of the action which they believe God must take to reach humanity in its fallen and depraved state. May God grant to humanity the grace to respond to His offer of salvation, so that all may believe (as Arminius taught)? Or must God's grace be irresistible in order to reach humanity (as Calvin taught), so that it is impossible for anyone to be saved unless God first extends to them His irresistible grace? Stated in this manner, there is no substantial difference in total depravity as embraced by Calvinists and Arminians; both agree that humanity is in a state of depravity which prevents them from responding to God. Rather, the two groups have a different belief in the grace which God extended to humanity in response to total depravity. Calvin taught Irresistible Grace; Arminius taught Prevenient Grace.\n\nThe Roman Catholic Church maintains that man cannot, \"be justified before God by his own works, ... without the grace of God through Jesus Christ\", thereby rejecting Pelagianism in accordance with the writings of Augustine and the Second Council of Orange (529). However, even strictly Augustinian Catholics disagree with the Protestant doctrine of total depravity. Referring to Scripture and the Church Fathers, Catholicism views human free will as deriving from God's image because humans are created in God's image. Accordingly, the Council of Trent, at its sixth session (January 1547), condemned as heresy any doctrine asserting \"since Adam's sin, the free will of man is lost and extinguished\".\n\nThe Orthodox Church embraces the \"semi-Augustinian\" position of John Cassian and also defends Augustine of Hippo relating to this doctrine. Seraphim Rose, for example, contends that Augustine never denied the free will of every human, thus he never taught total depravity. Archbishop Chrysostomos has likewise asserted that Augustine's teaching might have been used and distorted in Western Christianity to produce innovative theologizing, and it is not Augustine's fault.\n\n\n"}
{"id": "52525503", "url": "https://en.wikipedia.org/wiki?curid=52525503", "title": "Under2 Coalition", "text": "Under2 Coalition\n\nThe Under2 Coalition is a coalition of subnational governments that aims to achieve greenhouse gases emissions mitigation. It started as a memorandum of understanding, which was signed by twelve founding jurisdictions on May 19, 2015 in Sacramento, California. Although it was originally called the Under2 MOU, it became known as the Under2 Coalition in 2017. As of September 2018, the list of signatories has grown to over 220 jurisdictions which combined encompasses 1.3 billion people and 43% of the world economy. The Under2 MOU was conceived through a partnership between the governments of California and Baden-Wurttemberg, with The Climate Group acting as secretariat.\n\nThe intent of the memorandum signatories is for each to achieve Greenhouse gas \"emission reductions consistent with a trajectory of 80 to 95 percent below 1990 levels by 2050 and/or achieving a per capita annual emission goal of less than 2 metric tons by 2050. The signatories believe these actions are consistent with findings of the Intergovernmental Panel on Climate Change (IPCC) of what is necessary to avoid a 2 degree Celsius rise in average global temperatures. Organizers are concerned that a rise in global temperature above 2 degrees Celsius would cause widespread environmental harm.\n\nSignatories to the memorandum are asked to submit a plan to meet the target reduction of green house gas emissions by 2 metric tons per capita by 2050. Each of the governments also pledges to assist each other with scientific research, sharing of the available technologies and best practices in energy efficiency.\n\nThe memorandum was developed just before the 2015 United Nations Climate Change Conference also known as COP 21 or Paris Agreement. The Under2 MOU allows subnational governments such as cities, counties and states to highlight their work to reduce greenhouse gas emissions.\n\nSubnational governments like cities, states and provinces have traditionally relied on national governments to take the lead on transnational climate governance aimed at addressing climate change mitigation through inter-governmental agreements. Some subnational governments have expressed frustration at the inaction of national leaders and took it upon themselves to create the subnational Under2 MOU agreement. The major difference between an international treaty and the Under2 MOU agreement between subnational governments is that the Under2 MOU is non-binding.\n\nIn December 2015, California and Baden-Wurttemberg, who spearheaded the Under 2 MOU, announced that The Climate Group would take on the role of secretariat for the pact.\n\nThere have been efforts in the past to organize subnational governments to address climate change most notably through the Cities for Climate Protection Program - an effort associated with the International Union of Local Authorities and the United Nations Environment Programme. At its peak in 2010 the program had 700 municipal members who were required to provide among other things inventories and targets for greenhouse emissions. the International Union of Local Authorities provided technical assistance to the municipalities engaged in this planning.\n\nBefore the Under2 MOU was conceived many subnational governments had taken the initiative to create a climate action plan. The purpose of a climate action plan is to identify the amount of greenhouse gas emissions produced by the jurisdiction and, in many cases, provide strategies to lower or stop greenhouse gas emissions altogether. Some governments have found that the data produced by the climate action plan increases transparency and helps with longterm planning to reduce greenhouse gas emissions. Since signatories to the Under2 MOU submit their action plans as an appendix to the document this is the first time some cities and states around the world are coming up with plans to reduce greenhouse gas emissions in their jurisdiction.\n\nCanada, Costa Rica, Czech Republic, Denmark, Fiji, France, Germany, Italy, Japan, Luxembourg, Mexico, The Netherlands, Norway, Panama, Peru, Sweden, United Kingdom\n\n"}
{"id": "31038410", "url": "https://en.wikipedia.org/wiki?curid=31038410", "title": "Units of measurement in France before the French Revolution", "text": "Units of measurement in France before the French Revolution\n\nBefore the French Revolution, which started in 1789, French units of measurement were based on the Carolingian system, introduced by the first Holy Roman Emperor Charlemagne which in turn were based on ancient Roman measures. Charlemagne brought a consistent system of measures across the entire empire. However, after his death, the empire fragmented and many rulers introduced their own variants of the units of measure.\n\nSome of Charlemagne's units of measure, such as the pied du Roi (the king's foot) remained virtually unchanged for about a thousand years, while others, such as the \"aune\" (ell—used to measure cloth) and the \"livre\" (pound) varied dramatically from locality to locality. By the time of the revolution, the number of units of measure had grown to the extent that it was almost impossible to keep track of them.\n\nAlthough in the pre-revolutionary era (before 1795) France used a system and units of measure that had many of the characteristics of contemporary English units (or the later Imperial System of units), France still lacked a unified, countrywide system of measurement. Whereas in England the Magna Carta decreed that \"there shall be one unit of measure throughout the realm\", Charlemagne and successive kings had tried but failed to impose a unified system of measurement in France.\n\nThe names and relationships of many units of measure were adopted from Roman units of measure and much more were added – it has been estimated that there were seven or eight hundred different names for the various units of measure. Moreover, the quantity associated with each unit of measure differed from town to town and even from trade to trade to the extent that the \"lieue\" (league) could vary from 3.268 km in Beauce to 5.849 km in Provence. It has been estimated that on the eve of the Revolution a quarter of a million different units of measure were in use in France. Although certain standards, such as the \"pied du Roi\" (the King's foot) had a degree of pre-eminence and were used by \"savants\", many traders chose to use their own measuring devices giving scope for fraud and hindering commerce and industry.\n\nAs an example, the weights and measures used at Pernes-les-Fontaines in southeastern France differ from those cataloged later in this article as having been used in Paris. In many cases the names are different, while the \"livre\" is shown as being 403 g, as opposed to 489 g – the value of the \"livre du Roi\".\n\nThese definitions use the Paris definitions for the \"couture\" of Paris, and definitions for other Ancien régime civil jurisdictions varied, at times quite significantly.\n\nThe mediaeval royal units of length were based on the \"toise\" and in particular the \"toise de l'Écritoire\", the distance between the fingertips of the outstretched arms of a man which was introduced in 790 AD by Charlemagne. The \"toise\" had 6 \"pieds\" (feet) each of 326.6 mm (12.86 in). In 1668 the reference standard was found to have been deformed and it was replaced by the \"toise du Châtelet\" which, to accommodate the deformation of the earlier standard, was 11 mm (0.55%) shorter. In 1747 this \"toise\" was replaced by a new \"toise\" of near-identical length – the \"Toise du Pérou\", custody of which was given to \"l'Académie des Sciences au Louvre\".\n\nAlthough the \"pouce\" (inch), \"pied\" (foot) and \"toise\" (fathom) were fairly consistent throughout most of pre-revolutionary France, some areas had local variants of the \"toise\". Other units of measure such as the \"aune\" (ell), the \"perche\" (perch/rood), the \"arpent\" and the \"lieue\" (league) had a number of variations, particularly the \"aune\" (which was used to measure cloth\n\nThe \"loi du 19 frimaire an VIII\" (Law of 10 December 1799) states that one decimal metre is exactly 443.296 French lines, or \"3 pieds 11.296 lignes de la \"Toise du Pérou\"\". Thus the French royal foot is exactly 9000/27,706 metres (about 0.3248 m).\n\nIn Quebec, the surveys in French units were converted using the relationship 1 \"pied\" (of the French variety, the same word being used for English feet as well) = 12.789 English inches. This makes the Quebec \"pied\" very slightly smaller (about 4 parts in one million) than the \"pied\" used in France.\n\n\nAccording to the law of 19 Frimaire An VIII (December 10, 1799),\n\n\nTraditionally, the French pound (\"livre\") was defined as the mass of exactly of a French cubic foot of water. When the kilogram was defined, the knowledge that a \"pied du Roi cube\" filled with water masses exactly 70 French pounds was apparently lost. According to the traditional (cubic foot) definition, one \"livre\" would have been about 489.675  grams. According to the kilogramme definition, one \"livre\" was about 489.506 grammes. The difference is about 0.035%. However, a small difference in salinity (i.e. the difference between distilled water and very good quality drinking water) is enough to explain this difference.\n\nThe units in the following table are (except for the talent) calculated based on the kilogram definition of the \"livre\".\n\n"}
{"id": "6290771", "url": "https://en.wikipedia.org/wiki?curid=6290771", "title": "Whitehead's point-free geometry", "text": "Whitehead's point-free geometry\n\nIn mathematics, point-free geometry is a geometry whose primitive ontological notion is \"region\" rather than point. Two axiomatic systems are set out below, one grounded in mereology, the other in mereotopology and known as \"connection theory\". A point can mark a space or objects.\n\nPoint-free geometry was first formulated in Whitehead (1919, 1920), not as a theory of geometry or of spacetime, but of \"events\" and of an \"extension relation\" between events. Whitehead's purposes were as much philosophical as scientific and mathematical.\n\nWhitehead did not set out his theories in a manner that would satisfy present-day canons of formality. The two formal first order theories described in this entry were devised by others in order to clarify and refine Whitehead's theories. The domain for both theories consists of \"regions.\" All unquantified variables in this entry should be taken as tacitly universally quantified; hence all axioms should be taken as universal closures. No axiom requires more than three quantified variables; hence a translation of first order theories into relation algebra is possible. Each set of axioms has but four existential quantifiers.\n\nThe axioms G1-G7 are, but for numbering, those of Def. 2.1 in Gerla and Miranda (2008) (see also Gerla (1995)). The identifiers of the form WPn, included in the verbal description of each axiom, refer to the corresponding axiom in Simons (1987: 83).\n\nThe fundamental primitive binary relation is \"Inclusion\", denoted by infix \"≤\". (\"Inclusion\" corresponds to the binary \"Parthood\" relation that is a standard feature of all mereological theories.) The intuitive meaning of \"x\"≤\"y\" is \"\"x\" is part of \"y\".\" Assuming that identity, denoted by infix \"=\", is part of the background logic, the binary relation \"Proper Part\", denoted by infix \"<\", is defined as:\n\nformula_1\n\nThe axioms are:\n\n\n\n\n\nA model of G1–G7 is an \"inclusion space\".\n\nDefinition (Gerla and Miranda 2008: Def. 4.1). Given some inclusion space, an abstractive class is a class \"G\" of regions such that \"G\" is totally ordered by Inclusion. Moreover, there does not exist a region included in all of the regions included in \"G\".\n\nIntuitively, an abstractive class defines a geometrical entity whose dimensionality is less than that of the inclusion space. For example, if the inclusion space is the Euclidean plane, then the corresponding abstractive classes are points and lines.\n\nInclusion-based point-free geometry (henceforth \"point-free geometry\") is essentially an axiomatization of Simons's (1987: 83) system W. In turn, W formalizes a theory in Whitehead (1919) whose axioms are not made explicit. Point-free geometry is W with this defect repaired. Simons (1987) did not repair this defect, instead proposing in a footnote that the reader do so as an exercise. The primitive relation of W is Proper Part, a strict partial order. The theory of Whitehead (1919) has a single primitive binary relation \"K\" defined as \"xKy\" ↔ \"y\"<\"x\". Hence \"K\" is the converse of Proper Part. Simons's WP1 asserts that Proper Part is irreflexive and so corresponds to G1. G3 establishes that inclusion, unlike Proper Part, is anti-symmetric.\n\nPoint-free geometry is closely related to a dense linear order D, whose axioms are G1-3, G5, and the totality axiom formula_9 Hence inclusion-based point-free geometry would be a proper extension of D (namely D∪{G4, G6, G7}), were it not that the D relation \"≤\" is a total order.\n\nIn his 1929 \"Process and Reality\", A. N. Whitehead proposed a different approach, one inspired by De Laguna (1922). Whitehead took as primitive the topological notion of \"contact\" between two regions, resulting in a primitive \"connection relation\" between events. Connection theory C is a first order theory that distills the first 12 of the 31 assumptions in chpt. 2 of \"Process and Reality\" into 6 axioms, C1-C6. C is a proper fragment of the theories proposed in Clarke (1981), who noted their mereological character. Theories that, like C, feature both inclusion and topological primitives, are called mereotopologies.\n\nC has one primitive relation, binary \"connection,\" denoted by the prefixed predicate letter \"C\". That \"x\" is included in \"y\" can now be defined as \"x\"≤\"y\" ↔ ∀z[\"Czx\"→\"Czy\"]. Unlike the case with inclusion spaces, connection theory enables defining \"non-tangential\" inclusion, a total order that enables the construction of abstractive classes. Gerla and Miranda (2008) argue that only thus can mereotopology unambiguously define a point.\n\nThe axioms C1-C6 below are, but for numbering, those of Def. 3.1 in Gerla and Miranda (2008).\n\n\n\n\n\n\n\nA model of C is a \"connection space\".\n\nFollowing the verbal description of each axiom is the identifier of the corresponding axiom in Casati and Varzi (1999). Their system SMT (\"strong mereotopology\") consists of C1-C3, and is essentially due to Clarke (1981). Any mereotopology can be made atomless by invoking C4, without risking paradox or triviality. Hence C extends the atomless variant of SMT by means of the axioms C5 and C6, suggested by chpt. 2 of \"Process and Reality\". For an advanced and detailed discussion of systems related to C, see Roeper (1997).\n\nBiacino and Gerla (1991) showed that every model of Clarke's theory is a Boolean algebra, and models of such algebras cannot distinguish connection from overlap. It is doubtful whether either fact is faithful to Whitehead's intent.\n\n\n\n"}
