{"id": "12415190", "url": "https://en.wikipedia.org/wiki?curid=12415190", "title": "Action algebra", "text": "Action algebra\n\nIn algebraic logic, an action algebra is an algebraic structure which is both a residuated semilattice and a Kleene algebra. It adds the star or reflexive transitive closure operation of the latter to the former, while adding the left and right residuation or implication operations of the former to the latter. Unlike dynamic logic and other modal logics of programs, for which programs and propositions form two distinct sorts, action algebra combines the two into a single sort. It can be thought of as a variant of intuitionistic logic with star and with a noncommutative conjunction whose identity need not be the top element. Unlike Kleene algebras, action algebras form a variety, which furthermore is finitely axiomatizable, the crucial axiom being \"a\"•(\"a\" → \"a\")* ≤ \"a\". Unlike models of the equational theory of Kleene algebras (the regular expression equations), the star operation of action algebras is reflexive transitive closure in every model of the equations.\n\nAn action algebra (\"A\", ∨, 0, •, 1, ←, →, *) is an algebraic structure such that (\"A\", ∨, •, 1, ←, →) forms a residuated semilattice while (\"A\", ∨, 0, •, 1, *) forms a Kleene algebra. That is, it is any model of the joint theory of both classes of algebras. Now Kleene algebras are axiomatized with quasiequations, that is, implications between two or more equations, whence so are action algebras when axiomatized directly in this way. However, action algebras have the advantage that they also have an equivalent axiomatization that is purely equational. The language of action algebras extends in a natural way to that of action lattices, namely by the inclusion of a meet operation.\n\nIn the following we write the inequality \"a\" ≤ \"b\" as an abbreviation for the equation \"a\" ∨ \"b\" = \"b\". This allows us to axiomatize the theory using inequalities yet still have a purely equational axiomatization when the inequalities are expanded to equalities.\n\nThe equations axiomatizing action algebra are those for a residuated semilattice, together with the following equations for star.\n\nThe first equation can be broken out into three equations, 1 ≤ \"a\"*, \"a\"*•\"a\"* ≤ \"a\"*, and \"a\" ≤ \"a\"*. These force \"a\"* to be reflexive, transitive, and greater or equal to \"a\" respectively. The second axiom asserts that star is monotone. The third axiom can be written equivalently as \"a\"•(\"a\"→\"a\")* ≤ \"a\", a form which makes its role as induction more apparent. These two axioms in conjunction with the axioms for a residuated semilattice force \"a\"* to be the least reflexive transitive element of the semilattice greater or equal to \"a\". Taking that as the definition of reflexive transitive closure of \"a\", we then have that for every element \"a\" of any action algebra, \"a\"* is the reflexive transitive closure of \"a\".\n\nThe equational theory of the implication-free fragment of action algebras, those equations not containing → or ←, can be shown to coincide with the equational theory of Kleene algebras, also known as the regular expression equations. In that sense the above axioms constitute a finite axiomatization of regular expressions. Redko showed in 1967 that these equations had no finite axiomatization, for which John Horton Conway gave a shorter proof in 1971. Salomaa gave an equation schema axiomatizing this theory which Kozen subsequently reformulated as a finite axiomatization using quasiequations or implications between equations, the crucial quasiequations being those of induction: if \"x\"•\"a\" ≤ \"x\" then \"x\"•\"a\"* ≤ \"x\", and if \"a\"•\"x\" ≤ \"x\" then \"a\"*•\"x\" ≤ \"x\". Kozen defined a Kleene algebra to be any model of this finite axiomatization.\n\nConway showed that the equational theory of regular expressions admit models in which \"a\"* was not the reflexive transitive closure of \"a\", by giving a four-element model 0 ≤ 1 ≤ \"a\" ≤ \"a\"* in which \"a\"•\"a\" = \"a\". In Conway's model, \"a\" is reflexive and transitive, whence its reflexive transitive closure should be \"a\". However the regular expressions do not enforce this, allowing \"a\"* to be strictly greater than \"a\". Such anomalous behavior is not possible in an action algebra.\n\nAny Heyting algebra (and hence any Boolean algebra) is made an action algebra by taking • to be ∧ and \"a\"* = 1. This is necessary and sufficient for star because the top element 1 of a Heyting algebra is its only reflexive element, and is transitive as well as greater or equal to every element of the algebra.\n\nThe set 2 of all formal languages (sets of finite strings) over an alphabet Σ forms an action algebra with 0 as the empty set, 1 = {ε}, ∨ as union, • as concatenation, \"L\"←\"M\" as the set of all strings \"x\" such that \"xM\" ⊆ \"L\" (and dually for \"M\"→\"L\"), and \"L\"* as the set of all strings of strings in \"L\" (Kleene closure).\n\nThe set 2 of all binary relations on a set \"X\" forms an action algebra with 0 as the empty relation, 1 as the identity relation or equality, ∨ as union, • as relation composition, \"R\"←\"S\" as the relation consisting of all pairs (\"x,y\") such that for all \"z\" in \"X\", \"ySz\" implies \"xRz\" (and dually for \"S\"→\"R\"), and \"R*\" as the reflexive transitive closure of \"R\", defined as the union over all relations \"R\" for integers \"n\" ≥ 0.\n\nThe two preceding examples are power sets, which are Boolean algebras under the usual set theoretic operations of union, intersection, and complement. This justifies calling them Boolean action algebras. The relational example constitutes a relation algebra equipped with an operation of reflexive transitive closure. Note that every Boolean algebra is a Heyting algebra and therefore an action algebra by virtue of being an instance of the first example.\n\n\n"}
{"id": "51565398", "url": "https://en.wikipedia.org/wiki?curid=51565398", "title": "Arm and hammer", "text": "Arm and hammer\n\nThe arm and hammer is a symbol consisting of a muscular arm holding a hammer. Used in ancient times as a symbol of the god Vulcan, it came to be known as a symbol of industry, for example blacksmithing and gold-beating. It has been used as a symbol by many different kinds of organizations, including banks, local government, and socialist political parties. \n\nIt has been used in heraldry, appearing in the Coat of arms of Birmingham and Seal of Wisconsin.\n\nThe similarity to the name of the industrialist Armand Hammer is not a coincidence: he was named after the symbol, as his father Julius Hammer was a supporter of socialist causes, including the Socialist Labor Party of America, with its arm-and-hammer logo.\n\nThe Arm & Hammer brand is a registered trademark of Church & Dwight, an American manufacturer of household products. According to the company, the logo originally represented Vulcan. Armand Hammer made an offer to outright purchase this company having this brand with the similarity to his name, and while this offer was refused, he eventually acquired enough stock to have a controlling interest and join the board of directors. He remained an owner until his death in 1990.\n\nAn arm-and-hammer sign can be seen in Manette Street, Soho, symbolizing the trade of gold-beating carried on there in the nineteenth century. It is referred to by Charles Dickens in \"A Tale of Two Cities\". As of 2016, the sign there is a replica, with the original being held in the Dickens Museum.\n\n"}
{"id": "24957728", "url": "https://en.wikipedia.org/wiki?curid=24957728", "title": "Battle of egos", "text": "Battle of egos\n\nA battle of egos is a phrase used metaphorically to describe competitions that are based on pride and often entail prodigious and arrogant demonstrations of prowess. A type of dueling similar to a pissing contest, ego battles are often seen as an arrogant way to determine who is the \"bigger man\" (as far as being superior right in an argument) by a competitive methodology that is not especially productive. The idiom is usually used figuratively and often refer to forms of ego-driven battling in a pejorative manner.\n\nA Tehran newspaper described the dispute between George W. Bush and Saddam Hussein as a battle of egos. The competition between television advertisements during the Super Bowl has been described as a battle of egos nicknamed \"The Ego Bowl\". A 1998 collective bargaining dispute in the National Basketball Association was also described as a battle of egos.\n\n"}
{"id": "4805", "url": "https://en.wikipedia.org/wiki?curid=4805", "title": "Behavior", "text": "Behavior\n\nBehavior (American English) or behaviour (Commonwealth English) is the range of actions and mannerisms made by individuals, organisms, systems, or artificial entities in conjunction with themselves or their environment, which includes the other systems or organisms around as well as the (inanimate) physical environment. It is the response of the system or organism to various stimuli or inputs, whether internal or external, conscious or subconscious, overt or covert, and voluntary or involuntary.\n\nTaking a behavior informatics perspective, a behavior consists of behavior actor, operation, interactions, and their properties. A behavior can be represented as a behavior vector.\n\nAlthough there is some disagreement as to how to precisely define behavior in a biological context, one common interpretation based on a meta-analysis of scientific literature states that \"behavior is the internally coordinated responses (actions or inactions) of whole living organisms (individuals or groups) to internal and/or external stimuli\".\n\nA broader definition of behavior, applicable to plants and other organisms, is similar to the concept of phenotypic plasticity. It describes behavior as a response to an event or environment change during the course of the lifetime of an individual, differing from other physiological or biochemical changes that occurs more rapidly, and excluding changes that are result of development (ontogeny).\n\nBehaviors can be either innate or learned from the environment.\n\nBehavior can be regarded as any action of an organism that changes its relationship to its environment. Behavior provides outputs from the organism to the environment.\n\nHuman behavior is believed to be influenced by the endocrine system and the nervous system. It is most commonly believed that complexity in the behavior of an organism is correlated to the complexity of its nervous system. Generally, organisms with more complex nervous systems have a greater capacity to learn new responses and thus adjust their behavior.\n\nConsumers behavior\n\nConsumer behavior refers to the processes consumers go through, and reactions they have towards products or services (Dowhan, 2013). It is to do with consumption, and the processes consumers go through around purchasing and consuming goods and services (Szwacka-Mokrzycka, 2015). Consumers recognise needs or wants, and go through a process to satisfy these needs. Consumer behavior is the process they go through as customers, which includes types of products purchased, amount spent, frequency of purchases and what influences them to make the purchase decision or not. There is a lot that influences consumer behavior, with contributions from both internal and external factors (Szwacka-Mokrzycka, 2015). Internal factors include attitudes, needs, motives, preferences and perceptual processes, whilst external factors include marketing activities, social and economic factors, and cultural aspects (Szwacka-Mokrzycka, 2015). Doctor Lars Perner of the University of Southern California claims that there are also physical factors that influence consumer behavior, for example if a consumer is hungry, then this physical feeling of hunger will influence them so that they go and purchase a sandwich to satisfy the hunger (Perner, 2008).\n\nConsumer decision making\n\nThere is a model described by Lars Perner which illustrates the decision making process with regards to consumer behavior. It begins with recognition of a problem, the consumer recognises a need or want which has not been satisfied. This leads the consumer to search for information, if it is a low involvement product then the search will be internal, identifying alternatives purely from memory. If the product is high involvement then the search be more thorough, such as reading reviews or reports or asking friends. The consumer will then evaluate his or her alternatives, comparing price, quality, doing trade-offs between products and narrowing down the choice by eliminating the less appealing products until there is one left. After this has been identified, the consumer will purchase the product. Finally the consumer will evaluate the purchase decision, and the purchased product, bringing in factors such as value for money, quality of goods and purchase experience (Model taken from Perner, 2008).\n\nHow the 4P's influence consumer behavior\n\nThe 4 P's are a marketing tool, and stand for Price, Promotion, Product and Place or Product Placement (Clemons, 2008). Consumer behavior is influenced greatly by business to consumer marketing, so being a prominent marketing tool, the 4 P's will have an effect on consumer's behavior. The price of a good or service is largely determined by the market, as businesses will set their prices to be similar to that of other business so as to remain competitive whilst making a profit (Clemons, 2008). When market prices for a product are high, it will cause consumers to purchase less and use purchased goods for longer periods of time, meaning they are purchasing the product less often. Alternatively, when market prices for a product are low, consumers are more likely to purchase more of the product, and more often. The way that promotion influences consumer behavior has changed over time. In the past, large promotional campaigns and lots of advertising would convert into sales for a business, but nowadays businesses can have success on products with little or no advertising (Clemons, 2008). This is due to the internet, and in particular social media. They rely on word of mouth from consumers using social media, and as products trend online, so sales increase as products effectively promote themselves (Clemons, 2008). Thus, promotion by businesses does not necessarily result in consumer behavior trending towards purchasing products. The way that product influences consumer behavior is through consumer willingness to pay, and consumer preferences (Clemons, 2008). This means that even if a company were to have a long history of products in the market, consumers will still pick a cheaper product over the company in question's product if it means they will pay less for something that is very similar (Clemons, 2008). This is due to consumer willingness to pay, or their willingness to part with their money they have earned. Product also influences consumer behavior through customer preferences. For example, take Pepsi vs Coca-Cola, a Pepsi drinker is less likely to purchase Coca-Cola, even if it is cheaper and more convenient. This is due to the preference of the consumer, and no matter how hard the opposing company tries they will not be able to force the customer to change their mind. Product placement in the modern era has little influence on consumer behavior, due to the availability of goods online (Clemons, 2008). If a customer can purchase a good from the comfort of their home instead of purchasing in-store, then the placement of products is not going to influence their purchase decision.\n\nBehavior outside of psychology includes:\n\nIn management, behaviors are associated with desired or undesired focuses. Managers generally note what the desired outcome is, but behavioral patterns can take over. These patterns are the reference to how often the desired behavior actually occurs. Before a behavior actually occurs, antecedents focus on the stimuli that influence the behavior that is about to happen. After the behavior occurs, consequences fall into place. Consequences consist of rewards or punishments.\n\nBehavior informatics also called behavior computing, explores behavior intelligence and behavior insights from the informatics and computing perspectives.\n\nHealth behavior refers to a person's beliefs and actions regarding their health and well-being. Health behaviors are direct factors in maintaining a healthy lifestyle. Health behaviors are influenced by the social, cultural and physical environments in which we live and work. They are shaped by individual choices and external constraints. Positive behaviors help promote health and prevent disease, while the opposite is true for risk behaviors. Health behaviors are early indicators of population health. Because of the time lag that often occurs between certain behaviors and the development of disease, these indicators may foreshadow the future burdens and benefits of health-risk and health-promoting behaviors. Health behaviors do not occur in isolation—they are influenced and constrained by social and cultural norms.\n\nA variety of studies have examined the relationship between health behaviors and health outcomes (e.g., Blaxter 1990) and have demonstrated their role in both morbidity and mortality.\n\nThese studies have identified seven features of lifestyle which were associated with lower morbidity and higher subsequent long-term survival (Belloc and Breslow 1972):\n\nHealth behaviors impact upon individuals' quality of life, by delaying the onset of chronic disease and extending active lifespan. Smoking, alcohol consumption, diet, gaps in primary care services and low screening uptake are all significant determinants of poor health, and changing such behaviors should lead to improved health.\nFor example, in USA, Healthy People 2000, United States Department of Health and Human Services (USDHHS), lists increased physical activity, changes in nutrition and reductions in tobacco, alcohol and drug use as important for health promotion and disease prevention.\n\nAny interventions done are matched with the needs of each individual in an ethical and respected manner. HBM encourages increasing individuals' perceived susceptibility to negative health outcomes and making individuals aware of the severity of such negative health behavior outcomes. E.g. through health promotion messages. In addition, the HBM suggests the need to focus on the benefits of health behaviors and the fact that barriers to action are easily overcome. The theory of planned behavior (TPB) suggests using persuasive messages for tackling behavioral beliefs to increase the readiness to perform a behavior, called \"intentions\". TPB advocates the need to tackle normative beliefs and control beliefs in any attempt to change behavior. Challenging the normative beliefs isn't enough but to follow through the \"intention\" with self-efficacy from individual's mastery in problem solving and task completion is important to bring about a positive change. Self efficacy is often cemented through standard persuasive techniques.\n\n\n\n"}
{"id": "34966304", "url": "https://en.wikipedia.org/wiki?curid=34966304", "title": "Beneficence (ethics)", "text": "Beneficence (ethics)\n\nBeneficence is a concept in research ethics which states that researchers should have the welfare of the research participant as a goal of any clinical trial or other research study. The antonym of this term, maleficence, describes a practice which opposes the welfare of any research participant.\n\nThe concept that medical professionals and researchers would always practice beneficence seems natural to most patients and research participants, but in fact, every health intervention or research intervention has potential to harm the recipient. There are many different precedents in medicine and research for conducting a cost–benefit analysis and judging whether a certain action would be a sufficient practice of beneficence, and the extent to which treatments are acceptable or unacceptable is under debate.\n\nDespite differences in opinion, there are many concepts on which there is wide agreement. One is that there should be community consensus when determining best practices for dealing with ethical problems.\n\nThese four concepts often arise in discussions about beneficence:\n\nOrdinary moral discourse and most philosophical systems state that a prohibition on doing harm to others as in #1 is more compelling than any duty to benefit others as in #2–4. This makes the concept of \"first do no harm\" different from the other aspects of beneficence. One example illustrating this concept is the trolley problem.\n\nMorality and ethical theory allows for judging relative costs, so in the case when a harm to be inflicted in violating #1 is negligible and the harm prevented or benefit gained in #2–4 is substantial, then it may be acceptable to cause one harm to gain another benefit. Academic literature discusses different variations of such scenarios. There is no objective evidence which dictates the best course of action when health professionals and researchers disagree about the best course of action for participants except that most people agree that the discussions about ethics should happen.\n\nSome outstanding problems in discussing beneficence occur repeatedly. Researchers often describe these problems in the following categories:\n\nMany people share the view that when it is trivial to do so, people should help each other. The situation becomes more complicated when one person can help another by making various degrees of personal sacrifice.\n\nResearchers should apply the concept of beneficence to individuals within the patient/physician relationship or the research-participant/researcher relationship. However, there is debate about the extent to which the interests of other parties, such as future patients and endangered persons, ought to be considered. When a researcher risks harm to a willing volunteer to do research with the intent to develop knowledge which will better humanity, this may be a practice of beneficence.\n\n\n"}
{"id": "8349310", "url": "https://en.wikipedia.org/wiki?curid=8349310", "title": "Biangular coordinates", "text": "Biangular coordinates\n\nIn mathematics, biangular coordinates are a coordinate system for the plane where formula_1 and formula_2 are two fixed points, and the position of a point \"P\" not on the line formula_3 is determined by the angles formula_4 and formula_5 .\n\n"}
{"id": "941613", "url": "https://en.wikipedia.org/wiki?curid=941613", "title": "Binding problem", "text": "Binding problem\n\nThe binding problem is a term used at the interface between neuroscience, cognitive science and philosophy of mind that has multiple meanings.\n\nFirstly, there is the segregation problem: a practical computational problem of how brains segregate elements in complex patterns of sensory input so that they are allocated to discrete \"objects\". In other words, when looking at a blue square and a yellow circle, what neural mechanisms ensure that the square is perceived as blue and the circle as yellow, and not vice versa? The segregation problem is sometimes called BP1.\n\nSecondly, there is the combination problem:The problem of how objects, background and abstract or emotional features are combined into a single experience. The combination problem is sometimes called BP2.\n\nHowever, the difference between these two problems is not always clear. Moreover, the historical literature is often ambiguous as to whether it is addressing the segregation or the combination problem.\n\nThe segregation problem is the problem of how brains segregate elements in complex patterns of sensory input so that they are allocated to discrete \"objects\".\n\nSmythies defined BP1 in these terms: \"How is the representation of information built up in the neural networks that there is one single object 'out there' and not a mere collection of separate shapes, colours and movements?\" Revonsuo refers to this as the problem of \"stimulus-related binding\" – of sorting stimuli. Although usually referred to as a problem of binding, the computational problem is arguably one of discrimination. Thus, in the words of Canales et al.: \"to bind together all the features of one object and segregate them from features of other objects and the background\". Bartels and Zeki describe it as \"determining that it is the same (or a different) stimulus which is activating different cells in a given visual area or in different visual areas\".\n\nMost experimental work is on vision, where it is known that humans and other mammals process different aspects of perception by separating information about those aspects and processing them in distinct regions of the brain. For example, Bartels and Zeki have shown that different areas in the visual cortex specialize in processing the different aspects of colour, motion, and shape. This type of modular coding has been claimed to yield a potential for ambiguity. When humans view a scene containing a blue square and a yellow circle, some neurons signal in response to blue, others signal in response to yellow, still others to a square shape or a circle shape. Here, the binding problem is the issue of how the brain correctly pairs colour and shape, i.e. indicates that blue goes with square, rather than yellow.\n\nA popular hypothesis perhaps first suggested by Milner has been that features of individual objects are bound/segregated via synchronisation of the activity of different neurons in the cortex. The theory is that when two feature-neurons fire synchronously they are bound, while when they fire out of synchrony they are unbound. Empirical testing of the idea was given impetus when von der Malsburg proposed that feature binding posed a special problem that could not be covered simply by cellular firing rates. A number of studies suggested that there is indeed a relationship between rhythmic synchronous firing and feature binding. This rhythmic firing appears to be linked to intrinsic oscillations in neuronal somatic potentials, typically in the gamma range close to 40 Hz. However, Thiele and Stoner found that perceptual binding of two moving patterns had no effect on synchronisation of the neurons responding to the two patterns. In the primary visual cortex, Dong et al. found that whether two neurons were responding to contours of the same shape or different shapes had no effect on neural synchrony. Revonsuo reports similar negative findings.\n\nThe positive arguments for a role for rhythmic synchrony in resolving the segregational object-feature binding problem (BP1) have been summarized by Singer. There is certainly extensive evidence for synchronization of neural firing as part of responses to visual stimuli. However, there is inconsistency between findings from different laboratories. Moreover, a number of recent reviewers, including Shadlen and Movshon and Merker have raised concerns.\n\nShadlen and Movshon, raise a series of doubts about both the theoretical and the empirical basis for the idea of segregational binding by temporal synchrony. Firstly, it is not clear that binding does pose a special computational problem of the sort proposed by von der Malsburg. Secondly, it is unclear how synchrony would come to play a distinct role in terms of local computational logic. Thirdly, it is difficult to envisage a situation in which pre-synaptic firing rate and synchrony could be usefully interpreted independently by a post-synaptic cell, since the two are interdependent over plausible time scales.\n\nAnother point that has been raised is that within standard time frames for neuronal firing very few distinct phases of synchrony would be distinguishable even under optimal conditions. However, this would only be significant if the same pathways are potentially fed spike (signal) trains in multiple phases. In contrast, Seth describes an artificial brain-based robot that demonstrates multiple, separate, widely distributed neural circuits, firing at different phases, suggesting that synchrony may assist the establishment of discrete object-related re-entrant circuits in a system exposed to randomly timed stimuli.\n\nGoldfarb and Treisman point out that a logical problem appears to arise for binding solely via synchrony if there are several objects that share some of their features and not others. When viewing a display of variously coloured letters, internal representation of a red X, a green O, a red O and a green X cannot be accounted for purely by synchrony of signals for red and X shape, for instance. At best synchrony can facilitate segregation supported by other means (as von der Malsburg acknowledges).\n\nA number of neuropsychological studies suggest that the association of colour, shape and movement as \"features of an object\" is not simply a matter of linking or \"binding\". Purves and Lotto give extensive evidence for top-down feedback signals that ensure that sensory data are handled as features of (sometimes wrongly) postulated objects early in processing. In many illusions data appear as if pre-consciously adjusted in accordance with object expectations. Pylyshyn has also emphasized the way the brain seems to pre-conceive objects to which features are to be allocated and which are attributed continuing existence even if features, like color change.\n\nIn her feature integration theory, Treisman suggested that binding between features is mediated by the features' links to a common location. Psychophysical demonstrations of binding failures under conditions of full attention provide support for the idea that binding is accomplished through common location tags.\nAn implication of these approaches is that sensory data such as colour or motion may not normally exist in \"unallocated\" form. For Merker: \"The 'red' of a red ball does not float disembodied in an abstract color space in V4.\" If colour information allocated to a point in the visual field is converted directly, via the instantiation of some form of propositional logic (analogous to that used in computer design) into colour information allocated to an \"object identity\" postulated by a top-down signal as suggested by Purves and Lotto, (e.g. There is blue here + Object 1 is here = Object 1 is blue) no special computational task of \"binding together\" by means such as synchrony may exist. (Although Von der Malsburg poses the problem in terms of binding \"propositions\" such as \"triangle\" and \"top\", these, in isolation, are not propositional.)\n\nHow signals in the brain come to have propositional content, or meaning, is a much larger issue. However, both Marr and Barlow suggested, on the basis of what was known about neural connectivity in the 1970s that the final integration of features into a percept would be expected to resemble the way words operate in sentences.\n\nThe role of synchrony in segregational binding remains controversial. Merker has recently suggested that synchrony may be a feature of areas of activation in the brain that relates to an \"infrastructural\" feature of the computational system analogous to increased oxygen demand indicated via MRI. Apparent specific correlations with segregational tasks may be explainable on the basis of interconnectivity of the areas involved. As a possible manifestation of a need to balance excitation and inhibition over time it might be expected to be associated with reciprocal re-entrant circuits as in the model of Seth et al. (Merker gives the analogy of the whistle from an audio amplifier receiving its own output.)\n\nIf it transpires that synchronized activity plays at most an infrastructural role in segregative computational \"binding\", the question arises as to whether we need another explanation. The implication both of Shadlen and Movshon's and of Merker's analyses seems to be that there may be no special binding problem in this sense. The problem may be merely an integral part of the more general problem of the computational logic used by neurons, or what is often referred to as the \"neural code\". In particular it may be inappropriate to analyse binding in perception without taking into account the way features are bound in memory, as addressed by Zimmer and colleagues, and how that informs the way the brain pre-conceives objects.\n\nSmythies defines BP2 as \"How do the brain mechanisms actually construct the phenomenal object?\". Revonsuo equates this to \"consciousness-related binding\", emphasizing the entailment of a phenomenal aspect. As Revonsuo explores in 2006, there are nuances of difference beyond the basic BP1:BP2 division. Smythies speaks of constructing a phenomenal object (\"local unity\" for Revonsuo) but philosophers such as Descartes, Leibniz, Kant and James (see Brook and Raymont) have typically been concerned with the broader unity of a phenomenal experience (\"global unity\" for Revonsuo) – which, as Bayne illustrates may involve features as diverse as seeing a book, hearing a tune and feeling an emotion. Further discussion will focus on this more general problem of how sensory data that may have been segregated into, for instance, \"blue square\" and \"yellow circle\" are to be re-combined into a single phenomenal experience of a blue square next to a yellow circle, plus all other features of their context. There are a wide range of views on just how real this \"unity\" is, but the existence of medical conditions in which it appears to be subjectively impaired, or at least restricted, suggests that it is not entirely illusory.\n\nEarly philosophers such as Descartes and Leibniz noted that the apparent unity of our experience is an all-or-none qualitative characteristic that does not appear to have an equivalent in the known quantitative features, like proximity or cohesion, of composite matter. William James, in the nineteenth century, considered the ways the unity of consciousness might be explained by known physics and found no satisfactory answer. He coined the term \"combination problem\", in the specific context of a \"mind-dust theory\" in which it is proposed that a full human conscious experience is built up from proto- or micro-experiences in the way that matter is built up from atoms. James claimed that such a theory was incoherent, since no causal physical account could be given of how distributed proto-experiences would \"combine\". He favoured instead a concept of \"co-consciousness\" in which there is one \"experience of A, B and C\" rather than combined experiences. A detailed discussion of subsequent philosophical positions is given by Brook and Raymont (see 26). However, these do not generally include physical interpretations. James remained concerned about the absence of a \"single physical thing\", other than an atom, that could be co-conscious (of A, B and C), echoing Leibniz.\n\nWhitehead proposed a fundamental ontological basis for a relation consistent with James's idea of co-consciousness, in which many causal elements are co-available or \"compresent\" in a single event or \"occasion\" that constitutes a unified experience. Whitehead did not give physical specifics but the idea of compresence is framed in terms of causal convergence in a local interaction consistent with physics. Where Whitehead goes beyond anything formally recognized in physics is in the \"chunking\" of causal relations into complex but discrete \"occasions\". Even if such occasions can be defined, Whitehead's approach still leaves James's difficulty with finding a site, or sites, of causal convergence that would make neurobiological sense for \"co-consciousness\". Sites of signal convergence do clearly exist throughout the brain but there is a concern to avoid re-inventing what Dennett calls a Cartesian Theater or single central site of convergence of the form that Descartes proposed.\n\nDescartes's central \"soul\" is now rejected because neural activity closely correlated with conscious perception is widely distributed throughout the cortex. The remaining choices appear to be either separate involvement of multiple distributed causally convergent events or a model that does not tie a phenomenal experience to any specific local physical event but rather to some overall \"functional\" capacity. Whichever interpretation is taken, as Revonsuo indicates, there is no consensus on what structural level we are dealing with – whether the cellular level, that of cellular groups as \"nodes\", \"complexes\" or \"assemblies\" or that of widely distributed networks. There is probably only general agreement that it is not the level of the whole brain, since there is evidence that signals in certain primary sensory areas, such as the V1 region of the visual cortex (in addition to motor areas and cerebellum), do not contribute directly to phenomenal experience.\n\nDennett has proposed that our sense that our experiences are single events is illusory and that, instead, at any one time there are \"multiple drafts\" of sensory patterns at multiple sites. Each would only cover a fragment of what we think we experience. Arguably, Dennett is claiming that consciousness is not unified and there is no phenomenal binding problem. Most philosophers have difficulty with this position (see Bayne). Dennett's view might be in keeping with evidence from recall experiments and change blindness purporting to show that our experiences are much less rich than we sense them to be – what has been called the Grand Illusion. However, few, if any, other authors suggest the existence of multiple partial \"drafts\". Moreover, also on the basis of recall experiments, Lamme has challenged the idea that richness is illusory, emphasizing that phenomenal content cannot be equated with content to which there is cognitive access.\n\nDennett does not tie drafts to biophysical events. Multiple sites of causal convergence are invoked in specific biophysical terms by Edwards and Sevush. In this view the sensory signals to be combined in phenomenal experience are available, in full, at each of multiple sites. To avoid non-causal combination each site/event is placed within an individual neuronal dendritic tree. The advantage is that \"compresence\" is invoked just where convergence occurs neuro-anatomically. The disadvantage, as for Dennett, is the counter-intuitive concept of multiple \"copies\" of experience. The precise nature of an experiential event or \"occasion\", even if local, also remains uncertain.\n\nThe majority of theoretical frameworks for the unified richness of phenomenal experience adhere to the intuitive idea that experience exists as a single copy, and draw on \"functional\" descriptions of distributed networks of cells. Baars has suggested that certain signals, encoding what we experience, enter a \"Global Workspace\" within which they are \"broadcast\" to many sites in the cortex for parallel processing. Dehaene, Changeux and colleagues have developed a detailed neuro-anatomical version of such a workspace. Tononi and colleagues have suggested that the level of richness of an experience is determined by the narrowest information interface \"bottleneck\" in the largest sub-network or \"complex\" that acts as an integrated functional unit. Lamme has suggested that networks supporting reciprocal signaling rather than those merely involved in feed-forward signaling support experience. Edelman and colleagues have also emphasized the importance of re-entrant signaling. Cleeremans emphasizes meta-representation as the functional signature of signals contributing to consciousness.\n\nIn general, such network-based theories are not explicitly theories of how consciousness is unified, or \"bound\" but rather theories of functional domains within which signals contribute to unified conscious experience. A concern about functional domains is what Rosenberg has called the boundary problem; it is hard to find a unique account of what is to be included and what excluded. Nevertheless, this is, if anything is, the consensus approach.\n\nWithin the network context, a role for synchrony has been invoked as a solution to the phenomenal binding problem as well as the computational one. In his book, The Astonishing Hypothesis, Crick appears to be offering a solution to BP2 as much as BP1. Even von der Malsburg, introduces detailed computational arguments about object feature binding with remarks about a \"psychological moment\". The Singer group also appear to be interested as much in the role of synchrony in phenomenal awareness as in computational segregation.\n\nThe apparent incompatibility of using synchrony to both segregate and unify might be explained by sequential roles. However, Merker points out what appears to be a contradiction in attempts to solve the phenomenal unification problem (BP2) in terms of a functional (effectively meaning computational) rather than a local biophysical, domain, in the context of synchrony.\n\nFunctional arguments for a role for synchrony are in fact underpinned by analysis of local biophysical events. However, Merker points out that the explanatory work is done by the downstream integration of synchronized signals in post-synaptic neurons: \"It is, however, by no means clear what is to be understood by 'binding by synchrony' other than the threshold advantage conferred by synchrony at, and only at, sites of axonal convergence onto single dendritic trees...\" In other words, although synchrony is proposed as a way of explaining binding on a distributed, rather than a convergent, basis the justification rests on what happens at convergence. Signals for two features are proposed as bound by synchrony because synchrony effects downstream convergent interaction. Any theory of phenomenal binding based on this sort of computational function would seem to follow the same principle. The phenomenality would entail convergence, if the computational function does.\n\nAlthough BP1 and BP2 are different, this need not invalidate the assumption, implicit in many of the quoted models, that computational and phenomenal events, at least at some point in the sequence of events, parallel each other in some way. The difficulty remains in identifying what that way might be. Merker's analysis suggests that either (1) both computational and phenomenal aspects of binding are determined by convergence of signals on neuronal dendritic trees, or (2) that our intuitive ideas about the need for \"binding\" in a \"holding together\" sense in both computational and phenomenal contexts are misconceived. We may be looking for something extra that is not needed. Merker, for instance, argues that the homotopic connectivity of sensory pathways does the necessary work.\n\nThe nature of, and solution to, BP2 remains a matter of controversy.\n\nA new study published in Psychological Review and \"Interface Focus\" 2018, the Royal Society's cross-disciplinary journal at the interface between the physical and life sciences, sheds new light on how the visual system may represent which features are bound together as part of the same object. \n\nThe research team, the Oxford Centre for Theoretical Neuroscience and Artificial Intelligence , led by Dr Simon Stringer from University of Oxford, performed bio-inspired Spiking Neural Network simulations of the primate ventral visual system to address this enduring question. By training the biophysical model on a set of visual stimuli, has been observed the emergence of a subpopulation of neurons, called \"polychronous neuronal groups\" (PNG), that exhibits regularly repeating spatio-temporal patterns of spikes. The underlying phenomenon of such spatio-temporal patterns responses is known as polychronization. The main point of this proposed novel approach is that within these PNGs exist neurons, called \"binding neurons\", that learn to represent the hierarchical binding relationships between lower and higher level visual features at every spatial scale and across the entire visual field. These binding neurons have been firstly formulated by Christoph von der Malsburg , however, it has not previously been shown how these neurons may develop naturally through a biologically plausible process of visually guided learning and self-organization of polychronous neuronal groups. This latest research describe that such binding neurons emerge automatically within the PNGs during visually training when key properties of the visual cortex are incorporated into the model. This finding is consistent with the hierarchical nature of primate vision depicted by John Duncan & Glyn W. Humphreys:\"A fully hierarchical representation is created by repeating segmentation at different levels of scale. Each structural unit, contained by its own boundary, is further subdivided into parts by the major boundaries within it. Thus, a human body may be subdivided into head, torso, and limbs, and a hand into palm and fingers. Such subdivision serves two purposes. The description of a structural unit at one level of scale (animal, letter, etc.) must depend heavily on the relations between the parts defined within it (as well as on properties such as colour or movement that may be common to the parts). Then, at the next level down, each part becomes a new structural unit to be further described with its own properties, defined among other things by the relations between its own subparts. At the top of the hierarchy may be a structural unit corresponding to the whole input scene, described with a rough set of properties (e.g. division into light sky above and dark ground below).\"\n\nMoreover, this hierarchy feature binding theory proposes that information about visual features at every spatial scale, including the binding relations between these features, would be projected upwards to the higher layers of the network, where spatial information would be available for readout by later brain systems to guide behavior. This mechanism has been called the \"holographic principle\". Lastly, by representing the hierarchical binding relationships between visual features at every spatial scale across a visual scene, these kinds of binding neurons could underpin visual consciousness itself, the capacity of the visual brain to perceive and make sense of its visuospatial world. Therefore, this work may represent a significant advancement towards the future development of Artificial General Intelligence and Machine Consciousness, opening up new perspectives on building machines endowed with human-level intelligence.\n\n"}
{"id": "23497374", "url": "https://en.wikipedia.org/wiki?curid=23497374", "title": "Bossnapping", "text": "Bossnapping\n\nBossnapping is a form of lock-in where employees detain management in the workplace, often in protest against lay-offs and redundancies, and has especially been carried out in France. The term gained wide usage in the media following a series of bossnapping incidents in the spring of 2009 in France where workers used the tactic in the context of widespread labor unrest resulting from the late 2000s recession.\n\nThese incidents resulted in a public call for an end to the practice by French President Nicolas Sarkozy, despite public opinion polling which showed widespread sympathy for the protesters.\n\nThe term 'bossnapping' began receiving widespread use in the media following a series of high-profile incidents in France in the spring of 2009 where managers were detained by their workers. In early March, 2009, workers in southwestern France held the Chief Executive Officer and Human Resources Director of Sony France overnight, demanding a better severance package for workers who had been laid off.\n\nThey barricaded the entrance to the facility with tree trunks, and held their hostages until the CEO agreed to a renegotiation of the severance package that laid-off workers were to receive. Later that month, workers at a pharmaceutical factory owned by 3M in Pithiviers held their boss in his office demanding similar concessions for laid-off workers as well as protections for remaining workers whose jobs had not been cut. The workers claimed their actions were not intended to be aggressive, but were rather their \"only currency.\" French police did not intervene in either the Sony or 3M incidents, in the expectation that each would be resolved peacefully, as they were in the end.\n\nWorkers in the 3M incident provided a dinner of mussels and French Fries to their kidnapped boss while he was being held.\n\nIn May 2010, workers of a Caterpillar Inc. plant in Grenoble took five managers as hostages to enforce negotiations about 733 job losses. They ended their doing after 10 hours when policemen began to record the names of the participating people.\n\nDuring the same month, about 300 workers of a Toyota factory in Onnaing (Northern France) blocked all entrances and hindered all trucks to leave the premises.\n\nFurther bossnappings took place in a worksite of Hewlett-Packard in France and a lock-in of managers occurred at market research firm Synovate in Auckland, New Zealand as a part of a labor dispute during a contract renegotiation there. The wave of high-profile incidents led to publication of advice for managers who might be bossnapped, although news reports made clear that the hostages had been treated well by their bossnappers. This advice included the preparation of special bossnapping kits that included a change of clothes and a cell phone pre-programmed with numbers of family members, police, and a psychologist who might assist with the psychological stress caused by being the hostage in such an incident.\n\nThese incidents took place in the context of wider French labor unrest related to the recession of the late 2000s, in which many companies in France and across the globe laid off workers during restructuring the companies did in response to falling profits or rising losses. Other labor incidents in France during this period included one in which laid-off workers threw eggs at their manager and burned effigies of him and another in which workers at a car parts factory threatened to blow it up after the factory was closed completely.\n\nIn April, 2009, in response to the ongoing series of bossnapping incidents, French President Nicolas Sarkozy pledged to end the practice, saying, \"We are a nation of laws. I won't allow this sort of thing.\" However, public opinion polls in France at the time showed significant support for those using the strategy, a majority who disapproved of the tactic but sympathized with those practicing it, and only a small minority who completely opposed it. Thus, observers suggested that any strong action on the part of Sarkozy government to end bossnappings, included more proactive action by police to rescue hostages and arrest bossnappers, might lead to further unrest in other forms.\n\n"}
{"id": "3430211", "url": "https://en.wikipedia.org/wiki?curid=3430211", "title": "Calculus of voting", "text": "Calculus of voting\n\nCalculus of voting refers to any mathematical model which predicts voting behaviour by an electorate, including such features as participation rate. A calculus of voting represents an hypothesized decision-making process.\n\nThese models are used in political science in an attempt to capture the relative importance of various factors influencing an elector to vote (or not vote) in a particular way.\n\nOne such model was proposed by Anthony Downs (1957) and is adapted by William H. Riker and Peter Ordeshook, in “A Theory of the Calculus of Voting” (Riker and Ordeshook 1968)\n\nwhere\n\nA political science model based on rational choice used to explain why citizens do or do not vote.\nThe alternative equation is\n\nWhere for voting to occur the (P)robability the vote will matter \"times\" the (B)enefit of one candidate winning over another combined with the feeling of civic (D)uty, must be greater than the (C)ost of voting\n\n"}
{"id": "24383048", "url": "https://en.wikipedia.org/wiki?curid=24383048", "title": "Cherenkov radiation", "text": "Cherenkov radiation\n\nCherenkov radiation (pronunciation: /tʃɛrɛnˈkɔv/) is an electromagnetic radiation emitted when a charged particle (such as an electron) passes through a dielectric medium at a speed greater than the phase velocity of light in that medium. It is also known as the Vavilov–Cherenkov radiation (VCR) (named after Sergey Vavilov and Pavel Cherenkov). It is named after the Soviet scientist Pavel Cherenkov, the 1958 Nobel Prize winner who was the first to detect it experimentally. A theory of this effect was later developed within the framework of Einstein's special relativity theory by Igor Tamm and Ilya Frank, who also shared the Nobel Prize. Cherenkov radiation had been theoretically predicted by the English polymath Oliver Heaviside in papers published in 1888–89.\n\nThe characteristic blue glow of an underwater nuclear reactor is due to Cherenkov radiation.\n\nWhile electrodynamics holds that the speed of light \"in a vacuum\" is a universal constant (\"c\"), the speed at which light propagates in a material may be significantly less than \"c\". For example, the speed of the propagation of light in water is only 0.75\"c\". Matter can be accelerated beyond this speed (although still to less than \"c\") during nuclear reactions and in particle accelerators. Cherenkov radiation results when a charged particle, most commonly an electron, travels through a dielectric (electrically polarizable) medium with a speed greater than that at which light propagates in the same medium.\n\nMoreover, the velocity that must be exceeded is the phase velocity of light rather than the group velocity of light. The phase velocity can be altered dramatically by employing a periodic medium, and in that case one can even achieve Cherenkov radiation with \"no\" minimum particle velocity, a phenomenon known as the Smith–Purcell effect. In a more complex periodic medium, such as a photonic crystal, one can also obtain a variety of other anomalous Cherenkov effects, such as radiation in a backwards direction (whereas ordinary Cherenkov radiation forms an acute angle with the particle velocity).\n\nIn their original work on the theoretical foundations of Cherenkov radiation, Tamm and Frank wrote,\"This peculiar radiation can evidently not be explained by any common mechanism such as the interaction of the fast electron with individual atom or as radiative scattering of electrons on atomic nuclei. On the other hand, the phenomenon can be explained both qualitatively and quantitatively if one takes in account the fact that an electron moving in a medium does radiate light even if it is moving uniformly provided that its velocity is greater\nthan the velocity of light in the medium.\". However, some misconceptions regarding Cherenkov radiation exist: for example, it is believed that the medium becomes electrically polarized by the particle's electric field. If the particle travels slowly then the disturbance elastically relaxes back to mechanical equilibrium as the particle passes. When the particle is traveling fast enough, however, the limited response speed of the medium means that a disturbance is left in the wake of the particle, and the energy contained in this disturbance radiates as a coherent shockwave. Such conceptions do not have any analytical foundation, as electromagnetic radiation is emitted when charged particles move in a dielectric medium at subluminal velocities which are not considered as Cherenkov radiation.\n\nA common analogy is the sonic boom of a supersonic aircraft or bullet. The sound waves generated by the supersonic body propagate at the speed of sound itself; as such, the waves travel slower than the speeding object and cannot propagate forward from the body, instead forming a shock front. In a similar way, a charged particle can generate a light shock wave as it travels through an insulator.\n\nIn the figure, the particle (red arrow) travels in a medium with speed formula_1 such that formula_2, where formula_3 is speed of light in vacuum, and formula_4 is the refractive index of the medium. (If the medium is water, the condition is formula_5, since formula_6 for water at 20 °C.)\n\nWe define the ratio between the speed of the particle and the speed of light as formula_7. The emitted light waves (blue arrows) travel at speed formula_8.\n\nThe left corner of the triangle represents the location of the superluminal particle at some initial moment (\"t\"=0). The right corner of the triangle is the location of the particle at some later time t. In the given time \"t\", the particle travels the distance\n\nwhereas the emitted electromagnetic waves are constricted to travel the distance\n\nSo:\n\nNote that since this ratio is independent of time, one can take arbitrary times and achieve similar triangles. The angle stays the same, meaning that subsequent waves generated between the initial time \"t\"=0 and final time \"t\" will form similar triangles with coinciding right endpoints to the one shown.\n\nA reverse Cherenkov effect can be experienced using materials called negative-index metamaterials (materials with a subwavelength microstructure that gives them an effective \"average\" property very different from their constituent materials, in this case having negative permittivity and negative permeability). This means, when a charged particle (usually electrons) passes through a medium at a speed greater than the phase velocity of light in that medium, that particle will emit trailing radiation from its progress through the medium rather than in front of it (as is the case in normal materials with, both permittivity and permeability positive). One can also obtain such reverse-cone Cherenkov radiation in non-metamaterial periodic media (where the periodic structure is on the same scale as the wavelength, so it cannot be treated as an effectively homogeneous metamaterial).\n\nCherenkov radiation can also radiate in an arbitrary direction using properly engineered one dimensional metamaterials. The latter is designed to introduce a gradient of phase retardation along the trajectory of the fast travelling particle (formula_12 ), reversing or steering Cherenkov emission at arbitrary angles given by the generalized relation:\n\nThe frequency spectrum of Cherenkov radiation by a particle is given by the Frank–Tamm formula:\n\nformula_14\n\nThe Frank-Tamm formula describes the amount of energy emitted from Cherenkov radiation, per wavelength, per unit length traveled. formula_15 is the permeability and formula_16 is the index of refraction of the material the charge particle moves through.\n\nUnlike fluorescence or emission spectra that have characteristic spectral peaks, Cherenkov radiation is continuous. Around the visible spectrum, the relative intensity per unit frequency is approximately proportional to the frequency. That is, higher frequencies (shorter wavelengths) are more intense in Cherenkov radiation. This is why visible Cherenkov radiation is observed to be brilliant blue. In fact, most Cherenkov radiation is in the ultraviolet spectrum—it is only with sufficiently accelerated charges that it even becomes visible; the sensitivity of the human eye peaks at green, and is very low in the violet portion of the spectrum.\n\nThere is a cut-off frequency above which the equation formula_17 can no longer be satisfied. The refractive index formula_4 varies with frequency (and hence with wavelength) in such a way that the intensity cannot continue to increase at ever shorter wavelengths, even for very relativistic particles (where v/c is close to 1). At X-ray frequencies, the refractive index becomes less than unity (note that in media the phase velocity may exceed \"c\" without violating relativity) and hence no X-ray emission (or shorter wavelength emissions such as gamma rays) would be observed. However, X-rays can be generated at special frequencies just below the frequencies corresponding to core electronic transitions in a material, as the index of refraction is often greater than 1 just below a resonant frequency (see Kramers-Kronig relation and anomalous dispersion).\n\nAs in sonic booms and bow shocks, the angle of the shock cone is directly related to the velocity of the disruption. The Cherenkov angle is zero at the threshold velocity for the emission of Cherenkov radiation. The angle takes on a maximum as the particle speed approaches the speed of light. Hence, observed angles of incidence can be used to compute the direction and speed of a Cherenkov radiation-producing charge.\n\nCherenkov radiation can be generated in the eye by charged particles hitting the vitreous humour, giving the impression of flashes, as in cosmic ray visual phenomena and possibly some observations of criticality accidents.\n\nCherenkov radiation is widely used to facilitate the detection of small amounts and low concentrations of biomolecules. Radioactive atoms such as phosphorus-32 are readily introduced into biomolecules by enzymatic and synthetic means and subsequently may be easily detected in small quantities for the purpose of elucidating biological pathways and in characterizing the interaction of biological molecules such as affinity constants and dissociation rates.\n\nMore recently, Cherenkov light has been used to image substances in the body. These discoveries have led to intense interest around the idea of using this light signal to quantify and/or detect radiation in the body, either from internal sources such as injected radiopharmaceuticals or from external beam radiotherapy in oncology. Radioisotopes such as the positron emitters F and N or beta emitters P or Y have measurable Cherenkov emission and isotopes F and I have been imaged in humans for diagnostic value demonstration. External beam radiation therapy has been shown to induce a substantial amount of Cherenkov light in the tissue being treated, due to the photon beam energy levels used in the 6 MeV to 18 MeV ranges. The secondary electrons induced by these high energy x-rays result in the Cherenkov light emission, where the detected signal can be imaged at the entry and exit surfaces of the tissue.\n\nCherenkov radiation is used to detect high-energy charged particles. In pool-type nuclear reactors, beta particles (high-energy electrons) are released as the fission products decay. The glow continues after the chain reaction stops, dimming as the shorter-lived products decay. Similarly, Cherenkov radiation can characterize the remaining radioactivity of spent fuel rods. This phenomenon is used to verify the presence of spent nuclear fuel in spent fuel pools for nuclear safeguards purposes.\n\nWhen a high-energy (TeV) gamma photon or cosmic ray interacts with the Earth's atmosphere, it may produce an electron-positron pair with enormous velocities. The Cherenkov radiation emitted in the atmosphere by these charged particles is used to determine the direction and energy of the cosmic ray or gamma ray, which is used for example in the Imaging Atmospheric Cherenkov Technique (IACT), by experiments such as VERITAS, H.E.S.S., MAGIC. Cherenkov radiation emitted in tanks filled with water by those charged particles reaching earth is used for the same goal by the Extensive Air Shower experiment HAWC, the Pierre Auger Observatory and other projects. Similar methods are used in very large neutrino detectors, such as the Super-Kamiokande, the Sudbury Neutrino Observatory (SNO) and IceCube. Other projects operated in the past applying related techniques, such as STACEE, a former solar tower refurbished to work as a non-imaging Cherenkov observatory, which was located in New Mexico.\n\nAstrophysics observatories using the Cherenkov technique to measure air showers are key to determine the properties of astronomical objects that emit Very High Energy gamma rays, such as supernova remnants and blazars.\n\nCherenkov radiation is commonly used in experimental particle physics for particle identification. One could measure (or put limits on) the velocity of an electrically charged elementary particle by the properties of the Cherenkov light it emits in a certain medium. If the momentum of the particle is measured independently, one could compute the mass of the particle by its momentum and velocity (see four-momentum), and hence identify the particle.\n\nThe simplest type of particle identification device based on a Cherenkov radiation technique is the threshold counter, which gives an answer as to whether the velocity of a charged particle is lower or higher than a certain value (formula_19, where formula_3 is the speed of light, and formula_4 is the refractive index of the medium) by looking at whether this particle does or does not emit Cherenkov light in a certain medium. Knowing particle momentum, one can separate particles lighter than a certain threshold from those heavier than the threshold.\n\nThe most advanced type of a detector is the RICH, or Ring-imaging Cherenkov detector, developed in the 1980s. In a RICH detector, a cone of Cherenkov light is produced when a high speed charged particle traverses a suitable medium, often called radiator. This light cone is detected on a position sensitive planar photon detector, which allows reconstructing a ring or disc, the radius of which is a measure for the Cherenkov emission angle. Both focusing and proximity-focusing detectors are in use. In a focusing RICH detector, the photons are collected by a spherical mirror and focused onto the photon detector placed at the focal plane. The result is a circle with a radius independent of the emission point along the particle track. This scheme is suitable for low refractive index radiators—i.e. gases—due to the larger radiator length needed to create enough photons. In the more compact proximity-focusing design, a thin radiator volume emits a cone of Cherenkov light which traverses a small distance—the proximity gap—and is detected on the photon detector plane. The image is a ring of light, the radius of which is defined by the Cherenkov emission angle and the proximity gap. The ring thickness is determined by the thickness of the radiator. An example of a proximity gap RICH detector is the High Momentum Particle Identification Detector (HMPID), a detector currently under construction for ALICE (A Large Ion Collider Experiment), one of the six experiments at the LHC (Large Hadron Collider) at CERN.\n\nThe Cherenkov effect can occur in vacuum. In a slow-wave structure, the phase velocity decreases and the velocity of charged particles can exceed the phase velocity while remaining lower than formula_3. In such a system, this effect can be derived from conservation of the energy and momentum where the momentum of a photon should be formula_23 (formula_24 is phase constant) rather than the de Broglie relation formula_25. This type of radiation (VCR) is used to generate high power microwaves.\n\n\n\n"}
{"id": "37670783", "url": "https://en.wikipedia.org/wiki?curid=37670783", "title": "Classification of Buddha's teaching", "text": "Classification of Buddha's teaching\n\nThe schools of Mahayana Buddhism developed several different schemes of doctrinal classification (Chinese: jiaoxiang panshi or panjiao [tenet classification]).\n\nThe classification of Buddha teachings is discussed very early by Mahayana Buddhists in China. In 600 AD there were 10 main classifications.\nAnd it continued to develop. Kukai in Japan wrote Himitsumandara jūjūshinron (祕密曼荼羅十住心論, Treatise on The Ten Stages of the Development of Mind) and Enchin also wrote his classification. Tibetan Buddhists also developed different classifications.\n\nZhiyi wrote Mohe Zhiguan (摩訶止觀) and Fahua Xuanyi (法華玄義). He classifies the Buddha's teaching into 5 periods and 8 teachings (五時八教).\nThey are the Period of Avatamsaka (華嚴時), Agamas (阿含時), Vaipulya (方等時), Prajna (般若時) and Dharma-pundarik/Nirvana (法華涅槃時). The Eight Teachings are subdivided into two groups, one is the Fourfold Doctrines of Conversion (化法四教) and the other is the Fourfold Methods of Conversion (化儀四教). They are Tripitaka Teaching (三藏教), Shared Teaching (通教), Distinctive Teaching (別教), Complete Teaching (圓教), Sudden Teaching (頓教), Gradual Teaching (漸教), Secret Teaching (秘密教) and Variable Teaching (不定教).\nTiantai followed this classification.\n\nFazang wrote Huayan Yisheng Jiaoyi Fenqi Zhang (華嚴一乘教義分齊章). It classified Buddhist teachings into 5 Divisions and 10 Schools (五教十宗). They are the teaching of sravakas (sound-hearers) (小乘教), the elementary teaching of mahayana (大乘初教), the final teaching of mahayana (大乘終教), the abrupt or sudden teaching of mahayana (大乘頓教) and the all-round or complete teaching of one vehicle (一乘圓教). The 5 divisions are further separated into 10 schools, namely the reality of self and dharma (mind and matter) (我法俱有宗), the reality of dharma, but not self (法有我無宗), dharmas have neither creation nor destruction (法無去來宗), present dharmas are both apparent and real (現通假實宗), common or phenomenal ideas are wrong, and fundamental reality is the only truth (俗妄真實宗), dharmas are merely names (諸法但有宗), all Dharmas are empty (一切皆空宗), the Bhutatathata is not unreal (真實不空宗), phenomena and their perceptions are to be eradicated (相想俱絕宗) and the perfect, all inclusive and complete teaching of one vehicle (圓明具德宗).\n\nHuayan school followed this classification.\n\nThere are nine yanas in Nyingma teachings.\n"}
{"id": "405980", "url": "https://en.wikipedia.org/wiki?curid=405980", "title": "Cockade", "text": "Cockade\n\nA cockade is a knot of ribbons, or other circular- or oval-shaped symbol of distinctive colours which is usually worn on a hat.\n\nIn the 18th and 19th centuries, coloured cockades were used in Europe to show the allegiance of their wearers to some political faction, their rank, or as part of a servant's livery. Military uniforms would use cockades as well.\n\nA cockade was pinned on the side of a man's tricorne or cocked hat, or on his lapel. Women could also wear it on their hat or in their hair.\n\nIn pre-revolutionary France, the cockade of the Bourbon dynasty was all white. In the Kingdom of Great Britain supporters of a Jacobite restoration wore white cockades, while the recently established Hanoverian monarchy used a black cockade.\n\nDuring the 1780 Gordon Riots in London the blue cockade became a symbol of anti-government feelings and was worn by most of the rioters.\n\nDuring the American Revolution, the Continental Army initially wore cockades of various colors as an \"ad hoc\" form of rank insignia, as General George Washington wrote:\n\nBefore long however, the Continental Army reverted to wearing the black cockade they inherited from the British. Later, when France became an ally of the United States, the Continental Army pinned the white cockade of the French \"Ancien Régime\" onto their old black cockade; the French reciprocally pinned the black cockade onto their white cockade, as a mark of the French-American alliance. The black-and-white cockade thus became known as the \"Union Cockade\".\n\nIn the Storming of the Bastille, Camille Desmoulins initially encouraged the revolutionary crowd to wear green; this colour was later rejected as it was associated with the reactionary Count of Artois. Instead revolutionaries would wear cockades with the traditional colours of the arms of Paris, red and blue. Later the Bourbon white was added to this cockade - thus producing the original \"Tricolore\" cockade. Later, distinctive colours and styles of cockade would indicate the wearer's faction—although the meanings of the various styles were not entirely consistent, and varied somewhat by region and period.\n\nEchoing their use when Americans rebelled against Britain, cockades – usually made with blue ribbons and worn on clothing or hats – were widespread tokens of southern support for secession preceding the American Civil War of 1861–1865.\n\nFrom the 15th century various European monarchy realms used cockades to denote the nationalities of their military. Their origin reverts to the distinctive colored band or ribbon worn by late medieval armies or jousting knights on their arms or headgear to distinguish friend from foe in the field of battle. Ribbon-style cockades were worn later on helmets and brimmed hats or tricornes and bicornes just as the French did, and also on cocked hats and shakoes; metal cockades were worn at the right side of helmets; small button-type cockades were worn at the front of kepis and peaked caps. In addition to the significance of these symbols in denoting loyalty to a particular monarch, the coloured cockade served to provide a common and economical field sign at a time when the colours of uniform coats might vary widely between regiments in a single army.\nDuring the Napoleonic wars, the armies of France and Russia, had the imperial French cockade or the larger cockade of St. George pinned on the front of their shakos.\n\nThe Second German Empire (1870–1918) used two cockades on each army headgear: one (black-white-red) for the empire; the other for one of the monarchies the empire was composed of, which had used their own colors long before. The only exceptions were the Kingdoms of Bavaria and Württemberg, having preserved the right to keep their own armed forces which were not integrated in the Imperial Army. Their only cockades were either white and blue (Bavaria) or black-red-black (Württemberg).\n\nThe Weimar Republic (1919–1933) removed these, as they might promote separatism which would lead to the dissolution of the German nation-state into regional countries again. In the Second World War, the Nazis, who hated the republican German colours black-red-gold used by the Weimar Republic, reintroduced the imperial colours (in German: \"die kaiserlichen Farben\" or \"Reichsfarben\") of black on the outside, then white, and red on the inside were used on all army caps. These colours represented the biggest and the smallest countries of the Reich, Prussia (black and white) and the Hanseatic City States of Hamburg, Bremen and Lübeck (white and red).\n\nFrance began the first Air Service in 1909 and soon picked the traditional French cockade as the first national emblem, now usually termed a roundel, on military aircraft. During World War I, other countries adopted national cockades and used these coloured emblems as roundels on their military aircraft. These designs often bear an additional central device or emblem to further identify national aircraft, those from the French navy bearing a black anchor within the French cockade.\n\nHungarian revolutionaries wore cockades during the Hungarian revolution of 1848 and during the 1956 revolution. Because of this, Hungarians traditionally wear cockades on 15 March.\n\nBelow is a list of national cockades (colors listed from center to ring):\n\n\n\n"}
{"id": "22696673", "url": "https://en.wikipedia.org/wiki?curid=22696673", "title": "Cognitive synonymy", "text": "Cognitive synonymy\n\nCognitive synonymy is a type of synonymy in which synonyms are so similar in meaning that they cannot be differentiated either denotatively or connotatively, that is, not even by mental associations, connotations, emotive responses, and poetic value. It is a stricter (more precise) technical definition of synonymy, specifically for theoretical (e.g., linguistic and philosophical) purposes. In usage employing this definition, synonyms with greater differences are often called near-synonyms rather than synonyms.\n\nIf a word is cognitively synonymous with another word, they refer to the same thing independently of context. Thus, a word is cognitively synonymous with another word if and only if all instances of both words express the same exact thing, and the referents are necessarily identical, which means that the words' interchangeability is not context-sensitive.\n\nWillard Van Orman Quine used the concept of cognitive synonymy extensively in his famous 1951 paper \"Two Dogmas of Empiricism\", where two words were cognitively synonymous if they were interchangeable in every possible instance.\n\nFor example,\n\nQuine notes that if one is referring to the word itself, this doesn't apply, as in,\n\nAs compared to the substitution which is obviously false,\n\n\n"}
{"id": "2079352", "url": "https://en.wikipedia.org/wiki?curid=2079352", "title": "Confusion", "text": "Confusion\n\nConfusion (from Latin \"confusĭo, -ōnis\", from \"confundere\": \"to pour together;\" \"to mingle together;\" \"to confuse\") is the state of being bewildered or unclear in one’s mind about something.\n\nThe term \"acute mental confusion\" is often used interchangeably with delirium in the \"International Statistical Classification of Diseases and Related Health Problems\" and the \"Medical Subject Headings\" publications to describe the pathology. These refer to the loss of orientation, or the ability to place oneself correctly in the world by time, location and personal identity. Mental confusion is sometimes accompanied by disordered consciousness (the loss of linear thinking) and memory loss (the ability to correctly recall previous events or learn new material).\n\nConfusion may result from drug side effects or from a relatively sudden brain dysfunction. Acute confusion is often called delirium (or \"acute confusional state\"), although delirium often includes a much broader array of disorders than simple confusion. These disorders include the inability to focus attention; various impairments in awareness; and temporal or spatial dis-orientation. Mental confusion can result from chronic organic brain pathologies, such as dementia, as well.\n\n\nThe most common causes of drug induced acute confusion are dopaminergic drugs (used for the treatment of Parkinson's disease), diuretics, tricyclic or tetracyclic antidepressants and benzodiazepines. The elderly, and especially those with pre-existing dementia, are most at risk for drug induced acute confusional states. New research is finding a link between Vitamin D deficiency and cognitive impairment (which includes 'foggy brain').\n\n\n"}
{"id": "52851946", "url": "https://en.wikipedia.org/wiki?curid=52851946", "title": "Cool S", "text": "Cool S\n\nThe \"Cool S\", also known as Superman S, Stüssy S, Super S, Pointy S and Graffiti S, as well as many other names, is a graffiti signature of popular culture that is typically doodled on children's notebooks or graffitied on walls.\n\nThe Cool S consists of 14 line segments, forming a stylized, pointed S-shape. The tails of the S appear to link underneath so that it loops around on itself in the same way as the infinity symbol does. The Cool S has no reflection symmetry, but has 2-fold rotational symmetry and tessellates with squares. As illustrated, a common way to draw the shape begins with two sets of three parallel, vertical lines, one above the other.\n\nThe origin of the Cool S is unclear. The name 'Superman S' comes from a belief that it was a symbol for Superman, whose costume features a stylised S in a diamond shape, but that shape is quite different. Similarly, the name 'Stussy S' is from a theory that it might be a symbol of the American surfwear company Stüssy.\n\n"}
{"id": "42357752", "url": "https://en.wikipedia.org/wiki?curid=42357752", "title": "Curb stomp", "text": "Curb stomp\n\nA curb stomp, also called curbing, curb checking, curb painting, or making someone bite the curb is a form of assault in which a victim's mouth is forcefully placed on a curb and then stomped from behind, causing severe injuries and sometimes death.\n\n\nTristain Lynn Frye was sentenced to 13 years, nine months for second-degree murder. \n\nScotty James Butters and David Nikos Pillatos pled to first-degree murder, accepted a plea agreement and testified against Monschke in exchange for being able to request that their imprisonment be no more than approximately 30 years. in June 2004, the U.S. Supreme Court ruled \"a sentence longer than the standard range could not be ordered by a judge without action by a jury, meaning Butters and Pillatos could face no more than 31 years in prison.\"\nKurtis William Monschke was convicted of aggravated first degree murder and sentenced to life in prison without possibility of parole.\nThe incident has been described as a hate crime.\n\n\nJunko Furuta, a Japanese high school girl, was curb stomped among the various other tortures inflicted on her before her death.\n\nOn the twentieth season of American television series \"Big Brother\", Houseguest Sam Bledsoe defines for fellow Houseguest J.C. Mounduix a \"curb stomp\" and terrifies him in the process.\n\nIn the film \"American History X\" (1998), white power skinhead Derek Vinyard curb-stomps Lawrence, an African American burglar who had tried to steal his truck.\n\nNew Jersey mafia boss Tony Soprano curb stomps New York mobster Salvatore \"Coco\" Cogliano for making lewd comments to his daughter in \"The Sopranos\" episode \"The Second Coming\" (2007).\n\nWWE professional wrestler Seth Rollins previously used a Curb Stomp as his finishing move. Rollins said that he stopped using the move as \"from a PR standpoint... it was too perceptually violent... I never hurt anyone with it. It was just something we didn't want kids trying on each other\". On the January 15, 2018, edition of \"Monday Night Raw\", Rollins used the move once again to win a match against Finn Bálor, with announcer Corey Graves referring to it as the \"Blackout\".\n\nIn the Bollywood movie \"Shootout at Lokhandwala\", a scene is depicted where Vivek Oberoi curb stomps a local resident who informs the police about Aslam (Oberoi's friend) who is encountered by the police.\n\nA track by American hip hop artist JPEGMAFIA named after the act was included on his second album \"Veteran\".\n"}
{"id": "33908012", "url": "https://en.wikipedia.org/wiki?curid=33908012", "title": "Cutting Through Spiritual Materialism", "text": "Cutting Through Spiritual Materialism\n\nCutting Through Spiritual Materialism, by Chögyam Trungpa is a book addressing many common pitfalls of self-deception in seeking spirituality, which the author coins as \"Spiritual materialism\". It is the transcript of two series of lectures given by Trungpa Rinpoche in 1970–71.\n\nIn \"Psychology Today\" Michael J. Formica writes,\nAs soon as we cast something into a role, as soon as we put a label on it, as soon as we name it and give it life by virtue of our investment (read: ego), we take away all its power and it is nothing more than an event – it is no longer a spiritual revelation, but simply a material experience. That is spiritual materialism at its peak.\n\n"}
{"id": "3502851", "url": "https://en.wikipedia.org/wiki?curid=3502851", "title": "Day-care sex-abuse hysteria", "text": "Day-care sex-abuse hysteria\n\nDay-care sex-abuse hysteria was a moral panic that occurred primarily in the 1980s and early 1990s featuring charges against day-care providers of several forms of child abuse, including Satanic ritual abuse. A prominent case in Kern County, California first brought the issue of day-care sexual abuse to the forefront of the public awareness, and the issue figured prominently in news coverage for almost a decade. The Kern County case was followed by cases elsewhere in the United States as well as Canada, New Zealand, Brazil, and various European countries.\n\nIn the late 1970s and early 1980s, more and more mothers were working outside of the home, resulting in the opening of large numbers of day-care centers. Anxiety and guilt over leaving young children with strangers may have created a climate of fear and readiness to believe false accusations.\n\nChildren are vulnerable to outside influences that lead to fabrication of testimony. Their testimony can be influenced in a variety of ways.\nMaggie Bruck in her article published by the American Psychological Association wrote that children incorporate aspects of the interviewer's questions into their answers in an attempt to tell the interviewer what the child believes is being sought. Studies also show that when adults ask children questions that do not make sense (such as \"is milk bigger than water?\" or \"is red heavier than yellow?\"), most children will offer an answer, believing that there is an answer to be given, rather than understand the absurdity of the question. Furthermore, repeated questioning of children causes them to change their answers. This is because the children perceive the repeated questioning as a sign that they did not give the \"correct\" answer previously. Children are also especially susceptible to leading and suggestive questions. Some studies have shown that only a small percentage of children produce fictitious reports of sexual abuse on their own. Other studies have shown that children understate occurrences of abuse.\n\nInterviewer bias also plays a role in shaping child testimony. When an interviewer has a preconceived notion as to the truth of the matter being investigated, the questioning is conducted in a manner to extract statements that support these beliefs. As a result, evidence that could disprove the belief is never sought by the interviewer. Additionally, positive reinforcement by the interviewer can taint child testimony. Often such reinforcement is given to encourage a spirit of cooperation by the child, but the impartial tone can quickly disappear as the interviewer nods, smiles, or offers verbal encouragement to \"helpful\" statements. Some studies show that when interviewers make reassuring statements to child witnesses, the children are more likely to fabricate stories of past events that never occurred.\n\nPeer pressure also influences children to fabricate stories. Studies show that when a child witness is told that his or her friends have already testified that certain events occurred, the child witness was more likely to create a matching story. The status of the interviewer can also influence a child's testimony, because the more authority an interviewer has such as a police officer, the more likely a child is to comply with that person's agenda.\n\nFinally, while there are supporters of the use of anatomically correct dolls in questioning victims of sexual abuse/molestation, there are also critics of this practice. These critics say that because of the novelty of the dolls, children will act out sexually explicit acts with the dolls even if the child has not been sexually abused. Another criticism is that because the studies that compare the differences between how abused and non-abused children play with these dolls are conflicting (some studies suggest that sexually abused children play with anatomically correct dolls in a more sexually explicit manner than non-abused children, while other studies suggest that there is no correlation), it is impossible to interpret what is meant by how a child plays with these dolls.\n\n\nThe McMartin Preschool case was the first daycare abuse case to receive major media attention in the United States. The case centered upon the McMartin Preschool in Manhattan Beach, California, where seven teachers were accused of kidnapping children, flying them in a plane to another location, and forcing them to engage in group sex as well as forcing them to watch animals be tortured and killed. The case also involved accusations that children had been forced to participate in bizarre religious rituals, and been used to make child pornography. The case began with a single accusation, made by the mother - who was later found to be a paranoid schizophrenic - of one of the students, but grew rapidly when investigators informed parents of the accusation and began interviewing other students. The case made headlines nationwide in 1984, and seven teachers were arrested and charged that year. When a new district attorney took over the case in 1986, however, his office re-examined the evidence and dropped charges against all but two of the original defendants. Their trials became one of the longest and most expensive criminal trials in the history of the United States, but in 1990 all of these charges were also dropped. Both jurors at the trial and academic researchers later criticized the interviewing techniques that investigators had used in their investigations of the school, alleging that interviewers had \"coaxed\" children into making unfounded accusations, repeatedly asking children the same questions and offering various incentives until the children reported having been abused. Most scholars now agree that the accusations these interviews elicited from children were false. Sociologist Mary de Young and historian Philip Jenkins have both cited the McMartin case as the prototype for a wave of similar accusations and investigations between 1983 and 1995, which constituted a moral panic.\n\nIn 1985, Frank Fuster – the owner of the Country Walk Babysitting Service in the Country Walk suburb of Miami, Florida – was found guilty of 14 counts of abuse. He was sentenced to prison with a minimum length of 165 years. Fuster's alleged victims testified that he led them in Satanic rituals and terrorized them by forcing them to watch him mutilate birds, a lesson to children who might reveal the abuse. Fuster had been previously convicted for manslaughter and for molesting a 9-year-old child. Testimony from children in the case was elicited by University of Miami child psychologists Laurie and Joseph Braga, who resorted to coercive questioning of the alleged victims when the desired answers were not forthcoming. Fuster's wife recanted her court testimony in an interview with Frontline, saying that she had been kept naked in solitary confinement and subjected to other forms of physical and psychological duress until she had agreed to testify against her husband.\n\nThe case was prosecuted by Dade County state's attorney Janet Reno. As of June 2017, Fuster continues to serve a 165-year prison sentence. The incident also inspired a book and made-for-TV movie called \"Unspeakable Acts\" (1990).\n\nIn April 1984, Gerald Amirault was arrested and charged with abusing children in his care at the Fells Acres Day Care Center in Malden, Massachusetts. After Gerald changed the pants on a young boy who had wet himself, the boy's mother, uncle and therapist questioned him over a period of months, until the boy alleged that Gerald had been sexually abusing him. The boy's mother then called a child abuse hotline to report that her son had been sexually abused, and Gerald was arrested shortly thereafter. In the 1986 trial, Gerald was convicted of assaulting and raping nine children and sentenced to thirty to forty years in state prison. In a separate trial, his mother, Violet Amirault, and sister, Cheryl Amirault LeFave, were also convicted of similar charges and sentenced to jail for eight to 20 years. According to Richard Beck, the case developed \"in the usual way\" compared to other moral panic cases, with more and more children leveling increasingly bizarre allegations against the accused during the course of the investigation. The allegations included reports of \"bad clowns,\" robots, \"magic rooms,\" and tortured animals. According to Beck, one of the prosecutors responsible for the case commented that coaxing allegations out of the children had been like \"getting blood from a stone.\"\n\nIn 1995, \"The Wall Street Journal\" journalist Dorothy Rabinowitz questioned testimony from the children that had been elicited with dubious interrogation techniques, writing, \"no sane person reading the transcripts of these interrogations can doubt the wholesale fabrications of evidence on which this case was built.\" Rabinowitz was a finalist for the 1996 Pulitzer Prize for her newspaper columns on the issue, and made the Amirault's case a centerpiece of her book \"No Crueler Tyrannies: Accusation, False Witness, and Other Terrors of Our Times.\" Violet and Cheryl were granted a new trial in 1997, on the basis that they had been denied the right to face their accusers and had been inadequately represented at trial, but Violet died and a judge reduced Cheryl's sentence to time served before the new trial could proceed. In 2001, the Massachusetts Board of Pardons recommended that Gerald's sentence be commuted, citing \"substantial doubt\" about his guilt. He was granted parole in 2003. Writing about the case at the time of Gerald's release, \"The Economist\" suggested in an editorial that while the Amiraults had long maintained their innocence and had attracted a \"string of prominent supporters\" who believed that they had been wrongly convicted, \"many others continue to believe that Mr. Amirault committed the crimes.\"\n\nOn October 4, 1984, a drug addicted couple acting as police informants called their contact in the Pittsfield, Massachusetts, police department to accuse Bernard Baran of molesting their son. The child had been attending the government operated Early Childhood Development Center (ECDC) where Baran, an openly gay 19-year-old, worked as a teacher's aide. The accusers had previously complained to the board of directors that they \"didn't want no homo\" around their son.\n\nWithin days of the first allegation, ECDC hosted a puppet show and delivered letters to parents notifying them about a child at the day care with gonorrhea. Five other allegations emerged. Baran was tried in the Berkshire County courthouse 105 days after the first allegation, a swiftness noted in the later court rulings. The courtroom was closed during the children's testimony, which Baran claimed violated his right to a public trial. Baran's defense attorney was later ruled ineffective. Baran was convicted on January 30, 1985, on three counts of rape of a child and five counts of indecent assault and battery. He was sentenced to three life terms plus 8 to 20 years on each charge. He maintained his innocence throughout his case. In 1999 a new legal team took up his case. In 2004 hearings began in a motion for a new trial. In 2006, Baran was granted a new trial and released on $50,000 bail. In May 2009, the Massachusetts Appeals Court affirmed the new trial ruling, setting aside the 1985 convictions. The Berkshire County District Attorney's office dismissed the original charges and Baran's record was cleared.\n\nProsecutor Mario Merola brought prosecutions leading to the conviction of five men, including Nathaniel Grady, a 47-year-old Methodist minister, of sexually abusing children in day care centers throughout the Bronx. Grady spent ten years in prison before being released in 1996.\n\nThree employees of another Bronx day-care center were arrested in August 1984 on the charges of abusing at least ten children in their care. Federal and city investigators then questioned dozens of children at the day care. They used 'dolls, gentle words and a quiet approach.' More children reported being sexually abused, raising the total to 30. Three more day care centers also were investigated for sexual abuse. On August 11, 1984, federal funds were cut off to the Head Start preschool program at the Praca Day Care Center and three employees had been arrested. In June 1985, the day care center was reopened with new sponsorship.\n\nIn January 1986, Albert Algerin, employed at the Praca Day Care center, was sentenced to 50 years for rape and sexual abuse. In May, Praca employee Jesus Torres, a former teacher's aide was sentenced to 40 years. Praca employee Franklin Beauchamp had his case overturned in New York Court of Appeals in May 1989.\n\nAll five convictions were ultimately overturned.\n\nIn Maplewood, New Jersey, in April 1985, Margaret Kelly Michaels was indicted for 299 offenses in connection with the sexual assault of 33 children. Michaels denied the charges. \"The prosecution produced expert witnesses who said that almost all the children displayed symptoms of sexual abuse.\" Prosecution witnesses testified that the children \"had regressed into such behavior as bed-wetting and defecating in their clothing. The witnesses said the children became afraid to be left alone or to stay in the dark.\" Some of the other teachers testified against her. \"The defense argued that Michaels did not have the time or opportunity to go to a location where all the activities could have taken place without someone seeing her.\"\n\nMichaels was sentenced to 47 years in the \"sex case.\" Michaels \"told the judge that she was confident her conviction would be overturned on appeal.\" After five years in prison her appeal was successful and sentence was overturned by a New Jersey appeals court. The New Jersey Supreme Court upheld the appellate court's decision and declared \"the interviews of the children were highly improper and utilized coercive and unduly suggestive methods.\" A three judge panel ruled she had been denied a fair trial, because \"the prosecution of the case had relied on testimony that should have been excluded because it improperly used an expert's theory, called the child sexual abuse accommodation syndrome, to establish guilt.\" The original judge was also criticized \"for the way in which he allowed the children to give televised testimony from his chambers.\"\n\nJames Toward and Brenda Williams were accused of kidnapping and sexually abusing six boys who attended Glendale Montessori in Stuart, Florida, as preschoolers in 1986 and 1987. Investigators claimed to know up to 60 victims, mostly from the ages 2 to 5.\n\nIn 1988, Williams, an office manager, was convicted and sentenced to 10 years in prison. She pleaded no contest to sex and attempted kidnapping charges involving five boys and she was released from prison in 1993 after serving five years. In 1989, Toward, owner of Glendale Montessori School, pleaded guilty to child sexual abuse charges and received a 27-year sentence. While technically maintaining his innocence, he allowed a guilty plea to be entered against him, convicting him of molesting or kidnapping six boys. Toward was placed in involuntary commitment due to the Jimmy Ryce Act. Although he maintained his innocence, Toward said he plea-bargained to avoid an almost certain life sentence.\n\nIn Edenton, North Carolina, in January 1989, a parent accused Bob Kelly of sexual abuse. Over the next several months, investigations and therapy led to allegations against dozens of other adults in the town, culminating in the arrest of seven adults.\n\nDespite the severity of some of the alleged acts, parents noticed no abnormal behaviour in their children until after initial accusations were made. Bob Kelly's trial lasted eight months and on April 22, 1992, he was convicted of 99 out of 100 counts against him. In 1995, the North Carolina Court of Appeals granted new trials to two defendants, including Kelly. Charges were ultimately dropped for both.\n\nThe remainder of the defendants received a variety of sentences.\n\nDale Akiki, a developmentally-delayed man with Noonan syndrome, was accused of satanic ritual abuse in 1991. Akiki and his wife were volunteer babysitters at Faith Chapel in Spring Valley, California. The accusations started when a young girl told her mother that \"[Akiki] showed me his penis,\" after which the mother contacted the police. After interviews, nine other children accused Akiki of killing animals, such as a giraffe and an elephant, and drinking their blood in front of the children. He was found not guilty of the 35 counts of child abuse and kidnapping in his 1993 trial.\n\nIn 1994, the San Diego County Grand Jury reviewed the Akiki cases and concluded there was no reason to pursue the theory of ritual abuse. On August 25, 1994, he filed a suit against the County of San Diego, Faith Chapel Church, and many others, which was settled for $2 million. Akiki's public defenders received the Public Defender of the Year award for their work defending Akiki.\n\nFrances Keller and her husband, Dan Keller, both of Austin, Texas, were convicted of sexually abusing a 3-year-old girl in their care, and they spent 21 years in prison until their release in 2013.\n\nThe case began on August 15, 1991, when a 3-year-old girl told her mother that Dan Keller had hurt her. The mother and daughter were on their way to a scheduled appointment with the girl’s therapist, who drew out details that included Keller defecating on her head and sexually assaulting her with a pen. During the time leading up to the trial, two other children from the day care offered similar accusations. According to the children, the couple served blood-laced Kool-Aid and forced them to have videotaped sex with adults and other children. The Kellers, they said, sometimes wore white robes and lit candles before hurting them. The children also accused the Kellers of forcing them to watch or participate in the killing and dismemberment of cats, dogs and a crying baby. Bodies were unearthed in cemeteries and new holes dug to hide freshly killed animals and, once, an adult passer-by was shot and dismembered with a chain saw. The children recalled several plane trips, including one to Mexico, where they were sexually abused by soldiers before returning to Austin in time to meet their parents at the day care.\n\nThe only physical evidence of abuse in the case was presented by Michael Mouw, an emergency room physician at Brackenridge Hospital who examined the 3-year-old girl in 1991 on the night she first accused Dan Keller of abuse. Mouw testified at the Kellers’ trial that he found two tears in the girl’s hymen consistent with sexual abuse and determined that the injuries were less than 24 hours old. Three years after the trial, while attending a medical seminar, Mouw said a slide presentation on “normal” pediatric hymens included a photo that was identical to what he had observed in the girl.\n\nOn November 26, 2013, the Travis County district attorney's office announced that Fran Keller, now 63, was being released on bond and her husband, Dan Keller, who was convicted at the same time, would be released within a week in a deal reached with lawyers. \"There is a reasonable likelihood that (the medical expert's) false testimony affected the judgment of the jury and violated Frances Keller's right to a fair trial,\" said the district attorney.\n\nOn June 20, 2017, the Travis County district attorney's office announced that the case against the Kellers had been dismissed, citing actual innocence. They were awarded $3.4 million in compensation from the state of Texas for the 21 years they spent in prison.\n\nIn Wenatchee, Washington, in 1994 and 1995, police and state social workers undertook what was then called the nation's most extensive child sex-abuse investigation. Forty-three adults were arrested on 29,726 charges of child sex abuse involving 60 children. Parents, Sunday school teachers and a pastor were charged, and many were convicted of abusing their own children or the children of others in the community. However, prosecutors were unable to provide any physical evidence to support the charges. The main witness was the 13-year-old foster daughter of police officer Robert Perez, who had investigated the cases. A jury found the city of Wenatchee and Douglas County, Washington, negligent in the 1994–1995 investigations. In 2001, $3 million was awarded to a couple who had been wrongly accused in the inquiry.\n\nPeter Ellis, a child-care worker at the Christchurch Civic Crèche in New Zealand, was found guilty on 16 counts of sexual abuse against children in 1992 and served seven years in jail. Parents of the alleged abuse victims were entitled to claim NZ$10,000 (equivalent to about US$11,000 in 2010) in compensation for each allegation from the Accident Compensation Corporation, regardless of whether the allegations were proved or not. Many victims' families made multiple allegations. Four female co-workers were also arrested on 15 charges of abuse, but were released after these charges were dropped. Together with six other co-workers who lost their jobs when the centre was closed, they were awarded $1 million in compensation by the Employment Court in 1995, although this sum was reduced on appeal. Peter Ellis has consistently denied any abuse, and the case is still considered controversial by many New Zealanders.\n\nIn 1992, a mother in the central Saskatchewan city of Martensville, alleged that a local woman who ran a babysitting service and day care center in her home had sexually abused her child. Police began an investigation, leading to a sharp increase in allegations. More than a dozen persons, including five police officers from two different forces, ultimately faced over 100 charges connected with running a Satanic cult called The Brotherhood of The Ram, which allegedly practiced ritualized sexual abuse of numerous children at a \"Devil Church\".\n\nThe son of the day care owner was tried and found guilty, then a Royal Canadian Mounted Police task force took over the investigation. It concluded the original investigation was motivated by \"emotional hysteria.\" In 2003, defendants sued for wrongful prosecution. In 2004, Richard and Kari Klassen received $100,000 each, out of the $1.5 million compensation package awarded for the malicious prosecution.\n\n\n\n\n"}
{"id": "19194778", "url": "https://en.wikipedia.org/wiki?curid=19194778", "title": "Deformation (mechanics)", "text": "Deformation (mechanics)\n\nDeformation in continuum mechanics is the transformation of a body from a \"reference\" configuration to a \"current\" configuration. A configuration is a set containing the positions of all particles of the body.\n\nA deformation may be caused by external loads, body forces (such as gravity or electromagnetic forces), or changes in temperature, moisture content, or chemical reactions, etc.\n\nStrain is a description of deformation in terms of \"relative\" displacement of particles in the body that excludes rigid-body motions. Different equivalent choices may be made for the expression of a strain field depending on whether it is defined with respect to the initial or the final configuration of the body and on whether the metric tensor or its dual is considered.\n\nIn a continuous body, a deformation field results from a stress field induced by applied forces or is due to changes in the temperature field inside the body. The relation between stresses and induced strains is expressed by constitutive equations, e.g., Hooke's law for linear elastic materials. Deformations which are recovered after the stress field has been removed are called elastic deformations. In this case, the continuum completely recovers its original configuration. On the other hand, irreversible deformations remain even after stresses have been removed. One type of irreversible deformation is plastic deformation, which occurs in material bodies after stresses have attained a certain threshold value known as the \"elastic limit\" or yield stress, and are the result of slip, or dislocation mechanisms at the atomic level. Another type of irreversible deformation is viscous deformation, which is the irreversible part of viscoelastic deformation.\n\nIn the case of elastic deformations, the response function linking strain to the deforming stress is the compliance tensor of the material.\n\nStrain is a measure of deformation representing the displacement between particles in the body relative to a reference length.\n\nA general deformation of a body can be expressed in the form where is the reference position of material points in the body. Such a measure does not distinguish between rigid body motions (translations and rotations) and changes in shape (and size) of the body. A deformation has units of length.\n\nWe could, for example, define strain to be\nwhere is the identity tensor.\nHence strains are dimensionless and are usually expressed as a decimal fraction, a percentage or in parts-per notation. Strains measure how much a given deformation differs locally from a rigid-body deformation.\n\nA strain is in general a tensor quantity. Physical insight into strains can be gained by observing that a given strain can be decomposed into normal and shear components. The amount of stretch or compression along material line elements or fibers is the \"normal strain\", and the amount of distortion associated with the sliding of plane layers over each other is the \"shear strain\", within a deforming body. This could be applied by elongation, shortening, or volume changes, or angular distortion.\n\nThe state of strain at a material point of a continuum body is defined as the totality of all the changes in length of material lines or fibers, the \"normal strain\", which pass through that point and also the totality of all the changes in the angle between pairs of lines initially perpendicular to each other, the \"shear strain\", radiating from this point. However, it is sufficient to know the normal and shear components of strain on a set of three mutually perpendicular directions.\n\nIf there is an increase in length of the material line, the normal strain is called \"tensile strain\", otherwise, if there is reduction or compression in the length of the material line, it is called \"compressive strain\".\n\nDepending on the amount of strain, or local deformation, the analysis of deformation is subdivided into three deformation theories:\n\nIn each of these theories the strain is then defined differently. The \"engineering strain\" is the most common definition applied to materials used in mechanical and structural engineering, which are subjected to very small deformations. On the other hand, for some materials, e.g. elastomers and polymers, subjected to large deformations, the engineering definition of strain is not applicable, e.g. typical engineering strains greater than 1%, thus other more complex definitions of strain are required, such as \"stretch\", \"logarithmic strain\", \"Green strain\", and \"Almansi strain\".\n\nThe Cauchy strain or engineering strain is expressed as the ratio of total deformation to the initial dimension of the material body in which the forces are being applied. The \"engineering normal strain\" or \"engineering extensional strain\" or \"nominal strain\" of a material line element or fiber axially loaded is expressed as the change in length per unit of the original length of the line element or fibers. The normal strain is positive if the material fibers are stretched and negative if they are compressed. Thus, we have\n\nwhere is the \"engineering normal strain\", is the original length of the fiber and is the final length of the fiber. Measures of strain are often expressed in parts per million or microstrains.\n\nThe \"true shear strain\" is defined as the change in the angle (in radians) between two material line elements initially perpendicular to each other in the undeformed or initial configuration. The \"engineering shear strain\" is defined as the tangent of that angle, and is equal to the length of deformation at its maximum divided by the perpendicular length in the plane of force application which sometimes makes it easier to calculate.\n\nThe stretch ratio or extension ratio is a measure of the extensional or normal strain of a differential line element, which can be defined at either the undeformed configuration or the deformed configuration. It is defined as the ratio between the final length and the initial length of the material line.\n\nThe extension ratio is approximately related to the engineering strain by\n\nThis equation implies that the normal strain is zero, so that there is no deformation when the stretch is equal to unity.\n\nThe stretch ratio is used in the analysis of materials that exhibit large deformations, such as elastomers, which can sustain stretch ratios of 3 or 4 before they fail. On the other hand, traditional engineering materials, such as concrete or steel, fail at much lower stretch ratios.\n\nThe logarithmic strain , also called, \"true strain\" or \"Hencky strain\" . Considering an incremental strain (Ludwik)\n\nthe logarithmic strain is obtained by integrating this incremental strain:\n\nwhere is the engineering strain. The logarithmic strain provides the correct measure of the final strain when deformation takes place in a series of increments, taking into account the influence of the strain path.\n\nThe Green strain is defined as:\n\nThe Euler-Almansi strain is defined as\n\nStrains are classified as either \"normal\" or \"shear\". A \"normal strain\" is perpendicular to the face of an element, and a \"shear strain\" is parallel to it. These definitions are consistent with those of normal stress and shear stress.\n\nFor an isotropic material that obeys Hooke's law, a normal stress will cause a normal strain. Normal strains produce \"dilations\".\n\nConsider a two-dimensional, infinitesimal, rectangular material element with dimensions , which, after deformation, takes the form of a rhombus. From the geometry of the adjacent figure we have\nand\nFor very small displacement gradients the squares of the derivatives are negligible and we have\nThe normal strain in the -direction of the rectangular element is defined by\nSimilarly, the normal strain in the - and -directions becomes\n\nThe engineering shear strain () is defined as the change in angle between lines and . Therefore,\n\nFrom the geometry of the figure, we have\nFor small displacement gradients we have\nFor small rotations, i.e. and are ≪ 1 we have , . Therefore,\nthus\n\nBy interchanging and and and , it can be shown that .\n\nSimilarly, for the - and -planes, we have\n\nThe tensorial shear strain components of the infinitesimal strain tensor can then be expressed using the engineering strain definition, , as\n\nA strain field associated with a displacement is defined, at any point, by the change in length of the tangent vectors representing the speeds of arbitrarily parametrized curves passing through that point. A basic geometric result, due to Fréchet, von Neumann and Jordan, states that, if the lengths of the tangent vectors fulfil the axioms of a norm and the parallelogram law, then the length of a vector is the square root of the value of the quadratic form associated, by the polarization formula, with a positive definite bilinear map called the metric tensor.\n\nDeformation is the change in the metric properties of a continuous body, meaning that a curve drawn in the initial body placement changes its length when displaced to a curve in the final placement. If none of the curves changes length, it is said that a rigid body displacement occurred.\n\nIt is convenient to identify a reference configuration or initial geometric state of the continuum body which all subsequent configurations are referenced from. The reference configuration need not be one the body actually will ever occupy. Often, the configuration at is considered the reference configuration, . The configuration at the current time is the \"current configuration\".\n\nFor deformation analysis, the reference configuration is identified as \"undeformed configuration\", and the current configuration as \"deformed configuration\". Additionally, time is not considered when analyzing deformation, thus the sequence of configurations between the undeformed and deformed configurations are of no interest.\n\nThe components of the position vector of a particle in the reference configuration, taken with respect to the reference coordinate system, are called the \"material or reference coordinates\". On the other hand, the components of the position vector of a particle in the deformed configuration, taken with respect to the spatial coordinate system of reference, are called the \"spatial coordinates\"\n\nThere are two methods for analysing the deformation of a continuum. One description is made in terms of the material or referential coordinates, called material description or Lagrangian description. A second description is of deformation is made in terms of the spatial coordinates it is called the spatial description or Eulerian description.\n\nThere is continuity during deformation of a continuum body in the sense that:\n\nA deformation is called an affine deformation if it can be described by an affine transformation. Such a transformation is composed of a linear transformation (such as rotation, shear, extension and compression) and a rigid body translation. Affine deformations are also called homogeneous deformations.\n\nTherefore, an affine deformation has the form\nwhere is the position of a point in the deformed configuration, is the position in a reference configuration, is a time-like parameter, is the linear transformer and is the translation. In matrix form, where the components are with respect to an orthonormal basis,\n\nThe above deformation becomes \"non-affine\" or \"inhomogeneous\" if or .\n\nA rigid body motion is a special affine deformation that does not involve any shear, extension or compression. The transformation matrix is proper orthogonal in order to allow rotations but no reflections.\n\nA rigid body motion can be described by\nwhere\nIn matrix form,\n\nA change in the configuration of a continuum body results in a displacement. The displacement of a body has two components: a rigid-body displacement and a deformation. A rigid-body displacement consists of a simultaneous translation and rotation of the body without changing its shape or size. Deformation implies the change in shape and/or size of the body from an initial or undeformed configuration to a current or deformed configuration (Figure 1).\n\nIf after a displacement of the continuum there is a relative displacement between particles, a deformation has occurred. On the other hand, if after displacement of the continuum the relative displacement between particles in the current configuration is zero, then there is no deformation and a rigid-body displacement is said to have occurred.\n\nThe vector joining the positions of a particle \"P\" in the undeformed configuration and deformed configuration is called the displacement vector in the Lagrangian description, or in the Eulerian description.\n\nA \"displacement field\" is a vector field of all displacement vectors for all particles in the body, which relates the deformed configuration with the undeformed configuration. It is convenient to do the analysis of deformation or motion of a continuum body in terms of the displacement field. In general, the displacement field is expressed in terms of the material coordinates as\n\nor in terms of the spatial coordinates as\n\nwhere are the direction cosines between the material and spatial coordinate systems with unit vectors and , respectively. Thus\n\nand the relationship between and is then given by\n\nKnowing that\nthen\n\nIt is common to superimpose the coordinate systems for the undeformed and deformed configurations, which results in , and the direction cosines become Kronecker deltas:\n\nThus, we have\n\nor in terms of the spatial coordinates as\n\nThe partial differentiation of the displacement vector with respect to the material coordinates yields the \"material displacement gradient tensor\" . Thus we have:\n\nwhere is the \"deformation gradient tensor\".\n\nSimilarly, the partial differentiation of the displacement vector with respect to the spatial coordinates yields the \"spatial displacement gradient tensor\" . Thus we have,\n\nHomogeneous (or affine) deformations are useful in elucidating the behavior of materials. Some homogeneous deformations of interest are\nPlane deformations are also of interest, particularly in the experimental context.\n\nA plane deformation, also called \"plane strain\", is one where the deformation is restricted to one of the planes in the reference configuration. If the deformation is restricted to the plane described by the basis vectors , , the deformation gradient has the form\nIn matrix form,\nFrom the polar decomposition theorem, the deformation gradient, up to a change of coordinates, can be decomposed into a stretch and a rotation. Since all the deformation is in a plane, we can write\nwhere is the angle of rotation and , are the principal stretches.\n\nIf the deformation is isochoric (volume preserving) then and we have\nAlternatively,\n\nA simple shear deformation is defined as an isochoric plane deformation in which there is a set of line elements with a given reference orientation that do not change length and orientation during the deformation.\n\nIf is the fixed reference orientation in which line elements do not deform during the deformation then and .\nTherefore,\nSince the deformation is isochoric,\nDefine\nThen, the deformation gradient in simple shear can be expressed as\nNow,\nSince\nwe can also write the deformation gradient as\n\n\n"}
{"id": "1020046", "url": "https://en.wikipedia.org/wiki?curid=1020046", "title": "Detachment (philosophy)", "text": "Detachment (philosophy)\n\nDetachment, also expressed as non-attachment, is a state in which a person overcomes his or her attachment to desire for things, people or concepts of the world and thus attains a heightened perspective. It is considered a wise virtue and is promoted in various Eastern religions, such as Taoism and Buddhism.\n\nDetachment as release from desire and consequently from suffering is an important principle, or even ideal, in the Bahá'í Faith, Buddhism, Hinduism, Jainism, Stoicism, and Taoism.\n\nIn Buddhist and Hindu religious texts the opposite concept is expressed as \"upādāna\", translated as \"attachment\". Attachment, that is the inability to practice or embrace detachment, is viewed as the main obstacle towards a serene and fulfilled life. Many other spiritual traditions identify the lack of detachment with the continuous worries and restlessness produced by desire and personal ambitions.\n\n\"Thou hast inquired about detachment. It is well known to thee that by detachment is intended the detachment of the soul from all else but God. That is, it consisteth in soaring up to an eternal station, wherein nothing that can be seen between heaven and earth deterreth the seeker from the Absolute Truth. In other words, he is not veiled from divine love or from busying himself with the mention of God by the love of any other thing or by his immersion therein.\"\n\nThe second definition is in the Words of Wisdom: \"The essence of detachment is for man to turn his face towards the courts of the Lord, to enter His Presence, behold His Countenance, and stand as witness before Him.\" (Tablets of Baha'u'llah, p. 155)\n\nRegarding the concept of detachment, or non-attachment, Buddhist texts in Pali mention \"nekkhamma\", a word generally translated as \"renunciation\". This word also conveys more specifically the meaning of \"giving up the world and leading a holy life\" or \"freedom from lust, craving and desires.\"\n\nThe writings of Milarepa, are canonical Mahayana Buddhist texts that emphasize the temporary nature of the physical body and the need for non-attachment.\n\nDetachment is a central concept in Zen Buddhist philosophy. One of the most important technical Chinese terms for detachment is \"wú niàn\" (無念), which literally means \"no thought.\" This does not signify the literal absence of thought, but rather the state of being \"unstained\" (\"bù rán\" 不染) by thought. Therefore, \"detachment\" is being detached from one's thoughts. It is to separate oneself from one's own thoughts and opinions in detail as to not be harmed mentally and emotionally by them.\n\nEastern Christian monasticism cultivated practices of detached watchfulness which were designed to calm the passions and lead to an ongoing state of calm detachment known as apatheia.\n\nIn Western Christianity, Ignatian spirituality encourages detachment, sometimes referred to as \"indifference\", in order to maximize a person's \"availability\" to God and to their neighbors.\n\nThe Hindu view of detachment comes from the understanding of the nature of existence and the true ultimate state sought is that of being in the moment. In other words, while one is responsible and active, one does not worry about the past or future. The detachment is towards the result of one's actions rather than towards everything in life. This concept is cited extensively within Puranic and Vedic literature, for example:\nVairagya is a Hindu term which is often translated as detachment.\n\nDetachment is one of the supreme ideals of Jainism, together with non-violence. Non-possession/Non-attachment is one of the Mahavratas, the five great vows Jain monks observe.\n\nThe Tao Te Ching expressed the concept (in chapter 44) as:\n\nFame or Self: Which matters more?\nSelf or Wealth: Which is more precious?\nGain or Loss: Which is more painful?\nHe who is attached to things will suffer much.\nHe who saves will suffer heavy loss.\nA contented man is rarely disappointed.\nHe who knows when to stop does not find himself in trouble.\nHe will stay forever safe.\n"}
{"id": "25386783", "url": "https://en.wikipedia.org/wiki?curid=25386783", "title": "Disability benefits", "text": "Disability benefits\n\nDisability benefits are funds provided from public or private sources to a person who is ill or who has a disability.\n\nIn the United Kingdom disability benefits are covered by Department for Work and Pensions, and include:\n\n\nIn the United States, disability benefits for most Americans are covered and paid for by the Social Security Administration (a government agency). There are two main programs administered by the SSA; Social Security Disability Insurance program (SSDI) and the Supplemental Security Income (SSI) program. There is also a specific program for children with disabilities.\n\nSocial Security Disability Insurance provides benefits to individuals who have worked and paid Social Security taxes. Insurance eligibility is dependent upon Quarters of Coverage (QCs), commonly called \"work credits\". These are allotted based on the earnings for each quarter the individual has worked. Work credits ensure coverage until they \"expire\" on the individual's Date Last Insured (DLI). Medical evidence must prove that the onset of disability was before their DLI to receive benefits. SSDI recipients become eligible for Medicare after two years of SSDI eligibility.\n\nSupplemental Security Income (SSI) provides benefits to low-income individuals who are disabled and unable to work, regardless of whether they have worked in the past. Individuals must meet income and resource requirements. SSI also provides benefits to children under 18 years old, who are disabled and whose parents or guardians have limited income. The monthly SSI payment is calculated based upon the Federal Benefit Rate (FBR), and the individual's income. Most SSI recipients are immediately eligible for Medicaid and Supplemental Nutrition Assistance Program (SNAP), though program requirements vary by state. \n\nSome individuals are eligible for both SSI and SSDI.\n\nIn Canada, there are a variety of public Disability Benefit Programs. The largest programs are the Canada Pension Plan and Quebec Pension Plan disability benefits, and provincial workers' compensation and social assistance programs. Some individuals, in addition, have private disability insurance coverage, purchased either individually, or through an employer. Different programs use different rules to decide whether or not someone is eligible for benefits.\n\nTo access Canada Pension Plan or the Quebec Pension Plan disability benefits, an individual needs to have a disability that is \"severe and prolonged\", and which prevents them from working on a regular basis. As of 2018, CPP disability benefits are a minimum of $485.20 a month. Individuals who have contributed more to CPP or QPP during their working career receive higher benefits. The average monthly CPP disability benefit was $971.23 in 2018 and the maximum monthly amount was $1,335.83. \n\nPeople receiving CPP disability benefits may earn up to $5,500 a year without losing their benefits. Benefits stop when an individual has the ability to work regularly, or is no longer disabled. When an the individual reaches the age of 65, CPP Disability Benefits are replaced by a Retirement Pension.\n\nEmployment Insurance is a benefit plan that offers temporary financial assistance to those individuals who cannot work due to sickness, injury, or quarantine. \n\nTo be eligible to receive EI sickness benefits:\n\n\nPeople are only eligible for these benefits if they are unable to work due to their sickness, injury, or quarantine, but would be able to work otherwise. To receive EI sickness benefit a medical certificate signed by the doctor is required.\n\nTo qualify for EI you must have a required amount of insurable employment hours, which are used to calculate your benefit period, these insurable employment hours must be accumulated throughout the qualifying period. \n\nThe qualifying period: \n\n\nIt is important to note that each individual’s case is different and requirements may vary from case to case. But a general way of calculating EI benefits is 55% of the average insurable weekly earnings. The maximum amount you can be eligible for as of January 1 2018 is $51,700. Typically EI sickness benefits can only be paid for up to 15 weeks, but can vary depending on how long the individual is unable to work. \n\nWeekly EI sick benefits are calculated based on income before its been deducted during the individuals “best weeks”. Best weeks are the weeks in which the individual earned the most amount of amount, including any tips and commissions, the best weeks are chosen out of the qualifying period. \n\nIn Canada areas with high rates of unemployment will use the best 14 weeks, and in areas with low unemployment rates will use the best 22 weeks. \n\nThe weekly amount is calculated: \n\n\nIf you are considered a low income family you may be eligible to receive the EI Family Supplement. An individual can be considered to be a part of this category if the yearly family net income is $25, 921 or less, has children, and if the individual or their spouse receives Canada child tax benefit. If both the individual and their spouse claims EI sickness benefits only one of them can be eligible for the family supplement. \n\nFor more information on Benefits and eligibility, please visit the Government of Canada website. \n"}
{"id": "21345091", "url": "https://en.wikipedia.org/wiki?curid=21345091", "title": "Explanatory model", "text": "Explanatory model\n\nAn explanatory model is a useful description of why and how a thing works or an explanation of why a phenomenon is the way it is. The explanatory model is used as a substitute for \"the full explanation\" of the thing in question: \n\nExplanatory models do not claim to be a complete description/explanation of the absolute about the thing/phenomenon, nor do they even claim to, necessarily, be fully accurate. The description/explanation does, however, need to fit well enough to a sufficient portion of all the knowledge, observations and theoretical circumstances known about the thing/phenomenon, so that the explanatory model becomes useful.\nThat is: the description/explanation in an explanatory model, should be useful/helpful when one is about to make a decision or choice or when trying to successfully understand, explain or in some other way relate to the reality of the world around .Tewodros kassa\n\nAs most, if not all, explanations of anything, to a certain degree depend on axioms, and thereby are incomplete and not really \"the \"full\" explanation\", then, strictly speaking, all explanations are in fact explanatory models.<br>\nYet, the term \"explanatory model\" generally is used only when one feels the need to \"emphasize awareness of\" the incompleteness of an explanation (due to intentional simplification or due to lack of knowledge and understanding).\n\nBy being mindful of the difference between on the one hand: \"absolute reality\" and on the other hand: \"the explanatory models that one has become accustomed to\", then one will be better equipped to avoid erroneously rejecting important new knowledge, even when this new knowledge seem to clearly contradict that which one \"knows\" from before.\n\n"}
{"id": "899541", "url": "https://en.wikipedia.org/wiki?curid=899541", "title": "Flaying", "text": "Flaying\n\nFlaying, also known colloquially as skinning, is a method of slow and painful execution in which skin is removed from the body. Generally, an attempt is made to keep the removed portion of skin intact.\n\nA dead animal may be flayed when preparing it to be used as human food, or for its hide or fur. This is more commonly called skinning.\n\nFlaying of humans is used as a method of torture or execution, depending on how much of the skin is removed. This is often referred to as \"flaying alive\". There are also records of people flayed after death, generally as a means of debasing the corpse of a prominent enemy or criminal, sometimes related to religious beliefs (e.g. to deny an afterlife); sometimes the skin is used, again for deterrence, esotheric/ritualistic purposes, etc. (e.g. scalping).\n\nDermatologist Ernst G. Jung notes that the typical causes of death due to flaying are shock, critical loss of blood or other body fluids, hypothermia, or infections, and that the actual death is estimated to occur from a few hours up to a few days after the flaying. Hypothermia is possible, as skin is essential for maintaining a person's body temperature, as it provides a person's natural insulation.\n\nErnst G. Jung, in his \"Kleine Kulturgeschichte der Haut\" (\"A small cultural history of the skin\"), provides an essay in which he outlines the Neo-Assyrian tradition of flaying human beings. Already from the times of Ashurnasirpal II (r. 883-859 BC), the practice is displayed and commemorated in both carvings and official royal edicts. The carvings show that the actual flaying process might begin at various places on the body, such as at the crus (lower leg), the thighs, or the buttocks.\n\nIn their royal edicts, the Neo-Assyrian kings seem to gloat over the terrible fate they imposed upon their captives, and that flaying seems, in particular, to be the fate meted out to rebel leaders. Jung provides some examples of this triumphant rhetoric. Here are some from Ashurnasirpal II: \n\nThe Rassam Cylinder, in the British Museum demonstrates this.\n\nSearing or cutting the flesh from the body was sometimes used as part of the public execution of traitors in medieval Europe. A similar mode of execution was used as late as the early 18th century in France; one such episode is graphically recounted in the opening chapter of Michel Foucault's \"Discipline and Punish\" (1979).\n\nIn 1303, the Treasury of Westminster Abbey was robbed while holding a large sum of money belonging to King Edward I. After arrest and interrogation of 48 monks, three of them, including the subprior and sacrist, were found guilty of the robbery and flayed. Their skin was attached to three doors as a warning against robbers of Church and State. The Copford church in Essex, England, has been found to have human skin attached to a door.\n\nIn Chinese history, Sun Hao, Fu Sheng and Gao Heng were known for removing skin from people's faces. The Hongwu Emperor flayed many servants, officials and rebels. In 1396 he ordered the flaying of 5000 women. Hai Rui suggested that his emperor flay corrupt officials. The Zhengde Emperor flayed six rebels, and Zhang Xianzhong also flayed many people. Lu Xun said the Ming Dynasty was begun and ended by flaying.\n\n\n\n\n\n\n"}
{"id": "13811213", "url": "https://en.wikipedia.org/wiki?curid=13811213", "title": "Greco-Iberian alphabet", "text": "Greco-Iberian alphabet\n\nThe Greco-Iberian alphabet is a direct adaptation of an Ionic variant of a Greek alphabet to the specifics of the Iberian language, thus this script is an alphabet and lacks the distinctive characteristic of the rest of paleohispanic scripts that present signs with syllabic value, for the occlusives and signs with monophonemic value for the rest of consonants and vowels.\n\nThe inscriptions that use the Greco-Iberian alphabet had been found mainly in Alicante and Murcia and the direction of the writing is left to right. The number of known Greco-Iberian inscriptions is small: fewer than two dozen ceramic inscriptions and a dozen lead plaques, among them the lead plaque from La Serreta (Alcoy, Alicante) and the lead plaque from El Cigarralejo (Mula, Murcia). The archaeological context of the Greco-Iberian inscriptions seems to concentrate in the 4th century BC, but the paleographic characteristics of the model indicate that the adaptation may date from the 5th century BC.\n\nThe Greco-Iberian alphabet contains 16 signs identical to Greek signs, except for the sign corresponding to the second rhotic consonant: five vowels, three voiced occlusives (labial, dental and velar), but only two voiceless occlusives (dental and velar), two sibilants, two rhotics, one lateral, and only one nasal sign. To represent the second rhotic rho gets an additional stroke. Eta is used instead of epsilon to represent /e/. The only letter not found in the modern variant of the Greek alphabet is sampi. \n\nThe letter forms are petroglyphic since the writing surface (ceramic, stone, lead) affords angular glyphs with straight lines.\n\n\n"}
{"id": "10621538", "url": "https://en.wikipedia.org/wiki?curid=10621538", "title": "Henosis", "text": "Henosis\n\nHenosis () is the classical Greek word for mystical \"oneness\", \"union\" or \"unity.\" In Platonism, and especially Neoplatonism, the goal of henosis is union with what is fundamental in reality: the One (Τὸ Ἕν), the Source, or Monad. The Neoplatonic concept has precedents in the Greek mystery religions as well as parallels in Eastern philosophy. It is further developed in the Corpus Hermeticum, in Christian theology, Alevism, soteriology and mysticism, and is an important factor in the historical development of monotheism during Late Antiquity.\n\nThe term is relatively common in classical texts, and has the meaning of \"union\" or \"unity\".\n\nHenosis, or primordial unity, is rational and deterministic, emanating from indeterminism an uncaused cause. Each individual as a microcosm reflects the gradual ordering of the universe referred to as the macrocosm. In mimicking the demiurge (divine mind), one unites with The One or Monad. Thus the process of unification, of \"The Being\" and \"The One,\" is called henosis, the culmination of which is deification.\n\nHenosis for Plotinus (204/5–270 CE) was defined in his works as a reversing of the ontological process of consciousness via meditation (in the Western mind to uncontemplate) toward no thought (nous or demiurge) and no division (dyad) within the individual (being). As is specified in the writings of Plotinus on Henology, one can reach a tabula rasa, a blank state where the individual may grasp or merge with The One. This absolute simplicity means that the nous or the person is then dissolved, completely absorbed back into the Monad. \n\nWithin the Enneads of Plotinus the Monad can be referred to as the Good above the demiurge. The Monad or dunamis (force) is of one singular expression (the will or the one is the good), all is contained in the Monad and the Monad is all (pantheism). All division is reconciled in the one, the final stage before reaching singularity, called duality (dyad), is completely reconciled in the Monad, Source or One (see monism). As the one, source or substance of all things the Monad is all encompassing. As infinite and indeterminate all is reconciled in the dunamis or one. It is the demiurge or second emanation that is the nous in Plotinus. It is the demiurge (creator, action, energy) or nous that \"perceives\" and therefore causes the force (potential or One) to manifest as energy, or the dyad called the material world. Nous as being, being and perception (intellect) manifest what is called soul (World Soul).\n\nPlotinus words his teachings to reconcile not only Plato with Aristotle but also various World religions that he had personal contact with during his various travels. Plotinus' works have an ascetic character in that they reject matter as an illusion (non-existent). Matter was strictly treated as immanent, with matter as essential to its being, having no true or transcendential character or essence, substance or ousia. This approach is called philosophical Idealism.\n\nWithin the works of Iamblichus of Chalcis (c. 245 – c. 325 AD), The One and reconciliation of division can be obtained through the process of theurgy. By mimicking the demiurge, the individual is returned to the cosmos to implement the will of the divine mind. One goes through a series of theurgy or rituals that unites the initiate to the Monad. These rituals mimic the ordering of the chaos of the Universe into the material world or cosmos. They also mimic the actions of the demiurge as the creator of the material world. Iamblichus used the rituals of the mystery religions to perform rituals on the individual to unite their outer and inner person. Thus one without conflict internal or external is united (henosis) and is The One (hen).\n\nIn Eastern Orthodox Christianity, but also in western mysticism, \"henosis\" can be acquired by theoria, hesychasm and contemplative prayer. Yet, the concept of theosis, or deification, differs from henosis, since created beings cannot become God in His transcendent essence, or ousia.\n\n\n.group || et/float.innerHtml\n\n\n"}
{"id": "407814", "url": "https://en.wikipedia.org/wiki?curid=407814", "title": "Hygiene hypothesis", "text": "Hygiene hypothesis\n\nIn medicine, the hygiene hypothesis states a lack of early childhood exposure to infectious agents, symbiotic microorganisms (such as the gut flora or probiotics), and parasites increases susceptibility to allergic diseases by suppressing the natural development of the immune system. In particular, the lack of exposure is thought to lead to defects in the establishment of immune tolerance.\n\nThe hygiene hypothesis has also been called the \"biome depletion theory\" and the \"lost friends theory\".\n\nThe original formulation of the hygiene hypothesis dates from 1989 when David Strachan proposed that lower incidence of infection in early childhood could be an explanation for the rapid 20th century rise in allergic diseases such as asthma and hay fever.\n\nIt is now also recognised that the \"reduced microbial exposure\" concept applies to a much broader range of chronic inflammatory diseases than asthma and hay fever, which includes diseases such as type 1 diabetes and multiple sclerosis, and also some types of depression and cancer.\n\nIn 2003 Graham Rook proposed the \"old friends hypothesis\" which some claim offers a more rational explanation for the link between microbial exposure and inflammatory disorders. He argues that the vital microbial exposures are not colds, influenza, measles and other common childhood infections which have evolved relatively recently over the last 10,000 years, but rather the microbes already present during mammalian and human evolution, that could persist in small hunter gatherer groups as microbiota, tolerated latent infections or carrier states. He proposes that humans have become so dependent on these \"old friends\" that their immune systems neither develop properly nor function properly without them.\n\nStrachan's original formulation of the hygiene hypothesis also centred around the idea that smaller families provided insufficient microbial exposure partly because of less person-to-person spread of infections, but also because of \"improved household amenities and higher standards of personal cleanliness\". It seems likely that this was the reason he named it the \"hygiene hypothesis\". Although the \"hygiene revolution\" of the nineteenth and twentieth centuries may have been a major factor, it now seems more likely that, although public health measures such as sanitation, potable water and garbage collection were instrumental in reducing our exposure to cholera, typhoid and so on, they also deprived people of their exposure to the \"old friends\" that occupy the same environmental habitats.\n\nThe rise of autoimmune diseases and acute lymphoblastic leukemia in young people in the developed world was linked to the hygiene hypothesis.\n\nSome evidence indicates that autism is correlated to factors (such as certain cytokines) that are indicative of an immune disease. One publication speculated that the lack of early childhood exposure could be a cause of autism.\n\nThe risk of chronic inflammatory diseases also depends on factors such as diet, pollution, physical activity, obesity, socio-economic factors and stress. Genetic predisposition is also a factor.\n\nAlthough the idea that exposure to certain infections may decrease the risk of allergy is not new, Strachan was one of the first to formally propose it, in an article published in the \"British Medical Journal\" in 1989. This article proposed to explain the observation that hay fever and eczema, both allergic diseases, were less common in children from larger families, which were presumably exposed to more infectious agents through their siblings, than in children from families with only one child.\n\nThe hypothesis was extensively investigated by immunologists and epidemiologists and has become an important theoretical framework for the study of chronic inflammatory disorders. It explains the increase in allergic diseases that has been seen since industrialization and the higher incidence of allergic diseases in more developed countries. Epidemiological studies continue to confirm the protective effect of large family size and of growing up on a farm. However, exposure to common childhood infections such as chickenpox or measles is not thought to be protective.\n\nThe \"old friends hypothesis\" proposed in 2003 may offer a better explanation for the link between microbial exposure and inflammatory diseases. This hypothesis argues that the vital exposures are not common childhood and other recently evolved infections, which are no older than 10,000 years, but rather microbes already present in hunter-gatherer times when the human immune system was evolving. Conventional childhood infections are mostly \"crowd infections\" that kill or immunise and thus cannot persist in isolated hunter-gatherer groups. Crowd infections started to appear after the neolithic agricultural revolution, when human populations increased in size and proximity. The microbes that co-evolved with mammalian immune systems are much more ancient. According to this hypothesis, humans became so dependent on them that their immune systems can neither develop nor function properly without them.\n\nRook proposed that these microbes most likely include:\n\n\nThe modified hypothesis later expanded to include exposure to symbiotic bacteria and parasites.\n\n\"Evolution turns the inevitable into a necessity.\" This means that the majority of mammalian evolution took place in mud and rotting vegetation and more than 90 percent of human evolution took place in isolated hunter-gatherer communities and farming communities. Therefore, the human immune systems have evolved to anticipate certain types of microbial input, making the inevitable exposure into a necessity. The organisms that are implicated in the hygiene hypothesis are not proven to cause the disease prevalence, however there are sufficient data on lactobacilli, saprophytic environment mycobacteria, and helminths and their association. These bacteria and parasites have commonly been found in vegetation, mud, and water throughout evolution.\n\nMultiple possible mechanisms have been proposed for how the 'Old Friends' microorganisms prevent autoimmune diseases and asthma. They include: \n\nThe \"microbial diversity\" hypothesis, proposed by Paolo Matricardi and developed by von Hertzen, holds that diversity and turnover of bacterial species in the gut mucosa and other sites is a key factor for priming and regulating the immune system, rather than stable colonization with a particular species. It is not clear whether diversity per se, or that a diverse population will include certain organisms without which the immune system fails to develop. Rook likened the embryonic immune system to a computer that contains programmes but little data. During gestation and infancy exposure to diverse organisms builds a \"database\" that allows the immune system to identify and respond to harmful agents and normalize once the danger is eliminated.\n\nFor allergic disease, the most important times for exposure are: early in development; later during pregnancy; and the first few days or months of infancy. Exposure needs to be maintained over a significant period. This fits with evidence that delivery by Caesarean section may be associated with increased allergies, whilst breastfeeding can be protective. The extent to which exposures need to be maintained after infancy and whether these conditions could be managed by on-going exposure is as yet unknown.\n\nHumans and the microbes they harbor have co-evolved for thousands of centuries; however, it is thought that the human species has gone through numerous phases in history characterized by different pathogen exposures. For instance, in very early human societies, small interaction between its members has given particular selection to a relatively limited group of pathogens that had high transmission rates. When societies became larger, the introduction of agriculture some 10,000 years ago made the spreading of new pathogens more likely, and thus exposures to pathogens that favored high population densities to thrive. Furthermore, pastoralism has made zoonotic pathogen transmissions even more favorable. It is considered that the human immune system is likely subjected to a selective pressure from pathogens that are responsible for down regulating certain alleles and therefore phenotypes in humans, the thalassemia genes that are shaped by the \"Plasmodium\" species expressing the selection pressure being a model for this theory.\n\nRecent comparative genomic studies have shown that immune response genes (protein coding and non-coding regulatory genes) have less evolutionary constraint, and are rather more frequently targeted by positive selection from pathogens that coevolve with the human subject. Of all the various types of pathogens known to cause disease in humans, helminths warrant special attention, because of their ability to modify the prevalence or severity of certain immune-related responses in human and mouse models. In fact recent research has shown that parasitic worms have served as a stronger selective pressure on select human genes encoding interleukins and interleukin receptors when compared to viral and bacterial pathogens. Helminths are thought to have been as old as the adaptive immune system, suggesting that they may have co-evolved, also implying that our immune system has been strongly focused on fighting off helminthic infections, insofar as to potentially interact with them early in infancy. The host-pathogen interaction is a very important relationship that serves to shape the immune system development early on in life.\n\nAllergic conditions are caused by inappropriate immunological responses to harmless antigens driven by a T2-mediated immune response, T2 cells produce interleukin 4, interleukin 5, interleukin 6, interleukin 13 and predominantly immunoglobulin E. Many bacteria and viruses elicit a T1-mediated immune response, which down-regulates T2 responses. T1 immune responses are characterized by the secretion of pro-inflammatory cytokines such as interleukin 2, IFNγ, and TNFα. Factors that favor a predominantly T1 phenotype include: older siblings, large family size, early day care attendance, infection (TB, measles, or hepatitis), rural living, or contact with animals. A T2-dominated phenotype is associated with high antibiotic use, western lifestyle, urban environment, diet, and sensitivity to dust mites and cockroaches. T1 and T2 responses are reciprocally inhibitory, so when one is active, the other is suppressed.\n\nThe mechanism of action of the hygiene hypothesis was insufficient stimulation of the T1 arm, stimulating the cell defence of the immune system and leading to an overactive mother T2 arm, stimulating the antibody-mediated immunity of the immune systems, which in turn led to allergic disease.\nThis explanation however, cannot explain the rise in incidence (similar to the rise of allergic diseases) of several T1-mediated autoimmune diseases, including inflammatory bowel disease, multiple sclerosis and type I diabetes. [Figure 1Bach] However, the North South Gradient seen in the prevalence of multiple sclerosis has been found to be inversely related to the global distribution of parasitic infection.[Figure 2Bach] Additionally, research has shown that MS patients infected with parasites displayed T2 type immune responses as opposed to the proinflammatory T1 immune phenotype seen in non-infected multiple sclerosis patients.[Fleming] Parasite infection has also been shown to improve inflammatory bowel disease and may act in a similar fashion as it does in multiple sclerosis.[Lee]\n\nAn alternative explanation is that the developing immune system must receive stimuli (from infectious agents, symbiotic bacteria, or parasites) to adequately develop regulatory T cells. Without that stimuli it becomes more susceptible to autoimmune diseases and allergic diseases, because of insufficiently repressed T1 and T2 responses, respectively. For example, all chronic inflammatory disorders show evidence of failed immunoregulation. Secondly, helminths, non-pathogenic ambient pseudocommensal bacteria or certain gut commensals and probiotics, drive immunoregulation. They block or treat models of all chronic inflammatory conditions. Thirdly, some such organisms (or molecules that they secrete), specifically expand populations of regulatory T cells (Treg), or cause dendritic cells to switch to regulatory forms that preferentially drive immunoregulation. Finally, when multiple sclerosis patients become infected with helminths, the disease stops progressing and circulating myelin-recognising regulatory T cells appear in the peripheral blood. This indicates that helminths act as adjuvants for regulatory T cells. This observation led to clinical trials.\n\nThe hygiene hypothesis is supported by epidemiological data. Studies have shown that various immunological and autoimmune diseases are much less common in the developing world than the industrialized world and that immigrants to the industrialized world from the developing world increasingly develop immunological disorders in relation to the length of time since arrival in the industrialized world. This is true for asthma and other chronic inflammatory disorders.\n\nRecently, \"Opisthorchis felineus\" chronic helminthic infection in the endemic region of Russia was found to be associated with lower total serum cholesterol levels and a significant attenuation of atherosclerosis in humans.\n\nIn developed countries where childhood diseases were eliminated, the asthma rate for youth is approximately 10%. In the 19th century, hay-fever, an easily recognisable allergy, was a very rare condition.\n\nLongitudinal studies in Ghana demonstrate an increase in immunological disorders as it grew more affluent and presumably cleaner. These results have been replicated by Weinberg et al. who amassed data from a variety of African countries comparing urban and rural environments as well as high and low socioeconomic status (SES). In all four countries urban and high SES groups had a higher prevalence of exercise induced bronchospasm. The use of antibiotics in the first year of life has been linked to asthma and other allergic diseases. The use of antibacterial cleaning products has also been associated with higher incidence of asthma. Increased asthma rates are associated with birth by Caesarean section. The data supporting links to antibiotic use and caesarean section (but not to antibacterial use) are rapidly strengthening.\n\nAntibiotic usage, which reduces the diversity of gut microbiota, is another cited factor. Although several studies have shown associations between antibiotic use and later development of asthma or allergy, other studies suggest that the effect is due to more frequent antibiotic use in asthmatic children. Trends in vaccine use may also be relevant, but epidemiological studies provide no consistent support for a detrimental effect of vaccination/immunization on atopy rates. In support of the old friends hypothesis, the intestinal microbiome was found to differ between allergic and non-allergic Estonian and Swedish children (although this finding was not replicated in a larger cohort), and the biodiversity of the intestinal flora in patients with Crohn’s disease was diminished.\n\nIn 2015, a study found that washing dishes by hand as opposed to using a dishwasher, along with eating food directly from a farm or fermented food, might lead to reduced risk of certain conditions, including asthma, eczema, and possibly hay fever, though the data found on hay fever was not regarded as statistically significant. It was stated, however, that more research was needed to determine if there was an actual causal effect between these practices and a reduced risk of allergies and asthma.\n\nOne study showed that Staphylococci helped reduce inflammation. Early life exposure to specific microbe-enriched environments decreases susceptibility to diseases, such as inflammatory bowel disease and asthma, whereas its absence, as in antibiotic treatment during childhood, may have the opposite effect.\n\nSince allergies and other chronic inflammatory diseases are largely diseases of the last 100 years or so, the \"hygiene\" revolution of the last 200 years came under scrutiny as a possible cause. During the 1800s radical improvements to sanitation and water quality occurred in Europe and North America. The introduction of toilets and sewer systems and the cleanup of city streets, and cleaner food were part of this program. This in turn led to a rapid decline in infectious diseases, particularly during the period 1900-1950, through reduced exposure to infectious agents.\n\nPublic health activities have also played a part in affecting diet and lifestyle, such as physical activity levels and locations.\n\nIt has been suggested that public awareness of the initial form of the \"hygiene hypothesis\" has led to an \"increased disregard\" for hygiene in the home.\n\nWhile no hygiene-related treatments are part of the standard of care, various approaches are under investigation. Helminth therapy is one alternative. Probiotics (drinks or foods) have never been shown to reintroduce microbes to the gut. As yet, therapeutically relevant microbes have not been specifically identified.\n\nLifestyle changes could increase microbial exposure, but whether this on balance improves the balance of risks remains the subject of research. Proposals include natural childbirth, sustained breast feeding and physical interaction between siblings, and encouraging children to spend more time in \"uncleaned\" outdoor environments.\n\nShould these therapies become accepted, public policy implications include providing green spaces in urban areas or even providing access to agricultural environments for children.\n\nHelminthic therapy is the treatment of autoimmune diseases and immune disorders by means of deliberate infestation with a helminth larva or ova. Helminthic therapy is currently being studied as a promising treatment for several (non-viral) autoimmune diseases including Crohn's disease, multiple sclerosis, asthma, and ulcerative colitis. Autoimmune liver disease can be modulated by active helminth infections.\n\nThe anti-inflammatory effects of helminth infection are prompting interest and research into diseases that involve inflammation but that are not currently considered to include autoimmunity or immune dysregulation as a causative factor. Heart disease and arteriosclerosis both have similar epidemiological profiles as autoimmune diseases and both involve inflammation. Their increased incidence cannot be solely attributed to environmental factors. Recent research explored the eradication of helminths as contributing to this discrepancy.\n\nHelminthic therapy emerged from the search for reasons why the incidence of immunological disorders and autoimmune diseases correlates with the level of industrial development.\n\nRelated therapies include use other types of infectious organisms, such as protozoa.\n\nNo evidence supports the idea that reducing modern practices of cleanliness and hygiene would have any impact on rates of chronic inflammatory and allergic disorders, but a significant amount of evidence that it would increase the risks of infectious diseases.\n\nIf home and personal cleanliness contributes to reduced exposure to vital microbes, its role is likely to be small. The idea that homes can be made “sterile” through excessive cleanliness is implausible. The evidence shows that, as fast as they are removed by cleaning, microbes are replaced, via dust and air from outdoors, by shedding from the body and other living things as well as from food. The key point may be that the microbial content of urban housing has altered, not because of home and personal hygiene habits, but because they are part of urban environments. Diet and lifestyle changes also affects the gut, skin and respiratory microbiota.\n\nAt the same time that concerns about allergies and other chronic inflammatory diseases have been increasing, so also have concerns about infectious disease. Infectious diseases continue to exert a heavy health toll. Preventing pandemics and reducing antibiotic resistance are global priorities. Hygiene is a cornerstone of containing these threats.\n\nThe International Scientific Forum on Home Hygiene has developed a risk management approach to reducing home infection risks. This approach uses microbiological and epidemiological evidence to identify the key routes of infection transmission in the home. These data indicate that the critical routes involve the hands, hand and food contact surfaces and cleaning utensils. Clothing and household linens involve somewhat lower risks. Surfaces that contact the body, such as baths and hand basins, can act as infection vehicles, as can surfaces associated with toilets. Airborne transmission can be important for some pathogens. A key aspect of this approach is that it maximises protection against pathogens and infection, but is more relaxed about visible cleanliness in order to sustain normal exposure to other human, animal and environmental microbes.\n\nThere are other hypotheses that try to explain the increase in allergies in developed nations. Major areas of focus include infant feeding, over-exposure and exposure to certain pollutants.\n\nInfant feeding topics includes breastfeeding, when babies begin to eat solid foods and the type of these foods, cow's milk vs other milks and variations in milk processing.\n\nOver-exposure to allergens in occupational situations can cause allergic responses, such as Laboratory animal allergy, bird lung, farmer's lung and bakers lung (See Wheat allergy).\n\nThe pool chlorine hypothesis was proposed by Albert Bernard and his colleagues as an alternative hypothesis based on epidemiological evidence in 2003.\n\n\n\n"}
{"id": "423331", "url": "https://en.wikipedia.org/wiki?curid=423331", "title": "IDEF", "text": "IDEF\n\nIDEF, initially abbreviation of ICAM Definition, renamed in 1999 as Integration DEFinition, refers to a family of modeling languages in the field of systems and software engineering. They cover a wide range of uses, from functional modeling to data, simulation, object-oriented analysis/design and knowledge acquisition. These \"definition languages\" were developed under funding from U.S. Air Force and although still most commonly used by them, as well as other military and United States Department of Defense (DoD) agencies, are in the public domain.\n\nThe most-widely recognized and used components of the IDEF family are IDEF0, a functional modeling language building on SADT, and IDEF1X, which addresses information models and database design issues.\n\nIDEF refers to a family of modeling language, which cover a wide range of uses, from functional modeling to data, simulation, object-oriented analysis/design and knowledge acquisition. Eventually the IDEF methods have been defined up to IDEF14: \n\nIn 1995 only the IDEF0, IDEF1X, IDEF2, IDEF3 and IDEF4 had been developed in full. Some of the other IDEF concepts had some preliminary design. Some of the last efforts were new IDEF developments in 1995 toward establishing reliable methods for business constraint discovery IDEF9, design rationale capture IDEF6, human system, interaction design IDEF8, and network design IDEF14. \n\nThe methods IDEF7, IDEF10, IDEF11, IDEF 12 and IDEF13 haven't been developed any further than their initial definition.\n\nIDEF originally stood for \"ICAM Definition\", initiated in the 1970s at the US Air Force Materials Laboratory, Wright-Patterson Air Force Base in Ohio by Dennis E. Wisnosky, Dan L. Shunk and others. and completed in the 1980s. IDEF was a product of the Integrated Computer-Aided Manufacturing (ICAM) initiative of the United States Air Force. The IEEE recast the IDEF abbreviation as \"Integration DEFinition.\"\n\nThe specific projects that produced IDEF were ICAM project priorities 111 and 112 (later renumbered 1102). The subsequent Integrated Information Support System (IISS) project priorities 6201, 6202, and 6203 attempted to create an information processing environment that could be run in heterogeneous physical computing environments. Further development of IDEF occurred under those projects as a result of the experience gained from applications of the new modeling techniques. The intent of the IISS efforts was to create 'generic subsystems' that could be used by a large number of collaborating enterprises, such as U.S. defense contractors and the armed forces of friendly nations.\n\nAt the time of the ICAM 1102 effort there were numerous, mostly incompatible, data model methods for storing computer data — sequential (VSAM), hierarchical (IMS), network (Cincom's TOTAL and CODASYL, and Cullinet's IDMS). The relational data model was just emerging as a promising way of thinking about structuring data for easy, efficient, and accurate access. Relational database management systems had not yet emerged as a general standard for data management.\n\nThe ICAM program office deemed it valuable to create a \"neutral\" way of describing the data content of large-scale systems. The emerging academic literature suggested that methods were needed to process data independently of the way it was physically stored. Thus the IDEF1 language was created to allow a neutral description of data structures that could be applied regardless of the storage method or file access method.\n\nIDEF1 was developed under ICAM program priority 1102 by Dr Robert R. Brown of the Hughes Aircraft Company, under contract to SofTech, Inc. Dr Brown had previously been responsible for the development of IMS while working at Rockwell International. Rockwell chose not to pursue IMS as a marketable product but IBM, which had served as a support contractor during development, subsequently took over the product and was successful in further developing it for market. Dr Brown credits his Hughes' colleague Mr Timothy Ramey as the inventor of IDEF1 as a viable formalism for modeling information structures. The two Hughes' researchers built on ideas from and interactions with many luminaries in the field at the time. In particular, IDEF1 draws on the following techniques:\nThe effort to develop IDEF1 resulted in both a new method for information modeling and an example of its use in the form of a \"reference information model of manufacturing.\" This latter artifact was developed by D. S. Coleman of the D. Appleton Company (DACOM) acting as a sub-contractor to Hughes and under the direction of Mr Ramey. Personnel at DACOM became quite expert at IDEF1 modeling and subsequently produced a training course and accompanying materials for the IDEF1 modeling technique.\n\nExperience with IDEF1 revealed that the translation of \"information requirements\" into \"database designs\" was more difficult than had originally been anticipated. The most beneficial value of the IDEF1 information modeling technique was its ability to represent data independent of how those data were to be stored and used. It provided data modelers and data analysts with a way to represent \"data requirements\" during the requirements-gathering process. This allowed designers to decide which DBMS to use after the nature of the data requirements was understood and thus reduced the \"misfit\" between data requirements and the capabilities and limitations of the DBMS. The translation of IDEF1 models to database designs, however, proved to be difficult.\n\nThe IDEF0 functional modeling method is designed to model the decisions, actions, and activities of an organization or system. It was derived from the established graphic modeling language structured analysis and design technique (SADT) developed by Douglas T. Ross and SofTech, Inc.. In its original form, IDEF0 includes both a definition of a graphical modeling language (syntax and semantics) and a description of a comprehensive methodology for developing models. The US Air Force commissioned the SADT developers to develop a function model method for analyzing and communicating the functional perspective of a system. IDEF0 should assist in organizing system analysis and promote effective communication between the analyst and the customer through simplified graphical devices.\n\nTo satisfy the data modeling enhancement requirements that were identified in the IISS-6202 project, a sub-contractor, DACOM, obtained a license to the logical database design technique (LDDT) and its supporting software (ADAM). LDDT had been developed in 1982 by Robert G. Brown of The Database Design Group entirely outside the IDEF program and with no knowledge of IDEF1. LDDT combined elements of the relational data model, the E-R model, and generalization in a way specifically intended to support data modeling and the transformation of the data models into database designs. The graphic syntax of LDDT differed from that of IDEF1 and, more importantly, LDDT contained interrelated modeling concepts not present in IDEF1. Mary E. Loomis wrote a concise summary of the syntax and semantics of a substantial subset of LDDT, using terminology compatible with IDEF1 wherever possible. DACOM labeled the result IDEF1X and supplied it to the ICAM program. \n\nBecause the IDEF program was funded by the government, the techniques are in the public domain. In addition to the ADAM software, sold by DACOM under the name Leverage, a number of CASE tools use IDEF1X as their representation technique for data modeling. \n\nThe IISS projects actually produced working prototypes of an information processing environment that would run in heterogeneous computing environments. Current advancements in such techniques as Java and JDBC are now achieving the goals of ubiquity and versatility across computing environments which was first demonstrated by IISS.\n\nThe third IDEF (IDEF2) was originally intended as a user interface modeling method. However, since the Integrated Computer-Aided Manufacturing (ICAM) program needed a simulation modeling tool, the resulting IDEF2 was a method for representing the time varying behavior of resources in a manufacturing system, providing a framework for specification of math model based simulations. It was the intent of the methodology program within ICAM to rectify this situation but limitation of funding did not allow this to happen. As a result, the lack of a method which would support the structuring of descriptions of the user view of a system has been a major shortcoming of the IDEF system. The basic problem from a methodology point of view is the need to distinguish between a description of what a system (existing or proposed) is supposed to do and a representative simulation model that will predict what a system will do. The latter was the focus of IDEF2, the former is the focus of IDEF3.\n\nThe development of IDEF4 came from the recognition that the modularity, maintainability and code reusability that results from the object-oriented programming paradigm can be realized in traditional data processing applications. The proven ability of the object-oriented programming paradigm to support data level integration in large complex distributed systems is also a major factor in the widespread interest in this technology from the traditional data processing community.\n\nIDEF4 was developed as a design tool for software designers who use object-oriented languages such as the Common Lisp Object System, Flavors, Smalltalk, Objective-C, C++, and others. Since effective usage of the object-oriented paradigm requires a different thought process than used with conventional procedural or database languages, standard methodologies such as structure charts, data flow diagrams, and traditional data design models (hierarchical, relational, and network) are not sufficient. IDEF4 seeks to provide the necessary facilities to support the object-oriented design decision making process.\n\nIDEF5, or \"integrated definition for ontology description capture method\", is a software engineering method to develop and maintain usable, accurate, domain ontologies. In the field of computer science ontologies are used to capture the concept and objects in a specific domain, along with associated relationships and meanings. In addition, ontology capture helps coordinate projects by standardizing terminology and creates opportunities for information reuse. The IDEF5 Ontology Capture Method has been developed to reliably construct ontologies in a way that closely reflects human understanding of the specific domain.\n\nIn the IDEF5 method, an ontology is constructed by capturing the content of certain assertions about real-world objects, their properties and their interrelationships, and representing that content in an intuitive and natural form. The IDEF5 method has three main components: A graphical language to support conceptual ontology analysis, a structured text language for detailed ontology characterization, and a systematic procedure that provides guidelines for effective ontology capture.\n\nIDEF6, or \"integrated definition for design rationale capture\", is a method to facilitate the acquisition, representation, and manipulation of the design rationale used in the development of enterprise systems. Rationale is the reason, justification, underlying motivation, or excuse that moved the designer to select a particular strategy or design feature. More simply, rationale is interpreted as the answer to the question, “Why is this design being done in this manner?” Most design methods focus on what the design is (i.e. on the final product, rather than why the design is the way it is). \n\nIDEF6 will be a method that possesses the conceptual resources and linguistic capabilities needed\nIDEF6 is applicable to all phases of the information system development process, from initial conceptualization through both preliminary and detailed design activities. To the extent that detailed design decisions for software systems are relegated to the coding phase, the IDEF6 technique should be usable during the software construction process as well.\n\nIDEF8, or \"integrated definition for human-system interaction design\", is a method for producing high-quality designs of interactions between users and the systems they operate. Systems are characterized as a collection of objects that perform functions to accomplish a particular goal. The system with which the user interacts can be any system, not necessarily a computer program. Human-system interactions are designed at three levels of specification within the IDEF8 method. The first level defines the philosophy of system operation and produces a set of models and textual descriptions of overall system processes. The second level of design specifies role-centered scenarios of system use. The third level of IDEF8 design is for human-system design detailing. At this level of design, IDEF8 provides a library of metaphors to help users and designers specify the desired behavior in terms of other objects whose behavior is more familiar. Metaphors provide a model of abstract concepts in terms of familiar, concrete objects and experiences.\n\nIDEF9, or \"integrated definition for business constraint discovery\", is designed to assist in the discovery and analysis of constraints in a business system. A primary motivation driving the development of IDEF9 was an acknowledgment that the collection of constraints that forge an enterprise system is generally poorly defined. The knowledge of what constraints exist and how those constraints interact is incomplete, disjoint, distributed, and often completely unknown. This situation is not necessarily alarming. Just as living organisms do not need to be aware of the genetic or autonomous constraints that govern certain behaviors, organizations can (and most do) perform well without explicit knowledge of the glue that structures the system. In order to modify business in a predictable manner, however, the knowledge of these constraints is as critical as knowledge of genetics is to the genetic engineer.\n\nIDEF14, or \"integrated definition for network design method\", is a method that targets the modeling and design of computer and communication networks. It can be used to model existing (\"as is\") or envisioned (\"to be\") networks. It helps the network designer to investigate potential network designs and to document design rationale. The fundamental goals of the IDEF14 research project developed from a perceived need for good network designs that can be implemented quickly and accurately.\n\n\n\n"}
{"id": "1857978", "url": "https://en.wikipedia.org/wiki?curid=1857978", "title": "Lantern clock", "text": "Lantern clock\n\nA lantern clock is a type of antique weight-driven wall clock, shaped like a lantern. They were the first type of clock widely used in private homes. They probably originated before 1500 but only became common after 1600; in Britain around 1620. They became obsolete in the 19th century.\n\nThere are two theories of the origin of the name \"lantern clock\". One is that it refers to brass, the main metal of which English lantern clocks are made. Clocks were first made on the continent, at first of iron with iron wheels, and then later with brass wheels. Later still, in France, Belgium and The Netherlands, clocks began to be made from brass. Brass alloys were then called latten, and it seems likely that brass clocks would have been called \"latten clocks\" (or \"latten horloge\" or \"latten uhr\" in the native languages) to distinguish them from iron clocks, and that \"lantern\" could be an English interpretation or corruption of latten. The other is that the name derived from the shape; the clock resembles a rectangular lantern of that period, and like a lantern was hung on the wall.\n\nIn inventories of deceased clockmakers, lantern clocks are usually referred to as \"house clocks\", \"chamber clocks\" or simply \"clocks\", since in 17th century England they were almost the only type of domestic clocks that existed. It was only after a century had passed, when other types of domestic clocks began to be used in British houses, that more descriptive names for it appeared. Other names used for lantern clocks are \"bedpost\", \"birdcage\" or \"Cromwellian\" clocks. \"Sheep's head clock\" was a nickname term for a type of lantern clock that had an extremely large chapter ring covering almost the entire front.\n\nThe English lantern clock is closely related to lantern clocks that can be found on the European continent. A group of craftsmen from the Low Lands (Flanders) and France, of which some were clockmakers, had established themselves in London at the end of the 16th century. At the same time the middle classes in towns and cities of England began to prosper and the need arose for domestic clocks. Until that time clocks in English houses were confined to the nobility; ordinary people were dependent on sundials, or the tower clocks of local churches.\n\nIt is generally accepted that the first lantern clocks in England were made by Frauncoy Nowe and Nicholas Vallin, two Huguenots who had fled from the Low Lands.\n\nLantern clocks were made almost entirely of brass, whereas most earlier clocks had been constructed from iron and wood. Typical lantern clocks comprised a square case on ball or urn feet, a large circular dial (with a chapter ring extending beyond the width of the case on early examples), a single hour hand, and a large bell and finial. The clocks usually had ornate pierced fretwork on top of the frame.\n\nThe main style characteristics of English lantern clocks are similar to its Continental relatives: a wall clock with square bottom and top plates surmounted by a large bell, four corner pillars, a series of vertical plates positioned behind each other and a 30-hour movement with one or more weights. At the start of the 17th century, the style gradually evolved to a standard to which all clockmakers more or less complied. The guild supervised the clockmakers, who were compelled to work within a prescribed method. Suppliers to the clockmakers' trade contributed to this general style as well. For example, the brass founders supplied stylistically identical clock posts to several clockmakers. In contrast to the Dutch variants, such as stool clocks, English lantern clocks were entirely made of metal (brass and steel).\n\nLantern clocks were originally weight-driven: usually one weight for time keeping and a second for striking. A few later lantern-style clocks were constructed with spring mechanisms, and many surviving examples of the original weight-driven type have been converted to spring or pendulum mechanisms.\n\nIn only a few decades the lantern clock became very popular in London, and from there its popularity spread to the entire country. This is evident from the large number of lantern clocks that still exist. Dozens of clockmakers produced great numbers of these clocks in the city of London during the 17th century. This huge productivity was the result of the high demand for this popular clock in combination with an effective guild system. In 1631 King Charles I granted a charter for a clockmaker guild in London: the Worshipful Company of Clockmakers, which exists to this day. Many of the well-known clockmakers from that era were freemen of this guild. Many small companies were established in Lothbury in London that functioned as suppliers for the clockmakers. A clockmaker could benefit from the services of brass founders who supplied cast brass clock parts, dial plates, finials, pillars, frets etc., or employ engravers who would carry out the engraving of the dial plates and frets. The guild assured the quality of the products that left the clockmakers' workshops. Before a clockmaker could become a freeman, able to set up his own shop, he had to spend 7 years as an apprentice learning the trade. This ensured independent clockmakers a plentiful supply of apprentices, who were also cheap labourers who helped to attain this high productivity.\n\nStyle characteristics were copied from prints that were available for craftsmen. Under the influence of the Renaissance, prints with motives and patterns from the Classical antiquity found their way to the workshops. They served as examples for the clock pillars that were inspired by columns from Greek temples. During the 17th century the tulip became very popular to an extent of a real tulip mania. Prints with pictures of tulips were a rewarding subject for the adornment of the dial plates of lantern clocks. In the early 17th century, lantern clocks got their characteristic shape, which hardly changed during the 17th and halfway through the next century as a result of all this.\n\nThe London Clockmakers equipped their lantern clocks with four pillars inspired by classical columns. Attached to these pillars are classical vase-shaped finials and well-shaped feet. To those finials a bell strap is attached that spreads from four corners and holds a bell. To hide the hammer and the clock movement from the spectator three frets are attached to the finials. The front fret is pierced and engraved whereas the two side frets are pierced but usually left blank. The front of the clock case consists of an engraved dial plate on which a circular dial ring is attached. Almost all lantern clocks originally had just one clock hand to indicate the hours. A standard lantern clock strikes the hours on a large bell and is often equipped with an alarm that rings the same bell. Two doors provide access to the movement and are hinged at the sides of the clock. One or more weights are hanging from ropes or chains at the bottom of the clock.\n\nLantern clocks were produced in vast numbers during the decades before the pioneering invention of the pendulum by the Dutch scientist Christiaan Huygens in 1656. Before this invention, lantern clocks used a balance wheel lacking a balance spring for their timekeeping element, which limited their accuracy to perhaps 15 minutes per day. Shortly after Huygens' invention, the bob pendulum was introduced in England, and most English clockmakers adopted the new system quickly. The pendulum increased the accuracy of clocks so greatly that many existing clocks were converted, with pendulums being added at the back. Measuring time became much more accurate, but most clockmakers kept building lantern clocks without minute hands: this maybe just a matter of tradition. The result was that clockmakers started to develop other types of domestic clocks. Longcase clocks with 8-day movements made lantern clocks obsolete, and gradually lantern clocks disappeared from the London interiors in the first decades of the 18th century. In rural areas lantern clocks were produced until the beginning of the 19th century, and in those years they were also exported to countries like Turkey, and supplied with oriental numbers on their dials. In the Victorian era there was a revival of interest in antique lantern clocks. Unfortunately this also meant that many clocks of renowned makers were stripped of their movements, which were replaced by 'modern' winding movements. Nowadays unmodified original lantern clocks are very rare.\n\n\n"}
{"id": "271786", "url": "https://en.wikipedia.org/wiki?curid=271786", "title": "Meliorism", "text": "Meliorism\n\nMeliorism is an idea in metaphysical thinking holding that progress is a real concept leading to an improvement of the world. It holds that humans can, through their interference with processes that would otherwise be natural, produce an outcome which is an improvement over the aforementioned natural one.\n\nMeliorism, as a conception of the person and society, is at the foundation of contemporary liberal democracy and human rights and is a basic component of liberalism.\n\nAnother important understanding of the meliorist tradition comes from the American Pragmatic tradition. One can read about it in the works of Lester Frank Ward, William James, and John Dewey.\n\nMeliorism has also been used by Arthur Caplan to describe positions in bioethics that are in favor of ameliorating conditions which cause suffering, even if the conditions have long existed (e.g. being in favor of cures for common diseases, being in favor of serious anti-aging therapies as they are developed).\n\nA closely related concept discussed by Jean-Jacques Rousseau and Marquis de Condorcet is that of perfectibility of man. \n\nCondorcet's statement, \"Such is the object of the work I have undertaken; the result of which will be to show, from reasoning and from facts, that no bounds have been fixed to the improvement of the human faculties; that the \"perfectibility of man\" is absolutely indefinite; that the progress of this perfectibility, henceforth above the controul of every power that would impede it, has no other limit than the duration of the globe upon which nature has placed us.\" anticipates James' meliorism.\n\nRousseau's treatment is somewhat weaker.\n\n\n"}
{"id": "32327804", "url": "https://en.wikipedia.org/wiki?curid=32327804", "title": "Memorialization", "text": "Memorialization\n\nMemorialization generally refers to the process of preserving memories of people or events. It can be a form of address or petition, or a ceremony of remembrance or commemoration.\n\nMemorialization is a universal need for both those being memorialized and those who are grieving. Although historically it was limited to the elite and only practiced in the highest societal classes, it is now almost considered a fundamental human right for all people.\n\nIn the context of transitional justice, memorialization is used to honor the victims of human rights abuses. Memorials can help governments reconcile tensions with victims by demonstrating respect and acknowledging the past. They can also help to establish a record of history, and to prevent the recurrence of abuse.\n\nMemorials can also be serious social and political forces in democracy-building efforts.\n\nMemorials are also a form of reparations, or compensation efforts that seek to address past human rights violations. They aim to provide compensation for losses endured by victims of abuse, and remedy prior wrongdoing. They also publicly recognize that victims are entitled to redress and respect. The United Nations Basic Principles on the Right to a Remedy and Reparation recognizes “commemorations and tributes to the victims” as a form of reparation.\n\nThere are numerous types of memorials used as transitional justice initiatives. These include architectural memorials, museums, and other commemorative events. For instance, in northern Uganda, monuments, annual prayer ceremonies, and a mass grave were created in response to the war conducted by and against the Lord’s Resistance Army there.\n\nAnother example is the Museum of Memory and Human Rights in Chile, which was created to document abuses by the former military dictatorship there.\n\nMemorialization can arouse controversy and present certain risks. In unstable political situations, memorials may increase desire for revenge and catalyze further violence. They are highly politicized processes that represent the will of those in power. They are thus difficult to shape, and international relief workers, peacekeepers, and NGOs risk being drawn into disputes about the creation or maintenance of memorial sites. Yet they also have the potential to redress historical grievances and enable societies to progress.\n\n\n\n"}
{"id": "2118562", "url": "https://en.wikipedia.org/wiki?curid=2118562", "title": "Moral economy", "text": "Moral economy\n\nA moral economy is an economy that is based on goodness, fairness, and justice, as opposed to one where the market is assumed to be independent of such concerns. A moral economy is generally only possible in small, closely knit communities, where the principles of mutuality—i.e. \"I'll scratch your back if you'll scratch mine\"—operate to avoid the free rider problem. Where economic transactions arise between strangers who cannot be sanctioned by a social network, the free rider problem lacks a solution and a moral economy becomes harder to maintain. The concept was an elaboration by English historian E.P. Thompson, from a term already used by various eighteenth century authors, who felt that economic and moral concerns increasingly seemed to drift apart (see Götz 2015). \n\nThompson wrote of the moral economy of the poor in the context of widespread food riots in the English countryside in the late eighteenth century. According to Thompson these riots were generally peaceable acts that demonstrated a common political culture rooted in feudal rights to \"set the price\" of essential goods in the market. These peasants held that a traditional \"fair price\" was more important to the community than a \"free\" market price and they punished large farmers who sold their surpluses at higher prices outside the village while there were still those in need within the village. In the 1970s the concept of a moral economy was developed further in anthropological studies of peasant economies. The notion of a non-capitalist cultural mentality using the market for its own ends has been linked by others (with Thompson's approval) to subsistence agriculture and the need for subsistence insurance in hard times.\nThe concept was widely popularized in anthropology through the book \"\" by James C. Scott (1976). The book begins with a telling metaphor of peasants being like a man standing up to his nose in water; the smallest wave will drown him. Similarly, peasants generally live so close to the subsistence line that it takes little to destroy their livelihoods. From this, he infers a set of economic principles that it would be rational for them to live by. It is important to emphasize that this book was not based on fieldwork, and itself proposed a cross-cultural universalistic model of peasant economic behaviour based upon a set of fixed theoretical principles, not a reading of peasant culture. Firstly, he argued that peasants were \"risk averse\", or, put differently, followed a \"safety first\" principle. They would not adopt risky new seeds or technologies, no matter how promising, because tried and true traditional methods had demonstrated, not promised, effectiveness. This gives peasants an unfair reputation as \"traditionalist\" when in fact they are just risk averse. Secondly, Scott argues that peasant society provides \"subsistence insurance\" for its members to tide them over those occasions when natural or man-made disaster strikes.\n\nA moral economy, in one interpretation, is an economy that is based on goodness, fairness, and justice. Such an economy is generally only stable in small, closely knit communities, where the principles of mutuality—i.e. \"I'll scratch your back if you'll scratch mine\"—operate to avoid the free rider problem. Where economic transactions arise between strangers who cannot be informally sanctioned by a social network, the free rider problem lacks a solution and a moral economy becomes harder to maintain.\n\nIn traditional societies, each person and each household is a consumer as well as a producer. Social networks create mutual understandings to promote the survival of these social units in the face of scarcity; these social ties operate to prevent the economic actors in traditional societies from behaving to maximize personal profit. Traditional understandings arise as to the relative value of various goods and services; they are not independently renegotiated for each transaction in an impersonal, anonymous market. Traditional staple foods and other goods deemed necessary for the survival of the community acquire customary prices; dearth or plenty should be shared by all. These traditional understandings acquire the force of custom, and with increased social complexity may eventually acquire the force of law.\n\n\"The Efficient Society\" by Joseph Heath discusses the nature of a moral economy in these terms, and argues that Canada has achieved the proper balance between social needs and economic freedom, and as such comes close to being a moral economy. Other economists such as John P. Powelson relate the concept of a \"moral economy\" to the balance of economic power; in their view, a moral economy is an economy in which economic factors are balanced against ethical norms in the name of social justice.\n\n\"Right Relationship\" by Brown and Garver, discusses the urgent need for achieving an economy that is recognized to be a subsidiary of the overall ecosystem of the planet. They address key questions regarding the purpose, function, appropriate size, fairness, and governance of a world economic system and propose new ideas to place our economy in correct relationship with the Earth's ecosystem. They argue that such a moral economy is essential if we are to avoid systemic collapse as our \"growth economy\" outstrips the Earth's limited ability to recycle our waste, and as the Earth's inventory of critical raw materials and minerals is used up, in the face of growing population and growing affluence within those populations.\n\nIn a related sense, \"moral economy\" is also a name given in economics, sociology and anthropology to the interplay between cultural mores and economic activity. It describes the various ways in which custom and social pressure coerce economic actors in a society to conform to traditional norms even at the expense of profit.\n\nPrior to the rise of classical economics in the eighteenth century, the economies in Europe and its North American colonies were governed by a variety of (formal and informal) regulations designed to prevent \"greed\" from overcoming \"morality\". In its most formal manifestations, examples such as the traditional Christian and Muslim prohibitions on usury represent the limits imposed by religious values on economic activity, and as such are part of the moral economy. Laws that determine what sort of contracts will be given effect by the judiciary, and what sort of contracts are void or voidable, often incorporate concepts of a moral economy; in many jurisdictions, traditionally a contract involving gambling was considered void in law because it was against public policy. These restrictions on freedom of contract are the results of moral economy. According to the beliefs which inspired these laws, economic transactions were supposed to be based on mutual obligation, not individual gain. In colonial Massachusetts, for example, prices and markets were highly regulated, even the fees physicians could charge.\n\nOther forms of moral economy are more informal. In the sixteenth and seventeenth century, for instance, clergymen often preached against various economic practices that were not strictly illegal, but were deemed to be \"uncharitable\". Their condemnations of selling food at high prices or raising rents probably influenced the behavior of many people who regarded themselves as Christian and worried about their reputations.\n\nLikewise, during the rapid expansion of capitalism over the past several centuries, the tradition of a pre-capitalist \"moral economy\" was used to justify popular action against unscrupulous merchants and traders. For example, the poor regularly rioted against grain merchants who raised their prices in years of dearth in an attempt to reassert the concept of the just price. The Marxist historian E. P. Thompson emphasized the continuing force of this tradition in his pioneering article on \"The Moral Economy of the English Crowd in the Eighteenth Century\" (1971). Later historians and sociologists have uncovered the same phenomenon in a variety of other situations, including peasants' riots in continental Europe during the nineteenth century and in many developing countries in the twentieth. The political scientist James C. Scott, for example, showed how this ideology could be used as a method of resisting authority in \"\" (1976).\n\nIt must be remembered, however, that sometimes a moral economy may not act in conformity to morality as it is now generally understood. Social pressures to enforce racial segregation even when willing buyers and sellers would erode the racial barriers, are clearly an example of cultural pressures imposing economic inefficiency, and therefore fall within the purview of moral economy.\n\nIn modern times, \"utopian moral economies\" have arisen to systematically reorganize their economic system to reflect a particular moral or ethical code that rejects the free-market ethos of capitalist economies. Societies that pursue some derivative of Socialism or Communism are obvious examples of this impulse, along with small-scale attempts in the form of the Israeli kibbutz and the intentional communities of the 1960s and 70s.\n\nVery few of these experiments—with the possible exception of the kibbutz—turned out the way their founders had imagined. Unsurprisingly, a revolutionary reorganization of some of the most fundamental parts of society often resulted in the severe dislocation of many people's everyday lives and the loss of whole generations to schemes like Stalin's failed policy of collective farming. However, many of the small and pragmatic attempts to make the capitalist economy more moral (e.g. fair trade, moral investment funds, the development of renewable energy sources, recycling, cooperatives, etc.) have grown from the same impulse that drove the utopian revolutionaries. These developments, however, do not fully realize their intentions, being fundamentally at odds with the mechanisms in the capitalist economy, such as cyclical consumption, the inherent duplicity of goods in competition, and the process of \"externalizing\" those costs which are not directly pertinent to an actor's finances.\n\n\n\n"}
{"id": "3917275", "url": "https://en.wikipedia.org/wiki?curid=3917275", "title": "Mottainai", "text": "Mottainai\n\nMottainai is a term of Japanese origin that has been used by environmentalists. The term in Japanese conveys a sense of regret over waste; the exclamation \"\"Mottainai!\" can translate as \"What a waste!\" Japanese environmentalists have used the term to encourage people to \"reduce, reuse and recycle\", and Kenyan environmentalist Wangari Maathai used the term at the United Nations as a slogan to promote environmental protection.\n\n\"Mottainai\" is a Japanese term conveying a sense of regret concerning waste. The expression \"Mottainai!\" can be uttered alone as an exclamation when something useful, such as food or time, is wasted, meaning roughly \"what a waste!\" In addition to its primary sense of \"wastefulness\", the word is also used to mean \"impious; irreverent\" or \"more than one deserves\".\n\n\"Mottainai\" in Japanese refers both to physical waste and to wasteful action. MacQuillan and Preston propose a more elaborate translation that conveys a sense of value and worthiness as \"do not destroy (or lay waste to) that which is worthy\".\n\nIn November 2002, the English-language, Japan-based magazine \"Look Japan\" ran a cover story entitled \"Restyling Japan: Revival of the 'Mottainai' Spirit\", documenting the motivation amongst volunteers in a \"toy hospital\" in Japan to \"develop in children the habit of looking after their possessions,\" the re-emergence of repair shops specializing in repairing household appliances or children's clothes, the recycling of PET bottles and other materials, the collection of waste edible oil, and more generally the efforts to stop the trend of throwing away everything that can no longer be used, i.e. the efforts of reviving \"the spirit of \"mottainai\"\". In that context, Hitoshi Chiba, the author, described \"mottainai\" as follows:\n\nA modern observance that practices mottainai is the yearly festival of Hari-Kuyō, or the Festival of Broken Needles.\n\nAt the Opening Ceremony of the Science and Technology in Society Forum in 2005, Japanese Prime Minister Junichiro Koizumi stated: \"In Japan, there has long been a spirit characterized by the word \"mottainai\", which could be translated as 'don't waste what is valuable'.\"\n\nAt a session of the United Nations, Kenyan environmentalist Wangari Maathai introduced the word \"mottainai\" as a slogan for environmental protection. According to Mizue Sasaki, Maathai has worked to popularize the word \"mottainai\" in places outside Japan. At the 2009 United Nations Summit on Climate Change, she said \"Even at personal level, we can all reduce, re-use and recycle, what is embraced as Mottainai in Japan, a concept that also calls us to express gratitude, to respect and to avoid wastage.\"\n\n\n"}
{"id": "1647173", "url": "https://en.wikipedia.org/wiki?curid=1647173", "title": "Ocular tremor", "text": "Ocular tremor\n\nOcular microtremor (OMT) is a constant, physiological, high frequency (peak 80Hz), low amplitude (estimated circa 150-2500nm (1)) eye tremor. \n\nIt occurs in all normal people even when the eye is apparently still and is due to the constant activity of brainstem oculomotor units. In coma there is a loss of high frequency components of tremor and the extent of this reduction is related to the patient's prognosis (2). Ocular microtremor can potentially help in the difficult diagnosis of brainstem death, as well as monitoring patients while under anaesthesia (3). Abnormal OMT records are seen in neurological conditions such as Parkinson's disease and multiple sclerosis(4). The frequency spectrum also changes with age. \n\nThe first description of what is now known as ocular microtremor was made in 1934 (5). More recent studies are less common for ocular microtremor than for other fixational eye movements. Some have suggested that the reason ocular microtremor studies are more rare may be because of the difficulty inherent in measuring microtremor. It is contentious whether ocular microtremor assists vision. Visual processes deteriorate rapidly in the absence of retinal image motion, with Stabilized Images.\n\nSome have suggested that tremor may not be a distinct eye movement at all.\n\n\n1. Sheahan, N. F., Coakley, D., et al. (1993). \"Ocular microtremor measurement system: design and performance.\" Med Biol Eng Comput 31(3): 205-12.\n\n2. Coakley, D. and J. G. Thomas (1977). \"The ocular microtremor record and the prognosis of the unconscious patient.\" Lancet 1(8010): 512-5.\n\n3. Bojanic, S., T. Simpson, et al. (2001). \"Ocular microtremor: a tool for measuring depth of anaesthesia?\" Br J Anaesth 86(4): 519-22.\n\n4. Bolger, C., S. Bojanic, et al. (2000). \"Ocular microtremor (OMT): a new neurophysiological approach to multiple sclerosis.\" J Neurol Neurosurg Psychiatry 68(5): 639-42.\n\n5. Adler, F. H. M., Fliegelman, Maurice (AB) (1934). \"Influence of Fixation on the Visual Acuity.\" Archives of Ophthalmology 12: 475-483.\n"}
{"id": "229060", "url": "https://en.wikipedia.org/wiki?curid=229060", "title": "Old age", "text": "Old age\n\nOld age refers to ages nearing or surpassing the life expectancy of human beings, and is thus the end of the human life cycle. Terms and euphemisms include old people, the elderly (worldwide usage), seniors (American usage), senior citizens (British and American usages), older adults (in the social sciences), and the elders (in many cultures—including the cultures of aboriginal people).\n\nOld people often have limited regenerative abilities and are more susceptible to disease, syndromes, injuries and sickness than younger adults. The organic process of ageing is called senescence, the medical study of the aging process is called gerontology, and the study of diseases that afflict the elderly is called geriatrics. The elderly also face other social issues around retirement, loneliness, and ageism.\n\nOld age is not a definite biological stage, as the chronological age denoted as \"old age\" varies culturally and historically.\n\nIn 2011, the United Nations proposed a human rights convention that would specifically protect older persons.\n\nDefinitions of old age include official definitions, sub-group definitions, and four dimensions as follows.\n\nOld age comprises \"the later part of life; the period of life after youth and middle age . . ., usually with reference to deterioration\". At what age old age begins cannot be universally defined because it differs according to the context. The United Nations has agreed that 65+ years may be usually denoted as old age and this is the first attempt at an international definition of old age. However, for its study of old age in Africa, the World Health Organization (WHO) set 55 as the beginning of old age. At the same time, the WHO recognized that the developing world often defines old age, not by years, but by new roles, loss of previous roles, or inability to make active contributions to society.\n\nMost developed Western countries set the age of 60 to 65 for retirement. Being 60–65 years old is usually a requirement for becoming eligible for senior social programs. However, various countries and societies consider the onset of old age as anywhere from the mid-40s to the 70s. The definitions of old age continue to change especially as life expectancy in developed countries has risen to beyond 80 years old. In October 2016, a paper published in the science journal \"Nature\" presented the conclusion that the maximum human lifespan is an average age of 115, with an absolute upper limit of 125 years. However, the authors' methods and conclusions drew criticism from the scientific community, who concluded that the study was flawed.\n\nGerontologists have recognized the very different conditions that people experience as they grow older within the years defined as old age. In developed countries, most people in their 60s and early 70s are still fit, active, and able to care for themselves. However, after 75, they will become increasingly frail, a condition marked by serious mental and physical debilitation.\n\nTherefore, rather than lumping together all people who have been defined as old, some gerontologists have recognized the diversity of old age by defining sub-groups. One study distinguishes the young old (60 to 69), the middle old (70 to 79), and the very old (80+). Another study's sub-grouping is young-old (65 to 74), middle-old (75–84), and oldest-old (85+). A third sub-grouping is \"young old\" (65–74), \"old\" (74–84), and \"old-old\" (85+). Delineating sub-groups in the 65+ population enables a more accurate portrayal of significant life changes.\n\nTwo British scholars, Paul Higgs and Chris Gilleard, have added a \"fourth age\" sub-group. In British English, the \"third age\" is \"the period in life of active retirement, following middle age\". Higgs and Gilleard describe the fourth age as \"an arena of inactive, unhealthy, unproductive, and ultimately unsuccessful ageing\".\n\n\"Key Concepts in Social Gerontology\" lists four dimensions: chronological, biological, psychological, and social. Wattis and Curran add a fifth dimension: developmental. Chronological age may differ considerably from a person's functional age. The distinguishing marks of old age normally occur in all five senses at different times and different rates for different persons. In addition to chronological age, people can be considered old because of the other dimensions of old age. For example, people may be considered old when they become grandparents or when they begin to do less or different work in retirement.\n\nSenior citizen is a common euphemism for an old person used in American English, and sometimes in British English. It implies that the person being referred to is retired. This in turn usually implies that the person is over the retirement age, which varies according to country. Synonyms include old age pensioner or pensioner in British English, and retiree and senior in American English. Some dictionaries describe widespread use of \"senior citizen\" for people over the age of 65.\n\nWhen defined in an official context, \"senior citizen\" is often used for legal or policy-related reasons in determining who is eligible for certain benefits available to the age group.\n\nIt is used in general usage instead of traditional terms such as \"old person\", \"old-age pensioner\", or \"elderly\" as a courtesy and to signify continuing relevance of and respect for this population group as \"citizens\" of society, of \"senior\" rank.\n\nThe term was apparently coined in 1938 during a political campaign. Famed caricaturist Al Hirschfeld claimed on several occasion that his father Isaac Hirschfeld invented the term 'senior citizen'. It has come into widespread use in recent decades in legislation, commerce, and common speech. Especially in less formal contexts, it is often abbreviated as \"senior(s)\", which is also used as an adjective.\n\nIn commerce, some businesses offer customers of a certain age a \"senior discount\". The age at which these discounts are available varies between 55, 60, 62 or 65, and other criteria may also apply. Sometimes a special \"senior discount card\" or other proof of age needs to be obtained and produced to show entitlement.\n\nThe age which qualifies for senior citizen status varies widely. In governmental contexts, it is usually associated with an age at which pensions or medical benefits for the elderly become available. In commercial contexts, where it may serve as a marketing device to attract customers, the age is often significantly lower.\n\nIn the United States, the standard retirement age is currently 66 (gradually increasing to 67).\n\nIn Canada, the OAS (Old Age Security) pension is available at 65 (the Conservative government of Stephen Harper had planned to gradually increase the age of eligibility to 67, starting in the years 2023–2029, although the Liberal government of Justin Trudeau is considering leaving it at 65), and the CPP (Canada Pension Plan) as early as age 60.\n\nThe AARP allows couples in which one spouse has reached the age of 50 to join, regardless of the age of the other spouse.\n\nThe distinguishing characteristics of old age are both physical and mental. The marks of old age are so unlike the marks of middle age that legal scholar Richard Posner suggests that, as an individual transitions into old age, he/she can be thought of as different persons \"time-sharing\" the same identity.\n\nThese marks do not occur at the same chronological age for everyone. Also, they occur at different rates and order for different people. Marks of old age can easily vary between people of the same chronological age.\n\nA basic mark of old age that affects both body and mind is \"slowness of behavior\". This \"slowing down principle\" finds a correlation between advancing age and slowness of reaction and physical and mental task performance. However, studies from Buffalo University and Northwestern University have shown that the elderly are a happier age group than their younger counterparts.\n\nPhysical marks of old age include the following:\n\n\nMental marks of old age include the following:\n\n\nMany books by middle-age writers depict their perceptions of old people. One writer notices the change in his parents: they move slowly, they have lost strength, they repeat stories, their minds wander, and they fret. Another writer sees her aged parents and is bewildered: they refuse to follow her advice, they are obsessed with the past, they avoid risk, they live at a \"glacial pace\".\n\nOther writers treat the perceptions of middle-age people regarding their own old age. In her \"The Denial of Aging\", Dr. Muriel R. Gillick, a baby boomer, accuses her contemporaries of believing that by proper exercise and diet they can avoid the scourges of old age and proceed from middle age to death. Studies find that many people in the 55–75 range can postpone morbidity by practicing healthy lifestyles. These discourses take part in a general idea of successful ageing. However, at about age 80, all people experience similar morbidity. Even with healthy lifestyles, most 85+ people will undergo extended \"frailty and disability\".\n\nEarly old age is a pleasant time; children are grown, retirement from work, time to pursue interests. Many people are also willing to get involved in community and activist organizations to promote their well-being. In contrast, perceptions of old age by writers 80+ years old (old age in the real meaning of the term) tend to be negative.\n\nO Sovereign my Lord! Oldness has come; old age has descended. Feebleness has arrived; dotage is here anew. The heart sleeps wearily every day. The eyes are weak, the ears are deaf, the strength is disappearing because of weariness of the heart and the mouth is silent and cannot speak. The heart is forgetful and cannot recall yesterday. The bone suffers old age. Good is become evil. All taste is gone. What old age does to men is evil in every respect.\n\nMinois comments that the scribe's \"cry shows that nothing has changed in the drama of decrepitude between the age of the Pharaoh and the atomic age\" and \"expresses all the anguish of old people in the past and the present\".\n\nLillian Rubin, active in her 80s as an author, sociologist, and psychotherapist, opens her book \"60 on Up: The Truth about Aging in America\" with \"getting old sucks. It always has, it always will.\" Dr. Rubin contrasts the \"real old age\" with the \"rosy pictures\" painted by middle-age writers.\n\nWriting at the age of 87, Mary C. Morrison delineates the heroism required by old age: to live through the disintegration of one's own body or that of someone you love. Morrison concludes, \"old age is not for the fainthearted.\" In the book \"Life Beyond 85 Years\", the 150 interviewees had to cope with physical and mental debilitation and with losses of loved ones. One interviewee described living in old age as \"pure hell\".\n\nBased on his survey of old age in history, concludes that \"it is clear that always and everywhere youth has been preferred to old age.\" In Western thought, \"old age is an evil, an infirmity and a dreary time of preparation for death.\" Furthermore, death is often preferred over \"decrepitude, because death means deliverance\".\n\n\"The problem of the ambiguity of old age has . . . been with us since the stage of primitive society; it was both the source of wisdom and of infirmity, experience and decrepitude, of prestige and suffering.\"\n\nIn the Classical period of Greek and Roman cultures, old age was denigrated as a time of \"decline and decrepitude\". \"Beauty and strength\" were esteemed and old age was viewed as defiling and ugly. Old age was reckoned as one of the unanswerable \"great mysteries\" along with evil, pain, and suffering. \"Decrepitude, which shrivels heroes, seemed worse than death.\"\n\nThe Medieval and Renaissance periods depicted old age as \"cruel or weak\".\n\nHistorical periods reveal a mixed picture of the \"position and status\" of old people, but there has never been a \"golden age of aging\". Studies have disproved the popular belief that in the past old people were venerated by society and cared for by their families. Veneration for and antagonism toward the aged have coexisted in complex relationships throughout history. \"Old people were respected or despised, honoured or put to death according to circumstance.\"\n\nIn ancient times, although some strong and healthy people lived until they were over 70 most died before they were 50. The general understanding is that those who lived into their 40's were treated with respect and awe. In contrast, those who were frail were seen as a burden and ignored or in extreme cases killed. People were defined as \"old\" because of their inability to perform useful tasks rather than their years.\n\n\"The Olympians did not like old people.\" Their youth rebelled against the old, driving them off or killing them.\n\nAlthough he was skeptical of the gods, Aristotle concurred in the dislike of old people. In his \"Ethics\", he wrote that \"old people are miserly; they do not acknowledge disinterested friendship; only seeking for what can satisfy their selfish needs.\"\n\nThe 16th-century Utopians, Thomas More and Antonio de Guevara, allowed no decrepit old people in their fictional lands.\n\nFor Thomas More, on the island of Utopia, when people are so old as to have \"out-lived themselves\" and are terminally ill, in pain, and a burden to everyone, the priests exhort them about choosing to die. The priests assure them that \"they shall be happy after death.\" If they choose to die, they end their lives by starvation or by taking opium.\n\nAntonio de Guevara's utopian nation \"had a custom, not to live longer than sixty five years\". At that age, they practiced self-immolation. Rather than condemn the practice, Bishop Guevara called it a \"golden world\" in which people \"have overcome the natural appetite to desire to live\".\n\nIn the Modern period, the \"cultural status\" of old people has declined in many cultures. Joan Erikson observed that \"aged individuals are often ostracized, neglected, and overlooked; elders are seen no longer as bearers of wisdom but as embodiments of shame.\"\n\nResearch on age-related attitudes consistently finds that negative attitudes exceed positive attitudes toward old people because of their looks and behavior. In his study \"Aging and Old Age\", Posner discovers \"resentment and disdain of older people\" in American society.\n\nHarvard University's implicit-association test measures implicit \"attitudes and beliefs\" about Young vis a vis Old. \"Blind Spot: Hidden Biases of Good People\", a book about the test, reports that 80% of Americans have an \"automatic preference for the young over old\" and that attitude is true worldwide. The young are \"consistent in their negative attitude\" toward the old. \"Ageism\" documents that Americans generally have \"little tolerance for older persons and very few reservations about harboring negative attitudes\" about them.\n\nDespite its prevalence, ageism is seldom the subject of public discourse.\n\nIn 2014, a documentary film called \"The Age of Love\" used humor and poignant adventures of 30 seniors who attend a speed dating event for 70- to 90-year-olds, and discovered how the search for romance changes; or does not change; from a childhood sweetheart to older age.\n\nSimone de Beauvoir wrote that \"there is one form of experience that belongs only to those that are old – that of old age itself.\" Nevertheless, simulations of old age attempt to help younger people gain some understanding.\n\nTexas A&M University offers a plan for an \"Aging Simulation\" workshop. The workshop is adapted from \"Sensitizing People to the Processes of Aging\".\nSome of the simulations follow:\n\nThe Macklin Intergenerational Institute conducts Xtreme Aging workshops, as depicted in \"The New York Times\". A condensed version was presented on \nNBC's Today Show and is available online. One exercise was to lay out 3 sets of 5 slips of paper. On set #1, write your 5 most enjoyed activities; on set #2, write your 5 most valued possessions; on set #3, write your 5 most loved people. Then \"lose\" them one by one, trying to feel each loss, until you have lost them all as happens in old age.\n\nMost people in the age range of 60–80 (the years of retirement and early old age), enjoy rich possibilities for a full life, but the condition of frailty distinguished by \"bodily failure\" and greater dependence becomes increasingly common after that. In the United States, hospital discharge data from 2003 to 2011 shows that injury was the most common reason for hospitalization among patients aged 65+.\n\nGerontologists note the lack of research regarding and the difficulty in defining frailty. However, they add that physicians recognize frailty when they see it.\n\nA group of geriatricians proposed a general definition of frailty as \"a physical state of increased vulnerability to stressors that results from decreased reserves and disregulation in multiple physiological systems\".\n\nFrailty is a common condition in later old age \nbut different definitions of frailty produce diverse assessments of prevalence. One study placed the incidence of frailty for ages 65+ at 10.7%. Another study placed the incidence of frailty in age 65+ population at 22% for women and 15% for men. A Canadian study illustrated how frailty increases with age and calculated the prevalence for 65+ as 22.4% and for 85+ as 43.7%.\n\nA worldwide study of \"patterns of frailty\" based on data from 20 nations found (a) a consistent correlation between frailty and age, (b) a higher frequency among women, and (c) more frailty in wealthier nations where greater support and medical care increases longevity.\n\nIn Norway, a 20-year longitudinal study of 400 people found that bodily failure and greater dependence became prevalent in the 80+ years. The study calls these years the \"fourth age\" or \"old age in the real meaning of the term\". Similarly, the \"Berlin Aging Study\" rated over-all functionality on four levels: good, medium, poor, and very poor. People in their 70s were mostly rated good. In the 80–90 year range, the four levels of functionality were divided equally. By the 90–100 year range, 60% would be considered frail because of very poor functionality and only 5% still possessed good functionality.\n\nIn the United States, the 85+ age group is the fastest growing, a group that is almost sure to face the \"inevitable decrepitude\" of survivors. (Frailty and decrepitude are synonyms.)\n\nThree unique markers of frailty have been proposed: (a) loss of any notion of invincibility, (b) loss of ability to do things essential to one's care, and (c) loss of possibility for a subsequent life stage.\n\nOld age survivors on-average deteriorate from agility in their 65–80s to a period of frailty preceding death. This deterioration is gradual for some and precipitous for others. Frailty is marked by an array of chronic physical and mental problems which means that frailty is not treatable as a specific disease. These problems coupled with increased dependency in the basic activities of daily living (ADLs) required for personal care add emotional problems: depression and anxiety. In sum, frailty has been depicted as a group of \"complex issues,\" distinct but \"causally interconnected,\" that often include \"comorbid diseases\", progressive weakness, stress, exhaustion, and depression.\n\nJohnson and Barer did a pioneering study of \"Life Beyond 85 Years\" by interviews over a six-year period. In talking with 85-year-olds and older, they found some popular conceptions about old age to be erroneous. Such erroneous conceptions include (1) people in old age have at least one family member for support, (2) old age well-being requires social activity, and (3) \"successful adaptation\" to age-related changes demands a continuity of self-concept. In their interviews, Johnson and Barer found that 24% of the 85+ had no face-to-face family relationships; many have outlived their families. Second, that contrary to popular notions, the interviews revealed that the reduced activity and socializing of the over-85s does not harm their well-being; they \"welcome increased detachment\". Third, rather than a continuity of self-concept, as the interviewees faced new situations they changed their \"cognitive and emotional processes\" and reconstituted their \"self–representation\".\n\nFrail people require a high level of care. Medical advances have made it possible to \"postpone death\" for years. This added time costs many frail people \"prolonged sickness, dependence, pain, and suffering\".\n\nAccording to a study by the Agency for Healthcare Research and Quality (AHRQ), the rate of emergency department visits was consistently highest among patients ages 85 years and older in 2006–2011 in the United States. Additionally, patients aged 65+ had the highest percentage of hospital stays for adults with multiple chronic conditions but the second highest percentage of hospital costs in 2003–2014.\n\nThese final years are also costly in economic terms. One out of every four Medicare dollars is spent on the frail in their last year of life . . . in attempts to postpone death.\n\nMedical treatments in the final days are not only economically costly, they are often unnecessary, even harmful. Nortin Hadler, M.D. warns against the tendency to medicalize and overtreat the frail. In her \"Choosing Medical Care in Old Age\", Michael R. Gillick M.D. argues that appropriate medical treatment for the frail is not the same as for the robust. The frail are vulnerable to \"being tipped over\" by any physical stress put on the system such as medical interventions.\n\nOld age, death, and frailty are linked because approximately half the deaths in old age are preceded by months or years of frailty.\n\n\"Older Adults' Views on Death\" is based on interviews with 109 people in the 70–90 age range, with a mean age of 80.7. \nAlmost 20% of the people wanted to use whatever treatment that might postpone death. About the same number said that, given a terminal illness, they would choose assisted suicide. Roughly half chose doing nothing except live day by day until death comes naturally without medical or other intervention designed to prolong life. This choice was coupled with a desire to receive palliative care if needed.\n\nAbout half of older adults suffer multimorbidity, that is, they have three or more chronic conditions. Medical advances have made it possible to \"postpone death,\" but in many cases this postponement adds \"prolonged sickness, dependence, pain, and suffering,\" a time that is costly in social, psychological, and economic terms.\n\nThe longitudinal interviews of 150 age 85+ people summarized in \"Life Beyond 85 Years\" found \"progressive terminal decline\" in the year prior to death: constant fatigue, much sleep, detachment from people, things, and activities, simplified lives. Most of the interviewees did not fear death; some would welcome it. One person said, \"Living this long is pure hell.\"\nHowever, nearly everyone feared a long process of dying. Some wanted to die in their sleep; others wanted to die \"on their feet\".\n\nThe study of \"Older Adults' Views on Death\" found that the more frail people were, the more \"pain, suffering, and struggles\" they were enduring, the more likely they were to \"accept and welcome\" death as a release from their misery. Their fear about the process of dying was that it would prolong their distress. Besides being a release from misery, some saw death as a way to reunion with departed loved ones. Others saw death as a way to free their caretakers from the burden of their care.\n\nGenerally speaking, old people have always been more religious than young people. At the same time, wide cultural variations exist.\nIn the United States, 90% of old age Hispanics view themselves as very, quite, or somewhat religious. The Pew Research Center's study of black and white old people found that 62% of those in ages 65–74 and 70% in ages 75+ asserted that religion was \"very important\" to them. For all 65+ people, more women (76%) than men (53%) and more blacks (87%) than whites (63%) consider religion \"very important\" to them. This compares to 54% in the 30–49 age range.\n\nIn a British 20-year longitudinal study, less than half of the old people surveyed said that religion was \"very important\" to them, and a quarter said they had become less religious in old age. The late-life rise in religiosity is stronger in Japan than in the United States, but in the Netherlands it is minimal.\n\nIn the practice of religion, a study of 60+ people found that 25% read the Bible every day and over 40% look at religious TV. Pew Research found that in the age 65+ range, 75% of whites and 87% of blacks pray daily.\n\nParticipation in organized religion is not a good indicator of religiosity because transportation and health problems often hinder participation.\n\nIn the industrialized countries, life expectancy and, thus, the old age population have increased consistently over the last decades. In the United States the proportion of people aged 65 or older increased from 4% in 1900 to about 12% in 2000. In 1900, only about of the nation's citizens were 65 or older (out of 76 million total American citizens). By 2000, the number of senior citizens had increased to about 35 million (of 280 million US citizens). Population experts estimate that more than Americans—about 17 percent of the population—will be 65 or older in 2020. By 2050, it is projected that at least 400,000 Americans will be 100 or older.\n\nThe number of old people is growing around the world chiefly because of the post–World War II baby boom and increases in the provision and standards of health care. By 2050, 33% of the developed world's population and almost 20% of the less developed world's population will be over 60 years old.\n\nThe growing number of people living to their 80s and 90s in the developed world has strained public welfare systems and has also resulted in increased incidence of diseases like cancer and dementia that were rarely seen in premodern times. When the United States Social Security program was created, persons older than 65 numbered only around 5% of the population and the average life expectancy of a 65-year-old in 1936 was approximately 5 years, while in 2011 it could often range from 10 to 20 years. Other issues that can arise from an increasing population are growing demands for health care and an increase in demand for different types of services.\n\nOf the roughly 150,000 people who die each day across the globe, about two thirds—100,000 per day—die of age-related causes.<ref name=\"doi10.2202/1941-6008.1011\"></ref> In industrialized nations, the proportion is much higher, reaching 90%.\n\nAccording to Erik Erikson's \"Stages of Psychosocial Development\", the human personality is developed in a series of eight stages that take place from the time of birth and continue on throughout an individual's complete life. He characterises old age as a period of \"Integrity vs. Despair\", during which a person focuses on reflecting back on his life. Those who are unsuccessful during this phase will feel that their life has been wasted and will experience many regrets. The individual will be left with feelings of bitterness and despair. Those who feel proud of their accomplishments will feel a sense of integrity. Successfully completing this phase means looking back with few regrets and a general feeling of satisfaction. These individuals will attain wisdom, even when confronting death. Coping is a very important skill needed in the aging process to move forward with life and not be 'stuck' in the past. The way a person adapts and copes, reflects his aging process on a psycho-social level.\n\nFor people in their 80s and 90s, Joan Erikson added a ninth stage in \"The Life Cycle Completed: Extended Version\". As she wrote, she added the ninth stage because the Integrity of the eighth stage imposes \"a serious demand on the senses of elders\" and the Wisdom of the eighth stage requires capacities that ninth stage elders \"do not usually have\".\n\nNewman & Newman also proposed a ninth stage of life, Elderhood. Elderhood refers to those individuals who live past the life expectancy of their birth cohorts. There are two different types of people described in this stage of life. The \"young old\" are the healthy individuals who can function on their own without assistance and can complete their daily tasks independently. The \"old old\" are those who depend on specific services due to declining health or diseases. This period of life is characterized as a period of \"immortality vs. extinction\". Immortality is the belief that your life will go on past death, some examples are an afterlife or living on through one's family. Extinction refers to feeling as if life has no purpose.\n\nSocial theories, or concepts, propose explanations for the distinctive relationships between old people and their societies.\n\nOne of the theories is the disengagement theory proposed in 1961. This theory proposes that in old age a mutual disengagement between people and their society occurs in anticipation of death. By becoming disengaged from work and family responsibilities, according to this concept, people are enabled to enjoy their old age without stress.\nThis theory has been subjected to the criticism that old age disengagement is neither natural, inevitable, nor beneficial. Furthermore, disengaging from social ties in old age is not across the board: unsatisfactory ties are dropped and satisfying ones kept.\n\nIn opposition to the disengagement theory, the activity theory of old age argues that disengagement in old age occurs not by desire, but by the barriers to social engagement imposed by society. This theory has been faulted for not factoring in psychological changes that occur in old age as shown by reduced activity, even when available. It has also been found that happiness in old age is not proportional to activity.\n\nAccording to the continuity theory, in spite of the inevitable differences imposed by their old age, most people try to maintain continuity in personhood, activities, and relationships with their younger days.\n\nSocioemotional selectivity theory also depicts how people maintain continuity in old age. The focus of this theory is continuity sustained by social networks, albeit networks narrowed by choice and by circumstances. The choice is for more harmonious relationships. The circumstances are loss of relationships by death and distance.\n\nLife expectancy by nation at birth in the year 2011 ranged from 48 years to 82. Low values indicate high death rates for infants and children.\n\nIn most parts of the world women live, on average, longer than men; even so, the disparities vary between 12 years in Russia to no difference or higher life expectancy for men in countries such as Zimbabwe and Uganda.\n\nThe number of elderly persons worldwide began to surge in the second half of the 20th century. Up to that time (and still true in underdeveloped countries), five or less percent of the population was over 65. Few lived longer than their 70s and people who attained advanced age (i.e. their 80s) were rare enough to be a novelty and were revered as wise sages. The worldwide over-65 population in 1960 was one-third of the under 5 population. By 2013, the over-65 population had grown to equal the under 5 population. The over-65 population is projected to double the under five by 2050.\n\nBefore the surge in the over-65 population, accidents and disease claimed many people before they could attain old age, and health problems in those over 65 meant a quick death in most cases. If a person lived to an advanced age, it was due to genetic factors and/or a relatively easy lifestyle, since diseases of old age could not be treated before the 20th century.\n\nIn October 2016, scientists identified the maximum human lifespan at an average age of 115, with an absolute upper limit of 125 years. However, the concept of a maximum lifespan in humans is still widely debated among the scientific community.\n\nGerman chancellor Otto von Bismarck created the world's first comprehensive government social safety net in the 1880s, providing for old age pensions.\n\nIn the United States of America, and the United Kingdom, 65 (UK 60 for women) was traditionally the age of retirement with full old age benefits.\n\nIn 2003, the age at which a United States citizen became eligible for full Social Security benefits began to increase gradually, and will continue to do so until it reaches 67 in 2027. Full retirement age for Social Security benefits for people retiring in 2012 is age 66. In the United Kingdom, the state pension age for men and women will rise to 66 in 2020 with further increases scheduled after that.\"\n\nOriginally, the purpose of old age pensions was to prevent elderly persons from being reduced to beggary, which is still common in some underdeveloped countries, but growing life expectancies and older populations have brought into question the model under which pension systems were designed. By 1990, the United States was spending 30 per cent of its budget on the elderly, compared with 2 per cent on education. The dominant perception of the American old age population changed from \"needy\" and \"worthy\" to \"powerful\" and \"greedy,\" old people getting more than their share of the nation's resources. However, in 2011, using a Supplemental Poverty Measure (SPM), the old age American poverty rate was measured as 15.9%.\n\nIn the United States in 2008, 11 million people aged 65+ lived alone: 5 million or 22% of ages 65–74, 4 million or 34% of ages 75–84, and 2 million or 41% of ages 85+. The 2007 gender breakdown for all people 65+ was men 19% and women 39%.\n\nMany new assistive devices made especially for the home have enabled more old people to care for themselves activities of daily living (ADL). Able Data lists 40,000 assistive technology products in 20 categories. Some examples of devices are a medical alert and safety system, shower seat (making it so the person does not get tired in the shower and fall), a bed cane (offering support to those with unsteadiness getting in and out of bed) and an ADL cuff (used with eating utensils for people with paralysis or hand weakness).\n\nA Swedish study found that at age 76, 46% of the subjects used assistive devices. When they reached age 86, 69% used them. The subjects were ambivalent regarding the use of the assistive devices: as \"enablers\" or as \"disablers\". People who view assistive devices as enabling greater independence accept and use them. Those who see them as symbols of disability reject them. However, organizations like Love for the Elderly aim to combat such age-related prejudice by educating the public about the importance of appreciating growing older, while also providing services of kindness to elders in senior homes.\n\nEven with assistive devices as of 2006, 8.5 million Americans needed personal assistance because of impaired basic activities of daily living required for personal care or impaired instrumental activities of daily living (IADL) required for independent living. Projections place this number at 21 million by 2030 when 40% of Americans over 70 will need assistance. There are many options for such long term care to those who require it. There is home care in which a family member, volunteer, or trained professional will aid the person in need and help with daily activities. Another option is community services which can provide the person with transportation, meal plans, or activities in senior centers. A third option is assisted living where 24-hour round-the-clock supervision is given with aid in eating, bathing, dressing, etc. A final option is a nursing home which provides professional nursing care.\n\nA scholarly literature has emerged, especially in Britain, showing historical trends in the visual depiction of old age.\n\n\n"}
{"id": "30867658", "url": "https://en.wikipedia.org/wiki?curid=30867658", "title": "Orbital perturbation analysis", "text": "Orbital perturbation analysis\n\nOrbital perturbation analysis is the activity of determining why a satellite's orbit differs from the mathematical ideal orbit. A satellite's orbit in an ideal two-body system describes a conic section, usually an ellipse. In reality, there are several factors that cause the conic section to continually change. These deviations from the ideal Kepler's orbit are called perturbations.\n\nIt has long been recognized that the Moon does not follow a perfect orbit, and many theories and models have been examined over the millennia to explain it. Isaac Newton determined the primary contributing factor to orbital perturbation of the moon was that the shape of the Earth is actually an oblate spheroid due to its spin, and he used the perturbations of the lunar orbit to estimate the oblateness of the Earth.\n\nIn Newton's Philosophiæ Naturalis Principia Mathematica, he demonstrated that the gravitational force between two mass points is inversely proportional to the square of the distance between the points, and he fully solved the corresponding \"two-body problem\" demonstrating that the radius vector between the two points would describe an ellipse. But no exact closed analytical form could be found for the three-body problem. Instead, mathematical models called \"orbital perturbation analysis\" have been developed. With these techniques a quite accurate mathematical description of the trajectories of all the planets could be obtained. Newton recognized that the Moon's perturbations could not entirely be accounted for using just the solution to the three-body problem, as the deviations from a pure Kepler orbit around the Earth are much larger than deviations of the orbits of the planets from their own Sun-centered Kepler orbits, caused by the gravitational attraction between the planets. With the availability of digital computers and the ease with which we can now compute orbits, this problem has partly disappeared, as the motion of all celestial bodies including planets, satellites, asteroids and comets can be modeled and predicted with almost perfect accuracy using the method of the numerical propagation of the trajectories. Nevertheless, several analytical closed form expressions for the effect of such additional \"perturbing forces\" are still very useful.\n\nThe precise modeling of the motion of the Moon has been a difficult task. The best and most accurate modeling for the lunar orbit before the availability of digital computers was obtained with the complicated Delaunay and Brown's lunar theories.\n\nAll celestial bodies of the Solar System follow in first approximation a Kepler orbit around a central body. For a satellite (artificial or natural) this central body is a planet. But both due to gravitational forces caused by the Sun and other celestial bodies and due to the flattening of its planet (caused by its rotation which makes the planet slightly oblate and therefore the result of the Shell theorem not fully applicable) the satellite will follow an orbit around the Earth that deviates more than the Kepler orbits observed for the planets.\n\nFor man-made spacecraft orbiting the Earth at comparatively low altitudes the deviations from a Kepler orbit are much larger than for the Moon. The approximation of the gravitational force of the Earth to be that of a homogeneous sphere gets worse the closer one gets to the Earth surface and the majority of the artificial Earth satellites are in orbits that are only a few hundred kilometers over the Earth surface. Furthermore, they are (as opposed to the Moon) significantly affected by the solar radiation pressure because of their large cross-section-to-mass ratio; this applies in particular to 3-axis stabilized spacecraft with large solar arrays and is allowed for in calculation of graveyard orbits. In addition they are significantly affected by rarefied air below 800–1000 km. The air drag at high altitudes is also dependent on solar activity.\n\nConsider any function \n\nof the position\n\nand the velocity\n\nFrom the chain rule of differentiation one gets that the time derivative of formula_4 is\n\nwhere formula_6 are the components of the force per unit mass acting on the body.\n\nIf now formula_4 is a \"constant of motion\" for a Kepler orbit like for example an orbital element and the force is corresponding \"Kepler force\"\none has that formula_9.\n\nIf the force is the sum of the \"Kepler force\" and an additional force (force per unit mass)\n\ni.e.\n\none therefore has\n\nand that the change of formula_13 in the time from formula_14 to formula_15 is\n\nIf now the additional force formula_17 is sufficiently small that the motion will be close to that of a Kepler orbit one gets an approximate value for formula_18 by evaluating this integral assuming \nformula_19 to precisely follow this Kepler orbit.\n\nIn general one wants to find an approximate expression for the change formula_18 over one orbital revolution using the true anomaly formula_21 as integration variable, i.e. as\n</math>|}}\n\nThis integral is evaluated setting formula_22, the elliptical Kepler orbit in polar angles.\nFor the transformation of integration variable from time to true anomaly it was used that the angular momentum formula_23 by definition of the parameter formula_24 for a Kepler orbit (see equation (13) of the Kepler orbit article).\n\nFor the special case where the Kepler orbit is circular or almost circular\n\nwhere formula_26 is the orbital period\n\nFor an elliptic Kepler orbit, the sum of the kinetic and the potential energy\n\nwhere formula_28 is the orbital velocity, is a constant and equal to\n\nIf formula_30 is the perturbing force and formula_31is the velocity vector of the Kepler orbit the equation () takes the form:\n\n</math>|}}\n\nand for a circular or almost circular orbit\n\nFrom the change formula_18 of the parameter formula_13 the new semi-major axis formula_34 and the new period formula_35 are computed (relations (43) and (44) of the Kepler orbit article).\n\nLet formula_36 and formula_37 make up a rectangular coordinate system in the plane of the reference Kepler orbit. If formula_38 is the argument of perigee relative the formula_36 and formula_37 coordinate system the true anomaly formula_21 is given by formula_42 and the approximate change formula_43 of the orbital pole formula_44 (defined as the unit vector in the direction of the angular momentum) is\n=\\ \\frac{1}{\\mu p}\\left[\\hat{g}\\int\\limits_{0}^{2\\pi}f_z r^3 \\cos u \\ du\n\nwhere formula_45 is the component of the perturbing force in the formula_44 direction, formula_47 is the velocity component of the Kepler orbit orthogonal to radius vector and formula_48 is the distance to the center of the Earth.\nFor a circular or almost circular orbit () simplifies to\n\nExample\n\nIn a circular orbit a low-force propulsion system (Ion thruster) generates a thrust (force per unit mass) of formula_49 in the direction of the orbital pole in the half of the orbit for which formula_50 is positive and in the opposite direction in the other half. The resulting change of orbit pole after one orbital revolution of duration formula_26 is\n\nThe average change rate formula_52 is therefore\n\n</math>|}}\nwhere formula_53 is the orbital velocity in the circular Kepler orbit.\n\nRather than applying (1) and (2) on the partial derivatives of the orbital elements eccentricity and argument of perigee directly one should apply these relations for the eccentricity vector. First of all the typical application is a near-circular orbit. But there are also mathematical advantages working with the partial derivatives of the components of this vector also for orbits with a significant eccentricity.\n\nEquations (60), (55) and (52) of the Kepler orbit article say that the eccentricity vector is\n\nwhere\n\n\nwhere\n\n\nThe eccentricity vector is by definition always in the osculating orbital plane spanned by formula_54 and formula_55 and formally there is also a derivative \n\nwith\n\ncorresponding to the rotation of the orbital plane\n\nBut in practice the in-plane change of the eccentricity vector is computed as\n\n&\\frac {1}{\\mu}\\ \\int\\limits_{0}^{2\\pi}\\left(-\\hat{t}\\ f_r\\ + \\ \\left(2\\ \\hat{r}-\\frac{V_r}{V_t}\\ \\hat{t}\\right)\\ f_t\\right) r^2 du \\end{align}\n\nignoring the out-of-plane force and the new eccentricity vector \nis subsequently projected to the new orbital plane orthogonal to the new orbit normal\ncomputed as described above.\n\nExample\n\nThe Sun is in the orbital plane of a spacecraft in a circular orbit with radius formula_60 and consequently with a constant orbital velocity formula_61 . If formula_62 and formula_63 make up a rectangular coordinate system in the orbital plane such that formula_62 points to the Sun and assuming that the solar radiation pressure force per unit mass formula_65 is constant one gets that \n\nwhere formula_70 is the polar angle of formula_71 in the formula_62, formula_63 system. Applying () one gets that\n\nThis means the eccentricity vector will gradually increase in the direction formula_63 orthogonal to the Sun direction. This is true for any orbit with a small eccentricity, the direction of the small eccentricity vector does not matter. As formula_75 is the orbital period this means that the average rate of this increase will be \nformula_76\n\nIn the article Geopotential model the modeling of the gravitational field as a sum of spherical harmonics is discussed. By far, the dominating term is the \"J2-term\". This is a \"zonal term\" and corresponding force is therefore completely in a longitudinal plane with one component formula_77 in the radial direction and one component formula_78 with the unit vector formula_79 orthogonal to the radial direction towards north. These directions formula_71 and formula_79 are illustrated in Figure 1.\n\nTo be able to apply relations derived in the previous section the force component formula_78 must be split into two orthogonal components formula_83 and formula_84 as illustrated in figure 2\n\nLet formula_85 make up a rectangular coordinate system with origin in the center of the Earth (in the center of the Reference ellipsoid) such that formula_86 points in the direction north and such that formula_87 are in the equatorial plane of the Earth with formula_88 pointing towards the ascending node, i.e. towards the blue point of Figure 2.\n\nThe components of the unit vectors\n\nmaking up the local coordinate system (of which formula_90 are illustrated in figure 2) relative the formula_85 are\n\nwhere formula_70 is the polar argument of formula_71 relative the orthogonal unit vectors formula_103 and formula_104 in the orbital plane\n\nFirstly\n\nwhere formula_106 is the angle between the equator plane and formula_71 (between the green points of figure 2) and from equation (12) of the article Geopotential model one therefore gets that\n\nSecondly the projection of direction north, formula_86, on the plane spanned by formula_90 is\n\nand this projection is \n\nwhere formula_79 is the unit vector formula_113 orthogonal to the radial direction towards north illustrated in figure 1.\n\nFrom equation (12) of the article Geopotential model one therefore gets that\n\nand therefore:\n\nFrom () and () one gets that\n\nThe fraction formula_115 is\nwhere formula_117 is the eccentricity\nand formula_38 is the argument of perigee \nof the reference Kepler orbit\n\nAs all integrals of type\nare zero if not both formula_120 and formula_121 are even one gets from () that\n\nAs\n\nthis can be written\n\nAs formula_124 is an inertially fixed vector (the direction of the spin axis of the Earth) relation () is the equation of motion for a unit vector formula_125 describing a cone around formula_124 with a precession rate (radians per orbit) of formula_127\n\nIn terms of orbital elements this is expressed as\n\nwhere\n\nFrom (), () and () follows that in-plane perturbation of the eccentricity vector is\n\nthe new eccentricity vector being the projection of \n\non the new orbital plane orthogonal to\nwhere formula_132 is given by ()\n\nRelative the coordinate system\n\none has that\n\nUsing that\n\nand that\n\nwhere\n\nare the components of the eccentricity vector in the formula_141 coordinate system this integral () can be evaluated analytically, the result is\n\nThis the difference equation of motion for the eccentricity vector formula_142 to form a circle, the magnitude of the eccentricity formula_117 staying constant.\n\nTranslating this to orbital elements it must be remembered that the new eccentricity vector obtained by adding formula_144 to the old formula_145 must be projected to the new orbital plane obtained by applying () and ()\nThis is illustrated in figure 3:\n\nTo the change in argument of the eccentricity vector\n\nmust be added an increment due to the precession of the orbital plane (caused by the out-of-plane force component) amounting to\n\nOne therefore gets that\n\nIn terms of the components of the eccentricity vector formula_148 relative the coordinate system formula_149 that precesses around the polar axis of the Earth the same is expressed as follows\n\nwhere the first term is the in-plane perturbation of the eccentricity vector and the second is the effect of the new position of the ascending node in the new plane\n\nFrom () follows that formula_150 is zero if formula_151. This fact is used for Molniya orbits having an inclination of 63.4 deg. An orbit with an inclination of 180 - 63.4 deg = 116.6 deg would in the same way have a constant argument of perigee.\n\nProof that the integral\n\nwhere:\n\nhas the value \n\nIntegrating the first term of the integrand one gets:\n\nand\n\nFor the second term one gets:\n\nand\n\nFor the third term one gets:\n\nand\n\nFor the fourth term one gets:\n\nand\n\nAdding the right hand sides of (), (), () and () one gets\nformula_156\n\nAdding the right hand sides of (), (), () and () one gets\nformula_157\n\n\n"}
{"id": "74678", "url": "https://en.wikipedia.org/wiki?curid=74678", "title": "Playoffs", "text": "Playoffs\n\nThe playoffs, play-offs, postseason and/or finals of a sports league are a competition played after the regular season by the top competitors to determine the league champion or a similar accolade. Depending on the league, the playoffs may be either a single game, a series of games, or a tournament, and may use a single-elimination system or one of several other different playoff formats. Playoff, in regard to international fixtures, is to qualify or progress to the next round of a competition or tournament.\n\nIn team sports in the U.S. and Canada, the vast distances and consequent burdens on cross-country travel have led to regional divisions of teams. Generally, during the regular season, teams play more games in their division than outside it, but the league's best teams might not play against each other in the regular season. Therefore, in the postseason a playoff series is organized. Any group-winning team is eligible to participate, and as playoffs became more popular they were expanded to include second- or even lower-placed teams – the term \"wild card\" refers to these teams.\n\nIn England and Scotland, playoffs are used in association football to decide promotion for lower-finishing teams, rather than to decide a champion in the way they are used in North America. In the EFL Championship (the second tier of English football), teams finishing 3rd to 6th after the regular season compete to decide the final promotion spot to the Premier League.\n\nEvidence of playoffs in professional football dates to at least 1919, when the \"New York Pro Championship\" was held in Western New York (it is possible one was held in 1917, but that is not known for sure). The Buffalo and Rochester metropolitan areas each played a championship game, the winners of which would advance to the \"New York Pro Championship\" on Thanksgiving weekend. The top New York teams were eventually absorbed into the NFL upon its founding in 1920, but the league (mostly driven by an Ohio League that did not have true championship games, though they frequently scheduled \"de facto\" championship matchups) did not adopt the New York league's playoff format, opting for a championship based on regular season record for its first twelve seasons; as a result, four of the first six \"championships\" were disputed. Technically, a vote of league owners was all that was required to win a title, but the owners had a gentlemen's agreement to pledge votes based on a score (wins divided by the sum of wins and losses, with a few tiebreakers). When two teams tied at the top of the standings in 1932, an impromptu playoff game was scheduled to settle the tie.\n\nThe National Football League divided its teams into divisions in 1933 and began holding a single playoff championship game between division winners. In 1950 the NFL absorbed three teams from the rival All-America Football Conference, and the former \"Divisions\" were now called \"Conferences\", echoing the college use of that term. In 1967, the NFL expanded and created four divisions under the two conferences, which led to the institution of a larger playoff tournament. After the AFL-NFL merger brought the American Football League into the NFL, the NFL began to use three divisions and a single wild card team in each conference for its playoffs, in order to produce eight contenders out of six divisions; this was later expanded in 1978 & 1990 so that more wild card teams could participate.\n\nIn 2002 the NFL added its 32nd team, the Houston Texans, and significantly reshuffled its divisional alignment. The league went from 6 division winners and 6 wild card spots to 8 division winners and only 4 wild card qualifiers. The winners of each division automatically earn a playoff spot and a home game in their first rounds, and the two top non-division winners from each conference will also make the playoffs as wild-card teams. The top two teams with the best records in the regular season get a first round bye, and each of the bottom two division winners plays one of the two wild-card teams. Each winner of a wild-card game then plays one of the two bye teams. The winners of these two games go to the conference championships, and the winners of those conference championship games then face each other in the Super Bowl.\n\nThe College Football Playoff National Championship is a post-season college football bowl game, used to determine a national champion of the NCAA Division I Football Bowl Subdivision (FBS), which began play in the 2014 college football season. The game serves as the final of the College Football Playoff, a bracket tournament between the top four teams in the country as determined by a selection committee, which was established as a successor to the Bowl Championship Series and its similar BCS National Championship Game. Unlike the BCS championship, the participating teams in the College Football Playoff National Championship are determined by two semi-final bowls—hosted by two of the consortium's six member bowls yearly—and the top two teams as determined by the selection committee do not automatically advance to the game in lieu of other bowls.\n\nThe game is played at a neutral site, determined through bids by prospective host cities (similarly to the Super Bowl and NCAA Final Four). When announcing it was soliciting bids for the 2016 and 2017 title games, playoff organizers noted that the bids must propose host stadiums with a capacity of at least 65,000 spectators, and cities cannot host both a semi-final game and the title game in the same year.\n\nThe winner of the game is awarded a new championship trophy instead of the \"crystal football\", which has been given by the American Football Coaches Association (AFCA) since 1986; officials wanted a new trophy that was unconnected with the previous BCS championship system. The new College Football Playoff National Championship Trophy is sponsored by Dr Pepper, which paid an estimated $35 million for the sponsorship rights through 2020. The 26.5-inch high, 35-pound trophy was unveiled on July 14, 2014.\n\nThe NCAA Division I Football Championship is an American college football tournament played each year to determine the champion of the NCAA Division I Football Championship Subdivision (FCS). Prior to 2006, the game was known as the NCAA Division I-AA Football Championship. The FCS is the highest division in college football to hold a playoff tournament sanctioned by the NCAA to determine its champion. The four-team playoff system used by the Bowl Subdivision is not sanctioned by the NCAA.\n\nThe NCAA Division II Football Championship is an American college football tournament played annually to determine a champion at the NCAA Division II level. It was first held in 1973. Prior to 1973, four regional bowl games were played in order to provide postseason action for what was then called the \"NCAA College Division\" and a poll determined the final champion.\n\nThe National Championship game was held at Sacramento, California from 1973 to 1975. It was in Wichita Falls, Texas in 1976 and 1977. The game was played in Longview, Texas in 1978. For 1979 and 1980, Albuquerque, New Mexico hosted the game. McAllen, Texas hosted the championship games from 1981 to 1985. From 1986 to 2013, the Division II championship game was played at Braly Municipal Stadium near the campus of the University of North Alabama in Florence, Alabama. Between 2014 and 2017, the championship game was played at Children's Mercy Park in Kansas City, Kansas. Since 1994, the games have been broadcast on ESPN.\n\nThe NCAA Division III Football Championship began in 1973. Before 1973, most of the schools now in Division III competed either in the NCAA College Division or the National Association of Intercollegiate Athletics (NAIA). NCAA Divisions II and III were created by splitting the College Division in two, with schools that wished to continue awarding athletic scholarships placed in Division II and those that did not want to award them placed in Division III.\n\nThe Division III playoffs begin with 32 teams selected to participate in the playoffs. The Division III championship game, known as the Stagg Bowl, has been played annually in Salem, Virginia at Salem Football Stadium since 1993. It was previously played in Phenix City, Alabama at Garrett-Harrison Stadium (1973–1982, 1985–1989), at the College Football Hall of Fame, when the Hall was located in Kings Island, Ohio at Galbreath Field (1983–1984), and Bradenton, Florida at Hawkins Stadium (1990–1992).\n\nAs a rule, international association football has only had championship playoffs when a league is divided into several equal divisions/conferences/groups (Major League Soccer) and/or when the season is split into two periods (as in many leagues in Latin America, such as Mexico's Liga MX). In leagues with a single table done only once a year, as in most of Europe, playoff systems are not used to determine champions, although in some countries such systems are used to determine teams to be promoted to higher leagues (e.g., England) or qualifiers for European club competitions (such as Greece and the Netherlands), usually between teams that didn't perform well enough to earn an automatic spot.\n\nA \"test match\" is a match played at the end of a season between a team that has done badly in a higher league and one that has done well in a lower league of the same football league system. The format of a test match series varies; for instance it can be a head-to-head between one of the worse finishers of the higher league and one of the better finishers of the lower league, or it can be a mini league where all participants play each other or teams only play those from the other league. The winner of the test match series play in the higher league the following season, and the loser in the lower league.\n\nIn international football, playoffs were a feature of the 1954 and 1958 FIFA World Cup final tournaments. They are still a feature of the qualification tournaments for the FIFA World Cup and the UEFA European Football Championship.\n\nIn the qualification playoffs for the 2006 FIFA World Cup, for example:\n\nIn addition to their league competitions, most European footballing nations also have knockout cup competitions – English football, for example, has the FA Cup and the League Cup. These competitions are open to many teams — 92 clubs compete for the League Cup, and hundreds compete for the FA Cup. These competitions run concurrently with the \"regular season\" league competitions and are not regarded as playoffs.\n\nIn Argentine football, playoffs in the style of the English leagues occur in the Primera B Metropolitana, part of the third tier, and leagues below it (Primera C Metropolitana and Primera D Metropolitana). All \"Primera Metropolitana\" tourneys cover the area in and around Buenos Aires, the capital city. The \"Torneo Reducidos\" (reduced tournaments), however, involve 8 teams below the top two, as opposed to 4.\n\nBefore the top-flight Argentine Primera División abandoned its traditional Apertura and Clausura format in 2015 in favor of an expanded single season, there was no playoff between the Apertura and Clausura winners. As a result, the league crowned two champions each year. After each Clausura, the two teams with the lowest points-per-game total for the previous six tournaments (three years, counting only Primera División games) were relegated to Primera B Nacional to be replaced by that league's champion and runner-up teams; the two teams immediately above contested promotion/relegation series with the third and fourth places in Primera B Nacional, counted by its aggregate table. In Primera B Nacional, the same procedure continues in use for relegation to either Primera B Metropolitana or Torneo Argentino A for non-Buenos Aires clubs. From 2015 onward, relegation from the Primera División will be based solely on league position at the end of the season (which, effective in 2016–17, changed from a February–December format to an August–June format).\n\nThe Australian A-League, which also features a team in New Zealand, has determined its champions via a playoff system, officially known as the \"Finals Series\" (reflecting standard Australian English usage), since its inception in the 2005–06 season.\n\nFrom the league's inception through the 2008–09 season, the top four teams advanced to the finals series, employed using a modified Page playoff system. The top two teams at the end of league play were matched in one semifinal, with the winner advancing directly to the Grand Final and the loser going into the Preliminary Final. The next two teams played a semifinal for a place in the Preliminary Final, whose winner took the other place in the Grand Final. Both semifinals were two-legged, while the Preliminary Final and Grand Final were one-off matches.\n\nWhen the league expanded to 10 teams beginning in 2009–10, the finals expanded to six teams. The format of the six-team playoff established at that time was:\n\nStarting with the 2012–13 season, the finals format has been changed to a pure knockout tournament consisting entirely of one-off matches:\n\nIt should be noted that the concept of a finals series/playoff is standard in Australian sport.\n\nThe Belgian First Division A (previously known as the \"First Division\" and \"Pro League\") has a fairly complex playoff system, currently consisting of two levels and at one time three.\n\nSince the 2009–10 season, playoffs have been held to determine the champion and tickets for the Champions League and Europa League. The six highest ranked teams play home-and-away matches against each other; a total of 10 matches each. The 6 participating teams start with the points accumulated during the regular competition divided by two. The first 3 teams after the play-offs get a European ticket. The fourth ranked team (or fifth, when the cup holder is already qualified for European football) plays a knock-out match against the winner of play-off 2. From 2009–10 through 2015–16, teams ranked 7–14 played in two groups; from 2016–17 forward, this playoff will continue to be contested in two groups, but with a total of 12 teams (details below). All points gained from the regular competition are lost. The two group winners play a final match to determine the winner of play-off 2. The winning team plays a final match against the fourth-ranked team (or fifth) for the last European ticket.\n\nThe play-off system has been criticized because more points per match can be earned in the play-off stage than in the regular competition. This way the team who wins the most matches isn't automatically the national champion. The biggest upside in favor of the play-off system is the higher number of matches (40 instead of 34 compared to the previous season) and more top matches. The extra matches also generate higher revenues for the teams.\n\nNonetheless, the higher number of matches takes an extra toll on teams and players. Besides play-offs, the Royal Belgian Football Association (KBVB) also introduced Christmas football in order to complete the extra matches in time. This posed some problems because a few matches had to be cancelled due to snowy pitches. The delays will probably cause the tight schedule to fail and postpone the end of the season.\n\nSome structural changes were instituted in 2015–16:\n\nFrom 1974 through 2015, the 15th team out of 16 in the final standings was involved in a playoff pool with three teams from the Belgian Second Division after each season, to determine which of these teams played in the First Division/Pro League the oncoming season. The lowest ranked team of the First Division/Pro League was relegated and replaced by the Second Division champion.\n\nOriginally, these playoffs were introduced in 1974 and were part of the Second Division, to determine which team was promoted to the highest level together with the division champions. From the 2005–06 season on, only one team was relegated directly from the First Division, with the 17th team taking part in the playoff. As a result, this playoff was still called the Belgian Second Division Final Round, although one team from the Pro League took part each year.\n\nStarting in 2015–16, this playoff was scrapped and replaced with direct relegation for the bottom Pro League/First Division A team only.\n\nFurther changes will be introduced to the Europa League playoffs from 2016–17 forward. The playoff will involve a total of 12 teams—nine from First Division A, and three from First Division B (the renamed Second Division). The First Division A qualifiers will be those finishing between 7th and 15th on the regular-season table. The First Division B qualifiers will be the top three teams from that league's regular-season table, excluding the division champion, which instead earns promotion to First Division A. As in the previous format, the teams will be divided into two groups, each playing home-and-away within the group, and the two group winners will play a one-off final, with the winner of that match advancing to a one-off match against the fourth- or fifth-place team from the championship playoff (depending on available European slots) for the final Europa League place.\n\nIn Brazil, the Copa do Brasil, the second most prestigious country-wide competition, is contested in pure \"knockout\" format since its inception in 1989. While the top two tiers in the Brazilian League – Série A and Série B – are contested in double round robin format, the lower tiers Série C and Série D include knockout rounds in their final stages.\n\nBulgaria instituted an elaborate playoff system in its top flight, the First League, in the 2016–17 season.\n\nAfter the league's 14 teams play a full home-and-away season, the league splits into two playoffs—a 6-team \"championship playoff\" and an 8-team \"qualifying playoff\", with the latter split into two 4-team groups. Each playoff begins with teams carrying over all goals and statistics from the home-and-away season.\n\nEach team in the championship playoff plays the others home and away one additional time. At the end of this stage:\n\nEach group within the qualifying playoff also plays home and away within its group; after this stage, teams enter another level of playoffs depending on their positions in the group.\n\nThe top two teams in each group enter a knockout playoff consisting entirely of two-legged matches (unless one of these teams is the winner of that season's Bulgarian Cup, in which case it will not enter the playoff and the team that it would have played receives a bye into the playoff final). The winner of this playoff then contests a one-off match against the third-place (or fourth-place) team from the championship playoff, with the winner claiming the final Europa League place.\n\nThe bottom two teams from each group begin a series of relegation playoffs. The series starts with a knockout playoff that also consists entirely of two-legged matches. The winner of the playoff remains in the First League for the following season. The losing teams then enter the following series of two-legged promotion/relegation matches:\n\nWith the creation of the Liga Dominicana de Fútbol in 2014 to replace the Primera División de Republica Dominicana, it introduced a playoff system to determine the champion of the season.\n\nWhen the Football League was first expanded to two divisions in 1892, test matches were employed to decide relegation and promotion between them, but the practice was scrapped in favour of automatic relegation and promotion in 1898.\n\nThe use of play-offs to decide promotion issues returned to the League in 1986 with the desire to reduce the number of mid-table clubs with nothing to play for at the end of the season. The Football Conference, now known as the National League, introduced play-offs in 2002 after the Football League agreed to a two-club exchange with the Conference.\n\nThe top two teams in the EFL Championship and in EFL League One are automatically promoted to the division above and thus do not compete in the play-offs. The top three teams in EFL League Two and the champion of the National League (formerly known as Conference Premier) are also automatically promoted. In each of these divisions the four clubs finishing below the automatic promotion places compete in two-legged semi-finals with the higher-placed club enjoying home advantage in the second leg. The away goals rule does not apply for the semi-finals. The Football League play-off finals were originally played in two legs, at both teams' home grounds, but were later changed to one-off affairs, which are played at Wembley Stadium in London.\n\nTeams are also promoted using a play-off tournament from levels six to eight of the football pyramid. At level six, the play-off semi finals are two leg ties with the final being a single match played at the home ground of the highest placed of the two teams. At levels seven and eight, all of the ties are single matches played at the home ground of the team with the highest league position.\n\nIn 2003, Gillingham proposed replacing the current play-off system with one involving six clubs from each division and replacing the two-legged ties with one-off matches. If adopted, the two higher-placed clubs in the play-offs would have enjoyed first-round byes and home advantage in the semi-finals. It was a controversial proposal — some people did not believe a club finishing only in eighth position in the League could (or should) compete in the Premiership while others found the system too American for their liking. Although League chairmen initially voted in favour of the proposal, it was blocked by The FA and soon abandoned.\n\nThe championship of every division in English football is determined solely by the standings in the league. However, a championship play-off would be held if the top two teams were tied for points, goal difference and goals scored in both their overall league record; to date, this has never happened. A play-off would also be scheduled if two teams are tied as above for a position affecting promotion, relegation, or European qualification.\n\nStarting in the 2007–08 season, Superleague Greece instituted a playoff system to determine all of its places in European competition for the following season, except for those of the league champion and the cup winner. Currently, the league is entitled to two Champions League places and three in the Europa League, with one of the Europa League places reserved for the cup winner. The playoff currently takes the form of a home-and-away mini-league involving the second- through fifth-place teams, under the following conditions:\n\nIn 2004-05, Italy's professional league introduced a promotion playoff to its second tier of football, Serie B. It operates almost identically to the system currently used in England. The top two clubs in Serie B earn automatic promotion to Serie A with the next four clubs entering a playoff to determine who wins the third promotion place, as long as fewer than 10 points separate the third and fourth-placed teams (which often occurs).\n\nLike the English playoffs, the Italian playoffs employ two-legged semi-finals, with the higher finisher in the league table earning home advantage in the second leg. If the teams are level on aggregate after full-time of the second leg, away goals are not used, but extra time is used. Unlike England, the Italian playoff final is two-legged, again with the higher finisher earning home advantage in the second leg. In both rounds, if the tie is level on aggregate after extra time in the second leg, the team that finished higher in the league standings wins.\n\nIn 2003–04, Italy's football league used a two-legged test match to determine one spot in the top level of its system, Serie A. Some leagues in continental Europe combine automatic promotion/relegation with test matches. For example, in the Netherlands, only one club is automatically relegated from its top level, the Eredivisie, each season, with the winner of the second-flight being promoted. The next two lower-placed teams enter a promotion/relegation mini-league with high-placed teams from the Dutch First Division\n\nJ.League in Japan used a test match series between the third-from-bottom team in J1 and third-place team in J2 (see J. League Promotion/Relegation Series) from 2004 to 2008. The Promotion/Relegation Series concept dates as far back as 1965 and the first season of the Japan Soccer League.\n\nThe Japan Football League, the current Japanese third division, uses the Promotion/Relegation Series only when the number of clubs in the league needs to be filled with clubs from the Japanese Regional Leagues.\n\nA new Promotion/Relegation Series will occur beginning with the 2012 season of J. League Division 2, conditional on the top two JFL teams fulfilling J. League club criteria. In turn, J2 will implement a playoff on the style of England for the 3rd to 6th clubs.\n\nMexico's top flight league, Liga MX, is contested annually by 18 teams. In each of two annual tournaments, every team plays every other team in the league once (17 games), after which the top eight teams advance to the \"Liguilla\".\n\nIn the Liguilla, all rounds are home-and-away. Teams are drawn so the best team plays the worst, the second-best plays the second-worst, and so on. After one round, the teams are redrawn so the best remaining team again plays the worst remaining one and the second-best faces the second-worst in the semi-finals. The two winners of this round play each other for the championship.\n\nThere is no playoff between the Apertura and Clausura winner. As a result, the league crowns two champions each year. After each Clausura, the team with the lowest points-per-game total for the previous six tournaments (three years, counting only Liga MX games) is relegated to Ascenso MX to be replaced by that league's champion (if eligible).\n\nIn the Netherlands, a playoff was introduced in season 2005–2006. It is used to determine which teams from the Eredivisie qualify for European football. The playoff system has been criticized by clubs, players and fans as the number of matches will increase. Under the original playoff format, it was possible, though thoroughly unlikely, that the runner-up would not qualify for Europe; the following year, the format was changed so that the second-place team was assured of no worse than a UEFA Cup berth. Starting in 2008–09, the format was changed yet again. The champion goes directly to the Champions League; the runner-up enters the second qualification round of the CL; the number three enters the fourth (and last) qualification round of the UEFA Europa League (EL; the new name of the UEFA Cup from 2009–10 onward) and the number four goes to the third qualification round of the EL. The only play-off will be for the clubs placed 5th through 8th. The winner of that play-off receives a ticket for the second qualification round of the EL.\n\nPlayoffs are also part of the promotion and relegation structure between the Eredivisie and the Eerste Divisie, the two highest football leagues in the Netherlands.\n\nThe Scottish Football League (SFL) experimented briefly with test matches in 1995–96 and 1996–97, contested between the second-bottom team of the Premier Division and the second-placed team of the First Division.\n\nAfter the Scottish Premier League (SPL) and SFL merged in 2013 to form the Scottish Professional Football League (SPFL), reuniting the top four divisions of Scotland since the breakaway of the SPL in 1998–99, a modified test match format was introduced between the Scottish Premiership and Scottish Championship. The bottom team from the first-tier Premiership is automatically relegated and is replaced by the winners of the second-tier Championship, provided that club meets Premiership entry criteria. The second-, third- and fourth-placed teams from the Championship qualify for a play-off consisting of two-legged ties, with the second-placed team receiving a bye to play the winner of the teams that finished third and fourth. The winner of this play-off then faces the second-bottom Premiership team, also over two legs, with the winner of that tie taking up the final Premiership place (again, assuming that the Championship club meets Premiership criteria).\n\nThe three lower divisions of the SPFL — the Championship, League One and League Two — continue with the promotion/relegation play-off system their predecessor SFL leagues used (the First Division, Second Division and Third Division, respectively). In the Championship/League One and League One/League Two, while the champions are automatically promoted and the bottom team relegated, there are play-offs of the second-bottom teams against the second-, third- and fourth-placed teams from the division below. Home and away ties decide semi-finals and a final, and the overall winner plays in the higher division the following season, with the loser playing in the lower division.\n\nBeginning with the 2014–15 season, promotion and relegation between the SPFL and the Scottish regional leagues were introduced. Following the end of the league season, the winners of the fifth-level Highland and Lowland Leagues compete in a two-legged playoff. The winner then enters a two-legged playoff against the bottom team from Scottish League Two, with the winner of that tie either remaining in or promoted to League Two.\n\nLong before the SPL era, two situations arose in which the top two teams in the table had to share the title as neither goal average nor goal difference had been instituted to break ties. The first was the inaugural season, in which Dumbarton and Rangers both earned 29 points and had to play off for the title. The match ended in a 0-0 draw and both teams shared the title. The second happened 19 years later, in the Second Division, when Leith Athletic and Raith Rovers both earned 33 points. This time, the clubs chose not to play off. In 1915 goal average was finally instituted.\n\nFor the 2010/11 season, the Segunda División experimented with promotion playoffs between the 3rd to 6th placed teams, similar to the rules in the English and Italian systems. However, due to reserve teams being allowed to compete in the same football league system, subsequent places may be allowed to play off depending on reserve teams finishing within the 3rd to 6th places.\n\nAt a lower level, playoffs in Segunda División B take place to decide the divisional title between the 4 group winners, and to decide which other teams would be promoted, as follows:\n\n\nPreviously a play off system had been used in which the teams finishing 3rd and 4th from last in La Liga had played off against the teams finishing 3rd and 4th in the Segunda División. This system had been introduced in the 1980s but ended in 1998-99.\n\nIn Major League Soccer in the United States and Canada, at the end of the regular season, the top five teams in each of its two conferences qualify for the playoffs (from the 2012 season to the 2014 season). Under this system, the conferences have separate playoff brackets. From 2015 to 2018, six teams per conference qualify, 12 teams in total, and Audi is the official sponsor. \nIn the first round of the postseason knockout tournament, the fourth-place team in each conference hosts the fifth-place team from the same conference in a one-off match, with the winner advancing to the Conference Semifinals. From the 2015 season the third-place team plays host to the sixth in the other one-off match, with the two winners advancing to the Conference Semifinals.\n\nThe Conference Semifinals and the Conference Championships are conducted under a home-and-away, aggregate-goal format. For each conference, the top seed plays the first-round winner, and the 2nd seed faces the 3rd seed in the Conference Semifinal series, with the lower seeded team hosting the first game. From the 2015 season the top seed plays the lowest remaining seed, and the 2nd plays the next-lowest seed in the Conference Semifinals.\n\nThe team that scores the most goals in the home-and-away series advances to the Conference Championship, which expands from a one-off match to a two-legged match starting in 2012. If the teams are tied after 90 minutes of the second leg in either the Conference Semifinal or Conference Championship, a 30-minute extra time period (divided into two 15-minute periods) would be played followed by a penalty-kick shootout, if necessary. As in the Conference Semifinals, the lower seed in the Conference Championship hosts the first leg.\n\nThe winner of each conference will play for the MLS Cup, the league championship. Since 2012, the MLS Cup is hosted by the conference champion with the most table points during the regular season.\n\nIn the case of ties after regulation in the First Round and MLS Cup, 30 minutes of extra time (divided into two 15-minute periods) would be played followed by a penalty-kick shootout, if necessary, to determine the winners.\n\nHistorically MLS did not use the away goals rule in any playoff series, but it has begun to do so in 2014 to be consistent with international practice.\n\nThe defunct Women's Professional Soccer (WPS), which operated only in the U.S., conducted a four-team stepladder tournament consisting of one-off knockout matches. The third seed hosted the fourth seed in the first round. The winner of that game advanced to the \"Super Semifinal\", hosted by the second seed. The Super Semifinal winner traveled to the top seed for the championship game. The replacement of WPS, the National Women's Soccer League (which launched in 2013), has a more standard four-team knockout playoff in which the winners of two one-off semifinals advance to the one-off final.\n\nPlayoffs are used throughout Australia in Australian rules football to determine the premiership. The term finals is most commonly used to describe them.\n\nIn each league, between four and eight teams (depending on league size) qualify for the finals based on the league ladder at the end of the season. Australian rules football leagues employ finals systems which act as a combination between a single elimination tournament for lower-ranked teams and a double elimination tournament for higher-ranked teams in order to provide teams with an easier pathway to the Grand Final as reward for strong performances throughout the season. Finals are decided by single matches, rather than series.\n\nThe Australian Football League, which is the top level of the sport, currently has eight teams qualify for the finals under a system designed by the league in 2000. Between 1931–1999, variants of the McIntyre System were used to accommodate four, five, six and eight teams, and prior to 1930, six different finals systems were used.\n\nIn most other leagues, from state-level leagues such as the South Australian National Football League and West Australian Football League, down to local suburban leagues, it is most common for either four or five teams to qualify for the finals. In these cases the Page–McIntyre final four system or the McIntyre final five system are used universally.\n\nThe Australian Football League (which was known until 1990 as the Victorian Football League) was the first league to introduce regular finals when it was established in 1897. The South Australian National Football League introduced finals in 1898, and other leagues soon followed.\n\nPrior to 1897, the premiership was generally awarded to the team with the best overall win-loss record at the end of the season. If two teams had finished with equal records, a playoff match for the premiership was required: this occurred in the Challenge Cup in 1871, the SAFA in 1889 and 1894, and in the VFA in 1896.\n\nThe teams finishing in fourth and fifth place in the regular season face each other in the wildcard game. The winner of the wildcard game faces the team that finished in third place in the first round of the play-offs. The winner of the first round faces the team that finished in second place during the regular season, and the winner of that round faces the team that finished in first place for the championship in the Korean Series. This type of format is known as the stepladder playoff.\n\nBefore 1950 the original Japanese Baseball League had been a single-table league of franchises. After it was reorganized into the Nippon Professional Baseball (NPB) system, a series of playoffs ensued between the champions of the Central League and Pacific League.\n\nBefore the playoff system was developed in both professional leagues, the Pacific League had applied a playoff system on two occasions. The first was between 1973–1982, when a split-season was applied with a 5-game playoff between the winning teams from both halves of season (unless a team won both of the halves so that they did not need to play such a game). The second time was between 2004–2006, when the top three teams played a two-staged stepladder knockout (3 games in the first stage and 5 games in the second stage) to decide the League Champion (and the team playing in the Japan Series). After this system was applied, the Seibu Lions (now Saitama Seibu Lions), Chiba Lotte Marines and Hokkaido Nippon Ham Fighters, which claimed the Pacific League Championship under this system, were all able to clinch the following Japan Series in that season. The success of such a playoff system convinced the Central League to consider a similar approach. In 2007, a new playoff system, named the \"Climax Series\", was introduced to both professional leagues in NPB to decide the teams that would compete for the Japan Series. The Climax Series basically applied the rule of the playoff system in the Pacific League, with one important change: each League championship is awarded to the team finishing the regular season at the top of their respective league, regardless of their fate in the playoffs. This means that the two League Champions are not guaranteed to make the Japan Series. The Chunichi Dragons took advantage of this in the first Climax Series season, finishing second in the regular season but sweeping the Hanshin Tigers and the League Champion Yomiuri Giants in the Central League to win a place in the Japan Series; they subsequently defeated the Hokkaido Nippon Ham Fighters to claim their first Japan Series in 52 years.\n\nIn 2008, the format of Climax Series will have a slight change, in which the second stage will be played over a maximum of six games, with the League Champion starting with an automatic one game advantage.\n\nMajor League Baseball (MLB) itself does not use the term \"playoffs\" for championship tournaments. Instead they use the term \"postseason\" as the title of the official elimination tournament held after the conclusion of Major League Baseball's regular season. Since the , it has consisted of a first round single-elimination knockout game between the two wildcards in each league, a best-of-5 second round series called the Division Series, and two rounds of best-of-seven series for the League Championship and World Series.\n\nMLB uses a \"2-3-2\" format for the final two rounds of its postseason tournament. In the Majors, the singular term \"playoff\" is reserved for the rare situation in which two (or more) teams find themselves tied at the end of the regular season and are forced to have a tiebreaking playoff game (or games) to determine which team will advance to the postseason. Thus, in the majors, a \"playoff\" is actually \"part of the regular season\" and thus can be called a \"Pennant playoff\". However, the plural term \"Playoffs\" is conventionally used by fans and media to refer to baseball's postseason tournament (and has always been used by minor league baseball for its own postseason play), not including the \"World Series\" (see below), so this article defers to that usage.\n\nMLB is the oldest of the major American professional sports, dating back to the 1870s. As such, it is steeped in tradition. The final series to determine its champion has been called the \"World Series\" (originally \"World's Championship Series\" and then \"World's Series\") as far back as the National League's contests with the American Association during the 1880s. The \"Playoffs\" determine which two teams play in the \"World Series\".\n\nTaiwan's playoff is different to many such competitions, due to the league's split-season format. The winners of the first half-season and the winners of the second half-season are eligible to play in the playoffs, but if the best overall team have not won either half season then they qualify into a wild card series against the weaker half-season winner, with the winner of this advancing into the Taiwan Series to face the other half-season winner. If the first and second half winners are different, but one of them is also the best overall team, then both teams progress directly to the Taiwan Series. Finally, if one team wins both halves of the season then a playoff will take place between the second and third best teams for the right to play them in the Final Series; in this case the team winning both halves of the season will begin the Taiwan Series with an automatic one game advantage.\n\nThe present organization known as the National Basketball Association, then called the BAA (Basketball Association of America), had its inaugural season in 1946–47. Teams had always have different strength of schedule from each other; currently, a team plays a team outside its conference twice, a team within its conference but outside its division three or four times, and a team from its own division four times.\n\nIn the current system, eight clubs from each of the league's two conferences qualify for the playoffs, with separate playoff brackets for each conference. In the 2002–03 season, the first-round series were expanded from best-of-5 to best-of-7; all other series have always been best-of-7. In all series, home games alternate between the two teams in a 2-2-1-1-1 format.\n\nThe 2-3-2 finals format was adopted from the 1985 Finals to 2013, copying the format that was then in effect in the National Hockey League. Prior to 1985, almost all finals were played in the 2-2-1-1-1 format (although the 1971 Finals between Milwaukee and Baltimore were on an alternate-home basis, some 1950s finals used the 2-3-2 format, and the 1975 Golden State-Washington and 1978 and 1979 Seattle-Washington Finals were on a 1-2-2-1-1 basis). Also, prior to the 1980s, Eastern and Western playoffs were on an alternate-home basis except for series when distance made the 2-2-1-1-1 format more practical. Since 2014, the NBA Finals restored the original format.\n\nTeams are seeded according to their regular-season record. Through the 2014–15 season, the three division champions and best division runner-up received the top four seeds, with their ranking based on regular-season record. The remaining teams were seeded strictly by regular-season record. However, if the best division runner-up had a better record than other division champs, it could be seeded as high as second. Beginning in 2015–16, the NBA became the first major American league to eliminate automatic playoff berths for division champions; the top eight teams overall in each conference now qualify for the playoffs, regardless of divisional alignment.\n\nTop flight basketball leagues elsewhere also employ a playoff system mimicking the NBA's. However, most leagues are not divided into divisions and conferences, and employ a double round robin format akin to league association football, unlike the NBA where teams are divided into divisions and conferences, which leads to different strengths of schedule per team. Teams are seeded on regular season record. The playoff structure can be single-elimination or a best-of series, with the higher seed, if held the playoffs are not held at a predetermined venue, having the home court advantage.\n\nAside from the playoffs, some leagues also have a knockout tournament akin to the FA Cup running in parallel to the regular season. These are not considered playoffs.\n\nIn the EuroLeague, after the regular season plays a best-of-5 playoffs in a 2–2–1 format. However, from the semifinals on, it is a single elimination tournament held at a predetermined venue. Still others also have a relegation playoff.\n\nIn NCAA Division I basketball conferences, a playoff or \"postseason tournament\" is held after the regular season. Most conferences, including all of the \"major\" basketball conferences (ACC, American, Big East, Big Ten, Big 12, Pac-12, SEC), hold their tournaments at a predetermined venue, with all conference teams participating (unless barred due to NCAA sanctions). A few conferences hold early rounds at campus sites and later rounds at a predetermined site. For example, the Mid-American Conference holds its first-round games at campus sites, but the rest of the tournament in Cleveland. The Big South Conference holds its first round at campus sites, gives hosting rights for its quarterfinals and semifinals to the regular-season champion, and plays its final at the home court of the top remaining seed. The American East Conference, ASUN Conference, and Patriot League hold all tournament games at campus sites. A small number of conferences do not invite all of their teams to the conference tournament, with one example being the Ivy League, in which only four of the eight members advance to the tournament (which is at a predetermined site). In many such tournaments, higher seeds are afforded byes. The winners, and some losers which are selected as \"at-large bids\", play in the NCAA tournament, which is also single-elimination and held at predetermined venues.\n\nIn the WNBA Playoffs, the league's best 8 teams, regardless of conference alignment, compete, and are seeded based on their regular-season records. The top two seeds get double byes and the next two seeds first-round byes. The first two rounds are one-off knockout games, and the league semifinals & Finals are best-of-5 on a 2-2-1 basis.\n\nIn the Canadian Football League, the playoffs begin in November. After the regular season, the top team from each division has an automatic home game berth in the Division Final, and a bye week during the Division Semifinal. The second-place team from each division hosts the third-place team in the Division Semifinal, unless the fourth-place team from the opposite division finishes with a better record. This \"crossover rule\" does not come into play if the teams have identical records—there are no tiebreakers. While the format means that it is possible for two teams in the same division to play for the Grey Cup, so far only two crossover teams have won the divisional semifinal game. The winners of each Division's Semifinal game then travel to play the first place teams in the Division Finals. Since 2005, the Division Semifinals and Division Finals have been sponsored by Scotiabank and are branded as the \"Scotiabank East Championship\" and \"Scotiabank West Championship\". The two division champions then face each other in the Grey Cup game, which is held on the third or fourth Sunday of November.\n\nThe Edmonton Eskimos are notable for qualifying for the CFL playoffs every year from 1972 to 2005, a record in North American pro sports. The Eskimos are also notable for being the first crossover team to ever win the divisional semifinal game.\n\nThe National Hockey League playoff system is an elimination tournament competition for the Stanley Cup, consisting of four rounds of best-of-seven series. The first three rounds determine which team from each conference will advance to the final round, dubbed the Stanley Cup Final. The winner of that series becomes the NHL and Stanley Cup champion.\n\nSince 2014 the Conference Quarterfinals consists of four match-ups in each conference, based on the seedings division-wise (# 1 vs. # 4, and # 2 vs. # 3). The division winner with the best record in the conference plays the lowest wild-card seed, while the other division winner plays the top wild-card seed (wild-card teams, who are de facto 4th seeds, may cross over to another division within the conference). In the Conference Semifinals, the four remaining teams in the conference face each other. In the third round, the Conference Finals, the two surviving teams play each other, with the conference champions proceeding to the Stanley Cup Final.\n\nFor the first two rounds, the higher-seeded team has home-ice advantage (regardless of point record). Thereafter, it goes to the team with the better regular season record. In all rounds the team with home-ice advantage hosts Games 1, 2, 5 and 7, while the opponent hosts Games 3, 4 and 6 (Games 5–7 are played \"if necessary\").\n\nThe Kontinental Hockey League, based in Russia and including teams from several nearby countries, operates a playoff system similar to that of the NHL, also consisting of four rounds of single-elimination best-of-seven series. The first three rounds determine which team from each conference will advance to the final round, dubbed the Gagarin Cup Finals. The winner of that series becomes the KHL and Gagarin Cup champion.\n\nLike the NHL, the Conference Quarterfinals consists of four match-ups in each conference. The winner of each division receives one of the top two seeds in its conference; the others are based on regular-season record. Unlike the NHL, divisional alignment plays no added role in playoff seeding—all teams are seeded solely within their conference. Playoff pairings are based on seeding number within the conference (# 1 vs. # 8, # 2 vs. # 7, # 3 vs. # 6, and # 4 vs. # 5). The division winner with the best record in the conference plays the lowest wild-card seed, while the other division winner plays the next-lowest seed (wild-card teams, who are de facto 4th seeds, may cross over to another division within the conference). The playoff pairings are reseeded after the first round (a feature that was once used in the NHL, but now abandoned). Therefore, the Conference Semifinals feature the top remaining seed in the conference playing the lowest remaining seed, and the two other first-round survivors playing one another. In the third round, the Conference Finals, the two surviving teams play each other, with the conference champions proceeding to the Gagarin Cup Finals.\n\nFor the first two rounds, the higher-seeded team has home-ice advantage (regardless of point record). Thereafter, it goes to the team with the better regular season record. In all rounds the team with home-ice advantage hosts Games 1, 2, 5 and 7, while the opponent hosts Games 3, 4 and 6 (Games 5–7 are played \"if necessary\").\n\nIn the United Kingdom, the Elite Ice Hockey League playoffs are an elimination tournament where the draw is based on the finishing position of teams in the league. Of the 10 teams which compete, the top 8 qualify for the playoffs. The first round (the quarter-finals) are played over two legs (home and away) where the team who finished in 1st place in the regular season plays the team which finished 8th, 2nd plays 7th and so on, with the aggregate score deciding which team progresses.\n\nThe semi-finals and final are held over the course of a single weekend at the National Ice Centre in Nottingham. Each consists of a single game with the losing team being eliminated, with the two semi-final games being played on the Saturday and the final on the Sunday. There is also a third-place game held earlier on the Sunday between the losing teams from the semi-finals. Unlike in the NHL, the winners of the Elite League playoffs are not considered to be the league champions for that season (that title goes to the team which finishes in first place in the league), rather the playoffs are considered to be a separate competition although being crowned playoff champions is a prestigious accolade nonetheless. The most recent playoff champions are the Sheffield Steelers.\n\nNASCAR implemented a \"playoff\" system beginning in 2004, which they coined the \"Chase for the NEXTEL Cup\". When first introduced, only NASCAR's top series used the system, although the other two national racing series (currently known as the Xfinity Series and the Camping World Truck Series) have since adopted similar systems. One unique feature of the Nascar playoffs is that the non-qualifying drivers continue to compete alongside the playoff drivers: the qualifying drivers merely have their championship points reset one or more times during the playoffs to figures so high that no non-qualifying driver could ever catch them.\n\nThere are actually two different playoffs going on at the end of the season in each series: one for the drivers and another \"owner's points\" playoff for the racing teams. Only one multi-driver team has ever won the Cup series owner's point championship: in 2015, Kyle Busch won the championship after missing the first 11 races of the season due to injury; 3 other drivers drove the #18 car during his absence. Because of the way the playoffs were structured that year, however, both he and his #18 team won their respective championships with 5043 points. There have been two cases where a playoff driver failed to enter every playoff race. In 2005, Kurt Busch was fired by Roush Racing with two races left in the season. Busch finished 10th out of 10 Chase drivers, but Kenny Wallace stepped in to drive the #97 car to an 8th-place finish in the owner's points race. In 2012, Dale Earnhardt, Jr. missed two playoff races due to injury. Regan Smith drove the #88 car for two races, including a top-10 finish at Kansas Speedway. In that case, Smith's 43 additional championship points on top of Earnhardt's 2,245 were not enough to pull the #88 team out of 12th place out of 12 playoff contenders. \n\nIn the original version of the Chase (2004–2006), following the 26th race of the season, all drivers in the top 10 and any others within 400 points of the leader got a spot in the 10-race playoff. Like the current system, drivers in the Chase had their point totals adjusted. However, it was based on the number of points at the conclusion of the 26th race. The first-place driver in the standings led with 5,050 points; the second-place driver started with 5,045. Incremental five-point drops continued through 10th place with 5,005 points.\n\nThe first major change to the Chase was announced by NASCAR chairman and CEO Brian France on January 22, 2007. After 26 races, the top 12 drivers advanced to contend for the points championship and points were reset to 5000. Each driver within the top 12 received an additional 10 points for each win during the \"regular season\", or first 26 races, thus creating a seeding based on wins. As in previous years, the Chase consisted of 10 races and the driver with the most points at the conclusion of the 10 races was the NEXTEL Cup Series Champion. Under the points system then in use, drivers could earn 5 bonus points for leading the most laps, and 5 bonus points for leading a single lap.\nBrian France explained why NASCAR made the changes to the chase:\n\"The adjustments taken [Monday] put a greater emphasis on winning races. Winning is what this sport is all about. Nobody likes to see drivers content to finish in the top 10. We want our sport -- especially during the Chase -- to be more about winning.\"\n\nBeginning with the 2008 season, the playoff became known as the \"Chase for the Sprint Cup\" due to the NEXTEL/Sprint merger.\n\nThe next format of the Chase was announced by France on January 26, 2011, along with several other changes, most significantly to the points system. After 26 races, 12 drivers still advanced to the Chase, but the qualifying criteria changed, as well as the number of base points that drivers received at the points reset.\n\nUnder this system. only the top 10 drivers in points automatically qualified for the Chase. They were joined by two \"wild card\" qualifiers, specifically the two drivers ranked from 11th through 20th in points who had the most race wins (with tiebreakers used if needed to select exactly two qualifiers). These drivers then had their base points reset to 2,000 instead of the previous 5,000, reflecting the greatly reduced points available from each race (a maximum of 48 for the race winner, as opposed to a maximum of 195 in the pre-2011 system). After the reset, the 10 automatic qualifiers received 3 bonus points for each race win, while the wild card qualifiers received no bonus.\n\nOn January 30, 2014. even more radical changes to the Chase were announced; these took effect for the 2014 season:\n\nThe Chase for the Sprint Cup has been generally panned since its inception, as many drivers and owners have criticized the declining importance of the first 26 races, as well as very little change in schedule from year to year. Mike Fisher, the director of the NASCAR Research and Development Center, has been one of the more vocal critics of the system, saying that \"Due to NASCAR having the same competitors on the track week in, week out, a champion emerges. In stick-and-ball sports, every team has a different schedule, so head-to-head series are necessary to determine a champion. That does not apply to auto racing.\"\n\nNASCAR extended the Chase format to its other two national touring series, the Xfinity Series and Camping World Truck Series, beginning in 2016. The formats used in the two lower series are broadly similar to the format used in the Cup Series, but have some significant differences:\n\nStarting with the 2017 season, NASCAR abandoned the term \"Chase\", instead calling its final series the \"playoffs\".\n\nPlay-offs are used to decide the premiers of the National Rugby League (NRL) in Australasia, where they are known as \"finals\" (also as \"semi finals\" or \"semis\") – as in Australian rules football, the participating teams only come from within a single division, and the tournament is staged as single matches rather than a series. Currently, in the NRL, eight teams qualify for the finals; starting with the 2012 season, the system was changed from the McIntyre Final Eight to the same system used by the AFL.\n\nPreviously, the term play-off was used in the NSWRL competition to describe matches which were played as tie breakers to determine qualification for the finals series. Since 1995, points differential decides finals' qualification and play-offs are no longer held.\n\nThe European Super League rugby league competition has used a play-off system to decide its champion since 1998. The original play-off format featured the top five highest-ranked teams after the regular season rounds. Starting in 2002, the play-offs added an extra spot to allow the top six to qualify. With the addition of two new teams for the 2009 season, the play-offs expanded to eight teams. The next format, scrapped after the 2014 season, worked as follows:\n\nWeek One\n\nWeek Two\n\nWeek Three\n\nWeek Four\n\n\"* Opponents decided by the QPO winner (in Week 1) that finished higher in the regular season\n\nBeginning in 2015, the Super League season was radically reorganised, and more closely integrated with that of the second-level Championship. Following a home-and-away season of 22 matches, the top eight clubs in Super League now enter a single round-robin mini-league known as the Super 8s, with the top four teams after that stage entering a knockout play-off to determine the champion. The four bottom teams in Super League at the end of the home-and-away season are joined by the top four from the Championship after its home-and-away season. These eight teams play their own single-round-robin mini-league known as The Qualifiers; at its end, the top three teams are assured of places in the next season's Super League, with the fourth- and fifth-place teams playing a single match billed as the \"Million Pound Game\", with the winner also playing in Super League in the following season.\n\nThe two tiers directly below Super League, the Championship and League 1 (the latter of which was known as Championship 1 from 2009–2014)—formerly the National Leagues until the 2009 addition of a French club to the previously all-British competition—used the old top six system to determine which teams were promoted between its levels through the 2014 season. After that season, both leagues abandoned the top six system. Before the 2008 season, when Super League established a franchising system and ended automatic promotion and relegation in Super League, the National Leagues also used this system to determine the team that earned promotion to Super League. The top six system involved the following:\n\nWeek One\n\nWeek Two\n\nWeek Three\n\nWeek Four\n\nSince 2015, all clubs in Super League and the Championship play a 22-match home-and-away season. Upon the end of the home-and-away season, the clubs will split into three leagues, with two of them including Championship clubs. As previously noted, the Super 8s will feature the top eight Super League sides. The second league, The Qualifiers, will include the bottom four clubs from Super League and the top four from the Championship, whilst the third will feature the remaining eight Championship sides. The bottom two leagues will begin as single round-robin tournaments. In The Qualifiers, the top three sides will either remain in or be promoted to Super League, with the fourth- and fifth-place teams playing the aforementioned \"Million Pound Game\" for the final Super League place. In the third league, the sides compete for the Championship Shield, with the top four teams after the round-robin phase entering a knockout playoff for the Shield. The bottom two teams are relegated to League 1.\n\nLeague 1 currently conducts a 15-match, single round-robin regular season. At that time, the league splits in two. The top eight clubs play in their own Super 8s, also contested as a single round robin. At the end of the Super 8s, the top club earns the season title and immediate promotion to the Championship. The second- through fifth-place clubs contest a knockout playoff for the second place in the Championship. The bottom eight clubs play their own single round-robin phase; at its end, the top two teams play a one-off match for the League 1 Shield.\n\nIn the Gallagher Premiership, the top four qualify for the play-offs, where they are not referred to by that name. The tournament is a Shaughnessy playoff: the team that finished first after the league stage plays the team that finished fourth, while the team that finished second plays the team that finished third in the Semi-Finals, with the higher-ranked team having home advantage. The winners of these semi-finals qualify for the Premiership Final at Twickenham, where the winner will be champions of the league.\n\nThrough the 2016–17 season, the second-level RFU Championship used play-offs—but unlike the Premiership, the Championship officially used the term \"play-offs\". At the end of the league stage, top teams advanced to a series of promotion play-offs. From the first season of the Championship in 2009–10 to 2011–12, the top eight teams advanced; from 2012–13 through to 2016–17, the top four advanced. A relegation play-off involving the bottom four teams existed through the 2011–12 season, but was scrapped from 2012–13 on.\n\nThe original promotion play-offs divided the eight teams into two groups of four each, with the teams within each group playing a home-and-away mini-league. The top two teams in each group advanced to a knockout phase. In 2010, the semi-finals were one-off matches; in 2011, they became two-legged. The top team in each pool played the second-place team from the other group in the semi-finals; the winners advanced to the two-legged final, where the ultimate winner earned promotion to the Premiership (assuming that the team met the minimum criteria for promotion).\n\nIn the first year of the play-offs in 2010, all eight teams started equal. After that season, it was decided to reward teams for their performance in league play. in 2011 and 2012, the top two teams at the end of the league stage carried over 3 competition points to the promotion play-offs; the next two teams carried over 2; the next two carried over 1; and the final two teams carried over none. (Points were earned using the standard bonus points system.)\n\nThe relegation play-offs, like the first stage of the promotion play-offs, were conducted as a home-and-away league, with the bottom team at the end of league play relegated to National League 1. As with the 2010 promotion play-offs, that season's relegation play-offs started all teams equal. in 2011 and 2012, each team in the relegation play-offs carried over 1 competition point for every win in the league season.\n\nBeginning with the 2012–13 season, the pool stage of the promotion playoffs was abolished, with the top four sides directly entering the semi-finals. The format of the knockout stage remained unchanged from 2012, with two-legged semi-finals followed by a two-legged final. At the other end of the table, the bottom club is now automatically relegated.\n\nEffective with the 2017–18 season, the promotion play-offs were scrapped for a minimum of three seasons, to be replaced with automatic promotion for the club finishing atop the league at the end of the home-and-away season (provided said club meets minimum Premiership standards).\n\nThe highest level of French rugby union, the Top 14, expanded its playoffs starting with the 2009–10 season from a four-team format to six teams. In the new system, the top two teams after the double round-robin season receive first-round byes. The first-round matches involve the third- through sixth-place teams, bracketed so that 3 hosts 6 and 4 hosts 5. The winners then advance to face the top two teams in the semifinals, which are held at nominally neutral sites (a traditional feature in the French playoffs)—although in the 2011–12 season, the semifinals were held at Stadium de Toulouse, occasionally used as a \"big-game\" venue by traditional Top 14 power Stade Toulousain. The winners of these semifinals qualify for the final at Stade de France (though in 2016, the final was at Camp Nou in Barcelona due to conflict with UEFA Euro 2016), where the winner will be champions of the league and receive the Bouclier de Brennus. Before 2009–10, the playoffs format was identical to that of the English Premiership with the exception of neutral sites for the semifinals.\n\nBeginning in 2017–18, only the bottom club is automatically relegated to Rugby Pro D2. The second-from-bottom Top 14 side plays a one-off match against the runner-up of the Pro D2 playoffs for the final place in the next Top 14 season.\n\nPro D2 adopted the Top 14 playoff system effective in 2017–18, though with all matches held at the higher seed's home field. The playoff champion earns automatic promotion; as noted above, the runner-up enters a one-off match for potential promotion to Top 14. Previously, Pro D2 used a four-team playoff, involving the second- through fifth-place teams, to determine the second of two teams promoted to the next season's Top 14, with the regular-season champions earning automatic promotion. Under this system, the promotion semifinals were held at the home fields of the second- and third-place teams, and the promotion final was held at a neutral site.\n\nThe Pro14, originally known as the Celtic League and later as Pro12, adopted a four-team playoff starting with the 2009–10 season. The format was essentially identical to that of the English Premiership. Through the 2013–14 season, the final was held at a ground chosen by the top surviving seed, with the caveat that the venue must have a capacity of at least 18,000. In 2012–13, top seed Ulster could not use its regular home ground of Ravenhill for that reason (the ground was later expanded to meet the requirement). The league changed to using a predetermined site for its championship final in 2014–15.\n\nWith the addition of two South African sides in 2017–18, the league split into two conferences and expanded its playoffs to six teams. The top team of each conference earns a bye into the semifinals, where they will host the winners of matches between the second- and third-place teams from the other conferences (with the second-place team hosting the third-place team from the opposite conference).\n\nBoth domestic competitions in New Zealand rugby — the semi-professional Mitre 10 Cup (formerly Air New Zealand Cup and ITM Cup) and the nominally amateur Heartland Championship — use a playoff system to determine their champions, although the term \"playoff\" is also not used in New Zealand, with \"finals\" used instead.\n\nIn the 2006 Air New Zealand Cup, the first season of the revamped domestic structure in that country, the top six teams after Round One of the competition automatically qualified for the finals, officially known as Round Three. Their relative seeding was determined by their standings at the end of the Top Six phase of Round Two. The teams that finished below the top six entered repechage pools in Round Two, with the winner of each pool taking up one of the final two finals slots. The seventh seed was the repechage winner with the better record, and the eighth seed was the other repechage winner.\n\nFrom 2007 onward, the former Rounds One and Two were collapsed into a single pool phase of play in which all teams participated. In 2007 and 2008, the top eight teams advanced to the playoffs; in what was intended to be the final season of the Air New Zealand Cup format in 2009, the Shaughnessy format was used, with the top four advancing to the finals. The New Zealand Rugby Union (NZRU) ultimately decided to stay with the previous format for the rebranded 2010 ITM Cup, with the same four-team playoff as in 2009. Starting in 2011, the NZRU split the ITM Cup into two seven-team leagues, the top-level Premiership and second-level Championship, and instituted promotion and relegation in the ITM Cup (a feature of the country's former National Provincial Championship). The competition was renamed the Mitre 10 Cup in 2016.\n\nThe playoffs in each season format have consisted of a single-elimination tournament. The teams are bracketed in the normal fashion, with the higher seed receiving home-field advantage. In 2007 and 2008, the playoff was rebracketed after the quarterfinals, with the highest surviving seed hosting the lowest surviving seed and the second-highest surviving seed hosting the third surviving seed. The winners of these semifinals qualify for the Cup Final (2006–10) or Premiership/Championship Final (2011–), held at the home ground of the higher surviving seed. From 2011 onward, the winner of the Championship Final is promoted to the Premiership, replacing that league's bottom team.\n\nBecause the 2011 season ran up against that year's Rugby World Cup in New Zealand, the competition window was truncated, with only the top two teams in each division advancing to the final match. The Shaughnessy finals series returned to both divisions in 2012, and is currently used in non-World Cup years.\n\nIn the Heartland Championship, teams play for two distinct trophies — the more prestigious Meads Cup and the Lochore Cup. The 12 Heartland Championship teams are divided into two pools for round-robin play in Round One, with the top three in each pool advancing to the Meads Cup and the bottom three dropping to the Lochore Cup.\n\nRound Two in both the Meads and Lochore Cups is an abbreviated round-robin tournament, with each team playing only the teams it did not play in Round One. The top four teams in the Meads Cup pool at the end of Round Two advance to the Meads Cup semifinals; the same applies for the Lochore Cup contestants.\n\nThe semifinals of both cups are seeded 1 vs 4 and 2 vs 3, with the higher seeds earning home field advantage. The semifinal winners advance to their respective cup final, hosted by the higher surviving seed.\n\nThroughout the pre-2011 history of Super Rugby—both in the Super 12 and Super 14 formats—the competition's organiser, SANZAR (renamed SANZAAR in 2016), held a Shaughnessy playoff involving the top four teams. The top two teams on the league ladder each hosted a semifinal, with the top surviving team hosting the final.\n\nIn May 2009, SANZAR announced that it would adopt an expanded playoff when the competition added a new Australian team for the 2011 season. Through 2015, the Super Rugby playoff involved six teams—the winners of each of three conferences (Australia, New Zealand and South Africa conferences), plus the three non-winners with the most competition points without regard to conference affiliation.\n\nThe top two conference winners received a first-round bye; each played at home against the winner of an elimination match involving two of the four other playoff teams. As in the previous system, the final was hosted by the top surviving seed.\n\nFurther expansion of the competition in 2016 to 18 teams, with one extra entry from South Africa and new teams based in Argentina and Japan, saw the playoff bracket expand to eight teams. The teams were split into African and Australasian groups, with the Argentine and Japanese teams joining the African group. Each group in turn was divided into two conferences (Australia, New Zealand, Africa 1, Africa 2). Conference winners received the top four playoff seeds, and were joined by the top three remaining Australasian teams and the top remaining team from the African group on table points, again without regard to conference affiliation. The higher seed still hosted all playoff matches, including the final.\n\nWith the contraction of the league to 15 teams for 2018, with one Australian and two South African teams being axed, the playoff format changed yet again. The number of conferences was reduced from four to three—Australia, New Zealand and South Africa, with the Argentine team joining the South Africa conference and the Japanese team joining the Australia conference. The playoff will remain at eight teams, with the three conference winners joined by five \"wildcards\", specifically the top remaining teams without regard to conference affiliation. The conference winners and the top wildcard will host quarterfinals, with all remaining matches hosted by the higher seed.\n\n\n"}
{"id": "10323080", "url": "https://en.wikipedia.org/wiki?curid=10323080", "title": "Production leveling", "text": "Production leveling\n\nProduction leveling, also known as production smoothing or – by its Japanese original term – , is a technique for reducing the Mura (Unevenness) which in turn reduces muda (waste). It was vital to the development of production efficiency in the Toyota Production System and lean manufacturing. The goal is to produce intermediate goods at a constant rate so that further processing may also be carried out at a constant and predictable rate.\n\nWhere demand is constant, production leveling is easy, but where customer demand fluctuates, two approaches have been adopted: 1) \"demand leveling\" and 2) \"production leveling\" through flexible production.\n\nTo prevent fluctuations in production, even in outside affiliates, it is important to minimize fluctuation in the final assembly line. Toyota's final assembly line never assembles the same automobile model in a batch. Instead, they level production by assembling a mix of models in each batch and the batches are made as small as possible.\n\nProduction leveling can refer to leveling by volume, or leveling by product type or mix, although the two are closely related.\n\nIf for a family of products that use the same production process there is a demand that varies between 800 and 1,200 units then it might seem a good idea to produce the amount ordered. Toyota's view is that production systems that vary in the required output suffer from mura and muri with capacity being 'forced' in some periods. So their approach is to manufacture at the long-term average demand and carry an inventory proportional to the variability of demand, stability of the production process and the frequency of shipments. So for our case of 800–1,200 units, if the production process were 100% reliable and the shipments once a week, then the production would be with minimum standard inventory of 200 at the start of the week and 1,200 at the point of shipment. The advantage of carrying this inventory is that it can smooth production throughout the plant and therefore reduce process inventories and simplify operations which reduces costs.\n\nMost value streams produce a mix of products and therefore face a choice of production mix and sequence. It is here that the discussions on economic order quantities take place and have been dominated by changeover times and the inventory this requires. Toyota's approach resulted in a different discussion where it reduced the time and cost of changeovers so that smaller and smaller batches were not prohibitive and lost production time and quality costs were not significant. This meant that the demand for components could be leveled for the upstream sub-processes and therefore lead time and total inventories reduced along the entire value stream. To simplify leveling of products with different demand levels a related visual scheduling board known as a heijunka box is often used in achieving these heijunka style efficiencies. Other production leveling techniques based on this thinking have also been developed. Once leveling by product is achieved then there is one more leveling phase, that of \"Just in Sequence\" where leveling occurs at the lowest level of product production.\n\nThe use of production leveling as well as broader lean production techniques helped Toyota massively reduce vehicle production times as well as inventory levels during the 1980s.\n\nEven Toyota hasn't reached the final stage in this journey, single-piece flows, across all of their processes; indeed they recommend following their journey rather than trying to jump into an intermediate stage. The reason Toyota advocate this is that each production stage is accompanied by adjustments and adaptations to support services to production; if those services are not given these adaptation steps then major issues can arise.\n\n\nDemand leveling is the deliberate influencing of demand itself or the demand processes to deliver a more predictable pattern of customer demand. Some of this influencing is by manipulating the product offering, some by influencing the ordering process and some by revealing the demand amplification induced variability of ordering patterns. Demand levelling does not include influencing activities designed to clear existing stock.\n\nHistorically demand leveling evolved as subset of production levelling and has been approached in a variety of ways:\n\n\nIf it is accepted that a large part of demand variability in high volume products can be substantially caused by sales and ordering process artifacts then analysis and leveling can be attempted.\n\nThe use of long delay supply chains to reduce manufacturing costs often means that production orders are placed long before customer demand can be realistically estimated. The much later arrival of forecast product demand volumes makes demand leveling irrelevant since the issue has now switched to disposal at best price possible products that are already created and possibly paid for. Demand leveling has only proven possible where build times have been made relatively low and production has been made relatively reliable and flexible. Examples of these are fast airborne supply chains (e.g. Apple iPod) or direct to customer selling through web sites allowing late customisation (e.g. NIKEiD custom shoes) or local manufacture (e.g. Timbuk2 custom courier bags).\n\nWhere actual build-delivery times can be brought within the same scale as customer time horizons then effort to modify impulse buying and make it somewhat planned can be successful. Reliable, flexible manufacturing will then mean that low stock levels (if any) do not interfere with customer satisfaction and that incentives to sell what has been produced eliminated.\n\nWhere demand follows a predictable pattern, e.g. flat, then regular deliveries of constant amounts can be agreed with variances in actual demand ignored unless it exceeds some agreed trigger level. Where this cannot be agreed then it can be simulated and the benefits gained through frequent deliveries and a market location.\n\nThe predictable pattern does not have to be flat and may, for example, be an annual pattern with higher volumes at particular periods. Here again the deliveries can be agreed to follow a simplified but similar pattern, perhaps one delivery volume for six months of the year and another for the other six months.\n\n"}
{"id": "34208889", "url": "https://en.wikipedia.org/wiki?curid=34208889", "title": "Raga (Buddhism)", "text": "Raga (Buddhism)\n\nRaga (Sanskrit, also \"rāga\"; Pali \"lobha\"; Tibetan: \" 'dod chags\") is a Buddhist concept of character affliction or poison referring to any form of \"greed, sensuality, lust, desire\" or \"attachment to a sensory object\". Raga (lobha) is identified in the following contexts within the Buddhist teachings:\n\n\"Rāga\" literally means \"color or hue\" in Sanskrit, but appears in Buddhist texts as a form of blemish, personal impurity or fundamental character affliction. As a philosophical concept, the term refers to \"greed, sensuality, desire\" or \"attachment to a sensory object\". It includes any form of desire including sexual desire and sensual passion, as well as attachments to, excitement over and pleasure derived from objects of the senses. Some scholars render it as \"craving\".\n\n\"Raga\" is one of three poisons and afflictions, also called the \"threefold fires\" in Buddhist Pali canon, that prevents a being from reaching \"nirvana\". To extinguish all \"Raga\" (greed, lust, desire, attachment) is one of the requirements of \"nirvana\" (liberation) in Buddhism.\n\nThe Abhidharma-samuccaya states: \n\nRaga is said to arise from the identification of the self as being separate from everything else. This mis-perception or misunderstanding is referred to as \"avidya\" (ignorance).\n\n\n\n"}
{"id": "55308287", "url": "https://en.wikipedia.org/wiki?curid=55308287", "title": "Riyan Airport prison in Yemen", "text": "Riyan Airport prison in Yemen\n\nRiyan Airport prison in Yemen is a secret prison located in the cargo terminal in Riyan Airport in Yemen. The airport is a civilian one in the city of Mukalla. US special forces fighting al-Qaida are based close to this prison. Witnesses have claimed some detainees have been handed over to the US forces for further interrogation.\nAccording to Associated press, there are at least eighteen secret detention centers in Southern Yemen. \nAccording to Hussein Arab, Yemen Minister of Interior, these clandestine prisons are located in an airport, military bases and private villas.\n\nSenior American defense officials have acknowledged that US forces have been involved in interrogations of detainees in Yemen, however they have denied any mistreatment of the detainees.They said that there had not been any abuse when US forces were present.\nAllegedly there are at least 18 clandestine prisons across southern Yemen run by the United Arab Emirates (UAE) or by Yemeni forces. UAE’s government has denied the allegations.</ref>\n"}
{"id": "739974", "url": "https://en.wikipedia.org/wiki?curid=739974", "title": "Rostow's stages of growth", "text": "Rostow's stages of growth\n\nRostow's Stages of Economic Growth model is one of the major historical models of economic growth. It was published by American economist Walt Whitman Rostow in 1960. The model postulates that economic growth occurs in five basic stages, of varying length:\n\n\nRostow's model is one of the more structuralist models of economic growth, particularly in comparison with the \"backwardness\" model developed by Alexander Gerschenkron, although the two models are not mutually exclusive.\n\nRostow argued that economic take-off must initially be led by a few individual economic sectors. This belief echoes David Ricardo's comparative advantage thesis and criticizes Marxist revolutionaries' push for economic self-reliance in that it pushes for the \"initial\" development of only one or two sectors over the development of all sectors equally. This became one of the important concepts in the theory of modernization in social evolutionism.\n\nBelow is an outline of Rostow's Five Stages of Growth:\n\nRostow claimed that these stages of growth were designed to tackle a number of issues, some of which he identified himself, writing: \"Under what impulses did traditional, agricultural societies begin the process of their modernization? When and how did regular growth become a built-in feature of each society? What forces drove the process of sustained growth along and determined its contours? What common social and political features of the growth process may be discerned at each stage? What forces have determined relations between the more developed and less developed areas; and what relation if any did the relative sequence of growth bear to outbreak of war? And finally where is compound interest taking us? Is it taking us to communism; or to the affluent suburbs, nicely rounded out with social overhead capital; to destruction; to the moon; or where?\"Rostow asserts that countries go through each of these stages fairly linearly, and set out a number of conditions that were likely to occur in investment, consumption, and social trends at each state. Not all of the conditions were certain to occur at each stage, however, and the stages and transition periods may occur at varying lengths from country to country, and even from region to region.\n\nRostow's model is a part of the liberal school of economics, laying emphasis on the efficacy of modern concepts of free trade and the ideas of Adam Smith. It disagrees with Friedrich List's argument which states that economies which rely on exports of raw materials may get \"locked in\", and would not be able to diversify, regarding this Rostow's model states that economies may need to depend on raw material exports to finance the development of industrial sector which has not yet of achieved superior level of competitiveness in the early stages of take-off. Rostow's model does not disagree with John Maynard Keynes regarding the importance of government control over domestic development which is not generally accepted by some ardent free trade advocates. The basic assumption given by Rostow is that countries want to modernize and grow and that society will agree to the materialistic norms of economic growth.\n\nAn economy in this stage has an unlimited production function which barely attains the minimum level of potential output. This does not entirely mean that the economy's production level is static. The output level can still be increased, as there was often a surplus of uncultivated land which can be used for increasing agricultural production. Modern science and technology has yet to be introduced. As a result, these pre-Newtonian societies, unaware of the possibilities to manipulate the external world, rely heavily on manual labor and self-sufficiency to survive. States and individuals utilize irrigation systems in many instances, but most farming is still purely for subsistence. There have been technological innovations, but only on ad hoc basis. All of that this can result in increases in output, but never beyond an upper limit which cannot be crossed. Trade is predominantly regional and local, largely done through barter, and the monetary system is not well developed. Investment's share never exceeds 5% of total economic production.\n\nWars, famines and epidemics like plague cause initially expanding populations to halt or shrink, limiting the single greatest factor of production: human manual labor. Volume fluctuations in trade due to political instability are frequent; historically, trading was subject to great risk and transport of goods and raw materials was expensive, difficult, slow and unreliable. The manufacturing sector and other industries have a tendency to grow but are limited by inadequate scientific knowledge and a \"backward\" or highly traditionalist frame of mind which contributes to low labour productivity. In this stage, some regions are entirely self-sufficient.\n\nIn settled agricultural societies before the Industrial Revolution, a hierarchical social structure relied on near-absolute reverence for tradition, and an insistence on obedience and submission. This resulted in concentration of political power in the hands of landowners in most cases; everywhere, family and lineage, and marriage ties, constituted the primary social organization, along with religious customs, and the state only rarely interacted with local populations and in limited spheres of life. This social structure was generally feudalistic in nature. Under modern conditions, these characteristics have been modified by outside influences, but the least developed regions and societies fit this description quite accurately..\n\nIn the second stage of economic growth the economy undergoes a process of change for building up of conditions for growth and take off. Rostow said that these changes in society and the economy had to be of fundamental nature in the socio-political structure and production techniques. This pattern was followed in Europe, parts of Asia, the Middle East and Africa. There is also a second or third pattern in which he said that there was no need for change in socio-political structure because these economies were not deeply caught up in older, traditional social and political structures. The only changes required were in economic and technical dimensions. The nations which followed this pattern were in North America and Oceania (New Zealand and Australia).\n\nThere are three important dimensions to this transition: firstly, the shift from an agrarian to an industrial or manufacturing society begins, albeit slowly. Secondly, trade and other commercial activities of the nation broaden the market's reach not only to neighboring areas but also to far-flung regions, creating international markets. Lastly, the surplus attained should not be wasted on the conspicuous consumption of the land owners or the state, but should be spent on the development of industries, infrastructure and thereby prepare for self-sustained growth of the economy later on. Furthermore, agriculture becomes commercialized and mechanized via technological advancement; shifts increasingly towards cash or export-oriented crops; and there is a growth of agricultural entrepreneurship.\n\nThe strategic factor is that investment level should be above 5% of the national income. This rise in investment rate depends on many sectors of the economy. According to Rostow capital formation depends on the productivity of agriculture and the creation of social overhead capital. Agriculture plays a very important role in this transition process as the surplus quantity of the produce is to be utilized to support an increasing urban population of workers and also becomes a major exporting sector, earning foreign exchange for continued development and capital formation. Increases in agricultural productivity also lead to expansion of the domestic markets for manufactured goods and processed commodities, which adds to the growth of investment in the industrial sector.\n\nSocial overhead capital creation can only be undertaken by government, in Rostow's view. Government plays the driving role in development of social overhead capital as it is rarely profitable, it has a long gestation period, and the pay-offs accrue to all economic sectors, not primarily to the investing entity; thus the private sector is not interested in playing a major role in its development.\n\nAll these changes effectively prepare the way for \"take-off\" only if there is basic change in attitude of society towards risk taking, changes in working environment, and openness to change in social and political organisations and structures.\nAccording to Rostow, the preconditions to take-off begins from an external intervention by more developed and advanced societies, which \"set in motion ideas and sentiments which initiated the process by which a modern alternative to the traditional society was constructed out of the old culture.\" The pre-conditions of take-off closely track the historic stages of the (initially) British Industrial Revolution. \n\nReferring to the graph of savings and investment, notably, there is a steep increase in the rate of savings and investment from the stage of \"Pre Take-off\" till \"Drive to Maturity:\" then, following that stage, the growing rate of savings and investment moderates. This initial and accelerating steep increase in savings and investment is a pre-condition for the economy to reach the \"Take-off\" stage and far beyond.\n\nThis stage is characterized by dynamic economic growth. As Rostow suggests, all is premised on a sharp stimulus (or multiple stimuli) that is/are any or all of economic, political and technological change. The main feature of this stage is rapid, self-sustained growth. Take-off occurs when sector led growth becomes common and society is driven more by economic processes than traditions. At this point, the norms of economic growth are well established and growth becomes a nation's \"second nature\" and a shared goal. In discussing the take-off, Rostow is noted to have adopted the term \"transition\", which describes the process of a traditional economy becoming a modern one. After take-off, a country will generally take as long as fifty to one hundred years to reach the mature stage according to the model, as occurred in countries that participated in the Industrial Revolution and were established as such when Rostow developed his ideas in the 1950s.\n\nPer Rostow there are three main requirements for take-off:1. The rate of productive investment should rise from approximately 5% to over 10% of national income or net national product2. The development of one or more substantial manufacturing sectors, with a high rate of growth;3. The existence or quick emergence of a political, social and institutional framework which exploits the impulses to expansion in the modern sector and the potential external economy effects of the take-off. The third requirement implies that the needed capital must be mobilized from domestic resources and steered into the economy, rather than into domestic or state consumption.Industrialization becomes a crucial phenomenon as it helps to prepare the basic structure for structural changes on a massive scale. Rostow says that this transition does not follow a set trend as there are a variety of different motivations or stimulus which began this growth process.\nTake off requires a large and sufficient amount of loanable funds for expansion of the industrial sector which generally come from two sources which are:\n\nThe take-off also needs a group of entrepreneurs in the society who pursue innovation and accelerate the rate of growth in the economy. For such an entrepreneurial class to develop, firstly, an ethos of \"delayed gratification\", a preference for capital accumulation over expenditure, and high tolerance of risk must be present. Secondly, entrepreneurial groups typically develop because they can not secure prestige and power in their society via marriage, via participating in well-established industries, or through government or military service (among other routes to prominence) because of some disqualifying social or legal attribute; and lastly, their rapidly changing society must tolerate unorthodox paths to economic and political power.\n\nThe ability of a country to make it through this stage depends on the following major factors:\n\nAn example of a country in the Take-off stage of development is Equatorial Guinea. It has the largest increases in GDP growth since 1980 and the rate of productive investment has risen from 5% to over 10% of income or product.\n\nIn the table note that Take-off periods of different countries are the same as the industrial revolution in those countries.\n\nAfter take-off, there follows a long interval of sustained growth known as the stage of drive to maturity. Rostow defines it \"as the period when a society has effectively applied the range of modern technology to the bulk of its resources.\" Now regularly growing economy drives to extend modern technology over the whole front of its economic activity. Some 10-20% of the national income is steadily invested, permitting output regularly to outstrip the increase in population. The makeup of the economy changes unceasingly as technique improves, new industries accelerate, older industries level off. The economy finds its place in the international economy: goods formerly imported are produced at home; new import requirements develop, and new export commodities to match them. The leading sectors in an economy will be determined by the nature of resource endowments and not only by technology.\nOn comparing the dates of take-off and drive to maturity, these countries reached the stage of maturity in approximately 60 years.\n\nThe structural changes in the society during this stage are in three ways:\nDuring this stage a country has to decide whether the industrial power and technology it has generated is to be used for the welfare of its people or to gain supremacy over others, or the world \"in toto\".\n\nA prime example of a country in the Drive to Maturity stage is South Africa. It is developing a world-class infrastructure- including a modern transport network, widely available energy, and sophisticated telecommunications facilities. Additionally, the commercial farm sector shed 140,000 jobs, a decline of roughly 20%, in the eleven-year period from 1988 to 1998.\n\nThis diversity leads to reduction in poverty rate and increasing standards of living, as the society no longer needs to sacrifice its comfort in order to build up certain sectors.\n\nThe age of high mass consumption refers to the period of contemporary comfort afforded many western nations, wherein consumers concentrate on durable goods, and hardly remember the subsistence concerns of previous stages. Rostow uses the Buddenbrooks dynamics metaphor to describe this change in attitude. In Thomas Mann's 1901 novel, \"Buddenbrooks\", a family is chronicled for three generations. The first generation is interested in economic development, the second in its position in society. The third, already having money and prestige, concerns itself with the arts and music, worrying little about those previous, earthly concerns. So too, in the age of high mass consumption, a society is able to choose between concentrating on military and security issues, on equality and welfare issues, or on developing great luxuries for its upper class. Each country in this position chooses its own balance between these three goals. There is a desire to develop an egalitarian society and measures are taken to reach this goal. According to Rostow, a country tries to determine its uniqueness and factors affecting it are its political, geographical and cultural structure and also values present in its society.\n\nHistorically, the United States is said to have reached this stage first, followed by other western European nations, and then Japan in the 1950s.\n\nThis step is more of a theoretical speculation by Rostow rather than an analytical step in the process by Rostow. Individuals begin having larger families and do not value income as a pre-requisite for more vacation days. Consumer products become more durable and more diverse. New Americans will behave in a way where the high economic security and level mass consumption is considered normal. Rostow does make the point that it is possible with the large baby boom it could either cause economic issues or dictate an even larger diffusion of consumer goods. With increasing urban and suburban population there will be undoubtedly an increase in consumer goods and services as well.\n\n\nRostow's thesis is biased towards a western model of modernization, but at the time of Rostow the world's only mature economies were in the west, and no controlled economies were in the \"era of high mass consumption.\" The model de-emphasizes differences between sectors in capitalistic vs. communistic societies, but seems to innately recognize that modernization can be achieved in different ways in different types of economies.\n\nAnother assumption that Rostow took is of trying to fit economic progress into a linear system. This assumption is questioned due to empirical evidence of many countries making 'false starts' then reaching a degree of progress and change and then slipping back. E.g.: In the case of contemporary Russia slipping back from high mass consumption to a country in transition.\n\nAnother criticism of Rostow's work is that it consideres large countries with a large population (Japan), with natural resources available at just the right time in its history (Coal in Northern European countries), or with a large land mass (Argentina). He has little to say and indeed offers little hope for small countries, such as Rwanda, which do not have such advantages. Neo-liberal economic theory to Rostow, and many others, does offer hope to much of the world that economic maturity is coming and the age of high mass consumption is nigh.This does leave a potential \"grim meathook future\" for the outliers, which do not have the resources, political will, or external backing to become competitive with already developed economies. (See Dependency theory)\n\n\n"}
{"id": "146075", "url": "https://en.wikipedia.org/wiki?curid=146075", "title": "Self-help", "text": "Self-help\n\nSelf-help or self-improvement is a self-guided improvement—economically, intellectually, or emotionally—often with a substantial psychological basis. Many different self-help group programs exist, each with its own focus, techniques, associated beliefs, proponents and in some cases, leaders. Concepts and terms originating in self-help culture and Twelve-Step culture, such as recovery, dysfunctional families, and codependency have become firmly integrated in mainstream language.\n\nSelf-help often utilizes publicly available information or support groups, on the Internet as well as in person, where people in similar situations join together. From early examples in self-driven legal practice and home-spun advice, the connotations of the word have spread and often apply particularly to education, business, psychology and psychotherapy, commonly distributed through the popular genre of self-help books. According to the \"APA Dictionary of Psychology\", potential benefits of self-help groups that professionals may not be able to provide include friendship, emotional support, experiential knowledge, identity, meaningful roles, and a sense of belonging.\n\nGroups associated with health conditions may consist of patients and caregivers. As well as featuring long-time members sharing experiences, these health groups can become support groups and clearing-houses for educational material. Those who help themselves by learning and identifying about health problems can be said to exemplify self-help, while self-help groups can be seen more as peer-to-peer support.\n\nWithin classical antiquity, Hesiod's \"Works and Days\" \"opens with moral remonstrances, hammered home in every way that Hesiod can think of.\" The Stoics offered ethical advice \"on the notion of \"eudaimonia\"—of well-being, welfare, flourishing.\" The genre of mirror-of-princes writings, which has a long history in Greco-Roman and Western Renaissance literature, represents a secular cognate of Biblical wisdom-literature. Proverbs from many periods, collected and uncollected, embody traditional moral and practical advice of diverse cultures.\n\nThe hyphenated compound word \"self-help\" often appeared in the 1800s in a legal context, referring to the doctrine that a party in a dispute has the right to use lawful means on their own initiative to remedy a wrong.\n\nFor some, George Combe's \"Constitution\" [1828], in the way that it advocated personal responsibility and the possibility of naturally sanctioned self-improvement through education or proper self-control, largely inaugurated the self-help movement;\" In 1841, an essay by Ralph Waldo Emerson, entitled Compensation, was published suggesting \"every man in his lifetime needs to thank his faults\" and \"acquire habits of \"self-help\"\" as \"our strength grows out of our weakness.\" Samuel Smiles (1812–1904) published the first self-consciously personal-development \"self-help\" book—entitled \"Self-Help\"—in 1859. Its opening sentence: \"Heaven helps those who help themselves\", provides a variation of \"God helps them that help themselves\", the oft-quoted maxim that had also appeared previously in Benjamin Franklin's \"Poor Richard's Almanac\" (1733–1758). In the 20th century, \"Carnegie's remarkable success as a self-help author\" further developed the genre with \"How to Win Friends and Influence People\" in 1936. Having failed in several careers, Carnegie became fascinated with success and its link to self-confidence, and his books have since sold over 50 million copies. Earlier, in 1902, James Allen published \"As a Man Thinketh\", which proceeds from the conviction that \"a man is literally what he thinks, his character being the complete sum of all his thoughts.\" Noble thoughts, the book maintains, make for a noble person, whilst lowly thoughts make for a miserable person; and Napoleon Hill's \"Think and Grow Rich\" (1937) described the use of repeated positive thoughts to attract happiness and wealth by tapping into an \"Infinite Intelligence\".\n\nIn the final third of the 20th century, \"the tremendous growth in self-help publishing...in self-improvement culture\" really took off—something which must be linked to postmodernism itself—to the way \"postmodern subjectivity constructs self-reflexive subjects-in-process.\" Arguably at least, \"in the literatures of self-improvement...that crisis of subjecthood is not articulated but enacted—demonstrated in ever-expanding self-help book sales.\"\n\nThe conservative turn of the neoliberal decades also meant a decline in traditional political activism, and increasing \"social isolation; Twelve-Step recovery groups were one context in which individuals sought a sense of community...yet another symptom of the psychologizing of the personal\" to more radical critics. Indeed, \"some social theorist have argued that the late-20th century preoccupation with the self serves as a tool of social control: soothing political unrest...[for] one's own pursuit of self-invention.\"'\n\nWithin the context of the market, group and corporate attempts to aid the \"seeker\" have moved into the \"self-help\" marketplace, with Large Group Awareness Trainings, LGATs\nand psychotherapy systems represented. These offer more-or-less prepackaged solutions to instruct people seeking their own individual betterment, just as \"the literature of self-improvement directs the reader to familiar frameworks...what the French \"fin de siècle\" social theorist Gabriel Tarde called 'the grooves of borrowed thought'.\"\n\nA subgenre of self-help book series also exists: such as the \"for Dummies\" guidesand \"The Complete Idiot's Guide to...\"—compare how-to books.\n\nAt the start of the 21st century, \"the self-improvement industry, inclusive of books, seminars, audio and video products, and personal coaching, [was] said to constitute a 2.48-billion dollars-a-year industry\" in the United States alone. By 2006, research firm Marketdata estimated the \"self-improvement\" market in the U.S. as worth more than $9 billion—including infomercials, mail-order catalogs, holistic institutes, books, audio cassettes, motivation-speaker seminars, the personal coaching market, weight-loss and stress-management programs. Marketdata projected that the total market size would grow to over $11 billion by 2008. \nIn 2012 Laura Vanderkam wrote of a turnover of 12 billion dollars.\nIn 2013 Kathryn Schulz examined \"an $11 billion industry\".\n\nSelf-help and mutual-help are very different from—though they may complement—service delivery by professionals: note for example the interface between local self-help and International Aid's service delivery model.\n\nConflicts can and do arise on that interface, however, with some professionals considering that \"the twelve-step approach encourages a kind of contemporary version of 19th-century amateurism or enthusiasm in which self-examination and very general social observations are enough to draw rather large conclusions.\"\n\nThe rise of self-help culture has inevitably led to boundary disputes with other approaches and disciplines. Some would object to their classification as \"self-help\" literature, as with \"Deborah Tannen's denial of the self-help role of her books\" so as to maintain her academic credibility, aware of the danger that \"writing a book that becomes a popular success...all but ensures that one's work will lose its long-term legitimacy.\"\n\nPlacebo effects can never be wholly discounted. Thus careful studies of \"the power of subliminal self-help tapes...showed that their content had no real effect...But that's not what the participants thought.\" \"If they thought they'd listened to a self-esteem tape (even though half the labels were wrong), they felt that their self-esteem had gone up. No wonder people keep buying subliminal tape: even though the tapes don't work, people think they do.\" One might then see much of the self-help industry as part of the \"skin trades. People need haircuts, massage, dentistry, wigs and glasses, sociology and surgery, as well as love and advice.\"—a skin trade, \"not a profession and a science\" Its practitioners would thus be functioning as \"part of the personal service industry rather than as mental health professionals.\" While \"there is no proof that twelve-step programs 'are superior to any other intervention in reducing alcohol dependence or alcohol-related problems',\" at the same time it is clear that \"there is something about 'groupishness' itself which is curative.\" Thus for example \"smoking increases mortality risk by a factor of just 1.6, while social isolation does so by a factor of 2.0...suggest[ing] an added value to self-help groups such as Alcoholics Anonymous as surrogate communities.\"\n\nSome psychologists advocate a positive psychology, and explicitly embrace an empirical self-help philosophy; \"the role of positive psychology is to become a bridge between the ivory tower and the main street—between the rigor of academe and the fun of the self-help movement.\" They aim to refine the self-improvement field by way of an intentional increase in scientifically sound research and well-engineered models. The division of focus and methodologies has produced several subfields, in particular: general positive psychology, focusing primarily on the study of psychological phenomenon and effects; and personal effectiveness, focusing primarily on analysis, design and implementation of qualitative personal growth. This includes the intentional training of new patterns of thought and feeling. As business strategy communicator Don Tapscott puts it, \"The design industry is something done to us. I'm proposing we each become designers. But I suppose 'I love the way she thinks' could take on new meaning.\"\n\nBoth self-talk, the propensity to engage in verbal or mental self-directed conversation and thought, and social support can be used as instruments of self-improvement, often by empowering, action-promoting messages. Psychologists have designed series of experiments that are intended to shed light into how self-talk can result in self-improvement. In general, research has shown that people prefer to use second person pronouns over first person pronouns when engaging in self-talk to achieve goals, regulate one’s own behavior, thoughts, or emotions, and facilitate performance. If self-talk has the expected effect, then writing about personal problems using language from their friends’ perspective should result in greater amount of motivational and emotional benefits comparing to using language from their own perspective. When you need to finish a difficult task and you are not willing to do something to finish this task, trying to write a few sentence or goals imaging what your friends have told you gives you more motivational resources comparing to you write to yourself. Research done by Ireland and others have revealed that, as expected, when people are writing using many physical and mental words or even typing a standard prompt with these kinds of words, adopting a friend’s perspective while freely writing about a personal challenge can help increase people’s intention to improve self-control by promoting the positivity of emotions such as pride and satisfaction, which can motivate people to reach their goal.\n\nThe use of self-talk goes beyond the scope of self-improvement for performing certain activities, self-talk as a linguistic form of self-help also plays a very important role in regulating people’s emotions under social stress. First of all, people using non-first-person language tend to exhibit higher level of visual self-distancing during the process of introspection, indicating that using non-first-person pronouns and one’s own name may result in enhanced self-distancing. More importantly, this specific form of self-help also has been found can enhance people’s ability to regulate their thoughts, feelings, and behavior under social stress, which would lead them to appraise social-anxiety-provoking events in more challenging and less threatening terms. Additionally, these self-help behaviors also demonstrate noticeable self-regulatory effects through the process of social interactions, regardless of their dispositional vulnerability to social anxiety.\n\nScholars have targeted self-help claims as misleading and incorrect. In 2005 Steve Salerno portrayed the American self-help movement—he uses the acronym \"SHAM: the Self-Help and Actualization Movement\"—not only as ineffective in achieving its goals, but also as socially harmful. \"Salerno says that 80 percent of self-help and motivational customers are repeat customers and they keep coming back 'whether the program worked for them or not'.\" Others similarly point out that with self-help books \"supply increases the demand... The more people read them, the more they think they need them... more like an addiction than an alliance.\"\n\nSelf-help writers have been described as working \"in the area of the ideological, the imagined, the narrativized... although a veneer of scientism permeates the[ir] work, there is also an underlying armature of moralizing.\"\n\nChristopher Buckley in his book \"God is My Broker\" asserts: \"The only way to get rich from a self-help book is to write one\".\n\nIn 1987 Gerald M. Rosen reported that people do not gain as much from reading self-help material as people would from the same material received in therapy. In general, he was critical of proliferation of self-help books.\n\nKathryn Schulz suggests that \"the underlying theory of the self-help industry is contradicted by the self-help industry’s existence\".\n\nThe self-help world has become the target of parodies. Walker Percy's odd genre-busting \"Lost in the Cosmos\" has been described as \"a parody of self-help books, a philosophy textbook, and a collection of short stories, quizzes, diagrams, thought experiments, mathematical formulas, made-up dialogue\". In their 2006 book \"Secrets of The Superoptimist\", authors W.R. Morton and Nathanel Whitten revealed the concept of \"superoptimism\" as a humorous antidote to the overblown self-help book category. In his comedy special \"Complaints and Grievances\" (2001), George Carlin observes that there is \"no such thing\" as self-help: anyone looking for help from someone else does not technically get \"self\" help; and one who accomplishes something without help, did not need help to begin with. In Margaret Atwood's semi-satiric dystopia \"Oryx and Crake\", university literary studies have declined to the point that the protagonist, Snowman, is instructed to write his thesis on self-help books as literature; more revealing of the authors and of the society that produced them than genuinely helpful.\n\n"}
{"id": "54299694", "url": "https://en.wikipedia.org/wiki?curid=54299694", "title": "Shooting bias", "text": "Shooting bias\n\nThe term shooting bias, also known as \"shooter bias\", is a form of implicit racial bias which refers to the tendency among the police to shoot black civilians rather than white civilians, even when they are unarmed.\n\nThe probability of being shot by the police depends on factors such as ethnicity, location, the income of the neighborhood and whether or not the person is carrying a weapon as well as the emotions shown by the victim.\n\nPolice data could be biased due to police reporting practices. Departments can voluntary include justifiable homicides in the crime statistics of the FBI’s Uniform Crime Reports, which means that a lot of departments don't provide data at all. Some cities haven't reported their data in years. This means that the official data doesn't accurately reflect the number of civilians that are shot by the police.\n\nNewspapers like \"The Guardian\" and \"The Washington Post\" have started gathering a database of fatal police shootings, revealing that in 2015 twice as many civilians had been fatally shot than the FBI's data suggested. An FBI working group has started working on a proposal for making the reports more accurate, but they would still rely on voluntary data and therefore wouldn't fix the main reporting issue.\n\nThe database developed by \"The Guardian\" is currently the largest database on fatal shootings available. They gather data through police reports, monitoring of regional news, fact-checked witness statements and other crowdsourced police fatality databases.\n\nIn 2016, \"The Guardian\" counted 1093 people who were killed by the police in the United States. Out of these 574 were white and 266 were black. 95 of the white victims were unarmed, 42 of the black victims were unarmed.\n\nMore white than black people are shot. It is important to distinguish to differentiate between the number of deaths of an ethnic group and the likelihood of being shot by police. The likelihood of being shot as a black rather than a white person is higher, whether the victim is armed or not.\n\nProminent examples of unarmed black civilians being fatally shot by the police include: \n\n\nA study carried out at the University of California found \"evidence of a significant bias in the killing of unarmed black Americans compared to unarmed white Americans\". In this study, the probability of being shot by the police as a black, unarmed person versus as a white, unarmed person was 3.49 times higher. Unarmed Hispanics' likelihood to be shot was 1.67 times higher than for unarmed Whites.\n\nThe number vary greatly depending on the county, sometimes reaching a probability of 20 to 1 or more for unarmed blacks to be shot. There are several maps that showcase the distribution of fatal shootings across the country.\n\nContrary to popular belief, crime rate was not associated with likelihood of being shot in this study.\n\nEmpirical research suggests the following factors to influence the decision to shoot: \n\nAn implicit racial bias refers to unintentional judgments a person makes of a group (e.g. good/bad) of a certain ethnicity. So a person who shows implicit racial bias might not be aware of it. Police officers have been found to show a racial bias against black people in the decision to shoot.\n\nIn one of the studies, researchers investigated how stereotypes affected police officers' decisions to shoot. They used a video game and exposed their participants to pictures of either Whites or Non-Whites who were armed or unarmed. During this video game, the participants were asked to choose between \"shoot\" or \"don't shoot\" as quickly as possible. The results were that the participants shot armed black people faster than armed white people and chose \"don't shoot\" faster for unarmed white than unarmed black persons.\n\nThe authors explained those findings by the activation of stereotype thinking which lead white people to associate black people with danger. Because of this stereotype, the participants expected blacks to carry a gun and therefore were quicker to make the \"shooting\" decision. Other studies have found similar results. The time pressure to make a decision in decisions to shoot might magnify the effects of racial bias.\n\nThe American police officers live and work in a society full of prejudices against minorities. In addition, police officers usually deal with high crime rates in minority neighborhoods. These experiences reinforce their existing prejudices by ignoring that most people with a non-white ethnic background don’t become criminals. This leads to discrimination against minorities. Other factors that lead to discrimination by the police are institutionalized language barriers between police and some ethnic groups, experiences with disrespectful or hostile residents in certain minority neighborhoods and low punishment for police officers who misbehave towards minorities.\n\nThose prejudices by the police are of course being noticed by the groups of minorities. So in return, the groups believe that the officers have prejudices against them. This might lead to the findings that Non-Whites are more likely to behave disrespectfully or to insult officers.\n\nTo sum up, there may exist a mutual influence between the prejudices that police officers have against minorities and the prejudices that minorities have against officers.\n\n\nPossibly the biggest change could be made by holding police officers accountable for their actions. Internal investigations usually don't lead to punishments. The work culture within the police is highly racist, as many recent scandals have shown.\n\nKnowing the factors that influence police officers decisions to shoot there are a few possible solutions for reducing shooting bias. The main factor is implicit racial bias, which in turn is exuberated by certain factors that could be addressed.\n\nDiversity in police departments might not reduce shooting bias, but reducing fatigue might lower the impact of racial bias on the decision to shoot. Changing the training of police officers so as to not showcase black armed targets more often than white ones could help reduce racial bias.\n\nTraining police officers in making decisions under stress as well as assigning officers to certain locations so they can become familiar with its residents could reduce the numbers of civilians killed, says Richmond's police chief, Chris Magnus.\n\nRacial bias can be contagious within a social group or neighborhood. Training police officers to be aware of this might help in reducing this effect.\n"}
{"id": "4082701", "url": "https://en.wikipedia.org/wiki?curid=4082701", "title": "Shooting reconstruction", "text": "Shooting reconstruction\n\nShooting incident reconstruction is the examination of the physical evidence recovered or documented at the scene of a shooting. Shooting reconstruction may also include the laboratory analysis of the evidence recovered at the scene. The goal is an attempt to gain an understanding of what may or may not have happened during the incident. Once all reasonable explanations have been considered, one can evaluate the significance of witness or suspect accounts of the incident.\n\nIn many cases valuable evidence necessary for reconstruction analysis exists at the crime scene. Should this evidence go undocumented or unrecovered during the initial processing of the shooting scene, the information it can give investigators may be lost forever. Poor shooting incident processing can not be compensated for by excellent laboratory work.\n\nThere are many questions that can be answered from the proper reconstruction of a shooting incident. Some of the questions typically answered by a shooting reconstruction investigation include (but not limited to) the distance of the shooter from the target, The path of the bullet(s), The number of shots fired and possibly the sequence of multiple discharges at a shooting incident.\n\nThe Association of Firearm and Tool Mark Examiners is an international non-profit organization dedicated to the advancement of firearm and tool mark identification, including shooting reconstruction.\n\n\n"}
{"id": "747210", "url": "https://en.wikipedia.org/wiki?curid=747210", "title": "Snob", "text": "Snob\n\nSnob is a pejorative term for a person that believes there is a correlation between social status and human worth. \"Snob\" also refers to a person that feels superiority over those from lower social classes, education levels, or other social areas. The word \"snobbery\" came into use for the first time in England during the 1820s.\n\nA snob is also a tool (an anvil) used by cobblers in the manufacture of footwear.\n\nSnobs can through time be found ingratiating themselves with a range of prominent groups – soldiers (Sparta, 400 BC), bishops (Rome, 1500), poets (Weimar, 1815), farmers (China, 1967) – for the primary interests of snobs is distinction, and as its definition changes, so, naturally and immediately, will the objects of the snob's admiration.\n\nSnobbery existed also in mediaeval feudal aristocratic Europe, when the clothing, manners, language and tastes of every class were strictly codified by customs or law. Chaucer, a poet moving in the court circles, noted the provincial French spoken by the Prioress among the Canterbury pilgrims:\n\nAnd French she spoke full fair and fetisly<br>\nAfter the school of Stratford atte Bowe,<br>\nFor French of Paris was to her unknowe.\nWilliam Rothwell notes \"the simplistic contrast between the 'pure' French of Paris and her 'defective' French of Stratford atte Bowe that would invite disparagement\".\n\nSnobbery surfaced more strongly as the structure of the society changed, and the bourgeoisie had the possibility to \"imitate\" aristocracy. Snobbery appears when elements of culture are perceived as belonging to an aristocracy or elite, and some people (the snobs) feel that the mere adoption of the fashion and tastes of the elite or aristocracy is sufficient to include someone in the elites, upper classes or aristocracy.\n\nHowever, a form of snobbery can be adopted by someone not a part of that group; a pseudo-intellectual, a celebrity worshipper, and a poor person idolizing money and the rich are types of snobs who do not base their snobbery on their personal attributes. Such a snob idolizes and imitates, if possible, the manners, worldview, and lifestyle of a classification of people to which they aspire, but do not belong, and to which they may never belong (wealthy, famous, intellectual, beautiful, etc.).\n\nThe term \"snob\" is often misused when describing a \"gold-tap owner\", i.e. a person who insists on displaying (sometimes non-existent) wealth through conspicuous consumption of luxury goods such as clothes, jewelry, cars etc. Displaying awards or talents in a rude manner, boasting, is a form of snobbery. A popular example of a \"snob victim\" is the television character Hyacinth Bucket of the BBC comedy series \"Keeping Up Appearances\".\n\nWilliam Hazlitt observed, in a culture where deference to class was accepted as a positive and unifying principle, \"Fashion is gentility running away from vulgarity, and afraid of being overtaken by it,\" adding subversively, \"It is a sign the two things are not very far apart.\" The English novelist Bulwer-Lytton remarked in passing, \"Ideas travel upwards, \"manners\" downwards.\" It was not the deeply ingrained and fundamentally accepted idea of \"one's betters\" that has marked snobbery in traditional European and American culture, but \"aping one's betters\".\n\nSnobbery is a defensive expression of social insecurity, flourishing most where an Establishment has become less than secure in the exercise of its traditional prerogatives, and thus it was more an organizing principle for Thackeray's glimpses of British society in the threatening atmosphere of the 1840s than it was of Hazlitt, writing in the comparative social stability of the 1820s.\n\n\n"}
{"id": "1786948", "url": "https://en.wikipedia.org/wiki?curid=1786948", "title": "Tantrum", "text": "Tantrum\n\nA tantrum, temper tantrum, meltdown or hissy fit is an emotional outburst, usually associated with children or those in emotional distress, that is typically characterized by stubbornness, crying, screaming, violence, defiance, angry ranting, a resistance to attempts at pacification, and, in some cases, hitting, and other physically violent behavior. Physical control may be lost; the person may be unable to remain still; and even if the \"goal\" of the person is met, he or she may not be calmed.\nA tantrum may be expressed in a tirade: a protracted, angry speech.\n\nTantrums are one of the most common forms of problematic behavior in young children, but tend to decrease in frequency and intensity as the child grows older. For the toddler, tantrums can be considered as normal, even as gauges of a developing strength of character.While tantrums are sometimes seen as a predictor of future anti-social behaviour, in another sense they are simply an age-appropriate sign of excessive frustration, and will diminish over time given a calm and consistent handling. Parental containment where a child cannot contain itself—rather than what the child is ostensibly demanding—may be what is really required.\n\nSelma Fraiberg warned against \"too much pressure or forceful methods of control from the outside\" in child-rearing: \"if we turn every instance of pants changing, treasure hunting, napping, puddle wading and garbage distribution into a governmental crisis we can easily bring on fierce defiance, tantrums, and all the fireworks of revolt in the nursery\".\n\nSome people who have neurological disorders such as autism, ADHD, and intellectual disability could be more vulnerable to tantrums than others, although anyone experiencing brain damage (temporary or permanent) can suffer from tantrums. Anyone may be prone to tantrums once in a while, regardless of gender or age. However, a meltdown due to sensory overload (which even neurotypical children can experience) is not the same as a temper tantrum.\n\nFreud considered that the Wolf Man's development of temper tantrums was connected with his seduction by his sister: he became \"discontented, irritable and violent, took offence on every possible occasion, and then flew into a rage and screamed like a savage\". Freud linked the tantrums to an unconscious need for punishment driven by feelings of guilt—something which he thought could be generalised to many other cases of childhood tantrums.\n\nHeinz Kohut contended that tantrums were narcissistic rages, caused by the thwarting of the infant's grandiose-exhibitionist core. The blow to the inflated self-image, when a child's wishes are (however justifiably) refused, creates fury because it strikes at the feeling of omnipotence.\n\nJealousy over the birth of a sibling, and resulting aggression, may also provoke negativistic tantrums, as the effort at controlling the feelings overloads the child's system of self-regulation.\n\nThackeray claimed that in later life \"you may tell a Tantrum as far as you can see one, by the distressed and dissatisfied expression of its countenance—'Tantrumical', if we may term it so\".\n\nHeinz Kohut contended that \"the infant's core is likely to contain a self-centred, grandiose-exhibitionist part\", and that \"tantrums at being frustrated thus represent narcissistic rages\" at the blow to the inflated self-image. With \"a child confronted with some refusal ... regardless of its justifications, the refusal automatically provokes fury, since it offends his sense of omnipotence\".\n\nThe willingness of the celebrity to throw tantrums whenever thwarted to the least degree is a kind of Acquired Situational Narcissism or tantrumical behavior.\n\nIf tantrums are shown by older people they might often be signs of immaturity and a mental disability; however, many people can have them under extreme stress.\n\n"}
{"id": "2671314", "url": "https://en.wikipedia.org/wiki?curid=2671314", "title": "Uniclass", "text": "Uniclass\n\nUniclass 2015 is a unified classification system for all sectors of the UK construction industry. It contains consistent tables classifying items of all scales, from facilities such as a railway to products like anchor plates, flue liners or LED lamps. \n\nOriginally released in 1997, Uniclass allows project information to be structured to a recognised standard. This original version has now been heavily revised, to make it more suitable for use with modern construction industry practice, and to make it compatible with BIM (Building information modeling) now and in the future. \n\nLed by the National Building Specification (NBS), experts from across the industry have developed the new system, known as Uniclass 2015. This significantly extends the scope of the previous version, and responds to industry feedback on the draft tables known as Uniclass 2, published by CPI in 2013.\n\nUniclass 2015 provides:\n\n\nUniclass 2015 has been restructured and redeveloped to provide a comprehensive system suitable for use by the entire industry, including the infrastructure, landscape, and engineering services as well as the building sector, and for all stages in a project life cycle.\n\nUniclass 2015 provides a means of structuring project information essential for the adoption of BIM Level 2. Information about a project can be generated, used and retrieved throughout the life cycle.\n\nThe initial classification work has focussed on the seven core tables that describe an asset required to support the Digital Plan of Work; additional tables covering Form of Information, Project Management and Construction Aids are also under development.\n\nUniclass 2015 has been carefully structured to be in accordance with ISO 12006-2 \"Building construction – Organization of information about construction works – Part 2: Framework for classification\". This means that Uniclass 2015 is particularly suited to use in an international context, as mapping to other similarly compliant schemes around the world is streamlined.\n\nUniclass 2015 is divided into a set of tables, each accommodating a different ‘class’ of information. These can be used to categorise information for costing, briefing, CAD layering, etc. as well as when preparing specifications or other production documents.\n\nThese tables are also suitable for buildings and other assets in use, and maintaining asset management and facilities management information.\n\nThe suite of tables are broadly hierarchical, and allow information about a project to be defined from the broadest view of it to the most detailed. For detailed design and construction, the main starting point are Entities, which are composed of Elements; Elements are made up of Systems which in turn contain Products. \n\nEntities can also be described using the Spaces and Activities tables if required, and at the more general level the Complexes table contains terms that can be thought of as groupings of Entities, Activities and Spaces. \n\nLooked at more closely, the tables comprise:\n\n\nThe tables need to be flexible and to be able to accommodate sufficient codings to ensure coverage, to allow for a multitude of items and circumstances, including new technologies and developments that are yet to emerge.\n\nEach code consists of either four or five pairs of characters. The initial pair identifies which table is being used and employs letters. The four following pairs represent groups, sub-groups, sections and objects. By selecting pairs of numbers, up to 99 items can be included in each group of codes, allowing plenty of scope for inclusion.\n\nFor example, Systems are arranged in groups with subgroups which are sub divided, which leads to the final object code. For instance: \n\n\nor\n\n\n"}
{"id": "27950254", "url": "https://en.wikipedia.org/wiki?curid=27950254", "title": "Voluntariness", "text": "Voluntariness\n\nIn law and philosophy, voluntariness is a choice being made of a person's free will, as opposed to being made as the result of coercion or duress. Philosophies such as libertarianism and voluntaryism, as well as many legal systems, hold that a contract must be voluntarily agreed to by a party in order to be binding on that party. The social contract rests on the concept of the voluntary consent of the governed.\n\nThe Federal Rules of Criminal Procedure provide that \"Before accepting a plea of guilty or nolo contendere, the court must address the defendant personally in open court and determine that the plea is voluntary and did not result from force, threats, or promises (other than promises in a plea agreement).\" The actual voluntariness is suspect, in that it is common for prosecutors to threaten to seek more prison time unless the defendant agrees to plead guilty. For this reason, common law courts historically took a negative view of guilty pleas.\n"}
{"id": "1062391", "url": "https://en.wikipedia.org/wiki?curid=1062391", "title": "ZKM Center for Art and Media Karlsruhe", "text": "ZKM Center for Art and Media Karlsruhe\n\nFounded in 1989, the ZKM | Center for Art and Media Karlsruhe is a cultural institution which, since 1997, has been located in a historical industrial building in Karlsruhe, Germany that formerly housed a munitions factory. The ZKM organizes special exhibitions and thematic events, carries out research projects, produces works in the field of new media and offers public as well as individualized communications and educational programs.\n\nThe ZKM houses under one roof two museums, three research institutes as well as a media center; in this way it groups research and production, exhibitions and events, archives and collections. It works on the interface of art and science, and takes up cutting-edge insights in media technologies with the objective of developing them further. Since the death of founding director Heinrich Klotz (1935-1999), the ZKM has been directed by Prof. Peter Weibel. In addition to the ZKM, the associated Karlsruhe University of Arts and Design, as well as the Städtische Galerie Karlsruhe [Municipal Gallery Karlsruhe] are likewise housed in the former munitions factory.\n\nThe founding of the Center for Art and Media goes back to the early 1980s. In the context of an ever-expanding media landscape, and in conjunction with a transformation of the art world, representatives from local government, the University of Karlsruhe, the University of Music Karlsruhe, the Kernforschungszentrum Karlsruhe [Center for Nuclear Research Karlsruhe] and other institutes, as well as organizations and representatives of the Karlsruhe art scene formed the “Projektgruppe ZKM” [ZKM Project Group] in 1986. In February 1988 the Projektgruppe ZKM presented the content of their work as “Konzept 88” [Concept ‘88], in which the initiative for the fusion of the arts and the new media was outlined in both theory and practice.\n\nWith the founding of a board of trustees in 1989, and the appointment of Heinrich Klotz as founding director, the realization of the ZKM became concrete. Three dates mark the ZKM’s foundation: the resolution by the local council dated May 9, 1989, the decision by the Council of Ministers of the State of Baden-Württemberg dated June 3, 1989, and the constitution of the board of trustees with effect from August 12, 1989. When initially founded, the ZKM was located in various buildings around the city. Prior to the move to its present location, the media art festival MultiMediale (MultiMediale 1-5, 1989-1997) took place at various sites.\n\nFor some considerable time, an area to the south of the Karlsruhe Central Station had been designated. To this end, an international architect’s competition for the new building was announced on March 1989, from which the visionary design by Dutch architect Rem Koolhaas was to result. However, the construction of the so-called Koolhaas-Cube was abandoned in 1992 for reasons of costs and space in favor of the conversion of the disused factory building.\nKarlsruhe opted for the conversion of the so-called “Hallenbau A” [Hall A], an industrial ruin erected between 1914 and 1918 by architect Philipp Jakob Manz as a weapons and munitions factory. The building, divided into ten atria and with a length of 312 meters had been built on the former factory site of the Industriewerke Karlsruhe-Augsburg (IWKA), an industrial wasteland since the 1970s to the south-west that separated the city center from the surrounding urban areas. The conversion, based on plans drafted by the Hamburg office of Schweger, as well as the extension of the Media Cube which takes account of the Koolhaas design, started in 1993. With the move to Hallenbau A in 1997, the ZKM disposed over a media theater, over concert and events spaces, a media center, studios and institutes for research and production as well as a media museum. In a second stage of construction, the spaces for the Museum of Contemporary Art (move in 1999) and the Karlsruhe University of Arts and Design (move in 2001) were finalized. From 2004 to 2005 the Museum of Contemporary Art was integrated into the ZKM.\n\n“The task envisaged for the ZKM is the sounding out of the creative possibilities between the traditional arts and media technologies for the purpose of achieving innovative results. The objective is the enrichment of the arts, not their technical amputation. For this reason both traditional and media arts must compete with one another. At the ZKM either aspect – each for itself and with one another – are given a voice. The Bauhaus, founded in Weimar in 1919, may serve as a model.” (Heinrich Klotz)\n\nThe basic idea as formulated by founding director Heinrich Klotz in 1992, was implemented and developed further in the years that followed. Today, the ZKM is chiefly characterized by four guiding ideas.\n\n\nThe ZKM | Karlsruhe is home to two museums, three research institutions, a media center, a laboratory as well as a number of events spaces:\nMuseums\n\nInstitutes for Research and Production \nEvents Spaces\n\nThe two museums are open to the public as is the library together with media lounge. Furthermore, the Info Point, the Museum Shop and the Café/Bistro \"mint\" are located in the Foyer and accessible to visitors.\n\nIn the exhibitions and events, the ZKM shows approaches and themes of contemporary art, but also presents practically forgotten artists and art movements, as well as works of art in various media and genre – from App through to oil paintings.\n\nThe central concerns of the Media Museum turn on the history and critique of the new media, which have been transforming the forms of everyday life for the past 50 years. Computer, telephone and Internet intervene in social and individual lives in that technical components become increasingly important. A further focus of the Media Museum is the interaction between observer and work of art: only through the actions and reactions of every single visitor do works actually emerge – the visitor itself becomes an element in the installation and, in this way, is able to explore the treatment with new technologies. With works of media art and interactive installations artists and scientists question media-technological developments and visions. Temporary exhibitions, such as “net_condition. Art/Politics in the Online Universe” (September 1999 to February 2000), “Iconoclash. Beyond Image Wars in Science, Religion and Art” (from May to September, 2002) or “bit international” (February 2008 to January 2009) were highly respected both nationally and abroad.\n\n Since December 1999, the Museum of Contemporary Art has been located in the atria 1 and 2 of the former munitions factory. Across 7.000 sqm exhibition space, the museum displays works from private collections, namely the FER COLLECTION, the Sammlung Grässlin [Grässlin Collection], the Sammlung Siegfried Weishaupt [Weishaupt Collection], the collection of the Landesbank Baden-Württemberg, VAF-Stiftung / Museo di arte moderna e contemporanea di Trento e Rovereto [VAF Foundation / Museum of Modern and Contemporary Art of Trento and Rovereto] (MART), as well as the Boros Collection, together with exponents drawn from the ZKM Collection and further collaborating collections. Temporary exhibitions, above all from the second half of the twentieth centuries through to contemporary approaches in contemporary art are presented. Among others monographic exhibitions, on Bruce Nauman, Bill Viola, Sigmar Polke, Franz West, Sylvie Fleury, Martin Kippenberger and Tobias Rehberger have been taking place since 1999. Special, thematic exhibitions were, inter alia “Making Things Public. Atmospheres of Democracy” (March to October, 2005), “Light Art from Artificial Light” (November 2005 to August 2006), “Medium Religion” (November 2008 till April 2009) or “The Global Contemporary. Art Worlds after 1989” (September 2011 to February 2012). Furthermore, smaller scale exhibitions are held in the museum project spaces.\n\nAlongside exhibitions, events as platforms for the exchange with visitors and actors from various spheres of social life such as politics, the economy or philosophy take place. The form and content of events vary: from opera with multimedial stages to scientific symposia and popular concerts through to performances, dance or film-screenings. Here, the ZKM functions both as an organizer and cooperation partner, but also leases out its facilities. The events take place in various spaces, among others, the Lecture Hall, the Media Theater, the Foyer and the ZKM_Cube.\n\nThe research institutes of the ZKM facilitate the development of trans-disciplinary projects. The research performance is, in part, carried out independently at the ZKM, but mostly as part of collaborations with further educational and research institutions. Their objective is to analyze and investigate the most recent image, music and communications technologies with respect to their applicability and relevance for art and an increasingly globally networked scientific (online) community.\n\nFounded in 1991 the Institute for Visual Media creatively and critically investigates the permanently transforming media culture. Unique artistic developments, the cooperation with internal guest artists (William Forsythe, Bill Viola et al.) as well as cooperation with culture and research institutes, comprise some of the elements of the institute's work. The work results which follow from this are presented in the context of exhibitions and conferences. The Institute's production spectrum ranges from digital video and 3D animation to interactive installations and environments, from software systems through to the real-time generation of natural and architectonic environments and audio-visual applications for performance contexts. Until 2011, one of the institute's focal points comprised the sphere of immersive projection environments (e.g. PanoramaScreen), in the context of which hardware and software solutions for artistic projects were developed. Since then, emphasis has been placed on, among others, the development of augmented reality productions.\n\nThe Institute for Music and Acoustics carries out research and development in the field of electro-acoustic and experimental music, as well as in the spheres of digital sound systems and algorithmic composition. It also supports guest artists and scientists in productions, organizes contemporary concerts and initiates symposia and festivals. Thus, the European meeting of the electronic studios, called “next_generation”, takes place biannually and the concert festival Quantensprünge [Quantum Leaps] on a six-monthly basis. In addition to this, the institute awards the Giga-Hertz Prize that is the most generously endowed award for electronic music, as well as the Walter-Fink Prize for Dance, Electronic Music and Media since 2009. Located in the interior of the “Blue Cube” there is a sound studio for artistic productions.\n\nThe ZKM Collection was founded by the ZKM's first director, Heinrich Klotz, at the beginning of the 1990s and has been augmented ever since. It is sub-divided into the collections: the Museum of Contemporary Art and the Media Museum collections, the Video and Audio collections as well as several archives. The collection is based on a specific approach to various artistic genre and media: whereas painting and sculpture were hermetically sealed off from new influences of video art and photography which were gradually establishing themselves, the ZKM's collecting activity is marked by the transcendence of such traditional genre borders.\nWhile the Museum of Contemporary Art collection embraced works of art from all genre from the outset, the Media Museum collection initially contained only works of interactive media most of which were produced in-house. Over 500 international guest artists produced a vast number of works, which then became part of the ZKM Collection after being initially presented in Karlsruhe. Consequently, the ZKM disposes over one of the largest collections of media art extending back to the beginning of video art, electronic installation and holography. To these also belong the collection of approx. 1.200 art videos and 13.800 audio tracks, which are not stored in the museum but are accessible in the ZKM | Media Library. They may also be found on Dieter Mankau's four listening chairs and, for the most part, can be researched via the Internet.\n\nThe Video Collection was constructed as the first of its kind in Germany and increased the public awareness of video as an independent art form. The collection encompasses works of video art ranging from the 1960s, 1970s and 1980s, among others, the video magazine “Infermental”. Through the Laboratory for Antiquated Video Systems, which is connected to the Media Library it has been possible to rescue comprehensive video collections in Europe and the US from decomposition, and make them accessible to the public.\n\nThe Audio Collection comprises the titles of contemporary music with an emphasis on electro-acoustic music. In addition to audio recordings, the collection includes scores, specialist publications, historical photography and posters. Especial importance is attributed to the International Digital Electro-acoustic Music Archive (IDEAMA), which includes pieces of electro-acoustic music from its inception through to the present.\n\nIn addition to works of video art and electronic music, the ZKM collects archives and documents on the electronic arts, such as on video art, electro-acoustic music, computer art and inter-medial forms. It provides researchers with insight and an understanding for the artistic developments of the foregoing 50 years.\n\nThe common ZKM Library shared with the Karlsruhe University of Arts and Design (HfG) comprises approx. 53.000 books, journals and digital storage media. Thematically, its inventory concentrates on twentieth and twenty-first century art and, above all, on media art, architecture, design, media theory, film, photography and electro-acoustic music. All the library's inventories may be researched on the Internet.\n\nIn cooperation with publishers, the ZKM publishes exhibition catalogs and specialist works on the monographic and thematic exhibitions.\n\nZKM Publications (a selection):\n\n\nPublications about the ZKM (a selection):\n\n\n"}
