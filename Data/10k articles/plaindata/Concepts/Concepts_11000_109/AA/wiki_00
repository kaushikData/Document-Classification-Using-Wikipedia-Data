{"id": "8077211", "url": "https://en.wikipedia.org/wiki?curid=8077211", "title": "Anantarika-karma", "text": "Anantarika-karma\n\nĀnantarika-karma or ānantarika-kamma is a heinous crime that through karmic process brings immediate disaster. They are called 'anantarika' because they are 'an' (without) 'antara' (interval), in other words the results immediately come to fruition in the next life, i.e. the participant goes straight to hell. These are considered so heinous that Buddhists and non-Buddhists must avoid them. According to Buddhism, committing such a crime would prevent the perpetrator from attaining the stages of sotāpanna, sakadagami, anāgāmi or arhat in that lifetime. The five crimes are:\n\n\nIn Mahayana Buddhism these five crimes are referred to as \"pañcānantarya\" and are mentioned in \"The Sutra Preached by the Buddha on the Total Extinction of the Dharma\".\n\nDevadatta is noted for attempting to kill the Sakyamuni Buddha on several occasions including:\n\nAccording to Sutta Pitaka, after trying to kill Sakyamuni Buddha a number of times, Devadatta set up his own Buddhist monastic order by splitting the (sangha). During his efforts to become the leader of his own Sangha, he proposed five extra-strict rules for monks, which he knew Buddha would not allow. Devadatta's reasoning was that after he had proposed those rules and Buddha had not allowed them, Devadatta could claim that \"he\" did follow and practice these five rules, making him a better and more pure monk. One of these five extra rules required monks to be vegetarian. In the Contemplation Sutra, Devadatta is said to have convinced Prince Ajatasattu to murder his father King Bimbisara and ascend the throne. Ajatasattu follows the advice, and this action prevents him from attaining enlightenment at a later time, when listening to a teaching of the Buddha. Devadatta is the only individual from the early Buddhist tradition to have committed three anantarika-karmas.\n\nKing Suppabuddha was the father of Devadatta and Yasodharā and the father-in-law of Prince Siddhattha. One day Suppabuddha blocked the Buddha's path, refused to make way, and sent a message saying, \"I cannot give way to the Buddha, who is so much younger than I\". Finding the road blocked, the Buddha and the bhikkhus turned back. As the Buddha turned back, he said to Ananda, \"Because the king has refused to give way to a Buddha, he has committed a bad kamma and before long he will have to face the consequences\". It is said that the king died on the seventh day after that event had taken place. He fell down the stairs, collapsed and died and was born in a suffering state, being unable to escape the effects of his evil kamma (according to Buddhist belief). According to the Buddha's prediction the earth swallowed him. It is said, \"So the king went down the stairs and as soon as he stepped on the earth, it opened and swallowed him up and dragged him right down to Avici Niraya.\"\n\nAnyone who commits an anantarika-karma will go to hell. The five different actions which each constitute an anantarika-karma are the only actions which can produce a definite result.\n\nAccounts claim that toward the end of Devadatta's life, he was struck by a severe remorse caused by his past misdeeds and did indeed manage to approach the Buddha and retook refuge in the Triple Gem, dying shortly afterwards. Because of the gravity of his actions, he was condemned to suffer for several hundred millennia in Avici. However, it was also said that he would eventually be admitted into the heavens as a Pratyekabuddha due to his past merits prior to his corruption.\n\nIn the Samaññaphala Sutta, Gautama Buddha said that if Ajatasattu hadn't killed his father, he would have attained sotapannahood, a degree of enlightenment. But because he had killed his father he could not attain it.\n\n\n"}
{"id": "20324399", "url": "https://en.wikipedia.org/wiki?curid=20324399", "title": "Artificial language", "text": "Artificial language\n\nArtificial languages are languages of a typically very limited size which emerge either in computer simulations between artificial agents, robot interactions or controlled psychological experiments with humans. They are different from both constructed languages and formal languages in that they have not been consciously devised by an individual or group but are the result of (distributed) conventionalisation processes, much like natural languages. Opposed to the idea of a central \"designer\", the field of artificial language evolution in which artificial languages are studied can be regarded as a sub-part of the more general cultural evolution studies.\n\nThe idea of creation of artificial language arose in 17th and 18th century as a result of gradually decreasing international role of Latin. The initial schemes ere mainly aimed at the development of a rational language free from inconsistence of living language and based on classification of concepts. The material of living languages also appears later.\n\nThe lack of empirical evidence in the field of evolutionary linguistics has led many researchers to adopt computer simulations as a means to investigate the ways in which artificial agents can self-organize languages with natural-like properties. This research is based on the hypothesis that natural language is a complex adaptive system that emerges through interactions between individuals and continues to evolve in order to remain adapted to the needs and capabilities of its users. By explicitly building all assumptions into computer simulations, this strand of research strives to experimentally investigate the dynamics underlying language change as well as questions regarding the origin of language under controlled conditions.\n\nDue to its success the paradigm has also been extended to investigate the emergence of new languages in psychological experiments with humans, leading up to the new paradigm of experimental semiotics.\n\nBecause the focus of the investigations lies on the conventionalisation dynamics and higher-level properties of the resulting languages rather than specific details of the conventions, artificially evolved languages are typically not documented or re-used outside the single experiment trial or simulation run in which they emerge. In fact, the limited size and short-lived nature of artificial languages are probably the only things that sets them apart from \"natural\" languages, since \"all\" languages are artificial insofar as they are conventional (see also Constructed language#Planned, constructed, artificial).\n\nArtificial languages have been used in research in developmental psycholinguistics. Because researchers have a great deal of control over artificial languages, they have used these languages in statistical language acquisition studies, in which it can be helpful to control the linguistic patterns heard by infants.\n\n"}
{"id": "23571212", "url": "https://en.wikipedia.org/wiki?curid=23571212", "title": "Campaign for Social Justice", "text": "Campaign for Social Justice\n\nThe Campaign for Social Justice (CSJ) was an organisation based in Northern Ireland which campaigned for civil rights in that region. \n\nThe CSJ was inaugurated on 17 January 1964 in Dungannon, County Tyrone, by Patricia McCluskey, who became its first chairwoman, and her husband, local general practitioner Dr Conn McCluskey. The couple had in 1963 established a Homeless Citizens' League to campaign against discrimination in the allocation of public housing. The CSJ was established, according to the founding statement, for \"the purpose of bringing the light of publicity to bear on the discrimination which exists in our community against the Catholic section of that community representing more than one-third of the total population\".\n"}
{"id": "218268", "url": "https://en.wikipedia.org/wiki?curid=218268", "title": "Characteristic polynomial", "text": "Characteristic polynomial\n\nIn linear algebra, the characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots. It has the determinant and the trace of the matrix as coefficients. The characteristic polynomial of an endomorphism of vector spaces of finite dimension is the characteristic polynomial of the matrix of the endomorphism over any base; it does not depend on the choice of a basis. The characteristic equation is the equation obtained by equating to zero the characteristic polynomial.\n\nThe characteristic polynomial of a graph is the characteristic polynomial of its adjacency matrix. It is a graph invariant, though it is not complete: the smallest pair of non-isomorphic graphs with the same characteristic polynomial have five nodes.\n\nGiven a square matrix \"A\", we want to find a polynomial whose zeros are the eigenvalues of \"A\". For a diagonal matrix \"A\", the characteristic polynomial is easy to define: if the diagonal entries are \"a\", \"a\", \"a\", etc. then the characteristic polynomial will be:\n\nThis works because the diagonal entries are also the eigenvalues of this matrix.\n\nFor a general matrix \"A\", one can proceed as follows. A scalar \"λ\" is an eigenvalue of \"A\" if and only if there is an eigenvector v ≠ 0 such that\n\nor \n\n(where I is the identity matrix). Since v is non-zero, this means that the matrix \"λ\" I − \"A\" is singular (non-invertible), which in turn means that its determinant is 0. Thus the roots of the function det(\"λ\" I − \"A\") are the eigenvalues of \"A\", and it is clear that this determinant is a polynomial in \"λ\".\n\nWe consider an \"n\"×\"n\" matrix \"A\". The characteristic polynomial of \"A\", denoted by \"p\"(\"t\"), is the polynomial defined by\nwhere \"I\" denotes the \"n\"-by-\"n\" identity matrix.\n\nSome authors define the characteristic polynomial to be det(\"A\" - \"t\" \"I\"). That polynomial differs from the one defined here by a sign (−1), so it makes no difference for properties like having as roots the eigenvalues of \"A\"; however the current definition always gives a monic polynomial, whereas the alternative definition is monic only when formula_5 is even.\n\nSuppose we want to compute the characteristic polynomial of the matrix\nWe now compute the determinant of\nwhich is formula_8 the characteristic polynomial of \"A\".\n\nAnother example uses hyperbolic functions of a hyperbolic angle φ.\nFor the matrix take\nIts characteristic polynomial is\n\nThe polynomial \"p\"(\"t\") is monic (its leading coefficient is 1) and its degree is \"n\". The most important fact about the characteristic polynomial was already mentioned in the motivational paragraph: the eigenvalues of \"A\" are precisely the roots of \"p\"(\"t\") (this also holds for the minimal polynomial of \"A\", but its degree may be less than \"n\"). The coefficients of the characteristic polynomial are all polynomial expressions in the entries of the matrix. In particular its constant coefficient \"p\" (0)  is det(−\"A\") = (−1) det(\"A\"), the coefficient of is one, and the coefficient of is tr(−\"A\") = −tr(\"A\"), where is the matrix trace of \"A\". (The signs given here correspond to the formal definition given in the previous section; for the alternative definition these would instead be det(\"A\") and (−1)tr(\"A\") respectively.)\n\nFor a 2×2 matrix \"A\", the characteristic polynomial is thus given by\n\nUsing the language of exterior algebra, one may compactly express the characteristic polynomial of an \"n\"×\"n\" matrix \"A\" as \nwhere tr(ΛA) is the trace of the \"k\" exterior power of \"A\", which has dimension formula_13. This trace may be computed as the sum of all principal minors of \"A\" of size \"k\". The recursive Faddeev–LeVerrier algorithm computes these coefficients more efficiently.\n\nWhen the characteristic is 0 it may alternatively be computed as a single determinant, that of the matrix,\n\nThe Cayley–Hamilton theorem states that replacing \"t\" by \"A\" in the characteristic polynomial (interpreting the resulting powers as matrix powers, and the constant term \"c\" as \"c\" times the identity matrix) yields the zero matrix. Informally speaking, every matrix satisfies its own characteristic equation. This statement is equivalent to saying that the minimal polynomial of \"A\" divides the characteristic polynomial of \"A\".\n\nTwo similar matrices have the same characteristic polynomial. The converse however is not true in general: two matrices with the same characteristic polynomial need not be similar.\n\nThe matrix \"A\" and its transpose have the same characteristic polynomial. \"A\" is similar to a triangular matrix if and only if its characteristic polynomial can be completely factored into linear factors over \"K\" (the same is true with the minimal polynomial instead of the characteristic polynomial). In this case \"A\" is similar to a matrix in Jordan normal form.\n\nIf \"A\" and \"B\" are two square \"n×n\" matrices then characteristic polynomials of \"AB\" and \"BA\" coincide: \n\nWhen \"A\" is non-singular this result follows from the fact that \"AB\" and \"BA\" are similar:\n\nFor the case where both \"A\" and \"B\" are singular, one may remark that the desired identity is an equality between polynomials in \"t\" and the coefficients of the matrices. Thus, to prove this equality, it suffices to prove that it is verified on a non-empty open subset (for the usual topology, or, more generally, for the Zariski topology) of the space of all the coefficients. As the non-singular matrices form such an open subset of the space of all matrices, this proves the result.\n\nMore generally, if \"A\" is a matrix of order \"m×n\" and \"B\" is a matrix of order \"n×m\", then \"AB\" is \"m×m\" and \"BA\" is \"n×n\" matrix, and one has\n\nTo prove this, one may suppose \"n\" > \"m\", by exchanging, if needed, \"A\" and \"B\". Then, by bordering \"A\" on the bottom by \"n\" – \"m\" rows of zeros, and \"B\" on the right, by, \"n\" – \"m\" columns of zeros, one gets two \"n×n\" matrices \"A' \"and \"B' \" such that \"B'A' \"= \"BA\", and \"A'B' \" is equal to \"AB\" bordered by \"n\" – \"m\" rows and columns of zeros. The result follows from the case of square matrices, by comparing the characteristic polynomials of \"A'B' \" and \"AB\".\n\nThe term secular function has been used for what is now called \"characteristic polynomial\" (in some literature the term secular function is still used). The term comes from the fact that the characteristic polynomial was used to calculate secular perturbations (on a time scale of a century, i.e. slow compared to annual motion) of planetary orbits, according to Lagrange's theory of oscillations.\n\n\"Secular equation\" may have several meanings.\n\n\n\n\n\n"}
{"id": "5401558", "url": "https://en.wikipedia.org/wiki?curid=5401558", "title": "Cleaner production", "text": "Cleaner production\n\nCleaner production is a preventive, company-specific environmental protection initiative. It is intended to minimize waste and emissions and maximize product output. By analysing the flow of materials and energy in a company, one tries to identify options to minimize waste and emissions out of industrial processes through source reduction strategies. Improvements of organisation and technology help to reduce or suggest better choices in use of materials and energy, and to avoid waste, waste water generation, and gaseous emissions, and also waste heat and noise. \n\nThe concept was developed during the preparation of the Rio Summit as a programme of UNEP (United Nations Environmental Programme) and UNIDO (United Nations Industrial Development Organization) under the leadership of Jacqueline Aloisi de Larderel, the former Assistant Executive Director of UNEP. The programme was meant to reduce the environmental impact of industry. It built on ideas used by 3M in its 3P programme (pollution prevention pays). It has found more international support than all other comparable programmes. The programme idea was described \"...to assist developing nations in leapfrogging from pollution to less pollution, using available technologies\". Starting from the simple idea to produce with less waste Cleaner Production was developed into a concept to increase the resource efficiency of production in general. UNIDO has been operating National Cleaner Production Centers and Programmes (NCPCs/NCPPs) with centres in Latin America, Africa, Asia and Europe.\n\nIn the US, the term pollution prevention is more commonly used for cleaner production.\n\nExamples for cleaner production options are: \n\n\nOne of the first European initiatives in cleaner production was started in Austria in 1992 by the BMVIT (Bundesministerium für Verkehr, Innovation und Technologie). This resulted in two initiatives: \"Prepare\" and EcoProfit.\n\nThe \"PIUS\" initiative was founded in Germany in 1999. Since 1994, the United Nations Industrial Development Organization operates the National Cleaner Production Centre Programme with centres in Central America, South America, Africa, Asia, and Europe. \n\n"}
{"id": "34633634", "url": "https://en.wikipedia.org/wiki?curid=34633634", "title": "Cry of Alcorta", "text": "Cry of Alcorta\n\nThe Cry of Alcorta () was an agrarian rebellion that took place in 1912 in Alcorta, Argentina. It began at the south of the Santa Fe Province, with requests of better working conditions and better contracts. These demonstrations bore no similarity to the contemporary anarchists. On June 25 the workers refused to harvest before their requests were met. The protest quickly expanded to Buenos Aires, Córdoba, Entre Ríos and Santa Fe. Although the workers had discontent with the leasing system, they did not request the complete eradication but just better terms; the few workers that proposed a radical goal did not prevail. The protest achieved the desired goals, and the Federación Agraria Argentina was established on August 15.\n"}
{"id": "13262036", "url": "https://en.wikipedia.org/wiki?curid=13262036", "title": "Death threat", "text": "Death threat\n\nA death threat is a threat, often made anonymously, by one person or a group of people to kill another person or group of people. These threats are often designed to intimidate victims in order to manipulate their behaviour, and thus a death threat can be a form of coercion. For example, a death threat could be used to dissuade a public figure from pursuing a criminal investigation or an advocacy campaign.\n\nIn most jurisdictions, death threats are a serious type of criminal offence. Death threats are often covered by coercion statutes. For instance, the coercion statute in Alaska says:\nA death threat can be communicated via a wide range of media, among these letters, newspaper publications, telephone calls, internet blogs and e-mail. If the threat is made against a political figure, it can also be considered treason. If a threat is against a non-living location that frequently contains living individuals (e.g. a building), it could be a terrorist threat. Sometimes, death threats are part of a wider campaign of abuse targeting a person or a group of people (see terrorism, mass murder).\n\nIn some monarchies and republics, both democratic and authoritarian, threatening to kill the head of state and/or head of government (such as the sovereign, president, or prime minister), is considered a crime, for which punishments vary. The United States law provides for up to five years in prison for threatening any type of government official. In the United Kingdom, under the Treason Felony Act 1848, it is illegal to attempt to kill or deprive the monarch of his/her throne; this offense was originally punished with penal transportation, and then was changed to the death penalty, and currently the penalty is life imprisonment.\n\nNamed after a high-profile case, \"Osman v United Kingdom\", these are warnings of death threat or high risk of murder that are issued by British police or legal authorities to the possible victim. They are used when there is intelligence of the threat, but there is not enough evidence to justify the police arresting the potential murderer.\n\n\n"}
{"id": "2725360", "url": "https://en.wikipedia.org/wiki?curid=2725360", "title": "Deconstruction (building)", "text": "Deconstruction (building)\n\nIn the context of physical construction, deconstruction is the selective dismantlement of building components, specifically for re-use, repurposing, recycling, and waste management. It differs from demolition where a site is cleared of its building by the most expedient means. Deconstruction has also been defined as “construction in reverse”. The process of dismantling structures is an ancient activity that has been revived by the growing field of sustainable, green method of building. Buildings, like everything, have a life-cycle. Deconstruction focuses on giving the materials within a building a new life once the building as a whole can no longer continue.\n\nWhen buildings reach the end of their useful life, they are typically demolished and hauled to landfills. Building implosions or ‘wrecking-ball’ style demolitions are relatively inexpensive and offer a quick method of clearing sites for new structures. On the other hand, these methods create substantial amounts of waste. Components within old buildings may still be valuable, sometimes more valuable than at the time the building was constructed. Deconstruction is a method of harvesting what is commonly considered “waste” and reclaiming it into useful building material.\n\nDeconstruction has strong ties to environmental sustainability. In addition to giving materials a new life cycle, deconstructing buildings helps to lower the need for virgin resources. This in turn leads to energy and emissions reductions from the refining and manufacture of new materials. As deconstruction is often done on a local level, many times on-site, energy and emissions are also saved in the transportation of materials. Deconstruction can potentially support communities by providing local jobs and renovated structures. Deconstruction work typically employs 3-6 workers for every one employed in a comparable demolition job. In addition, solid waste from conventional demolition is diverted from landfills. This is a major benefit because construction and demolition waste accounts for approximately 20% of the solid waste stream.\n\nIn Canada, the CO2 Neutral Alliance has created a website with resources for regulators and municipalities, developers and contractors, business owners and operators, and individuals and households. Benefits for municipalities include:\nImproving the local environment and overall sustainability of your community \nFor every three square feet of deconstruction, enough lumber can be salvaged to build one square foot of new construction. At this rate, if deconstruction replaced residential demolition, the United States could generate enough recovered wood to construct 120,000 new affordable homes each year. The deconstruction of a typical wood frame home can yield 6,000 board feet of reusable lumber. \nEvery year the United States buries about 33 million tons of wood-related construction and demolition debris in landfills. As anaerobic microorganisms decompose this wood, it will release about five million tons of carbon equivalent in the form of methane gas.\n\nDeconstruction is commonly separated into two categories; structural and non-structural. Non-structural deconstruction, also known as “soft-stripping”, consists of reclaiming non-structural components, appliances, doors, windows, and finish materials. The reuse of these types of materials is commonplace and considered to be a mature market in many locales.\n\nStructural deconstruction involves dismantling the structural components of a building. Traditionally this had only been performed to reclaim expensive or rare materials such as used brick, dimension stone, and extinct wood. In antiquity, it was common to raze stone buildings and reuse the stone; it was also common to steal stones from a building that was not being totally demolished: this is the literal meaning of the word \"dilapidated.\" Used brick and dimension limestone in particular have a long tradition of reuse due to their durability and color changes over time. Recently, the rise of environmental awareness and sustainable building has made a much wider range of materials worthy of structural deconstruction. Low-end, commonplace materials such as dimensional lumber have become part of this newly emerging market.\n\nThe United States military has utilized structural deconstruction in many of its bases. The construction methods of barracks, among other base structures, are usually relatively simple. They typically contained large amounts of lumber and used minimal adhesives and finish-work. In addition, the buildings are often identical, making the process of deconstructing multiple buildings much easier. Many barracks were built during the era prior to WWII, and have aged to the point where they now need to be torn down. Deconstruction was deemed very practical due to the abundance of labor the military has access to and the value of the materials themselves.\n\nNatural disasters, such as hurricanes, floods, tsunamis, and earthquakes often leave a vast amount of usable building materials in their wake. Structures that remain standing are often deconstructed to provide materials for rebuilding the region.\n\nDeconstruction’s economic viability varies from project to project. The amount of time and cost of labor are the main drawbacks. Harvesting materials from a structure can take weeks, whereas demolition may be completed in roughly a day. However, some of the costs, if not all, can be recovered. Reusing the materials in a new on-site structure, selling reclaimed materials, donating materials for income tax write-offs, and avoiding landfill “tipping fees” are all ways in which the cost of deconstruction can be made comparable to demolition.\n\nReclaiming the materials for a new on-site structure is the most economically and environmentally efficient option. Tipping fees and the costs of new materials are avoided; in addition, the transportation of the materials is non-existent. Selling the used materials or donating them to non-profit organizations are another effective way of gaining capital. Donations to NPO’s such as Habitat for Humanity’s ReStore are tax deductible. Many times it is possible to claim the value to be half of what that particular material would cost new. When donating rare or antique components it is sometimes possible to claim a higher value than a comparable, brand-new material.\n\nValue can also be added to new structures that are built by implementing reused materials. The United States Green Building Council's program entitled Leadership in Energy and Environmental Design (LEED) offers seven credits relating to reusing materials. (This accounts for seven out of a maximum sixty-nine credits) These include credits for building-shell reuse, material reuse, and diverting waste from landfills. Building shell-reuse is particularly appropriate for shells made of dimension stone.\n\nDeconstruction is well suited to job training for the construction trades. Taking down a building is an excellent way for a worker to learn how to put a building up. This is vital for the economic recovery of inner-city communities. Unskilled and low-skilled workers can receive on-the-job training in use of basic carpentry tools and techniques, as well as learning teamwork, problem-solving, critical thinking and good work habits.\n\nWhen choosing to deconstruct a building there are some important aspects that need to be taken into consideration. Developing a list of local contacts that are able to take used materials is an essential first step. These might include commercial architectural salvage businesses, reclamation yards, not-for-profit and social enterprise salvage warehouses, and dismantling contractors. Materials that cannot be salvaged may be recycled on-site or off-site, or taken to landfills. The next step involves identifying which, if any, are hazardous materials. Lead paint and asbestos are two substances in particular that need to be handled extremely cautiously and disposed of properly.\n\nIt is common practice, and common sense, to “soft-strip” the structure first; remove all appliances, windows, doors, and other finishing materials. These will account for a large percentage of the marketable components. After the non-structural deconstruction, structural is the next step. It is best to start at the roof and work down to the foundation.\n\nBuilding components that are dismantled will need to be stored in a secure, dry location. This will protect them from water damage and theft. Once separated from the structure, materials can also be cleaned and/or refinished to increase value. Building an inventory list of the materials at hand will help determine where each item will be sent.\n\n\"The end of the building’s useful life generates a stream of used materials that can be reprocessed for new construction. The selection of materials for reuse or recycling should not start at the end of the building’s life cycle, it should start at the design stage. Architects and engineers should keep the whole life cycle of the building in mind and select construction materials based on their capacity to be reused or recycled after the building has served its purpose.\"\n\nAn upstream approach to deconstruction can be implemented into buildings during their design process. This is a current trend in sustainable architecture. Often, simple construction methods combined with high-grade, durable materials work best for DfD structures. Separating layers of a building’s infrastructure and making them visible can significantly simplify its deconstruction. Making components within systems separable also assists in being able to dismantle materials quickly and efficiently. This can be achieved by using mechanical fasteners such as bolts to connect parts. Allowing physical access to the fasteners is another needed aspect of this design. Also, it is important to use standardized materials and assemble them in a consistent manner throughout the project.\n\nSome conventional construction methods and materials are difficult or impossible to deconstruct and should be avoided when designing for deconstruction. The use of nails and adhesives significantly slows down the deconstruction process and has a tendency to ruin otherwise reusable materials. Avoid hazardous materials altogether as they are detrimental to the natural environment and are non-reusable. Using mixed material grades makes the process of identifying pieces for resale difficult.\n\nDeconstruction is important for more than just the end of a building’s life-cycle. Buildings that have been designed with deconstruction in mind are often easier to maintain and adapt to new uses. Saving the shell of a building or adapting interior spaces to meet new needs ensures that new structures have a small environmental impact.\n\nAn alternative worth considering is modular building, like the Habitat 67 project in Montreal, Quebec, Canada. This was a residential structure consisting of separate, functional apartments that could be put together in a variety of ways. As people moved in or out, the units could be reconfigured as desired.\n\n\n"}
{"id": "7509291", "url": "https://en.wikipedia.org/wiki?curid=7509291", "title": "Depth chart", "text": "Depth chart\n\nIn sports, a depth chart is used to show the placements of the starting players and the secondary players. Generally a starting player will be listed first or on top while a back-up will be listed below. Depth charts also tend to resemble the actual position locations of certain players.\n\nThe typical Major League Baseball depth chart consists of a list of players at each position, with the starter or first-string player listed first, followed by replacement and platoon players. For fantasy baseball, typical preseason projection systems such as PECOTA construct depth charts that specify not just the order of the players at each position (starter, replacement or bench player) but also the amount of playing time each person at that position will have. For example, at first base one player may be projected as playing 60 percent of the innings, and another 40 percent for the coming season, while at catcher one player may be projected as playing 80 percent of the innings, and another 20 percent. For pitchers, the depth charts project the number of innings each roster player will pitch.\n\nThe line system in ice hockey follows the same structure as a depth chart.\n\nThe term depth chart is now also being used from the perspective of management theory, to address the process of key positioning leaders within the organization, considering a dynamic life cycle perspective which includes developmental tasks such as those cited in books and articles related to the leadership pipeline subject. A depth chart analysis for key positioning leaders should affect the development of leadership training programs and high performance development initiatives in modern corporations and enterprises.\n"}
{"id": "42327240", "url": "https://en.wikipedia.org/wiki?curid=42327240", "title": "Dignity of risk", "text": "Dignity of risk\n\nDignity of risk is the idea that self-determination and the right to take reasonable risks are essential for dignity and self esteem and so should not be impeded by excessively-cautious caregivers, concerned about their duty of care. The concept is applicable to adults who are under care such as elderly people, disabled people, and people with mental health problems.\n\nThe concept was first articulated in a 1972 article \"The dignity of risk and the mentally retarded\" by Robert Perske:\nOverprotection may appear on the surface to be kind, but it can be really evil. An oversupply can smother people emotionally, squeeze the life out of their hopes and expectations, and strip them of their dignity. Overprotection can keep people from becoming all they could become. Many of our best achievements came the hard way: We took risks, fell flat, suffered, picked ourselves up, and tried again. Sometimes we made it and sometimes we did not. Even so, we were given the chance to try. Persons with special needs need these chances, too. Of course, we are talking about prudent risks. People should not be expected to blindly face challenges that, without a doubt, will explode in their faces. Knowing which chances are prudent and which are not – this is a new skill that needs to be acquired. On the other hand, a risk is really only when it is not known beforehand whether a person can succeed. The real world is not always safe, secure, and predictable, it does not always say “please,” “excuse me”, or “I’m sorry”. Every day we face the possibility of being thrown into situations where we will have to risk everything … In the past, we found clever ways to build avoidance of risk into the lives of persons living with disabilities. Now we must work equally hard to help find the proper amount of risk these people have the right to take. We have learned that there can be healthy development in risk taking and there can be crippling indignity in safety!\n\nAllowing people under care to take risks is often perceived to be in conflict with the caregivers' duty of care. Finding a balance between these competing considerations can be difficult when formulating policies and guidelines for caregiving.\n\nOverprotection of people with disabilities causes low self-esteem and underachievement because of lowered expectations that come with overprotection. Internalisation of low expectations causes the disabled person to believe that they are less capable than others in similar situations.\n\nIn elderly people, overprotection can result in learned dependency and a decreased ability for self-care:\n\n\"It is possible to deliver physical care that has positive outcomes and returns a person to full function, yet, if during that care they have not been involved, allowed to make choices and respectfully assisted with activities of daily living, it may be possible to cause psychological damage through undermining that person's dignity.\"\n\nThe right to fail and the dignity of risk is one of the basic tenets of the philosophy of the independent living movement.\n\nThe first of eight \"guiding principles\" of the United Nations' Convention on the Rights of Persons with Disabilities states: \"Respect for inherent dignity, individual autonomy including the freedom to make one’s own choices, and independence of persons.\"\n\n"}
{"id": "8586", "url": "https://en.wikipedia.org/wiki?curid=8586", "title": "Dyson sphere", "text": "Dyson sphere\n\nA Dyson sphere is a hypothetical megastructure that completely encompasses a star and captures a large percentage of its power output. The concept is a thought experiment that attempts to explain how a spacefaring civilization would meet its energy requirements once those requirements exceed what can be generated from the home planet's resources alone. Only a fraction of a star's energy emissions reach the surface of any orbiting planet. Building structures encircling a star would enable a civilization to harvest far more energy.\n\nThe first contemporary description of the structure was by Olaf Stapledon in his science fiction novel \"Star Maker\" (1937), in which he described \"every solar system... surrounded by a gauze of light traps, which focused the escaping solar energy for intelligent use.\" The concept was later popularized by Freeman Dyson in his 1960 paper \"Search for Artificial Stellar Sources of Infrared Radiation\". Dyson speculated that such structures would be the logical consequence of the escalating energy needs of a technological civilization and would be a necessity for its long-term survival. He proposed that searching for such structures could lead to the detection of advanced, intelligent extraterrestrial life. Different types of Dyson spheres and their energy-harvesting ability would correspond to levels of technological advancement on the Kardashev scale.\n\nSince then, other variant designs involving building an artificial structure or series of structures to encompass a star have been proposed in exploratory engineering or described in science fiction under the name \"Dyson sphere\". These later proposals have not been limited to solar-power stations, with many involving habitation or industrial elements. Most fictional depictions describe a solid shell of matter enclosing a star, which is considered the least plausible variant of the idea. In May 2013, at the Starship Century Symposium in San Diego, Dyson repeated his comments that he wished the concept had not been named after him.\n\nThe concept of the Dyson sphere was the result of a thought experiment by physicist and mathematician Freeman Dyson, when he theorized that all technological civilizations constantly increased their demand for energy. He reasoned that if human civilization expanded energy demands long enough, there would come a time when it demanded the \"total\" energy output of the Sun. He proposed a system of orbiting structures (which he referred to initially as a \"shell\") designed to intercept and collect all energy produced by the Sun. Dyson's proposal did not detail how such a system would be constructed, but focused only on issues of energy collection, on the basis that such a structure could be distinguished by its unusual emission spectrum in comparison to a star. His 1960 paper \"Search for Artificial Stellar Sources of Infra-Red Radiation\", published in the journal \"Science\", is credited with being the first to formalize the concept of the Dyson sphere.\n\nHowever, Dyson was not the first to advance this idea. He was inspired by the mention of the concept in the 1937 science fiction novel \"Star Maker\", by Olaf Stapledon, and possibly by the works of J. D. Bernal, Raymond Z. Gallun, and Edgar Rice Burroughs, who seem to have explored similar concepts in their work.\n\nAlthough such megastructures may be theoretically possible, all plans to build a fixed-in-place Dyson sphere are currently far beyond humanity's engineering capacity. The number of craft required to obtain, transmit, and maintain a complete Dyson sphere far exceeds present-day industrial capabilities. George Dvorsky has advocated use of self-replicating robots to overcome this limitation in the relatively near term. Some have suggested that such habitats could be built around white dwarfs and even pulsars.\n\nIn fictional accounts, the Dyson-sphere concept is often interpreted as an artificial hollow sphere of matter around a star. This perception is based on a literal interpretation of Dyson's original short paper introducing the concept. In response to letters prompted by some papers, Dyson replied, \"A solid shell or ring surrounding a star is mechanically impossible. The form of 'biosphere' which I envisaged consists of a loose collection or swarm of objects traveling on independent orbits around the star.\"\n\nThe variant closest to Dyson's original conception is the \"Dyson swarm\". It consists of a large number of independent constructs (usually solar power satellites and space habitats) orbiting in a dense formation around the star. This construction approach has advantages: components could be sized appropriately, and it can be constructed incrementally. Various forms of wireless energy transfer could be used to transfer energy between swarm components and a planet.\n\nDisadvantages resulting from the nature of orbital mechanics would make the arrangement of the orbits of the swarm extremely complex. The simplest such arrangement is the \"Dyson ring\", in which all such structures share the same orbit. More-complex patterns with more rings would intercept more of the star's output, but would result in some constructs eclipsing others periodically when their orbits overlap. Another potential problem is that the increasing loss of orbital stability when adding more elements increases the probability of orbital perturbations.\n\nAs noted below, such a cloud of collectors would alter the light emitted by the star system. However, the disruption compared to a star's overall natural emitted spectrum would most likely be too small for Earth-based astronomers to observe.\n\nA second type of Dyson sphere is the \"Dyson bubble\". It would be similar to a Dyson swarm, composed of many independent constructs and likewise could be constructed incrementally.\n\nUnlike the Dyson swarm, the constructs making it up are not in orbit around the star, but would be statites—satellites suspended by use of enormous light sails using radiation pressure to counteract the star's pull of gravity. Such constructs would not be in danger of collision or of eclipsing one another; they would be totally stationary with regard to the star, and independent of one another. Because the ratio of radiation pressure to the force of gravity from a star is constant regardless of the distance (provided the statite has an unobstructed line-of-sight to the surface of its star), such statites could also vary their distance from their central star.\n\nThe practicality of this approach is questionable with modern material science, but cannot yet be ruled out. A 100% reflective statite deployed around the Sun would have an overall density of 0.78 grams per square meter of sail. To illustrate the low mass of the required materials, consider that the total mass of a bubble of such material 1 AU in radius would be about 2.17 kg, which is about the same mass as the asteroid Pallas. Another illustration: Regular printing paper has a density of around 80 g/m.\n\nSuch a material has not yet been produced in the form of a working light sail. The lightest carbon-fiber light-sail material currently produced has a density—without payload—of 3 g/m, or about four times as heavy as would be needed to construct a solar statite.\n\nA single sheet of graphene, the two-dimensional form of carbon, has a density of only 0.37 mg per square meter, making such a single sheet of graphene possibly effective as a solar sail. However, as of 2015 graphene has not been fabricated in large sheets, and it has a relatively high rate of radiation absorption, about 2.3% (i.e., still about 97.7% will be transmitted). For frequencies in the upper GHz and lower THz range, the absorption rate is as high as 50–100% due to voltage bias and/or doping.\n\nUltra-light carbon nanotubes meshed through molecular manufacturing techniques have densities between 1.3 g/m to 1.4 g/m. By the time a civilization is ready to use this technology, the carbon nanotube's manufacturing might be optimised enough for them to have a density lower than the necessary 0.7 g/m, and the average sail density with rigging might be kept to 0.3 g/m (a \"spin stabilized\" light sail requires minimal additional mass in rigging). If such a sail could be constructed at this areal density, a space habitat the size of the L5 Society's proposed O'Neill cylinder—500 km, with room for over 1 million inhabitants, massing 3 tons—could be supported by a circular light sail 3,000 km in diameter, with a combined sail/habitat mass of 5.4 kg. For comparison, this is just slightly smaller than the diameter of Jupiter's moon Europa (although the sail is a flat disc, not a sphere), or the distance between San Francisco and Kansas City. Such a structure would, however, have a mass quite a lot less than many asteroids. Although the construction of such a massive habitable statite would be a gigantic undertaking, and the required material science behind it is early stage, there are other engineering feats and required materials proposed in other Dyson sphere variants.\n\nIn theory, if enough satellites were created and deployed around their star, they would compose a non-rigid version of the Dyson shell mentioned below. Such a shell would not suffer from the drawbacks of massive compressive pressure, nor are the mass requirements of such a shell as high as the rigid form. Such a shell would, however, have the same optical and thermal properties as the rigid form, and would be detected by searchers in a similar fashion (see below).\n\nThe variant of the Dyson sphere most often depicted in fiction is the \"Dyson shell\": a uniform solid shell of matter around the star. Such a structure would completely alter the emissions of the central star, and would intercept 100% of the star's energy output. Such a structure would also provide an immense surface that many envision would be used for habitation, if the surface could be made habitable.\n\nA spherical shell Dyson sphere in the Solar System with a radius of one astronomical unit, so that the interior surface would receive the same amount of sunlight as Earth does per unit solid angle, would have a surface area of approximately , or about 550 million times the surface area of Earth. This would intercept the full 384.6 yottawatts (3.846 × 10 watts) of the Sun's output. Non-shell designs would intercept less, but the shell variant represents the maximum possible energy captured for the Solar System at this point of the Sun's evolution. This is approximately 33 trillion times the power consumption of humanity in 1998, which was 12 terawatts.\n\nThere are several serious theoretical difficulties with the solid shell variant of the Dyson sphere:\n\nSuch a shell would have no net gravitational interaction with its englobed star (see shell theorem), and could drift in relation to the central star. If such movements went uncorrected, they could eventually result in a collision between the sphere and the star—most likely with disastrous results. Such structures would need either some form of propulsion to counteract any drift, or some way to repel the surface of the sphere away from the star.\n\nFor the same reason, such a shell would have no net gravitational interaction with anything else inside it. The contents of any biosphere placed on the inner surface of a Dyson shell would not be attracted to the sphere's surface and would simply fall into the star. It has been proposed that a biosphere could be contained between two concentric spheres, placed on the interior of a rotating sphere (in which case, the force of artificial \"gravity\" is perpendicular to the axis of rotation, causing all matter placed on the interior of the sphere to pool around the equator, effectively rendering the sphere a Niven ring for purposes of habitation, but still fully effective as a radiant-energy collector) or placed on the outside of the sphere where it would be held in place by the star's gravity. In such cases, some form of illumination would have to be devised, or the sphere made at least partly transparent, because the star's light would otherwise be completely hidden.\n\nIf assuming a radius of one AU, then the compressive strength of the material forming the sphere would have to be immense to prevent implosion due to the star's gravity. Any arbitrarily selected point on the surface of the sphere can be viewed as being under the pressure of the base of a dome 1 AU in height under the Sun's gravity at that distance. Indeed, it can be viewed as being at the base of an infinite number of arbitrarily selected domes, but because much of the force from any one arbitrary dome is counteracted by those of another, the net force on that point is immense, but finite. No known or theorized material is strong enough to withstand this pressure, and form a rigid, static sphere around a star. It has been proposed by Paul Birch (in relation to smaller \"Supra-Jupiter\" constructions around a large planet rather than a star) that it may be possible to support a Dyson shell by dynamic means similar to those used in a space fountain. Masses travelling in circular tracks on the inside of the sphere, at velocities significantly greater than orbital velocity, would press outwards on magnetic bearings due to centrifugal force. For a Dyson shell of 1-AU radius around a star with the same mass as the Sun, a mass travelling ten times the orbital velocity (297.9 km/s) would support 99 (a=v/r) times its own mass in additional shell structure.\n\nAlso if assuming a radius of one AU, then there may not be sufficient building material in the Solar System to construct a Dyson shell. Anders Sandberg estimates that there is 1.82 kg of easily usable building material in the Solar System, enough for a 1-AU shell with a mass of 600 kg/m—about 8–20 cm thick on average, depending on the density of the material. This includes the hard-to-access cores of the gas giants; the inner planets alone provide only 11.79 kg, enough for a 1-AU shell with a mass of just 42 kg/m.\n\nThe shell would be vulnerable to impacts from interstellar bodies, such as comets, meteoroids, and material in interstellar space that is currently being deflected by the Sun's bow shock. The heliosphere, and any protection it theoretically provides, would cease to exist.\n\nAnother possibility is the \"Dyson net\", a web of cables strung about the star that could have power or heat collection units strung between the cables. The Dyson net reduces to a special case of Dyson shell or bubble, however, depending on how the cables are supported against the sun's gravity.\n\nA bubbleworld is an artificial construct that consists of a shell of living space around a sphere of hydrogen gas. The shell contains air, people, houses, furniture, etc. The idea was conceived to answer the question, \"What is the largest space colony that can be built?\" However, most of the volume is not habitable and there is no power source.\n\nTheoretically, any gas giant could be enclosed in a solid shell; at a certain radius the surface gravity would be terrestrial, and energy could be provided by tapping the thermal energy of the planet. This concept is explored peripherally in the novel \"Accelerando\" (and the short story \"Curator\", which is incorporated into the novel as a chapter) by Charles Stross, in which Saturn is converted into a human-habitable world.\n\nStellar engines are a class of hypothetical megastructures whose purpose is to extract useful energy from a star, sometimes for specific purposes. For example, Matrioshka brains extract energy for purposes of computation; Shkadov thrusters extract energy for purposes of propulsion. Some of the proposed stellar engine designs are based on the Dyson sphere.\n\nA black hole could be the power source instead of a star in order to increase the matter-to-energy conversion efficiency. A black hole would also be smaller than a star. This would decrease communication distances that would be important for computer-based societies as those described above.\n\nIn Dyson's original paper, he speculated that sufficiently advanced extraterrestrial civilizations would likely follow a similar power-consumption pattern to that of humans, and would eventually build their own sphere of collectors. Constructing such a system would make such a civilization a Type II Kardashev civilization.\n\nThe existence of such a system of collectors would alter the light emitted from the star system. Collectors would absorb and reradiate energy from the star. The wavelength(s) of radiation emitted by the collectors would be determined by the emission spectra of the substances making them up, and the temperature of the collectors. Because it seems most likely that these collectors would be made up of heavy elements not normally found in the emission spectra of their central star—or at least not radiating light at such relatively \"low\" energies compared to what they would be emitting as energetic free nuclei in the stellar atmosphere—there would be atypical wavelengths of light for the star's spectral type in the light spectrum emitted by the star system. If the percentage of the star's output thus filtered or transformed by this absorption and reradiation was significant, it could be detected at interstellar distances.\n\nGiven the amount of energy available per square meter at a distance of 1 AU from the Sun, it is possible to calculate that most known substances would be reradiating energy in the infrared part of the electromagnetic spectrum. Thus, a Dyson sphere, constructed by life forms not dissimilar to humans, who dwelled in proximity to a Sun-like star, made with materials similar to those available to humans, would most likely cause an increase in the amount of infrared radiation in the star system's emitted spectrum. Hence, Dyson selected the title \"Search for Artificial Stellar Sources of Infrared Radiation\" for his published paper.\n\nSETI has adopted these assumptions in their search, looking for such \"infrared heavy\" spectra from solar analogs. Fermilab has an ongoing survey for such spectra by analyzing data from the Infrared Astronomical Satellite (IRAS). Identifying one of the many infrared sources as a Dyson sphere would require improved techniques for discriminating between a Dyson sphere and natural sources. Fermilab discovered 17 potential \"ambiguous\" candidates, of which four have been named \"amusing but still questionable\". Other searches also resulted in several candidates, which are, however, unconfirmed.\n\nOn 14 October 2015, the realization of a strange pattern of light from star KIC 8462852, nicknamed \"Tabby's Star\" after Tabetha S. Boyajian — the lead researcher who discovered the irregular light fluctuation— captured by the Kepler Space Telescope, raised speculation that a Dyson sphere may have been discovered.\nIn February 2016, Boyajian gave a TED talk where she explained the story of how her research on the star quickly took a turn into the mysterious. However, she was skeptical and in the talk she reminded everyone that skepticism is the best policy whenever delving into alien territory. Her exact quote is as follows:\n\nWanting to understand the strange light pattern, Tabetha S. Boyajian put several hypotheses to the test. The first general assumption was an exoplanet transiting (eclipsing) its massive star, but the dips in light lasted between 5 and 80 days and were erratically spaced apart, thus ruling out any kind of an orbit for one celestial object.\nA dust cloud was proposed but the star showed no signs of being young so a dust cloud was highly improbable. Lastly, a comet shower was hypothesized. However, as Boyajian pointed out in her TED talk this was also highly improbable. \n\"It would take hundreds of comets to reproduce what we're observing. And these are only the comets that happen to pass between us and the star. And so in reality, we're talking thousands to tens of thousands of comets.\"\n\nSo after all the natural explanations turned up weak, her team decided to send off their research to SETI (Search for extraterrestrial life) to rule out alien structures. After reviewing the research, the SETI Institute was so intrigued that they decided to study the star themselves and pointed their Allen Telescope Array (ATA) at the star \"with hopes of catching a tell-tale signal that might reveal a technological civilization.\"\n\nThe SETI Institute mentioned that what caught their interest was that \"the timing of the present dip (in light) suggests that whatever this material is, it is situated at just the right distance from the star to be in the habitable zone, where we believe life like ours could develop as it has on Earth.\"\n\nBeing skeptical as Boyajian was, she finally decided to take SETI's approach and allow herself to have a bit of fun in hypothesizing what the light pattern could have been. In her Ted Talk she joked: \"Another idea that's one of my personal favorites is that we had just witnessed an interplanetary space battle and the catastrophic destruction of a planet. Now, I admit that this would produce a lot of dust that we don't observe. But if we're already invoking aliens in this explanation, then who is to say they didn't efficiently clean up all this mess for recycling purposes?\"\nThe search for answers to KIC 8462852 is still ongoing.\n\nOn August 25, 2016, a similar phenomenon was reported for another stellar object: EPIC 204278916, a young M-type pre-main-sequence star with a resolved disk. Dimmings of up to 65% for 25 consecutive days (out of 79 total observation) were observed. The variability is highly periodic and attributed to stellar rotation. The researchers hypothesize that the irregular dimmings are caused by either a warped inner-disk edge or transiting cometary-like objects in either circular or eccentric orbits.\n\nAs noted above, the Dyson sphere originated in fiction, and it is a concept that has appeared often in science fiction since then. In fictional accounts, Dyson spheres are most often depicted as a \"Dyson shell\" with the gravitational and engineering difficulties of this variant noted above largely ignored.\n\n"}
{"id": "3102824", "url": "https://en.wikipedia.org/wiki?curid=3102824", "title": "Eco-cities", "text": "Eco-cities\n\nAn eco-city is a city built from the principles of living within environment means. The ultimate goal of many eco-cities is to eliminate all carbon waste (zero-carbon city), to produce energy entirely through renewable resources, and to merge the city harmoniously with the natural environment; however, eco-cities also have the intentions of stimulating economic growth, reducing poverty, using higher population densities, and therefore obtaining higher efficiency, and improving health.\n\nThe concept of the “Emils Gejs” was born out of one of the first organizations focused on eco-city development, “Urban Emiils.” The group was founded by Richard Register in Berkeley, California in 1975, and was founded with the idea of reconstructing cities to be in balance with nature. They worked to plant trees along the main streets, built solar greenhouses, and worked within the Berkeley legal system to pass environmentally friendly policies and encourage public transportation. Urban Ecology then took the movement another step further with the creation of The Urban Ecologist, a journal they started publishing in 1987.\n\nUrban ecology further advanced the movement when they hosted the first International Eco-City Conference in Berkeley, California in 1990. The Conference focused on urban sustainability problems and encouraged the over 700 participants to submit proposals on how to best reform cities to work within environmental means. In 1992 Richard Register founded the organization Ecocity Builders which has acted as convener of the conference series ever since. \nEco-City Conferences have been held in Adelaide, Australia; Yoff, Senegal; Curitiba, Brazil; Shenzhen, China; Bangalore, India; San Francisco, United States; Istanbul, Turkey; Montreal, Canada; Nantes, France and Abu Dhabi (2015). An International Conference on: Green Urbanism will be held in Italy from 12–14 October 2016 will discuss Eco-cities and Different other Topics.\n\nAlso, the conference Ice Cool Earth (ICEF), first held in Tokyo, Japan in 2014, and now every year at the same place, aims to discuss the future of economics and discuss the possibility for Eco-cities and smart-design from an energy & economic perspective. The conference gather important political leaders (Prime Minister of Japan did an apparition), leading enterprise from Europe and Asia, and few academics and also have an active forum.\n\nSeveral sets of criteria for Eco-cities have been suggested, encompassing the economic, social, and environmental qualities that an eco-city should satisfy. The ideal \"eco-city\" has been described as a city that fulfils the following requirements:\n\n\nIn addition to these initial requirements, the city design must be able to grow and evolve as the population grows and the needs of the population change. This is especially important when taking into consideration infrastructure designs, such as for water systems, power lines, etc. These must be built in such a way that they are easy to modernize (as opposed to the dominant current strategy of placing them underground, and therefore making them highly inaccessible).\n\nEach individual eco-city development has also set its own requirements to ensure their city is environmentally sustainable; these criteria range from zero-waste and zero-carbon emissions, such as in the Sino-Singapore Tianjin Eco-city project and the Abu Dhabi Masdar City project, to simple urban revitalization and green roof garden projects in Augustenborg, Malmö, Sweden.\n\nUsing a different set of criteria, the International Eco-Cities Initiative recently identified as many as 178 significant eco-city initiatives at different stages of planning and implementation around the world. To be included in this census, initiatives needed to be at least district-wide in their scale, to cover a variety of sectors, and to have official policy status. Although such schemes display great variety in their ambitions, scale, and conceptual underpinnings, since the late 2000s there has been an international proliferation of frameworks of urban sustainability indicators and processes designed to be implemented across different contexts. This may suggest that a process of de facto eco-city 'standardisation' is underway.\n\nOne of the major and most noticeable economic impacts of the movement towards becoming an eco-city is the notable increase in productivity across existing industries as well as the introduction of new industries, thus creating jobs.\n\nFirst, the movement away from carbon-producing energy sources to more renewable energy sources, such as wind, water and solar power, provides local economies with new, thriving industries. The creation of these industries, in turn, births an increase in the demand for labor; thus, not only does total employment increase, but an increase in wages also mimics increasing employment.\n\nMoreover, one of the main priorities of a sustainable city is to reduce its ecological footprint by reducing total carbon emissions, which, economically speaking means increasing productivity. Merely increasing the rate of productivity in an industry reduces costs, both monetary and environmental; that is, as an industry becomes more productive, it can more efficiently allocate and use both its physical and human capital, reducing the time it takes to make the same amount of goods which also allows for a higher wage (because employees are doing more) and a lesser environmental impact (because using less energy and resources to produce the same amount).\n\nIn all, although the initial movement towards becoming a sustainable city may be quite costly for a smaller, poorer city, the benefits of such movement are plentiful in the long-run economic model. Moreover, as more and more countries move towards becoming more sustainable, the technologies required to initiate this movement will become more readily accessible and cheaper; therefore, many rich, developed nations should put themselves forth as an example of what other cities should model themselves like, thus sparking the innovation towards a future of sustainable technology.\n\nAlthough local environmental standards may differ across eco-cities, each city nonetheless has its own appropriate and practical goals and expectations that have provided the foundation for their recognition as a sustainable city. Differences in these goals and expectations are to be expected, however, due to the limitations of technology and local financing.\n\nThe primary goal for all sustainable cities is to significantly decrease total carbon emissions as quickly as possible in order to work towards becoming a carbon-free city; that is, sustainable cities work to move towards an economy based solely on renewable energy. Actions towards carbon-reductions can be seen on both the corporate and individual levels: many industries are working towards cleaner production, but individuals are also moving away from environmentally costly forms of transportation to more sustainable methods, such as public transportation or biking. On this note, another common environmental goal is to increase and make more efficient the public transportation systems.\n\nMany sustainable cities also work towards becoming more densely populated (urban density); having its citizens living closer to energy production means less environmental costs of transporting said energy to citizen households. Additionally, citizens living closer to the city-center also mean that transportation to work is significantly reduced.\n\nOften a city’s primary goal is to increase environmental education in hopes of achieving better citizen involvement and cooperation. By making the private sector more aware of how its behavior affects the environment, a reduction in carbon emissions becomes more of a reality.\n\nIn terms of international standards, however, we can look to the International Finance Corporation (IFC). The IFC has a long history of implementing environmental and social standards in localized economies, and its primary mission is to promote sustainable development across the globe, primarily in developing countries. One of its plans to accomplish this goal is to encourage international cooperation in order to accelerate and promote sustainable growth across nations.\n\nOverall, the most important aspect of setting an environmental goal is making it plausible. Many cities across the globe set goals that, although they may be super-sustainable, are not entirely possible. These exaggerated goals include too much sustainable development for a small time period or an expectation that is simply too expensive. The globe needs to work together to make steps towards a sustainable future that are possible and execute them well, ultimately resulting in an overall spiral towards complete global sustainability.\n\nThe development of eco-cities has aided in reducing poverty in various locations via job creation in environmentally friendly business sectors. By promoting social equity based on meeting the needs of local populations, eco-cities create sustainable business models that encourage local investment and the subsequent expansion of the job market. Johannesburg, South Africa serves as a prime example of the manner in which adopting eco-city standards can aid in reducing urban poverty. According to the United Nations Environment Program, the “EcoCity [program] has mobilized the disadvantaged and unemployed people of Ivory Park (part of the city of Johannesburg) to form co-operatives to grow and buy food, to recycle, to repair bicycles, to build homes, to use and promote green energy solutions, to become eco-tourism guides and more than 300 jobs have been created” between 1991 and 2001. By creating small local businesses, residents of eco-cities create self-sufficient small enterprises that, as an aggregate, greatly alleviate the scarcity of quality employment and create economic opportunities that continuously aid in poverty reduction. These ecologically sound small-scale practices are additionally less sensitive to economic shocks, allowing for enduring economic sustainability in eco-cities. In addition to creating green jobs, eco-cities promote the deployment of green methods of saving money, such as investing in ecologically sustainable local infrastructure, carpooling, and reducing consumption of water and energy, to decrease the financial burden on the poor.\n\nIncreasing proportions of the world population are now located in cities. As a result, eco-city models place substantial attention on mitigating and reducing the environmental damage caused by growing urban populations. Because urbanization does not appear to be slowing, eco-cities aim to increase urban density while integrating “green infrastructure” or \"green spaces\" into the urban landscape. Eco-cities promote compact use of land by people for residential and commercial purposes. In this way, increasing urban density reduces the strain on the environment by centralizing and, thereby, reducing resource consumption. For example, the 2006 plans for the Chinese eco-city of Dongtan employed this strategy. At the time, the city planned to divide its residential and commercial land into three compact districts divided by farms, parks, lakes, and pagodas. Additionally, residents would live in ecologically designed apartment buildings six to eight stories high but appropriately spaced apart as to avoid heat island effects. Although no construction of Dongtan has happened yet, these principles are generally applicable to all eco-cities.\n\nFurthermore, increased urban density reduces urban sprawl, thus, decreasing dependence on cars. According to Kenworthy, urban density is accountable for 84% of the variance in car travel. Because of the compact urban layout of eco-cities, residents are able to easily navigate their surrounding environment on foot, by bicycle, or through use of public transportation. As a result, eco-cities avoid much of the negative effects of car pollution.\n\nAdditionally, by centralizing the population within a given area, eco-cities increase the amount of land that can be used for parks and urban agriculture. As such, eco-cities increase food security and promote ecological preservation within urban areas. Urban agriculture allows for “production of fresh food and vegetables, reduction on transportation load and enrichment of environmental quality” (Lim 2010).\n\nEco-cities aid in creating healthier urban populations through the implementation of sustainable practices that improve environmental standards and, as a result, decrease the strain on public health. By employing practices that aim to reduce air pollution, eco-city standards have an indirect effect on decreasing rates of respiratory disease within urban areas. According to the World Health Organization, urban outdoor air pollution is responsible for over 1.3 million deaths worldwide per year. Additionally, “the mortality in cities with high levels of pollution exceeds that observed in relatively cleaner cities by 15–20%.\" Through the implementation of “clean” practices, eco-cities greatly assist in decreasing the disease burden placed on urban residents by decreasing the risk factors associated with cardiovascular and respiratory diseases as well as various forms of cancer.\n\nAdditionally, the “greenspaces” that constitute the infrastructure of eco-cities provide a unique method of reducing air pollution and promoting clean air. Urban foliage naturally cleans the air by absorbing carbon monoxide, nitrogen dioxide, and sulfur dioxide. Green spaces also absorb airborne particulates and reduce heat, allowing for improved levels of public health.\n\nThe decreased dependency on cars encouraged by the compact, walkable layout of eco-cities will also help combat obesity and other chronic diseases by encouraging frequent physical activity. The World Health Organization estimates that physical inactivity leads to 3.2 million deaths per year. 2.6 million of these deaths are centralized in low and middle-income countries. Thus, by reducing urban sprawl, eco-cities may help decrease rates of coronary heart disease and stroke, diabetes, hypertension, colon cancer, breast cancer, osteoporosis, and depression. Furthermore, by decreasing the concentration of cars within city limits, eco-cities are also able to reduce the number of preventable deaths among the working age population. It is estimated that traffic accidents kill 1.2 million people per year and is the leading cause of death among people under the age of 25 (WHO, 2011).\n\nIncreased access to affordable vegetation via urban agriculture also permits the improvement of public health conditions by making healthy foods more available and affordable. Lye and Chen note, “An eco-city must not become or be perceived as an enclave for only the rich and powerful but must welcome and be accessible to people from various walks of life.\" By investing in urban agriculture, eco-cities can help eliminate the prominent issue of food deserts in urban poor areas. Expanded access to vegetables will in hand aid in decreasing rates of obesity, cancer, cardiovascular disease, diabetes and other chronic illness, especially among low-income residents.\n\nBy decreasing urban sprawl, eco-cities decrease the residential and commercial dependence on automobiles. Concurrently, improved public transportation further decreases the demand for cars. The development of metro station and light rail transit systems provide mass transit not only within sectors of a city but between cities. Furthermore, many eco-cities are employing expanded “clean” bus routes in order to decrease the emissions from single household vehicles. Critics note, however, that the high price of “clean” diesel, CNG/LNG, hybrid electric buses, and super capacitor-powered buses may not prove “economically and operationally viable” (World Bank, 2009).\n\nEco-cities may also seek to create sustainable urban environments with long-lasting structures, buildings and a great liveability for its inhabitants. The most clearly defined form of walkable urbanism is known as the \"Charter of New Urbanism\". It is an approach for successfully reducing environmental impacts by altering the built environment to create and preserve smart cities which support sustainable transport. Residents in compact urban neighborhoods drive fewer miles, and have significantly lower environmental impacts across a range of measures, compared with those living in sprawling suburbs. The concept of Circular flow land use management has also been introduced in Europe to promote sustainable land use patterns that strive for compact cities and a reduction of greenfield land take by urban sprawl.\n\nIn sustainable architecture the recent movement of New Classical Architecture promotes a sustainable approach towards construction, that appreciates and develops smart growth, walkability, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as opposing solitary housing estates and suburban sprawl. Both trends started in the 1980s.\n\nEco-cities primarily employ green roofs, vertical landscaping, and bridge links as methods of decreasing the environmental impact of land use. Constructing green roofs and investing in vertical landscaping create natural insulation for residential and commercial properties as well as allows for rainfall collection. Additionally, green roofs and vertical landscaping lower urban temperatures and help prevent the heat island effect. Bridge links allow for development of a walkable city without disrupting the soil to run utility lines by connecting buildings with above ground walkways.\n\nEco-cities look to employ renewable energy sources, such as wind turbines, solar panels, and biogas, to reduce emissions. Wind turbines present the opportunity of being able to provide both localized districts within eco-cities and the larger region as a whole with emission-free renewable energy that can additionally supplement existing power sources. Furthermore, by designing buildings with natural ventilation systems, eco-cities reduce the need for air conditioning, thus, drastically decreasing commercial and residential energy use. The energy generated can come from large scale energy production systems such as solar farms which supply many homes and businesses or from individual buildings energying at least in part their own energy from solar photovoltaic or small scale wind turbines or biomass.\nMany eco-cities additionally look to deploy solar thermal energy. By installing solar collectors, developers will be able to provide hot water for space heating and individual and community needs while reducing dependence on gas fueled boilers. While solar thermal energy appears to be a more efficient source of renewable energy, many urban planners also view photovoltaics as a viable source of energy. Photovoltaics directly convert solar energy into electricity; however, the extensive costs associated with developing this technology on the city-scale may limit its use when compared to its potential payback. Biogas technology is also deployed as a source of renewable energy as the organic material from wastewater is converted into fuel.\n\nEco-cities aim to decrease water consumption by employing technologies that reduce the amount of water that is needed for irrigation and sewage flow while also preventing blackwater and greywater runoff from entering ground water sources. Developers suggest installing low flow fixtures, rainwater harvesting systems, and sustainable urban drainage systems to meet eco-city standards. Additionally, advanced irrigation systems (xeriscaping) aid in maintaining green infrastructure while decreasing green space consumption of water for irrigation.\n\nThe city of Curitiba, Brazil proactively began to address the challenges of sustainable urban development in 1966 with a master plan that outlined future integration between urban development, transportation and public health.\n\nThis plan has been realized in modern Curitiba, which is defined by linear stretches of urban development surrounded by green space and low-density residential areas. The city was designed for the mobility of people, not the mobility of cars. The city’s bus system is highly developed, with high-capacity busses and dedicated lanes, it effectively reaches about 90% of the population. This bus system is utilized by 45% of the population, which has caused private automobile use to drop to 22%. Despite this decline, to prevent congestion central areas of the city have been closed to cars. These road closures have led to dynamic economic growth for local shops and the development of community space for pedestrians.\n\nThe resulting public health and education gains from this initiative have also been substantial. Curitiba maintains the lowest air pollution rates in Brazil and over 300,000 trees in the city helps reduce natural flooding. Curitiba has also dedicated resources to environmental education in primary school, which has translated into environmentally conscious citizens. Over 70% of city residents participate in recycling programs which fuels the city’s progressive waste processing system.\n\nCuritiba has maintained a consistent vision of the future and worked to attain it by through careful urban planning that takes into account transportation, while also encouraging environmental initiatives and public health. In 2010, Curitiba recognized for their achievement with the Globe Sustainable City Award due to “their understanding of sustainable city development – both regarding policy and implementation.”\n\nAuroville was founded in 1968 with the intention of realizing human unity, and is now home to approximately 2,300 individuals from over 45 nations around the world (substantially less than the 50,000 anticipated). Its focus is its vibrant community culture and its expertise in renewable energy systems, habitat restoration, ecology skills, mindfulness practices, and holistic education.\n\nThe city of Freiburg, Germany, whose sustainable policies date all the way back to the 1970s, has constructed itself as a sustainable city by actively committing to its target areas of energy, transportation, and to its three pillars for sustainable development: energy saving, new technology, and renewable energy sources. One of the largest motivators for success can be accredited to citizen’s engagement; in the 1970s opposition to local nuclear power led to the creation of a campaign for sustainable solutions for the energy needs of the city. A network of environmentalists, research organizations, and businesses was established, helping the agenda of a sustainable city push forward.\nTaking advantage of Freiburg’s location, educated and active residents, and political priorities invested in the environment and economy has led Freiburg to be considered a Solar Capital. Along with high solar electricity rates, Freiburg hosts such innovations as the world’s first football stadium with its own solar power plant and the world’s first self-sustaining solar energy building. In terms of both ecology and economy, Freiburg has been extremely successful in the fields of research and marketing of renewable energy. The Freiburg science network and solar industry embraces many research institutions, like the Fraunhofer Institute for Solar Energy Systems ISE, Europe’s largest solar research institute.\nIn addition to solar initiatives, over the last four decades Freiburg has made improvements to their transportation systems. Freiburg has over 500 km of bicycle paths and more than 5000 bicycle parking spaces as well as car-free centers, 30 kph zones, a region wide bus service, and tram lines.\nLong before it was taken seriously Freiburg was resolving to cut carbon dioxide emissions. In 1966, the city resolved to lower carbon dioxide by 25 percent by 2010. Although they did not reach their initial goal by 2010, they are continually extending their goals. By 2030, they resolved to cut carbon dioxide emissions by 40 percent and be climate neutral by 2050.\n\nFreiburg also focuses initiatives on waste management. Paper products are composed to 80 percent recycled materials. Financial incentive programs, like discounts for collective waste disposal and people who compost, are used to increase waste avoidance. Since 2005, Freiburg’s non-recyclable waste has been incinerated and the heat energy released is converted to supply electricity to almost 25,000 households in the city.\nFreiburg is a green city. 43 percent of borough area is woodland. In 2001, the Freiburg Woodland Convention was adopted and since 2009, the city officially supported the Freiburg Convention on the Protection of Ancient Woodland. For over 20 years Freiburg has worked to maintain their public parks with principles that work with nature: they no longer use pesticides, grass is mown less, and almost 50,000 trees line streets and parks.\n\nStockholm in Sweden has been an environmentally focused city that is redeveloping itself to become an eco-city through efficient urban planning and resource use. Stockholm has established six environmental goals, called Vision 2030, that act as the foundation of this initiative. These goals include development of efficient transportation, sustainable energy, land, and water use, waste treatment improvements, and safe building and product materials. Beyond Vision 2030, Stockholm is planning to be fossil fuel free by 2050.\n\nIn terms of urban planning, Stockholm currently requires mandatory reuse of land before urban sprawl can continue. This policy has led to complete revitalization of run-down and abandoned industrial areas that have been transformed into modern, efficient and integrated residential and business communities. The Hammarby Sjostad district of Stockholm is the primary example of this practice, as this resurrected industrial area has become twice as energy efficient as the rest of the city after an environmentally focused redevelopment.\n\nThese gains are measured by the environmental load profile of the area, a life-cycle assessment tool developed by the City of Stockholm, the Royal Institute of Technology, and a consultancy firm. This unique measure allows for environmental performance analyses, on both the small and large scale, in terms of environmental costs and benefits. This comprehensive measure has allowed Stockholm to quantify their environmental progress and could be applied as a decision-making tool in other cities or districts to aid their environmental efforts.\n\nStockholm has pursued green development and optimization of urban systems and achieved results. These efforts were recognized in 2010 by European Union, which deemed Stockholm the European Green Capital for “leading the way towards environmentally friendly urban living.”\n\nUrban forests\n\nIn Adelaide, South Australia (a city of 1.3 million people) Premier Mike Rann (2002 to 2011) launched an urban forest initiative in 2003 to plant 3 million native trees and shrubs by 2014 on 300 project sites across the metro area. The projects range from large habitat restoration projects to local biodiversity projects. Thousands of Adelaide citizens have participated in community planting days. Sites include parks, reserves, transport corridors, schools, water courses and coastline. Only trees native to the local area are planted to ensure genetic integrity. Premier Rann said the project aimed to beautify and cool the city and make it more liveable; improve air and water quality and reduce Adelaide's greenhouse gas emissions by 600,000 tonnes of C0 a year. He said it was also about creating and conserving habitat for wildlife and preventing species loss.\n\nSolar power\n\nThe Rann government also launched an initiative for Adelaide to lead Australia in the take-up of solar power. In addition to Australia's first 'feed-in' tariff to stimulate the purchase of solar panels for domestic roofs, the government committed millions of dollars to place arrays of solar panels on the roofs of public buildings such as the Museum, Art Gallery, Parliament, Adelaide Airport, 200 schools and Australia's biggest rooftop array on the roof of Adelaide Showgrounds' convention hall which was registered as a power station.\n\nWind power\n\nSouth Australia went from zero wind power in 2002 to wind power making up 26% of its electricity generation by October 2011. In 5 years to 2011 there was a 15% drop in emissions, despite strong economic growth.\n\nWaste recycling\n\nFor Adelaide the South Australian government also embraced a Zero Waste recycling strategy, achieving a recycling rate of nearly 80% by 2011 with 4.3 million tonnes of materials diverted from landfill to recycling. On a per capita basis this was the best result in Australia, the equivalent of preventing more than a million tonnes of C02 entering the atmosphere. In the 1970s container deposit legislation was introduced. Consumers are paid a 10 cent rebate on each bottle/can/container they return to recycling. In 2009 non-reusable plastic bags used in supermarket checkouts were banned by the Rann Government preventing 400 million plastic bags per year entering the litter stream. In 2010 Zero Waste SA was commended by a UN Habitat Report entitled 'Solid Waste Management in the World Cities'.\n\nDespite the sustainability, efficiency and other established benefits of ecocities, actual implementation can be difficult to attain. Existing infrastructure, both in terms of the physical city layout and existing local bureaucracy, are major, often insurmountable, obstacles to large-scale sustainable development. The high cost of the technological integration necessary for eco-city development is a major challenge, as many cities either can’t afford, or are not willing to take on, the extra costs. New-build eco-cities avoid these problems.\n\nChallenges associated with planning and managing sustainable programs are also large. Cities that want to become more sustainable are faced with retrofitting existing structures and concurrent management of sustainable urban expansion and development. The costs and infrastructure needed to manage these large scale, two-pronged projects are great, and beyond the capabilities of most cities. In addition, many cities around the world are currently struggling to maintain the status quo, with budgetary issues, high rates of poverty, transportation inefficiencies, and rapid population growth encouraging reactive, coping policy. While there are many examples worldwide, the development of ecocities is still limited due to the vast challenges and high costs associated with sustainability.\n\n"}
{"id": "33512760", "url": "https://en.wikipedia.org/wiki?curid=33512760", "title": "Ethnic identity development", "text": "Ethnic identity development\n\nEthnic identity development or ethnic-racial identity (ERI) development includes the identity formation in an individual's self-categorization in, and psychological attachment to, (an) ethnic group(s). Ethnic identity is characterized as part of one’s overarching self-concept and identification. It is distinct from the development of ethnic group identities.\n\nWith some few exceptions, ethnic and racial identity development is associated positively with good psychological outcomes, psychosocial outcomes (e.g., better self-beliefs, less depressive symptoms), academic outcomes (e.g., better engagement in school), and health outcomes (e.g., less risk of risky sexual behavior or drug use).\n\nDevelopment of ethnic identity begins during adolescence but is described as a process of the construction of identity over time due to a combination of experience and actions of the individual and includes gaining knowledge and understanding of in-group(s), as well as a sense of belonging to (an) ethnic group(s). It is important to note that given the vastly different histories of various racial groups, particularly in the United States, that ethnic and racial identity development looks very different between different groups, especially when looking at minority (e.g., Black American) compared to majority (e.g., White American) group comparisons.\n\nEthnic identity is sometimes interchanged with, held distinct from, or considered as overlapping with racial, cultural and even national identities. This disagreement in the distinction (or lack thereof) between these concepts may originate from the incongruity of definitions of race and ethnicity, as well as the historic conceptualization of models and research surrounding ethnic and racial identity. Research on racial identity development emerged from the experiences of African Americans during the civil rights movement, however expanded over time to include the experiences of other racial groups. The concept of racial identity is often misunderstood and can have several meanings which are derived from biological dimensions and social dimensions. Race is socially understood to be derived from an individual's physical features, such as white or black skin tone. The social construction of racial identity can be referred as a sense of group or collective identity based on one's perception that he or she shares a common heritage with a particular racial group. Racial identity is a surface-level manifestation based on what people look like yet has deep implications in how people are treated.\n\nGenerally, group level processes of ethnic identity have been explored by social science disciplines, including sociology and anthropology. In contrast, ethnic identity research within psychology usually focuses on the individual and interpersonal processes. Within psychology, ethnic identity is typically studied by social, developmental and cross-cultural psychologists.\nModels of ethnic development emerged both social and developmental psychology, with different theoretical roots.\n\nEthnic identity emerged in social psychology out of social identity theory. Social identity theory posits that belonging to social groups (e.g. religious groups or occupational groups) serves an important basis for one’s identity. Membership in a group(s), as well as one’s value and emotional significance attached to this membership, is an important part of one’s self-concept. One of the earliest statements of social identity was made by Kurt Lewin, who\nemphasized that individuals need a firm sense of group identification\nin order to maintain a sense of well-being. Social identity theory emphasizes a need to maintain a positive sense of self. Therefore in respect to ethnic identity, this underscores affirmation to and salience of ethnic group membership(s). In light of this, affirmation of ethnicity has been proposed to be more salient among groups who have faced greater discrimination, in order to maintain self-esteem. There has also been research on family influences, such as cultural values of the family. Also, specific aspects of parenting, such as their racial socialization of youth, can contribute to the socialization of adolescents.\n\nRelatedly, collective identity is an overarching framework for different types of identity development, emphasizing the multidimensionality of group membership. Part of collective identity includes positioning oneself psychologically in a group to which you share some characteristic(s). This positioning does not require individuals to have direct contact with all members of the group. The collective identity framework has been related to ethnic identity development, particularly in recognizing the importance of personal identification of ethnicity through categorical membership. Collective identity also includes evaluation of one’s category. This affective dimension is related to the importance of commitment and attachment toward one’s ethnic group(s). A behavioral component of collective identity recognizes that individuals reflect group membership through individual actions, such as language usage, in respect to ethnic identity.\n\nIdentity becomes especially salient during adolescence as recognized by Erik Erikson’s stage theory of psychosocial development. An individual faces a specific developmental crisis at each stage of development. In adolescence, identity search and development are critical tasks during what is termed the ‘Identity versus Role-confusion’ stage.\n\nAchievement of this stage ultimately leads to a stable sense of self. The idea of an achieved identity includes reconciling identities imposed on oneself with one’s need to assert control and seek out an identity that brings satisfaction, feelings of industry and competence. In contrast, identity confusion occurs when individuals fail to achieve a secure identity, and lack clarity about their role in life.\n\nJames Marcia elaborated on Erik Erikson’s model to include identity formation in a variety of life domains. Marcia’s focus of identity formation includes two processes which can be applied to ethnic identity development: an exploration of identity and a commitment. Marcia defines four identity statuses which combines the presence or absence of the processes of exploration and commitment: Identity diffusion (not engaged in exploration or commitment), identity foreclosure (a lack of exploration, yet committed), moratorium (process of exploration without having made a commitment), and identity achievement (exploration and commitment of identity).\n\nResearchers believe and have frequently reported that older individuals are more likely to be in an achieved identity status than younger people. Evidence shows that increasing age and a wide range of life experiences helps individuals develop cognitive skills. This combination of age, life experiences, and improved cognitive skills helps adolescents and young adults find their authentic selves. Adolescents with strong commitments to their ethnic identities also tend to explore these identities more than their peers.\n\nWhile children in early to middle childhood develop the ability to categorize themselves and others using racial and ethnic labels, it is largely during adolescence that ethnic and racial identity develops. Adriana J. Umaña-Taylor and colleagues write about the following concepts as playing key roles during this stage:\n\nCognitive milestones include: abstract thinking, introspection, metacognition, and further development of social-cognitive abilities.\n\nPhysiological changes include puberty and development of body image\n\nSocial and environmental context includes: family, peers, social demands and transitions, navigating an expanding world, and media\n\nERI components about process: \n\nERI components about content: \n\nJean Phinney’s model of ethnic identity development is a multidimensional model, with theoretical underpinnings of both Erikson and Marcia. In line with Erikson's identity formation, Phinney focuses on the adolescent, acknowledging significant changes during this time period, including greater abilities in cognition to contemplate ethnic identity, as well as a broader exposure outside of their own community, a greater focus on one's social life, and an increased concern for physical appearance.\n\nPhinney's Three Stage Progression: \nBroadly, socialization in the context of ethnic identity development refers to the acquisition of behaviors, perceptions, values, and attitudes of an ethnic group(s). This process recognizes that feelings about one’s ethnic group(s) can be influenced by family, peers, community, and larger society. These contextual systems or networks of influence delineate from ecological systems theory. These systems influence children’s feelings of belonging and overall affect toward ethnic group(s). Children may internalize both positive and negative messages and therefore hold conflicting feelings about ethnicity. Socialization highlights how early experiences for children are considered crucial in regards to their ethnic identity development.\n\n\nMore recently, Phinney has focused on the continuous dimensions of one's exploration and commitment to one's ethnic group(s), rather than on distinct identity statuses.\n\nResearch reveals ethnic identity development is related to psychological well-being. Ethnic identity has been linked with positive self-evaluation and self-esteem. Ethnic identity development has also been shown to serve as a buffer between perceived discrimination and depression.\nSpecifically, commitment of an ethnic identity may help to abate depressive symptoms experienced soon after experiencing discrimination, which in turn alleviates overall stress. Researchers posit commitment to an ethnic identity group(s) is related to additional resources accumulated through the exploration process, including social support. \nEthnic identity development has been linked to happiness and decreased anxiety. Specifically, regard for one’s ethnic group may buffer normative stress. Numerous studies show many positive outcomes associated with strong and stable ethnic identities, including increased self-esteem, improved mental health, decreased self-destructive behaviors, and greater academic achievement. In contrast, empirical evidence suggests that ethnic identity exploration may be related to vulnerability to negative outcomes, such as depression. Findings suggest this is due to an individual’s sensitivity to awareness of discrimination and conflicts of positive and negative images of ethnicity during exploration. Also, while commitment to an ethnic group(s) is related to additional resources, exploration is related to a lack of ready-access resources.\n\nStudies have found that in terms of family cohesion, the closer adolescents felt to their parents, the more they reported feeling connected to their ethnic group. Given the family is a key source of ethnic socialization, closeness with the family may highly overlap with closeness with one’s ethnic group. Resources like family cohesion, proportion of same-ethnic peers, and ethnic centrality act as correlates of within-person change in ethnic identity, but it is only on the individual level and not as adolescents as a group.\n\nEthnic identity development has been conceptualized and researched primarily within the United States. Due to the fact the individuals studied are typically from the United States, it may not be appropriate to extend findings or models to individuals in other countries. Some research has been conducted outside of the United States, however a majority of these studies were in Europe or countries settled by Europeans.\n\nFurther, researchers also suggest that racial and ethnic identity development must be viewed, studied, and considered alongside the other normative developmental processes (e.g., gender identity development) and cannot be considered in a vacuum - racial and ethnic identity exist in particular contexts.\nResearch considers some studies of ethnic developments cross-sectional in design. This type of design pales in comparison to longitudinal design whose topic of investigation is developmental in nature. This is because cross-sectional studies collect data at or around the same time from multiple individuals of different ages of interest, instead of collecting data over multiple time points for each individual in the study, which would allow the researcher to compare change for individuals over time, as well as differences between individuals.\n\nAnother research consideration in the field is why certain ethnic and racial groups are looking towards their own expanding community for mates instead of continuing interracial marriages. An article in \"The New York Times\" explained that Asian-American couples have been kicking the trend and finding Asian mates because it gives them resurgence of interest in language and ancestral traditions. Further research can be found and explored throughout the many different racial and ethnic groups.\n\nSome researchers question the number of dimensions of ethnic identity development. For example, some measures of ethnic identity development include measures of behaviors, such as eating ethnic food or participating in customs specific to an ethnic group. One argument is that while behaviors oftentimes express identity, and are typically correlated with identity, ethnic identity is an internal structure that can exist without behavior. It has been suggested one can be clear and confident about one's ethnicity, without wanting to maintain customs. Others have found evidence of a behavioral component of ethnic identity development, separate from cognition and affect, and pertaining to one's ethnic identity.\n\nEthnic identity development points toward the importance of allowing an individual to self-identify ethnicity during data collection. This method helps us collect the most accurate and relevant information about the subjective identification of the participant, and can be useful in particular with respect to research with multiethnic individuals.\n\n"}
{"id": "1608493", "url": "https://en.wikipedia.org/wiki?curid=1608493", "title": "Experimenter's regress", "text": "Experimenter's regress\n\nIn science, experimenter's regress refers to a loop of dependence between theory and evidence. In order to judge whether evidence is erroneous we must rely on theory-based expectations, and to judge the value of competing theories we rely on evidence. Cognitive bias affects experiments, and experiments determine which theory is valid. This issue is particularly important in new fields of science where there is no community consensus regarding the relative values of various competing theories, and where sources of experimental error are not well known. \n\nIn a true scientific process, no consensus does exist and no consensus can exist as the process is conducted scientifically in the pursuit of knowledge. If any party involved in the process stands to personally lose or gain from the result, the process will be flawed and unscientific. \n\nIn a true scientific process, a theory is formed after a scientist - amateur or professional - has observed a phenomenon and has asked \"why?\" as a result. The theory is the answer the scientist creates using logic and reason to explain the phenomenon. The scientist then focuses on how to conduct experiments to test the theory incrementally and the theory is either proven to be true or false through repeatable and legitimate experimentation. Legitimate scientific experiments conducted by the person who formulated the theory seek to prove the theory false rather than prove it true specifically to counter the effects of bias.\n\nIf experimenter's regress acts a positive feedback system, it can be a source of pathological science. An experimenter's strong belief in a new theory produces confirmation bias, and any biased evidence they obtain then strengthens their belief in that particular theory. Neither individual researchers nor entire scientific communities are immune to this effect; see N-rays and Polywater.\n\nExperimenter's regress is a typical relativistic phenomenon in the Empirical Programme of Relativism (EPOR). EPOR is very much concerned with a focus on social interactions, by looking at particular (local) cases and controversial issues in the context in which they happen. In EPOR, all scientific knowledge is perceived to be socially constructed and is thus \"not given by nature\".\n\nIn his article \"Son of seven sexes: The Social Destruction of a Physical Phenomenon\", Harry Collins argued that scientific experiments are subject to what he calls \"experimenter's regress\". The outcome of a phenomenon that is studied for the first time is always uncertain and judgment in these situations, about what matters, requires considerable experience, tacit and practical knowledge. When a scientist runs an experiment, and a result comes out of this, he can never be sure whether this is the result he'd expected. The result looks good because he knows the experiment he conducted was right or that the results are wrong. The scientist, in other words, has to get the right answers in order to know that the experiment is working, or to know that the experiment is working, to get the right answer.\n\nExperimenter's regress occurs at the \"research frontier\" where the outcome of research is uncertain, for the scientist is dealing with \"novel phenomena\". Collins puts it this way: \"usually, successful practice of an experimental skill is evident in a successful outcome to an experiment, but where the detection of a novel phenomenon is in question, it is not clear what should count as a 'successful outcome' – detection or non detection of the phenomenon\" (Collins 1981: 34). In new fields of research where no paradigm has yet evolved and where no consensus exists as what counts as proper research, experimenter's regress is a problem that often occurs. Also in situations where there is much controversy over a discovery or claim due to opposing interests, dissenters will often question experimental evidence that founds a theory.\n\nBecause for Collins, all scientific knowledge is socially constructed, there are no purely cognitive reasons or objective criteria that determine whether a claim is valid or not. The regress must be broken by \"social negotiation\" between scientists in the respective field. In the case of Gravitational Radiation, Collins notices that Weber, the scientist who is said to have discovered the phenomenon, could refute all the critique and had \"a technical answer for every other point\" but he was not able to convince other scientists and in the end he was not taken seriously anymore.\n\nThe problems that come with \"experimenter's regress\" can never be fully avoided because scientific outcomes in EPOR are seen as negotiable and socially constructed. Acceptance of claims boils down to persuasion of other people in the community. Experimenter's regress can always become a problem in a world where \"the natural world in no way constrains what is believed to be\". Moreover, attempts to falsify a claim, by replicating an experiment, are hard and problematic for it involves tacit knowledge (i.e. unarticulated knowledge), matters of time and money and replication of exact similar conditions, which is hard. Tacit knowledge can never be fully articulated or translated into a set of rules.\n\nSome commentators have argued that Collins's \"experimenter's regress\" is foreshadowed by Sextus Empiricus' argument that \"if we shall judge the intellects by the senses, and the senses by the intellect, this involves circular reasoning inasmuch as it is required that the intellects should be judged first in order that the intellects may be tested [hence] we possess no means by which to judge objects\" (quoted after Godin & Gingras 2002: 140). Others have extended Collins's argument to the cases of theoretical practice (\"theoretician's regress\"; Kennefick 2000) and computer simulation studies (\"simulationist's regress\"; Gelfert 2011).\n\n\n\n"}
{"id": "51698", "url": "https://en.wikipedia.org/wiki?curid=51698", "title": "Extended real number line", "text": "Extended real number line\n\nIn mathematics, the affinely extended real number system is obtained from the real number system by adding two elements: and (read as positive infinity and negative infinity respectively). These new elements are not real numbers. It is useful in describing various limiting behaviors in calculus and mathematical analysis, especially in the theory of measure and integration. The affinely extended real number system is denoted formula_1 or or .\n\nWhen the meaning is clear from context, the symbol is often written simply as .\n\nWe often wish to describe the behavior of a function formula_2, as either the argument formula_3 or the function value formula_2 gets \"very big\" in some sense. For example, consider the function\n\nThe graph of this function has a horizontal asymptote at y = 0. Geometrically, as we move farther and farther to the right along the formula_3-axis, the value of formula_7 approaches 0. This limiting behavior is similar to the limit of a function at a real number, except that there is no real number to which formula_3 approaches.\n\nBy adjoining the elements formula_9 and formula_10 to formula_11, we allow a formulation of a \"limit at infinity\" with topological properties similar to those for formula_11.\n\nTo make things completely formal, the Cauchy sequences definition of formula_11 allows us to define formula_9 as the set of all sequences of rationals which, for any formula_15, from some point on exceed formula_16. We can define formula_10 similarly.\n\nIn measure theory, it is often useful to allow sets that have infinite measure and integrals whose value may be infinite.\n\nSuch measures arise naturally out of calculus. For example, in assigning a measure to formula_11 that agrees with the usual length of intervals, this measure must be larger than any finite real number. Also, when considering improper integrals, such as\n\nthe value \"infinity\" arises. Finally, it is often useful to consider the limit of a sequence of functions, such as \n\nWithout allowing functions to take on infinite values, such essential results as the monotone convergence theorem and the dominated convergence theorem would not make sense.\n\nThe affinely extended real number system turns into a totally ordered set by defining formula_21 for all formula_22. This order has the desirable property that every subset has a supremum and an infimum: it is a complete lattice.\n\nThis induces the order topology on formula_1. In this topology, a set formula_24 is a neighborhood of formula_9 if and only if it contains a set formula_26 for some real number formula_22, and analogously for the neighborhoods of formula_10. formula_29 is a compact Hausdorff space homeomorphic to the unit interval formula_30. Thus the topology is metrizable, corresponding (for a given homeomorphism) to the ordinary metric on this interval. There is no metric that is an extension of the ordinary metric on formula_11.\n\nWith this topology the specially defined limits for formula_3 tending to formula_9 and formula_10, and the specially defined concepts of limits equal to formula_9 and formula_10, reduce to the general topological definitions of limits.\n\nThe arithmetic operations of formula_11 can be partially extended to formula_38 as follows:\n\nFor exponentiation, see Exponentiation#Limits of powers.\nHere, \"formula_40\" means both \"formula_41\" and \"formula_42\", while \"formula_43\" means both \"formula_44\" and \n\nThe expressions formula_45 and formula_46 (called indeterminate forms) are usually left undefined. These rules are modeled on the laws for infinite limits. However, in the context of probability or measure theory, formula_47 is often defined as formula_48.\n\nThe expression formula_49 is not defined either as formula_9 or formula_10, because although it is true that whenever formula_52 for a continuous function formula_2 it must be the case that formula_54 is eventually contained in every neighborhood of the set formula_55, it is \"not\" true that formula_54 must tend to one of these points. An example is formula_57 which is of the form formula_54 but does not tend to either formula_9 or formula_10 when formula_61. For instance, formula_62 but formula_63 does not exist because formula_64 but formula_65. (The modulus formula_66, nevertheless, does approach formula_9.)\n\nWith these definitions formula_38 is not even a semigroup, let alone a group, a ring or a field, like formula_11 is one. However, it still has several convenient properties:\n\nIn general, all laws of arithmetic are valid in formula_38 as long as all occurring expressions are defined.\n\nSeveral functions can be continuously extended to formula_38 by taking limits. For instance, one defines formula_91 etc.\n\nSome singularities may additionally be removed. For example, the function formula_92 can be continuously extended to formula_38 (under \"some\" definitions of continuity) by setting the value to formula_9 for formula_95, and formula_48 for formula_97 and formula_98. The function formula_99 can \"not\" be continuously extended because the function approaches formula_10 as formula_3 approaches 0 from below, and formula_9 as formula_3 approaches formula_48 from above.\n\nCompare the projectively extended real line, which does not distinguish between formula_9 and formula_10. As a result, on one hand a function may have limit formula_107 on the projectively extended real line, while in the affinely extended real number system only the absolute value of the function has a limit, e.g. in the case of the function formula_99 at formula_95. On the other hand\ncorrespond on the projectively extended real line to only a limit from the right and one from the left, respectively, with the full limit only existing when the two are equal. Thus formula_112 and formula_113 cannot be made continuous at formula_114 on the projectively extended real line.\n\n\n"}
{"id": "146699", "url": "https://en.wikipedia.org/wiki?curid=146699", "title": "Fine (penalty)", "text": "Fine (penalty)\n\nA fine or mulct is money that a court of law or other authority decides has to be paid as punishment for a crime or other offence. The amount of a fine can be determined case by case, but it is often announced in advance.\n\nThe most usual use of the term is for financial punishments for the commission of crimes, especially minor crimes, or as the settlement of a claim. A synonym, typically used in civil law actions, is mulct.\n\nOne common example of a fine is money paid for violations of traffic laws. Currently in English common law, relatively small fines are used either in place of or alongside community service orders for low-level criminal offences. Larger fines are also given independently or alongside shorter prison sentences when the judge or magistrate considers a considerable amount of retribution is necessary, but there is unlikely to be significant danger to the public. For instance, fraud is often punished by very large fines since fraudsters are typically banned from the position or profession they abused to commit their crimes.\n\nFines can also be used as a form of tax. Money for bail may be applied toward a fine.\n\nA day-fine is a fine that, above a minimum, is based on personal income.\n\nSome fines are small, such as for loitering, for which fines range from about $25 to $100. In some areas of the United States (for example California, New York, Texas, and Washington D.C.), fines for petty crimes, such as criminal mischief (shouting in public places, projecting an object at a police car) range from $2500 to $5000.\n\nIn the Magistrates' Courts Act 1980, unless the context otherwise requires, the expression \"fine\", except for the purposes of any enactment imposing a limit on the amount of any fine, includes any pecuniary penalty or pecuniary forfeiture or pecuniary compensation payable under a conviction.\n\nIn section 32 of that Act, the expression \"fine\" includes a pecuniary penalty but does not include a pecuniary forfeiture or pecuniary compensation.\n\nIn sections 15 to 32 and 48 of the Criminal Law Act 1977, the expression \"fine\" includes any pecuniary penalty.\n\nIn England, there is now a system whereby the court gives the offender a 'fine card' which is somewhat like a credit card; at any shop that has a paying-in machine he pays the value of the fine to the shop, which then uses the fine card to pass that money on to the court's bank account.\n\nA related concept is the fixed penalty notice, a pecuniary penalty for some minor crimes that can be either accepted (instead of prosecution, thus saving time and paperwork, or taken to court for normal proceedings for that crime. While technically not a fine, which, under the Bill of Rights 1689, may be levied only following a conviction, it serves the same purpose of punishment.\n\nEarly examples of fines include the weregild or blood money payable under Anglo-Saxon common law for causing a death. The murderer would be expected to pay a sum of money or goods dependent on the social status of the victim.\n\nSee also English criminal law#General power to impose a fine on indictment.\n\nThe Dutch Criminal Code \"(Dutch: Wetboek van Strafrecht (WvSr))\" doesn't contain specific amounts for fines for every violation of the law. Instead of that the Criminal Code provides six fine categories. Every penalty clause of the Criminal Code contains a fine category. The categories are:\n\nThese sums are only an upper limit, it's up to the judge or the prosecutor to determine the exact sum of the fine. However, the amount of the fine must be at least €3. The sums of categories are always 1, 10, 20, 50, 200 and 2000 times the amount of the first category. In addition to the fine, the convict also has to pay an administration fee of €9. The amounts are established by the government, via a royal order.\n\nWhen the judge convicts an individual to a fine, the judge must also set a term of substitute imprisonment. This substitute imprisonment will be executed in the case that the fine remains unpaid. The judge may count one day imprisonment for every unpaid €25, however normally judges reckon one day for every €50 which stays unpaid. Though, the substitute imprisonment must be at least one day (even though the fine was €3) and cannot exceed one year (even though the fine was €100.000).\n\nOnce a person is irrevocable convicted to a fine, it's up to the public prosecutor to collect the fine. To do so, the cjib \"(centraal justiteel incassobureau (English: central judicial collection agency))\" is established.\n\nFirst, the CJIB will send the convict the fine. If the convict pays the fine the case is closed (by paying, the convict loses the right to go into appeal as well), if he doesn't, the case will be continued. The CJIB will then send the convict a reminder, though this reminder will contain an increment of €15. If this doesn't lead to the payment of the fine, the CJIB will send another reminder, now with a raise of 20%, however, the raise must be at least €30. When the fine continues to be unpaid, the CJIB will instruct a bailiff to collect the fine nonetheless. This bailiff may, for example, seize the convict's income and sell his possessions. If these measures do not result in the full collection of the fine, the bailiff will return the case to the prosecutor. The prosecutor will order the police to arrest the convict, in order to execute the earlier written substitute imprisonment. The length of the imprisonment will be percentage-wise reduced if the convict has paid a sum, but not the entire amount of the fine. After the substitute imprisonment the convict will be a free man again. He also won't have to pay the fine anymore and the case will be closed.\n\nBefore 1 September 1990, all traffic violations were punished via the criminal law. The suspects were first offered a sort of plea bargain. This mostly contains a fine. If the suspect didn't pay the fine of this plea bargain, the public prosecutor had to open a criminal case, otherwise he wasn't authorized to collect the fine through force. The case had to be withdrawn when the capacity of the courts or the prosecutor's office didn't allow the start of a criminal case for a traffic violation. This was the case very often. This situation led to a negative spiral, because traffic offenders hoped and expected their case to be withdrawn, and didn't pay the plea bargain fine. This led to a growing pressure on the capacity of the courts, which causes more sepots (decisions not to prosecute). This encouraged more offenders not to pay, etc.\n\nIn order to stop this spiral, the secretary general of the justice department (at that time), Dr. Albert Mulder, designed a new system of law enforcement. Under this new system, the government acquired the right of summary foreclosure. The summary foreclosure means that the CJIB can execute the fine directly, unless the fined subject goes to appeal.\n\nThe system regarding frequently committed traffic violations is regulated by Administrative Enforcement of traffic Rules Act \"(Dutch: Wet administratiefrechtelijke handhaving verkeersvoorschriften (WAHV))\". According to the WAHV the maximum sum of the administrative fine is the same as the maximum amount of the first category (Art. 2 section 3 WvSr Criminal Code). The exact fine per violation is determined by an annex of the WAHV. In addition to the fine, the fined subject will also have to pay €9 administration costs as well. The amount of the administration costs will also be determined by the minister.\n\nOnce a subject has been fined by an officer or photographed by a speed camera, he will receive a decision within four months. This decision will contain a short description of the violation, the place and time the violation was committed and sum of the fine.\nThe subject will have two choices now. He can pay the fine or he can go into appeal. In contrast to the court imposed fine, when the subject has paid the fine, he will keep the right to go into appeal. The subject can go into appeal within six weeks. In the first instance, the subject appeals to the public prosecutor. The prosecutor shall withdraw the fine completely when he thinks the appellant has right. He will lower the sum of the fine if he thinks that the suspect is partially right. If the prosecutor thinks that the suspect isn't right, he will uphold the fine. As long as the prosecutor has not made a decision on the appeal, the suspect does not have to pay the fine yet.\n\nOnce the prosecutor has made a decision, the suspect will again have two choices. He can pay or he goes into appeal at the sub-district judge of his arrondissement (or the arrondissement of the place where the disputed violation was committed). But now, the suspect has to pay the fine as a surety. If the suspect doesn't pay the surety, the judge will declare him inadmissible (thus the fine will be upheld). The judge will have the same choices as the prosecutor. He can withdraw the fine, lower the fine, or uphold the fine.\n\nIf the (remaining) fine is higher than €70 and the suspect or/nor the prosecutor doesn't agree with the sub-district judge's verdict, the suspect or the prosecutor can go into appeal for the last time. He does that at the court of appeal of Arnhem-Leeuwarden in Leeuwarden. This appeal will be in writing, unless the appellant, per se, wants to do it orally. If the fine is lower than €70, or the appellant's appeal is rejected in Leeuwarden, there will be no legal remedies anymore and the case will be closed. The appellant's surety will be transformed to a fine.\n\n"}
{"id": "20762753", "url": "https://en.wikipedia.org/wiki?curid=20762753", "title": "Glass crusher", "text": "Glass crusher\n\nA glass crusher provides for pulverization of glass to a yield size of 2\" or less.\nRecycling operations may range from simple, manually-fed, self-contained machines to extravagant crushing systems complete with screens, conveyors, crushers and separators. All non-glass contaminants must generally be removed from the glass prior to recycling. The processes used in glass crushing for recycling involves the same methods used by the aggregate industry for crushing rock into sand (rock crusher).\n\nThe use of VSI crushers in large scale operations allow the production of up to 125 tons per hour of crushed glass cullet.\nVSI crushers use a high speed rotor with wear-resistant tips and a crushing chamber designed to 'throw' the glass against. The VSI crushers utilize velocity rather than surface force as the predominant force to break glass as this allows the breaking force to be applied evenly both across the surface of the material as well as through the mass of the material. In its shattered state, glass has a jagged and uneven surface. Applying surface force (pressure) results in unpredictable and typically non-cubicle particles. As glass is 'thrown' by a VSI rotor against a solid anvil, it fractures and breaks along fissures. Final particle size can be controlled by 1) the velocity at which the glass is thrown against the anvil and 2) the distance between the end of the rotor and the impact point on the anvil. The product resulting from VSI crushing is generally of a consistent cubicle shape which may optimize yield in consumptive applications such as the fabrication of fiberglass, ceramic ware, flux agents and abrasives. Due to the highly abrasive nature of the glass material, a VSI crushing process is generally preferred over Horizontal Shaft Impact and most other crushing methods with higher maintenance and lower wear part lives.\n\nVSI crushers generally utilize a high speed spinning rotor at the center of the crushing chamber and an outer impact surface of either abrasive resistant metal anvils or crushed glass (or rock in an aggregate applications). Utilizing cast metal surfaces 'anvils' are traditionally referred to as a \"Shoe and Anvil VSI\". Utilizing crushed material on the outer walls of the crusher for new material to be crushed against is traditionally referred to as \"rock on rock VSI\".\n\n"}
{"id": "54801504", "url": "https://en.wikipedia.org/wiki?curid=54801504", "title": "Helen Peterson", "text": "Helen Peterson\n\nHelen Peterson (native name: Wa-Cinn-Ya-Win-Pi-Mi, August 3, 1915 – July 10, 2000) was a Cheyenne-Lakota activist and lobbyist. She was the first director of the Denver Commission on Human Relations. She was the second Native American woman to become director of the National Congress of American Indians at a time when the government wanted to discharge their treaty obligations to the tribes by eliminating their tribal governments through the Indian termination policy and forcing the tribe members to assimilate into the mainstream culture. She authored a resolution on Native American education, which was ratified at the second Inter-American Indian Conference, held in Cuzco, Peru. She created a model program for summer school programs on ethnic studies, which was used throughout the United States. In 1986, Peterson was inducted into the Colorado Women's Hall of Fame and the following year, her papers were donated to the Smithsonian's National Anthropological Archives.\n\nHelen Louise White was born on August 3 1915 on the Pine Ridge Indian Reservation in Bennett County, South Dakota to Lucy (née Henderson) and Robert B. White. She was given the native name Wa-Cinn-Ya-Win-Pi-Mi, meaning \"woman to trust and depend on\". The family lived in northern Nebraska and White attended Hay Springs High School, graduating in 1932. She went on to further her education at Chadron State College, studying business education. On August 29 1935, White married Richard F. Peterson in Garden County, Nebraska and she worked at the U.S. Land Use Resettlement Administration to pay their way through school. Richard enlisted in the war effort and Peterson had their only child, R. Max, soon after. In 1942, the couple divorced and Peterson moved with her mother to Denver.\n\nPeterson began work at the University of Denver as the executive director of the Rocky Mountain Council of Inter-American Affairs. In 1948, she was hired by the newly elected mayor, J. Quigg Newton, to work on the Commission on Community Telations. The mayor had a goal of desegregating the community and to do that, he needed voters willing to change the municipal charter. Working with Bernie Valdez, Director of the Denver Welfare Department, Peterson attempted to build bridges between the established Latin American citizens and the new migrant farm workers who had come to work on the beetroot farms. She went door to door in Hispanic neighborhoods, registering voters and organizing the community. Peterson developed cultural programs and met with city leaders to provide lecture series on issues, such as fair labor and housing laws. At the end of the year, she was made the director of the Committee on Human Relations, the first person to hold the post. In that capacity, she led a drive to hire minority workers and assisted the mayor in passing anti-discriminatory employment and housing regulations. In 1949, she was asked to go to Peru as an advisor to the United States delegation attending the Second Inter-American Indian Conference. She authored a resolution to improve education for indigenous people, which was ratified by the conference.\n\nIn 1953, Peterson was urged by Eleanor Roosevelt to move to Washington, D. C. and help reorganize the National Congress of American Indians (NCAI). The organization, founded in 1944 to fight against the government's Indian termination policy was in disarray, on the verge of bankruptcy, and was facing pressure from President Dwight D. Eisenhower for its dissolution. Because of Peterson's experience in organizing minority programs, she was able to slow the assimilationist aims of Congress and assist tribes in asserting their own sovereign rights. Peterson was hired to replace Frank George, who had in turn replaced Ruth Muskrat Bronson as executive director. \n\nEarly in 1954, Peterson scheduled an emergency conference with tribal leaders to discuss termination. The meeting was the largest gathering of protest that had ever been assembled by American Indians, and was scheduled in response to the passage of House concurrent resolution 108, which called for the end of federal responsibility for selected tribes, which were to be debated beginning on February 15, 1954. She and her mother prepared the materials for the conference on a hand cranked mimeograph machine in her basement. Another bill was introduced that year to eliminate competency restrictions on land transactions and required Peterson to mobilize tribal leaders to wire their congressmen to defeat the bill. At issue was whether property patents would be assigned by allotment directly to tribal members who had no real knowledge of property values or laws governing transfer, or whether the deeds to allotted property were held in trust until allottees actually had an understanding of property ownership and fair market value. Her efforts in advancing Native Americans and fighting against discriminatory legislation was recognized by the American Indian Exposition of Anadarko, Oklahoma, which named her the \"Outstanding Indian of 1955\".\n\nAs NCAI made progress in slowing termination, Peterson helped develop new tactics to protect Native rights, such as creating a summer school program with D'Arcy McNickle in 1956 for ethnic studies and convincing NBC to air a program on the policy and its effect on the Klamath Tribes in 1957. In 1958, Peterson and NCAI president Joseph R. Garry went to Puerto Rico to study the methods of Operation Bootstrap, which had transformed the economic relationship between the island and the United States government. They were hopeful that the program could be mirrored for Native Americans to become self-sufficient, but legislators refused to act. In 1960, at the invitation of Sol Tax, an anthropologist, Peterson met with McNickle and John Rainer to prepare materials for a conference to be held in Chicago the following year. Largely drafted by McNickle the, \"Declaration of Indian Purpose\" for the 1961 American Indian Chicago Conference contained provisions for a reversal of the termination policy to be replaced by programs focused on development of economic, educational, social and legal nature. The declaration also called for the Commissioner of Indian Affairs to be replaced by a Commission of six members, half of whom were Native American, to evaluate issues effecting tribes. As the conference date neared in June 1961, factions emerged. Some felt that the organized NCAI operated more in the manner of a non-Indian reform association, rather than one that used traditional methods to address problems, whereas others felt that its focus did not adequately represent the issues of tribal identity and reservation realities. By August, the factionalism which had become apparent in planning the convention, created calls for restructuring the NCAI and Peterson resigned.\n\nReturning to Denver in 1962, Peterson again took up the post as the director of the Commission on Community Relations. The Indian Relocation Act of 1956 caused a large influx of Native Americans to the Denver area, but Congress had failed to sufficiently fund the program. Peterson's office tried to fill the gap by providing social and employment services, as well as job training for Denver's Native American community. Though no longer employed at the NCAI, controversy continued and her replacement, Robert Burnette, accused both Peterson and Garry of mismanagement during their tenure. The dispute between Burnette and his supporters and Peterson and hers, continued through the 1960s dividing the NCAI membership. Burnette was forced out in 1964 and replaced by Vine Deloria Jr., who had the difficult task of trying to bring the organization back to financial stability and heal the factionalism. After eight years, of directing the Commission, Peterson accepted a position with the Bureau of Indian Affairs (BIA), working as a field liaison officer and coordinator with the United States Customs Service in Denver. In 1971, she returned to Washington, D. C. and served as the assistant for the Commissioner of Indian Affairs. In 1978, the BIA transferred her to serve as a tribal government services officer in Portland, Oregon. Focusing on treaty obligations and Indian health, she worked to ensure that federal, state, local and tribal governments worked together in serving the American Indian community. She remained with the BIA until her retirement in 1985.\n\nThe year after her retirement, Peterson was inducted into the Colorado Women's Hall of Fame. The following year, her papers were donated to the National Anthropological Archives of the Smithsonian Institution. When the National Museum of the American Indian Archive Center was created in 2007, her papers were transferred there. Upon her retirement, Peterson devoted her time to local and regional projects in and around Portland for the Episcopal Church. She remained an active member in the NCAI through the early 1990s, participating in the 1993 Albuquerque conference held at the University of New Mexico on developing inter-tribal relationships.\n\nPeterson died on July 10, 2000 in a nursing home in Vancouver, Washington. Peterson is credited with having led NCAI to stop, or at least slow, the termination movement while she served as director of NCAI. The ethnic studies program that she and McNickle developed for Colorado College between 1956 and 1970 became a model for universities throughout the United States.\n\n"}
{"id": "39211489", "url": "https://en.wikipedia.org/wiki?curid=39211489", "title": "Hierarchy of death", "text": "Hierarchy of death\n\nHierarchy of death is a phrase used by journalists, social scientists, and academics to describe disproportionate amounts of media attention paid to various incidents of death around the world.\n\nDefinitions of the hierarchy of death vary, but several themes remain consistent in terms of media coverage: domestic deaths trump foreign deaths, deaths in the developed world trump deaths in the developing world, deaths of whites trump deaths of darker skinned people, and deaths in ongoing conflicts garner relatively little media attention.\nBritish media commentator Roy Greenslade has been credited with coining the term while writing on the newsworthiness of those who died during The Troubles. Greenslade continues to critique the phenomenon, including media reactions to the Boston Marathon bombings. \n\nNPR discussed the disparity in media coverage between the 2015 Beirut bombings and the November 2015 Paris attacks, which happened with a day of each other.\n\nScottish journalist Allan Massie has also written on the topic.\n\nThe hierarchy of death has been compared to missing white woman syndrome.\n"}
{"id": "1209983", "url": "https://en.wikipedia.org/wiki?curid=1209983", "title": "High yellow", "text": "High yellow\n\nHigh yellow, occasionally simply yellow (dialect: yaller, yeller), is a term used to describe persons classified as black who also have a high proportion of white ancestry. The term was in common use in the United States at the end of the 19th century and the early decades of the 20th century, and is reflected in such popular songs of the era as \"The Yellow Rose of Texas\". It is now considered offensive.\n\n\"High\" is usually considered a reference to a social class system in which skin color (and associated ancestries) is a major factor, placing those of lighter skin (with more European ancestry) at the top and those of darker skin at the bottom. High yellows, while still considered part of the African-American ethnic group, were thought to gain privileges because of their skin and ancestry. \"Yellow\" is in reference to the usually very pale undertone to the skin color of members of this group, due to mixture with Europeans. Another reading of the etymology of the word \"high\" is that it is a slang word for \"very\", often used in Southern English, therefore \"very yellow\" (as opposed to brown).\n\nIn an aspect of colorism, \"high yellow\" was also related to social class distinctions among people of color. In post-Civil War South Carolina, according to one account by historian Edward Ball, \"Members of the colored elite were called 'high yellow' for their shade of skin\", as well as slang terms meaning snobbish. In New Orleans, the term \"high-yellow\" was associated with Creoles of colour \"brahmins\". In his biography of Duke Ellington, a native of Washington, D.C., David Bradbury wrote that Washington's\n\nIn some cases the confusion of color with class came about because some of the lighter-skinned blacks came from families of mixed heritage free before the Civil War, who had begun to accumulate education and property. In addition, some wealthier white planters made an effort to have their \"natural sons\" (the term for children outside of marriage who were produced with enslaved women) educated or trained as apprentices; some passed on property to them. For instance, in 1860, most of the 200 subscription students at Wilberforce College were the mixed-race sons of white planters, who paid for their education.\n\nThese social distinctions made the cosmopolitan Harlem more appealing to many blacks. The Cotton Club of the Prohibition era \"had a segregated, white-only audience policy and a color-conscious, 'high yellow' hiring policy for chorus girls\". It was common for lighter-skinned African Americans to hold \"paper bag parties,\" which admitted only those whose complexion was lighter than that of a brown paper bag.\n\nIn her 1942 \"Glossary of Harlem Slang\", Zora Neale Hurston placed \"high yaller\" at the beginning of the entry for colorscale, which ran:\n\nThe French author Alexandre Dumas, père was the son of a French mulatto general (born in Saint-Domingue but educated by his father in France) and his French wife. He was described as having skin \"with a yellow so high it was almost white\". In a 1929 review, \"Time\" referred to him as a \"High Yellow Fictioneer\".\n\nThe terminology and its cultural aspects were explored in Dael Orlandersmith's play \"Yellowman\", a 2002 Pulitzer Prize Finalist in drama. The play depicts a dark-skinned girl whose own mother \"inadvertently teaches her the pain of rejection and the importance of being accepted by the 'high yellow' boys\". One reviewer described the term as having \"the inherent, unwieldy power to incite black Americans with such intense divisiveness and fervor\" as few others.\n\nThe phrase survives in folk songs such as \"The Yellow Rose of Texas\", which originally referred to Emily West Morgan, a \"mulatto\" indentured servant apocryphally associated with the Battle of San Jacinto. Blind Willie McTell's song \"Lord, Send Me an Angel\" has its protagonist forced to choose among three women, described as \"Atlanta yellow\", \"Macon brown\", and a \"Statesboro blackskin\". Bessie Smith's song \"I've Got What It Takes\", by Clarence Williams, refers to \"a slick high yeller\" boyfriend who \"turned real pale\" when she wouldn't wait for him to get out of jail. Curtis Mayfield's song \"We the People Who Are Darker Than Blue\" makes reference to a \"high yellow girl\". In \"Big Leg Blues\", Mississippi John Hurt sings: \"Some crave high yellow. I like black and brown.\"\n\nOn the 1988 album Chalk Mark in a Rain Storm by Joni Mitchell, the song \"Dancin' Clown\" contains the lyrics \"Down the street comes last word Susie, she's high yellow, looking top nice.\"\n\nOn Ice Cube's album War and Peace Vol 2. released in 2000, the song \"Hello\" contains the lyrics \"I'm looking for a big yellow, in 6-inch stilettos\".\n\nAs recently as 2004, white R&B singer-songwriter Teena Marie released a song titled \"High Yellow Girl\", said to be about her daughter Alia Rose, who is biracial. The related phrase \"high brown\" was used in Irving Berlin's original lyrics for \"Puttin' on the Ritz\".\n\n"}
{"id": "5006843", "url": "https://en.wikipedia.org/wiki?curid=5006843", "title": "Hopf conjecture", "text": "Hopf conjecture\n\nIn mathematics, Hopf conjecture may refer to one of several conjectural statements from differential geometry and topology attributed to either Eberhard Hopf or Heinz Hopf.\n\nFor surfaces, this follows from the Gauss–Bonnet theorem. For four-dimensional manifolds, this follows from the finiteness of the fundamental group and Poincaré duality. The conjecture has been proved for manifolds of dimension 4\"k\"+2 or 4\"k\"+4 admitting an isometric torus action of a \"k\"-dimensional torus and for manifolds \"M\" admitting an isometric action of a compact Lie group \"G\" with principal isotropy subgroup \"H\" and cohomogeneity \"k\" such that\n\nIn a related conjecture, \"positive\" is replaced with \"nonnegative\".\n\nIn particular, the four-dimensional manifold \"S\"×\"S\" should admit no Riemannian metric with positive sectional curvature.\n\nThis topological version of Hopf conjecture for Riemannian manifolds is due to William Thurston. Ruth Charney and Michael Davis conjectured that the same inequality holds for a nonpositively curved piecewise Euclidean (PE) manifold.\n\nThis theorem was proved by Dmitri Burago and Sergei Ivanov.\n\n"}
{"id": "58221871", "url": "https://en.wikipedia.org/wiki?curid=58221871", "title": "International Day of Remembrance and Tribute to the Victims of Terrorism", "text": "International Day of Remembrance and Tribute to the Victims of Terrorism\n\nThe International Day of Remembrance and Tribute to the Victims of Terrorism was established by the United Nations General Assembly in 2017. It designated 21 August as the International Day of Remembrance and Tribute to the Victims of Terrorism in order to honor the victims and survivors of terrorism. In 2017 alone, nearly three-quarters of all deaths caused by terrorism were in just five countries: Afghanistan, Iraq, Nigeria, Somalia and Syria. According to a statement by the UN, the day is meant to allow victims of terrorism to have their needs supported and their rights upheld.\n"}
{"id": "24997119", "url": "https://en.wikipedia.org/wiki?curid=24997119", "title": "Internet art", "text": "Internet art\n\nInternet art (often referred to as net art) is a form of digital artwork distributed via the Internet. This form of art has circumvented the traditional dominance of the gallery and museum system, delivering aesthetic experiences via the Internet. In many cases, the viewer is drawn into some kind of interaction with the work of art. Artists working in this manner are sometimes referred to as net artists.\n\nInternet art can happen outside the technical structure of the Internet, such as when artists use specific social or cultural Internet traditions in a project outside it. Internet art is often—but not always—interactive, participatory, and multimedia-based. Internet art can be used to spread a message, either political or social, using human interactions.\n\nThe term \"Internet art\" typically does not refer to art that has been simply digitized and uploaded to be viewable over the Internet. This can be done through a web browser, such as images of paintings uploaded for viewing in an online gallery.\nRather, this genre relies intrinsically on the Internet to exist, taking advantage of such aspects as an interactive interface and connectivity to multiple social and economic cultures and micro-cultures. It refers to the Internet as a whole, not only to web-based works.\n\nTheorist and curator Jon Ippolito defined \"Ten Myths\" about Internet art in 2002. He cites the above stipulations, as well as defining it as distinct from commercial web design, and touching on issues of permanence, archivability, and collecting in a fluid medium.\n\nInternet art is rooted in disparate artistic traditions and movements, ranging from Dada to Situationism, conceptual art, Fluxus, video art, kinetic art, performance art, telematic art and happenings.\n\nIn 1974, Canadian artist Vera Frenkel worked with the Bell Canada Teleconferencing Studios to produce the work \"String Games\", the first artwork to use telecommunications technologies.\n\nAn early telematic artwork was Roy Ascott's work, \"La Plissure du Texte\",\nperformed in collaboration created for an exhibition at the Musée d'Art Moderne de la Ville de Paris in 1983.\n\nMedia art institutions such as Ars Electronica Festival in Linz, or the Paris-based IRCAM (a research center for electronic music), would also support or present early Networked art.\n\nWith the rise of search engines as a gateway to accessing the web in the late 1990s, many net artists turned their attention to related themes. The 2001 'Data Dynamics' exhibit at the Whitney Museum of American Art featured 'Netomat' (Maciej Wisniewski) and 'Apartment' (Marek Walczak and Martin Wattenberg, which used search queries as raw material. Mary Flanagan's 'The Perpetual Bed' received attention for its novel use of 3D nonlinear narrative space, or what she called \"navigable narratives.\" \n\nHer 2001 work in the Whitney Biennial, 'collection' collected items from hard drives around the world and displayed them in a 'computational collective unconscious.' Golan Levin's 'The Secret Lives of Numbers' (2000) visualized the \"popularity\" of the numbers 1 to 1,000,000 as measured by Alta Vista search results. Such works pointed to alternative interfaces and questioned the dominant role of search engines in controlling access to the net.\n\nNevertheless, the Internet is not reducible to the web, nor to search engines. Besides these unicast (point to point) applications, suggesting that there is some reference points, there is also a multicast (multipoint and acentered) internet that has been explored by very few artistic experiences, such as the Poietic Generator. Internet art has, according to Juliff and Cox, suffered under the privileging of the user interface inherent within computer art. They argue that Internet is not synonymous with a specific user and specific interface, but rather a dynamic structure that encompasses coding and the artist's intention.\n\nThe emergence of social networking platforms, understood to be \"web-based services that allow individuals to... construct a public or semi-public profile within a bounded system... articulate a list of other users with whom they share a connection, and... view and traverse their list of connections and those made by others within the system\", facilitated a transformative shift in the distribution of internet art. Early online communities were organized around specific \"topical hierarchies\", whereas social networking platforms consist of egocentric networks, with the \"individual at the center of their own community\". Artistic communities on the Internet underwent a similar transition in the mid-2000s, shifting from Surf Clubs, \"15 to 30 person groups whose members contributed to an ongoing visual-conceptual conversation through the use of digital media\" and whose membership was restricted to a select group of individuals, to image-based social networking platforms, like Flickr, which permit access to any individual with an e-mail address. Internet artists make extensive use of the networked capabilities of social networking platforms, and are rhizomatic in their organization, in that \"production of meaning is externally contingent on a network of other artists' content\".\n\n\n\n\n"}
{"id": "1839724", "url": "https://en.wikipedia.org/wiki?curid=1839724", "title": "Irresistible force paradox", "text": "Irresistible force paradox\n\nMany believe this to be an impossible paradigm (see below) but it's the basis of nuclear fusion. In nuclear physics, nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei and subatomic particles. The difference in mass between the reactants and products is manifested as either the release or absorption of energy. So, what happens when an unstoppable force meets an immovable object? The colliders combine and create energy. \n\nThe unstoppable force paradox, also called the irresistible force paradox, shield and spear paradox, is a classic paradox formulated as \"What happens when an unstoppable force meets an immovable object?\" The immovable object and the unstoppable force are both implicitly assumed to be indestructible, or else the question would have a trivial resolution. Furthermore, it is assumed that they are two entities.\n\nThe paradox arises because it rests on two incompatible premises: that there can exist simultaneously such things as \"unstoppable forces\" and \"immovable objects\". The \"paradox\" is flawed because if there exists an unstoppable force, it follows logically that there cannot be any such thing as an immovable object and vice versa.\n\nAn example of this paradox in non-western thought can be found in the origin of the Chinese word for contradiction (). This term originates from a story in the 3rd century BC philosophical book \"Han Feizi\". In the story, a man was trying to sell a spear and a shield. When asked how good his spear was, he said that his spear could pierce any shield. Then, when asked how good his shield was, he said that it could defend from all spear attacks. Then one person asked him what would happen if he were to take his spear to strike his shield; the seller could not answer. This led to the idiom of \"zìxīang máodùn\" (自相矛盾, \"from phrase spear shield\"), or \"self-contradictory\".\n\nAnother ancient and mythological example illustrating this theme can be found in the story of the Teumessian fox, who can never be caught, and the hound Laelaps, who never misses what it hunts. Realizing the paradox, Zeus turns both creatures into static stars.\n\nThe problems associated with this paradox can be applied to any other conflict between two abstractly defined extremes that are opposite.\n\nOne of the answers generated by seeming paradoxes like these is that there is no contradiction – that there is a false dilemma. Dr. Christopher Kaczor suggested that the need to change indicates a lack of power rather than the possession thereof, and as such a person who was omniscient would never need to change their mind – not changing the future would be consistent with omniscience rather than contradicting it.\n\nIn the same way, an unstoppable force, an object or force with infinite inertia, would be consistent with the definition of an immovable object, in that they would be one and the same. Any object whose momentum or motion cannot be changed is an immovable object, and it would halt any object that moved relative to it, making it an unstoppable force.\n\nA deterministic universe may contain more than one of such forces/objects as long as they are never determined to meet in the entire history of such a universe. Indeed, in the context of such a universe, one could redefine the words \"irresistible\" and \"immovable\" to \"is never successfully resisted\" and \"is never successfully moved\" (within the fixed history of said deterministic universe) instead of the counterfactual possibilities. This is similar to the Novikov self-consistency principle of the grandfather paradox in time-travel scenarios.\n\nIn Iain Banks's novel, \"Walking on Glass\", a supremely satisfying solution to the paradox is given.\n\nIn DC Comics' \"All-Star Superman\" by Grant Morrison and Frank Quitely, Superman encounters the Ultrasphinx, who asks \"What happens when the unstoppable force meets the immovable object?\", to which Superman answers \"They surrender.\"\n\nAt Wrestlemania III, color commentator Gorilla Monsoon described the match of Andre the Giant vs Hulk Hogan as \"The irresistible force meeting the immovable object.\"\n\nIn the film \"Imagine Me And You\" (2005) the irresistible force paradox is a recurring theme first mentioned by H and last referenced by Heck who compares Rachel's love for Luce as the unstoppable force and himself as the immovable object saying, \"What you're feeling is the unstoppable force, which means I have to move.\"\n\nIn \"Knight Rider\", KITT references the paradox when he was faced with the possibility of having a Head-on collision with his prototype version called KARR, who has most of KITT's abilities including a nearly indestructible body.\n\nIn DC's film \"The Dark Knight\" (2008), the Joker references the irresistible force paradox in both his final scene, as well as his final dialogue between himself and Batman. The Joker attempts to ultimately explain the reason for Batman's inability to shed his Batman identity, saying, \"You just couldn't let me go, could you? This is what happens when an unstoppable force meets an immovable object. You truly are incorruptible, aren't you, huh? You won't kill me out of some misplaced sense of self-righteousness, and I won't kill you because you're just too much fun. I think you and I are destined to do this forever.\"\n\nIn the video game World of Warcraft, players could gain a mace named \"The Unstoppable Force\" and a shield called \"The Immovable Object\". And humorously, there also appears an apparent resolution to the paradox with \"The Stoppable Force\" and \"The Movable Object\".\n\nThe rock band Jane's Addiction use this paradox in the lyrics of the song \"Irresistible Force\" on their album \"The Great Escape Artist\" (2011)\n\nIn season 5, episode 13 of Burn notice, Oswald refers to Michael Westen as an \"unstoppable force\", and refers to himself as an \"immovable object\". (2011)\n\n"}
{"id": "905646", "url": "https://en.wikipedia.org/wiki?curid=905646", "title": "List of memory biases", "text": "List of memory biases\n\nIn psychology \"and\" cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many different types of memory biases, including:\n\n\n\n"}
{"id": "1188531", "url": "https://en.wikipedia.org/wiki?curid=1188531", "title": "Medical model of disability", "text": "Medical model of disability\n\nThe medical model of disability, or medical model, arose from the biomedical perception of disability. This model links a disability diagnosis to an individual's physical body. The model supposes that this disability may reduce the individual's quality of life and the aim is, with medical intervention, this disability will be diminished or corrected.\n\nThe medical model focuses on curing or managing illness or disability. By extension, the medical model supposes a \"compassionate\" or just society invests resources in health care and related services in an attempt to cure or manage disabilities \"medically\". This is in an aim to expand functionality and/or improve functioning, and to allow disabled persons a more \"normal\" life. The medical profession's responsibility and potential in this area is seen as central.\n\nBefore the introduction of the biomedical model, patients relaying their narratives to the doctors was paramount. Through these narratives and developing an intimate relationship with the patients, the doctors would develop treatment plans in a time when diagnostic and treatment options were limited. This could particularly be illustrated with aristocratic doctors treating the elite during the 17th and 18th century.\n\nThe reliance of doctors on the narratives of patients diminished with the growth of bio-medicine.\n\nIn 1980, the World Health Organization (WHO) introduced a framework for working with disability, publishing the \"International Classification of Impairments, Disabilities and Handicaps.\" The framework proposed to approach disability by using the terms Impairment, Handicap and Disability.\n\nWhile personal narrative is present in interpersonal interactions, and particularly dominant in Western Culture, personal narrative during interactions with medical personnel is reduced to relaying information about specific symptoms of the disability to medical professionals. The medical professionals then interpret the information provided about the disability by the patient to determine a diagnosis, which likely will be linked to biological causes.The medical professionals now define what is \"normal\" and what is \"abnormal\" in terms of biology and disability.\n\nIn some countries, the medical model of disability has influenced legislation and policy pertaining to persons with disabilities on a national level.\n\nThe International Classification of Functioning, Disability and Health (ICF), published in 2001, defines disability as an umbrella term for impairments, activity limitations and participation restrictions. Disability is the interaction between individuals with a health condition (such as cerebral palsy, Down syndrome and depression) and personal and environmental factors (such as negative attitudes, inaccessible transportation and public buildings, and limited social supports). \n\nThe altered language and words used show a marked change in emphasis from talking in terms of disease or impairment to talking in terms of levels of health and functioning. It takes into account the social aspects of disability and does not see disability only as a 'medical' or 'biological' dysfunction. That change is consistent with widespread acceptance of the social model of disability.\n\nThe medical model of disability focuses on the individual's limitations and ways to reduce those impairments or using adaptive technology to adapt them to society. Current definitions of disability accept biomedical assistance but focus more on factors causing environmental and social exclusion. Uncritical reliance on the medical model produces unwanted consequences.\n\nAmong advocates of disability rights, who tend to subscribe to the social model instead, the medical model of disability is often cited as the basis of an unintended social degradation of disabled people; further, resources are seen as excessively misdirected towards an almost-exclusively medical focus when those same resources could be used towards things like universal design and societal inclusionary practices. This includes the monetary and the societal costs and benefits of various interventions, be they medical, surgical, social or occupational, from prosthetics, drug-based and other \"cures\", and medical tests such as genetic screening or preimplantation genetic diagnosis. Often, a medical model of disability is used to justify large investment in these procedures, technologies and research, when adaptation of the disabled person's environment might ultimately be more beneficial to the society at large, as well as financially cheaper and physically more attainable.\n\nAlso, some disability rights groups see the medical model of disability as a civil rights issue and criticise charitable organizations or medical initiatives that use it in their portrayal of disabled people, because it promotes a pitiable, essentially negative, largely disempowered image of people with disabilities rather than casting disability as a political, social and environmental problem (see also the political slogan \"Piss On Pity\").\n\n\n"}
{"id": "1045761", "url": "https://en.wikipedia.org/wiki?curid=1045761", "title": "Minnesota nice", "text": "Minnesota nice\n\nThe stereotypical behavior of people from Minnesota to be courteous, reserved, and mild-mannered, is popularly known as Minnesota nice. The cultural characteristics of \"Minnesota nice\" include polite friendliness, an aversion to confrontation, a tendency toward understatement, a disinclination to make a fuss or stand out, emotional restraint, and self-deprecation.\n\nPlaywright and corporate communications consultant Syl Jones suggested that \"Minnesota nice\" is not entirely about being \"nice\" but is more about keeping up appearances, maintaining the social order, and keeping people in their place. He relates these social norms to the literary work of Danish-Norwegian novelist Aksel Sandemose, the fictional Law of Jante, and more generally, Scandinavian culture. Garrison Keillor's \"A Prairie Home Companion\" discusses \"Wobegonics\", the supposed language of Minnesotans which includes \"no confrontational verbs or statements of strong personal preference\".\n\nThe generosity of state citizens has been commented on; the heavily-reported influenza vaccine shortage of late 2004 did not strike the state as hard as elsewhere since many people willingly gave up injections for others. The concept has also received some support from the academic community; a national study by Peter Rentfrow, Samuel Gosling, and Jeff Potter done in 2008 found that Minnesota was the second most agreeable and fifth most extroverted state in the nation, traits associated with \"nice\".\n\nThe tradition of social progressivism in Minnesota politics has been linked to the Minnesota Nice culture. \n\nMinnesota nice was an influence on the Coen brothers movie \"Fargo\", set in both Minnesota and neighboring North Dakota; a documentary about the making of the movie was entitled \"Minnesota Nice\".\n\n"}
{"id": "56998725", "url": "https://en.wikipedia.org/wiki?curid=56998725", "title": "Nirmāṇakāya", "text": "Nirmāṇakāya\n\nNirmāṇakāya is the third aspect of the trikāya and the physical manifestation of a buddha in time and space. In Vajrayāna it is also referred as \"the dimension of ceaseless manifestation\".\n"}
{"id": "22700", "url": "https://en.wikipedia.org/wiki?curid=22700", "title": "Omphalos hypothesis", "text": "Omphalos hypothesis\n\nThe omphalos hypothesis is one attempt to reconcile the scientific evidence that the universe is billions of years old with the Genesis creation narrative, which implies that the Earth is only a few thousand years old. It is based on the religious belief that the universe was created by a divine being, within the past ten thousand years (in keeping with flood geology), and that the presence of objective, verifiable evidence that the universe is older than approximately ten millennia is entirely due to the creator introducing false evidence that makes the universe appear much, much older.\n\nThe idea was named after the title of an 1857 book, \"Omphalos\" by Philip Henry Gosse, in which Gosse argued that in order for the world to be \"functional\", God must have created the Earth with mountains and canyons, trees with growth rings, Adam and Eve with hair, fingernails, and navels (ὀμφαλός \"omphalos\" is Greek for \"navel\"), and that therefore \"no\" empirical evidence about the age of the Earth or universe can be taken as reliable.\nVarious supporters of Young Earth creationism have given different explanations for their belief that the universe is filled with false evidence of the universe's age, including a belief that some things needed to be created at a certain age for the ecosystems to function, or their belief that the creator was deliberately planting deceptive evidence.\n\nThe idea was widely rejected in the 19th century, when Gosse published his book. It saw some revival in the 20th century by some Young Earth creationists, who extended the argument to include visible light that appears to originate in far-off stars and galaxies.\n\nStories of the beginning of human life based on the creation story in Genesis have been published for centuries. The 4th-century theologian Ephrem the Syrian described a world in which divine creation instantly produced fully grown organisms:\n\nBy the 19th century, scientific evidence of the Earth's age had been collected, and it disagreed with a literal reading of the biblical accounts. This evidence was rejected by some writers at the time, such as François-René de Chateaubriand. Chateaubriand wrote in his 1802 book, \"Génie du christianisme\" (Part I Book IV Chapter V) that \"God might have created, and doubtless did create, the world with all the marks of antiquity and completeness which it now exhibits.\" In modern times, Rabbi Dovid Gottlieb supported a similar position, saying that the objective scientific evidence for an old universe is strong, but wrong, and that the traditional Jewish calendar is correct.\n\nIn the middle of the 19th century, the disagreement between scientific evidence about the age of the Earth and the Western religious traditions was a significant debate among intellectuals. Gosse published \"Omphalos\" in 1857 to explain his answer to this question. He concluded that the religious tradition was correct. Gosse began with the earlier idea that the Earth contained mature organisms at the instant they were created, and that these organisms had false signs of their development, such as hair on mammals, which grows over time. He extended this idea of creating a single mature organism to creating mature systems, and concluded that fossils were an artifact of the creation process and merely part of what was necessary to make creation work. Therefore, he reasoned, fossils and other signs of the Earth's age could not be used to prove the age. His book sold poorly and was widely rejected.\n\nOther contemporary proposals for reconciling the stories of creation in Genesis with the scientific evidence included the \"interval theory\" or gap theory of creation, in which a large interval of time passed in between the initial creation of the universe and the beginning of the six days of creation. This idea was put forward by Archbishop John Bird Sumner of Canterbury in \"Treatise on the Records of Creation\". Another popular idea, promoted by the English theologian John Pye Smith, was that the Garden of Eden described the events of only one small location. A third proposal, by French naturalist Georges-Louis Leclerc, Comte de Buffon, held that the six \"days\" of the creation story were arbitrary and large ages rather than 24-hour periods.\n\nTheologians rejected Gosse's proposal on the grounds that it seemed to make the divine creator tell lies – either lying in the scriptures, or lying in nature. Scientists rejected it on the grounds that it disagreed with uniformitarianism, an explanation of geology that was widely supported at the time, and the impossibility of testing or falsifying the idea.\n\nSome modern creationists still argue against scientific evidence in the same way. For instance, John D. Morris, president of the Institute for Creation Research wrote in 1990 about the \"appearance of age\":\nHe does not extend this idea to the geological record, preferring to believe that it was all created in the Flood, but others such as Gerald E. Aardsma go further, with his idea of \"virtual history\". This appears to suggest that events after the creation have changed the \"virtual history\" we now see, including the fossils:\nThe past president of the Missouri Association for Creation has said:\n\nThough Gosse's original omphalos hypothesis specifies a popular creation story, others have proposed that the idea does not preclude creation as recently as five minutes ago, including memories of times before this created \"in situ\". This idea is sometimes called Last Thursdayism by its opponents, as in \"the world might as well have been created last Thursday.\"\n\nThe concept is both unverifiable and unfalsifiable through any conceivable scientific study—in other words, it is impossible even \"in principle\" to subject it to any form of test, by reference to any empirical data, because the empirical data themselves are considered to have been arbitrarily created to look the way they do at every observable level of detail.\n\nFrom a religious viewpoint, it can be interpreted as God having \"created a fake\", such as illusions of light in space of stellar explosions (supernovae) that never really happened, or volcanic mountains that were never really volcanoes in the first place and that never actually experienced erosion.\n\nThis conception has therefore drawn harsh rebuke from some theologians. Reverend Canon Brian Hebblethwaite, for example, preached against Bertrand Russell's Five-minute hypothesis:\n\nThe basis for Hebblethwaite's objection, however, is the presumption of a God that would not deceive people about their very humanity—an unprovable presumption that the omphalos hypothesis rejects at the outset. Hebblethwaite also suggests that God necessarily had to create certain elements of the Universe in combination with the creation of man:\n\nIn a rebuttal of the claim that God might have implanted a false history of the age of the Universe in order to test our faith in the truth of the Torah, Rabbi Natan Slifkin, an author whose works have been banned by several Haredi rabbis for going against the tenets of the Talmud, writes:\n\nThe five-minute hypothesis is a skeptical hypothesis put forth by the philosopher Bertrand Russell that proposes that the universe sprang into existence five minutes ago from nothing, with human memory and all other signs of history included. It is a commonly used example of how one may maintain extreme philosophical skepticism with regard to memory.\n\nJorge Luis Borges, in his 1940 work, \"Tlön, Uqbar, Orbis Tertius\", describes a fictional world in which some essentially follow as a religious belief a philosophy much like Russell's discussion on the logical extreme of Gosse's theory:\n\nBorges had earlier written a short essay, \"The Creation and P. H. Gosse\" that explored the rejection of Gosse's \"Omphalos\". Borges argued that its unpopularity stemmed from Gosse's explicit (if inadvertent) outlining of what Borges characterized as absurdities in the Genesis story.\n\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "21437222", "url": "https://en.wikipedia.org/wiki?curid=21437222", "title": "Poincaré plot", "text": "Poincaré plot\n\nA Poincaré plot, named after Henri Poincaré, is a species of recurrence plot used to quantify self-similarity in processes, usually periodic functions. It is also known as a return map. Poincaré plots can be used to distinguish chaos from randomness by embedding a data set into a higher-dimensional state space.\n\nGiven a time series of the form\n\na return map in its simplest form first plots (\"x\", \"x\"), then plots (\"x\", \"x\"), then (\"x\", \"x\"), and so on.\n\nAn electrocardiogram (ECG) is a tracing of the voltage changes in the chest generated by the heart, whose contraction in the normal person is triggered by an electrical impulse that originates the sinoatrial node. The ECG normally consists of a series of waves, labeled the P, Q, R, S and T waves. The P wave represents depolarization of the atria, Q-R-S series of waves the depolarization of the ventricles, and T wave the repolarization of the ventricles. The interval between two successive R waves (the RR interval) is a measure of the heart rate.\n\nThe heart rate normally varies slightly: during a deep breath, it speeds up and during a deep exhalation, it slows down. (The RR interval will shorten when the heart speeds up, and lengthen when it slows.) An RR tachograph is a graph of the numerical value of the RR-interval versus time. \n\nIn the context of RR tachography, a Poincaré plot is a graph of RR(\"n\") on the \"x\"-axis versus RR(\"n\" + 1) (the succeeding RR interval) on the \"y\"-axis, i.e. one takes a sequence of intervals and plots each interval against the following interval.\nThe recurrence plot is used as a standard visualizing technique to detect the presence of oscillations in non-linear dynamic systems. In the context of electrocardiography, the rate of the healthy heart is normally tightly controlled by the body's regulatory mechanisms (specifically, by the autonomic nervous system). Several research papers demonstrate the potential of ECG signal-based Poincaré plots in detecting heart-related diseases or abnormalities.\n\n"}
{"id": "31014293", "url": "https://en.wikipedia.org/wiki?curid=31014293", "title": "Principle of transformation groups", "text": "Principle of transformation groups\n\nThe principle of transformation groups is a rule for assigning \"epistemic\" probabilities in a statistical inference problem. It was first suggested by Edwin T. Jaynes and can be seen as a generalisation of the principle of indifference.\n\nThis can be seen as a method to create \"objective ignorance probabilities\" in the sense that two people who apply the principle and are confronted with the same information will assign the same probabilities.\n\nThe method is motivated by the following normative principle, or desideratum:\n\n\"In two problems where we have the same prior information we should assign the same prior probabilities\"\n\nThe method then comes about from \"transforming\" a given problem into an equivalent one. This method has close connections with group theory, and to a large extent is about finding symmetry in a given problem, and then exploiting this symmetry to assign prior probabilities.\n\nIn problems with discrete variables (e.g. dice, cards, categorical data) the principle reduces to the principle of indifference, as the \"symmetry\" in the discrete case is a permutation of the labels, that is the permutation group is the relevant transformation group for this problem.\n\nIn problems with continuous variables, this method generally reduces to solving a differential equation. Given that differential equations do not always lead to unique solutions, this method cannot be guaranteed to produce a unique solution. However, in a large class of the most common types of parameters it does lead to unique solutions (see the examples below)\n\nConsider a problem where all you are told is that there is a coin, and it has a head (H) and a tail (T). Denote this information by \"I\". You are then asked \"what is the probability of Heads?\". Call this \"problem 1\" and denote the probability \"P(H|I)\". Consider another question \"what is the probability of Tails?\". Call this \"problem 2\" and denote this probability by \"P(T|I)\".\n\nNow from the information which was actually in the question, there is no distinction between heads and tails. The whole paragraph above could be re-written with \"Heads\" and \"Tails\" interchanged, and \"H\" and \"T\" interchanged, and the problem statement would not be any different. Using the desideratum then demands that\n\nformula_1\n\nThe probabilities must add to 1, this means that\n\nformula_2.\n\nThus we have a unique solution. This argument easily extents to \"N\" categories, to give the \"flat\" prior probability \"1/N\".\nThis provides a \"consistency\" based argument to the principle of indifference which goes as follows: \"if someone is truly ignorant about a discrete/countable set of outcomes apart from their potential existence, but does not assign them equal prior probabilities, then they are assigning different probabilities when given the same information\".\n\nThis can be alternatively phrased as: \"a person who does not use the principle of indifference to assign prior probabilities to discrete variables, is either not ignorant about them, or reasoning inconsistently\".\n\nThis is the easiest example for continuous variables. It is given by stating one is \"ignorant\" of the location parameter in a given problem. The statement that a parameter is a \"location parameter\" is that the sampling distribution, or likelihood of an observation \"X\" depends on a parameter formula_3 only through the difference\n\nformula_4\n\nfor some normalised, but otherwise arbitrary distribution \"f(.)\". Examples of location parameters include mean parameter of normal distribution with known variance and median parameter of Cauchy distribution with known inter-quartile range.\nThe two \"equivalent problems\" in this case, given ones knowledge of the sampling distribution formula_4, but no other knowledge about formula_3, is simply given by a \"shift\" of equal magnitude in \"X\" and formula_3. This is because of the relation:\n\nformula_8\n\nSo simply \"shifting\" all quantities up by some number \"b\" and solving in the \"shifted space\" and then \"shifting\" back to the original one should give exactly the same answer as if we just worked on the original space. Making the transformation from formula_3 to formula_10 has a Jacobian of simply 1, and so the prior probability formula_11 must satisfy the functional equation:\n\nformula_12\n\nAnd the only function which satisfies this equation is the \"constant prior\":\n\nformula_13\n\nThus the uniform prior is justified for expressing complete ignorance of a location parameter.\n\nAs in the above argument, a statement that formula_14 is a scale parameter means that the sampling distribution has the functional form:\n\nformula_15\n\nWhere, as before \"f(.)\" is a normalised probability density function. The requirement that probabilities be finite and positive forces the condition formula_16. Examples include the standard deviation of a normal distribution with known mean, the gamma distribution. The \"symmetry\" in this problem is found by noting that\n\nformula_17\n\nBut, unlike in the location parameter case, the Jacobian of this transformation in the sample space and the parameter space is \"a\", not 1. so the sampling probability changes to:\n\nformula_18\n\nWhich is invariant (i.e. has the same form before and after the transformation), and the prior probability changes to:\n\nformula_19\n\nWhich has the unique solution (up to a proportionality constant):\n\nformula_20\n\nWhich is the well-known Jeffreys prior for scale parameters, which is \"flat\" on the log scale, although it should be noted that it is derived using a different argument to that here, based on the Fisher information function. The fact that these two methods give the same results in this case does not imply it in general.\n\nEdwin Jaynes used this principle to provide a resolution to Bertrand's Paradox\nby stating his ignorance about the exact position of the circle. The details are available in the reference or in the link.\n\nThis argument depends crucially on \"I\"; changing the information may result in a different probability assignment. It is just as crucial as changing axioms in deductive logic - small changes in the information can lead to large changes in the probability assignments allowed by \"consistent reasoning\".\n\nTo illustrate suppose that the coin flipping example also states as part of the information that the coin has a side (S) (i.e. it is a \"real coin\"). Denote this new information by \"N\". The same argument using \"complete ignorance\", or more precisely, the information actually described, gives:\n\nformula_21\n\nBut this seems absurd to most people - intuition tells us that we should have P(S) very close to zero. This is because most people's intuition do not see \"symmetry\" between a coin landing on its side compared to landing on heads. Our intuition says that the particular \"labels\" actually carry some information about the problem. A simple argument could be used to make this more formal mathematically (e.g. the physics of the problem make it difficult for a flipped coin to land on its side) - we make a distinction between \"thick\" coins and \"thin\" coins [here thickness is measured relative to the coin's diameter]. It could reasonably be assumed that:\n\nformula_22\n\nNote that this new information probably wouldn't break the symmetry between \"heads\" and \"tails\", so \"that\" permutation would still apply in describing \"equivalent problems\", and we would require:\n\nformula_23\n\nThis is a good example of how the principle of transformation groups can be used to \"flesh out\" personal opinions. All of the information used in the derivation is explicitly stated. If a prior probability assignment doesn't \"seem right\" according to what your intuition tells you, then there must be some \"background information\" which has not been put into the problem. It is then the task to try and work out what that information is. In some sense, by combining the method of transformation groups with one's intuition can be used to \"weed out\" the actual assumptions one has. This makes it a very powerful tool for prior elicitation.\n\nIntroducing the thickness of the coin is permissible because it was not specified in the problem, so this is still only using information in the question. Introducing a \"nuisance parameter\" and then making the answer invariant to this parameter is a very useful technique for solving supposedly \"ill-posed\" problems like Bertrand's Paradox. This has been called \"the well-posing strategy\" by some.\n\nThe real power of this principle lies in its application to continuous parameters, where the notion of \"complete ignorance\" is not so well defined as in the discrete case. However, if applied with infinite limits, it often gives improper prior distributions. Note that the discrete case for a countably infinite set, such as (0,1,2...) also produces an improper discrete prior. For most cases where the likelihood is sufficiently \"steep\" this does not present a problem. However, in order to be absolutely sure to avoid incoherent results and paradoxes, the prior distribution should be approached via a well defined and well behaved limiting process. One such process is the use of a sequence of priors with increasing range, such as formula_24 where the limit formula_25 is to be taken \"at the end of the calculation\" i.e. after the normalisation of the posterior distribution. What this effectively is doing, is ensuring that one is taking the limit of the ratio, and not the ratio of two limits. See Limit of a function#Properties for details on limits and why this order of operations is important.\n\nIf the limit of the ratio does not exist or diverges, then this gives an improper posterior (i.e. a posterior which does not integrate to one). This indicates that the data are so uninformative about the parameters that the prior probability of arbitrarily large values still matters in the final answer. In some sense, an improper posterior means that the information contained in the data has not \"ruled out\" arbitrarily large values. Looking at the improper priors this way, it seems to make some sense that \"complete ignorance\" priors should be improper, because the information used to derive them is so meager that it cannot rule out absurd values on its own. From a state of complete ignorance, only the data or some other form of additional information can rule out such absurdities.\n\n"}
{"id": "18540101", "url": "https://en.wikipedia.org/wiki?curid=18540101", "title": "Project Eagle", "text": "Project Eagle\n\nProject Eagle is an interactive art demo of a colony on Mars, developed by Blackbird Interactive in collaboration with NASA's Jet Propulsion Laboratory. It was released on Steam on 27 November 2018, in honor of the successful \"InSight\" landing.\n\nProject Eagle was built in the Unity (Game Engine) and utilizes design elements similar to that of the RTS game \"\", including the sensors manager view and camera systems.\n\nThe Martian terrain was generated using radar data from NASA's HiRISE camera on board the Mars Reconnaissance Orbiter.\n\n\"Project Eagle\" is set in 2117 in a hypothetical future after the first human colonists arrive on Mars in the year 2034 The fictional \"Eagle Base\" is located at the foot of Mount Sharp (Aeolis Mons), in Quad 51 of Aeolis Palus in Gale Crater, near the site of the \"Curiosity\" rover landing. The \"Curiosity\" landing site is marked with a plinth in Project Eagle.\n\nProject Eagle was presented on stage at D.I.C.E. 2017 by NASA’s Dr. Jeff Norris, and BBI’s CEO Rob Cunningham and CCO Aaron Kambeitz. The talk took place directly after the conference keynote speech by Jeffrey Kaplan from Blizzard Entertainment\n\n\"We wanted to publicly exhibit a project that shows what this medium could do for inspiring space exploration.” -Jeff Norris \n"}
{"id": "31595228", "url": "https://en.wikipedia.org/wiki?curid=31595228", "title": "Psychological stress", "text": "Psychological stress\n\nIn psychology, stress is a feeling of strain and pressure. Stress is a type of psychological pain. Small amounts of stress may be desired, beneficial, and even healthy. Positive stress helps improve athletic performance. It also plays a factor in motivation, adaptation, and reaction to the environment. Excessive amounts of stress, however, may lead to bodily harm. Stress can increase the risk of strokes, heart attacks, ulcers, and mental illnesses such as depression.\n\nStress can be external and related to the environment, but may also be caused by internal perceptions that cause an individual to experience anxiety or other negative emotions surrounding a situation, such as pressure, discomfort, etc., which they then deem stressful.\n\nHumans experience stress, or perceive things as threatening, when they do not believe that their resources for coping with obstacles (stimuli, people, situations, etc.) are enough for what the circumstances demand. When people think the demands being placed on them exceed their ability to cope, they then perceive stress.\nA very much overlooked side of stress is its positive adaptations. Positive psychological stress can lead to motivation and challenge instead of anxiety. The effects of experiencing eustress, which is positive stress, versus distress, which is negative stress, are significant. While colloquially lumped together, the various types of stress should be treated as separate concepts.\n\nSelye proposed that there are four variations of stress. On one axis, there is good stress (eustress) and bad stress (distress). On the other is over-stress (hyperstress) and understress (hypostress). The goal is to balance these as much as possible. The ultimate goal would be to balance hyperstress and hypostress perfectly and have as much eustress as possible. It is extremely useful for a productive lifestyle because it makes working enjoyable instead of a chore, as seen with distress.\n\nEustress comes from the Greek root “eu” which means good as in euphoria. Eustress is when a person perceives a stressor as positive. Distress stems from the Latin root “dis” as in dissonance or disagreement. Distress is a threat to the quality of life. It is when a demand vastly exceeds a person’s capabilities.\n\nThere is likely a connection between stress and illness. Theories of the stress–illness link suggest that both acute and chronic stress can cause illness, and several studies found such a link. According to these theories, both kinds of stress can lead to changes in behavior and in physiology. Behavioral changes can be smoking and eating habits and physical activity. Physiological changes can be changes in sympathetic activation or hypothalamic pituitary adrenocorticoid activation, and immunological function. However, there is much variability in the link between stress and illness.\n\nStress can make the individual more susceptible to physical illnesses like the common cold. Stressful events, such as job changes, may result in insomnia, impaired sleeping, and health complaints. Research indicates the type of stressor (whether it is acute or chronic) and individual characteristics such as age and physical well-being before the onset of the stressor can combine to determine the effect of stress on an individual. An individual's personality characteristics (such as level of neuroticism), genetics, and childhood experiences with major stressors and traumas may also dictate their response to stressors.\n\nChronic stress and a lack of coping resources available or used by an individual can often lead to the development of psychological issues such as depression and anxiety (see below for further information). This is particularly true regarding chronic stressors. These are stressors that may not be as intense as an acute stressor like a natural disaster or a major accident, but they persist over longer periods of time. These types of stressors tend to have a more negative impact on health because they are sustained and thus require the body's physiological response to occur daily. This depletes the body's energy more quickly and usually occurs over long periods of time, especially when these microstressors cannot be avoided (i.e. stress of living in a dangerous neighborhood). See allostatic load for further discussion of the biological process by which chronic stress may affect the body. For example, studies have found that caregivers, particularly those of dementia patients, have higher levels of depression and slightly worse physical health than noncaregivers.\n\nStudies have also shown that perceived chronic stress and the hostility associated with Type A personalities are often associated with much higher risks of cardiovascular disease. This occurs because of the compromised immune system as well as the high levels of arousal in the sympathetic nervous system that occur as part of the body's physiological response to stressful events. However, it is possible for individuals to exhibit hardiness a term referring to the ability to be both chronically stressed and healthy. Chronic stress can be associated with psychological disorders such as delusions. Pathological anxiety and chronic stress lead to structural degeneration and impaired functioning of the hippocampus.\n\nIt has long been believed that negative affective states, such as feelings of anxiety and depression, could influence the pathogenesis of physical disease, which in turn, have direct effects on biological process that could result in increased risk of disease in the end. However, studies done by the University of Wisconsin-Madison and other places have shown this to be partly untrue; although stress seems to increase the risk of reported poor health, the \"perception\" that stress is harmful increases the risk even further. For example, when humans are under chronic stress, permanent changes in their physiological, emotional, and behavioral responses are most likely to occur. Such changes could lead to disease. Chronic stress results from stressful events that persist over a relatively long period of time, such as caring for a spouse with dementia, or results from brief focal events that continue to be experienced as overwhelming even long after they are over, such as experiencing a sexual assault.\n\nExperiments show that when healthy human individuals are exposed to acute laboratory stressors, they show an adaptive enhancement of some markers of natural immunity but a general suppression of functions of specific immunity. By comparison, when healthy human individuals are exposed to real-life chronic stress, this stress is associated with a biphasic immune response where partial suppression of cellular and humoral function coincides with low-grade, nonspecific inflammation.\n\nEven though psychological stress is often connected with illness or disease, most healthy individuals can still remain disease-free after confronting chronic stressful events. Also, people who do not believe that stress will affect their health do not have an increased risk of illness, disease, or death. This suggests that there are individual differences in vulnerability to the potential pathogenic effects of stress; individual differences in vulnerability arise due to both genetic and psychological factors. In addition, the age at which the stress is experienced can dictate its effect on health. Research suggests chronic stress at a young age can have lifelong impacts on the biological, psychological, and behavioral responses to stress later in life.\n\nAs stress has a physical effect on the body, some individuals may not distinguish this from other more serious illnesses.\nIf the symptom is unambiguous (e.g. a breast lump), individuals are motivated to seek care regardless if they are under stress.\nHowever, if the symptom is ambiguous (e.g. headache), they will not seek care attributing the symptom to stress if the stressor's onset is recent which began in the previous 3 weeks, and will seek care if the onset is not recent.\n\nIn animals, stress contributes to the initiation, growth, and metastasis of select tumors, but studies that try to link stress and cancer incidence in humans have had mixed results. This can be due to practical difficulties in designing and implementing adequate studies. Personal belief in stress as a risk factor for cancer was common in one UK study, though awareness of risk factors overall was found to be low.\n\nStress is a non-specific response. It is neutral, and what varies is the degree of response. It is all about the context of the individual and how they perceive the situation. Selye defined stress as “the nonspecific (that is, common) result of any demand upon the body, be the effect mental or somatic.” This includes the medical definition of stress as a physical demand and the colloquial definition of stress as a psychological demand. A stressor is inherently neutral meaning that the same stressor can cause either distress or eustress. It is individual differences and responses that induce either distress or eustress.\n\nA stressor is any event, experience, or environmental stimulus that causes stress in an individual. These events or experiences are perceived as threats or challenges to the individual and can be either physical or psychological. Researchers have found that stressors can make individuals more prone to both physical and psychological problems, including heart disease and anxiety.\n\nStressors are more likely to affect an individual's health when they are \"chronic, highly disruptive, or perceived as uncontrollable\". In psychology, researchers generally classify the different types of stressors into four categories: 1) crises/catastrophes, 2) major life events, 3) daily hassles/microstressors, and 4) ambient stressors.\n\nThis type of stressor is unforeseen and unpredictable and, as such, is completely out of the control of the individual. Examples of crises and catastrophes include: devastating natural disasters, such as major floods or earthquakes, wars, etc. Though rare in occurrence, this type of stressor typically causes a great deal of stress in a person's life. A study conducted by Stanford University found that after natural disasters, those affected experienced a significant increase in stress level. Combat stress is a widespread acute and chronic problem. With the rapid pace and the urgency of firing first, tragic episodes of accidentally killing friendly forces (“brother” killing “brother” or fratricide) may happen. Prevention requires stress reduction, emphasis on vehicle and other identification training, awareness of the tactical situation, and continual risk analysis by leaders at all echelons.\n\nCommon examples of major life events include: marriage, going to college, death of a loved one, birth of a child, moving houses, etc. These events, either positive or negative, can create a sense of uncertainty and fear, which will ultimately lead to stress. For instance, research has found the elevation of stress during the transition from high school to university, with college freshmen being about two times more likely to be stressed than final year students. Research has found major life events are somewhat rare to be major causes of stress, due to its rare occurrences.\n\nThe length of time since occurrence and whether or not it is a positive or negative event are factors in whether or not it causes stress and how much stress it causes. Researchers have found that events that have occurred within the past month generally are not linked to stress or illness, while chronic events that occurred more than several months ago are linked to stress and illness and personality change. Additionally, positive life events are typically not linked to stress and if so, generally only trivial stress while negative life events can be linked to stress and the health problems that accompany it. However, positive experiences and positive life changes can predict decreases in neuroticism.\n\nThis category includes daily annoyances and minor hassles. Examples include: making decisions, meeting deadlines at work or school, traffic jams, encounters with irritating personalities, etc. Often, this type of stressor includes conflicts with other people. Daily stressors, however, are different for each individual, as not everyone perceives a certain event as stressful. For example, most people find public speaking to be stressful, nevertheless, a seasoned politician most likely will not.\n\nDaily hassles are the most frequently occurring type of stressor in most adults. The high frequency of hassles causes this stressor to have the most physiological effect on an individual. Carolyn Aldwin, Ph.D., conducted a study at the Oregon State University that examined the perceived intensity of daily hassles on an individual's mortality. Aldwin's study concluded that there is a strong correlation between individuals who rate their hassles as very intense and a high level of mortality. One's perception of his/her daily stressors can have a modulating effect on the physiological impact of daily stressors.\n\nThere are three major psychological types of conflicts that can cause stress.\n\n\nTravel-related stress results from three main categories: lost time, surprises (an unforeseen event such as lost or delayed baggage) and routine breakers (inability to maintain daily habits).\n\nAs their name implies, these are global (as opposed to individual) low-grade stressors that are a part of the background environment. They are defined as stressors that are \"chronic, negatively valued, non-urgent, physically perceptible, and intractable to the efforts of individuals to change them\". Typical examples of ambient stressors are pollution, noise, crowding, and traffic. Unlike the other three types of stressor, ambient stressors can (but do not necessarily have to) negatively impact stress without conscious awareness. They are thus low on what Stokols called \"perceptual salience\".\n\nStudies conducted in military and combat fields show that some of the most potent stressors can be due to personal organizational problems in the unit or on the home front. Stress due to bad organizational practices is often connected to \"Toxic Leadership\", both in companies and in governmental organizations.\n\nStress management refers to a wide spectrum of techniques and psychotherapies aimed at controlling a person's levels of stress, especially chronic stress, usually for the purpose of improving everyday functioning. It involves controlling and reducing the tension that occurs in stressful situations by making emotional and physical changes.\n\nDecreasing stressful behaviors is a part of prevention, some of the common strategies and techniques are: Self-monitoring, tailoring, material reinforcement, social reinforcement, social support, self-contracting, contracting with significant other, shaping, reminders, self-help groups, professional help.\n\nAlthough many techniques have traditionally been developed to deal with the consequences of stress considerable research has also been conducted on the prevention of stress, a subject closely related to psychological resilience-building. A number of self-help approaches to stress-prevention and resilience-building have been developed, drawing mainly on the theory and practice of cognitive-behavioral therapy.\n\nBiofeedback may also play a role in stress management. A randomized study by Sutarto et al. assessed the effect of resonant breathing biofeedback (recognize and control involuntary heart rate variability) among manufacturing operators; depression, anxiety and stress significantly decreased.\n\nThe Lazarus and Folkman model suggests that external events create a form of pressure to achieve, engage in, or experience a stressful situation. Stress is not the external event itself, but rather an interpretation and response to the potential threat; this is when the coping process begins.\n\nThere are various ways individuals deal with perceived threats that may be stressful. However, people have a tendency to respond to threats with a predominant coping style, in which they dismiss feelings, or manipulate the stressful situation.\n\nThere are different classifications for coping, or defense mechanisms, however they all are variations on the same general idea: There are good/productive and negative/counterproductive ways to handle stress. Because stress is perceived, the following mechanisms do not necessarily deal with the actual situation that is causing an individual stress. However, they may be considered coping mechanisms if they allow the individual to cope better with the negative feelings/anxiety that they are experiencing due to the perceived stressful situation, as opposed to actually fixing the concrete obstacle causing the stress. The following mechanisms are adapted from the DSM-IV Adaptive Functioning Scale, APA, 1994.\n\nThese skills are what one could call as “facing the problem head on”, or at least dealing with the negative emotions experienced by stress in a constructive manner. (generally adaptive)\n\n\n\nThe final path model fitted well (CF1 = 1, RMSEA = 0.00) and showed that direct quality of life paths with β = -0.2, and indirect social support with β = -0.088 had the most effects on reduction of stress during pregnancy.\nOther adaptive coping mechanisms include anticipation, altruism, and self-observation.\n\nThese mechanisms cause the individual to have a diminished (or in some cases non-existent) awareness about their anxiety, threatening ideas, fears, etc., that come from being conscious of the perceived threat.\n\n\nOther inhibition coping mechanisms include undoing, dissociation, denial, projection, and rationalization. Although some people claim that inhibition coping mechanisms may eventually increase the stress level because the problem is not solved, detaching from the stressor can sometimes help people to temporarily release the stress and become more prepared to deal with problems later on.\n\nThese methods deal with stress by an individual literally taking action, or withdrawing.\n\n\nThere is an alternative method to coping with stress, in which one works to minimize their anxiety and stress in a preventative manner. If one works towards coping with stress daily, the feeling of stress and the ways in which one deals with it as the external event arises becomes less of a burden.\n\nSuggested strategies to improve stress management include:\n\n\nDepending on the situation, all of these coping mechanisms may be adaptive, or maladaptive.\n\nThe body responds to stress in many ways. Readjusting chemical levels is just one of them. Here are some examples of adjustments and changes that affect communication.\n\nIn terms of measuring the body's response to stress, psychologists tend to use Hans Selye's general adaptation syndrome. This model is also often referred to as the classic stress response, and it revolves around the concept of homeostasis. General adaptive syndrome occurs in three stages:\n\n\nThis physiological stress response involves high levels of sympathetic nervous system activation, often referred to as the \"fight or flight\" response. The response involves pupil dilation, release of endorphins, increased heart and respiration rates, cessation of digestive processes, secretion of adrenaline, arteriole dilation, and constriction of veins. This high level of arousal is often unnecessary to adequately cope with micro-stressors and daily hassles; yet, this is the response pattern seen in humans, which often leads to health issues commonly associated with high levels of stress.\n\nSleep allows people to rest and re-energize for another day filled with interactions and tasks. If someone is stressed it is extremely important for them to get enough sleep so that they can think clearly. Unfortunately, chemical changes in the body caused by stress can make sleep a difficult thing. Glucocorticoids are released by the body in response to stress which can disrupt sleep. Sleep comes in four stages and the deepest, most restful sleep can only be attained after having been asleep for an hour.\n\nWhen someone is stressed, many challenges can arise; a recognized challenge being communication difficulties. Here are some examples of how stress can hinder communication.\n\nThe cultures of the world generally fall into two categories; individualistic and collectivistic.\n\n\nThese cultural differences can affect how people communicate when they are stressed. For example, a member of an individualistic cultural would be hesitant to ask for pain medication for fear of being perceived as weak. A member of a collectivistic culture would not hesitate. They have been brought up in a cultural where everyone helps each other and is one functional unit whereas the member of the individualistic culture is not as comfortable asking others for aid.\n\nLanguage barriers can also diminish communication due to stress. All languages have their own way of using names, titles, and just interacting. These differences can make inter lingual communication relatively stressful. Not speaking the same languages, different ways of showing respect, and different use of body language can make things difficult. Being uncomfortable with the communication around a person can discourage them from communicating at all.\n\nDivorce, death, and remarriage are all disruptive events in a household. Although everyone involved is affected by events such as these, it can be most drastically seen in children. Due to their age, children have relatively undeveloped coping skills. For this reason a stressful event may cause some changes in their behavior. Falling in with a new crowd, developing some new and sometimes undesirable habits are just some of the changes stress may trigger in their lives.\n\nA particularly interesting response to stress is talking to an imaginary friend. A child may feel angry with a parent or their peers who they feel brought this change on them. They need someone to talk to but it definitely won’t be the person with whom they are angry. That’s when the imaginary friend comes in. They “talk” to this imaginary friend but in doing so they cut off communication with the real people around them.\n\nResearchers have long been interested in how an individual's level and types of social support impact the effect of stress on their health. Studies consistently show that social support can protect against physical and mental consequences of stress. This can occur through a variety of mechanisms. One model, known as the \"direct effects\" model, holds that social support has a direct, positive impact on health by increasing positive affect, promoting adaptive health behaviors, predictability and stability in life, and safeguarding against social, legal, and economic concerns that could negatively impact health. another model, the \"buffering effect\", says that social support exerts greatest influence on health in times of stress, either by helping individuals appraise situations in less threatening manners or coping with the actual stress. Researchers have found evidence to support both these pathways.\n\nSocial support is defined more specifically as psychological and material resources provided by a social network that are aimed at helping an individual cope with stress. Researchers generally distinguish among several types of social support: instrumental support – which refers to material aid (e.g., financial support or assistance in transportation to a physician's appointment), informational support (e.g., knowledge, education or advice in problem-solving), and emotional support (e.g., empathy, reassurance, etc.). Social support can reduce the rate of stress during pregnancy.\n\nSocial support from friends and the community can be very beneficial to helping someone communicate while stressed. Social support is giving a person the knowledge that they are part of a mutual network of caring, interested others, that enable them to lower levels of stress and be better able to cope with the stress that they undergo. The social and emotional support people provide for each other demonstrates that they are important and valued members of social networks.\n\nThe stress of a person can greatly affect those around them, especially in families. “Families can experience many conflicting emotions when placed in the position of providing protected care for a loved one. Compassion, protectiveness, and caring can be intermingled with feelings of helplessness and being trapped.\" Emotional support is crucial to helping families cope with the challenge of supporting their loved one (stressed person). This emotional support can be expressed through many communication methods.\n\nIn order to be able to effectively communicate with someone who is stressed, it is important to know how to interact with them in a way that can be beneficial for them. Therapeutic communication techniques can help with different types of communication. These techniques include but are not limited to listening, making open-ended comments, reducing distance, restating, seeking clarification, reflecting, and planning. Actively listening to someone when they are stressed can help them release frustrations and cope with their problems. Listening shows that you are interested in the person, and can have great therapeutic value. It is important to show that the stressed person's needs are above the caregiver's in order for the interaction to be therapeutic. It is important that you remain prepared mentally, emotionally, and physically to assist him or her. It is favorable to remain punctual and polite in the manner of relating to them, and that the best methods are used to promote their well-being and comfort.\n\n\n\nCommunication is an important stress-management skill. Although this seems like an easy skill, there is much more to communication than simply speaking. In fact, communication can cause problems such as misunderstandings when not used effectively. When miscommunication happen there tends to be more problems, anger and resentment then if communication were effective in the first place. There are certain things that need to be done to achieve effective communication.\n\nThe \"first guideline\" is to be clear about is what is wanted or needed when speaking with others.\n\nThis technique requires the individual’s recognition of distorted and exaggerated expectations and thoughts.\n\nAn easy way to meet this guideline is by reflecting the purpose of the conversation in the statement. By reflecting what the desired outcome of the conversation is, there is little room for miscommunication.\n\nThe \"second guideline\" for effective communication is to use assertive communication.\n\nAn assertive statement is non-judgmental, expresses feelings and opinions and reaffirms perceived rights.\nThe best way to use the assertive technique is with manipulating the following formula:\n\n\nWhen people are stressed, they cannot verbalize their feelings correctly. When the receiver in the conversation cannot understand the needs of the person, miscommunications happen and the person may feel victimized and blame others for not understanding. The third guideline is empathy which is defined as the ability to consider another person’s perspective and to communicate this perspective back to that person.\n\nThe \"final guideline\" to prevent misunderstandings when communicating while stressed is cognitive restructuring which facilitates assertive communication as it requires the person to identify their thoughts and feelings. Some ways to restructure cognitively is by stopping and understanding what the conversation holds.\n\nBreathing deeply as this will release any tension and promote relaxation which will allow you to reflect on the true emotions.\n\nReflecting on how you feel emotionally and how you feel immediately allow you to choose the right answer.\n\nChoosing the more realistic and helpful way of thinking allows the communication to be straight forward and upfront leaving little room for miscommunication.\n\nBy following the above techniques and guidelines, the chance of a miscommunication in a conversation will decrease. Once the ability to communicate with assertive techniques is worked into everyday life, the frequency of misunderstandings will decrease significantly.\n\nThe importance of understanding how to communicate assertively is critical for daily life. With the knowledge of how to properly communicate, whether stressed or not, the ability to communicate will become easier and result in less misunderstandings and frustrations which can contribute to one’s stress.\n\nLife events scales can be used to assess stressful things that people experience in their lives. One such scale is the Holmes and Rahe Stress Scale, also known as the Social Readjustment Rating Scale, or SRRS. Developed by psychiatrists Thomas Holmes and Richard Rahe in 1967, the scale lists 43 stressful events.\n\nTo calculate one's score, add up the number of \"life change units\" if an event occurred in the past year. A score of more than 300 means that individual is at risk for illness, a score between 150 and 299 means risk of illness is moderate, and a score under 150 means that individual only has a slight risk of illness.\n\nA modified version was made for non-adults. The scale is below.\n\nThe SSRS is used in psychiatry to weight the impact of life events.\n"}
{"id": "48530303", "url": "https://en.wikipedia.org/wiki?curid=48530303", "title": "Pukefejden", "text": "Pukefejden\n\nPukefejden (\"The Puke Feud\") was a feud taking between the followers of riksråd Erik Puke and the followers of the later Charles VIII of Sweden in 1436-1437. \n\nThe feud originated from the Engelbrekt rebellion, as the Puke party accused the party of Charles of having murdered Engelbrekt Engelbrektsson. After the rebellion, many fiefs had been left without a holder, and after having failed to distribute them fairly among the nobility in 1436, the opposition around Puke challenged Charles in open civil war. A number of cities, Arboga, Köping and Örebro, rebelled, and the rebels defeated the forces of Charles in the Battle of Hällaskogen on 17 January 1437. During peace negotiations in Västerås, Eric Puke was captured and brought to Stockholm, and was executed there in March 1437. \n\n"}
{"id": "25779023", "url": "https://en.wikipedia.org/wiki?curid=25779023", "title": "Recurrent tensor", "text": "Recurrent tensor\n\nIn mathematics, a recurrent tensor, with respect to a connection formula_1 on a manifold \"M\", is a tensor \"T\" for which there is a one-form \"ω\" on \"M\" such that\n\nAn example for recurrent tensors are parallel tensors which are defined by \nwith respect to some connection formula_1.\n\nIf we take a pseudo-Riemannian manifold formula_5 then the metric \"g\" is a parallel and therefore recurrent tensor with respect to its Levi-Civita connection, which is defined via\nand its property to be torsion-free.\n\nParallel vector fields (formula_7) are examples of recurrent tensors that find importance in mathematical research. For example, if formula_8 is a recurrent non-null vector field on a pseudo-Riemannian manifold satisfying\nfor some closed one-form formula_10, then X can be rescaled to a parallel vector field. In particular, non-parallel recurrent vector fields are null vector fields.\n\nAnother example appears in connection with Weyl structures. Historically, Weyl structures emerged from the considerations of Hermann Weyl with regards to properties of parallel transport of vectors and their length. By demanding that a manifold have an affine parallel transport in such a way that the manifold is locally an affine space, it was shown that the induced connection had a vanishing torsion tensor\nAdditionally, he claimed that the manifold must have a particular parallel transport in which the ratio of two transported vectors is fixed. The corresponding connection formula_12 which induces such a parallel transport satisfies\nfor some one-form formula_14. Such a metric is a recurrent tensor with respect to formula_12. As a result, Weyl called the resulting manifold formula_5 with affine connection formula_1 and recurrent metric formula_18 a metric space. In this sense, Weyl was not just referring to one metric but to the conformal structure defined by formula_18. \n\nUnder the conformal transformation formula_20, the form formula_14 transforms as formula_22. This induces a canonical map formula_23 on formula_24 defined by\nwhere formula_26 is the conformal structure. formula_27 is called a Weyl structure, which more generally is defined as a map with property\n\nOne more example of a recurrent tensor is the curvature tensor formula_29 on a recurrent spacetime, for which\n\n"}
{"id": "49198", "url": "https://en.wikipedia.org/wiki?curid=49198", "title": "Reductionism", "text": "Reductionism\n\nReductionism is any of several related philosophical ideas regarding the associations between phenomena which can be described in terms of other simpler or more fundamental phenomena.\n\n\"The Oxford Companion to Philosophy\" suggests that reductionism is \"one of the most used and abused terms in the philosophical lexicon\" and suggests a three part division:\n\nReductionism can be applied to any phenomenon, including objects, explanations, theories, and meanings.\n\nFor the sciences, application of methodological reductionism attempts explanation of entire systems in terms of their individual, constituent parts and their interactions. For example, the temperature of a gas is reduced to nothing beyond the average kinetic energy of its molecules in motion. Thomas Nagel speaks of 'psychophysical reductionism' (the attempted reduction of psychological phenomena to physics and chemistry), as do others and 'physico-chemical reductionism' (the attempted reduction of biology to physics and chemistry), again as do others. In a very simplified and sometimes contested form, such reductionism is said to imply that a system is nothing but the sum of its parts. However, a more nuanced opinion is that a system is composed entirely of its parts, but the system will have features that none of the parts have. \"The point of mechanistic explanations is usually showing how the higher level features arise from the parts.\"\n\nOther definitions are used by other authors. For example, what John Polkinghorne terms 'conceptual' or 'epistemological' reductionism is the definition provided by Simon Blackburn and by Jaegwon Kim: that form of reductionism concerning a program of replacing the facts or entities entering statements claimed to be true in one type of discourse with other facts or entities from another type, thereby providing a relationship between them. Such an association is provided where the same idea can be expressed by \"levels\" of explanation, with higher levels reducible if need be to lower levels. This use of levels of understanding in part expresses our human limitations in remembering detail. However, \"most philosophers would insist that our role in conceptualizing reality [our need for an hierarchy of \"levels\" of understanding] does not change the fact that different levels of organization in reality do have different 'properties'.\"\n\nReductionism strongly represents a certain perspective of causality. In a reductionist framework, the phenomena that can be explained completely in terms of relations between other more fundamental phenomena, are termed epiphenomena. Often there is an implication that the epiphenomenon exerts no causal agency on the fundamental phenomena that explain it. The epiphenomena are sometimes said to be \"nothing but\" the outcome of the workings of the fundamental phenomena, although the epiphenomena might be more clearly and efficiently described in very different terms. There is a tendency to avoid considering an epiphenomenon as being important in its own right. This attitude may extend to cases where the fundamentals are not obviously able to explain the epiphenomena, but are expected to by the speaker. In this way, for example, morality can be deemed to be \"nothing but\" evolutionary adaptation, and consciousness can be considered \"nothing but\" the outcome of neurobiological processes.\n\nReductionism should be distinguished from eliminationism: reductionists do not deny the existence of phenomena, but explain them in terms of another reality; eliminationists deny the existence of the phenomena themselves. For example, eliminationists deny the existence of life by their explanation in terms of physical and chemical processes.\n\nReductionism also does not preclude the existence of what might be termed emergent phenomena, but it does imply the ability to understand those phenomena completely in terms of the processes from which they are composed. This reductionist understanding is very different from emergentism, which intends that what emerges in \"emergence\" is more than the sum of the processes from which it emerges.\n\nMost philosophers delineate three types of reductionism and anti-reductionism.\n\nOntological reductionism is the belief that reality is composed of a minimum number of kinds of entities or substances. This claim is usually metaphysical, and is most commonly a form of monism, in effect claiming that all objects, properties and events are reducible to a single substance. (A dualist who is an ontological reductionist would believe that everything is reducible to two substances—as one possible example, a dualist might claim that reality is composed of \"matter\" and \"spirit\".)\n\nRichard Jones divides ontological reductionism into two: the reductionism of substances (e.g., the reduction of mind to matter) and the reduction of the number of structures operating in nature (e.g., the reduction of one physical force to another). This permits scientists and philosophers to affirm the former while being anti-reductionists regarding the latter.\n\nNancey Murphy has claimed that there are two species of ontological reductionism: one that denies that wholes are anything more than their parts; and the stronger thesis of atomist reductionism that wholes are not \"really real\". She admits that the phrase \"really real\" is apparently senseless but nonetheless has tried to explicate the supposed difference between the two.\n\nOntological reductionism denies the idea of ontological emergence, and claims that emergence is an epistemological phenomenon that only exists through analysis or description of a system, and does not exist fundamentally.\n\nOntological reductionism takes two different forms: token ontological reductionism and type ontological reductionism.\n\nToken ontological reductionism is the idea that every item that exists is a sum item. For perceivable items, it affirms that every perceivable item is a sum of items with a lesser degree of complexity. Token ontological reduction of biological things to chemical things is generally accepted.\n\nType ontological reductionism is the idea that every type of item is a sum type of item, and that every perceivable type of item is a sum of types of items with a lesser degree of complexity. Type ontological reduction of biological things to chemical things is often rejected.\n\nMichael Ruse has criticized ontological reductionism as an improper argument against vitalism.\n\nMethodological reductionism is the position that the best scientific strategy is to attempt to reduce explanations to the smallest possible entities. Methodological reductionism would thus include the claim that the atomic explanation of a substance's boiling point is preferable to the chemical explanation, and that an explanation based on even smaller particles (quarks and leptons, perhaps) would be even better. Methodological reductionism, therefore, is the opinion that all scientific theories either can or should be reduced to a single super~theory through the process of theoretical reduction.\n\nTheory reduction is the process by which one theory absorbs another. For example, both Kepler's laws of the motion of the planets and Galileo's theories of motion formulated for terrestrial objects are reducible to Newtonian theories of mechanics because all the explanatory power of the former are contained within the latter. Furthermore, the reduction is considered to be beneficial because Newtonian mechanics is a more general theory—that is, it explains more events than Galileo's or Kepler's. Theoretical reduction, therefore, is the reduction of one explanation or theory to another—that is, it is the absorption of one of our ideas about a particular item into another idea.\n\nReductionist thinking and methods form the basis for many of the well-developed topics of modern science, including much of physics, chemistry and cell biology. Classical mechanics in particular is seen as a reductionist framework, and statistical mechanics can be considered as a reconciliation of macroscopic thermodynamic laws with the reductionist method of explaining macroscopic properties in terms of microscopic components.\n\nIn science, reductionism implies that certain topics of study are based on areas that study smaller spatial scales or organizational units. While it is commonly accepted that the foundations of chemistry are based in physics, and molecular biology is based on chemistry, similar statements become controversial when one considers less rigorously defined intellectual pursuits. For example, claims that sociology is based on psychology, or that economics is based on sociology and psychology would be met with reservations. These claims are difficult to substantiate even though there are obvious associations between these topics (for instance, most would agree that psychology can affect and inform economics). The limit of reductionism's usefulness stems from emergent properties of complex systems, which are more common at certain levels of organization. For example, certain aspects of evolutionary psychology and sociobiology are rejected by some who claim that complex systems are inherently irreducible and that a holistic method is needed to understand them.\n\nSome strong reductionists believe that the behavioral sciences should become \"genuine\" scientific disciplines based on genetic biology, and on the systematic study of culture (see Richard Dawkins's concept of memes). In his book \"The Blind Watchmaker\", Dawkins introduced the term \"hierarchical reductionism\" to describe the opinion that complex systems can be described with a hierarchy of organizations, each of which is only described in terms of objects one level down in the hierarchy. He provides the example of a computer, which using hierarchical reductionism is explained in terms of the operation of hard drives, processors, and memory, but not on the level of logic gates, or on the even simpler level of electrons in a semiconductor medium.\n\nOthers argue that inappropriate use of reductionism limits our understanding of complex systems. In particular, ecologist Robert Ulanowicz says that science must develop techniques to study ways in which larger scales of organization influence smaller ones, and also ways in which feedback loops create structure at a given level, independently of details at a lower level of organization. He advocates (and uses) information theory as a framework to study propensities in natural systems. Ulanowicz attributes these criticisms of reductionism to the philosopher Karl Popper and biologist Robert Rosen.\n\nThe idea that phenomena such as emergence and work within the topic of complex systems theory pose limits to reductionism has been advocated by Stuart Kauffman. Emergence is especially relevant when systems exhibit historicity. Emergence is strongly related to nonlinearity. The limits of the application of reductionism are claimed to be especially evident at levels of organization with higher amounts of complexity, including living cells, neural networks, ecosystems, society, and other systems formed from assemblies of large numbers of diverse components linked by multiple feedback loops.\n\nNobel laureate Philip Warren Anderson used the idea that symmetry breaking is an example of an emergent phenomenon in his 1972 \"Science\" paper \"More is different\" to make an argument about the limitations of reductionism. One observation he made was that the sciences can be arranged roughly in a linear hierarchy—particle physics, solid state physics, chemistry, molecular biology, cellular biology, physiology, psychology, social sciences—in that the elementary entities of one science obeys the principles of the science that precedes it in the hierarchy; yet this does not imply that one science is just an applied version of the science that precedes it. He writes that \"At each stage, entirely new laws, concepts and generalizations are necessary, requiring inspiration and creativity to just as great a degree as in the previous one. Psychology is not applied biology nor is biology applied chemistry.\"\n\nDisciplines such as cybernetics and systems theory imply non-reductionism, sometimes to the extent of explaining phenomena at a given level of hierarchy in terms of phenomena at a higher level, in a sense, the opposite of reductionism.\n\nIn mathematics, reductionism can be interpreted as the philosophy that all mathematics can (or ought to) be based on a common foundation, which for modern mathematics is usually axiomatic set theory. Ernst Zermelo was one of the major advocates of such an opinion; he also developed much of axiomatic set theory. It has been argued that the generally accepted method of justifying mathematical axioms by their usefulness in common practice can potentially weaken Zermelo's reductionist claim.\n\nJouko Väänänen has argued for second-order logic as a foundation for mathematics instead of set theory, whereas others have argued for category theory as a foundation for certain aspects of mathematics.\n\nThe incompleteness theorems of Kurt Gödel, published during 1931, caused doubt about the attainability of an axiomatic foundation for all of mathematics. Any such foundation would have to include axioms powerful enough to describe the arithmetic of the natural numbers (a subset of all mathematics). Yet Gödel proved that for any self-consistent recursive axiomatic system powerful enough to describe the arithmetic of the natural numbers, there are propositions about the natural numbers that cannot be proved from the axioms, but which we can prove in the natural language with which we described the axioms. Such propositions are known as formally undecidable propositions. For example, the continuum hypothesis is undecidable in the Zermelo-Fraenkel set theory as shown by Cohen.\n\nReligious reductionism generally attempts to explain religion by explaining it in terms of nonreligious causes. A few examples of reductionistic explanations for the presence of religion are: that religion can be reduced to humanity's conceptions of right and wrong, that religion is fundamentally a primitive attempt at controlling our environments, that religion is a way to explain the existence of a physical world, and that religion confers an enhanced survivability for members of a group and so is reinforced by natural selection. Anthropologists Edward Burnett Tylor and James George Frazer employed some religious reductionist arguments. Sigmund Freud held that religion is nothing more than an illusion, or even a mental illness, and Marx claimed that religion is \"the sigh of the oppressed,\" and the opium of the people providing only \"the illusory happiness of the people,\" thus providing two influential examples of reductionistic views against the idea of religion.\n\nLinguistic reductionism is the idea that everything can be described or explained by a language with a limited number of concepts, and combinations of those concepts. An example is the language Toki Pona.\n\nThe concept of downward causation poses an alternative to reductionism within philosophy. This opinion is developed by Peter Bøgh Andersen, Claus Emmeche, Niels Ole Finnemann, and Peder Voetmann Christiansen, among others. These philosophers explore ways in which one can talk about phenomena at a larger-scale level of organization exerting causal influence on a smaller-scale level, and find that some, but not all proposed types of downward causation are compatible with science. In particular, they find that constraint is one way in which downward causation can operate. The notion of causality as constraint has also been explored as a way to shed light on scientific concepts such as self-organization, natural selection, adaptation, and control.\n\nPhilosophers of the Enlightenment worked to insulate human free will from reductionism. Descartes separated the material world of mechanical necessity from the world of mental free will. German philosophers introduced the concept of the \"noumenal\" realm that is not governed by the deterministic laws of \"phenomenal\" nature, where every event is completely determined by chains of causality. The most influential formulation was by Immanuel Kant, who distinguished between the causal deterministic framework the mind imposes on the world—the phenomenal realm—and the world as it exists for itself, the noumenal realm, which included free will. To insulate theology from reductionism, 19th century post-Enlightenment German theologians, especially Friedrich Schleiermacher and Albrecht Ritschl, used the Romantic method of basing religion on the human spirit, so that it is a person's feeling or sensibility about spiritual matters that comprises religion.\n\nThe anti-reductionist considers as minimum requirement upon the reductionist: \"At the very least the anti-reductionist is owed an account of why the intuitions arise if they are not accurate.\"\n\nA contrast to reductionism is holism or emergentism. Holism is the idea that items can have properties, (emergent properties), as a whole that are not explainable from the sum of their parts. The principle of holism was summarized concisely by Aristotle in the Metaphysics: \"The whole is more than the sum of its parts\".\n\nThe development of systems thinking has provided methods for describing issues in a holistic rather than a reductionist way, and many scientists use a holistic paradigm. When the terms are used in a scientific context, holism and reductionism refer primarily to what sorts of models or theories offer valid explanations of the natural world; the scientific method of falsifying hypotheses, checking empirical data against theory, is largely unchanged, but the method guides which theories are considered. The conflict between reductionism and holism in science is not universal—it usually concerns whether or not a holistic or reductionist method is appropriate in the context of studying a specific system or phenomenon.\n\nIn many cases (such as the kinetic theory of gases), given a good understanding of the components of the system, one can predict all the important properties of the system as a whole. In other systems, emergent properties of the system are said to be almost impossible to predict from knowledge of the parts of the system. Complexity theory studies systems and properties of the latter type.\n\nAlfred North Whitehead's metaphysics opposed reductionism. He refers to this as the \"fallacy of the misplaced concreteness\". His scheme was to frame a rational, general understanding of phenomena, derived from our reality.\n\nSven Erik Jorgensen, an ecologist, states both theoretical and practical arguments for a holistic method in certain topics of science, especially ecology. He argues that many systems are so complex that it will not ever be possible to describe all their details. Making an analogy to the Heisenberg uncertainty principle in physics, he argues that many interesting and relevant ecological phenomena cannot be replicated in laboratory conditions, and thus cannot be measured or observed without influencing and changing the system in some way. He also indicates the importance of interconnectedness in biological systems. His opinion is that science can only progress by outlining what questions are unanswerable and by using models that do not attempt to explain everything in terms of smaller hierarchical levels of organization, but instead model them on the scale of the system itself, taking into account some (but not all) factors from levels both higher and lower in the hierarchy.\n\nIn cognitive psychology, George Kelly developed \"constructive alternativism\" as a form of personal construct psychology, this provided an alternative to what he considered \"accumulative fragmentalism\". For this theory, knowledge is seen as the construction of successful mental models of the exterior world, rather than the accumulation of independent \"nuggets of truth\".\n\n\"Fragmentalism\" is an alternative term for ontological reductionism, although \"fragmentalism\" is frequently used in a pejorative sense. Anti-realists use the term fragmentalism in arguments that the world does not exist of separable entities, instead consisting of wholes. For example, advocates of this idea claim that: \n\nThe linear deterministic approach to nature and technology promoted a fragmented perception of reality, and a loss of the ability to foresee, to adequately evaluate, in all their complexity, global crises in ecology, civilization and education.\nThe term \"fragmentalism\" is usually applied to reductionist modes of thought, frequently with the related pejorative term of \"scientism\". This usage is popular amongst some ecological activists: There is a need now to move away from scientism and the ideology of cause-and-effect determinism toward a radical empiricism, such as William James proposed, as an epistemology of science. These perspectives are not new and during the early twentieth century, William James noted that rationalist science emphasized what he termed fragmentation and disconnection.\n\nSuch opinions also motivate many criticisms of the scientific method: \nThe scientific method only acknowledges monophasic consciousness. The method is a specialized system that emphasizes studying small and distinctive parts in isolation, which results in fragmented knowledge.\n\n\n"}
{"id": "24233616", "url": "https://en.wikipedia.org/wiki?curid=24233616", "title": "SOLID", "text": "SOLID\n\nIn object-oriented computer programming, SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable. It is not related to the GRASP software design principles. The principles are a subset of many principles promoted by Robert C. Martin. Though they apply to any object-oriented design, the SOLID principles can also form a core philosophy for methodologies such as agile development or adaptive software development. The theory of SOLID principles was introduced by Martin in his 2000 paper \"Design Principles and Design Patterns\", although the SOLID acronym itself was introduced later by Michael Feathers.\n\n\n"}
{"id": "25948756", "url": "https://en.wikipedia.org/wiki?curid=25948756", "title": "Salva congruitate", "text": "Salva congruitate\n\nSalva congruitate is a Latin scholastic term in logic, which means \"without becoming ill-formed\", \"salva\" meaning \"rescue\", \"salvation\", \"welfare\" and \"congruitate\" meaning \"combine\", \"coincide\", \"agree\". Salva Congruitate is used in logic to mean that two terms may be substituted for each other while preserving grammaticality in all contexts.\n\nTimothy C. Potts describes \"salva congruitate\" as a form of replacement in the context of meaning. It is a replacement which preserves semantic coherence and should be distinguished from a replacement which preserves syntactic coherence but may yield an expression to which no meaning has been given. This means that supposing an original expression is meaningful, the new expression obtained by the replacement will also be meaningful, though it will not necessarily have the same meaning as the original one, nor, if the expression in question happens to be a proposition, will the replacement necessarily preserve the truth value of the original.\n\nBob Hale explains \"salva congruitate\", as applied to singular terms, as substantival expressions in natural language, which are able to replace singular terms without destructive effect on the grammar of a sentence. Thus the singular term 'Bob' may be replaced by the definite description 'the first man to swim the English Channel' \"salva congruitate\". Such replacement may shift both meaning and reference, and so, if made in the context of a sentence, may cause a change in truth-value. Thus terms which may be interchanged \"salva congruitate\" may not be interchangeable \"salva veritate\" (preserving truth). More generally, expressions of any type are interchangeable \"salva congruitate\" if and only if they can replace one another preserving grammaticality or well-formedness.\n\n"}
{"id": "1605470", "url": "https://en.wikipedia.org/wiki?curid=1605470", "title": "Satya", "text": "Satya\n\nSatya is the Sanskrit word for truth. It also refers to a virtue in Indian religions, referring to being truthful in one's thought, speech and action. In Yoga, \"satya\" is one of five yamas, the virtuous restraint from falsehood and distortion of reality in one's expressions and actions.\n\nIn the Vedas and later sutras, the meaning of the word satya () evolves into an ethical concept about truthfulness and is considered an important virtue. It means being true and consistent with reality in one's thought, speech and action.\n\nA related concept, sattva, also derived from \"sat\", means true essence, nature, spiritual essence, character. Sattva is also a guṇa, a psychology concept particularly in the Samkhya school of philosophy, where it means goodness, purity, clean, positive, one that advances good true nature of self.\n\nSatya has cognates in a number of diverse Indo-European languages, including the word \"sooth\" in English, \"istina\" (\"истина\") in Russian, \"sannhet\" in Norwegian and \"haithya\" in Avestan, the liturgical language of Zoroastrianism.\n\n\"Satya\" is a central theme in the Vedas. It is equated with and considered necessary to the concept Ṛta (Sanskrit ऋतं ṛtaṃ) – that which is properly joined, order, rule, nature, balance, harmony. Ṛta results from Satya in the Vedas, states Holdrege, as it regulates and enables the operation of the universe and everything within it. Satya (truth) is considered essential, and without it, the universe and reality falls apart, cannot function.\n\nIn Rigveda, opposed to \"rita\" and \"satya\" are \"anrita\" and \"asatya\" (falsehood). Truth and truthfulness is considered as a form of reverence for the divine, while falsehood a form of sin. \"Satya\" includes action and speech that is factual, real, true and reverent to \"Ṛta\" in Book 1, 4, 6, 7, 9 and 10 of Rigveda. However, \"Satya\" isn't merely about one's past that is in context in the Vedas, it has one's current and one's future contexts as well. De Nicolás states, that in Rigveda, \"\"Satya\" is the modality of acting in the world of \"Sat\", as the truth to be built, formed or established\".\n\n\"Satya\" is a widely discussed concept in various Upanishads, including the Brihadaranyaka Upanishad where \"satya\" is called the means to Brahman, as well as Brahman (Being, true self). In hymn 1.4.14 of Brihadaranyaka Upanishad, \"Satya\" (truth) is equated to Dharma (morality, ethics, law of righteousness), as\n\nTaittiriya Upanishad's hymn 11.11 states, \"Speak the Satya (truth), conduct yourself according to the Dharma (morality, ethics, law)\".\n\nTruth is sought, praised in the hymns of Upanishads, held as one that ultimately, always prevails. The Mundaka Upanishad, for example, states in Book 3, Chapter 1,\n\nSandilya Upanishad of Atharvanaveda, in Chapter 1, includes ten forbearances as virtues, in its exposition of Yoga. It defines Satya as \"the speaking of the truth that conduces to the well being of creatures, through the actions of one's mind, speech or body.\"\n\nDeussen states that \"Satya\" is described in the major Upanishads with two layers of meanings - one as empirical truth about reality, another as abstract truth about universal principle, being and the unchanging. Both these ideas are explained in early Upanishads, composed before 500 BC, by variously breaking the word \"satya\" or \"satyam\" into two or three syllables. In later Upanishads, the ideas evolve and transcend into \"satya\" as truth (or truthfulness), and Brahman as the Being, Be-ness, real Self, the eternal.\n\nThe \"Shanti Parva\" of the Mahabharata states, \"The righteous hold that forgiveness, truth, sincerity and compassion are the foremost (of all virtues). Truth is the essence of the Vedas.\"\n\nThe Epic repeatedly emphasizes that \"Satya\" is a basic virtue, because everything and everyone depends on and relies on \"Satya\".\n\nIn the Yoga Sutras of Patanjali, it is written, “When one is firmly established in speaking truth, the fruits of action become subservient to him.\" In Yoga sutra, \"Satya\" is one of the five yamas, or\nvirtuous restraints, along with ahimsa (restraint from violence or injury to any living being); asteya (restraint from stealing); brahmacharya (celibacy or restraint from sexually cheating on one's partner); and aparigraha (restraint from covetousness and craving). Patanjali considers \"satya\" as a restraint from falsehood in one's action (body), words (speech, writing), or feelings / thoughts (mind). In Patanjali's teachings, one may not always know the truth or the whole truth, but one knows if one is creating, sustaining or expressing falsehood, exaggeration, distortion, fabrication or deception. \"Satya\" is, in Patanjali's Yoga, the virtue of restraint from such falsehood, either through silence or through stating the truth without any form of distortion.\n\nSatya is one of the five vows prescribed in Jain Agamas. Satya was also preached by Mahavira. According to Jainism, not to lie or speak what is not commendable. The underlying cause of falsehood is passion and therefore, it is said to cause \"hiṃsā\" (injury).\n\nAccording to the Jain text \"Sarvārthasiddhi\": \"that which causes pain and suffering to the living is not commendable, whether it refers to actual facts or not\".\n\nAccording to Jain text, \"Puruşārthasiddhyupāya\":\n\nThe term \"satya\" (Sanskrit; in Pali: \"sacca\") is translated in English as \"reality\" or \"truth.\" In terms of the Four Noble Truths (\"ariyasacca\"), the Pali can be written as \"sacca\", \"tatha\", \"anannatatha\" and \"dhamma\".\n\n'The Four Noble Truths' (\"ariya-sacca\") are the briefest synthesis of the entire teaching of Buddhism, since all those manifold doctrines of the threefold Pali canon are, without any exception, included therein. They are the truth of suffering (mundane mental and physical phenomenon), of the origin of suffering (tanha 'pali' the craving), of the extinction of suffering (Nibbana or nirvana), and of the Noble Eightfold Path leading to the extinction of suffering (the eight supra-mundane mind factors ).\n\nThe motto of the republic of India's emblem is Satyameva Jayate which is literally translated as 'Truth alone triumphs'.\n\n\n"}
{"id": "27838", "url": "https://en.wikipedia.org/wiki?curid=27838", "title": "Sequence", "text": "Sequence\n\nIn mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed. Like a set, it contains members (also called \"elements\", or \"terms\"). The number of elements (possibly infinite) is called the \"length\" of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order matters. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first \"n\" natural numbers (for a sequence of finite length \"n\"). The position of an element in a sequence is its \"rank\" or \"index\"; it is the natural number from which the element is the image. It depends on the context or a specific convention, if the first element has index 0 or 1. When a symbol has been chosen for denoting a sequence, the \"n\"th element of the sequence is denoted by this symbol with \"n\" as subscript; for example, the \"n\"th element of the Fibonacci sequence is generally denoted \"F\".\n\nFor example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be \"finite\", as in these examples, or \"infinite\", such as the sequence of all even positive integers (2, 4, 6, ...). In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.\nA sequence can be thought of as a list of elements with a particular order. Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. In particular, sequences are the basis for series, which are important in differential equations and analysis. Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.\n\nThere are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. One way to specify a sequence is to list the elements. For example, the first four odd numbers form the sequence (1, 3, 5, 7). This notation can be used for infinite sequences as well. For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...). Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements. Other ways to denote a sequence are discussed after the examples.\n\nThe prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves. Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...). The prime numbers are widely used in mathematics and specifically in number theory.\n\nThe Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements. The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).\n\nFor a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.\n\nOther examples of sequences include ones made up of rational numbers, real numbers, and complex numbers. The sequence (.9, .99, .999, .9999, ...) approaches the number 1. In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion. For instance, is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). A related sequence is the sequence of decimal digits of , i.e. (3, 1, 4, 1, 5, 9, ...). This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.\n\nOther notations can be useful for sequences whose pattern cannot be easily guessed, or for sequences that do not have a pattern such as the digits of . One such notation is to write down a general formula for computing the \"n\"th term as a function of \"n\", enclose it in parentheses, and include a subscript indicating the range of values that \"n\" can take. For example, in this notation the sequence of even numbers could be written as formula_1. The sequence of squares could be written as formula_2. The variable \"n\" is called an index, and the set of values that it can take is called the index set.\n\nIt is often useful to combine this notation with the technique of treating the elements of a sequence as variables. This yields expressions like formula_3, which denotes a sequence whose \"n\"th element is given by the variable formula_4. For example:\nNote that we can consider multiple sequences at the same time by using different variables; e.g. formula_6 could be a different sequence than formula_3. We can even consider a sequence of sequences: formula_8 denotes a sequence whose \"m\"th term is the sequence formula_9.\n\nAn alternative to writing the domain of a sequence in the subscript is to indicate the range of values that the index can take by listing its highest and lowest legal values. For example, the notation formula_10 denotes the ten-term sequence of squares formula_11. The limits formula_12 and formula_13 are allowed, but they do not represent valid values for the index, only the supremum or infimum of such values, respectively. For example, the sequence formula_14 is the same as the sequence formula_3, and does not contain an additional term \"at infinity\". The sequence formula_16 is a bi-infinite sequence, and can also be written as formula_17.\n\nIn cases where the set of indexing numbers is understood, the subscripts and superscripts are often left off. That is, one simply writes formula_18 for an arbitrary sequence. Often, the index \"k\" is understood to run from 1 to ∞. However, sequences are frequently indexed starting from zero, as in\nIn some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred. In these cases the index set may be implied by a listing of the first few abstract elements. For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.\n\n\nMoreover, the subscripts and superscripts could have been left off in the third, fourth, and fifth notations, if the indexing set was understood to be the natural numbers. Note that in the second and third bullets, there is a well-defined sequence formula_25, but it is not the same as the sequence denoted by the expression.\n\nSequences whose elements are related to the previous elements in a straightforward way are often defined using recursion. This is in contrast to the definition of sequences of elements as functions of their positions.\n\nTo define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it. In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule. The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions. Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.\n\nThe Fibonacci sequence can be defined using a recursive rule along with two initial elements. The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.\nThe first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34. A more complicated example of a sequence that is defined recursively is Recaman's sequence. We can define Recaman's sequence by\n\nNot all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated. For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).\n\nMany sequences have the property that each element of a sequence can be computed from the previous element. In this case, there is some function \"f\" such that for all \"n\", formula_32.\n\nThere are many different notions of sequences in mathematics, some of which (\"e.g.\", exact sequence) are not covered by the definitions and notations introduced below.\n\nFor the purposes of this article, we define a sequence to be a function whose domain is an interval of integers. This definition covers several different uses of the word \"sequence\", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions). However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers. The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice. In some contexts, to shorten exposition, the codomain of the sequence is fixed by context, for example by requiring it to be the set R of real numbers, the set C of complex numbers, or a topological space.\n\nAlthough sequences are a type of function, they are usually distinguished notationally from functions in that the input is written as a subscript rather than in parentheses, i.e. \"a\" rather than \"f\"(\"n\"). There are terminological differences as well: the value of a sequence at the input 1 is called the \"first element\" of the sequence, the value at 2 is called the \"second element\", etc. Also, while a function abstracted from its input is usually denoted by a single letter, e.g. \"f\", a sequence abstracted from its input is usually written by a notation such as formula_33, or just as formula_34. Here \"A\" is the domain, or index set, of the sequence.\n\nSequences and their limits (see below) are important concepts for studying topological spaces. An important generalization of sequences is the concept of nets. A net is a function from a (possibly uncountable) directed set to a topological space. The notational conventions for sequences normally apply to nets as well.\n\nThe length of a sequence is defined as the number of terms in the sequence.\n\nA sequence of a finite length \"n\" is also called an \"n\"-tuple. Finite sequences include the empty sequence ( ) that has no elements.\nNormally, the term \"infinite sequence\" refers to a sequence that is infinite in one direction, and finite in the other—the sequence has a first element, but no final element. Such a sequence is called a singly infinite sequence or a one-sided infinite sequence when disambiguation is necessary. In contrast, a sequence that is infinite in both directions—i.e. that has neither a first nor a final element—is called a bi-infinite sequence, two-way infinite sequence, or doubly infinite sequence. A function from the set Z of \"all\" integers into a set, such as for instance the sequence of all even integers ( …, −4, −2, 0, 2, 4, 6, 8… ), is bi-infinite. This sequence could be denoted formula_35.\n\nA sequence is said to be \"monotonically increasing\", if each term is greater than or equal to the one before it. For example, the sequence formula_36 is monotonically increasing if and only if \"a\" formula_37 \"a\" for all \"n\" ∈ N. If each consecutive term is strictly greater than (>) the previous term then the sequence is called strictly monotonically increasing. A sequence is monotonically decreasing, if each consecutive term is less than or equal to the previous one, and strictly monotonically decreasing, if each is strictly less than the previous. If a sequence is either increasing or decreasing it is called a monotone sequence. This is a special case of the more general notion of a monotonic function.\n\nThe terms nondecreasing and nonincreasing are often used in place of \"increasing\" and \"decreasing\" in order to avoid any possible confusion with \"strictly increasing\" and \"strictly decreasing\", respectively.\n\nIf the sequence of real numbers (\"a\") is such that all the terms are less than some real number \"M\", then the sequence is said to be bounded from above. In other words, this means that there exists \"M\" such that for all \"n\", \"a\" ≤ \"M\". Any such \"M\" is called an \"upper bound\". Likewise, if, for some real \"m\", \"a\" ≥ \"m\" for all \"n\" greater than some \"N\", then the sequence is bounded from below and any such \"m\" is called a \"lower bound\". If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.\n\nA subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements. For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...). The positions of some elements change when other elements are deleted. However, the relative positions are preserved.\n\nFormally, a subsequence of the sequence formula_3 is any sequence of the form formula_39, where formula_40 is a strictly increasing sequence of positive integers.\n\nSome other types of sequences that are easy to define include:\n\nAn important property of a sequence is \"convergence\". If a sequence converges, it converges to a particular value known as the \"limit\". If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.\n\nInformally, a sequence has a limit if the elements of the sequence become closer and closer to some value formula_41 (called the limit of the sequence), and they become and remain \"arbitrarily\" close to formula_41, meaning that given a real number formula_43 greater than zero, all but a finite number of the elements of the sequence have a distance from formula_41 less than formula_43. \n\nFor example, the sequence formula_46 shown to the right converges to the value 0. On the other hand, the sequences formula_47 (which begins 1, 8, 27, …) and formula_48 (which begins -1, 1, -1, 1, …) are both divergent. \n\nIf a sequence converges, then the value it converges to is unique. This value is called the limit of the sequence. The limit of a convergent sequence formula_34 is normally denoted formula_50. If formula_34 is a divergent sequence, then the expression formula_50 is meaningless.\n\nA sequence of real numbers formula_34 converges to a real number formula_41 if, for all formula_55, there exists a natural number formula_56 such that for all formula_57 we have formula_58\n\nIf formula_34 is a sequence of complex numbers rather than a sequence of real numbers, this last formula can still be used to define convergence, with the provision that formula_60 denotes the complex modulus, i.e. formula_61. If formula_34 is a sequence of points in a metric space, then the formula can be used to define convergence, if the expression formula_63 is replaced by the expression formula_64, which denotes the distance between formula_4 and formula_41.\n\nIf formula_34 and formula_68 are convergent sequences, then the following limits exist, and can be computed as follows:\n\n\nMoreover:\n\nA Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is \"Cauchy characterization of convergence for sequences\":\nIn contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined by\n\"x\" = 1 and \"x\" = \nis Cauchy, but has no rational limit, cf. . More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.\n\nMetric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.\n\nIn calculus, it is common to define notation for sequences which do not converge in the sense discussed above, but which instead become and remain arbitrarily large, or become and remain arbitrarily negative. If formula_4 becomes arbitrarily large as formula_87, we write\nIn this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is .\n\nIf formula_4 becomes arbitrarily negative (i.e. negative and large in magnitude) as formula_87, we write\nand say that the sequence diverges or converges to negative infinity.\n\nA series is, informally speaking, the sum of the terms of a sequence. That is, it is an expression of the form formula_92 or formula_93, where formula_34 is a sequence of real or complex numbers. The partial sums of a series are the expressions resulting from replacing the infinity symbol with a finite number, i.e. the \"N\"th partial sum of the series formula_92 is the number\nThe partial sums themselves form a sequence formula_97, which is called the sequence of partial sums of the series formula_92. If the sequence of partial sums converges, then we say that the series formula_92 is convergent, and the limit formula_100 is called the value of the series. The same notation is used to denote a series and its value, i.e. we write formula_101.\n\nSequences play an important role in topology, especially in the study of metric spaces. For instance:\n\nSequences can be generalized to nets or filters. These generalizations allow one to extend some of the above theorems to spaces without metrics.\n\nThe topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.\n\nMore formally, given a sequence of spaces formula_102, the product space\n\nis defined as the set of all sequences formula_104 such that for each \"i\", formula_105 is an element of formula_106. The canonical projections are the maps \"p\" : \"X\" → \"X\" defined by the equation formula_107. Then the product topology on \"X\" is defined to be the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections \"p\" are continuous. The product topology is sometimes called the Tychonoff topology.\n\nIn analysis, when talking about sequences, one will generally consider sequences of the form\nwhich is to say, infinite sequences of elements indexed by natural numbers.\n\nIt may be convenient to have the sequence start with an index different from 1 or 0. For example, the sequence defined by \"x\" = 1/log(\"n\") would be defined only for \"n\" ≥ 2. When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given \"N\".\n\nThe most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers. This type can be generalized to sequences of elements of some vector space. In analysis, the vector spaces considered are often function spaces. Even more generally, one can study sequences with elements in some topological space.\n\nA sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.\n\nThe most important sequences spaces in analysis are the ℓ spaces, consisting of the \"p\"-power summable sequences, with the \"p\"-norm. These are special cases of L spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted \"c\" and \"c\", with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.\n\nSequences over a field may also be viewed as vectors in a vector space. Specifically, the set of \"F\"-valued sequences (where \"F\" is a field) is a function space (in fact, a product space) of \"F\"-valued functions over the set of natural numbers.\n\nAbstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.\n\nIf \"A\" is a set, the free monoid over \"A\" (denoted \"A\", also called Kleene star of \"A\") is a monoid containing all the finite sequences (or strings) of zero or more elements of \"A\", with the binary operation of concatenation. The free semigroup \"A\" is the subsemigroup of \"A\" containing all elements except the empty sequence.\n\nIn the context of group theory, a sequence\nof groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:\n\nNote that the sequence of groups and homomorphisms may be either finite or infinite.\n\nA similar definition can be made for certain other algebraic structures. For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.\n\nIn homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations. Spectral sequences are a generalization of exact sequences, and since their introduction by , they have become an important research tool, particularly in homotopy theory.\n\nAn ordinal-indexed sequence is a generalization of a sequence. If α is a limit ordinal and \"X\" is a set, an α-indexed sequence of elements of \"X\" is a function from α to \"X\". In this terminology an ω-indexed sequence is an ordinary sequence.\n\nAutomata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ. Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a \"word\" (or input word). The sequence of states encountered by the automaton when processing a word is called a \"run\". A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter. This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.\n\nInfinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science. They are often referred to simply as \"sequences\" or \"streams\", as opposed to finite \"strings\". Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}). The set \"C\" = {0, 1} of all infinite binary sequences is sometimes called the Cantor space.\n\nAn infinite binary sequence can represent a formal language (a set of strings) by setting the \"n\" th bit of the sequence to 1 if and only if the \"n\" th string (in shortlex order) is in the language. This representation is useful in the diagonalization method for proofs.\n\n\n\n\n\n\n"}
{"id": "29719523", "url": "https://en.wikipedia.org/wiki?curid=29719523", "title": "Significant acts of violence against LGBT people", "text": "Significant acts of violence against LGBT people\n\nThis is a list of notable homophobic violence,\ne.g. attacks on victims thought by the attacker to be lesbian or gay and attacked for homophobic motives.\n\nSee list of unlawfully killed transgender people for homicides of transgender people.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn 2009, the Scottish parliament unanimously passed legislation that means that crimes motivated by hatred of gay or disabled people will now be considered as 'aggravated offences'.\n\n\n"}
{"id": "15144375", "url": "https://en.wikipedia.org/wiki?curid=15144375", "title": "Social network aggregation", "text": "Social network aggregation\n\nSocial network aggregation is the process of collecting content from multiple social network services, such as Instagram, Tumblr, Flickr, LinkedIn, Twitch, YouTube, etc. into one unified presentation. The task is often performed by a social network aggregator (such as Hootsuite, Taggbox and FriendFeed), which pulls together information into a single location, or helps a user consolidate multiple social networking profiles into one profile. Various aggregation services provide tools or widgets to allow users to consolidate messages, track friends, combine bookmarks, search across multiple social networking sites, read RSS feeds for multiple social networks, see when their name is mentioned on various sites, access their profiles from a single interface, provide \"lifestreams\", etc. Social network aggregation services attempt to organize or simplify a user's social networking experience, although the idea has been satirized by the concept of a \"social network aggregator aggregator\".\n\nThere are other related uses of social media aggregators aside from simplifying the user's social networking experiences. Some aggregators are designed to help companies (and bloggers) improve engagement with their brand(s) by creating aggregated social streams that can be embedded into an existing website and customized to look visually intrinsic to the site. This allows potential customers to interact with all the social media posts maintained by the brand without requiring them to jump from site to site. This has the benefit of keeping customers on the brand's site for a longer period of time (increasing \"time on site\" metrics).\n\nSocial network aggregation platforms allow social network members to share social network activities like Twitter, YouTube, Stumbleupon, Digg, Delicious, with other major platforms. All content appears in real time to other members who subscribe to a particular community, which eliminates the need to jump from one social media network to another, trying to keep an eye on one's interests.\n\nSocial network aggregation systems can rely on initiation by publishers or by readers. In the publisher-initiated aggregation systems, the publishers combine their own identities, which make their readers see all aggregated content once subscribed. In the reader-initiated systems (such as Windows Phone 7 people hub and Linked Internet UI), the readers combine the identities of others, which has no impact on the publishers or other readers. The publishers can still keep separate identities for different readers.\n\nTechnically, the aggregation is enabled by APIs provided by social networks. For the API to access a user's actions from another platform, the user will have to give permission to the social-aggregation platform, by specifying the user-id and password of the social media to be syndicated. This concept resembles open ID. In March 2008, \"The Economist\" reported that social network services are only beginning the move away from \"walled gardens\" to more open architectures. Some sites are working together on a \"data portability workgroup\", while others are focusing on a single sign-on system called OpenID to allow users to log on across multiple sites. Historically, the trend from private services to more open ones can be seen across many Internet services from email and instant messaging to the move that early online service providers made to become websites. The OpenSocial initiative aims to bridge the member overlap between various online social network services.\n\nThe attraction of social network aggregation comes from the fact that some users tend to use multiple social networks: they have accounts on several social-networking sites. In November 2007, Alex Patriquin of Compete.com reported on the member overlap between various online social network services:\n\nA 2009 study of 11,000 users\nreported that the majority of MySpace, LinkedIn, and Twitter users also have Facebook accounts.\n\n"}
{"id": "10012638", "url": "https://en.wikipedia.org/wiki?curid=10012638", "title": "Steven Goldberg", "text": "Steven Goldberg\n\nSteven Goldberg (born 14 October 1941) is a native of New York City and chaired the Department of Sociology at the City College of New York (CCNY) from 1988 until his retirement in 2008. He is most widely known for his theory of patriarchy, which attempts to explain male domination through biological causes.\n\n\n\n"}
{"id": "350990", "url": "https://en.wikipedia.org/wiki?curid=350990", "title": "Supervenience", "text": "Supervenience\n\nIn philosophy, supervenience refers to a relation between sets of properties or sets of facts. X is said to supervene on Y if and only if some difference in Y is necessary for any difference in X to be possible. Equivalently, X is said to supervene on Y if and only if X cannot vary unless Y varies. Here are some examples. \n\n\nA table could not be removed from a room without the positions of molecules in that room changing. In a logic with double negation elimination, the truth value of (A) could not change without the truth value of (¬A) changing. Facts about the properties of a molecule could not change without facts about its atomic constituents changing. And facts about the quality of Nixon's moral character could not change without facts about how he has acted, or about how he would be disposed to act, changing. \n\nSupervenience is of interest to philosophers because it differs from other nearby relations, for example entailment. It may be possible for some A to supervene on some B without being entailed by B. In such cases it may seem puzzling why A should supervene on B and equivalently why changes in A should require changes in B. Two important applications of supervenience involve cases like this. One of these is the supervenience of mental properties (like the sensation of pain) on physical properties (like the firing of ‘pain neurons’). A second is the supervenience of normative facts (facts about how things ought to be) on natural facts (facts about how things are).\n\nThese applications are elaborated below. But an illustrative note bears adding here. It is sometimes claimed (and has been claimed in earlier versions of this entry) that what is at issue in these problems is the supervenience claim itself. For example, it has been claimed that what is at issue with respect to the mind-body problem is whether mental phenomena do in fact supervene on physical phenomena. This is incorrect. It is by and large agreed that some form of supervenience holds in these cases: Pain happens when the appropriate neurons fire. The disagreement is over why this is so. Materialists claim that we observe supervenience because the neural phenomena entail the mental phenomena, while dualists deny this. The dualist’s challenge is to explain supervenience without entailment. \n\nThe problem is similar with respect to the supervenience of normative facts on natural facts. It is agreed that facts about how persons ought to act are not entailed by natural facts but cannot vary unless natural facts vary, and this rigid binding without entailment might seem puzzling. The possibility of \"supervenience without entailment\" or \"supervenience without reduction\" is contested territory among philosophers.\n\nSupervenience, which means literally \"coming or occurring as something novel, additional, or unexpected\", from \"super,\" meaning on, above, or additional, and \"venire,\" meaning to come, shows occurrences in the Oxford English Dictionary dating back to 1844.\n\nIts systematic use in philosophy is considered to have begun in early 20th-century meta-ethics and emergentism. As G.E. Moore wrote in 1922, \"if a given thing possesses any kind of intrinsic value in a certain degree, then... anything \"exactly like it\", must, under all circumstances, possess it in exactly the same degree\". This usage also carried over into the work of R. M. Hare. For discussion of the emergentist roots of supervenience see Stanford Encyclopedia of Philosophy: Supervenience.\n\nIn the 1970s, Donald Davidson was the first to use the term to describe a broadly physicalist (and non-reductive) approach to the philosophy of mind. As he said in 1970, \"supervenience might be taken to mean that there cannot be two events alike in all physical respects but differing in some mental respects, or that an object cannot alter in some mental respects without altering in some physical respects.\" \n\nIn subsequent years Terry Horgan, David Lewis, and especially Jaegwon Kim formalized the concept and began applying it to many issues in the philosophy of mind. This raised numerous questions about how various formulations relate to one another, how adequate the formulation is to various philosophical tasks (in particular, the task of formulating physicalism), and whether it avoids or entails reductionism.\n\nIn the contemporary literature, there are two primary (and non-equivalent) formulations of supervenience (for both definitions let A and B be sets of properties).\n\n(1) A-properties supervene on B-properties if and only if all things that are B-indiscernible are A-indiscernible. Formally:\n(2) A-properties supervene on B-properties if and only if anything that has an A-property has some B-property such that anything that has that B-property also has that A-property. Formally:\n\nFor example, if one lets A be a set of mental properties, lets B be a set of physical properties, and chooses a domain of discourse consisting of persons, then (1) says that any two persons who are physically indiscernible are mentally indiscernible, and (2) says that any person who has a mental property has some physical property such that any person with that physical property has that mental property.\n\nSome points of clarification: first, the definitions above involve quantification over properties and hence higher order logic. Second, in (1), expressions of the form formula_3 capture the concept of sharing all properties, or being indiscernible with respect to a set of properties. Thus, (1) can be understood more intuitively as the claim that all objects that are indiscernible with respect to a base set of properties are indiscernible with respect to a supervenient set of properties, or, as it is also sometimes said, that B-twins are A-twins. Finally, supervenience claims typically involve some modal force, however, the way that modal force is specified depends on which more specific variety of supervenience one decides upon (see below).\n\n(1) and (2) are sometimes called \"schemata\", because they do not correspond to actual supervenience relations until the sets of properties A and B, the domain of entities to which those properties apply, and a modal force have been specified. For modal forms of supervenience, the modal strength of the relation is usually taken to be a parameter (that is, the possible worlds appealed to may be physically possible, logically possible, etc.). Also, note that in the early literature properties were not always central, and there remain some who prefer to frame the relation in terms of predicates, facts, or entities instead, for example.\n\nBeginning in the 1980s, inspired largely by Jaegwon Kim's work, philosophers proposed many varieties of supervenience, which David Lewis called the \"unlovely proliferation\". These varieties are based both on (1) and (2) above, but because (1) is more common we shall focus on varieties of supervenience based on it.\n\nWe can begin by distinguishing between local and global supervenience:\n\n\nFor example, if mental states locally supervene on brain states, then being in the same brain state entails being in the same mental state.\n\n\nFor example, if psychological properties globally supervene on physical properties, then any two worlds physically the same will be psychologically the same. The value of global supervenience is that it allows for supervenient properties to be determined not by local properties of an individual thing alone, but by some wider spatiotemporal distribution of things and properties. For example, something's being a dollar bill depends not only on the paper and the inks it is made out of, but also on a widely dispersed variety of features of the world it occupies.\n\nBoth local and global supervenience come in many forms. Local supervenience comes in strong and weak varieties:\n\n\nThe difference is essentially whether correlations between base and supervenient properties hold within actual worlds only, or across possible worlds. For example, if psychological properties strongly locally supervene on physical properties, then any two people physically the same, in any two worlds, will also be psychologically the same. On the other hand, if psychological properties only weakly locally supervene on physical properties, then those correlations between base and supervenient properties that hold in virtue of the supervenience relation are maintained within each world, but can be different in different worlds. For example, my physical duplicates in the actual world will have the same thoughts as I have; but my physical duplicates in other possible worlds may have different thoughts than I have in the actual world.\n\nThere are also several kinds of global supervenience relations, which were introduced to handle cases in which worlds are the same at the base level and also at the supervenient level, but where the ways the properties are connected and distributed in the worlds differ. For example, it is consistent with global mental–physical supervenience on the simple formulation described above for two worlds to have the same number of people in the same physical states, but for the mental states to be distributed over those people in different ways (e.g. I have my father's thoughts in the other world, and he has my thoughts). To handle this, property-preserving isomorphisms (1-1 and onto functions between the objects of two worlds, whereby an object in one world has a property if and only if the object which that function takes you to in the other world does) are used, and once this is done, several varieties of global supervenience can be defined.\n\nOther varieties of supervenience include multiple-domains supervenience and similarity-based supervenience.\n\nThe value of a physical object to an agent is sometimes held to be supervenient upon the physical properties of the object. In aesthetics, the beauty of La Grande Jatte might supervene on the physical composition of the painting (the specific molecules that make up the painting), the artistic composition of the painting (in this case, dots), the figures and forms of the painted image, or the painted canvas as a whole. In ethics, the goodness of an act of charity might supervene on the physical properties of the agent, the mental state of the agent (his or her intention), or the external state of affairs itself. Similarly, the overall suffering caused by an earthquake might supervene on the spatio-temporal entities that constituted it, the deaths it caused, or the natural disaster itself. The claim that moral properties are supervenient upon non-moral properties is called moral supervenience.\n\nIn philosophy of mind, many philosophers make the general claim that the mental supervenes on the physical. In its most recent form this position derives from the work of Donald Davidson, although in more rudimentary forms it had been advanced earlier by others. The claim can be taken in several senses, perhaps most simply in the sense that the mental \"properties\" of a person are supervenient on their physical \"properties\". Then:\n\n\nAn alternative claim, advanced especially by John Haugeland, is a kind of \"weak local\" supervenience claim; or, weaker still, mere \"global\" supervenience. The claim that mental properties supervene globally on physical properties requires only a quite modest commitment: any difference between two possible worlds with respect to their instantiated mental properties entails at least \"some\" difference in the physical properties instantiated in those two worlds. Importantly, it does \"not\" require that the mental properties of an individual person supervene \"only\" on that person's physical state.\n\nThis weak global thesis is particularly important in the light of direct reference theories, and semantic externalism with regard to the content both of words and (more relevant to our concerns here) of \"thoughts\". Imagine two persons who are indistinguishable in their local physical properties. One has a dog in front of his eyes and the other has a dog-image artificially projected onto his retinae. It might be reasonable to say that the former is in the \"mental state\" of seeing a dog (and of knowing that he does so), whereas the latter is not in such a state of seeing a dog (but falsely believes that he sees one).\n\nThere is also discussion among philosophers about mental supervenience and our experience of duration. If all mental properties supervene only upon some physical properties at durationless moments, then it may be difficult to explain our experience of duration. The philosophical belief that mental and physical events exist as a series of durationless moments that lie between the physical past and the physical future is known as presentism, and is a form of belief in Galilean relativity.\n\nThere are several examples of supervenience to be found in computer networking. For example, in a dial-up internet connection, the audio signal on a phone line transports IP packets between the user's computer and the Internet service provider's computer. In this case, the arrangement of bytes in that packet supervenes on the physical properties of the phone signal. More generally, each layer of the OSI Model of computer networking supervenes on the layers below it.\n\nWe can find supervenience wherever a message is conveyed by a representational medium. When we see a letter \"a\" in a page of print, for example, the meaning \"Latin lowercase \"a\"\" supervenes on the geometry of the boundary of the printed glyph, which in turn supervenes on the ink deposition on the paper.\n\nIn biological systems phenotype can be said to supervene on genotype. This is because any genotype encodes a finite set of unique phenotypes, but any given phenotype is not produced by a finite set of genotypes. Innumerable examples of convergent evolution can be used to support this claim. Throughout nature, convergent evolution produces incredibly similar phenotypes from a diverse set of taxa with fundamentally different genotypes underpinning the phenotypes. One example is evolution on islands which is a remarkably predictable example of convergent evolution where the same phenotypes consistently evolve for the same reasons. Organisms released from predation tend to become larger, while organisms limited by food tend to become smaller. Yet there are almost infinite numbers of genetic changes that might lead to changes in body size. Another example of convergent evolution is the loss of sight that almost universally occurs in cave fish living in lightless pools. Eyes are expensive, and in lightless cave pools there is little reason for fish to have eyes. Yet, despite the remarkably consistent convergent evolution producing sightless cave fish, the genetics that produce the loss of type phenotype is different nearly every time. This is because phenotype supervenes on genotype.\n\nAlthough supervenience seems to be perfectly suited to explain the predictions of physicalism (i.e. the mental is dependent on the physical), there are four main problems with it. They are \"Epiphenomenal ectoplasm\", the \"lone ammonium molecule problem\", \"modal status problem\" and the \"problem of necessary beings.\"\n\n\"Epiphenomenal ectoplasm\" was proposed by Horgan and Lewis in 1983, in which they stated, a possible world (a world that could possibly exist) \"W\" is identical to our world in the distribution of all mental and physical characteristics (i.e. they are identical), except world \"W\" contains an experience called \"epiphenomenal ectoplasm\" that does not causally interact with that world. If supervenience physicalism is true, then such a world could not exist because a physical duplicate of the actual world (the world that is known to exist) could not possess an \"epiphenomenal ectoplasm\". This was rectified by Frank Jackson, by adjusting the application of supervenience within physicalism to state \"Physicalism is true at a possible world \"W\" if and only if any world which is a minimal physical duplicate (i.e. identical) of \"W\" is a duplicate of \"W simpliciter.\"\"\n\nThe \"lone ammonium molecule problem\" provides a problem for Jackson's solution to \"epiphenomenal ectoplasm\". It was proposed by Jaegwon Kim in 1993 when he stated that according to Jackson's idea of supervenience, a possible world \"W\" was identical to the actual world, except it possessed an extra ammonium molecule on one of Saturn's rings. This may not seem to provide much of a problem, but because Jackson's solution refers only to minimal physical duplicates, this allows for the mental properties of \"W\" to be vastly different from those in the actual world. If such a difference would cause mental differences on Earth, it would not be consistent with our understanding of physicalism.\n\nThe \"modal status problem\" is only problematic if one thinks of physicalism as a contingent truth (i.e. not necessary truth), because it is described in terms of modal notions (i.e. through modal realism). The problem is presented when, from the statement \"Minimal physical truths entail all truths,\" one derives the statement \"\"S\" (a statement that describes all minimal physical truths) entails \"S\"* (a statement that describes the world)\". This statement is a necessary truth, and therefore supervenience physicalism could not be contingent. The solution to this is to accept the above statement not as the equivalent of physicalism, but as an entailment of it.\n\nThe \"problem of necessary beings\" was proposed by Jackson in 1998, in which he stated that a necessary being exists in all possible worlds as a non-physical entity, and therefore proves physicalism false. However, physicalism allows for the existence of necessary beings, because any minimal physical duplicate would have the same mental properties as the actual world. This however is paradoxical, based on the fact that physicalism both permits and prevents the existence of such beings. This violates Hume's fork which states, \"there are no necessary connections between distinct existences\".\n\n\n"}
{"id": "40568313", "url": "https://en.wikipedia.org/wiki?curid=40568313", "title": "TAChart", "text": "TAChart\n\nTAChart is a component for the Lazarus IDE that provides charting services. Similar to Tchart and Teechart for Delphi it supports a collection of different chart types including bar charts, pie charts, line charts and point series. Apart from a screen canvas, output is possible in form of SVG, OpenGL, printer, WMF, and other formats.\n\nTAChart is bundled with the Lazarus Component Library. Although not intended to be a TChart clone, why its usage differs in certain points, its basic functionality is very similar and some source code written for TeeChart may be reused.\n\nThe first version of TAChart was developed by Philippe Martinole for the TeleAuto project, a program for automation of astronomic observations. Later functionality was introduced by Luis Rodrigues while porting the Epanet application from Delphi to Lazarus. In the ensuing years the code has extensively rewritten, expanded and is now maintained by Alexander Klenin. \n\n"}
{"id": "41304880", "url": "https://en.wikipedia.org/wiki?curid=41304880", "title": "Value criticism", "text": "Value criticism\n\nValue criticism (in German \"Wertkritik\") is a branch of post-Marxism which criticizes capitalistic society. Value criticism follows in the traditions of the Frankfurt School and critical theory developed under Max Horkheimer. Prominent adherents of value criticism include Robert Kurz, Roswitha Scholz, Moishe Postone and Jean-Marie Vincent.\nValue criticism takes crucial aspects of Marx's critiques of commodity fetishism, commodities and value, while criticizing Marx's theory of class struggle and historical materialism. As a result, proponents of Value Criticism feel that the working class is not necessarily a revolutionary subject. Instead, it is pointed out that labor has to be understood as a historic specific entity and that criticizing capitalism implies not only criticizing the distribution process in capitalism, but also the capitalistic production process and the productive powers. From this, they conclude that all actually existing socialisms thus far have been in essence forms of state-led capitalism, where production still followed capitalistic principles. This interpretation, which rejects the traditional Marxist point of view, is defended in the \"Manifesto Against Labour\".\n\nIn addition to value criticism, a new branch appeared within a circle of theorists led by Roswitha Scholz. The so-called critique of value direction (in German called \"Wertabspaltungskritik\") includes not only precise descriptions of the \"abstract and fetishized character of modern domination\", but also seeks to explain why irrational attitudes are delegated to women, while men are counted as relatively rational actors in the capitalistic society.\n\nMany leftists claim that books such as \"The Black Book of Capitalism\" neither offers a realistic alternative to capitalistic society nor provide any options for actions to overcome capitalism. Additionally, Gerhard Hanloser and Karl Reitter claim that value criticism is a critique from the standpoint of the circulation sphere which can not be explained from within itself.\n\nPublishers of value criticism in Germany are the \"Krisis\" group and Exit!, to both of which Robert Kurz made crucial contributions. Much value critical literature has still to be translated into English. Elmar Flatschart from the University of Vienna is also publishing in English for the Exit! group. Moishe Postone and his work \"Time, Labor and Social Domination\" is often cited by value critics.\n"}
{"id": "26188644", "url": "https://en.wikipedia.org/wiki?curid=26188644", "title": "Victim playing", "text": "Victim playing\n\nVictim playing (also known as playing the victim, victim card or self-victimization) is the fabrication of victimhood for a variety of reasons such as to justify abuse of others, to manipulate others, a coping strategy or attention seeking.\n\nVictim playing by abusers is either:\n\nIt is common for abusers to engage in victim playing. This serves two purposes:\n\nManipulators often play the victim role (\"poor me\") by portraying themselves as victims of circumstances or someone else's behavior in order to gain pity or sympathy or to evoke compassion and thereby get something from someone. Caring and conscientious people cannot stand to see anyone suffering, and the manipulator often finds it easy and rewarding to play on sympathy to get cooperation.\n\nWhile portraying oneself as a victim can be highly successful in obtaining goals over the short-term, this method tends to be less successful over time:\n\nVictim playing is also: \n\nThe language of \"victim playing\" has entered modern corporate life, as a potential weapon of all professionals. To define victim-players as dishonest may be an empowering response; as too may be awareness of how childhood boundary issues can underlay the tactic.\n\nIn the hustle of office politics, the term may however be abused so as to penalize the legitimate victim of injustice, as well as the role-player.\n\nTransactional analysis distinguishes real victims from those who adopt the role in bad faith, ignoring their own capacities to improve their situation. Among the predictable interpersonal \"games\" psychiatrist Eric Berne identified as common among by victim-players are \"Look How Hard I've Tried\" and \"Wooden Leg\". \n\nR. D. Laing considered that \"it will be difficult in practice to determine whether or to what extent a relationship is collusive\" – when \"the one person is predominantly the passive 'victim'\", and when they are merely playing the victim. The problem is intensified once a pattern of victimization has been internalised, perhaps in the form of a double bind.\n\nObject relations theory has explored the way possession by a false self can create a permanent sense of victimisation – a sense of always being in the hands of an external fate. \n\nTo break the hold of the negative complex, and to escape the passivity of victimhood, requires taking responsibility for one's own desires and long-term actions.\n\n\n"}
