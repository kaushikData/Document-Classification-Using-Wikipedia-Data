{"id": "30862604", "url": "https://en.wikipedia.org/wiki?curid=30862604", "title": "Achintya Bheda Abheda", "text": "Achintya Bheda Abheda\n\nAchintya-Bheda-Abheda (अचिन्त्यभेदाभेद, ' in IAST) is a school of Vedanta representing the philosophy of \"inconceivable one-ness and difference\". In Sanskrit \"achintya\" means 'inconceivable', \"bheda\" translates as 'difference', and \"abheda\" translates as 'non-difference'. The Gaudiya Vaishnava religious tradition employs the term in relation to the relationship of creation and creator (Krishna, Svayam Bhagavan), between God and his energies. It is believed that this philosophy was taught by the movement's theological founder Chaitanya Mahaprabhu (1486 - 1534) and differentiates the Gaudiya tradition from the other Vaishnava Sampradayas. It can be understood as an integration of the strict dualist (\"dvaita\") theology of Madhvacharya and the qualified monism (\"vishishtadvaita\") of Ramanuja.\n\nHistorically within Hinduism there are two conflicting philosophies regarding the relationship between living beings (Jiva or Atma) and God (Ishvara, Brahman or Bhagavan). Advaita schools assert the monistic view that \"the individual soul and God are one and the same\", whereas Dvaita schools give the dualistic argument that \"the individual soul and God are eternally separate\". The philosophy of Achintya-bheda-abheda includes elements of both viewpoints. The living soul is intrinsically linked with the Supreme Lord, and yet at the same time is not the same as God - the exact nature of this relationship being inconceivable to the human mind. The spirit Soul is considered to be part and parcel of the Supreme Lord. Same in quality but not in quantity. The Supreme Lord Sri Hari having all opulence in fullness, the spirit soul however having only a partial expression of His divine opulence. The Lord in this context is compared to a fire and the spirit souls as sparks coming off of the flame.\n\nThe theological tenet of \"achintya-bheda-abheda tattva\" reconciles the mystery that God is simultaneously \"one with and different from His creation\". In this sense Vaishnava theology is panentheistic as in no way does it deny the separate existence of God (Vishnu) in His own personal form. However, at the same time, creation (or what is termed in Vaishnava theology as the 'cosmic manifestation') is never separated from God. He always exercises supreme control over his creation. Sometimes directly, but most of the time indirectly through his different potencies or energies (Prakrti). Examples are given of a spider and its web; earth and plants that come forth and hair on the body of human being.\n\n\"One who knows God knows that the impersonal conception and personal conception are simultaneously present in everything and that there is no contradiction. Therefore Lord Caitanya established His sublime doctrine: acintya bheda-and-abheda-tattva -- simultaneous oneness and difference.\" (A. C. Bhaktivedanta Swami Prabhupada)\nThe analogy often used as an explanation in this context in the relationship between the Sun and the Sunshine. For example, both the sun and sunshine are part of the same reality, but there is a great difference between having a beam of sunshine in your room, and being in close proximity to the sun itself. Qualitatively the Sun and the Sunshine are not different, but as quantities they are very different. This analogy is applied to the living beings and God - the Jiva being of a similar quality to the Supreme being, but not sharing the qualities to an infinite extent, as would the Personality of Godhead himself. Thus there is a difference between the souls and the Supreme Lord.\n\nIt is clearly distinguished from the concept of \"anirvacaniya\" (inexpressible) of Advaita Vedanta. There is a clear difference between the two concepts as the two ideas arise for different reasons. Advaita concept is related to the ontological status of the world, whereas both Svayam bhagavan and his shaktis (in Lord himself and his powers) are empirically real, and they are different from each other, but at the same time they are the same. But that does not negate the reality of both.\n\nWhile it applied to relations between Purusha (the Lord) and Prakriti (be it material, marginal, or spiritual powers), in the theology of the concept there are areas of exceptions. Jiva Goswami also accepts that any object and its energy are non-different, such as fire and power of burning. While some maintain that its only a secondary extension of the principle that it is primarily applied to Svayam bhagavan and His energies. It does not, however, apply to differences between Avatars of Svayam bhagavan and Lord Himself, so the difference between Vishnu and His origin, is not covered by the concept of \"acintya bhedabheda\", i.e. it cannot be applied in cases where different levels of Purusha are compared.\n\nThe phrase is used as the chorus line in Kula Shaker's 1998 hit song Tattva. \"Achintya-bheda-abheda-tattva\".\n\n"}
{"id": "51097731", "url": "https://en.wikipedia.org/wiki?curid=51097731", "title": "Alfred Leo Smith", "text": "Alfred Leo Smith\n\nAlfred Leo Smith (November 6, 1919 - November 19, 2014), also known as Al Smith, was a Klamath Nation drug and alcohol counselor and Native American activist from Oregon.\n\nSmith was born on November 6, 1919 in Modoc Point, Oregon. He spent his early childhood on the Williamson River.\n\nAt age seven, Smith was sent to a nearby Catholic boarding school at the insistence of local Indian agents. He was ultimately sent to a mix of catholic schools and Bureau of Indian Affairs schools, as far away as Beaverton, Oregon and Stewart Indian School in Nevada. After leaving Chemawa Indian School, he began to live in Portland, Oregon as an alcoholic panhandler. He was drafted during World War II and was sent to federal prison for drinking on duty. He survived a bout of tuberculosis, and experienced the 1942 death of his sister and 1950 death of his mother. The United States Congress also voted to terminate the Klamath Nation in 1954, striking another blow against Smith.\n\nIn 1957, Smith became sober with the help of an Alcoholics Anonymous program, ultimately celebrating 56 years of sobriety. He also became interested in Native recovery through culturally relevant practices and indigenous spirituality.\n\nSmith began working for the Portland Alcoholism Counseling and Recovery Program, helping alcoholics in a community he knew well. In 1972, the Bureau of Indian Affairs hired Smith to set up a number of tribal treatment programs across the United States. He also worked in the Klamath Basin on drug and alcohol recovery issues, where he was able to reconnect with his heritage and culture.\n\nIn 1972, Al Smith began to work at Sweathouse Lodge, part of the Chicano-Indian Study Center of Oregon founded on the site of Camp Adair. His position as treatment coordinator allowed him to combine AA principles with traditional Native spiritual practices, particularly the daily sweat lodge ceremony.\n\nIn 1982, Smith began working at a nonprofit Drug and Alcohol counseling program in Roseburg, Oregon. After his colleague Galen Black was fired for ingesting peyote, Smith indignantly attended a ceremony of the Native American Church, declaring \"You can't tell me that I can't go to church!\".\n\nSmith was fired for using peyote as part of the ceremony. At the time, intentional possession of peyote was a crime under Oregon law without an affirmative defense for religious use. The counselors filed a claim for unemployment compensation with the state, but the claim was denied because the reason for their dismissal was deemed work-related \"misconduct.\" The Oregon Court of Appeals reversed that ruling, holding that denying them unemployment benefits for their religious use of peyote violated their right to exercise their religion. The Oregon Supreme Court agreed, although it relied not on the fact that peyote use was a crime but on the fact that the state's justification for withholding the benefits—preserving the \"financial integrity\" of the workers' compensation fund—was outweighed by the burden imposed on the employees' exercise of their religion. The state appealed to the U.S. Supreme Court, again arguing that denying the unemployment benefits was proper because possession of peyote was a crime.\n\nThe U.S. Supreme Court let stand the Oregon Supreme Court's judgment against the two employees and returned the case to the Oregon courts to determine whether or not sacramental use of illegal drugs violated Oregon's state drug laws (485 U.S. 660 (1988)). Writing for the majority, Antonin Scalia declared that the free exercise of religion did not protect minority religions from \"neutral, generally applicable laws.\" Scalia believed that \"...[l]eaving accommodation to the political process will place at a relative disadvantage those religious practices that are not widely engaged in, but that unavoidable consequence of democratic government must be preferred to a system in which each conscience is a law unto itself or in which judges weigh the social importance of all laws against the centrality of all religious beliefs.\"\n"}
{"id": "1870236", "url": "https://en.wikipedia.org/wiki?curid=1870236", "title": "Bad faith (existentialism)", "text": "Bad faith (existentialism)\n\nBad faith (French: \"mauvaise foi\") is a philosophical concept utilized by existentialist philosophers Simone de Beauvoir and Jean-Paul Sartre to describe the phenomenon in which human beings, under pressure from social forces, adopt false values and disown their innate freedom, hence acting inauthentically. It is closely related to the concepts of self-deception and \"ressentiment\".\n\nA critical claim in existentialist thought is that individuals are always free to make choices and guide their lives towards their own chosen goal or \"project\". The claim holds that individuals cannot escape this freedom, even in overwhelming circumstances. For instance, even an empire's colonized victims possess choices: to submit to rule, to negotiate, to commit suicide, to resist nonviolently, or to counter-attack.\n\nAlthough external circumstances may limit individuals (this limitation from the outside is called facticity), they cannot force a person to follow one of the remaining courses over another. In this sense the individual still has some freedom of choice. For this reason, individuals choose in \"anguish\": they know that they must make a choice, and that it will have consequences. For Sartre, to claim that one amongst many conscious possibilities takes undeniable precedence (for instance, \"I cannot risk my life, because I must support my family\") is to assume the role of an object in the world, not a free agent, but merely at the mercy of circumstance (a \"being-in-itself\" that is only its own facticity, i.e., it \"is\" inside itself, and acts there as a limitation).. For Sartre, this attitude is manifestly self-deceiving. \n\nAs conscious humans, we are always aware that we are more than what we are aware of, so we are not whatever we are aware of. We cannot, in this sense, be defined as our \"intentional objects\" of consciousness, including our restrictions imposed by (facticity) our personal history, character, bodies, or objective responsibility. Thus, as Sartre often repeated, \"Human reality is what it is not, and it is not what it is.\" An example of what he means is being a doctor but wishing to \"transcend\" that to become a pig farmer. One is who one is not: a pig farmer, not who one is, a doctor. \n\nOne can only define oneself negatively, as \"what it is not\", and this negation is the only positive definition of \"what it is.\"\n\nFrom this we are aware of a host of alternative reactions to our freedom to choose an objective situation, since no situation can dictate a single response. We pretend that these possibilities are denied to us by assuming social roles and value systems external to this nature . But this is itself a decision made possible by our freedom and our separation from these things. \n\n\"Bad faith\" is the paradoxical free decision to deny to ourselves this inescapable freedom.\n\nSartre cites a café waiter, whose movements and conversation are a little too \"waiter-esque\". His voice oozes with an eagerness to please; he carries food rigidly and ostentatiously; \"his movement is quick and forward, a little too precise, a little too rapid\". His exaggerated behaviour illustrates that he is play acting as a waiter, as an object in the world: an automaton whose essence is to be a waiter. But that he is obviously acting belies that he is aware that he is not (merely) a waiter, but is rather consciously deceiving himself.\n\nAnother of Sartre's examples involves a young woman on a first date. She ignores the obvious sexual implications of her date's compliments to her physical appearance, but accepts them instead as words directed at her as a human consciousness. As he takes her hand, she lets it rest indifferently in his, \"neither consenting nor resisting – a thing\" – refusing either to return the gesture or to rebuke it. Thus she delays the moment when she must choose either to acknowledge and reject his advances, or submit to them. She conveniently considers her hand only a thing in the world, and his compliments as unrelated to her body, playing on her dual human reality as a physical being, and as a consciousness separate and free from this physicality.\n\nSartre suggests that by acting in bad faith the waiter and the woman are denying their own freedom, but by actively using this freedom itself. They manifestly know they are free, but refuse to acknowledge it. Bad faith is paradoxical in this regard: when acting in bad faith, a person is actively denying their own freedom, however rely on it to perform the denial.\n\nDe Beauvoir described three main types of women acting in bad faith: the \"Narcissist\" who denies her freedom by construing herself as a desirable object; the \"Mystic\", who invests her freedom in an absolute; and the \"Woman in Love\", who submerges her identity in that of her male object.\n\nShe also considered what she called the \"Serious Man\", who subordinated himself to some outside cause, to be in bad faith inasmuch as he denies his own freedom.\n\nSartre claims that the consciousness with which we generally consider our surroundings is different from our reflecting on this consciousness, i.e., the consciousness of \"ourselves being conscious of these surroundings\". The first kind of consciousness, before we think about, or reflect on, our previous consciousness, is called \"pre-reflective\". Reflecting on the pre-reflective consciousness is called \"reflective\" consciousness. But this cannot be called unconsciousness, as Freud used the term. Sartre gives the example of running after a bus: one does not become conscious of \"one's running after the bus\" until one has ceased to run after it, because until then one's consciousness is focused on the bus itself, and not one's chasing it.\n\nIn this sense consciousness always entails being self-aware (\"being for-itself\"). Since for Sartre consciousness also entails a consciousness of our separation from the world, and hence freedom, we are also always aware of this. But we can manipulate these two levels of consciousness, so that our reflective consciousness interprets the factual limits of our objective situation as insurmountable, whilst our pre-reflective consciousness remains aware of alternatives.\n\nOne convinces oneself, in some sense, to be bound to act by external circumstance, in order to escape the anguish of freedom. Sartre says that man is \"condemned to be free\": whether he adopts an \"objective\" moral system to do this choosing for him, or follows only his pragmatic concerns, he cannot help but be aware that they are not – fundamentally – part of him. Moreover, as possible intentional objects of one's consciousness, one is fundamentally \"not\" part of oneself, but rather exactly what one, as consciousness, defines oneself in opposition to; along with everything else one could be conscious of.\n\nFundamentally, Sartre believes mankind cannot escape responsibility by adopting an external moral system, as the adoption of such is in itself a choice that we endorse, implicitly or explicitly, for which we must take full responsibility. Sartre argues that one cannot escape this responsibility, as each attempt to part one's self from the freedom of choice is in itself a demonstration of choice, and choice is dependent on a person's wills and desires.\n\nAs a human, one cannot claim his actions are determined by external forces; this is the core statement of existentialism. One is \"condemned\" to this eternal freedom; human beings exist before the definition of human identity exists. One cannot define oneself as a thing in the world, as one has the freedom to be otherwise. One is not \"a philosopher\", as at some point one must/will cease the activities that define the self as \"a philosopher\". Any role that one might adopt does not define one as there is an eventual end to one's adoption of the role; i.e. other roles will be assigned to us, \"a chef\", \"a mother\". The self is not constant, it cannot be a thing in the world. Though one cannot assign a positive value to definitions that may apply to oneself, one remains able to say what one is not.\n\nThis inner anguish over moral uncertainty is a central underlying theme in existentialism, as the anguish demonstrates a personal feeling of responsibility over the choices one makes throughout life. Without an emphasis on personal choice, one may make use of an external moral system as a tool to moralize otherwise immoral acts, leading to negation of the self. According to existentialism, dedicated professionals of their respective moral codes – priests interpreting sacred scriptures, lawyers interpreting the Constitution, doctors interpreting the Hippocratic oath – should, instead of divesting the self of responsibility in the discharge of one's duties, be aware of one's own significance in the process. This recognition involves the questioning of the morality of all choices, taking responsibility for the consequences of one's own choice and therefore; a constant reappraisal of one's own and others' ever-changing humanity. One must not exercise bad faith by denying the self's freedom of choice and accountability. Taking on the burden of personal accountability in all situations is an intimidating proposition – by pointing out the freedom of the individual, Sartre seeks to demonstrate that the social roles and moral systems we adopt protect us from being morally accountable for our actions.\n\n\n\n"}
{"id": "24077317", "url": "https://en.wikipedia.org/wiki?curid=24077317", "title": "Biological functionalism", "text": "Biological functionalism\n\nBiological functionalism is an anthropological paradigm, asserting that all social institutions, beliefs, values and practices serve to address pragmatic concerns. In many ways, the theorem derives from the longer-established structural functionalism, yet the two theorems diverge from one another significantly. While both maintain the fundamental belief that a social structure is composed of many interdependent frames of reference, biological functionalists criticise the structural view that a social solidarity and collective conscience is required in a functioning system. By that fact, biological functionalism maintains that our individual survival and health is the driving provocation of actions, and that the importance of social rigidity is negligible.\n\nAlthough the actions of humans without doubt do not always engender positive results for the individual, a biological functionalist would argue that the intention was still self-preservation, albeit unsuccessful. An example of this is the belief in luck as an entity; while a disproportionately strong belief in good luck may lead to undesirable results, such as a huge loss in money from gambling, biological functionalism maintains that the newly created ability of the gambler to condemn luck will allow them to be free of individual blame, thus serving a practical and individual purpose. In this sense, biological functionalism maintains that while bad results often occur in life, which do not serve any pragmatic concerns, an entrenched cognitive psychological motivation was attempting to create a positive result, in spite of its eventual failure.\n"}
{"id": "3731357", "url": "https://en.wikipedia.org/wiki?curid=3731357", "title": "Body without organs", "text": "Body without organs\n\nThe \"body without organs\" () is a concept used by French philosopher Gilles Deleuze. It usually refers to the deeper reality underlying some well-formed whole constructed from fully functioning parts. At the same time, it may also describe a relationship to one's literal body.\n\nDeleuze began using the term in \"The Logic of Sense\" (1969), while discussing the experiences of playwright Antonin Artaud. \"Body without Organs\" (or \"BwO\") later became a major part of the vocabulary for \"Capitalism and Schizophrenia\", two volumes (\"Anti-Oedipus\" [1972] and \"A Thousand Plateaus\" [1980]) written collaboratively with Félix Guattari. In these works, the term took on an expanded meaning, referring variously to literal bodies and to a certain perspective on realities of any type. The term's overloaded meaning is provocative, perhaps intentionally.\n\nThe term originates from Antonin Artaud's radio play \"To Have Done with the Judgment of God\" (1947):\nWhen you will have made him a body without organs, \nthen you will have delivered him from all his automatic reactions \nand restored him to his true freedom.\n\nDeleuze first mentions the phrase in a chapter of \"The Logic of Sense\" called \"The Schizophrenic and the Little Girl\", which contrasts two distinct and peripheral ways of encountering the world. The Little Girl (whose exemplar is Alice), explores a world of 'surfaces': the shifting realm of social appearances and nonsense words which nevertheless seem to function. The Schizophrenic (whose exemplar is Artaud) is by contrast an explorer of 'depths', one who rejects the surface entirely and returns instead to the body.\n\nFor the Schizophrenic, words collapse, not into nonsense, but into the bodies that produce and hear them. Deleuze refers to \"a new dimension of the schizophrenic body, an organism without parts which operates entirely by insufflation, respiration, evaporation and fluid transmission (the superior body or body without organs of Antonin Artaud).\" This body is also described as \"howling\", speaking a \"language without articulation\" that has more to do with the primal act of making sound than it does with communicating specific words.\n\nIn Deleuze and Guattari's collaboration, the term describes an undifferentiated, unhierarchical realm that lies deeper than the world of appearances. It relates to the proto-world described in the mythology of many different cultures. Deleuze and Guattari often use the example of the Dogon egg, based primarily on anthropological reports from Marcel Griaule. Describing the Dogon story of the origins of the cosmos, Griaule writes:\n\nThese primordial movements are conceived in terms of an ovoid form—'the egg of the world' (\"aduno tal\")—within which lie, already differentiated, the germs of things; in consequence of the spiral movement of extension the germs develop first in seven segments of increasing length, representing the seven fundamental seeds of cultivation, which are to be found again in the human body [...] \n\nAccording to Griaule, the basic patterns of organization within the egg reappear within all domains of Dogon life: kinship structures, village layout, understanding of the body, and so forth. The egg metaphor helps to suggest the gestation of a formation yet to come, and the potential formation of many actualities from a single origin.\n\nIn \"Anti-Oedipus\" Deleuze and Guattari expand the Body without Organs image by comparing its real potentials to the egg's : The body without organs is an egg: it is crisscrossed with axes and thresholds, with latitudes and longitudes and geodesic lines, traversed by gradients marking the transitions and the becomings, the destinations of the subject developing along these particular vectors (\"Capitalism and Schizophrenia\", p. 19).\n\nFor Deleuze and Guattari, every actual body has a limited set of traits, habits, movements, affects, etc. But every actual body also has a virtual dimension: a vast reservoir of potential traits, connections, affects, movements, etc. This collection of potentials is what Deleuze calls the BwO. The full body without organs is \"schizophrenia as a clinical entity\" (\"Anti-Oedipus,\" p. 310). This drop in intensity is a means of blocking all investments of reality: \"the unproductive, the sterile, the unengendered, the unconsumable\" (\"Anti-Oedipus,\" p. 9). Unlike other social machines such as the Body of the Earth, the Body of the Despot or the Body of Capital, the full body without organs cannot inscribe other bodies. The body without organs is \"not an original primordial entity\" (proof of an original nothingness) nor what is remains of a lost totality but is the \"ultimate residue of a deterritorialized socius\" (\"Anti-Oedipus,\" p. 309). To \"make oneself a body without organs,\" then, is to actively experiment with oneself to draw out and activate these virtual potentials. These potentials are mostly activated (or \"actualized\") through conjunctions with other bodies (or BwOs) that Deleuze calls \"becomings.\"\n\nDeleuze and Guattari use the term BwO in an extended sense, to refer to the virtual dimension of reality in general (which they more often call \"plane of consistency\" or \"plane of immanence\"). In this sense, they speak of a BwO of \"the earth.\" \"The Earth,\" they write, \"is a body without organs. This body without organs is permeated by unformed, unstable matters, by flows in all directions, by free intensities or nomadic singularities, by mad or transitory particles\" (\"A Thousand Plateaus\", p. 40). That is, we usually think of the world as composed of relatively stable entities (\"bodies,\" beings). But these bodies are really composed of sets of flows moving at various speeds (rocks and mountains as very slow-moving flows; living things as flows of biological material through developmental systems; language as flows of information, words, etc.). This fluid substratum is what Deleuze calls the BwO in a general sense.\n\nIn \"A Thousand Plateaus\", Deleuze and Guattari eventually differentiate between three kinds of \"BwO\": cancerous, empty, and full. Roughly, the empty \"BwO\" is the \"BwO\" of \"Anti-Oedipus\". This \"BwO\" is also described as \"catatonic\" because it is completely de-organ-ized; all flows pass through it freely, with no stopping, and no directing. Even though any form of desire can be produced on it, the empty \"BwO\" is non-productive. The full \"BwO\" is the healthy \"BwO\"; it is productive, but not petrified in its organ-ization. The cancerous \"BwO\" is caught in a pattern of endless reproduction of the self-same pattern. They give a rough recipe for building yourself a healthy BwO:\n\nDeleuze and Guattari suggest restraint here, writing that drug addicts and masochists may come closer to truly possessing bodies without organs—and die as a result. The 'healthy BwO' thus envisions the actual body without organs as a horizon, not a goal.\n\n\n"}
{"id": "27869995", "url": "https://en.wikipedia.org/wiki?curid=27869995", "title": "Burak Arıkan", "text": "Burak Arıkan\n\nBurak Arıkan (born in 1976, Istanbul) is a Turkish contemporary artist working with complex networks raising questions about economic, political, societal issues. His work is research-based, thus generates data and inputs into custom abstract machinery, builds network maps, results in performances, and procreates predictions to make inherent power relationships visible, thus discussable.\n\nArikan is the founder of Graph Commons, a platform for mapping, analyzing, and publishing data-networks. Graph Commons workshop for artists, activists, critical researchers, and civil society organizations are being conducted internationally.\n\nOne of Arikan's works MyPocket (2008) is a live software system that predicts what the artist buys every day and discloses his spending records to the world. MyPocket was shown in Neuberger Museum of Art New York, Künstlerhaus Bethanien Berlin, and Media Space / FilmWinter Stuttgart, and most recently in the New Observatory exhibition in FACT Liverpool.\n\nArikan is an adjunct faculty in Interactive Telecommunications Program, Tisch School of Arts, New York University.\n\nArikan completed his master's degree at the MIT Media Lab in Massachusetts Institute of Technology in the Physical Language Workshop led by John Maeda.\n\nArikan is a member of Alternative Informatics Association, a civil society organization in Turkey focusing on the issues of Internet freedom and digital rights.\n\n"}
{"id": "28086000", "url": "https://en.wikipedia.org/wiki?curid=28086000", "title": "Character mask", "text": "Character mask\n\nIn Marxist philosophy, a character mask () is a prescribed social role that serves to conceal the contradictions of a social relation or order. The term was used by Karl Marx in various published writings from the 1840s to the 1860s, and also by Friedrich Engels. It is related to the classical Greek concepts of mimesis (imitative representation using analogies) and prosopopoeia (impersonation or personification) as well as the Roman concept of persona, but also differs from them (see below). The notion of character masks has been used by neo-Marxist and non-Marxist sociologists, philosophers and anthropologists to interpret how people relate in societies with a complex division of labour, where people depend on trade to meet many of their needs. Marx's own notion of the character mask was not a fixed idea with a singular definition.\n\nAs a psychological term, \"character\" is traditionally used more in continental Europe, while in Britain and North America the term \"personality\" is used in approximately the same contexts. Marx however uses the term \"character mask\" analogously to a theatrical role, where the actor (or the characteristics of a prop) represents a certain interest or function, and intends by character both \"the characteristics of somebody\" and \"the characteristics of something\". Marx's metaphorical use of the term \"character masks\" refers back to carnival masks and the masks used in classical Greek theatre. At issue is the social form in which a practice is acted out.\n\nA sophisticated academic language for talking about the sociology of roles did not exist in the mid-19th century. Marx therefore borrowed from theatre and literature to express his idea. Although György Lukács pioneered a sociology of drama in 1909, a sociology of roles began only in the 1930s, and a specific sociology of theatre (e.g. by Jean Duvignaud) first emerged in the 1960s. Marx's concept is both that an identity appears differently from its true identity (it is masked or disguised), and that this difference has very real practical consequences (the mask is not simply a decoration, but performs a real function and has real effects, even independently of the mask bearer).\n\nThe nearest equivalent term in modern English for Marx's \"character masks\" is social masks. However, such a translation is not entirely satisfactory, for several reasons:\n\nThere is a link between character masks and the concept of deliberate misrepresentation and hypocrisy. Yet character masks need not be hypocritical, insofar as the motive for their use is genuine, sincere, principled or naive – or a product of (self-)delusion. People can also mask their behavior, or mask a situation, without being aware that they are doing so. Paul Ricœur explains:\n\nThe \"false awareness\" (\"falsches Bewusstsein\") in the classical sense used by Friedrich Engels does not necessarily refer to \"errors\" in the content of awareness. It refers rather to an \"absence of awareness\" of what is really behind the ideas being worked with, how they have originated, or what the real role or effect of the ideas is. The first result of that is, that the ideologists believe themselves to be performing certain intellectual operations with regard to an issue, which, in reality, have quite a different significance than what they imagine. The second result is that their intellectual creations can then function as a mask for what is really at stake, precisely because the issue is portrayed in a one-sided or distorted way – without the ideologists being aware of how that works. The ideologists are aware and unaware at the same time. The problem, says Engels, is that they exaggerate the power of ideas, even to the point where ideas seem to be the cause of all that happens. This occurs especially if the intellectual productions occur at quite some distance from the practical context to which they properly refer, or if they concern specialized, highly abstract ideas which cannot easily be verified\n\nThe \"character masks of an era\" refer, according to Marx and Engels, to its main symbolic expressions of self-justification or apologist, the function of which is to \"disguise\", \"embellish\" or \"mystify\" social contradictions (\"the bits that do not fit\"). A purported \"mystical truth\" in this context is a meaning which cannot be definitely proved, because it results from an abstract procedure or cognition which is not logical, and cannot be tested scientifically, only subjectively experienced.\n\nTerry Eagleton explains:\n\nMarx also argues that, insofar as capitalist class society is intrinsically a very contradictory system – it contains many conflicting and competing forces – the masking of its true characteristics becomes an integral feature of how it actually operates. Buyers and sellers compete with other buyers and sellers. Businesses cannot practically do so without confidentiality and secrecy. Workers compete for job opportunities and access to resources. Capitalists and workers compete for their share of the new wealth that is produced, and nations compete with other nations. The masks are therefore not optional, but necessary, and the more one is able to know about others, the more subtle, ingenious and sophisticated the masks become.\n\nOne of the centerpieces of Marx's critique of political economy is that the juridical labour contract between the worker and his capitalist employer obscures the true economic relationship, which is (according to Marx) that the workers do not sell their labor, but their labor power, making possible a profitable difference between what they are paid and the new value they create for the owners of capital (a form of economic exploitation). Thus, the very foundation of capitalist wealth creation involves – as Marx says explicitly – a \"mask\". More generally, Marx argues that transactions in the capitalist economy are often far from transparent – they appear different from what they really are. This is discovered, only when one probes the total context in which they occur. Hence Marx writes:\n\nThis implies another level of masking, because the economic character masks are then straightforwardly (\"vulgarly\") equated with authentic behaviour. The effect in this case is, that the theory of \"how the economy works\" masks how it actually works, by conflating its surface appearance with its real nature. Its generalities \"seem\" to explain it, but in reality they do not. The theory is therefore (ultimately) \"arbitrary\". Either things are studied in isolation from the total context in which they occur, or generalizations are formed which leave essential bits out. Such distortion can certainly be ideologically useful to justify an economic system, position or policy as a good thing, but it can become a hindrance to understanding.\n\nAbstractly, the masking processes specific to capitalist society mediate and reconcile social contradictions, which arise from three main sources:\n\n\nIn \"The Communist Manifesto\", Marx & Engels had stated that:\n\nThis \"naked self-interest\" seems to contradict the idea of \"masking\" in bourgeois society. Supposedly market trade creates transparency and an \"open society\" of free citizens. In reality, Marx and Engels claim, it does not. The \"nakedness\" may not reveal very much other than the requirements of trade; it is just that the cultural patterns of what is hidden and what is revealed differ from feudal and ancient society. According to Marx, the labour market appears as the \"very Eden of the innate rights of man\", insofar workers can choose to sell their labour-power freely, but in reality, workers are \"forced\" to do so, often on terms unfavourable to them, to survive. As soon as they are inside the factory or office, they have to follow orders and submit to the authority of the employer.\n\nEven in \"naked commerce\", the possible methods of \"masking\" what one is, what one represents or what one does, are extremely diverse. Human languages and numerical systems, for example, offer very subtle distinctions of meaning that can \"cover up\" something, or present it as different from what it really is. Anthropologists, sociologists and linguists have sometimes studied \"linguistic masking\".\n\nThe \"masking\" of quantitative relationships takes three main forms:\n\n\nData may be accepted as a valid result, but dismissed as irrelevant or unimportant in a given context, and therefore not worth paying attention to; or conversely, the importance of specific data may be highlighted as being more important than other related facts.\n\nThe theatrical mask, expressing an acting role, was supposedly first invented in the West by the Greek actor Thespis of Attica (6th century BC) and the Greek Aristotelian philosopher Theophrastus (circa 371–287 BC) is credited with being the first in the West to define human character in terms of a typology of personal strengths and weaknesses. Indeed, Marx's idea of character masks appears to have originated in his doctoral studies of Greek philosophy in 1837–39. At that time, the theatre was one of the few places in Germany where opinions about public affairs could be fairly freely aired, if only in fictionalized form.\n\nIndependently from Marx, the romantic novelist Jean Paul also used the concept, in portraying the human problems of individuation. In Jean Paul's aesthetics, the \"Charaktermaske\" is the observable face or appearance-form of a hidden self. It is Jean Paul's definition which is cited in the \"Deutsches Wörterbuch\" compiled by Jacob Grimm and Wilhelm Grimm from 1838 onward.\n\nOther early literary uses of the German term \"charaktermaske\" are found in Joseph von Eichendorff's 1815 novel \"Ahnung und Gegenwart\", a veiled attack against Napoleon, and some years later, in writings by Heinrich Heine. Heine was among the first to use the theatrical term \"Charaktermaske\" to describe a social setting. Perhaps the concept was also inspired by Hegel's discussion of masks in his \"The Phenomenology of Spirit\". In his \"Aesthetics\", Hegel contrasts the fixed, abstract and universal character masks of the \"Commedia dell'arte\" with the romantic depiction of \"character\" as a living, subjective individuality embodied in the whole person.\n\nIn 1841, the German theatre critic Heinrich Theodor Rötscher explicitly defined a \"character mask\" as a \"theatrical role\", acted out in such a way that it expresses all aspects of the assumed personality, his/her social station and background; successfully done, the audience would be able to recognize this personality on first impression.\n\nThe shift in Marx's use of the concept, from dramaturgy and philosophy to political and economic actors, was probably influenced by his well-known appreciation of drama and literature. Certainly, European writers and thinkers in the 17th and 18th centuries (the era of the Enlightenment) were very preoccupied with human character and characterology, many different typologies being proposed; human character was increasingly being defined in a secular way, independent of virtues and vices defined by religion.\n\nThe first known reference by Marx to character masks in a publication appears in an 1846 circular which Marx drafted as an exile in Brussels. It occurs again in his polemic against Karl Heinzen in 1847, called \"Moralizing criticism and critical morality\" and in part 5 of a satirical piece written in 1852 called \"Heroes of the Exile\".\n\nIn chapter 4 of \"The 18th Brumaire of Louis Napoleon\" (1852), a story about the sovereign's dissolution of the French legislative assembly in 1851 in order to reign as imperial dictator, Marx describes how Napoleon abandoned one character mask for another, after dismissing the Barrot-Falloux Ministry in 1849. In this story, character masks figure very prominently. Contrary to Hegel's belief that states, nations, and individuals are all the time the unconscious tools of the world spirit at work within them, Marx insists that:\n\nIn 1861–63, the Austrian writer Alfred Meissner, the \"king of the poets\" criticized by Engels in his 1847 essay \"The True Socialists\", published three volumes of novels under the title \"Charaktermasken\". It is unclear whether Marx was aware of this, but according to Jochen Hörisch it gave the term \"character mask\" a certain popularity among German speakers.\n\nCharacter masks are mentioned five times in \"Capital, Volume I\", and once in \"Capital, Volume II\". Here, the reference is specifically to \"economic\" character masks, not political character masks. However, both the official Moscow translation of \"Capital, Volume I\" into English, as well as the revised 1976 Penguin translation of \"Capital, Volume I\" into English by Ben Fowkes, deleted all reference to character masks, substituting a non-literal translation. English translators of other writings by Marx & Engels, or of classical Marxist texts, quite often deleted \"Charaktermaske\" as well, and often substituted other words such as \"mask\", \"role\", \"appearance\", \"puppet\", \"guise\" and \"persona\".\n\nMarx's concept of character masks has therefore been little known in the English-speaking world, except through the translated writings of the Frankfurt School and other (mainly German or Austrian) Marxists using the term. Tom Bottomore's sociological dictionary of Marxist thought has no entry for the important concept of character masks. The \"Penguin Dictionary of Critical Theory\" likewise does not refer to it. David Harvey, the world-famous New Left popularizer of Marx's writings, does not mention the concept at all in works such as his \"The Limits to Capital\". Likewise Fredric Jameson, the famous commentator on post modernity, offers no analysis of the concept. There is no entry for the concept in James Russell's \"Marx-Engels Dictionary\", in Terrell Carver's \"A Marx Dictionary\" or in the \"Historical Dictionary of Marxism\".\n\nJochen Hörisch claims that \"despite its systematic importance, the concept of character masks was conspicuously taboo in the dogmatic interpretation of Marx\".\n\nHowever, Dieter Claessens mentions the concept in his 1992 \"Lexikon\", there is another mention in \"Lexikon zur Soziologie\" and the more recent German-language \"Historical-Critical Dictionary of Marxism\" has a substantive entry for character masks by Wolfgang Fritz Haug. Haug suggests that the conjunction of \"character\" and \"mask\" is \"specifically German\", since in the French, English, Spanish, and Italian editions of \"Capital, Volume I\", the term \"mask\", \"bearer\" or \"role\" is used, but not \"character mask\". But since \"character mask\" is a technical term in theatre and costume hire – referring both to physical masks expressing specific characters (for example, Halloween masks), and to theatrical roles – it is not \"specifically German\", and most existing translations are simply inaccurate. However, Haug is correct insofar as \"character mask\" as a sociological or psychological term is rarely used by non-German speakers.\n\nMarx's argument about character masks in capitalism can be summarized in six steps.\n\nThe first step in his argument is that when people engage in trade, run a business or work in a job, they adopt and personify (personally represent) a certain function, role or behaviour pattern which is required of them to serve their obligations; their consent to the applicable rules is assumed, as a necessity to succeed in the activities. They have to act this way, because of the co-operative relationships they necessarily have to work with in the division of labour. People have to conform to them, whether they like it or not. If they take on a role, they have to fulfill the packet of tasks which is part of the job.\n\nPeople are initially born into a world in which these social relationships already exist, and \"socialized\" into them in the process of becoming \"well-adjusted adults\" – to the point where they internalize their meaning, and accept them as a natural reality. Consequently, they can learn to act spontaneously and automatically in a way consistent with these social relations, even if that is sometimes a problematic process.\n\nThe second step in his argument is that in acting according to an economic function, employees serve the impersonal (business, legal or political) interests of an abstract authority, which may have little or nothing to do with their own personal interests. They have to keep the two kinds of interests separated, and \"manage them\" appropriately in a \"mature, professional\" way. In this way, they \"personify\" or \"represent\" interests, and who they personally are, may well be completely irrelevant to that – it is relevant only to the extent that their true personality fits with the role.\n\nPeople are slotted into functions insofar as they have characteristics which are at least compatible with the functions. They always have a choice in how they perform their role and how they act it out, but they have no choice about taking it on. If they succeed in their role, they can advance their position or career, but if they fail to live up to it, they are demoted or fired. Human individuality is then conceptualized in terms of the relationship between buyer and seller.\n\nThe third step in his argument is, that the practices just described necessarily lead to the \"masking\" of behaviors and personalities, and to a transformation of personality and consciousness. It is not just that people can rarely be \"all of themselves\" while performing a specialized function in the division of labour, and must also express something new and different. There are also many competing, conflicting and contradictory interests at stake – and these must somehow be dealt with and reconciled by the living person.\n\nDifferent interests have to be constantly mediated and defended in everyday behaviour, with the aid of character masks; these masks exist to mediate conflict. It means that people are obliged or forced to express certain qualities and repress other qualities in themselves. In doing this, however, their own consciousness and personality is altered. To be part of an organization, or \"rise to the top\" of an organization, they have to be able to \"act out\" everything that it requires in a convincing way, and that can only happen if they either have, or acquire, real characteristics which are at least compatible with it. That requires not just an \"acculturation\" process, but also sufficient behavioral flexibility, intelligence, acumen and creativity – so that a person does not inappropriately \"fall out of the role\". Discord between identity and function is tolerated only in contexts where it does not matter.\n\nThe fourth step in his argument concerns an inversion of subject and object. It is not just that the commercial relationships between things being traded begins to dominate and reshape human behaviour, and remake social relations. In addition, human relations become the property of things. Inanimate things, and the relationships between them, are endowed with human characteristics. They become \"actors\" relating in their own right to which people much adjust their behaviour, and they are also theorized in that way. This is a special case of anthropomorphism because it occurs \"within\" human relations, not in relation to an object external to them.\n\nA symbolic language and way of communicating emerges, in which inanimate \"things\" are personified. A market (or a price, or a stock, or a state etc.) gains an independent power to act. Marx calls this commodity fetishism (or more generally, \"fetishism\"), and he regards it as a necessary reification of the symbolizations required to traverse life's situations in bourgeois society, because the relationships between people are constantly being mediated by the relationships between things. It means that people are eventually unable to take their mask off, because the masks are controlled by the business relationships between things being traded, and by broader legal, class, or political interests. If they are actually unable to take the mask off, they have effectively submitted fully to the power of abstract, impersonal market forces and legal rules. As many philosophical texts suggest, by being habituated to a role, the role is internalized by individuals, and becomes part of their personality: they become the thing that they acted out.\n\nThe fifth step in the argument is that on the world's stage, the \"dance of masked people, and of the things they have endowed with an independent power to act and relate\" leads to pervasive human alienation (the estrangement of people from themselves, and from others in contacts which have become impersonal and functional). It durably distorts human consciousness at the very least, and at worst it completely deforms human consciousness. It mystifies the real nature, and the real relationships, among people and things – even to the point where they can hardly be conceived anymore as they really are.\n\nThe masks influence the very way in which realities are categorized. People's theorizing about the world also becomes detached from the relevant contexts, and the interpretation of reality then involves multiple \"layers\" of meanings, in which \"part of the story\" hides the \"whole story\". What the whole story is, may itself become an almost impenetrable mystery, about which it may indeed be argued that it cannot be solved. The real truth about a person may be considered unknowable, but as long as the person can function normally, it may not matter; one is judged simply according to the function performed.\n\nIn what Marx calls \"ideological consciousness\", interests and realities are presented other than they really are, in justifying and defining the meaning of what happens. People may believe they can no longer solve problems, simply because they lack the categories to \"think\" them, and it requires a great deal of critical and self-critical thought, as well as optimism, to get beyond the surface of things to the root of the problems.\n\nThe last step is that effectively capitalist market society develops human beings in an inverted way. The capitalist economy is not primarily organized for the people, but people are organized for the capitalist economy, to serve others who already have plenty of wealth. In an increasingly complex division of labour offering little job security, there is more and more external pressure forcing people to act in all kinds of different roles, masking themselves in the process; by this act, they also acquire more and more behavioural and semiotic flexibility, and develop more and more relational skills and connections. The necessity to work and relate in order to survive thus accomplishes the \"economic formation of society\" at the same time, even if in this society people lack much control over the social relations in which they must participate. It is just that the whole development occurs in an imbalanced, unequal and uncoordinated way, in which the development of some becomes conditional on the lack of development by others.\n\nCommercial interests and political class interests ultimately prevail over the expressed interests of individuals. In the periodic economic crises, masses of people are condemned to the unemployment scrapheap, no matter what skills they may have; they are incompatible with the functioning of the bourgeois system, \"collateral rubbish\" that is swept aside. Even highly developed people can find that society regards them as worthless – which quite often tends to radicalize their opinions (see extremism and radicalization).\n\nA seventh step could in principle be added, namely a big crisis in society which sparks off a revolution and overturns the existing capitalist system. In that case, it could be argued, the false masks are torn off, and people have to stand up for what they really are, and what they really believe in. But that is a possibility which Marx did not comprehensively theorize in \"Das Kapital\".\n\nThe \"mask metaphor\" also appears already in the early writings of Friedrich Engels, and his influence on Marx is often underestimated.\n\nIn 1894, Engels referred to character masks in his \"Preface\" to \"Capital, Volume III\" – when rebutting a criticism of Marx's theory by Achille Loria. Engels's substantive sociological suggestion seems to be that:\n\n\nThe problem with this kind of argument is just that, in defining the meaning of what is happening in society, it is very difficult to provide definite scientific proof that this meaning is the objective truth. It remains an interpretation, which may make sense of things at a certain level, without providing the whole truth. Engels's comment illustrates that the concept of character masks is not infrequently used in a polemical way to describe a false or inauthentic representation.\n\nEngels, like Marx, also used the notion of a \"mask\" in the more general sense of a political \"guise\" or \"disguise\", for example in several of his historical analyses about religious movements.\n\n\nGyörgy Lukács referred to the \"very important category of economic character masks\", but he never provided a substantive analysis of its meaning. He only referred candidly to his own \"Socratic mask\" in a 1909 love letter to a friend. In a 1909 essay, Lukács opined that \"the bourgeois way of life\" is \"only a mask\", which \"like all masks\" \"negates\" something, i.e. the bourgeois mask denies vital parts of human life, in the interests of money-making.\n\nLukács restricted the application of the idea to capitalists only, claiming that Marx had considered capitalists as \"mere character masks\" – meaning that capitalists, as the personifications (\"agents\") of capital, did not do anything \"without making a business out of it\", given that their activity consisted of the correct management and calculation of the objective effects of economic laws. Marx himself never simply equated capitalists with their character masks; they were human beings entangled in a certain life predicament, like anybody else. Capitalists became the \"personification\" of their capital, because they had money which was permanently invested somewhere, and which necessarily had to obtain a certain yield. At most one could say that capitalists had more to hide, and that some had personal qualities enabling them to succeed in their function, while others lacked the personal prerequisites. According to Lukács, the character masks of the bourgeoisie express a \"necessary false consciousness\" about the class consciousness of the proletariat.\n\nIn the post-war tradition of Western Marxism, the concept of character masks was theorized about especially by scholars of the Frankfurt School, and other Marxists influenced by this school. Most of the Frankfurt theorists believed in Freud's basic model of human nature. Erich Fromm expanded it by developing the social-psychological concept of \"social character\".\n\nAdorno argues that Marx explained convincingly \"why\" the appearance-form and the real nature of human relations often does not directly coincide, not on the strength of a metaphysical \"philosophy\" such as transcendental realism, but by inferring the social meaning of human relations from the way they observably appear in practical life – using systematic critical and logical thought as a tool of discovery. Every step in the analysis can be logically and empirically tested. The hermeneutic assumption is that these relations require \"shared meanings\" in order to be able to function and communicate at all. These shared presuppositions have an intrinsic rationality, because human behaviour – ultimately driven by the need to survive – is to a large extent purposive (teleological), and not arbitrary or random (though some of it may be). If the \"essential relationships\" never became visible or manifest in any way, no science would be possible at all, only speculative metaphysics. It is merely that sense data require correct interpretation – they do not have a meaning independently of their socially mediated interpretation. In that sense, the mask presupposes the existence of something which for the time being remains invisible, but which can be revealed when one discovers what is behind the mask. It may be that the essence suddenly reveals itself on the stage of history, or more simply that the understandings which one already has, are altered so that the essence of the thing is finally grasped.\n\nInspired by Marx's concept of character masks, the founder of the Frankfurt School, Max Horkheimer, began to work out a critical, social-psychological understanding of human character in the so-called \"Dämmerung\" period (in 1931/34). Horkheimer stated the Frankfurt School perspective clearly:\n\nThe Frankfurt School, and especially Herbert Marcuse, was also concerned with how people might rebel against or liberate themselves from the character-masks of life in bourgeois society, through asserting themselves authentically as social, political and sexual beings. The Frankfurt School theorists intended to show, that if in bourgeois society things appear other than they really are, this masking is not simply attributable to the disguises of competitive business relationships in the marketplace. It is rooted in the very psychological make-up, formation and behaviour of individual people. In their adaptation to bourgeois society, they argued, people \"internalize\" specific ways of concealing and revealing what they do, repressing some of their impulses and expressing others. If people are dominated, they are not dominated only by forces external to themselves, but by ideas and habits which they have internalized, and accept as being completely \"natural\". Max Horkheimer puts it as follows: \"The principle of domination, based originally on brute force, acquired in the course of time a more spiritual character. The inner voice took the place of the master in issuing commands.\"\n\nThe \"masking\" of an alienated life, and the attempts to counteract it, are thought of in these Marxist theories as co-existing but contradictory processes, involving constant conflicts between what people really are, how they present themselves, and what they should be according to some external requirement imposed on them – a conflict which involves a perpetual struggle from which people can rarely totally withdraw, because they still depend for their existence on others, and have to face them, masked or unmasked.\n\nTo the extent that the commercial and public roles impose heavy personal burdens, and little space exists anymore \"to be oneself\", people can experience personal stress, mental suffering and personal estrangement (alienation), sometimes to the point where they \"lose themselves\", and no longer \"know who they are\" (identity crisis).\n\nUltimately, there exists no individual solution to such identity problems, because to solve them requires the positive recognition, acceptance and affirmation of an identity by others – and this can only happen, if the individual can \"join in\" and receive social acknowledgement of his identity. Marx himself tackled this problem – rather controversially – in his 1843/44 essay \"On the Jewish Question\".\n\nMuch of the scientific controversy about Marx's concept of character masks centres on his unique dialectical approach to analyzing the forms and structure of social relations in the capitalist system: in \"Das Kapital\", he had dealt with persons (or \"economic characters\") only insofar as they personified or symbolized – often in a reified way – economic categories, roles, functions and interests (see above). According to Marx, the capitalist system functioned as a \"system\", precisely because the bourgeois relations of production and trade, including property rights, were imposed on people whether they liked it or not. They had to act and conform in a specific way to survive and prosper. As the mass of capital produced grew larger, and markets expanded, these bourgeois relations spontaneously reproduced themselves on a larger and larger scale, be it with the assistance of state aid, regulation or repression. However, many authors have argued that this approach leaves many facets of capitalist social relations unexplained. In particular, it is not so easy to understand the interactions between individuals and the society of which they are part, in such a way, that each is both self-determining and determined by the other.\n\nMarx's concept of character masks has been interrogated by scholars primarily in the German-language literature. Werner Sombart stated in 1896 (two years after \"Capital, Volume III\" was published) that \"We want a psychological foundation of social events and Marx did not bother about it\".\n\nThe historian Sheila Fitzpatrick has recorded how, in the Soviet Union, \"The theatrical metaphor of masks was ubiquitous in the 1920s and '30s, and the same period saw a flowering of that peculiar form of political theater: the show trial.\" Those who supported the revolution and its communist leadership were politically defined as \"proletarian\" and those who opposed it were defined as \"bourgeois\". The enemies of the revolution had to be hunted down, unmasked, and forced to confess their counter-revolutionary (i.e. subversive) behaviour, whether real or imagined. It led to considerable political paranoia. Abandoning bourgeois and primitive norms, and becoming a cultured, socialist citizen, was \"akin to learning a role\". In the 1920s, the Russian Association of Proletarian Writers (RAPP) adopted the slogan \"tear off each and every mask from reality\". This was based on a quotation from Lenin, who wrote in his 1908 essay on \"Leo Tolstoy as mirror of the Russian revolution\" that the \"realism of Tolstoy was the tearing off of each and every mask\"(\"sryvanie vsekh i vsiacheskikh masok\"). The communist authorities kept detailed files on the class and political credentials of citizens, leading to what historians call \"file-selves\".\n\nMuch later, in 1973 (16 years before Slavoj Žižek entered the intellectual scene) the German New Left critic Michael Schneider claimed that:\n\nAccording to this interpretation, there was a \"blind spot\" in Marx's explanation of bourgeois society, because he had disregarded psychological factors. Moreover, Marxists had interpreted Marx's theory of the \"personification of economic functions\" as an \"alternative\" to psychology as such. Thus, equipped with a simplistic \"reflection theory of consciousness\" and an \"objectivist concept of class consciousness\", the Russian revolutionaries (naively) assumed that once the bourgeois had been liberated from his property, and the institutions of capitalism were destroyed, then there was no more need for masking anything – society would be open, obvious and transparent, and resolving psychological problems would become a purely practical matter (the \"re-engineering of the human soul\"). Very simply put, the idea was that \"the solution of psychological problems is communism\". However, Raymond A. Bauer suggests that the communist suspicion of psychological research had nothing directly to do with the idea of \"character masks\" as such, but more with a general rejection of all approaches which were deemed \"subjectivist\" and \"unscientific\" in a positivist sense (see positivism).\n\nThe USSR became increasingly interested in conceptions of human nature which facilitated social control by the communist party, and from this point of view, too, the concept of the unconscious was problematic and a nuisance: by definition, the unconscious is something which cannot easily be controlled consciously. However, psychoanalysis was considered bourgeois; this situation began to change only gradually Nikita Khrushchev had made his famous secret speech, in which he condemned the \"personality cult\" around Stalin (see \"On the Personality Cult and Its Consequences\"). The obligatory official broadsides against Freud and the neo-Freudians in the Soviet Union ceased only from 1972, after which psychoanalysis was to a large extent rehabilitated.\n\nThe New Left was a radical trend which began in 1956/57, a time when large numbers of intellectuals around the world resigned from the \"Old Left\" Communist parties in protest against the Soviet invasion of Hungary during the Hungarian Revolution of 1956. These New Left intellectuals broke with the official Marxism–Leninism ideology, and they founded new magazines, clubs and groups, which in turn strongly influenced a new generation of students. They began to study Marx afresh, to find out what he had really meant.\n\nIn Germany, the term \"Charaktermaske\" was popularized in the late 1960s and in the 1970s especially by \"red\" Rudi Dutschke, one of the leaders of the student radicals. By \"character masks\", Dutschke meant essentially that the official political personalities and business leaders were merely the interchangeable \"human faces\", the representatives or puppets masking an oppressive system; one could not expect anything else from them, than what the system required them to do. Focusing on individual personalities was a distraction from fighting the system they represented.\n\nAccording to the German educationist Ute Grabowski,\n\nThe positive utopian longing emerging in the 1960s was that of reaching a life situation in which people would be able to meet each other naturally, spontaneously and authentically, freed from any constraints of rank or status, archaic rituals, arbitrary conventions and old traditions. In their social criticism, the youth began to rebel against the roles which were formally assigned to them, and together with that, began to question the social theory of roles, which presented those roles as natural, necessary and inevitable. In particular, the women's liberation movement began to challenge gender roles as sexist and patriarchal. There seemed to be a big gap between the façade of roles, and the true nature of social relationships, getting in the way of personal authenticity (being \"for real\"). Official politics was increasingly regarded as the \"masquerade\" of those in power. To illustrate the spirit of the times, Anne-Marie Rocheblave-Spenlé who had previously authored a classic French text on role theory, in 1974 published a book titled, significantly, \"Le Pouvoir Demasque\" (Power unmasked).\n\nThe concept of \"character masks\" was by no means an unimportant political concept in Germany, since it was being used explicitly by terrorists in their justifications for assassinating people.\n\nQuestions subsequently arose in New Left circles about ten issues:\n\n\nGerman sociologist Uri Rapp theorized that \"Charaktermaske\" was not the same as \"role\"; rather \"Charaktermaske\" was a role forced on people, in a way that they could not really escape from it, i.e. all their vital relationships depended on it. People were compelled by the relations of production. Thus, he said, \"every class membership is a Charaktermaske and even the ideological penetration of masquerades (the 'class consciousness of the proletariat') could not change or cast off character masks, only transcend them in thought.\" In addition, \"Charaktermaske\" was \"present in the issue of the human being alienated from his own personality.\"\n\nJean L. Cohen complained that:\n\nAs the post-war economic boom collapsed in the 1970s, and big changes in social roles occurred, these kinds of controversies stimulated a focus by social theorists on the \"social construction of personal identity\". A very large academic literature was subsequently published on this topic, exploring identity-formation from many different angles. The discourse of identity resonated well with the concerns of adolescents and young adults who are finding their identity, and it has been a popular subject ever since. Another reason for the popularity of the topic, noted by Richard Sennett in his book \"The corrosion of character\", is the sheer number of different jobs people nowadays end up doing during their lifetime. People then experience multiple changes of identity in their lifetime – their identity is no longer fixed once and for all.\n\nMarx's \"big picture\" of capitalism often remained supremely abstract, although he claimed ordinary folks could understand his book. It seemed to many scholars that in Marx's \"Capital\" people become \"passive subjects\" trapped in a system which is beyond their control, and which forces them into functions and roles. Thus, it is argued that Marx's portrayal of the capitalist system in its totality is too \"deterministic\", because it downplays the ability of individuals as \"active human subjects\" to make free choices, and determine their own fate (see also economic determinism). The theoretical point is stated by Peter Sloterdijk as follows:\n\nIn the antihumanist version, the individual is viewed as \"a creation of the system\" or \"a product of society\" who personifies a social function. In this case, a person selected to represent and express a function is no more than a functionary (or a \"tool\"): the person \"himself\" is the character mask adopted by the system or the organization of which he is part. Hidden behind the human face is the (inhuman) system which it operates. In the humanist version, the process is not one of personification, but rather of impersonation, in which case the function is merely a role acted out by the individual. Since the role acted out may in this case not have much to do with the individual's true personality, the mask-bearer and the mask he bears are, in this case, two different things – creating the possibility of a conflict between the bearer and the role he plays. Such a conflict is generally not possible in the antihumanist interpretation (\"if you work for so-and-so, you are one of them\"), since any \"dysfunctional\" character mask would simply be replaced by another.\n\nIn the antihumanist, structural-functionalist philosophy of the French Marxist Louis Althusser, individuals as active subjects who have needs and make their own choices, and as people who \"make their own history\", are completely eradicated in the name of \"science\". In fact, Althusser recommended the psychological theory of Sigmund Freud and Jacques Lacan in the French Communist Party journal \"La Nouvelle Critique\" specifically as a \"science of the (human) \"unconscious\"\". In the glossary of his famous book \"Reading Capital\" (co-written with Étienne Balibar), Althusser announces:\n\nCritics of this idea argue people are not merely the \"bearers\" of social relations, they are also the \"conscious operators\" of social relations – social relations which would not exist at all, unless people consciously interacted and cooperated with each other. The real analytical difficulty in social science is, that people both make their social relations, but also participate in social relations which they did not make or consciously choose themselves. Some roles in society are consciously and voluntarily chosen by individuals, other roles are conferred on people simply by being and participating in society with a given status. Some roles are also a mixture of both: once people have chosen a role, they may have that role, whether they like it or not; or, once habituated to role, people continue to perform the role even although they could in principle choose to abandon it. That is why both the humanist and the antihumanist interpretations of character masks can have some validity in different situations.\n\nAlthusser's \"totalizing perspective\" – which, by destroying the dialectics of experience, cannot reconcile the ways in which people \"make history\" and are \"made by history\", and therefore falls from one contradiction into another – does not just destroy belief in the power of human action (because \"the system\" dominates everything); the super-human approach also invites the objection that it leads to totalitarianism.\n\nSpecifically, in the bid of Marxist ideologists to grab state power, extract a surplus from the workers and manage the introduction of the \"new order\", armed with an ideological tyranny of categories, real human beings become expendable. It is alleged to be a kind of \"upward mobility\" strategy utilizing sympathy for the oppressed and exploited, and social envy. This (fairly cynical) interpretation leads logically to the idea that Marxism or Marxism–Leninism is itself a character mask, by which leftists who are desirous of power and influence which they do not have, disguise their real motives. This is hotly disputed by many Marxists, who claim Marxism is something that grows out of their lives.\n\nC. Wright Mills developed a concept known as the sociological imagination, the idea being that understanding the link between \"private troubles\" and \"public issues\" requires creative insight by the researchers, who are personally involved in what they try to study. The analytical question for social scientists then is, how much the concept of \"character masks\" can really explain, or whether its application is overextended or overworked.\n\nFor example, Jon Elster argued that:\n\nJürgen Ritsert, a Frankfurt sociologist, queried the utility of the concept of character masks:\nFaced with the problem of understanding human character masks – which refers to how human beings have to deal with the relationship between the \"macro-world\" (the big world) and the \"micro-world\" (the small world) – scholarship has often flip-flopped rather uneasily between structuralism and subjectivism, inventing dualisms between structure and agency. The academic popularity of structural-functionalism has declined, \"role definitions\" have become more and more changeable and vague, and the Althusserian argument has been inverted: human behaviour is explained in terms of sociobiology. Here, \"the person\" is identified with \"the physical body\". This is closer to Marx's idea of \"the economic formation of society as a process of natural history\", but often at the cost of \"naturalizing\" (eternalizing) social phenomena which belong to a specific historical time – by replacing their real, man-made \"social\" causes with alleged \"biological\" factors. On this view, humans (except ourselves) are essentially, and mainly, animals. The treatment of humans as if they are animals is itself a strategy of domination.\n\nThe more recent postmodern criticism of Marx's portrayal of character masks concerns mainly the two issues of personal identity and privacy.\n\nIt is argued that modern capitalism has moved far beyond the type of capitalism that Marx knew. Capitalist development has changed the nature of people themselves, and how one's life will go is more and more unpredictable. There is no longer any clear and consensual view of how \"personal identity\" or \"human character\" should be defined anyway (other than by identity cards) and therefore, it is also no longer clear what it means to \"mask\" them, or what interests that can serve. Roles are constantly being redefined to manipulate power relationships, and shunt people up or down the hierarchy.\n\nThe postmodern concept of human identity – however it may be theorized – maximizes the flexibility, variability and plasticity of human behaviour, so that the individual can \"be and do many different things, in many different situations\", without any necessary requirement of continuity between different \"acts\" in space and time. The effect however is a lack of coherence; it becomes much more difficult to know or define what the identity of someone truly is. As soon as the self is viewed as a performance, masking becomes an intrinsic aspect of the self, since there still exists an \"I\" which directs the performance and which therefore simultaneously \"reveals and conceals\" itself. The corollary is, that it becomes much more difficult to \"generalize\" about human beings, since even at the most basic level the categories or units used to make comparisons remain vague. At most, one can objectively measure the incidence and frequency of different types of observable behaviour.\n\nAggregate human behaviour is then often explained either as a biological effect or as a statistical effect, estimated by probability theory. Some Marxists regard this perspective as a form of dehumanization, which signifies a deepening of human alienation, and leads to a return to religion to define humanity. Modern information technology and the sexual revolution, it is nowadays argued, have radically altered the whole idea of what is \"public\" and what is \"private\". Increasingly, information technology becomes a tool for social control. Some Marxists even refer to the spectre of \"totalitarian capitalism\". Human individuals then appear to be caught up in a stressful battle to defend their own definition of themselves against the definitions imposed or attributed by others, in which they can become trapped.\n\nIn their famous 1989 article \"The class struggle fetish\", the German neo-Marxists Robert Kurz and Ernst Lohoff reached the conclusion that the working class is ultimately just \"the character mask of variable capital\", a logical \"real category\" of Capital. The identities of all members of capitalist society, they argued, are ultimately formed as bourgeois character masks of self-valorizing value. In that case, people are valued according to the extent that they can make money for themselves, or for others.\n\nSlavoj Žižek also attempts to create a new theory of masks, by mixing together the philosophies of Hegel, Karl Marx and Jacques Lacan with his understanding of fictional literature and political events. In Žižek's theory, just as an oppressive social reality cannot exist and persist without ideological mystification, \"The mask is not simply hiding the real state of things; the ideological distortion is written into [the] very essence [of the real state of things].\" Thus, the mask is a necessary and integral component of an oppressive reality, and it is not possible simply to tear away the mask to reveal the oppressive reality underneath.\n\nIn \"The sublime object of ideology\", Žižek summarizes Peter Sloterdijk's concept of cynical reason:\n\nOften the pretense is kept up, because of a belief (or anxiety) that the alternative – i.e. dropping the pretense – would have a worse effect, or seriously compromise cherished values or beliefs. To maintain and build a team spirit or morale, a way of working is insisted on which affirms shared beliefs, regardless of whether those beliefs correspond to reality or not, and regardless of whether members of the organization are aware of the discrepancies or not. The result, Žižek claims, is a \"symbolic order\" of \"fetishist disavowal\" in which people act morally \"as if\" they are related in certain ways – to the point where \"the symbolic mask matters more than the direct reality of the individual who wears this mask.\" Using a basically Freudian theory, Žižek then aims to explain the psychological processes by which people are reconciled with the symbolic order, or at least make it \"liveable\" for themselves (see also Freudo-Marxism).\n\nFrank Furedi suggests that the concept of denial, so central to Žižek's understanding of masks, really plays a quite different role in contemporary post-Freudian society: \"In today's therapy culture, people who express views that contradict our own are often told that they are 'in denial'. It has become a way of discrediting their viewpoint, or shutting them up.\" If people disagree, or will not cooperate, they are not taken seriously in a dialogue, but instead accused of having a psychological problem which stands in need of professional treatment. Thus, a dissident is neutralized by being turned into a patient who is \"unhealthy\", and people are managed according to psychotherapeutic concepts designed to invalidate their own meanings. Furedi implies that yesterday's leftist concepts can be recycled as today's tools for psychological manipulation: an idea which originally had a progressive intention can evolve until, in reality, it plays the very opposite role – even although (and precisely because) people continue to sentimentally cherish the old idea. The point is not simply to interpret the processes by which oppressed people are reconciled with, or reproduce their own oppression (Althusser's and Bourdieu's structuralist theory of \"ideological reproduction\"); the challenge is to create new ideas which can free the oppressed out of their oppression. For this purpose, ideas have to be situated according to how they are \"actually being used\" in the real world, and the oppressed have to be regarded as active subjects who can change their own fate (not simply as the \"clients\" of officials, academics and professionals who monitor their behaviour).\n\nPhilip Rieff summarizes the main problem with, as well as the main achievement of psychoanalysis, from the point of view of freeing people from the masks that may oppress them:\n\nIf it is true that \"we do not even know who we are\", then it becomes difficult to understand how people could free themselves from deceptive masks, and change the world for the better, unless they all get a massive dose of psychotherapy to \"find themselves\".\n\nIf one successfully unmasks something, one understands it for what it really is, and can handle it; inversely, if one understands something and can handle it, it is unmasked. Yet, as Marx notes, \"in the analysis of economic forms neither microscopes nor chemical reagents are of assistance. The power of abstraction must replace both.\"\n\nEconomic analysis not only studies the total social effect of human actions, which is usually not directly observable to an individual, other than in the form of statistics or television. The \"economic actors\" are also human beings who \"create\" interactions and relationships which have human meanings. Those meanings cannot be observed directly, they are in people's heads, actively created in their social relationships, and expressed symbolically.\n\nCapitalism unmasks itself in the course of development, when its internal contradictions become so great, that they cause collapse – impelling the revolutionary transformation of capitalism by human action into a new social order, amidst all the political conflicts and class struggles. In trying to get on top of the relations they have created, human beings are themselves transformed. Scientific inquiry, Marx felt, should be an aid in the cause of human progress, to ensure that the new social order emerging will be a real open society. Human progress is achieved, to the degree that people abolish the oppressions of people by other people, and oppressions by the blind forces of nature.\n\n"}
{"id": "6049811", "url": "https://en.wikipedia.org/wiki?curid=6049811", "title": "Cildo Meireles", "text": "Cildo Meireles\n\nCildo Meireles (born 1948) is a Brazilian conceptual artist, installation artist and sculptor. He is noted especially for his installations, many of which express resistance to political oppression in Brazil. These works, often large and dense, encourage a phenomenological experience via the viewer's interaction.\n\nMeireles was born in Rio de Janeiro in 1948. From an early age, Meireles showed a keen interest in drawing and spatial relations. He was especially interested in how this has been explored in animated film. His father, who encouraged Meireles' creativity, worked for the Indian Protection Service and their family traveled extensively within rural Brazil.\n\nIn an interview with Nuria Enguita, Meireles described a time when he was \"seven or eight\" and living in the countryside that had a huge impact on him. He said that he was startled by an impoverished man wandering through the trees. The next day, the young Meireles went to investigate, but the man was gone and only a small but perfect hut the man had apparently made the night before remained. Meireles said that this hut \"was perhaps the most decisive thing for the path [he] followed in life...The possibility one has of making things and leaving them for others.\"\n\nDuring his time in rural Brazil, Meireles learned the beliefs of the Tupi people which he later incorporated into some of his works in order to highlight their marginalization in, or complete disappearance from, Brazilian society and politics. Installations which contain allusions to the Tupi include \"Southern Cross\" (1969–70) and \"Olvido\" (1990). Meireles cites Orson Welles' 1938 radio broadcast \"War of the Worlds\" as one of the greatest works of art of the 20th century because it \"seamlessly dissolved the border between art and life, fiction and reality.\" Recreating this concept of total audience investment was an important artistic goal of Meireles that is seen throughout his body of work.\n\nHe began his study of art in 1963 at the District Federal Cultural Foundation in Brasilia, under the Peruvian painter and ceramist Felix Barrenechea. In the late 1960s, Meireles discovered the work of Hélio Oiticica and Lygia Clark, thereby introducing him to the Brazilian Neo-Concrete movement. These artists, as well as Meireles, were all concerned with blurring the boundary between what is art and what is life, and responding to current political situations within their pieces.\n\nMeireles unintentionally participated in a political demonstration in April 1964, when he was sixteen years old. He has cited this moment has his \"political awakening\" and began to take an interest in student politics. In 1967 he moved to Rio de Janeiro and studied at the Escola Nacional de Belas Artes.\n\nMeireles currently lives and works in Rio de Janeiro.\n\nMeireles has stated that drawing was his main artistic medium until 1968, when he altogether abandoned expressionistic drawing in favor of designing things that he wanted to physically construct. A topic that he especially explored in his art was the concept of the ephemeral and the non-object, art that only exists with interaction, which prompted him to create installation pieces or situational art. This led to his \"Virtual Spaces\" project, which he began in 1968. This project was \"based on Euclidian principles of space\" and sought to show how objects in space can be defined by three different planes. He modeled this concept as a series of environments made to look like corners in rooms.\n\nFollowing the military coup in 1964, Meireles became involved in political art. When Meireles was \"first getting started as an artist,\" governmental censorship of various forms of media, including art, was standard in Brazil. Meireles found ways to create art that was subversive but subtle enough to make public by taking inspiration from Dadaist art, which he notes had the ability to seem \"tame\" and \"ironic.\" In the early 1970s he developed a political art project that aimed to reach a wide audience while avoiding censorship called \"Insertions Into Ideological Circuits\", which was continued until 1976. Many of his installation pieces since this time have taken on political themes, though now his art is \"less overtly political.\"\n\nHe was one of the founders of the Experimental Unit of the Museu de Arte Moderna in Rio de Janeiro in 1969 and in 1975, edited the art magazine \"Malasartes\".\n\nIn 1999, Meireles was honoured with a Prince Claus Award and in 2008 he won the Velazquez Plastic Arts Award, presented by the Ministry of Culture of Spain.\n\nA large-scale, three-room exploration of an entirely red environment. The title of the installation refers both to the scientific concept of chromatic shift (or chromatic aberration) as well as to the idea of a \"shift\" as a displacement or deviation.\n\nThe first room, called \"Impregnation\", is approximately 50 m² and filled with a number of everyday, domestic objects in a variety of different shades of red. The effect is an overwhelming visual saturation of the color. Upon entering the room, the participant experiences an initial shock from the visual inundation of red. Dan Cameron writes that \"one's gaze is literally thwarted in an effort to gain a purchase on the specificity of things.\" Because of its lack of chromatic differentiation, the environment appears to lack depth. Cameron argues that the longer a participant stays in the room the more aware they become of the color's negative, unsettling psychological impact on them.\n\nThe second room is called \"Spill/Environment\" and consists solely of a large pool of red ink spilled from a small bottle on the floor, evoking mental associations with blood. The amount of liquid on the floor in comparison to the amount which the bottle could conceivably hold is disproportionate. The redness on the floor extends throughout the small room to the edge of the darkened third room, an effect which lends itself to feelings of foreboding and uncertainty.\n\nThe third room, \"Shift\", contains a washbasin attached to the wall at a 30° angle illuminated by a direct beam of overhead light. A red stream pours into the washbasin from a tap, also at a 30° angle, allowing the liquid to pool in the sink before draining. The feelings of disturbance experienced by the participant throughout the installation culminate in this final room. Since the room is completely dark, the sole focus is placed on the washbasin. While the connotations of blood which appear throughout the installation are at first rather vague, like in the initial saturation of red in the first room and in the ink spill of the second room, in the third room this association with blood becomes much more explicit, creating a final, visceral reaction to the color within the participant.\n\nArt historian Anne Dezeuze has commented that the \"cinematic\" installation as a whole articulates a certain sense of menace within participants because of the intense repetition of the color red throughout the three rooms. Like most of Meireles' other artworks, \"Red Shift\" takes on political undertones when examined in light of Brazil's military dictatorship which lasted throughout the creation and exhibition of this piece. For instance, the red liquid pouring into the washbasin has been seen by some art historians as a visual representation of the blood of victims murdered by government authorities.\n\nA minimalist sculpture, on a Lilliputian scale: Meireles calls it an example of “humiliminimalism” – a humble brand of minimalism. He wanted it to be even smaller, “but when [he] sanded it down to [his] nails, [he] lost patience and stopped at nine millimeters.\" Unlike most minimalist sculptures it is no mere object, but it is meant to be as richly symbolic, sensuous and potent as an amulet. Each half of the tiny 9mm by 9mm by 9mm cube is made of pine and oak. These two types of wood are considered sacred by the Tupi people of Brazil. The title refers to an unofficial geographical (and metaphysical) region that lies to the west of Tordesillas. According to Meireles in a statement he made about the artwork in 1970, this region is \"the wild side, the jungle in one's head, without the lustre of intelligence or reason...our origins.\" It is a place where \"there are only individual truths.\" In the same statement, he notes that he wants \"Southern Cross\" to be perceived as a physical representation of the memory of the Tupi (\"people whose history is legends and fables\") and a warning to modernity of the growing self-confidence of the primordial which will eventually result in an overtaking of the urban by the natural. Meireles' statement is also political. It is a caution against indifference, especially against indifference towards Brazil's fading indigenous population. The tiny cube is meant to be placed alone in the middle of an empty room in order to emphasize the reality and the power of indigenous belief systems in the context of Eurocentric modernism.\n\nAn art project with political undertones that was designed to reach a mass-audience. This project manifested in multiple ways, two of the most well-known being the \"Coca-Cola\" project, and the \"Banknote\" project. \"Insertions Into Ideological Circuits\" was based upon three principles as defined by Meireles: 1) In society there are certain mechanisms for circulation (circuits); 2) these circuits clearly embody the ideology of the producer, but at the same time they are passive when they receive insertions into the circuit; 3) and this occurs whenever people initiate them. The goal of \"Insertions...\" was to literally insert some kind of counter-information or critical thought into a large system of circulated information. Meireles inserted something that is physically the same, though ideologically different, into a pre-existing system in order to counteract the original circuit without disrupting it. The project was achieved by printing images and messages onto various items that were already widely circulated and which had value discouraging them being destroyed, such as Coca-Cola bottles (which were recycled by way of a deposit scheme) and banknotes. Meireles screen-printed texts onto the Coca-Cola bottles that were supposed to encourage the buyer to become aware of their personal role in a consumerist society. The project simultaneously conveyed anti-imperialist and anti-capitalist messages. Building off of that concept, Meireles also used money as a theme and produced his own replica banknotes and coins (1974–1978) which appeared very similar to genuine Brazilian and US currency but with zero denominations clearly written on them, e.g. \"Zero Dollar\". Mieireles also wrote critiques of the Brazilian government on the banknotes, such as \"Who killed Herzog?\" (in reference to journalist Vladimir Herzog), \"Yankees go home!\" and \"Direct elections.\"\n\nA labyrinthine structure which invites the visitor to walk across eight tons of broken plate glass. The maze is composed of \"velvet museum ropes, street barriers, garden fences, blinds, railings, and aquariums\" and in the center of it is a three-meter ball of cellophane. Meireles notes that an essential part of \"Through\" is the sense of psychological unease that comes from the participant's realization of the different sensory capacities and capabilities between the eyes and the body. For instance, the eyes can see through the glass parts of the work, but the body is physically impeded from passing through parts of the space. Furthermore, the sound of crunching glass underfoot while navigating the maze can be off-putting. He wanted the participant to experience psychological tension between the appreciation of the sonic and the appreciation of the visual. The work, Meireles says, \"is based on the notion of an excess of obstacles and prohibitions.\" Meireles drew some of his inspiration for this installation from writer Jorge Luis Borges, whose subject matter sometimes included the concept of the labyrinth. Meireles also wanted the participant to experience feelings of awareness and attentiveness that come from walking a labyrinth.\n\nA tower of hundreds of radios, each just audible and tuned to stations of different languages to evoke resonances of the Tower of Babel in the Bible. In the story, before the destruction of the Tower of Babel by God, every person on Earth spoke the same language. Meireles' \"Babel\" acknowledges the multiplicity of language that resulted from the Tower's destruction in the story. The artwork contradicts the notion of one universal language, emphasizing that the pursuit of commonality is futile. Paul Herkenhoff points out that \"Babel\" also has autobiographical meaning for Meireles, as radio was a common method of widespread communication in Brazil during the artist's youth. The work also speaks to globalization. Meireles parallels the unity of humanity before the fall of the Tower of Babel with the present-day unity which has resulted from globalization despite numerous language barriers.\n\nMeireles considers his first exhibition to have occurred in 1965, when one of his canvases and two of his drawings were accepted by the Segundo Salão Nacional de Arte Moderna in Brasilia.\n\nA retrospective of his work was presented at the New Museum of Contemporary Art in New York in 1999 and then traveled to the Museu de Arte Moderna in Rio de Janeiro and the São Paulo Museum of Modern Art. In conjunction with the exhibition, a book entitled \"Cildo Meireles\", was published by Phaidon Press (1999).\n\nThe first extensive presentation of the artist’s work in the UK opened at Tate Modern in October 2008. Meireles was the first Brazilian artist to be given a full retrospective by Tate. This exhibition then moved to the Museu d'Art Contemporani in Barcelona, and later to the Museo Universitario Arte Contemporáneo (MUAC) in Mexico City until January 10, 2010.\n\nMeireles' most recent exhibition took place in Milan's HangarBicocca museum from March 27 to July 20, 2014. It featured twelve of his most renowned works.\n\n\n"}
{"id": "61346", "url": "https://en.wikipedia.org/wiki?curid=61346", "title": "Commutative ring", "text": "Commutative ring\n\nIn ring theory, a branch of abstract algebra, a commutative ring is a ring in which the multiplication operation is commutative. The study of commutative rings is called commutative algebra. Complementarily, noncommutative algebra is the study of noncommutative rings where multiplication is not required to be commutative.\n\nA \"ring\" is a set \"R\" equipped with two binary operations, i.e. operations combining any two elements of the ring to a third. They are called \"addition\" and \"multiplication\" and commonly denoted by \"+\" and \"⋅\"; e.g. and . To form a ring these two operations have to satisfy a number of properties: the ring has to be an abelian group under addition as well as a monoid under multiplication, where multiplication distributes over addition; i.e., . The identity elements for addition and multiplication are denoted 0 and 1, respectively.\n\nIf the multiplication is commutative, i.e.\nthen the ring \"R\" is called \"commutative\". In the remainder of this article, all rings will be commutative, unless explicitly stated otherwise.\n\nAn important example, and in some sense crucial, is the ring of integers Z with the two operations of addition and multiplication. As the multiplication of integers is a commutative operation, this is a commutative ring. It is usually denoted Z as an abbreviation of the German word \"Zahlen\" (numbers).\n\nA field is a commutative ring where formula_1 and every non-zero element \"a\" is invertible; i.e., has a multiplicative inverse \"b\" such that \"a\" ⋅ \"b\" = 1. Therefore, by definition, any field is a commutative ring. The rational, real and complex numbers form fields.\n\nIf \"R\" is a given commutative ring, then the set of all polynomials in the variable \"X\" whose coefficients are in \"R\" forms the polynomial ring, denoted \"R\"[\"X\"]. The same holds true for several variables.\n\nIf \"V\" is some topological space, for example a subset of some R, real- or complex-valued continuous functions on \"V\" form a commutative ring. The same is true for differentiable or holomorphic functions, when the two concepts are defined, such as for \"V\" a complex manifold.\n\nIn contrast to fields, where every nonzero element is multiplicatively invertible, the concept of divisibility for rings is richer. An element \"a\" of ring \"R\" is called a unit if it possesses a multiplicative inverse. Another particular type of element is the zero divisors, i.e. a non-zero element \"a\" such that there exists a non-zero element \"b\" of the ring such that \"ab = 0\". If \"R\" possesses no zero divisors, it is called an integral domain (or domain). An element \"a\" satisfying \"a\" = 0 for some positive integer \"n\" is called nilpotent.\n\nThe \"localization\" of a ring is a process in which some elements are rendered invertible, i.e. multiplicative inverses are added to the ring. Concretely, if \"S\" is a multiplicatively closed subset of \"R\" (i.e. whenever \"s\", \"t\" ∈ \"S\" then so is \"st\") then the \"localization\" of \"R\" at \"S\", or \"ring of fractions\" with denominators in \"S\", usually denoted \"S\"\"R\" consists of symbols\nsubject to certain rules that mimic the cancellation familiar from rational numbers. Indeed, in this language Q is the localization of Z at all nonzero integers. This construction works for any integral domain \"R\" instead of Z. The localization (\"R\" \\ {0})\"R\" is a field, called the quotient field of \"R\".\n\nMany of the following notions also exist for not necessarily commutative rings, but the definitions and properties are usually more complicated. For example, all ideals in a commutative ring are automatically two-sided, which simplifies the situation considerably.\n\nFor a ring \"R\", an \"R\"-\"module\" \"M\" is like what a vector space is to a field. That is, elements in a module can be added; they can be multiplied by elements of \"R\" subject to the same axioms as for a vector space. The study of modules is significantly more involved than the one of vector spaces in linear algebra, since several features of vector spaces fail for modules in general: modules need not be free, i.e., of the form\nEven for free modules, the rank of a free module (i.e. the analog of the dimension of vector spaces) may not be well-defined. Finally, submodules of finitely generated modules need not be finitely generated (unless \"R\" is Noetherian, see below).\n\n\"Ideals\" of a ring \"R\" are the submodules of \"R\", i.e., the modules contained in \"R\". In more detail, an ideal \"I\" is a non-empty subset of \"R\" such that for all \"r\" in \"R\", \"i\" and \"j\" in \"I\", both \"ri\" and \"i\" + \"j\" are in \"I\". For various applications, understanding the ideals of a ring is of particular importance, but often one proceeds by studying modules in general.\n\nAny ring has two ideals, namely the zero ideal {0} and \"R\", the whole ring. These two ideals are the only ones precisely if \"R\" is a field. Given any subset \"F\" = {\"f\"} of \"R\" (where \"J\" is some index set), the ideal \"generated by F\" is the smallest ideal that contains \"F\". Equivalently, it is given by finite linear combinations\n\nIf \"F\" consists of a single element \"r\", the ideal generated by \"F\" consists of the multiples of \"r\", i.e., the elements of the form \"rs\" for arbitrary elements \"s\". Such an ideal is called a principal ideal. If every ideal is a principal ideal, \"R\" is called a principal ideal ring; two important cases are Z and \"k\"[\"X\"], the polynomial ring over a field \"k\". These two are in addition domains, so they are called principal ideal domains.\n\nUnlike for general rings, for a principal ideal domain, the properties of individual elements are strongly tied to the properties of the ring as a whole. For example, any principal ideal domain \"R\" is a unique factorization domain (UFD) which means that any element is a product of irreducible elements, in a (up to reordering of factors) unique way. Here, an element \"a\" in a domain is called irreducible if the only way of expressing it as a product\nis by either \"b\" or \"c\" being a unit. An example, important in field theory, are irreducible polynomials, i.e., irreducible elements in \"k\"[\"X\"], for a field \"k\". The fact that Z is a UFD can be stated more elementarily by saying that any natural number can be uniquely decomposed as product of powers of prime numbers. It is also known as the fundamental theorem of arithmetic.\n\nAn element \"a\" is a prime element if whenever \"a\" divides a product \"bc\", \"a\" divides \"b\" or \"c\". In a domain, being prime implies being irreducible. The converse is true in a unique factorization domain, but false in general.\n\nThe definition of ideals is such that \"dividing\" \"I\" \"out\" gives another ring, the \"factor ring\" \"R\" / \"I\": it is the set of cosets of \"I\" together with the operations\nFor example, the ring Z/\"n\"Z (also denoted Z), where \"n\" is an integer, is the ring of integers modulo \"n\". It is the basis of modular arithmetic.\n\nAn ideal is \"proper\" if it is strictly smaller than the whole ring. An ideal that is not strictly contained in any proper ideal is called maximal. An ideal \"m\" is maximal if and only if \"R\" / \"m\" is a field. Except for the zero ring, any ring (with identity) possesses at least one maximal ideal; this follows from Zorn's lemma.\n\nA ring is called \"Noetherian\" (in honor of Emmy Noether, who developed this concept) if every ascending chain of ideals\nbecomes stationary, i.e. becomes constant beyond some index \"n\". Equivalently, any ideal is generated by finitely many elements, or, yet equivalent, submodules of finitely generated modules are finitely generated.\n\nBeing Noetherian is a highly important finiteness condition, and the condition is preserved under many operations that occur frequently in geometry. For example, if \"R\" is Noetherian, then so is the polynomial ring (by Hilbert's basis theorem), any localization \"S\"\"R\", and also any factor ring \"R\" / \"I\".\n\nAny non-noetherian ring \"R\" is the union of its Noetherian subrings. This fact, known as Noetherian approximation, allows the extension of certain theorems to non-Noetherian rings.\n\nA ring is called Artinian (after Emil Artin), if every descending chain of ideals\nbecomes stationary eventually. Despite the two conditions appearing symmetric, Noetherian rings are much more general than Artinian rings. For example, Z is Noetherian, since every ideal can be generated by one element, but is not Artinian, as the chain\nshows. In fact, by the Hopkins–Levitzki theorem, every Artinian ring is Noetherian. More precisely, Artinian rings can be characterized as the Noetherian rings whose Krull dimension is zero.\n\nAs was mentioned above, Z is a unique factorization domain. This is not true for more general rings, as algebraists realized in the 19th century. For example, in\nthere are two genuinely distinct ways of writing 6 as a product:\nPrime ideals, as opposed to prime elements, provide a way to circumvent this problem. A prime ideal is a proper (i.e., strictly contained in \"R\") ideal \"p\" such that, whenever the product \"ab\" of any two ring elements \"a\" and \"b\" is in \"p\", at least one of the two elements is already in \"p\". (The opposite conclusion holds for any ideal, by definition). Thus, if a prime ideal is principal, it is equivalently generated by a prime element. However, in rings such as formula_6, prime ideals need not be principal. This limits the usage of prime elements in ring theory. A cornerstone of algebraic number theory is, however, the fact that in any Dedekind ring (which includes formula_6 and more generally the ring of integers in a number field) any ideal (such as the one generated by 6) decomposes uniquely as a product of prime ideals.\n\nAny maximal ideal is a prime ideal or, more briefly, is prime. Moreover, an ideal \"I\" is prime if and only if the factor ring \"R\" / \"I\" is an integral domain. Proving that an ideal is prime, or equivalently that a ring has no zero-divisors can be very difficult. Yet another way of expressing the same is to say that the complement \"R\" \\ \"p\" is multiplicatively closed. The localisation (\"R\" \\ \"p\")\"R\" is important enough to have its own notation: \"R\". This ring has only one maximal ideal, namely \"pR\". Such rings are called local.\n\nThe \"spectrum of a ring R\", denoted by Spec \"R\", is the set of all prime ideals of \"R\". It is equipped with a topology, the Zariski topology, which reflects the algebraic properties of \"R\": a basis of open subsets is given by\nInterpreting \"f\" as a function that takes the value \"f\" mod \"p\" (i.e., the image of \"f\" in the residue field \"R\"/\"p\"), this subset is the locus where \"f\" is non-zero. The spectrum also makes precise the intuition that localisation and factor rings are complementary: the natural maps \"R\" → \"R\" and \"R\" → \"R\" / \"fR\" correspond, after endowing the spectra of the rings in question with their Zariski topology, to complementary open and closed immersions respectively. Even for basic rings, such as illustrated for \"R\" = Z at the right, the Zariski topology is quite different from the one on the set of real numbers.\n\nThe spectrum contains the set of maximal ideals, which is occasionally denoted mSpec (\"R\"). For an algebraically closed field \"k\", mSpec (k[\"T\", ..., \"T\"] / (\"f\", ..., \"f\")) is in bijection with the set\nThus, maximal ideals reflect the geometric properties of solution sets of polynomials, which is an initial motivation for the study of commutative rings. However, the consideration of non-maximal ideals as part of the geometric properties of a ring is useful for several reasons. For example, the minimal prime ideals (i.e., the ones not strictly containing smaller ones) correspond to the irreducible components of Spec \"R\". For a Noetherian ring \"R\", Spec \"R\" has only finitely many irreducible components. This is a geometric restatement of primary decomposition, according to which any ideal can be decomposed as a product of finitely many primary ideals. This fact is the ultimate generalization of the decomposition into prime ideals in Dedekind rings.\n\nThe notion of a spectrum is the common basis of commutative algebra and algebraic geometry. Algebraic geometry proceeds by endowing Spec \"R\" with a sheaf formula_8 (an entity that collects functions defined locally, i.e. on varying open subsets). The datum of the space and the sheaf is called an affine scheme. Given an affine scheme, the underlying ring \"R\" can be recovered as the global sections of formula_8. Moreover, this one-to-one correspondence between rings and affine schemes is also compatible with ring homomorphisms: any \"f\" : \"R\" → \"S\" gives rise to a continuous map in the opposite direction\nThe resulting equivalence of the two said categories aptly reflects algebraic properties of rings in a geometrical manner.\n\nSimilar to the fact that manifolds are locally given by open subsets of R, affine schemes are local models for schemes, which are the object of study in algebraic geometry. Therefore, several notions concerning commutative rings stem from geometric intuition.\n\nThe \"Krull dimension\" (or dimension) dim \"R\" of a ring \"R\" measures the \"size\" of a ring by, roughly speaking, counting independent elements in \"R\". The dimension of algebras over a field \"k\" can be axiomatized by four properties:\n\nThe dimension is defined, for any ring \"R\", as the supremum of lengths \"n\" of chains of prime ideals\nFor example, a field is zero-dimensional, since the only prime ideal is the zero ideal. The integers are one-dimensional, since chains are of the form (0) ⊊ (\"p\"), where \"p\" is a prime number. For non-Noetherian rings, and also non-local rings, the dimension may be infinite, but Noetherian local rings have finite dimension. Among the four axioms above, the first two are elementary consequences of the definition, whereas the remaining two hinge on important facts in commutative algebra, the going-up theorem and Krull's principal ideal theorem.\n\nA \"ring homomorphism\" or, more colloquially, simply a \"map\", is a map \"f\" : \"R\" → \"S\" such that\nThese conditions ensure \"f\"(0) = 0. Similarly as for other algebraic structures, a ring homomorphism is thus a map that is compatible with the structure of the algebraic objects in question. In such a situation \"S\" is also called an \"R\"-algebra, by understanding that \"s\" in \"S\" may be multiplied by some \"r\" of \"R\", by setting\n\nThe \"kernel\" and \"image\" of \"f\" are defined by ker (\"f\") = {\"r\" ∈ \"R\", \"f\"(\"r\") = 0} and im (\"f\") = \"f\"(\"R\") = {\"f\"(\"r\"), \"r\" ∈ \"R\"}. The kernel is an ideal of \"R\", and the image is a subring of \"S\".\n\nA ring homomorphism is called an isomorphism if it is bijective. An example of a ring isomorphism, known as the Chinese remainder theorem, is\nwhere \"n\" = \"p\"\"p\"...\"p\" is a product of pairwise distinct prime numbers.\n\nCommutative rings, together with ring homomorphisms, form a category. The ring Z is the initial object in this category, which means that for any commutative ring \"R\", there is a unique ring homomorphism Z → \"R\". By means of this map, an integer \"n\" can be regarded as an element of \"R\". For example, the binomial formula\nwhich is valid for any two elements \"a\" and \"b\" in any commutative ring \"R\" is understood in this sense by interpreting the binomial coefficients as elements of \"R\" using this map.\nGiven two \"R\"-algebras \"S\" and \"T\", their tensor product\nis again a commutative \"R\"-algebra. In some cases, the tensor product can serve to find a \"T\"-algebra which relates to \"Z\" as \"S\" relates to \"R\". For example,\n\nAn \"R\"-algebra \"S\" is called finitely generated (as an algebra) if there are finitely many elements \"s\", ..., \"s\" such that any element of \"s\" is expressible as a polynomial in the \"s\". Equivalently, \"S\" is isomorphic to\n\nA much stronger condition is that \"S\" is finitely generated as an \"R\"-module, which means that any \"s\" can be expressed as a \"R\"-linear combination of some finite set \"s\", ..., \"s\".\n\nA ring is called local if it has only a single maximal ideal, denoted by \"m\". For any (not necessarily local) ring \"R\", the localization\nat a prime ideal \"p\" is local. This localization reflects the geometric properties of Spec \"R\" \"around \"p\"\". Several notions and problems in commutative algebra can be reduced to the case when \"R\" is local, making local rings a particularly deeply studied class of rings. The residue field of \"R\" is defined as\nAny \"R\"-module \"M\" yields a \"k\"-vector space given by \"M\" / \"mM\". Nakayama's lemma shows this passage is preserving important information: a finitely generated module \"M\" is zero if and only if \"M\" / \"mM\" is zero.\n\nThe \"k\"-vector space \"m\"/\"m\" is an algebraic incarnation of the cotangent space. Informally, the elements of \"m\" can be thought of as functions which vanish at the point \"p\", whereas \"m\" contains the ones which vanish with order at least 2. For any Noetherian local ring \"R\", the inequality\nholds true, reflecting the idea that the cotangent (or equivalently the tangent) space has at least the dimension of the space Spec \"R\". If equality holds true in this estimate, \"R\" is called a regular local ring. A Noetherian local ring is regular if and only if the ring (which is the ring of functions on the tangent cone)\nis isomorphic to a polynomial ring over \"k\". Broadly speaking, regular local rings are somewhat similar to polynomial rings. Regular local rings are UFD's.\n\nDiscrete valuation rings are equipped with a function which assign an integer to any element \"r\". This number, called the valuation of \"r\" can be informally thought of as a zero or pole order of \"r\". Discrete valuation rings are precisely the one-dimensional regular local rings. For example, the ring of germs of holomorphic functions on a Riemann surface is a discrete valuation ring.\n\nBy Krull's principal ideal theorem, a foundational result in the dimension theory of rings, the dimension of\nis at least \"r\" − \"n\". A ring \"R\" is called a complete intersection ring if it can be presented in a way that attains this minimal bound. This notion is also mostly studied for local rings. Any regular local ring is a complete intersection ring, but not conversely.\n\nA ring \"R\" is a \"set-theoretic\" complete intersection if the reduced ring associated to \"R\", i.e., the one obtained by dividing out all nilpotent elements, is a complete intersection. As of 2017, it is in general unknown, whether curves in three-dimensional space are set-theoretic complete intersections.\n\nThe depth of a local ring \"R\" is the number of elements in some (or, as can be shown, any) maximal regular sequence, i.e., a sequence \"a\", ..., \n\"a\" ∈ \"m\" such that all \"a\" are non-zero divisors in\nFor any local Noetherian ring, the inequality\nholds. A local ring in which equality takes place is called a Cohen–Macaulay ring. Local complete intersection rings, and a fortiori, regular local rings are Cohen–Macaulay, but not conversely. Cohen–Macaulay combine desirable properties of regular rings (such as the property of being universally catenary rings, which means that the (co)dimension of primes is well-behaved), but are also more robust under taking quotients than regular local rings.\n\nThere are several ways to construct new rings out of given ones. The aim of such constructions is often to improve certain properties of the ring so as to make it more readily understandable. For example, an integral domain that is integrally closed in its field of fractions is called normal. This is a desirable property, for example any normal one-dimensional ring is necessarily regular. Rendering a ring normal is known as \"normalization\".\n\nIf \"I\" is an ideal in a commutative ring \"R\", the powers of \"I\" form topological neighborhoods of \"0\" which allow \"R\" to be viewed as a topological ring. This topology is called the \"I\"-adic topology. \"R\" can then be completed with respect to this topology. Formally, the \"I\"-adic completion is the inverse limit of the rings \"R\"/\"I\". For example, if \"k\" is a field, \"k\"<nowiki></nowiki>\"X\"<nowiki></nowiki>, the formal power series ring in one variable over \"k\", is the \"I\"-adic completion of \"k\"[\"X\"] where \"I\" is the principal ideal generated by \"X\". This ring serves as an algebraic analogue of the disk. Analogously, the ring of \"p\"-adic integers is the completion of Z with respect to the principal ideal (\"p\"). Any ring that is isomorphic to its own completion, is called complete.\n\nComplete local rings satisfy Hensel's lemma, which roughly speaking allows extending solutions (of various problems) over the residue field \"k\" to \"R\".\n\nSeveral deeper aspects of commutative rings have been studied using methods from homological algebra. lists some open questions in this area of active research.\n\nProjective modules can be defined to be the direct summands of free modules. If \"R\" is local, any finitely generated projective module is actually free, which gives content to an analogy between projective modules and vector bundles. The Quillen–Suslin theorem asserts that any finitely generated projective module over \"k\"[\"T\", ..., \"T\"] (\"k\" a field) is free, but in general these two concepts differ. \nA local Noetherian ring is regular if and only if its global dimension is finite, say \"n\", which means that any finitely generated \"R\"-module has a resolution by projective modules of length at most \"n\".\n\nThe proof of this and other related statements relies on the usage of homological methods, such as the\nExt functor. This functor is the derived functor of the functor\nThe latter functor is exact if \"M\" is projective, but not otherwise: for a surjective map \"E\" → \"F\" of \"R\"-modules, a map \"M\" → \"F\" need not extend to a map \"M\" → \"E\". The higher Ext functors measure the non-exactness of the Hom-functor. The importance of this standard construction in homological algebra stems can be seen from the fact that a local Noetherian ring \"R\" with residue field \"k\" is regular if and only if\nvanishes for all large enough \"n\". Moreover, the dimensions of these Ext-groups, known as Betti numbers, grow polynomially in \"n\" if and only if \"R\" is a local complete intersection ring. A key argument in such considerations is the Koszul complex, which provides an explicit free resolution of the residue field \"k\" of a local ring \"R\" in terms of a regular sequence.\n\nThe tensor product is another non-exact functor relevant in the context of commutative rings: for a general \"R\"-module \"M\", the functor\nis only right exact. If it is exact, \"M\" is called flat. Despite being defined in terms of homological algebra, flatness has profound geometric implications. For example, if an \"R\"-algebra \"S\" is flat, the dimensions of the fibers\n(for prime ideals \"p\" in \"R\") have the \"expected\" dimension, namely dim \"S\" − dim \"R\" + dim (\"R\" / \"p\").\n\nBy Wedderburn's theorem, every finite division ring is commutative, and therefore a finite field. Another condition ensuring commutativity of a ring, due to Jacobson, is the following: for every element \"r\" of \"R\" there exists an integer such that . If, \"r\" = \"r\" for every \"r\", the ring is called Boolean ring. More general conditions which guarantee commutativity of a ring are also known.\n\nA graded ring \"R\" = ⨁ \"R\" is called graded-commutative if\nIf the \"R\" are connected by differentials ∂ such that an abstract form of the product rule holds, i.e.,\n\"R\" is called a commutative differential graded algebra (cdga). An example is the complex of differential forms on a manifold, with the multiplication given by the exterior product, is a cdga. The cohomology of a cdga is a graded-commutative ring, sometimes referred to as the cohomology ring. A broad range examples of graded rings arises in this way. For example, the Lazard ring is the ring of cobordism classes of complex manifolds.\n\nA graded-commutative ring with respect to a grading by Z/2 (as opposed to Z) is called a superalgebra.\n\nA related notion is an almost commutative ring, which means that \"R\" is filtered in such a way that the associated graded ring\nis commutative. An example is the Weyl algebra and more general rings of differential operators.\n\nA simplicial commutative ring is a simplicial object in the category of commutative rings. They are building blocks for (connective) derived algebraic geometry. A closely related but more general notion is that of E-ring.\n\nThe ring of matrices is \"not\" commutative, since matrix multiplication fails to be commutative.\nHowever, any two matrices \"A\" and \"B\" that do commute can be simultaneously diagonalized, i.e., there is an invertible matrix \"P\" such that both \"PAP\" and \"PBP\" are diagonal matrices. This fact makes representations of commutative Lie groups particularly simpler to understand than in general.\n\nAn example is the set of matrices of divided differences with respect to a fixed set of nodes.\n\n\n"}
{"id": "39616709", "url": "https://en.wikipedia.org/wiki?curid=39616709", "title": "Condensation (psychology)", "text": "Condensation (psychology)\n\nIn Freudian psychology, a condensation () is when a single idea (an image, memory, or thought) or dream object stands for several associations and ideas.\n\nFreud considered that \"dreams are brief, meagre and laconic in comparison with the range and wealth of the dream-thoughts.\" Images and chains of association have their emotional charges displaced from the originating ideas to the receiving one, where they merge and \"condense\" together. Thus for example a dream figure may resemble A, wear B's clothes and act like C, but nevertheless we know somehow that they are 'really' D - rather as with the composite photographs of Francis Galton. While condensation could serve the purposes of the dream censorship by disguising thoughts, Freud considered condensation as primarily the preferred mode of functioning of the unconscious Id.\n\nFreud saw the same mechanism of condensation at work in phantasies and neurotic symptoms, as well as in parapraxis and jokes: he often cited as an instance Heine's quip about the rich man treating him 'famillionairily'.\n\nIn the 1950s the concept was used by linguist Roman Jakobson in his influential article on metaphor and metonymy. Comparing the linguistic evidence to Freud's account of the dream-work, Jakobson saw symbolism as relating to metaphor, condensation and displacement to metonymy. Jakobson's work encouraged Jacques Lacan to say that the unconscious is structured like a language, though he himself linked condensation to metaphor, not metonymy.\n\n\n"}
{"id": "4573760", "url": "https://en.wikipedia.org/wiki?curid=4573760", "title": "Curious Punishments of Bygone Days", "text": "Curious Punishments of Bygone Days\n\nCurious Punishments of Bygone Days is a history book published in 1896. It was written by Alice Morse Earle and printed by Herbert S. Stone & Company. Earle was a historian of Colonial America, and she writes in her introduction:\nIn ransacking old court records, newspapers, diaries and letters for the historic foundation of the books which I have written on colonial history, I have found and noted much of interest that has not been used or referred to in any of those books. An accumulation of notes on old-time laws, punishments and penalties has evoked this volume.\nAs the title suggests, the subject of the chapters is various archaic punishments. Morse seems to make a distinction between stocks for the feet, in the Stocks chapter, and stocks for the head, described in the Pillory article- which itself clashes with the modern day understanding of a pillory as a whipping post.\n\n\n"}
{"id": "2015626", "url": "https://en.wikipedia.org/wiki?curid=2015626", "title": "Dysteleology", "text": "Dysteleology\n\nDysteleology is the philosophical view that existence has no \"telos\" - no final cause from purposeful design. Ernst Haeckel (1834-1919) invented and popularized the word \"dysteleology\"\nDysteleology is an aggressive, yet optimistic, form of science-oriented atheism originally perhaps associated with Haeckel and his followers, but now perhaps more associated with the type of atheism of Richard Dawkins, Sam Harris, or Christopher Hitchens. Transcending traditional philosophical and religious perspectives, such as German idealism (including the philosophies of Hegel and Schelling) and contemporary New Age thinking, modern philosophical naturalism sees existence as having no inherent goal.\n"}
{"id": "26282827", "url": "https://en.wikipedia.org/wiki?curid=26282827", "title": "Economic abuse", "text": "Economic abuse\n\nEconomic abuse is a form of abuse when one intimate partner has control over the other partner's access to economic resources, which diminishes the victim's capacity to support him/herself and forces him/her to depend on the perpetrator financially.\n\nIt is related to, or also known as, financial abuse, which is the illegal or unauthorized use of a person’s property, money, pension book or other valuables (including changing the person's will to name the abuser as heir), often fraudulently obtaining power of attorney, followed by deprivation of money or other property, or by eviction from own home. Financial abuse applies to both elder abuse and domestic violence.\n\nA key distinction between economic abuse and financial abuse is that economic abuse also includes the control of someone's present or future earning potential by preventing them from obtaining a job or education.\n\nEconomic abuse in a domestic situation may involve:\n\nIn its extreme (and usual) form, this involves putting the victim on a strict \"allowance\", withholding money at will and forcing the victim to beg for the money until the abuser gives the victim some money. It is common for the victim to receive less and less money as the abuse continues. This also includes (but is not limited to) preventing the victim from finishing education or obtaining employment, or intentionally squandering or misusing communal resources.\n\nEconomic abuse is often used as a controlling mechanism as part of a larger pattern of domestic abuse, which may include verbal, emotional, physical and sexual abuse. Physical abuse may include threats or attempts to kill the spouse. By restricting the victim's access to economic resources, the offender has limited recourses to exit the abusive or violent relationship.\n\nThe following are ways that abusers may use economic abuse with other forms of domestic violence:\n\nVictimization occurs across all socio-economic levels, and when victims are asked why they stay in abusive relationships, \"lack of income\" is a common response.\n\nThere are several ways that abusers may impact a victim's economic resources. As mentioned earlier, the abuser may prevent the victim from working or make it very difficult to maintain a job. They may likewise impede their ability to obtain an education. Frequent phone calls, surprise visits and other harassing activities interfere with the spouse's work performance. In couples in which the spouse is lesbian, gay, bisexual, transgender, or questioning of their sexuality (LGBTQ), the abuser may threaten to \"out them\" with their employer.\n\nThe National Coalition Against Domestic Violence in the United States reports that:\n\nBy denying the victim access to money, such as forbidding the victim from maintaining a bank account, he or she is totally financially dependent upon the abuser for shelter, food, clothing and other necessities. In some cases the abuser may withhold those necessities, also including medicine and personal hygiene products. They may also greatly limit their ability to leave the abusive situation by refusing to pay court-ordered spousal or child support.\n\nAbusers may also use force their spouses to obtain credit and then through negligent activities ruin their credit rating and ability to get credit.\n\nThere are several ways to manage economic abuse: ensure one has safe access to important personal and financial records, ensure one's research activities are not traceable and, if you believe you are going to leave the relationship, prepare ahead of time.\n\nThe elderly are sometimes victims of financial abuse from people within their family:\n\nFamily members engaged in financial abuse of the elderly may include spouses, children, or grandchildren. They may engage in the activity because they feel justified, for instance, they are taking what they might later inherit or have a sense of \"entitlement\" due to a negative personal relationship with the older person. Or they may take money or property to prevent other family members from getting the money or for fear that their inheritance may be lost due to cost of treating illnesses. Sometimes, family members take money or property from their elders because of gambling or other financial problems or substance abuse.\n\nIt is estimated that there may be 5 million elderly citizens of the United States subject to financial abuse each year.\n\nThe Survivors’ Empowerment and Economic Security Act was introduced by the 110th United States Congress to the Senate (S. 1136) and House of Representatives (H.R. 2395) to allow for greater economic freedom for domestic violence victims by providing short-term emergency benefits where needed, guaranteeing employment leave and unemployment compensation, and prohibit insurance restriction or job discrimination to domestic violence victims.\n\n"}
{"id": "16105186", "url": "https://en.wikipedia.org/wiki?curid=16105186", "title": "Electric car", "text": "Electric car\n\nAn electric car (also battery electric car or all-electric car) is a plug-in electric automobile that is propelled by one or more electric motors, using energy typically stored in rechargeable batteries.\n\nSince 2008, a renaissance in electric vehicle manufacturing occurred due to advances in batteries, concerns about increasing oil prices, and the desire to reduce greenhouse gas emissions. Several national and local governments have established tax credits, subsidies, and other incentives to promote the introduction and adoption in the mass market of new electric vehicles, often depending on battery size, their electric range and purchase price. The current maximum tax credit allowed by the US Government is . Compared with internal combustion engine vehicles, electric cars are quieter and have no tailpipe emissions, and, often lower emissions in general. \nCharging an electric car can be done at a variety of charging stations, these charging stations can be installed in both houses and public areas. The two best selling electric vehicles, the Nissan Leaf and the Tesla Model S, have EPA ranges reaching 151 miles (243 km) and 335 miles (539 km) respectively.\n\n, there are over 4 million all-electric and plug-in hybrid cars in use around the world, of which, 2.6 million were pure electric cars (65%). The Nissan Leaf is the best-selling highway-capable electric car ever, with over 350,000 units sold globally by September 2018. Ranking second is the Tesla Model S with 250,000 units sold worldwide through September 2018.\n\nElectric cars are a variety of electric vehicle (EV). The term \"electric vehicle\" refers to any vehicle that uses electric motors for propulsion, while \"electric car\" generally refers to highway-capable automobiles powered by electricity. Low-speed electric vehicles, classified as neighborhood electric vehicles (NEVs) in the United States, and as electric motorised quadricycles in Europe, are plug-in electric-powered microcars or city cars with limitations in terms of weight, power and maximum speed that are allowed to travel on public roads and city streets up to a certain posted speed limit, which varies by country.\n\nWhile an electric car's power source is not explicitly an on-board battery, electric cars with motors powered by other energy sources are typically referred to by a different name. An electric car carrying solar panels to power it is a solar car, and an electric car powered by a gasoline generator is a form of hybrid car. Thus, an electric car that derives its power from an on-board battery pack is a form of battery electric vehicle (BEV). Most often, the term \"electric car\" is used to refer to battery electric vehicles, but may also refer to plug-in hybrid electric vehicles (PHEV).\n\nIn 1884, over 20 years before the Ford Model T, Thomas Parker built the first practical production electric car in London using his own specially designed high-capacity rechargeable batteries. The \"Flocken Elektrowagen\" of 1888 was designed by German inventor Andreas Flocken. Electric cars were among the preferred methods for automobile propulsion in the late 19th century and early 20th century, providing a level of comfort and ease of operation that could not be achieved by the gasoline cars of the time. The electric vehicle stock peaked at approximately 30,000 vehicles at the turn of the 20th century.\n\nIn 1897, electric cars found their first commercial use in the US. Based on the design of the Electrobat II, a fleet of twelve hansom cabs and one brougham were used in New York City as part of a project funded in part by the Electric Storage Battery Company of Philadelphia. During the 20th century, the main manufacturers of electric vehicles in the US were Anthony Electric, Baker, Columbia, Anderson, Edison, Riker, Milburn, Bailey Electric and others. Unlike gasoline-powered vehicles, the electric ones were less noisy, and did not require gear changes.\n\nAdvances in internal combustion engines (ICE) in the first decade of the 20th century lessened the relative advantages of the electric car. Their much quicker refueling times, and cheaper production costs, made them more popular. However, a decisive moment was the introduction in 1912 of the electric starter motor which replaced other, often laborious, methods of starting the ICE, such as hand-cranking.\n\nSix electric cars held the land speed record. The last of them was the rocket-shaped La Jamais Contente, driven by Camille Jenatzy, which broke the speed barrier by reaching a top speed of on 29 April 1899.\n\nIn the early 1990s, the California Air Resources Board (CARB) began a push for more fuel-efficient, lower-emissions vehicles, with the ultimate goal being a move to zero-emissions vehicles such as electric vehicles. In response, automakers developed electric models, including the Chrysler TEVan, Ford Ranger EV pickup truck, GM EV1, and S10 EV pickup, Honda EV Plus hatchback, Nissan Altra EV miniwagon, and Toyota RAV4 EV. Both US Electricar and Solectria produced 3-phase AC Geo-bodied electric cars with the support of GM, Hughes, and Delco. These early cars were eventually withdrawn from the U.S. market.\n\nCalifornia electric automaker Tesla Motors began development in 2004 on what would become the Tesla Roadster (2008), which was first delivered to customers in 2008. The Roadster was the first highway legal serial production all-electric car to use lithium-ion battery cells, and the first production all-electric car to travel more than per charge.\n\nTesla global sales passed 250,000 units in September 2017. The Renault–Nissan–Mitsubishi Alliance achieved the milestone of 500,000 units electric vehicles sold in October 2017. Tesla sold its 200,000th Model S in the fourth quarter of 2017. Global Leaf sales passed 300,000 units in January 2018, keeping its record as the world's top selling plug-in electric car ever. Tesla delivered its 100,000th Model 3 in October 2018.\n\nMany countries have set goals to ban the sales of gasoline and diesel powered vehicles in the future, notably; Norway by 2025, China by 2030, India by 2030, Germany by 2030, France by 2040, and Britain by 2040 or 2050. Similarly, more cities around the world have begun transitioning public transportation towards electric vehicles, than previously was the case.\n\n, electric cars are less expensive to run than comparable internal combustion engine vehicles due to the lower cost of repairs and energy. However, as of 2018, electric cars on average cost significantly more to initially buy, and depreciate more quickly than conventional cars.\n\nThe Chinese auto manufacturer BYD calculated on its website in 2015 that a BYD e6 taxi over five years would give a saving of about $74,000 over the equivalent petrol consumption.\n\nIn 2018 the Australian Federal Government’s advisory firm on vehicle emissions estimated the TCO for electric cars was 5 to 10 thousand dollars more per year than a roughly equivalent petrol powered car.\n\nSeveral national and local governments have established incentives to reduce the purchasing price of electric cars and other plug-ins.\n\nWhen designing an electric vehicle, manufacturers may find that for low production, converting existing platforms may be cheaper as development cost is lower, however, for higher production, a dedicated platform may be preferred to optimize design, and cost.\n\nAlmost 80% of electric vehicles in the U.S. are leased, while the lease rate for the country's entire fleet is about 30%. In early 2018, electric compact cars of 2014 are worth 23 percent of their original sticker price, as comparable cars with combustion engines worth 41 percent.\n\nBatteries play a significant cost when designing an electric vehicle, for example; Tesla Motors uses batteries that cost around $200 per kilowatt hour.\n\nAccording to a study done in 2018, the average operating cost of an electric vehicle in the United States is $485 per year, as opposed to an Internal combustion engines $1,117 per year.\n\nElectric cars have several benefits over conventional internal combustion engine automobiles, including a significant reduction of local air pollution, as they do not directly emit pollutants such as particulates (soot), volatile organic compounds, hydrocarbons, carbon monoxide, ozone, lead, and various oxides of nitrogen.\n\nDepending on the production process and the source of the electricity to charge the vehicle, emissions may be partly shifted from cities to the material transportation, production plants and generation plants. The amount of carbon dioxide emitted depends on the emissions of the electricity source, and the efficiency of the vehicle. For electricity from the grid, the emissions vary significantly depending on your region, the availability of renewable sources and the efficiency of the fossil fuel-based generation used.\n\nThe same is true of ICE vehicles. The sourcing of fossil fuels (oil well to tank) causes further damage and use of resources during the extraction and refinement processes,including high amounts of electricity. \nIn December 2014, Nissan announced that Leaf owners have accumulated together 1 billion kilometers (620 million miles) driven. This translates into saving 180 million kilograms of emissions by driving an electric car in comparison to travelling with a gasoline-powered car. In December 2016, Nissan reported that Leaf owners worldwide achieved the milestone of 3 billion kilometers (1.9 billion miles) driven collectively through November 2016.\n\nElectric motors can provide high power-to-weight ratios, batteries can be designed to supply the currents needed to support these motors. Electric motors have flat torque curve down to zero speed. For simplicity and reliability, many electric cars use fixed-ratio gearboxes and have no clutch.\n\nMany electric cars have motors that have high acceleration, relative to comparable cars, however, Neighborhood Electric Vehicles may have a low acceleration due to their relatively weak motors. This is largely due to the relatively constant torque of an electric motor, which often increase the acceleration relative to a similar motor power internal combustion engine.\n\nElectric vehicles can also use a direct motor-to-wheel configuration which increases the available power. Having motors connected directly to each wheel allows the wheels to be used both for propulsion and as braking systems, thereby increasing traction. When not fitted with an axle, differential, or transmission, electric vehicles have less drive-train inertia.\n\nFor example, the Venturi Fetish delivers supercar acceleration despite a relatively modest , and top speed of around . Some DC-motor-equipped drag racer EVs have simple two-speed manual transmissions to improve top speed. The Tesla Roadster (2008) 2.5 Sport can accelerate from in 3.7 seconds with a motor rated at . Tesla Model S P100D (Performance / 100kWh / 4-wheel drive) is capable of 2.28 seconds for 0–60 mph at a price of $140,000 . , the P100D is the second fastest production car ever built, taking only 0.08 seconds longer for , compared to a $847,975 Porsche 918 Spyder. The electric supercar Rimac Concept One can go from in 2.5 seconds.\n\nInternal combustion engines have thermodynamic limits on efficiency, expressed as fraction of energy used to propel the vehicle compared to energy produced by burning fuel. Gasoline engines effectively use only 15% of the fuel energy content to move the vehicle or to power accessories, and diesel engines can reach on-board efficiency of 20%, while electric vehicles have on-board efficiency of over 90%, when counted against stored chemical energy, or around 80%, when counted against required energy to recharge.\n\nElectric motors are more efficient than internal combustion engines in converting stored energy into driving a vehicle. Electric cars can not idle. Regenerative braking, which is most common in electric vehicles, can recover as much as one fifth of the energy normally lost during braking.\n\nProduction and conversion electric cars typically use 10 to 23 kW·h/100 km (0.17 to 0.37 kW·h/mi). Approximately 20% of this power consumption is due to inefficiencies in charging the batteries. Tesla Motors indicates that the vehicle efficiency (including charging inefficiencies) of their lithium-ion battery powered vehicle is 12.7 kW·h/100 km (0.21 kW·h/mi) and the well-to-wheels efficiency (if the electricity is generated from natural gas) is 24.4 kW·h/100 km (0.39 kW·h/mi).\n\nWhile heating can be provided with an electric resistance heater, higher efficiency and integral cooling can be obtained with a reversible heat pump. PTC junction cooling is also attractive for its simplicity — this kind of system is used, for example, in the Tesla Roadster (2008).\n\nTo avoid using part of the battery's energy for heating and thus reducing the range, some models allow the cabin to be heated while the car is plugged in. For example, the Nissan Leaf, the Mitsubishi i-MiEV and the Tesla Model S can be pre-heated while the vehicle is plugged in.\n\nSome electric cars, for example the Citroën Berlingo Electrique, use an auxiliary heating system (for example gasoline-fueled units manufactured by Webasto or Eberspächer) but sacrifice \"green\" and \"Zero emissions\" credentials. Cabin cooling can be augmented with solar power, or by automatically allowing outside air to flow through the car when parked. Two models of the 2010 Toyota Prius include this feature as an option.\n\nThe safety issues of BEVs are largely dealt with by the international standard ISO 6469. This document is divided in three parts dealing with specific issues:\n\n\nLike their internal combustion engine counterparts, electric vehicle batteries can catch fire after a crash or mechanical failure. Plug-in electric vehicle fire incidents have occurred, albeit less per mile than I.C.E vehicles. \nThe first modern crash-related fire was reported in China in May 2012, after a high-speed car crashed into a BYD e6 taxi in Shenzhen. The second reported incident occurred in the United States on October 1, 2013, when a Tesla Model S caught fire over ten minutes after the electric car hit metal debris on a highway in Kent, Washington state, and the debris punctured one of 16 modules within the battery pack. A third reported fire occurred on October 18, 2013 in Merida, Mexico. In this case the vehicle was being driven at high speed through a roundabout and crashed through a wall and into a tree. The fire broke out several minutes after the driver exited the vehicle.\n\nIn the United States, General Motors ran in several cities a training program for firefighters and first responders to demonstrate how to safely disable the Chevrolet Volt’s powertrain and its 12 volt electrical system. The Volt's high-voltage system is designed to shut down automatically in the event of an airbag deployment, and to detect a loss of communication from an airbag control module. GM also made available an Emergency Response Guide for the 2011 Volt for use by emergency responders. The guide also describes methods of disabling the high voltage system and identifies cut zone information. Nissan also published a guide for first responders that details procedures for handling a damaged 2011 Leaf at the scene of an accident, including a manual high-voltage system shutdown, rather than the automatic process built-in the car's safety systems.\n\nThe weight of the batteries themselves usually makes an EV heavier than a comparable gasoline vehicle, in a collision, the occupants of a heavy vehicle will on average, suffer fewer and less serious injuries than the occupants of a lighter vehicle; therefore, the additional weight brings safety benefits despite having a negative effect on the car's performance. Depending on where the battery is located, it may lower the center of gravity, increasing driving stability, lowering the risk of an accident through loss of control.\nAn accident in a vehicle will on average cause about 50% more injuries to its occupants than a vehicle.\n\nSome electric cars use low rolling resistance tires, which typically offer less grip than normal tires. The Insurance Institute for Highway Safety in America had condemned the use of low speed vehicles and \"mini trucks,\" referred to as neighborhood electric vehicles (NEVs) when powered by electric motors, on public roads. Mindful of this, several companies (Tesla Motors, BMW, Uniti) have succeeded in keeping the body light, while making it very strong.\n\nAt low speeds, electric cars produced less roadway noise than vehicles propelled by internal combustion engines. Blind or visually impaired people consider the noise of combustion engines a helpful aid while crossing streets, hence electric cars and hybrids could pose an unexpected hazard. Tests have shown that this is a valid concern, as vehicles operating in electric mode can be particularly hard to hear below , which affects all road users, not just the visually impaired. At higher speeds, the sound created by tire friction and the air displaced by the vehicle start to make sufficient audible noise.\n\nThe Government of Japan, the U.S. Congress, and the European Parliament passed legislation to regulate the minimum level of sound for hybrids and plug-in electric vehicles when operating in electric mode, so that blind people and other pedestrians and cyclists can hear them coming and detect from which direction they are approaching. The Nissan Leaf was the first electric car to use Nissan's Vehicle Sound for Pedestrians system, which includes one sound for forward motion and another for reverse. , most of the hybrids and plug-in electric and hybrids available in the United States, Japan and Europe make warning noises using a speaker system. The Tesla Model S is one of the few electric cars without warning sounds; Tesla Motors will wait until regulations are enacted. Volkswagen and BMW also decided to only add artificial sounds to their electric drive cars only when required by regulation.\n\nSeveral anti-noise and electric car advocates have opposed the introduction of artificial sounds as warning for pedestrians, as such an introduction is based on vehicle type and not actual noise level, a concern regarding ICE vehicles which themselves are becoming quieter.\n\n, most Electric cars have similar driving controls to that of a car with a conventional automatic transmission. Even though the motor may be permanently connected to the wheels through a fixed-ratio gear and no parking pawl may be present the modes \"P\" and \"N\" are often still provided on the selector. In this case the motor is disabled in \"N\" and an electrically actuated hand brake provides the \"P\" mode.\n\nIn some cars the motor will spin slowly to provide a small amount of creep in \"D\", similar to a traditional automatic.\n\nWhen the foot is lifted from the accelerator of an ICE, engine braking causes the car to slow. An EV would coast under these conditions, if it wasn't for regenerative braking which instead provides a more familiar response and recharges the battery to an extent. These features also reduce the use of the conventional brakes, significantly reducing wear and tear and maintenance costs as well as improving vehicle range.\n\nLithium-based batteries are often chosen for their high power and energy density, although may wear out over a long period of time. However, there are many emergring technologies trying to combat this issue.\n\nThere are also other battery types, such as Nickel metal hydride (NiMH) batteries which have a poorer power to weight ratio than lithium ion, but are cheaper. Several other battery chemistries are in development such as zinc-air battery which could be much lighter.\n\nThe range of an electric car depends on the number and type of batteries used, and as with all vehicles, the weight and type of vehicle, performance requirements, and the weather.\n\nThe range of production electric vehicles in 2017 ranged from (Renault Twizy) to (Tesla Model S 100D)\n\nThe majority of electric cars are fitted with a display of expected range. This may take into account many factors of how the vehicle is being used, and what the battery is powering. However, since factors can vary over the route, the estimate can vary from the actual achieved range. The display allows the driver to make informed choices about driving speed and whether to stop at a charging point en route. Some roadside assistance organizations offer charge trucks to recharge electric cars in case of emergency.\n\nA study in 2016 stated that 87% of US vehicle-days can be met by current affordable electric cars.\n\nElectric cars are typically charged overnight from a charging station installed in the owner's house, or from faster charging stations found in businesses and public areas.\n\nAn overnight charge of 8 hours will only give about a 40 mile charge with a standard 120 volt outlet whereas a 240 volt outlet would give around 180 miles in the same amount of time. \n\nWithin each major region of the world, electric car charging stations are essentially universal across car and charger brands, and simply plugging in a charger into an electric car will charge the car at the fastest rate that car and charger can support. A notable exception are the Tesla line of cars and charging stations, which use their own proprietary chargers. However, this can be solved by using a converter.\n\nSome companies have been experimenting with battery swapping to eliminate delay while charging.\n\nSome electric vehicles have built in generators, these are considered a type of hybrid vehicle.\n\nAs with all lithium-ion batteries, electric vehicle batteries may degrade over long periods of time, especially if they are frequently overcharged, however, this may take at least several years before being noticeable.\n\nHowever, Nissan stated in 2015 that thus far only 0.01 percent of batteries had to be replaced because of failures or problems, and then only because of externally inflicted damage. The vehicles that had already covered more than , have no problems with the battery.\n\nOver half of the world's cobalt, a key element in lithium-ion batteries, is mined in the Democratic Republic of Congo where the children are forced to mine the cobalt while having little to no protection. \n\n\nVolkswagen, in collaboration with six partners, is developing an EU research project that is focused on automating the parking and charging of electric vehicles. The objective of this project is to develop a smart car system that allows for autonomous driving in designated areas (e.g. valet parking, park and ride) and can offer advanced driver support in urban environments. Tesla has shown interest in making an arm that automatically charges their vehicles.\n\n\nIt is estimated that there are sufficient lithium reserves to power 4 billion electric cars. Most electric cars use a lithium-ion battery and an electric motor which uses rare-earth elements. The demand for lithium, heavy metals, and other elements (such as neodymium, boron and cobalt) required for the batteries and powertrain is expected to grow significantly due to the future sales increase of plug-in electric vehicles in the mid and long term. Some of the largest world reserves of lithium and other rare metals are located in countries with strong resource nationalism, unstable governments or hostility to U.S. interests, raising concerns about the risk of replacing dependence on foreign oil with a new dependence on hostile countries to supply strategic materials.\n\nExperimental supercapacitors and flywheel energy storage devices offer comparable storage capacity, faster charging, and lower volatility. They have the potential to overtake batteries as the preferred rechargeable storage for EVs. The FIA included their use in its sporting regulations of energy systems for Formula One race vehicles in 2007 (for supercapacitors) and 2009 (for flywheel energy storage devices).\n\nSolar cars are electric vehicles powered completely or significantly by direct solar energy, usually, through photovoltaic (PV) cells contained in solar panels that convert the sun's energy directly into electric energy, usually used to charge a battery.\n\nQualcomm, Hyundai, Ford, and Mitsubishi are the top patent holders of the close to 800 electric vehicle charging patents filed between 2014 and 2017. A majority of patents on electric vehicle charging were filed in Japan between 2014 and 2017. It is followed by the US and then by China.\n\nBattery Electric Vehicles are most commonly charged from the power grid overnight at the owner's house, provided they have their own charging station. The electricity on the grid is in turn generated from a variety of sources; such as coal, hydroelectricity, nuclear and others. Power sources such as photovoltaic solar cell panels, micro hydro or wind may also be used and are promoted because of concerns regarding global warming.\n\nCharging stations can have a variety of different speeds of charging, with slower charging being more common for houses, and more powerful charging stations on public roads and areas for trips. The BMW i3 can charge 0–80% of the battery in under 30 minutes in rapid charging mode. The superchargers developed by Tesla Motors provided up to 130 kW of charging, allowing a 300-mile charge in about an hour.\n\nMost electric cars have used conductive coupling to supply electricity for recharging after the California Air Resources Board settled on the SAE J1772-2001 standard as the charging interface for electric vehicles in California in June 2001. In Europe, the ACEA has decided to use the Type 2 connector from the range of IEC_62196 plug types for conductive charging of electric vehicles in the European Union, as the Type 1 connector (SAE J1772-2009) does not provide for three-phase charging.\n\nAnother approach is inductive charging using a non-conducting \"paddle\" inserted into a slot in the car. Delco Electronics developed the Magne Charge inductive charging system around 1998 for the General Motors EV1 which was also used for the Chevrolet S-10 EV and Toyota RAV4 EV vehicles.\n\nDuring peak load periods, when the cost of generation can be very high, electric vehicles could contribute energy to the grid. These vehicles can then be recharged during off-peak hours at cheaper rates while helping to absorb excess night time generation. Here the batteries in the vehicles serve as a distributed storage system to buffer power.\n\nElectric vehicles provide for less dependence on foreign oil, which for the United States and other developed or emerging countries is cause for concern about vulnerability to oil price volatility and supply disruption. Also for many developing countries, and particularly for the poorest in Africa, high oil prices have an adverse impact on their balance of payments, hindering their economic growth. In the United States, presidential candidate Obama proposed in 2008 \"1 million plug-in and electric\" cars by 2015. At the end of 2015 about 550 thousand plugin-in vehicles had been sold in the US.\n\n, there were over 30 models of highway-capable all-electric passenger cars and utility vans available in the market for retail sales. The Renault–Nissan–Mitsubishi Alliance is the world's leading all-electric vehicle manufacturer. By mid-2018, the Alliance's global all-electric vehicle sales totaled about 600,000 units, including those manufactured by Mitsubishi Motors, now part of the Alliance.\n\nTesla is the second best-selling all-electric vehicle manufacturer with almost 500,000 electric cars delivered worldwide by November 2018. Its Model S was the world's best selling plug-in electric car for two years in a row, 2015 and 2016. \nThe world's all-time top selling highway legal electric car is the Nissan Leaf, released in December 2010, with global sales of more than 350,000 units by September 2018. The Tesla Model S ranks second with global sales of 250,000 cars delivered . The Renault Kangoo Z.E. utility van is the leader of the light-duty all-electric segment with global sales of 35,310 units through September 2018.\n\nThe following table lists the all-time best-selling highway-capable all-electric passenger cars with cumulative global sales of around or more than 75,000 units since their inception through September 2018:\n\nGlobal sales of highway legal plug-in electric passenger cars and light utility vehicles achieved the one million milestone in September 2015, almost twice as fast as hybrid electric vehicles (HEV). While it took four years and 10 months for the plug-in segment to reach one-million sales, it took more than around nine years and a few months for HEVs to reach its first million sales. Cumulative global sales of light-duty all-electric vehicles reached one million units in September 2016.\n\nCumulative global sales of plug-in cars passed 2 million in December 2016, the 3 million mark in November 2017, and 4 million in September 2018.\nDespite the rapid growth experienced, the stock of plug-in electric cars represented just about 1 out of every 300 vehicles on the world's roads by September 2018. When global sales are broken down by type of powertrain, all-electric cars have oversold plug-in hybrids, the global ratio between the stock of all-electrics (BEVs) and plug-in hybrids (PHEVs) was 61:39 at the end of 2016. According to Navigant Research, there were about 2.6 million all-electric cars on the world's roads by November 2018.\n\nSeveral countries have established grants and tax credits for the purchase of new electric cars, often depending on battery size. The U.S. offers a federal income tax credit up to , and several states have additional incentives. The UK offers a Plug-in Car Grant up to a maximum of (). The U.S. government also pledged in federal grants for the development of advanced technologies for electric cars and batteries, despite the fact that overall sales aren't increasing at the expected speed.\n\nAs of April 2011, 15 European Union member states provide economic incentives for the purchase of new electrically chargeable vehicles, which consist of tax reductions and exemptions, as well as of bonus payments for buyers of all-electric and plug-in hybrid vehicles, hybrid electric vehicles, and some alternative fuel vehicles.\n\n\n"}
{"id": "31360441", "url": "https://en.wikipedia.org/wiki?curid=31360441", "title": "Forecast by analogy", "text": "Forecast by analogy\n\nForecast by analogy is a forecasting method that assumes that two different kinds of phenomena share the same model of behaviour. For example, one way to predict the sales of a new product is to choose an existing product which \"looks like\" the new product in terms of the expected demand pattern for sales of the product.\n\n\"Used with care, an analogy is a form of scientific model that can be used to analyze and explain the behavior of other phenomena.\"\n\nAccording to some experts, research has shown that the careful application of analogies improves the accuracy of the forecast.\n\n"}
{"id": "11241718", "url": "https://en.wikipedia.org/wiki?curid=11241718", "title": "Full body scanner", "text": "Full body scanner\n\nA full-body scanner is a device that detects objects on a person's body for security screening purposes, without physically removing clothes or making physical contact. Depending on the technology used, the operator may see an alternate-wavelength image of the person's naked body, or merely a cartoon-like representation of the person with an indicator showing where any suspicious items were detected. For privacy and security reasons, the display is generally not visible to other passengers, and in some cases is located in a separate room where the operator cannot see the face of the person being screened. Unlike metal detectors, full-body scanners can detect non-metal objects, which became an increasing concern after various airliner bombing attempts in the 2000s.\n\nStarting in 2007, full-body scanners started supplementing metal detectors at airports and train stations in many countries.\n\nThree distinct technologies have been used, though the use of Backscatter X-ray has now been discontinued in many countries:\nPassengers and advocates have objected to images of their naked bodies being displayed to screening agents or recorded by the government. Critics have called the imaging virtual strip searches without probable cause, and have suggested they are illegal and violate basic human rights. However, current technology is less intrusive and because of privacy issues most people are allowed to refuse this scan and opt for a traditional pat-down.\n\nThe first full body security scanner was developed by Dr. Steven W Smith, who developed the Secure 1000 whole body scanner in 1992. He subsequently sold the device and associated patents to Rapiscan Systems, who now manufacture and distribute the device.\n\nThe first passive, non-radiating full body screening device was developed by Lockheed Martin through a sponsorship by the National Institute of Justice (NIJ)'s Office of Science and Technology and the United States Air Force Research Laboratory. Proof of concept was conducted in 1995 through the Defense Advanced Research Projects Agency (DARPA). Rights to this technology were subsequently acquired by Brijot Imaging Systems, who further matured a commercial-grade product line and now manufacture, market and support the passive millimeter wave devices.\n\nSafety aspects of the Secure 1000 have been investigated in the US by the Food and Drug Administration and National Council on Radiation Protection and Measurements since the early 1990s.\n\nSchiphol in the Netherlands was the first airport in the world to implement this device on a large scale after a test with flight personnel the previous year. On May 15, 2007 two of 17 purchased security scans were installed. \n\nFull-body scanners was installed in at least one Florida courthouse in 2010 and have started to appear in courthouses around the US.\n\nAt least one New Jersey PATH train station used full-body scanners during a two-week trial in 2006..\n\nAs of September 3, 2014, Transportation Security Administration reported that there were almost 740 Millimeter Wave AIT (Advanced Imaging Technology) scanners now in use at 160 United States airports \n\nThe US Government also hinted in 2010 at the possibility of deploying the full body scanners at train stations and subways.\n\nThe Italian government had planned to install full-body scanners at all airport and train stations throughout the country, but announced in September 2010 plans to remove the scanners from airports, calling them \"slow and ineffective.\"\n\nThe Transportation Security Administration (TSA) has stated in 2010 they \"[have] not, will not and the machines cannot store images of passengers at airports\". However the TSA later disclosed, in a response to the house chair on homeland security, that its procurement of airport scanners requires manufacturers to include image storage and transmission features but that these features should be disabled before being placed in an airport. The TSA shows 45 individuals have the ability to turn these machines into 'test mode' which enables recording images, but states that they would never do this on a production system. The US Marshal Service did operate a backscatter machine in a courthouse which records images. However, in a statement they noted that only individuals involved in a test were recorded. A sample of these images was received and disseminated by Gizmodo in 2010, using a Freedom Of Information Request. It is not clear if the US Marshal service has put these new scanning machines, that have recording capabilities, into production.\n\nThe analyst is in a different room and is not supposed to be able to see the person being scanned by the Backscatter X-Ray AIT, but is in contact with other officials who can halt the scanned person if anything suspicious shows up on the scan.\n\nIn the U.S. TSA currently uses Millimeter Wave AIT scanners exclusively, which show no identifying characteristics of the person being scanned. Instead, a generic outline of a person is used. As of December 2015, \"While passengers may generally decline AIT screening in favor of physical screening, TSA may direct mandatory AIT screening for some passengers as warranted by security considerations in order to safeguard transportation security.\"\n\nThe European Union currently allows member states to decide whether to implement full body scanners in their countries:\n\nIn Australia the Government has decided a no opt-out policy will be enforced in relation to screening at airports. Persons with medical or physical conditions that prevent them from undertaking a body scan will be offered alternative screening methods suitable to their circumstances. Infants and young children under 140 cm will not be selected to undergo a body scan. \nBody-scanners are being used at eight of Australia’s international airports – Adelaide, Brisbane, Cairns, Darwin, Gold Coast, Melbourne, Perth and Sydney. So far only passengers exiting via international flights are affected. Passengers who refuse a scan may be banned from flying. The scanners proposed to be used in Australia have shown a high rate of error in testing. Public outrage over the nude images created by the body scanners being collected by policy resulted in a lawsuit in 2010 to stop body scanning.\n\nIn Canada 24 airports currently have these scanners in use. \"Passengers selected for a secondary search can choose between the full body scanner or a physical search.\"\n\nCivil rights groups in Britain in 2010 argued that the body scanning of children contravened the law relating to child pornography.\n\nThe implementation of widespread full-body scanners has raised a public controversy.\n\nSome argue that the use of full-body scanners is equivalent to a strip search, and that if used without probable cause this violate basic human rights.\n\nFull-body scanning technology allows screeners to see the surface of the skin under clothing, prosthetics including breast prostheses and prosthetic testicles, which may require a potentially embarrassing, physical inspection once detected. The scanners can also detect other medical equipment normally hidden, such as colostomy bags and catheters. The transgender community also has privacy concerns that body scanners could potentially lead to their harassment.\n\nIn the UK, in 2010, the Equality and Human Rights Commission argued that full-body scanners were a risk to human rights and might be breaking the law. \n\nA ruling of the European Council in 2013 required that person analyzing the image shall be in a separate location and the image shall not be linked to the screened person.\n\nIn 2010 the National Human Rights Commission of Korea opposed the use of full-body scanners and recommended that they were not deployed at airports.\n\nOpponents in the US argue that full body scanners and the new TSA patdowns are unconstitutional. A comprehensive student note came out in the Fall 2010 issue of the University of Denver Transportation Law Journal that argued that the full-body scanners are unconstitutional in the United States because they are (1) too invasive and (2) not effective enough because the process is too inefficient.\n\nOn July 2, 2010, the Electronic Privacy Information Center (EPIC) filed a lawsuit to suspend the deployment of full-body scanners at airports in the United States:\nEPIC claimed at that time that the full-body scanners violated the Fourth Amendment to the United States Constitution because they subject citizens to virtual strip searches without any evidence of wrongdoing.\n\nThe American Civil Liberties Union, in 2006, called the machines an invasion of privacy: \"This doesn't only concern genitals but body size, body shape and other things like evidence of mastectomies, colostomy appliances or catheter tubes. These are very personal things that people have every right to keep private and personal, aside from the modesty consideration of not wanting to be naked.\"\n\nIn Idaho a bill was introduced in 2011 to prevent the use of full-body scanners as a primary screening method, and to allow people to request alternative screening methods.\n\nTravelers at U.S. airports have complained that when they opted not to be scanned, they were subjected to a new type of invasive pat-down that one traveler in 2010 described as \"probing and pushing ... in my genital area.\" Another traveler in the United States complained in 2010 that the TSA employee \"inserted four fingers of both hands \"inside\" my trousers and ran his fingers all the way around my waist, his fingers extending at least 2–3 inches \"below\" my waistline.\"\n\nAs of December 15, 2015 the TSA published a new policy which required AIT to be \"mandatory\" for \"some\" passengers for \"security reasons\". However, most individuals in the US can still opt out of the scanner and choose a pat-down if they are uncomfortable going through the scanner. Individuals also have the right to be patted down in a private room and have it witnessed by a person of the individual's choice.\n\nIn November 2010, a female traveler who opted out of a full body scan at Fort Lauderdale International Airport claimed that TSA agents handcuffed her to a chair and ripped up her plane ticket when she asked them questions about the new type of invasive pat down she was about to receive. In response, the TSA posted parts of the security camera footage on their blog, though there is no sound in the video and the passenger is not directly in the camera during most of the incident.\n\nIn the United States, in 2010 the TSA required that their full-body scanners \"allow exporting of image data in real time\", and cases of the government's storing of images have been confirmed.\n\nIn August 2010, it was reported that United States Marshals Service saved thousands of images from a millimeter wave scanner. TSA – part of the Department of Homeland Security – reiterated that its own scanners do not save images and that the scanners do not have the capability to save images when they are installed in airports. However, these statements contradict the TSA's own Procurement Specs which specifically require that the machines have the ability to record and transmit images, even if those features might be initially turned off on delivery. Opponents have also expressed skepticism that if there were a successful terror attack that the machines would not have the capability to save images for later inspection to find out what went wrong with the scans. On November 16, 2010, 100 of the stored 35,000 body scan images were leaked online and posted by Gizmodo.\n\nIn February 2012 airport employees in Lagos were allegedly discovered wandering away from a cubicle located in a hidden corner on the right side of the screening area to where the 3D full-body scanner monitors are located.\nAt the Dallas Ft. Worth International Airport, TSA complaints have been reported to disproportionally stem from women who felt that they were singled out for repeated screening for the entertainment of male security officers.\n\nCurrent backscatter and millimeter wave scanners installed by the TSA are unable to screen adequately for security threats inside turbans, hijab, burqas, casts, prosthetics and loose clothing. This technology limitation of current scanners often requires these persons to undergo additional screening by hand or other methods and can cause additional delay or feelings of harassment.\n\nAccording to a manufacturer of the machines, the next generation of backscatter scanners will be able to screen all types of clothing. These improved scanners have been designed to equalize the screening process for religious minorities.\n\nCurrent machines installed by the TSA require agents in the US to designate each passenger as either male or female, after which the software compares the passenger's body against a normative body of that sex. Transgender passengers have reported that full body scanners at several U.S. airports have falsely raised alarms based on their anatomy.\nThe National LGBT Task Force asserts that policies related to these machines \"codify discrimination against transgender people.\"\n\nThere have been health concerns relating to the use of full body scanning technology, especially the use of x-ray scanners.\n\nCurrently adopted millimeter wave scanners operate in the millimeter or sub-terahertz band, using non-ionizing radiation, and have no proven adverse health effects, though no long term studies have been done. Thomas S. Tenforde, president of the National Council on Radiation Protection and Measurements, said in 2010 that millimeter wave scanners are probably within bounds [of standards for safe operation], but there should be an effort to verify that they are safe for frequent use. WHO (World Health Organization) in 2011 categorized RF (radio frequency) radiation as a possible carcinogen, however this has not been verified.\n\nIn the United States, the FAA Modernization and Reform Act of 2012 required that all full-body scanners operated in airports by the Transportation Security Administration use \"Automated Target Recognition\" software, which replaces the picture of a nude body with the cartoon-like representation. As a result of this law, all backscatter X-ray machines formerly in use by the Transportation Security Administration were removed from airports by May 2013, since the agency said the vendor (Rapiscan) did not meet their contractual deadline to implement the software.\n\nIn the European Union, backscatter X-ray screening of airline passengers was banned in 2012 to protect passenger safety, and the deployment at Manchester Airport was removed.\n\nSeveral radiation safety authorities including the National Council on Radiation Protection and Measurements, The Health Physics Society, and the American College of Radiology, have stated that they are \"not aware of any evidence\" that full-body scans are unsafe. However, other radiation authorities, including the International Atomic Energy Agency and Nuclear Energy Agency recommend against using ionizing radiation on certain populations like pregnant women and children, and opponents of the devices say that no long-term studies have been done on the health effects of either backscatter x-ray or millimeter wave scanners: \"I don't think the right questions have been asked. We don't have enough information to make a decision on whether there's going to be a biological effect or not\". (Douglas Boreham, professor in Medical Physics and Applied Radiation Sciences at McMaster University in Hamilton, Ont.)\n\nRichard Morin, a medical physicist at the Mayo Clinic has said that he is not concerned about health effects from backscatter x-ray scanners: \"From a radiation standpoint there has been no evidence that there is really any untoward effect from the use of this device [backscatter scanner], so I would not be concerned about it from a radiation dose standpoint – the issues of personal privacy are a different thing\". The health effects of the more common millimeter wave scanner are largely unknown, and at least one expert believes a safety study is warranted. \"I am very interested in performing a National Council on Radiation Protection and Measurements study on the use of millimeter-wave security screening systems,\" said Thomas S. Tenforde, council president. However, no long-term studies have been done on the health effects of millimeter wave scanners.\n\nPerhaps the most notable and debated professional opinion in regard to the safety of scanners is the so-called \"Holdren Letter\" from a number of world-renowned biochemists and biophysics researchers from the University of California to the Assistant to the US President for Science and Technology, Dr. John P. Holdren (Co-author of EcoSolutions). The opening paragraph of their letter of concern reads: \"We, a number of University of California, San Francisco faculty, are writing—see attached memo—to call your attention to our concerns about the potential serious health risks of the recently adopted whole body back scatter X-ray airport security scanners. This is an urgent situation as these X-ray scanners are rapidly being implemented as a primary screening step for all air travel passengers.\"\n\nCritics of backscatter x-ray scanners, including the head of the Center for Radiological Research at Columbia University, say that the radiation emitted by some full-body scanners is as much as 20 times stronger than officially reported and is not safe to use on large numbers of persons because of an increased risk of cancer to children and at-risk populations.\nResearchers at the University of California, San Francisco, (UCSF) have argued that the amount of radiation is higher than claimed by the TSA and body scanner manufacturers because the doses were calculated as if distributed throughout the whole body, but the radiation from backscatter x-ray scanners is focused on just the skin and surrounding tissues:\n\nThe majority of [the scanners'] energy is delivered to the skin and the underlying tissue. Thus, while the dose would be safe if it were distributed throughout the volume of the entire body, the dose to the skin may be dangerously high.\n\nThe X-ray dose from these devices has often been compared in the media to the cosmic ray exposure inherent to airplane travel or that of a chest X-ray. However, this comparison is very misleading: both the air travel cosmic ray exposure and chest X- rays have much higher X-ray energies and the health consequences are appropriately understood in terms of the whole body volume dose. In contrast, these new airport scanners are largely depositing their energy into the skin and immediately adjacent tissue, and since this is such a small fraction of body weight/vol, possibly by one to two orders of magnitude, the real dose to the skin is now high.\n\nIn addition, it appears that real independent safety data do not exist. A search, ultimately finding top FDA radiation physics staff, suggests that the relevant radiation quantity, the Flux [photons per unit area and time (because this is a scanning device)] has not been characterized. Instead an indirect test (Air Kerma) was made that emphasized the whole body exposure value, and thus it appears that the danger is low when compared to cosmic rays during airplane travel and a chest X-ray dose.\n\nHowever other professors in the UCSF radiology department disagree, saying that the radiation dose is low. \"The conclusions are wrong,\" Ronald Arenson, professor of radiology, tells SF Weekly of his own institution's letter. \"People who are totally unrelated to radiation wrote it. ... It was senior faculty at UCSF. They're smart people and well-intended, but their conclusions, I think, were off-base. They don't understand how radiation translates to an actual dose in the human body\".\n\nDr. Steve Smith, inventor of the body scanner in 1991, and president of Tek84, one of the companies that produces the machines, has stated that the concerns of Dr. Brenner and UCSF Scientists regarding the skin dose of backscatter scanners are incorrect. He states the values used for X-ray penetration were incorrectly based on the description of the imaging depth which describes what the instrument sees and is a few mm into the skin and the dosage depth which is deeper. He describes experimental proof that the X-rays have the same properties as any other X-Rays and the penetration is correct to be averaged over the whole body. Dr. Smith has provided measured data from an operating body scanner to explain his position.\n\nIn October 2010, The TSA responded to the concerns of UCSF researchers via the White House science advisor.\n\nScanners also concentrate the dose in time, because they deliver a high dose-rate at the moment of exposure. High dose-rate exposure has been shown to cause greater damage than the same radiation dose delivered at lower rates. This raises further questions about comparisons to background radiation.\n\nThe FDA report states:\n\nSince general-use x-ray systems emit ionizing radiation, the societal benefit of reliably detecting threats must be sufficient to outweigh the potential radiation risk, if any, to the individual screened. The dose from one screening with a general-use x-ray security screening system is so low that it presents an extremely small risk to any individual. To put the radiation dose received into perspective:\n\nThe U.S. TSA has also made public various independent safety assessments of the Secure 1000 Backscatter X-ray Scanner.\n\nDr. David Brenner, head of Columbia University's center for radiological research, said although the danger posed to the individual passenger is \"very low\", he is urging researchers to carry out more tests on the device to look at the way it affects specific groups who could be more sensitive to radiation. \n\nDr. Andrew J. Einstein, director of cardiac CT research at Columbia University, has made the following statements in support of the safety of body scanners:: \"A passenger would need to be scanned using a backscatter scanner, from both the front and the back, about 200,000 times to receive the amount of radiation equal to one typical CT scan,\" said Dr. Andrew J. Einstein, director of cardiac CT research at Columbia University Medical Center in New York City. \"Another way to look at this is that if you were scanned with a backscatter scanner every day of your life, you would still only receive a tenth of the dose of a typical CT scan,\" he said. \"By comparison, the amount of radiation from a backscatter scanner is equivalent to about 10 minutes of natural background radiation in the United States\", Einstein said. \"I believe that the general public has nothing to worry about in terms of the radiation from airline scanning,\" he added. \"For moms-to-be, no evidence supports an increased risk of miscarriage or fetal abnormalities from these scanners\", Einstein added. \"A pregnant woman will receive much more radiation from cosmic rays she is exposed to while flying than from passing through a scanner in the airport,\" he claimed.\n\nIn May 2010 the National Council on Radiation Protection and Measurements issued a press release in response to the health risk claims from UCSF and Columbia University (claims of excessive skin dose and risks to large populations vs. individuals). The NCRP claims that cancer risks cited by opponents are completely inaccurate, stating that:\n\nand that\n\ngeneral-use systems should adhere to an effective dose of 0.1 microsievert (μSv) (0.01 millirem) or less per scan, and can be used mostly without regard to the number of individuals scanned or the number of scans per individual in a year. An effective dose of 0.1 μSv (0.01 mrem) per scan would allow 2,500 scans of an individual annually [i.e., if each scan required 0.1 μSv (0.01 mrem)] without exceeding the administrative control of 0.25 mSv (25 mrem) to a member of the general public for a single source or set of sources under one control. Assuming 250 workdays per year, this would correspond to an average of 10 scans each day, a frequency that is unlikely to be encountered.\n\nAll the same the Inter-Agency Committee on Radiation Safety which includes the International Atomic Energy Agency, Nuclear Energy Agency and the World Health Organization, reported that, \"Pregnant women and children should not be subject to scanning, even though the radiation dose from body scanners is 'extremely small'\".\n\nIt has also been suggested that defects in the machines, damage from normal wear-and-tear, or software errors could focus an intense dose of radiation on just one spot of the body. The researchers write:\n\nMoreover, there are a number of 'red flags' related to the hardware itself. Because this device can scan a human in a few seconds, the X-ray beam is very intense. Any glitch in power at any point in the hardware (or more importantly in software) that stops the device could cause an intense radiation dose to a single spot on the skin. Who will oversee problems with overall dose after repair or software problems? The TSA is already complaining about resolution limitations; who will keep the manufacturers and/or TSA from just raising the dose, an easy way to improve signal-to-noise and get higher resolution? Lastly, given the recent incident (on December 25th), how do we know whether the manufacturer or TSA, seeking higher resolution, will scan the groin area more slowly leading to a much higher total dose?\n\nProponents of backscatter X-ray scanners argue that the ANSI N43.17 standard addresses safety requirements and engineering design of the systems to prevent the occurrence of accidental high radiation due to defects and errors in hardware and software. Safety requirements include \"fail-safe\" controls, multiple overlapping interlocks and engineering design to ensure that failure of any systems result in safe or non-operation of the system to reduce the chance of accidental exposures. Furthermore, TSA requires that certification to the ANSI N43.17 standard is performed by a third party and not by the manufacturer themselves. But there are cases where types of medical scanning machines, operated by trained medical personnel, have malfunctioned, causing serious injury to patients that were scanned. Critics of full-body scanners cite these incidents as examples of how radiation-based scanning machines can overdose people with radiation despite all safety precautions. In March 2011, it was found that some of the full body scanners in the US were emitting 10 times the normal level of radiation: Contractors charged with routinely examining the scanners submitted reports containing discrepancies, including mathematical miscalculations showing that some of the devices emitted radiation levels 10 times higher than normal: \"In our review of the surveys we found instances where a technician incorrectly did his math and came up with results that showed the radiation readings were off by a factor of 10,\" said Peter Kant, executive vice president of Rapiscan Systems.\n\nThe x-rays from backscatter scanners \"are a form of ionizing radiation, that is, radiation powerful enough to strip molecules in the body of their electrons, creating charged particles that cause cell damage and are thought to be the mechanism through which radiation causes cancer.\" Humans are exposed to background radiation every day, anywhere on earth, and proponents of backscatter X-ray scanners say that the devices expose subjects to levels of radiation equivalent to background radiation. Furthermore, when traveling on an airplane, passengers are exposed to much higher levels of radiation than on earth due to altitude. Proponents say that a backscatter X-ray scan is equivalent to the radiation received during two minutes of flying.\n\nThe UK Health Protection Agency has also issued a statement that the radiation dose from backscatter scanners is very low and \"about the same as one hour of background radiation\".\n\nThe European Commission issued a report stating that backscatter x-ray scanners pose no known health risk, but suggested that backscatter x-ray scanners, which expose people to ionizing radiation, should not be used when millimeter-wave scanners that \"have less effects on the human body\" are available:\n\nAssuming all other conditions equal, there is no reason to adopt X‐ray backscatters, which expose the subject to an additional – although negligible – source of ionizing radiations. Other WBI <nowiki>[Whole Body Imaging]</nowiki> technologies should be preferred for standard use. However, the European Commission's report provides no data substantiating the claim that \"all other conditions are equal\". One area where backscatter X-ray scanners can provide better performance than millimeter wave scanners, for example, is in the inspection of the shoes, groin and armpit regions of the body. The European Commission also recommended that alternate screening methods should be \"used on pregnant women, babies, children and people with disabilities\".\n\nIn the United States, Senator Susan Collins, Ranking Member of the Senate Homeland Security Committee sent a letter on August 6, 2010 to the Secretary of Homeland Security and Administrator of the TSA, requesting that the TSA \"have the Department’s Chief Medical Officer, working with independent experts, conduct a review of the health effects of their use for travelers, TSA employees, and airport and airline personnel.\" The TSA has completed this review.\n\nThe U.S. Government is also supplying higher-radiation through-body X-Ray machines to at least two African countries \"for the purposes of airport security — the kind that can see through flesh, and which deliver real doses of radiation. The U.S.-supplied scanners have apparently been deployed at one airport in Ghana and four in Nigeria\". which has caused some to question how far the U.S. Government intends to go with the technology.\n\nUnions for airline pilots working for American Airlines and US Airways have urged pilots to avoid the full body scanners.\n\nThere is controversy over full-body scanners in some countries because the machines create images of virtual strip searches on persons under the age of 18 which may violate child pornography laws. In the UK, the scanners may be breaking the Protection of Children Act of 1978 by creating images or pseudo-images of nude children.\n\nParents have complained that their young children are being virtually strip searched, sometimes without their parents present.\n\nSome critics suggest that full-body scanner technology is ineffective for multiple reasons, including that they can easily be bypassed and a study published in the November 2010 edition of the \"Journal of Transportation Security\" suggested terrorists might fool the Rapiscan machines and others like it employing the X-ray \"backscatter\" technique. A terrorist, the report found, could tape a thin film of explosives of about 15–20 centimeters in diameter to the stomach and walk through the machine undetected.\n\nTerrorists have already evolved their tactics with the use of surgically implanted bombs or bombs hidden in body cavities.\n\nIn March 2012, scientist and blogger Jonathan Corbett demonstrated the ineffectiveness of the machines by publishing a viral video showing how he was able to get a metal box through backscatter x-ray and millimeter wave scanners in two US airports. In April 2012, Corbett released a second video interviewing a TSA screener, who described firearms and simulated explosives passing through the scanners during internal testing and training. In another test of the full-body scanners, the machines failed to detect bomb parts hidden around a person's body. And in a different test in 2011, an undercover TSA agent was able to carry a handgun through full body scanners multiple times without the weapon being detected. However, fault was not that of the machine, but the TSA Agent who was in charge of viewing the scanned images was simply not paying attention.\n\nFurthermore an Israeli airport security expert Rafi Sela, who helped design security at Ben Gurion International Airport, has said: \"I don't know why everybody is running to buy these expensive and useless machines. I can overcome the body scanners with enough explosives to bring down a Boeing 747... That's why we haven't put them in our airport.\"\n\nAgain, despite the scanners, the TSA has been unable to stop weapons like box cutters and pistols from being carried onto airplanes.\n\nThe Australia government has been challenged over the effectiveness and cost of full body scanners by public media to which Australian Transport Minister Anthony Albanese has said he \"makes no apologies\" for mandating the installation of full body scanners at Australian airports.\n\nTwo alternatives that have been argued for by experts, such as Prof Chris Mayhew from Birmingham University, are chemical-based scanners and bomb-sniffing dogs. Others have argued that passenger profiling, as done by Israeli airport security, should replace full body scanners and patdowns.\n\nA Gallup poll given just after the 2009 Christmas Day bombing attempt suggested that 78% of American airline travelers approved of body scanners while 20% disapproved. 51% indicated that they would have some level of discomfort with full-body scans, while 48% said they would not be uncomfortable with the idea. The poll was given in the context of the 2009 Christmas Day bombing attempt, and some opponents of full body scanners say that the explosives used in that bombing attempt would not have been detected by full-body scanners.\n\nAn ABC/Washington Post poll conducted by Langer Associates and released November 22, 2010 found that 64 percent of Americans favored the full-body X-ray scanners, but that 50 percent think the \"enhanced\" pat-downs go too far; 37 percent felt so strongly. In addition the poll states opposition is lowest amongst those who fly less than once a year.\n\nAs of November 23, 2010 an online poll of 11,817 people on The Consumerist website, 59.41% said they would not fly as a result of the new scans. Additionally, as of November 23, 2010 a poll of MSNBC 8,500 online readers indicated 84.1% believe the new procedures would not increase travel safety. According to a CBS telephone poll of 1,137 people published in November 2010, 81% (+/- 5%) percent of those polled approved TSA's use of full-body scans.\n\nThere has been some debate about the safety of the scanners, however, the TSA argue that mmw scanners used emit no ionizing radiation.\n\nFormer Homeland Security secretary Michael Chertoff has been criticized for heavily promoting full-body scanners while not always fully disclosing that he is a lobbyist for one of the companies that makes the machines. Other full-body scanner lobbyists with Government connections include:\n\n\nForbes magazine reported, in March 2011, that:\n\nNewly uncovered documents show that as early as 2006, the Department of Homeland Security has been planning pilot programs to deploy mobile scanning units that can be set up at public events and in train stations, along with mobile x-ray vans capable of scanning pedestrians on city streets.\n\nand that the TSA had research proposals to:\n\nbring full-body scanners to train stations, mass transit, and public events. Contracts included in the EPIC release showed plans to develop long-range scans that could assess what a subject carried from 30 feet away, along with studies that involved systems for x-ray scanners mounted in vans and \"covert\" scans of pedestrians.\n\nThe new software for scanners has been applied by US Aviation Security, so the new full-body scanner will not give image of nudity of the person who is scanned to the operators of the scanner, but only give the image as a generic male or female figure with no features. Opponents of full body scanners still consider this to be an unconstitutional strip search, because even though the operators see an edited version of the image, a naked image is still captured by the machine, and there is no guarantee that the Government or private companies won't store the images in the case that a terrorist attack were successful.\n\nThis type of software has been applied at Washington, Atlanta and Las Vegas airports.\n\n\"Our top priority is the safety of the traveling public, and TSA constantly strives to explore and implement new technologies that enhance security and strengthen privacy protections for the traveling public,\" TSA Administrator John Pistole stated. \"This software upgrade enables us to continue providing a high level of security through advanced imaging technology screening, while improving the passenger experience at checkpoints.\" \n\nSome people wish to prevent either the loss of privacy or the possibility of health problems or genetic damage that might be associated with being subjected to a backscatter X-ray scan. One company sells X-ray absorbing underwear which is said to have X-ray absorption equivalent to 0.5 mm of lead. Another product, Flying Pasties, is \"... designed to obscure the most private parts of the human body when entering full body airport scanners\", but its description does not seem to claim any protection from the X-ray beam penetrating the body of the person being scanned.\n\n\n"}
{"id": "12239", "url": "https://en.wikipedia.org/wiki?curid=12239", "title": "Guilt (emotion)", "text": "Guilt (emotion)\n\nGuilt is a cognitive or an emotional experience that occurs when a person believes or realizes—accurately or not—that they have compromised their own standards of conduct or have violated a universal moral standard and bear significant responsibility for that violation.\nGuilt is closely related to the concept of remorse.\n\nGuilt is an important factor in perpetuating obsessive–compulsive disorder symptoms. Guilt and its associated causes, merits, and demerits are common themes in psychology and psychiatry. Both in specialized and in ordinary language, guilt is an affective state in which one experiences conflict at having done something that one believes one should not have done (or conversely, having not done something one believes one should have done). It gives rise to a feeling which does not go away easily, driven by 'conscience'. Sigmund Freud described this as the result of a struggle between the ego and the superego – parental imprinting. Freud rejected the role of God as punisher in times of illness or rewarder in time of wellness. While removing one source of guilt from patients, he described another. This was the unconscious force within the individual that contributed to illness, Freud in fact coming to consider \"the obstacle of an unconscious sense of guilt...as the most powerful of all obstacles to recovery.\" For his later explicator, Lacan, guilt was the inevitable companion of the signifying subject who acknowledged normality in the form of the Symbolic order.\n\nAlice Miller claims that \"many people suffer all their lives from this oppressive feeling of guilt, the sense of not having lived up to their parents' expectations...no argument can overcome these guilt feelings, for they have their beginnings in life's earliest period, and from that they derive their intensity.\" This may be linked to what Les Parrott has called \"the disease of false guilt...At the root of false guilt is the idea that what you \"feel\" must be true.\" If you \"feel\" guilty, you must \"be\" guilty!\n\nThe philosopher Martin Buber underlined the difference between the Freudian notion of guilt, based on internal conflicts, and \"existential guilt\", based on actual harm done to others.\n\nGuilt is often associated with anxiety. In mania, according to Otto Fenichel, the patient succeeds in applying to guilt \"the defense mechanism of denial by overcompensation...re-enacts being a person without guilt feelings.\"\n\nIn psychological research, guilt can be measured by using questionnaires, such as the Differential Emotions Scale (Izard's DES), or the Dutch Guilt Measurement Instrument.\n\nDefenses against feeling guilt can become an overriding aspect of one's personality. The methods that can be used to avoid guilt are multiple. They include:\n\nFeelings of guilt can prompt subsequent virtuous behavior. People who feel guilty may be more likely to exercise restraint, avoid self-indulgence, and exhibit less prejudice. Guilt appears to prompt reparatory behaviors to alleviate the negative emotions that it engenders. People appear to engage in targeted and specific reparatory behaviors toward the persons they wronged or offended.\n\nIndividuals high in psychopathy lack any true sense of guilt or remorse for harm they may have caused others. Instead, they rationalize their behavior, blame someone else, or deny it outright. A person with psychopathy has a tendency to be harmful to his or herself and to others. They have little ability to plan ahead for the future. An individual with psychopathy will never find themselves at fault because they will do whatever it takes to benefit themselves without reservation. A person that does not feel guilt or remorse would have no reason to find themselves at fault for something that they did with the intention of hurting another person. To a person high in psychopathy, their actions can always be rationalized to be the fault of another person. This is seen by psychologists as part of a lack of moral reasoning (in comparison with the majority of humans), an inability to evaluate situations in a moral framework, and an inability to develop emotional bonds with other people due to a lack of empathy.\n\nSome evolutionary psychologists theorize that guilt and shame helped maintain beneficial relationships, such as reciprocal altruism. If a person feels guilty when he harms another, or even fails to reciprocate kindness, he is more likely not to harm others or become too selfish. In this way, he reduces the chances of retaliation by members of his tribe, and thereby increases his survival prospects, and those of the tribe or group. As with any other emotion, guilt can be manipulated to control or influence others. As highly social animals living in large, relatively stable groups, humans need ways to deal with conflicts and events in which they inadvertently or purposefully harm others. If someone causes harm to another, and then feels guilt and demonstrates regret and sorrow, the person harmed is likely to forgive. Thus, guilt makes it possible to forgive, and helps hold the social group together.\n\nWhen we see another person suffering, it can also cause us pain. This constitutes our powerful system of empathy, which leads to our thinking that we should do something to relieve the suffering of others. If we cannot help another, or fail in our efforts, we experience feelings of guilt. From the perspective of group selection, groups that are made up of a high percentage of co-operators outdo groups with a low percentage of co-operators in between-group competition. People who are more prone to high levels of empathy-based guilt may be likely to suffer from anxiety and depression; however, they are also more likely to cooperate and behave altruistically. This suggests that guilt-proneness may not always be beneficial at the level of the individual, or within-group competition, but highly beneficial in between-group competition.\n\nAnother common notion is that guilt is assigned by social processes, such as a jury trial (i. e., that it is a strictly legal concept). Thus, the ruling of a jury that O. J. Simpson or Julius Rosenberg was \"guilty\" or \"not innocent\" is taken as an actual judgment by the whole society that they must act as if they were so. By corollary, the ruling that such a person is \"not guilty\" may not be so taken, due to the asymmetry in the assumption that one is assumed innocent until proven guilty, and prefers to take the risk of freeing a guilty party over convicting innocents. Still others—often, but not always, theists of one type or another—believe that the origin of guilt comes from violating universal principles of right and wrong. In most instances, people who believe this also acknowledge that even though there is proper guilt from doing 'wrong' instead of doing 'right', people endure all sorts of guilty feelings which do not stem from violating universal moral principles.\n\nCollective guilt (or group guilt) is the unpleasant and often emotional reaction that results among a group of individuals when it is perceived that the group illegitimately harmed members of another group. It is often the result of “sharing a social identity with others whose actions represent a threat to the positivity of that identity.” For an individual to experience collective guilt, he must identify himself as a part of the in-group. “This produces a perceptual shift from thinking of oneself in terms of ‘I’ and ‘me’ to ‘us’ or ‘we’.”\n\nFeeling guilt for one's own actions. This doesn't mean that you feel guilty for your own actions all the time, you can feel self-guilt even if someone else did something. Self-guilt can often lead to depression (mood) and worst-case scenarios.\n\nWhile dealing with self-guilt, there's even more stuff you need to deal with. Self-guilt can mentally eat up a person while they're in a relationship, making them feel guilt on one's ownself. This can occur due to many things, one of them in insecurties. Feeling insecure can lead to self-guilt, feeling like it's one's own fault for feeling that way. This can often feel stressful, causing mental-break downs, problems in the relationship, and depression..\n\nSelf-guilt is a feeling many people who self-harm get. Self-guilt is almost like mentally self-harming to one's ownself, except it's in the mind. People who self-harm often feel it's their fault for doing it, which makes them feel ashamed. Harming yourself is often felt like it's your own fault, giving the feeling of \"self-guilt\". .\n\nDepression is often related to self-guilt. Constant feeling of self-guilt can lead to depression, since an individual is constantly putting themselves down. Self-guilt sparks insecurity, indecision, and poor decisions. \n\nTraditional Japanese society, Korean society and Chinese culture are sometimes said to be \"shame-based\" rather than \"guilt-based\", in that the social consequences of \"getting caught\" are seen as more important than the individual feelings or experiences of the agent (see the work of Ruth Benedict). The same has been said of Ancient Greek society, a culture where, in Bruno Snell's words, if \"honour is destroyed the moral existence of the loser collapses.\"\n\nThis may lead to more of a focus on etiquette than on ethics as understood in Western civilization, leading some in Western civilizations to question why the word \"ethos\" was adapted from Ancient Greek with such vast differences in cultural norms. Christianity and Islam inherit most notions of guilt from Judaism, Persian, and Roman ideas, mostly as interpreted through Augustine, who adapted Plato's ideas to Christianity. The Latin word for guilt is \"culpa\", a word sometimes seen in law literature, for instance in \"mea culpa\" meaning \"my fault (guilt)\".\n\nGuilt, from O.E. \"gylt\" \"crime, sin, fault, fine, debt\", derived from O.E. \"gieldan\" \"to pay for, debt\". The mistaken use for \"sense of guilt\" is first recorded in 1690. \"Guilt by association\" is first recorded in 1941. \"Guilty\" is from O.E. \"gyltig\", from \"gylt\".\n\nGuilt is a main theme in John Steinbeck's \"East of Eden\", Fyodor Dostoyevsky's \"Crime and Punishment\", Tennessee Williams' \"A Streetcar Named Desire\", William Shakespeare's play \"Macbeth\", Edgar Allan Poe's \"The Tell-Tale Heart\" and \"The Black Cat\", and many other works of literature. In Sartre's \"The Flies\", the Furies (in the form of flies) represent the morbid, strangling forces of neurotic guilt which bind us to authoritarian and totalitarian power.\n\nGuilt is a major theme in many works by Nathaniel Hawthorne, and is an almost universal concern of novelists who explore inner life and secrets.\n\nGuilt in the Christian Bible is not merely an emotional state but is a legal state of deserving punishment. The Hebrew Bible does not have a unique word for guilt, but uses a single word to signify: \"sin, the guilt of it, the punishment due unto it, and a sacrifice for it.\" The Greek New Testament uses a word for guilt that means \"standing exposed to judgment for sin\" (e. g., Romans 3:19). In what Christians call the \"Old Testament\", Christians believe the Bible teaches that, through sacrifice, one's sins can be forgiven (Judaism categorically rejects this idea, holding that forgiveness of sin is exclusively through repentance, and the role of sacrifices was for atonement of sins committed by accident or ignorance ). The New Testament says that this forgiveness is given as written in 1 Corinthians 15:3–4: \"3 For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, for that he was buried, that he was raised on the third day according to the Scriptures.\" Some believe that the Old and New Testaments have differing opinions on the expiation of guilt because the Old Testaments were subject to the Age of Law and the New Testaments replace the Age of Law with the now current Age of Grace. However, both in the Old Testament and the New Testament salvation was granted based on God's grace and forgiveness (Gen 6:8; 19:19; Exo 33:12–17; 34:6–7). Animal sacrifices were only a symbol of the future sacrifice of Jesus Christ (Heb 10:1–4; 9–12). The whole world is guilty before God for abandoning him and his ways (Rom 3:19). In Jesus Christ, God took upon himself the sins of the world and died on the cross to pay our debt (Rom 6:23). Those who repent and accept the sacrifice of Jesus Christ for their sins, will be redeemed by God and thus not guilty before him. They will be granted eternal life which will take effect when Jesus comes the second time (1 Thess 4:13–18). In contrast to surrounding nations which addressed their guilt with human sacrifice, the Israeli authors of the Bible called that an abomination (1 Kings 11:7, Jer 32:35). The Bible agrees with pagan cultures that guilt creates a cost that someone must pay (Heb 9:22). (This assumption was expressed in the previous section, \"Defences\": \"Guilty people punish themselves if they have no opportunity to compensate the transgression that caused them to feel guilty. It was found that self-punishment did not occur if people had an opportunity to compensate the victim of their transgression.\") But unlike pagan deities who demanded it be paid by humans, God, according to the Bible, loved us enough to pay it Himself, as a good father would, while calling us His \"children\" and calling Himself our \"father\" (Mat 5:45).\n\n\n"}
{"id": "2085068", "url": "https://en.wikipedia.org/wiki?curid=2085068", "title": "Gödel metric", "text": "Gödel metric\n\nThe Gödel metric is an exact solution of the Einstein field equations in which the stress–energy tensor contains two terms, the first representing the matter density of a homogeneous distribution of swirling dust particles (dust solution), and the second associated with a nonzero cosmological constant (see lambdavacuum solution). It is also known as the Gödel solution or Gödel universe.\n\nThis solution has many unusual properties—in particular, the existence of closed timelike curves that would allow time travel in a universe described by the solution. Its definition is somewhat artificial in that the value of the cosmological constant must be carefully chosen to match the density of the dust grains, but this spacetime is an important pedagogical example.\n\nThe solution was found in 1949 by Kurt Gödel.\n\nLike any other Lorentzian spacetime, the Gödel solution presents the metric tensor in terms of some local coordinate chart. It may be easiest to understand the Gödel universe using the cylindrical coordinate system (presented below), but this article uses the chart that Gödel originally used. In this chart, the line element is\nwhere formula_2 is a nonzero real constant, which turns out to be the angular velocity of the surrounding dust grains around the \"y\" axis, as measured by a \"non-spinning\" observer riding one of the dust grains. \"Non-spinning\" means that it doesn't feel centrifugal forces, but in this coordinate frame it would actually be turning on an axis parallel to the \"y\" axis. As we shall see, the dust grains stay at constant values of \"x\", \"y\", and \"z\". Their density in this coordinate chart increases with \"x\", but their density in their own frames of reference is the same everywhere.\n\nTo study the properties of the Gödel solution, we will adopt the frame field (dual to the coframe read off the metric as given above),\nThis frame defines a family of inertial observers who are \"comoving with the dust grains\". However, computing the Fermi–Walker derivatives with respect to formula_7 shows that the spatial frames are \"spinning\" about formula_8 with angular velocity formula_9. It follows that the \"nonspinning inertial frame\" comoving with the dust particles is\n\nThe components of the Einstein tensor (with respect to either frame above) are\nHere, the first term is characteristic of a lambdavacuum solution and the second term is characteristic of a pressureless perfect fluid or dust solution. Notice that the cosmological constant is carefully chosen to partially cancel the matter density of the dust.\n\nThe Gödel spacetime is a rare example of a \"regular\" (singularity-free) solution of the Einstein field equation. Gödel's original chart (given here) is geodesically complete and singularity free; therefore, it is a global chart, and the spacetime is homeomorphic to R, and therefore, simply connected.\n\nIn any Lorentzian spacetime, the fourth-rank Riemann tensor is a multilinear operator on the four-dimensional space of tangent vectors (at some event), but a linear operator on the six-dimensional space of bivectors at that event. Accordingly, it has a characteristic polynomial, whose roots are the eigenvalues. In the Gödel spacetime, these eigenvalues are very simple:\n\nThis spacetime admits a five-dimensional Lie algebra of Killing vectors, which can be generated by \"time translation\" formula_17, two \"spatial translations\" formula_18, plus two further Killing vector fields:\nand\nThe isometry group acts \"transitively\" (since we can translate in formula_21, and using the fourth vector we can move along formula_22 as well), so the spacetime is \"homogeneous\". However, it is not \"isotropic\", as we shall see.\n\nIt is obvious from the generators just given that the slices formula_23 admit a transitive abelian three-dimensional transformation group, so a quotient of the solution can be reinterpreted as a stationary cylindrically symmetric solution. Less obviously, the slices formula_24 admit an SL(2,R) action, and the slices formula_25 admit a Bianchi III (c.f. the fourth Killing vector field). We can restate this by saying that our symmetry group includes as three-dimensional subgroups examples of Bianchi types I, III and VIII. Four of the five Killing vectors, as well as the curvature tensor, do not depend upon the coordinate y. Indeed, the Gödel solution is the Cartesian product of a factor R with a three-dimensional Lorentzian manifold (signature -++).\n\nIt can be shown that the Gödel solution is, up to local isometry, the \"only\" perfect fluid solution of the Einstein field equation admitting a five-dimensional Lie algebra of Killing vectors.\n\nThe Weyl tensor of the Gödel solution has Petrov type D. This means that for an appropriately chosen observer, the tidal forces have \"Coulomb form\".\n\nTo study the tidal forces in more detail, we compute the Bel decomposition of the Riemann tensor into three pieces, the tidal or electrogravitic tensor (which represents tidal forces), the magnetogravitic tensor (which represents spin-spin forces on spinning test particles and other gravitational effects analogous to magnetism), and the topogravitic tensor (which represents the spatial sectional curvatures).\n\nObservers comoving with the dust particles find that the \"tidal tensor\" (with respect to formula_26, which components evaluated in our frame) has the form\nThat is, they measure isotropic tidal tension orthogonal to the distinguished direction formula_28.\n\nThe gravitomagnetic tensor \"vanishes identically\"\nThis is an artifact of the unusual symmetries of this spacetime, and implies that the putative \"rotation\" of the dust does not have the gravitomagnetic effects usually associated with the gravitational field produced by rotating matter.\n\nThe principal Lorentz invariants of the Riemann tensor are\nThis gives the conditions\nPlugging these into the Einstein tensor, we see that in fact we now have formula_32. The simplest nontrivial spacetime we can construct in this way evidently would have this coefficient be some nonzero but \"constant\" function of the radial coordinate. Specifically, with a bit of foresight, let us choose formula_33. This gives\nFinally, let us demand that this frame satisfy\nThis gives formula_36, and our frame becomes\n\nFrom the metric tensor we find that the vector field formula_38, which is \"spacelike\" for small radii, becomes \"null\" at formula_39 where\nThis is because at that radius we find that formula_41 so formula_42 and is therefore null. The circle formula_43 at a given \"t\" is a closed null curve, but not a null geodesic.\n\nExamining the frame above, we can see that the coordinate formula_44 is inessential; our spacetime is the direct product of a factor R with a signature -++ three-manifold. Suppressing formula_44 in order to focus our attention on this three-manifold, let us examine how the appearance of the light cones changes as we travel out from the axis of symmetry formula_46:\n\nWhen we get to the critical radius, the cones become tangent to the closed null curve.\n\nAt the critical radius formula_43, the vector field formula_38 becomes null. For larger radii, it is \"timelike\". Thus, corresponding to our symmetry axis we have a timelike congruence made up of \"circles\" and corresponding to certain observers. This congruence is however \"only defined outside the cylinder\" formula_39.\n\nThis is not a geodesic congruence; rather, each observer in this family must maintain a \"constant acceleration\" in order to hold his course. Observers with smaller radii must accelerate harder; as formula_50 the magnitude of acceleration diverges, which is just what is expected, given that formula_39 is a null curve.\n\nIf we examine the past light cone of an event on the axis of symmetry, we find the following picture:\n\nRecall that vertical coordinate lines in our chart represent the world lines of the dust particles, but \"despite their straight appearance in our chart\", the congruence formed by these curves has nonzero vorticity, so the world lines are actually \"twisting about each other\". The fact that the null geodesics spiral inwards in the manner shown above means that when our observer looks \"radially outwards\", he sees nearby dust particles, not at their current locations, but at their earlier locations. This is just what we would expect if the dust particles are in fact rotating about one another.\n\nThe null geodesics are \"geometrically straight\"; in the figure, they appear to be spirals only because the coordinates are \"rotating\" in order to permit the dust particles to appear stationary.\n\nAccording to Hawking and Ellis (see monograph cited below), all light rays emitted from an event on the symmetry axis reconverge at a later event on the axis, with the null geodesics forming a circular cusp (which is a null curve, but not a null geodesic):\n\nThis implies that in the Gödel lambdadust solution, the absolute future of each event has a character very different from what we might naively expect.\n\nFollowing Gödel, we can interpret the dust particles as galaxies, so that the Gödel solution becomes a \"cosmological model of a rotating universe\". Besides rotating, this model exhibits no Hubble expansion, so it is not a realistic model of the universe in which we live, but can be taken as illustrating an alternative universe, which would in principle be allowed by general relativity (if one admits the legitimacy of a nonzero cosmological constant). Less well known solutions of Gödel's exhibit both rotation and Hubble expansion and have other qualities of his first model, but travelling into the past is not possible. According to S. W. Hawking, \"these models could well be a reasonable description of the universe that we observe\", however observational data are compatible only with a very low rate of rotation. The quality of these observations improved continually up until Gödel's death, and he would always ask \"is the universe rotating yet?\" and be told \"no, it isn't\".\n\nWe have seen that observers lying on the \"y\" axis (in the original chart) see the rest of the universe rotating clockwise about that axis. However, the homogeneity of the spacetime shows that the \"direction\" but not the \"position\" of this \"axis\" is distinguished.\n\nSome have interpreted the Gödel universe as a counterexample to Einstein's hopes that general relativity should exhibit some kind of Mach's principle, citing the fact that the matter is rotating (world lines twisting about each other) in a manner sufficient to pick out a preferred direction, although with no distinguished axis of rotation.\n\nOthers take \"Mach principle\" to mean some physical law tying the definition of nonspinning inertial frames at each event to the global distribution and motion of matter everywhere in the universe, and say that because the nonspinning inertial frames are precisely tied to the rotation of the dust in just the way such a Mach principle would suggest, this model \"does\" accord with Mach's ideas.\n\nMany other exact solutions that can be interpreted as cosmological models of rotating universes are known. See the book by Ryan and Shepley for some of these generalizations.\n\n\n"}
{"id": "20853872", "url": "https://en.wikipedia.org/wiki?curid=20853872", "title": "Health action process approach", "text": "Health action process approach\n\nThe health action process approach (HAPA) is a psychological theory of health behavior change, developed by Ralf Schwarzer, Professor of Psychology at the Free University of Berlin, Germany.\n\nHealth behavior change refers to a replacement of health-compromising behaviors (such as sedentary behavior) by health-enhancing behaviors (such as physical exercise). To describe, predict, and explain such processes, theories or models are being developed. Health behavioural change theories are designed to examine a set of psychological constructs that jointly aim at explaining what motivates people to change and how they take preventive action.\n\nHAPA is an open framework of various motivational and volitional constructs that are assumed to explain and predict individual changes in health behaviors such as quitting smoking or drinking, and improving physical activity levels, dental hygiene, seat belt use, breast self-examination, dietary behaviors, and avoiding drunk driving. HAPA suggests that the adoption, initiation, and maintenance of health behaviors should be conceived of as a structured process including a motivation phase and a volition phase. The former describes the intention formation while the latter refers to planning, and action (initiative, maintenance, recovery). The model emphasizes the particular role of perceived self-efficacy at different stages of health behavior change.\n\nModels that describe health behavior change can be distinguished in terms of the assumption whether they are continuum-based or stage-based. A continuum (mediator) model claims that change is a continuous process that leads from lack of motivation via action readiness either to successful change or final disengagement. Research on such mediator models are reflected by path diagrams that include distal and proximal predictors of the target behavior. On the other hand, the stage approach assumes that change is non-linear and consists of several qualitative steps that reflect different mindsets of people. A two-layer framework that can be applied either as a continuum or as a stage model is HAPA. It includes self-efficacy, outcome expectancies, and risk perception as distal predictors, intention as a middle-level mediator, and volitional factors (such as action planning) as the most proximal predictors of behavior. \"See Self-efficacy.\"\n\nGood intentions are more likely to be translated into action when people plan when, where, and how to perform the desired behavior. Intentions foster planning, which in turn facilitates behavior change. Planning was found to mediate the intention-behavior relation. A distinction has been made between action planning and coping planning. Coping planning takes place when people imagine scenarios that hinder them to perform their intended behavior, and they develop one or more plans to cope with such a challenging situation.\n\nHAPA is designed as a sequence of two continuous self-regulatory processes, a goal-setting phase (motivation) and a goal-pursuit phase (volition). The second phase is subdivided into a pre-action phase and an action phase. Thus, one can superimpose these three phases (stages) on the continuum (mediator) model as a second layer, and regard the stages as moderators. This two-layer architecture allows to switch between the continuum model and the stage model, depending on the given research question.\n\nHAPA has five major principles that make it distinct from other models.\n\n\"Principle 1: Motivation and volition\". The first principle suggests that one should divide the health behavior change process into two phases. There is a switch of mindsets when people move from deliberation to action. First comes the motivation phase in which people develop their intentions. Afterwards, they enter the volition phase.\n\n\"Principle 2: Two volitional phases\". In the volition phase there are two groups of individuals: those who have not yet translated their intentions into action, and those who have. There are inactive as well as active persons in this phase. In other words, in the volitional phase one finds intenders as well as actors who are characterized by different psychological states. Thus, in addition to health behavior change as a continuous process, one can also create three categories of people with different mindsets depending on their current point of residence within the course of health behavior change: preintenders, intenders, and actors. The assessment of stages is done by behavior-specific stage algorithms.\n\n\"Principle 3: Postintentional planning\". Intenders who are in the volitional preactional stage are motivated to change, but do not act because they might lack the right skills to translate their intention into action. Planning is a key strategy at this point. Planning serves as an operative mediator between intentions and behavior.\n\n\"Principle 4: Two kinds of mental simulation\". Planning can be divided into action planning and coping planning. Action planning pertains to the when, where, and how of intended action. Coping planning includes the anticipation of barriers and the design of alternative actions that help to attain one's goals in spite of the impediments. The separation of the planning construct into two constructs, action planning and coping planning, has been found useful as studies have confirmed the discriminant validity of such a distinction. Action planning seems to be more important for the initiation of health behaviors, whereas coping planning is required for the initiation and maintenance of actions as well.\n\n\"Principle 5: Phase-specific self-efficacy\". Perceived self-efficacy is required throughout the entire process. However, the nature of self-efficacy differs from phase to phase. This difference relates to the fact that there are different challenges as people progress from one phase to the next one. Goal setting, planning, initiation, action, and maintenance pose challenges that are not of the same nature. Therefore, one should distinguish between preactional self-efficacy, coping self-efficacy, and recovery self-efficacy. Sometimes the terms task self-efficacy instead of preaction self-efficacy, and maintenance self-efficacy instead of coping and recovery self-efficacy are preferred.\n\nWhen it comes to the design of interventions, one can consider identifying individuals who reside either at the motivational stage or the volitional stage. Then, each group becomes the target of a specific treatment that is tailored to this group. Moreover, it is theoretically meaningful and has been found useful to subdivide further the volitional group into those who perform and those who only intend to perform. In the postintentional preactional stage, individuals are labeled \"intenders\", whereas in the actional stage they are labeled \"actors\". Thus, a suitable subdivision within the health behavior change process yields three groups: nonintenders, intenders, and actors. The term \"stage\" in this context was chosen to allude to the stage theories, but not in the strict definition that includes irreversibility and invariance. The terms \"phase\" or \"mindset\" may be equally suitable for this distinction. The basic idea is that individuals pass through different mindsets on their way to behavior change. Thus, interventions may be most efficient when tailored to these particular mindsets. For example, nonintenders are supposed to benefit from confrontation with outcome expectancies and some level of risk communication. They need to learn that the new behavior (e.g., becoming physically active) has positive outcomes (e.g., well-being, weight loss, fun) as opposed to the negative outcomes that accompany the current (sedentary) behavior (such as developing an illness or being unattractive). In contrast, intenders should not benefit from such a treatment because, after setting a goal, they have already moved beyond this mindset. Rather, they should benefit from planning to translate their intentions into action. Finally, actors do not need any treatment at all unless one wants to improve their relapse prevention skills. Then, they should be prepared for particular high-risk situations in which lapses are imminent. Preparation can be exercised by teaching them to anticipate such situations and by acquiring the necessary levels of perceived recovery self-efficacy.\nThere are quite a few randomized controlled trials that have examined the notion of stage-matched interventions based on HAPA, for example in the context of dietary behaviors, physical activity,\nand dental hygiene.\n\n\n\n"}
{"id": "40277919", "url": "https://en.wikipedia.org/wiki?curid=40277919", "title": "How to Lose Your Virginity", "text": "How to Lose Your Virginity\n\nHow to Lose Your Virginity is an American documentary film directed by Therese Shechter and distributed by Women Make Movies. The film examines how the concept of virginity shapes the sexual lives of young women and men through the intersecting forces of history, politics, religion and popular culture. It premiered at DOC NYC, a New York City documentary festival, on November 17, 2013.\n\n\"How to Lose Your Virginity\" explores the concept of virginity from historical origins of the word, virgin, to the modern day definitions perpetuated in popular culture. The film takes a critical look at how virginity is ‘restored’ through hymenoplasty, fetishized by pornography, and celebrated at purity balls. Linking virginity culture to commerce, the film follows Natalie Dylan's virginity auction and the sales of artificial hymen on the internet. Shechter also visits the set of \"Barely Legal\" and discusses the success of the \"virginity porn\" genre. \"How to Lose Your Virginity\" questions the effectiveness of the Abstinence-only sex education movement and observes how sexuality continues to define a young woman’s morality and self-worth.\nThe meaning and necessity of virginity as a social construct is also examined through narration and interviews with notable sexuality experts, such as: former Surgeon General Dr. Joycelyn Elders, \"Scarleteen\" creator and editor Heather Corinna, historian Hanne Blank, author Jessica Valenti, and comprehensive sex education advocate Shelby Knox.\n\nThe film was directed by Therese Shechter, whose production company Trixie Films is based in Brooklyn. Working with Producer Lisa Esselstein, \"How to Lose Your Virginity\" was shot over several years in the U.S. and Canada. Other films produced by Trixie Films include the documentary feature \"I Was A Teenage Feminist\" and the documentary shorts \"How I Learned to Speak Turkish\" and \"#slutwalknyc\".\n\nShechter was inspired to make the film because of the growing abstinence until marriage movement and her own experiences as an older virgin. While making the film, Shechter became engaged and incorporated trying on white wedding dresses into the film as a way of looking at how the wedding industry sells virginity.\n\nOver the course of the film's production, its transmedia companion, \"The V-Card Diaries\" has crowd-sourced over 200 stories about what the site calls \"sexual debuts and deferrals.\" It was exhibited at The Kinsey Institute's 8th Annual Juried Art Show, the exhibit's first interactive piece.\n\nSoraya Chemaly wrote in the \"Huffington Post\", \"Virginity is a powerful and malleable concept, as evidenced by the teenagers in Therese Shechter's smart, funny and provoking documentary.\" Leigh Kolb of Bitch Flicks said that \"There's no anger, there's no judgment…Shechter’s ability to teach, dismantle, expose and explore is remarkable. The audience is left with newfound knowledge with which they can criticize myths of virginity in our culture. However, the audience is also left with respect for everyone’s stories. When a documentary can do that, it succeeds in a big way.\"\n\nIn the \"Jakarta Globe\", Paul Freelend wrote that \"her work to highlight what she calls the 'virginity culture' and the misconceptions surrounding it may resonate as loudly in Indonesia and other developing countries as in the United States.\" Basil Tsoikos, programmer for the Sundance Film Festival and DOC NYC, in What (not) to doc remarked that \"Shechter seems like the perfect filmmaker to tackle the complexities around virginity. It’s a topic that far too many people are obsessed about – probably for all the wrong reasons – so the film is sure to stimulate interest and provoke heated debate.\"\n\nJ. Maureen Henderson of Forbes.com said that Shechter's work \"tackles one of the last taboos in our culture’s discussion of sex – the deliberate decision not to participate in it.” Lena Corner of \"The Guardian\" wrote that \"It’s refreshing to hear such forthright voices in a world where any debate about virginity is often so conflicting or one-sided.\" Jennifer Wadsworth of SFGate.com remarked that the project is \"More than just a narrative about virginity. It’s about the connection of storytelling and how hearing about other people’s experience can make anyone else feel less alone in theirs.\"\n\n"}
{"id": "1805135", "url": "https://en.wikipedia.org/wiki?curid=1805135", "title": "Inequity aversion", "text": "Inequity aversion\n\nInequity aversion (IA) is the preference for fairness and resistance to incidental inequalities. The social sciences that study inequity aversion include sociology, economics, psychology, anthropology, and ethology.\n\nInequity aversion research on humans mostly occurs in the discipline of economics though it is also studied in sociology.\n\nResearch on inequity aversion began in 1978 when studies suggested that humans are sensitive to inequities in favor of as well as those against them, and that some people attempt overcompensation when they feel \"guilty\" or unhappy to have received an undeserved reward.\n\nA more recent definition of inequity aversion (resistance to inequitable outcomes) was developed in 1999 by Fehr and Schmidt. They postulated that people make decisions so as to minimize inequity in outcomes. Specifically, consider a setting with individuals {1,2...,\"n\"} who receive pecuniary outcomes \"x\". Then the utility to person \"i\" would be given by \n\nwhere α parametrizes the distaste of person \"i\" for disadvantageous inequality in the first nonstandard term, and β parametrizes the distaste of person \"i\" for advantageous inequality in the final term.\n\nFehr and Schmidt showed that disadvantageous inequity aversion manifests itself in humans as the \"willingness to sacrifice potential gain to block another individual from receiving a superior reward\". They argue that this apparently self-destructive response is essential in creating an environment in which bilateral bargaining can thrive. Without inequity aversion's rejection of injustice, stable cooperation would be harder to maintain (for instance, there would be more opportunities for successful free riders).\n\nJames H. Fowler and his colleagues also argue that inequity aversion is essential for cooperation in multilateral settings. In particular, they show that subjects in \"random income\" games (closely related to public goods games) are willing to spend their own money to reduce the income of wealthier group members and increase the income of poorer group members even when there is no cooperation at stake. Thus, individuals who free ride on the contributions of fellow group members are likely to be punished because they earn more, creating a decentralized incentive for the maintenance of cooperation.\n\nInequity aversion is broadly consistent with observations of behavior in three standard economics experiments:\n\nIn 2005, John List modified these experiments slightly to determine if something in the construction of the experiments was prompting specific behaviors. When given a choice to steal money from the other player, even a single dollar, the observed altruism all but disappeared. In another experiment, the two players were given a sum of money and the choice to give or take any amount from the other player. In this experiment, only 10% of the participants gave the other person any money at all, and fully 40% of the players opted to take all of the other player's money.\n\nThe last such experiment was identical to the former, where 40% were turned into a gang of robbers, with one catch: the two players were forced to earn the money by stuffing envelopes. In this last experiment, more than two thirds of the players neither took nor gave a cent, while just over 20% still took some of the other player's money.\n\nIn 2011, Ert Erev and Roth ran a model prediction competition on two datasets, each of which included 120 two-player games. In each game player 1 decides whether to \"opt out\" and determine the payoffs for both players, or to \"opt in\" and let player 2 decide about the payoff allocation by choosing between actions \"left\" or \"right\". The payoffs were randomly selected, so the dataset included games like the Ultimatum, Dictator, and Trust, as well as other games. The results suggested that inequity aversion could be described as one of many strategies that people might use in such games.\n\nOther research in experimental economics addresses risk aversion in decision making and the comparison of inequality measures to subjective judgments on perceived inequalities.\n\nSurveys of employee opinions within firms have shown modern labor economists that inequity aversion is very important to them. Employees compare not only relative salaries but also relative performance against that of co-workers. Where these comparisons lead to guilt or envy, inequity aversion may lower employee morale. According to Bewley (1999), the main reason that managers create formal pay structures is so that the inter-employee comparison is seen to be \"fair\", which they considered \"key\" for morale and job performance.\n\nIt is natural to think of inequity aversion leading to greater solidarity within the labor pool, to the benefit of the average employee. However, a 2008 paper by Pedro Rey-Biel shows that this assumption can be subverted, and that an employer can use inequity aversion to get higher performance for less pay than would be possible otherwise. This is done by moving away from formal pay structures and using off-equilibrium bonus payments as incentives for extra performance. He shows that the optimal contract for inequity aversion employees is less generous at the optimal production level than contracts for \"standard agents\" (who don't have inequity aversion) in an otherwise identical two-employee model.\n\nIn 2005 Avner Shaked distributed a \"pamphlet\" entitled \"The Rhetoric of Inequity Aversion\" that attacked the inequity aversion papers of Fehr & Schmidt. In 2010, Shaked has published an extended version of the criticism together with Ken Binmore in the \"Journal of Economic Behavior and Organization\" (the same issue also contains a reply by Fehr and Schmidt and a rejoinder by Binmore and Shaked). A problem of inequity aversion models is the fact that there are free parameters; standard theory is simply a special case of the inequity aversion model. Hence, by construction inequity aversion must always be at least as good as standard theory when the inequity aversion parameters can be chosen after seeing the data. Binmore and Shaked also point out that Fehr and Schmidt (1999) pick a distribution of alpha and beta without conducting a formal estimation. The perfect correlation between the alpha and beta parameters in Fehr and Schmidt (1999) is an assumption made in the appendix of their paper that is not justified by the data that they provide. \n\nMore recently, several papers have estimated Fehr-Schmidt inequity aversion parameters using estimation techniques such as maximum likelihood. The results are mixed. Some authors have found beta larger than alpha, which contradicts a central assumption made by Fehr and Schmidt (1999). Other authors have found that inequity aversion with Fehr and Schmidt's (1999) distribution of alphas and betas explains data of contract-theoretic experiments not better than standard theory; they also estimate average values of alpha that are much smaller than suggested by Fehr and Schmidt (1999). Moreover, Levitt and List (2007) have pointed out that laboratory experiments tend to exaggerate the importance of pro-social behaviors because the subjects in the laboratory know that they are being monitored. \n\nAn alternative to the concept of a general inequity aversion is the assumption, that the \"degree\" and the structure of inequality could lead either to acceptance or to aversion of inequality.\n\nAn experiment on capuchin monkeys (Brosnan, S and de Waal, F) showed that the subjects would prefer receiving nothing to receiving a reward awarded inequitably in favor of a second monkey, and appeared to target their anger at the researchers responsible for the inequitable distribution of food. Anthropologists suggest that this research indicates a biological and evolutionary sense of social \"fair play\" in primates, though others believe that this is learned behavior or explained by other mechanisms. There is also evidence for inequity aversion in chimpanzees (though see a recent study questioning this interpretation). The latest study shows that chimpanzees play the Ultimatum Game in the same way as children, preferring equitable outcomes. The authors claim that we now are near the point of no difference between humans and apes with regard to a sense of fairness. Recent studies suggest that animals in the canidae family also recognize a basic level of fairness, stemming from living in cooperative societies. Animal cognition studies in other biological orders have not found similar importance on \"relative\" \"equity\" and \"justice\" as opposed to \"absolute\" utility.\n\nFehr and Schmidt's model may partially explain the widespread opposition to economic inequality in democracies, but a distinction should be drawn between inequity aversion's \"guilt\" and egalitarianism's \"compassion\", which does not necessarily imply \"injustice\".\n\nInequity aversion should not be confused with the arguments against the \"consequences\" of inequality. For example, the pro-publicly funded health care slogan \"Hospitals for the poor become poor hospitals\" directly objects to a predicted decline in medical care, not the health-care apartheid that is supposed to cause it. The argument that average medical outcomes improve with reduction in healthcare inequality (at the same total spending) is separate from the case for public healthcare on the grounds of inequity aversion.\n\n\n"}
{"id": "25019252", "url": "https://en.wikipedia.org/wiki?curid=25019252", "title": "Interactionism (philosophy of mind)", "text": "Interactionism (philosophy of mind)\n\nInteractionism or interactionist dualism is the theory in the philosophy of mind which holds that matter and mind are two distinct and independent substances that exert causal effects on one another. It is one type of dualism, traditionally a type of substance dualism though more recently also sometimes a form of property dualism.\n\nInteractionism was propounded by the French rationalist philosopher René Descartes (1596–1650), and continues to be associated with him. Descartes posited that the body, being physical matter, was characterized by spatial extension but not by thought and feeling, while the mind, being a separate substance, had no spatial extension but could think and feel. Nevertheless, he maintained that the two interacted with one another, suggesting that this interaction occurred in the pineal gland of the brain.\n\nIn the 20th century, its most significant defenders have been the noted philosopher of science Karl Popper and the neurophysiologist John Carew Eccles. Popper in fact divided reality into three \"worlds\"—the physical, the mental, and objective knowledge (outside the mind)—all of which interact, and Eccles adopted this same \"trialist\" form of interactionism. Other notable recent philosophers to take an interactionist stance have been Richard Swinburne, John Foster, David Hodgson, and Wilfrid Sellars, in addition to the physicist Henry Stapp.\n\nIn his 1996 book \"The Conscious Mind\", David Chalmers rejected interactionism, but in 2002 he reversed his position and listed it along with epiphenomenalism and what he calls \"Type-F monism\" as a position worth examining. Rather than invoking two distinct substances, he defines interactionism as the view that \"microphysics is not causally closed, and that phenomenal properties play a causal role in affecting the physical world.\" (See property dualism.) He argues the most plausible place for consciousness to impact physics is the collapse of the wave function in quantum mechanics.\n\nThe \"New Catholic Encyclopedia\" argues that a non-physical mind and mind-body interaction follow necessarily from the Catholic doctrines of the soul and free will.\n\nOne objection often posed to interactionism is the \"problem of causal interaction\" – how the two different substances the theory posits, the mental and the physical, can exert an impact on one another. Descartes' theory that interaction between the mind and the physical world occurred in the pineal gland was seen as inadequate by a number of philosophers in his era, who offered alternate views: Nicholas Malebranche suggested occasionalism, according to which mind and body appear to interact but are in fact moved separately by God, while Gottfried Leibniz argued in \"The Monadology\" that mind and body are in a pre-established harmony. On the other hand, Baruch Spinoza rejected Descartes' dualism and proposed that mind and matter were in fact properties of a single substance, thereby prefiguring the modern perspective of neutral monism.\n\nToday the problem of causal interaction is frequently viewed as a conclusive argument against interactionism. On the other hand, it has been suggested that given many disciplines deal with things they do not entirely understand, dualists not entirely understanding the mechanism of mind-body interaction need not be seen as definitive refutation. The idea that causation necessarily depends on push-pull mechanisms (which would not be possible for a substance that did not occupy space) is also arguably based on obsolete conceptions of physics.\n\nThe problem of mental causation is also discussed in the context of other positions on the mind-body problem, such as property dualism and anomalous monism.\n\nA more recent related objection is the \"argument from physics\", which argues that a mental substance impacting the physical world would contradict principles of physics. In particular, if some external source of energy is responsible for the interactions, it would violate the law of conservation of energy. Two main responses to this have been to suggest the mind influences the distribution but not the quantity of energy in the brain and to deny that the brain is a causally closed system in which conservation of energy would apply.\n\nTaking the argument a step further, it has been argued that because physics fully accounts for the causes of all physical movements, there can be no place for the a non-physical mind to play a role. The principle, in slightly different iterations, has variously been called \"causal closure\", \"completeness of the physical\", \"physical closure\", and \"physical comprehensiveness\". This has been the foremost argument against interactionism in contemporary philosophy.\n\nSome philosophers have suggested the influence of the mind on the body could be reconciled with deterministic physical laws by proposing the mind's impacts instead take place at points of quantum indeterminacy. Karl Popper and John Eccles, as well as the physicist Henry Stapp, have theorized that such indeterminacy may apply at the macroscopic scale. (See quantum mind.) However, Max Tegmark has argued that classical and quantum calculations show that quantum decoherence effects do not play a role in brain activity. David Chalmers has noted (without necessarily endorsing) a second possibility within quantum mechanics, that consciousness' causal role is to collapse the wave function as per the Von Neumann-Wigner interpretation of quantum mechanics. He acknowledges this is at odds with the interpretations of quantum mechanics held by most physicists, but notes, \"There is some irony in the fact that philosophers reject interactionism on largely physical grounds (it is incompatible with physical theory), while physicists reject an interactionist interpretation of quantum mechanics on largely philosophical grounds (it is dualistic). Taken conjointly, these reasons carry little force...\".\n\nThere remains a literature in philosophy and science, albeit a much-contested one, that asserts evidence for emergence in various domains, which would undermine the principle of causal closure. (See emergentism.) Another option that has been suggested is that the interaction may involve dark energy, dark matter or some other currently unknown scientific process.\n\nAnother possible resolution is akin to parallelism—Eugene Mills holds that behavioral events are causally overdetermined, and can be explained by either physical or mental causes alone. An overdetermined event is fully accounted for by multiple causes at once. However, J. J. C. Smart and Paul Churchland have argued that if physical phenomena fully determine behavioral events, then by Occam's razor a non-physical mind is unnecessary. Andrew Melnyk argues that overdetermination would require an \"intolerable coincidence.\"\n\nWhile causal closure remains a key obstacle for interactionism, it is not relevant to all forms of dualism; epiphenomenalism and parallelism are unaffected as they do not posit that the mind affects the body.\n\nInteractionism can be distinguished from competing dualist theories of causation, including epiphenomenalism (which admits causation, but views causation as unidirectional rather than bidirectional), and parallelism (which denies causation, while seeking to explain the semblance of causation by other means such as pre-established harmony or occasionalism).\n\nIn \"The Conscious Mind\", David Chalmers argued that regardless of the mechanism by which the mental might impact the physical if interactionism were true, there was a deeper conceptual issue: the chosen mechanism could always be separated from its phenomenal component, leading to simply a new form of epiphenomenalism. Later, he suggested that while the causal component could be separated, interactionism was like \"type-F monism\" (Russellian monism, panpsychism, and panprotopsychism) in that it gave entities externally characterized by physical relationships the additional intrinsic feature of conscious properties.\n\n"}
{"id": "286918", "url": "https://en.wikipedia.org/wiki?curid=286918", "title": "Intermedia", "text": "Intermedia\n\nIntermedia was a term used in the mid-1960s by Fluxus artist Dick Higgins to describe various inter-disciplinary art activities that occurred between genres in the 1960s.\n\nThe areas such as those between drawing and poetry, or between painting and theatre could be described as \"intermedia\". With repeated occurrences, these new genres between genres could develop their own names (e.g. visual poetry, performance art); historically, an example is haiga, which combined brush painting and haiku into one composition.\n\nHiggins described the tendency of what he thought was the most interesting and best in the new art to cross boundaries of recognized media or even to fuse the boundaries of art with media that had not previously been considered for art forms, including computers. \n\nWith characteristic modesty, he often noted that Samuel Taylor Coleridge had first used the term.\n\nGene Youngblood also described intermedia, beginning in his \"Intermedia\" column for the Los Angeles Free Press beginning in 1967 as a part of a global network of multiple media that was \"expanding consciousness\"—the intermedia network—that would turn all people into artists by proxy. He gathered and expanded ideas from this series of columns in his 1970 book Expanded Cinema, with an introduction by Buckminster Fuller.\n\nIn 1968, Hans Breder founded the first university program in the United States to offer an M.F.A. in intermedia. The Intermedia Area at The University of Iowa graduated artists such as Ana Mendieta and Charles Ray. In addition, the program developed a substantial visiting artist tradition, bringing artists such as Dick Higgins, Vito Acconci, Allan Kaprow, Karen Finley, Robert Wilson and others to work directly with Intermedia students.\n\nOver the years, especially on the Iowa campus, \"intermedia\" has been used interchangeably with \"multi-media\". However, recently the latter term has become identified with electronic media in pop-culture. While Intermedia values both disciplines, the term \"Intermedia\" has become the preferred term for interdisciplinary practice.\n\nTwo other prominent University programs that focus on intermedia are the Intermedia program at Arizona State University and the Intermedia M.F.A. at the University of Maine, founded and directed by Fluxus scholar and author Owen Smith. Additionally, the Roski School of Fine Arts at the University of Southern California features Intermedia as an area of emphasis in their B.A. and B.F.A. programs. Concordia University in Montreal, QC offers a B.F.A. in Intermedia/Cyberarts. Herron School of Art and Design, Indiana University, Purdue University, Indianapolis, has a M.F.A. Program Photography and Intermedia degree. The University of Oregon offers a Master of Music degree in Intermedia Music Technology.\n\nIn the UK, Edinburgh College of Art [within University of Edinburgh] introduced a BA (Hons) Degree in Intermedia Arts, and intermedia can be a focus of study in Masters programmes.\n\n\n"}
{"id": "1861821", "url": "https://en.wikipedia.org/wiki?curid=1861821", "title": "Interpellation (philosophy)", "text": "Interpellation (philosophy)\n\nIn Marxist theory, interpellation is an important concept regarding the notion of ideology. It is associated in particular with the work of French philosopher Louis Althusser. According to Althusser, every society is made up of Ideological State Apparatuses (ISAs) and Repressive State Apparatuses (RSAs) which are instrumental to constant reproduction of the relations to production of that given society. While ISAs belong to the private domain and refer to private institutions (family, church but also the media and politics), the RSA is one public institution (police/military) controlled by the government. Consequently, 'interpellation' describes the process by which ideology, embodied in major social and political institutions (ISAs and RSAs), constitutes the very nature of individual subjects' identities through the process of \"hailing\" them in social interactions.\n\nAlthusser’s thought has made significant contributions to other French philosophers, notably Derrida, Kristeva, Barthes, Foucault, and Deleuze.\n\nIn \"Ideology and Ideological State Apparatuses (Notes Towards an Investigation)\", Althusser introduces the concepts of Ideological State Apparatuses (ISA), Repressive State Apparatuses (RSAs), ideology, and interpellation. In his writing, Althusser argues that \"there is no ideology except by the subject and for the subject\". This notion of subjectivity becomes central to his writings.\n\nTo illustrate this concept, Althusser gives the example of a friend who knocks on a door. The person inside asks \"Who is there?\" and only opens the door once the \"It’s me\" from the outside sounds familiar. By doing so, the person inside partakes in \"a material ritual practice of ideological recognition in everyday life\". In other words, Althusser’s central thesis is that \"you and I are always already subjects\" and are constantly engaging in everyday rituals, like greeting someone or shaking hands, which makes us subjected to ideology.\n\nAlthusser goes further to argue that \"all ideology hails or interpellates concrete individuals as concrete subjects\" and emphasizes that \"ideology ‘acts’ or ‘functions’ in such a way that it ... ‘transforms’ the individual into subjects\". This is made possible through Althusser’s notion of interpellation or hailing which is a non-specific and unconscious process. For example, when a police officer shouts (or hails) \"Hey, you there!\" and an individual turns around and so-to-speak ‘answers’ the call, he becomes a subject. Althusser argues that this is because the individual has realized that the hailing was addressed at him which makes him subjective to the ideology of democracy and law.\n\nConsequently, individual subjects are presented principally as produced by social forces, rather than acting as powerful independent agents with self-produced identities.\n\nAlthusser's argument here strongly draws from Jacques Lacan's concept of the mirror stage. However, unlike Lacan who distinguishes between the \"I\" and the \"subject\", Althusser collapses both concepts into one.\n\nGerman philosophers Theodor Adorno and Max Horkheimer employ a method of analysis similar to Althusser's notion of interpellation in their text \"Dialectic of Enlightenment\", although they do so 26 years before \"Ideology and Ideological State Apparatuses\" was released. Rather than situating their analysis on the State, Adorno and Horkheimer argue that the mass media is responsible for the construction of passive subjects. So unlike the police officer in Althusser’s example who reinforces the ideology of democracy and law, the mass media has now taken over this role and interpellates, or hails at, the passive consumer.\n\nFeminist scholar and queer theorist Judith Butler has critically applied a framework based on interpellation to highlight the social construction of gender identities. She argues that by hailing \"It’s a boy/girl,\" the newborn baby is ultimately positioned as subject.\n\nMedia theorist David Gauntlett argues that \"interpellation occurs when a person connects with a media text: when we enjoy a magazine or TV show, for example, this\nuncritical consumption means that the text has \"interpellated\" us into a certain set of assumptions, and caused us to tacitly accept a particular approach to the world.\"\n"}
{"id": "8729253", "url": "https://en.wikipedia.org/wiki?curid=8729253", "title": "Laughter-induced syncope", "text": "Laughter-induced syncope\n\nLaughter-induced syncope is an unusual but recognized form of situational syncope (fainting) likely to have a similar pathophysiological origin to tussive syncope. One reported case occurred while a patient was watching the television show \"Seinfeld\", and was given the name Seinfeld syncope.\n\nThere are few case reports of this syndrome in the literature. Patients, as in this case, might present initially to the ED, and laughter should be considered among the numerous differentials for syncope.\n\nLaughter-induced syncope should not be confused with cataplexy, a sudden loss of muscle tone triggered by strong emotions, particularly laughter. Unlike syncope, there is no loss of consciousness in cataplexy, which affects some sufferers of narcolepsy.\n\nTo date there have been few cases of laughter-induced syncope documented in medical literature.\n\n"}
{"id": "57718553", "url": "https://en.wikipedia.org/wiki?curid=57718553", "title": "Les Petites Bon-Bons", "text": "Les Petites Bon-Bons\n\nLes Petites Bon-Bons was a group of conceptual/life artists that originated in Milwaukee, Wisconsin, in the early 1970s. Their activities spanned the nascent gay activist movement, the international correspondence network and the Hollywood glitter rock scene. They are known largely by the work of Jerry Dreva and Robert Lambert, aka Jerry and Bobby Bonbon, and who achieved much of their fame by effectively manipulating major media outlets.\n\nLes Petites Bon-Bons was originally coined as the name of a review to be staged by a group of young gay activist friends called the Radical Queens, veterans of the local Milwaukee counterculture and gay liberation scene, but it was never produced. In the hands of Dreva and Lambert, the group identity became a vehicle for the synthesis of art, politics, and sex that concerned them at the time.\n\nAlthough other sub-identities became associated with the name, such as a fictional drum and bugle corps, the Bon-Bons became known mainly as a conceptual rock band. Although they had no musical abilities and had never played a concert, Jerry and Bobby encouraged the notion among the rock journalists they befriended when they moved to Los Angeles, in June 1973 and January 1974, respectively. Personal links were formed with David Bowie, Iggy Pop, Lou Reed, Sylvester and many others in the rock world.\n\nDreva and Lambert had begun sending art projects to artists and writers they admired in early 1971, generating responses from noted critic Jack Burnham, writer and gay activist Arthur Bell, and eventually generating an invitation to Richard Kostelanetz’s ASSEMBLING series, through which they became aware of the formal network of international artists known as The Eternal Network. These included Ray Johnson, the Western Front, General Idea and FILE megazine, Clemente Padin in Uruguay and COUM/Throbbing Gristle in the UK.\n"}
{"id": "195982", "url": "https://en.wikipedia.org/wiki?curid=195982", "title": "Levi-Civita symbol", "text": "Levi-Civita symbol\n\nIn mathematics, particularly in linear algebra, tensor analysis, and differential geometry, the Levi-Civita symbol represents a collection of numbers; defined from the sign of a permutation of the natural numbers , for some positive integer . It is named after the Italian mathematician and physicist Tullio Levi-Civita. Other names include the permutation symbol, antisymmetric symbol, or alternating symbol, which refer to its antisymmetric property and definition in terms of permutations.\n\nThe standard letters to denote the Levi-Civita symbol are the Greek lower case epsilon or , or less commonly the Latin lower case . Index notation allows one to display permutations in a way compatible with tensor analysis:\n\nwhere \"each\" index takes values . There are indexed values of , which can be arranged into an -dimensional array. The key defining property of the symbol is \"total antisymmetry\" in all the indices. When any two indices are interchanged, equal or not, the symbol is negated:\n\nIf any two indices are equal, the symbol is zero. When all indices are unequal, we have:\n\nwhere (called the parity of the permutation) is the number of pairwise interchanges of indices necessary to unscramble into the order , and the factor is called the sign or signature of the permutation. The value must be defined, else the particular values of the symbol for all permutations are indeterminate. Most authors choose , which means the Levi-Civita symbol equals the sign of a permutation when the indices are all unequal. This choice is used throughout this article.\n\nThe term \"-dimensional Levi-Civita symbol\" refers to the fact that the number of indices on the symbol matches the dimensionality of the vector space in question, which may be Euclidean or non-Euclidean, for example, or Minkowski space. The values of the Levi-Civita symbol are independent of any metric tensor and coordinate system. Also, the specific term \"symbol\" emphasizes that it is not a tensor because of how it transforms between coordinate systems; however it can be interpreted as a tensor density.\n\nThe Levi-Civita symbol allows the determinant of a square matrix, and the cross product of two vectors in three-dimensional Euclidean space, to be expressed in index notation.\n\nThe Levi-Civita symbol is most often used in three and four dimensions, and to some extent in two dimensions, so these are given here before defining the general case.\n\nIn two dimensions, Levi-Civita symbol is defined by:\n\nThe values can be arranged into a 2 × 2 antisymmetric matrix:\n\nUse of the two-dimensional symbol is relatively uncommon, although in certain specialized topics like supersymmetry and twistor theory it appears in the context of 2-spinors. The three- and higher-dimensional Levi-Civita symbols are used more commonly.\n\nIn three dimensions, the Levi-Civita symbol is defined by:\n\nThat is, is if is an even permutation of , if it is an odd permutation, and 0 if any index is repeated. In three dimensions only, the cyclic permutations of are all even permutations, similarly the anticyclic permutations are all odd permutations. This means in 3d it is sufficient to take cyclic or anticyclic permutations of and easily obtain all the even or odd permutations.\n\nAnalogous to 2-dimensional matrices, the values of the 3-dimensional Levi-Civita symbol can be arranged into a array:\n\nwhere is the depth (: ; : ; : ), is the row and is the column.\n\nSome examples:\n\nIn four dimensions, the Levi-Civita symbol is defined by:\n\nThese values can be arranged into a array, although in 4 dimensions and higher this is difficult to draw.\n\nSome examples:\n\nMore generally, in dimensions, the Levi-Civita symbol is defined by:\n\nThus, it is the sign of the permutation in the case of a permutation, and zero otherwise.\n\nUsing the capital pi notation for ordinary multiplication of numbers, an explicit expression for the symbol is:\n\nwhere the signum function (denoted ) returns the sign of its argument while discarding the absolute value if nonzero. The formula is valid for all index values, and for any (when or , this is the empty product). However, computing the formula above naively has a time complexity of , whereas the sign can be computed from the parity of the permutation from its disjoint cycles in only cost.\n\nA tensor whose components in an orthonormal basis are given by the Levi-Civita symbol (a tensor of covariant rank ) is sometimes called a permutation tensor.\n\nUnder the ordinary transformation rules for tensors the Levi-Civita symbol is unchanged under pure rotations, consistent with that it is (by definition) the same in all coordinate systems related by orthogonal transformations. However, the Levi-Civita symbol is a pseudotensor because under an orthogonal transformation of Jacobian determinant −1, for example, a reflection in an odd number of dimensions, it \"should\" acquire a minus sign if it were a tensor. As it does not change at all, the Levi-Civita symbol is, by definition, a pseudotensor. \n\nAs the Levi-Civita symbol is a pseudotensor, the result of taking a cross product is a pseudovector, not a vector.\n\nUnder a general coordinate change, the components of the permutation tensor are multiplied by the Jacobian of the transformation matrix. This implies that in coordinate frames different from the one in which the tensor was defined, its components can differ from those of the Levi-Civita symbol by an overall factor. If the frame is orthonormal, the factor will be ±1 depending on whether the orientation of the frame is the same or not.\n\nIn index-free tensor notation, the Levi-Civita symbol is replaced by the concept of the Hodge dual.\n\nIn a context where tensor index notation is used to manipulate tensor components, the Levi-Civita symbol may be written with its indices as either subscripts or superscripts with no change in meaning, as might be convenient. Thus, one could write\nIn these examples, superscripts should be considered equivalent with subscripts.\n\nSummation symbols can be eliminated by using Einstein notation, where an index repeated between two or more terms indicates summation over that index. For example,\nIn the following examples, Einstein notation is used.\n\nIn two dimensions, when all each take the values 1 and 2,\n\nIn three dimensions, when all each take values 1, 2, and 3:\n\nThe Levi-Civita symbol is related to the Kronecker delta. In three dimensions, the relationship is given by the following equations (vertical lines denote the determinant):\n\nA special case of this result is ():\n\nsometimes called the \"contracted epsilon identity\".\n\nIn Einstein notation, the duplication of the index implies the sum on . The previous is then denoted .\n\nIn dimensions, when all take values :\n\n^{j_{k+1}} \\dots \\delta_{i_n ]}^{j_n} = k!~\\delta^{j_{k+1} \\dots j_n}_{i_{k+1} \\dots i_n} </math>\nwhere the exclamation mark () denotes the factorial, and is the generalized Kronecker delta. For any , the property\n\nfollows from the facts that \n\nIn general, for dimensions, one can write the product of two Levi-Civita symbols as:\n\nFor (), both sides are antisymmetric with respect of and . We therefore only need to consider the case and . By substitution, we see that the equation holds for , that is, for and . (Both sides are then one). Since the equation is antisymmetric in and , any set of values for these can be reduced to the above case (which holds). The equation thus holds for all values of and .\n\nUsing (), we have for ()\n\nHere we used the Einstein summation convention with going from 1 to 2. Next, () follows similarly from ().\n\nTo establish (), notice that both sides vanish when . Indeed, if , then one can not choose and such that both permutation symbols on the left are nonzero. Then, with fixed, there are only two ways to choose and from the remaining two indices. For any such indices, we have\n\n(no summation), and the result follows.\n\nThen () follows since and for any distinct indices taking values , we have \n\nIn linear algebra, the determinant of a square matrix can be written\n\nSimilarly the determinant of an matrix can be written as\n\nwhere each should be summed over , or equivalently:\n\nwhere now each and each should be summed over . More generally, we have the identity\n\nIf and are vectors in (represented in some right-handed coordinate system using an orthonormal basis), their cross product can be written as a determinant:\n\nhence also using the Levi-Civita symbol, and more simply:\n\nIn Einstein notation, the summation symbols may be omitted, and the th component of their cross product equals\n\nThe first component is\n\nthen by cyclic permutations of the others can be derived immediately, without explicitly calculating them from the above formulae:\n\nFrom the above expression for the cross product, we have:\n\nIf is a third vector, then the triple scalar product equals\n\nFrom this expression, it can be seen that the triple scalar product is antisymmetric when exchanging any pair of arguments. For example,\n\nIf is a vector field defined on some open set of as a function of position (using Cartesian coordinates). Then the th component of the curl of equals\n\nwhich follows from the cross product expression above, substituting components of the gradient vector operator (nabla).\n\nIn any arbitrary curvilinear coordinate system and even in the absence of a metric on the manifold, the Levi-Civita symbol as defined above may be considered to be a tensor density field in two different ways. It may be regarded as a contravariant tensor density of weight +1 or as a covariant tensor density of weight −1. In \"n\" dimensions using the generalized Kronecker delta,\n\nNotice that these are numerically identical. In particular, the sign is the same.\n\nOn a pseudo-Riemannian manifold, one may define a coordinate-invariant covariant tensor field whose coordinate representation agrees with the Levi-Civita symbol wherever the coordinate system is such that the basis of the tangent space is orthonormal with respect to the metric and matches a selected orientation. This tensor should not be confused with the tensor density field mentioned above. The presentation in this section closely follows .\n\nThe covariant Levi-Civita tensor (also known as the Riemannian volume form) in any coordinate system that matches the selected orientation is\n\nwhere is the representation of the metric in that coordinate system. We can similarly consider a contravariant Levi-Civita tensor by raising the indices with the metric as usual,\n\nbut notice that if the metric signature contains an odd number of negatives , then the sign of the components of this tensor differ from the standard Levi-Civita symbol:\n\nwhere , and formula_39 is the usual Levi-Civita symbol discussed in the rest of this article. More explicitly, when the tensor and basis orientation are chosen such that formula_40, we have that formula_41.\n\nFrom this we can infer the identity,\nwhere\nis the generalized Kronecker delta.\n\nIn Minkowski space (the four-dimensional spacetime of special relativity), the covariant Levi-Civita tensor is\n\nwhere the sign depends on the orientation of the basis. The contravariant Levi-Civita tensor is\n\nThe following are examples of the general identity above specialized to Minkowski space (with the negative sign arising from the odd number of negatives in the signature of the metric tensor in either sign convention):\n\n\n"}
{"id": "990343", "url": "https://en.wikipedia.org/wiki?curid=990343", "title": "Lindenbaum–Tarski algebra", "text": "Lindenbaum–Tarski algebra\n\nIn mathematical logic, the Lindenbaum–Tarski algebra (or Lindenbaum algebra) of a logical theory \"T\" consists of the equivalence classes of sentences of the theory (i.e., the quotient, under the equivalence relation ~ defined such that \"p\" ~ \"q\" exactly when \"p\" and \"q\" are provably equivalent in \"T\"). That is, two sentences are equivalent if the theory \"T\" proves that each implies the other. The Lindenbaum–Tarski algebra is thus the quotient algebra obtained by factoring the algebra of formulas by this congruence relation.\n\nThe algebra is named for logicians Adolf Lindenbaum and Alfred Tarski. \nIt was first introduced by Tarski in 1935\nas a device to establish correspondence between classical propositional calculus and Boolean algebras. \nThe Lindenbaum–Tarski algebra is considered the origin of the modern algebraic logic.\n\nThe operations in a Lindenbaum–Tarski algebra \"A\" are inherited from those in the underlying theory \"T\". These typically include conjunction and disjunction, which are well-defined on the equivalence classes. When negation is also present in \"T\", then \"A\" is a Boolean algebra, provided the logic is classical. If the theory \"T\" consists of the propositional tautologies, the Lindenbaum–Tarski algebra is the free Boolean algebra generated by the propositional variables.\n\nHeyting algebras and interior algebras are the Lindenbaum–Tarski algebras for intuitionistic logic and the modal logic S4, respectively. \n\nA logic for which Tarski's method is applicable, is called \"algebraizable\". There are however a number of logics where this is not the case, for instance the modal logics S1, S2, or S3, which lack the rule of necessitation (⊢φ implying ⊢□φ), so ~ (defined above) is not a congruence (because ⊢φ→ψ does not imply ⊢□φ→□ψ). Another type of logics where Tarski's method is inapplicable are relevance logics, because given two theorems an implication from one to the other may not itself be a theorem in a relevance logic. The study of the algebraization process (and notion) as topic of interest by itself, not necessarily by Tarski's method, has led to the development of abstract algebraic logic.\n\n"}
{"id": "784621", "url": "https://en.wikipedia.org/wiki?curid=784621", "title": "Many-body theory", "text": "Many-body theory\n\nMany-body theory (or many-body physics) is an area of physics which provides the framework for understanding the collective behavior of large numbers of interacting particles, often on the order of Avogadro's number. In general terms, many-body theory deals with effects that manifest themselves only in systems containing large numbers of constituents. While the underlying physical laws that govern the motion of each individual particle may (or may not) be simple, the study of the collection of particles can be extremely complex. In some cases emergent phenomena may arise which bear little resemblance to the underlying elementary laws.\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "50859283", "url": "https://en.wikipedia.org/wiki?curid=50859283", "title": "Neville–Neville feud", "text": "Neville–Neville feud\n\nThe Neville–Neville feud was an inheritance dispute which took place in the north of England during the early fifteenth century between two branches of the Neville family. The inheritance in question was that of Ralph Neville, 1st Earl of Westmorland, a prominent northern nobleman who had issue from two marriages. Westmorland favoured as his heirs the children of his second wife, Joan Beaufort, closely related to the royal family, over those of his first wife, Margaret Stafford.\n\nAfter Ralph Neville's death in 1425, many of the Neville family holdings were transferred through legal means to the children of Joan Beaufort (the Middleham cadet branch of the Neville family), in effect disinheriting the senior branch (the Nevilles of Raby). This led to more than a decade of rivalry between both branches of the family. Ralph Neville's eldest son, John Neville, had died before his father. John Neville's son, also named Ralph, became the 2nd earl of Westmorland. Though the title earl of Westmorland passed to the senior Nevilles, for legal reasons, many holdings, particularly those of the Neville patrimony in Yorkshire and Raby Castle in Durham were transferred to Joan and her children. The Beaufort Nevilles were also able to consolidate their control over the County Palatine of Durham after Robert Neville assumed the office of Bishop of Durham in 1437.\n\nThe senior branch disputed their disinheritance — both legally and by force of arms — but Joan Beaufort's eldest son, Richard Neville, prevailed due to his family's greater political connections. The feud continued through the 1430s, until an agreement was reached in 1443. This settlement was largely favourable to Salisbury, and both branches of the family remained at odds with each other. The dispute between the senior and junior branches of the Neville family continued into the Wars of the Roses. During the prolonged civil war, the senior branch sided with the Lancastrians, while their cousins sided with the Yorkists. Margaret Stafford's grandsons gave Salisbury no support during the conflict and he was captured fighting for Richard of York at the Battle of Wakefield. Rather than being ransomed according to the usual custom of the time, Salisbury was beheaded by the common people \"who loved him not.\"\n\nDisputes over divided inheritances were not uncommon in later medieval England; apart from the dispute between branches of the Neville family, there were similar disputes within the Talbot and Mountford families. Historian Michael Hicks described these three disputes, where property was transferred from a senior line to a junior line, as \"particularly large scale and high profile\".\n\nRalph Neville, the 1st earl of Westmorland (c. 1364–1425) married twice. His first wife Margaret Stafford, daughter of the earl of Stafford, died in 1396. Shortly after her death, Ralph Neville married Joan Beaufort, daughter of John of Gaunt and cousin of King Richard II. They had 9 sons and 5 daughters together: the eldest, Richard Neville, became earl of Salisbury; their second son, William Neville, became earl of Kent and was created Baron Fauconberg; George Neville became the 1st Baron Latimer, Edward Neville was Baron Abergavenny and their youngest, Robert Neville, eventually assumed the office of Bishop of Durham. Historian Anthony Tuck writes that this marriage \"was to have major consequences both for the Neville family and for the English nobility\" throughout the 15th century. \n\nNeville's new proximity to the royal family through his marriage to the Richard II's cousin, and his loyalty to the Crown during the crisis of July 1397, led to his elevation to the peerage as earl of Westmorland in 1397. Joan and Ralph were granted numerous offices, lands, wardships and pensions. They continued to enjoy royal favour until the death of King Henry IV in 1413. \n\nThe Neville patrimony included lands in Yorkshire, Durham, Westmorland and Cumberland. After marrying Joan Beaufort, Ralph Neville began the process of disinheriting the children from his first marriage through a legal process called enfeoffment. The earl's eldest son John Neville, had previously agreed to a settlement in which he would inherit only Raby Castle and Brancepeth Castle in Durham. This transfer of property to the cadet branch resulted in the \"virtual disinheritance\" of the senior branch of the family. It was done legally and left the senior Nevilles with no legal recourse. Charles Ross has noted that the earl's eldest son does not seem to have attempted to stop his father or prevent his son's disinheritance, but may even have assisted with some of the transfers. \n\nThis process of transfer of property, the so-called \"Neville trust\" or \"Neville-Beaufort trust\", had as its architect William Gascoigne, one of the crown's most prominent lawyers. This might reflect an interest of the crown in retaining the Neville lands with Beaufort descendants, who would be closely related to the royal family due to their shared Lancastrian ancestry.\n\nWhen Ralph Neville died in 1425, the title earl of Westmorland passed to his eldest grandson Ralph \"in tail male\", but neither of his sons by Margaret Stafford were mentioned in his will. Joan immediately took possession of Middleham Castle, Penrith Castle and Sheriff Hutton Castle for her eldest son. She also held Raby Castle in Durham as part of her dower until her death in 1440. Historian J. R. Lander has written that the second earl of Westmorland was as \"poor in land as an Earl as his father had been in early life as a baron.\" \n\nOnly some estates in Brancepeth, Northumberland, Lincolnshire, two inns in London and Newcastle upon Tyne, Bywell Castle and property in Ripon were left for the senior Neville inheritance. The vast Yorkshire properties of Middleham Castle, Sheriff Hutton Castle and Wensleydale all went to Richard Neville, who also became Warden of the West March when he inherited the Honour of Penrith in Cumberland. His wife, 15-year-old Alice Montagu, was the sole heir of the late Earl of Salisbury, so Richard also inherited the title earl of Salisbury in 1428 when his father-in-law was killed in the Hundred Years' War. \n\nWestmorland spent much of life trying to recover the properties at Middleham, Sheriff Hutton, Penrith and Raby, but he was largely unsuccessful, because Joan Beaufort had powerful allies amongst the nobility including Thomas Langley, the Bishop of Durham, and her brother Cardinal Beaufort. Langley withheld the patronage of the county palatine from the second earl, and denied him any available official offices or positions under the bishop's grant. Westmorland entered into recognisances with the Beaufort-Nevilles in 1430, after Salisbury brought the matter before the King's council. When Salisbury departed for the Hundred Years' War in 1431 and again in 1436, Westmorland was once again bound over to keep the King's peace. However, in 1435, complaints from the North reached the Lord Chancellor that the dispute between the elder and junior branches of the Neville family had resulted in the assembling \"by manner of war and insurrection, great routs and companies upon the field, which have done all manner of great offences\".\n\nWhen the Bishop of Durham died in 1437, Cardinal Beaufort used his influence on the king's council to help Joan's younger son Robert Neville become the new Bishop. The House of Beaufort was able to gradually consolidate its control over the holding. By 1441, Salisbury's younger brother Lord Fauconberg was steward and military commander of Durham. There were a number of attempts to arbitrate a settlement in council, and between 1441 and 1443 both parties were constrained by bonds not to enter each other's estates except with permission. Historian R. L. Storey has questioned—with Salisbury being such a significant member of the council—whether Westmorland ever had \"much faith in its impartiality\".\n\nA peace was finally agreed between Salisbury and Westmorland on 26 August 1443. Pollard writes that the \"settlement\" signified a \"crushing defeat\" for Ralph II and that the \"odds had been heavily stacked\" against him from the start. True, he was confirmed in his right to the Lordship of Raby Castle, but had to surrender everything else he had previously claimed from Salisbury back to him. He was also placed in bonds of £400 to keep to the agreement in the future, not just to Salisbury, but to his four brothers as well. Westmorland had to renounce all claims to the Neville lands in County Durham, and had to pay annual rents to Salisbury for various Northumberland manors. Salisbury was not subject to similar constraints, merely having to agree to not claim the £400 pension while Westmorland adhered to their agreement.\n\nJ.R. Lander described the Neville–Neville feud as illustrating how the Neville family \"never could and never did work together\". The dispute between the senior and junior branches of the Neville family continued into the Wars of the Roses. During the prolonged conflict that ravaged the English nobility, Westmorland gave his half-brother no support at all; in fact Westmorland's younger brother, Lord John Neville died fighting for the Lancastrian Henry VI at the Battle of Towton in 1461. Salisbury himself was captured at the Battle of Wakefield and instead of being ransomed, he was beheaded by the common people, who \"loved him not.\" Lander also suggested that if had been united as a family behind Salisbury, who supported Richard of York during the Wars of the Roses, York's \"power in the land would have been overwhelming\". \n\n\n\n"}
{"id": "8854304", "url": "https://en.wikipedia.org/wiki?curid=8854304", "title": "On Truth", "text": "On Truth\n\nOn Truth is a 2006 book by Harry Frankfurt. It is a follow-up to his 1986 essay, \"On Bullshit\".\n\nIn \"On Truth\", Frankfurt develops the argument that individuals should care about the truth, regardless of whether they intend to be truthful. Frankfurt explicitly avoids a definition of \"truth\" beyond the idea of the commonsense concept of truth people commonly hold, i.e., that which corresponds to reality. \n\nFrankfurt's strategy is to show that the truth, whether an individual is to be truthful or not, is integral to nearly every endeavor, the final point of his argument being that it is a requirement for self-knowledge and therefore all distinctions between ourselves and the world. Frankfurt concludes that the importance of truth, and thus the need to care about it, is incorrigible thereby.\n"}
{"id": "1231526", "url": "https://en.wikipedia.org/wiki?curid=1231526", "title": "Physics and Astronomy Classification Scheme", "text": "Physics and Astronomy Classification Scheme\n\nThe Physics and Astronomy Classification Scheme (PACS) is a scheme developed in 1970 by the American Institute of Physics (AIP) for classifying scientific literature using a hierarchical set of codes. PACS has been used by over 160 international journals, including the \"Physical Review\" series since 1975. Since 2016, American Physical Society introduced the PhySH (Physics Subject Headings) system instead of PACS.\n\nAIP has announced that \"PACS 2010\" will be the final version, but it will continue to be available through their website. The decision was made to discontinue PACS, owing to the administrative complexity of the revision process and its future viability in light of changing technological and research trends. However, PACS is still in use by scientific journals.\n\nIn association with Access Innovations, Inc., the AIP has developed a new \"AIP Thesaurus\", which it states will enable faster, more accurate and more efficient searches.\n\n\n"}
{"id": "23432080", "url": "https://en.wikipedia.org/wiki?curid=23432080", "title": "Possession is nine-tenths of the law", "text": "Possession is nine-tenths of the law\n\nPossession is nine-tenths of the law is an expression meaning that ownership is easier to maintain if one has possession of something, or difficult to enforce if one does not. The expression is also stated as \"possession is nine points of the law\", which is credited as derived from the Scottish expression \"possession is eleven points in the law, and they say there are but twelve.\"\n\nAlthough the principle is an oversimplification, it can be restated as: \"In a property dispute (whether real or personal), in the absence of clear and compelling testimony or documentation to the contrary, the person in actual, custodial possession of the property is presumed to be the rightful owner. The rightful owner shall have their possession returned to them; if taken or used. The shirt or blouse you are currently wearing is presumed to be yours, unless someone can prove that it is not.\"\n\nThe adage is not literally true, that by law the person in possession is presumed to have a nine times stronger claim than anyone else, but that \"it places in a strong light the legal truth that every claimant must succeed by the strength of his own title, and not by the weakness of his antagonist's.\" The principle bears some similarity to uti possidetis (\"as you possess, so may you continue to possess\"), which currently refers to the doctrine that colonial administrative boundaries become international boundaries when a political subdivision or colony achieves independence. Under Roman law, it was an interdictum ordering the parties to maintain possession of property until it was determined who owned the property.\n\nIn the Hatfield-McCoy feud, with testimony evenly divided, the doctrine that possession is nine-tenths of the law caused Floyd Hatfield to retain possession of the pig that the McCoys claimed was their property. It has been argued that in some situations, possession is ten-tenths of the law. While the concept is older, the phrase \"Possession is nine-tenths of the law\" is often claimed to date from the 16th century. In some countries, possession is not nine-tenths of the law, but rather the onus is on the possessor to substantiate his ownership.\n\nThis concept has been applied to both tangible and intangible products. In particular, \"knowledge management\" presents problems with regard to this principle. Google's possession of a large amount of content has been the cause of some wariness due to this principle. It has been said that there was a time in which the attitude towards rights over genetic resources was that possession is nine tenths of the law, and for the other tenth reliance could be made on\nthe principle that biological resources were the heritage of mankind.\n\nAboriginal people frequently encounter this principle. There is some question as to whether the principle applies to Native American land claims. It has been said that “squatter's rights” and “possession is 9/10ths of the law” were largely responsible\nfor how the American west was really won.\n\nWalter Block has stated, \"Suppose that 100 slaves worked on the plantation, but only one heir of any of them, B, can now be found. Does B get the entire value of the landed estate (apart from the house), or only one percent of it. The answer is the latter. For possession is 9/10ths of the law. He who is the present land holder (W in our case) is always deemed to be the proper owner, unless evidence to the contrary can be adduced. But the claim of B, stemming from the work of his grandfather, B, can at most encompass what he, B, that is, contributed to the enhancement of the value of the property. The other ninety-nine percent of the value of this land will remain with W, until and unless other grandchildren of slaves come forth with proof of parentage.\"\n\nMurray Rothbard noted that libertarians \"conclude that even though the property was originally stolen, that if the victim or his heirs cannot be found, and if the current possessor was not the actual criminal who stole the property, then title to that property belongs properly, justly, and ethically to its current possessor.\"\n\n"}
{"id": "30035631", "url": "https://en.wikipedia.org/wiki?curid=30035631", "title": "Principles of grouping", "text": "Principles of grouping\n\nThe principles of grouping (or Gestalt laws of grouping) are a set of principles in psychology, first proposed by Gestalt psychologists to account for the observation that humans naturally perceive objects as organized patterns and objects, a principle known as Prägnanz. Gestalt psychologists argued that these principles exist because the mind has an innate disposition to perceive patterns in the stimulus based on certain rules. These principles are organized into five categories: Proximity, Similarity, Continuity, Closure, and Connectedness.\n\nIrvin Rock and Steve Palmer, who are acknowledged as having built upon the work of Max Wertheimer and others and to have identified additional grouping principles, note that Wertheimer's laws have come to be called the \"Gestalt laws of grouping\" but state that \"perhaps a more appropriate description\" is \"principles of grouping.\"Rock and Palmer helped to further Wertheimer's research to explain human perception of groups of objects and how we perceive \"parts\" of objects and form \"whole\" objects on the basis of these.\n\nThe Gestalt law of proximity states that \"objects or shapes that are close to one another appear to form groups\". Even if the shapes, sizes, and objects are radically different, they will appear as a group if they are close.\n\n\nThe principle of similarity states that, all else being equal, perception lends itself to seeing stimuli that physically resemble each other as part of the same object, and stimuli that are different as part of a different object. This allows for people to distinguish between adjacent and overlapping objects based on their visual texture and resemblance. Other stimuli that have different features are generally not perceived as part of the object. Our brain uses similarity to distinguish between objects which might lie adjacent to or overlap with each other based upon their visual texture. An example of this is a large area of land used by numerous independent farmers to grow crops. Each farmer may use a unique planting style which distinguishes his field from another. Another example is a field of flowers which differ only by color.\n\nThe principles of similarity and proximity often work together to form a Visual Hierarchy. Either principle can dominate the other, depending on the application and combination of the two. For example, in the grid to the left, the similarity principle dominates the proximity principle and you probably see rows before you see columns.\n\nThe principle of closure refers to the mind’s tendency to see complete figures or forms even if a picture is incomplete, partially hidden by other objects, or if part of the information needed to make a complete picture in our minds is missing. For example, if part of a shape’s border is missing people still tend to see the shape as completely enclosed by the border and ignore the gaps. This reaction stems from our mind’s natural tendency to recognize patterns that are familiar to us and thus fill in any information that may be missing.\n\nClosure is also thought to have evolved from ancestral survival instincts in that if one was to partially see a predator their mind would automatically complete the picture and know that it was a time to react to potential danger even if not all the necessary information was readily available.\n\nWhen there is an intersection between two or more objects, people tend to perceive each object as a single uninterrupted object. This allows differentiation of stimuli even when they come in visual overlap. We have a tendency to group and organize lines or curves that follow an established direction over those defined by sharp and abrupt changes in directionamasing...\n\nWhen visual elements are seen moving in the same direction at the same rate (optical flow), perception associates the movement as part of the same stimulus. For example, birds may be distinguished from their background as a single flock because they are moving in the same direction and at the same velocity, even when each bird is seen—from a distance—as little more than a dot. The moving 'dots' appear to be part of a unified whole. Similarly, two flocks of birds can cross each other in a viewer's visual field, but they will nonetheless continue to be experienced as separate flocks because each bird has a direction common to its flock.\n\nThis allows people to make out moving objects even when other details (such as the objects color or outline) are obscured. This ability likely arose from the evolutionary need to distinguish a camouflaged predator from its background.\n\nThe law of common fate is used extensively in user-interface design, for example where the movement of a scrollbar is synchronised with the movement (i.e. cropping) of a window's content viewport; The movement of a physical mouse is synchronised with the movement of an on-screen arrow cursor, and so on.\n\nThe principle of good form refers to the tendency to group together forms of similar shape, pattern, color, etc. Even in cases where two or more forms clearly overlap, the human brain interprets them in a way that allows people to differentiate different patterns and/or shapes. An example would be a pile of presents where a dozen packages of different size and shape are wrapped in just three or so patterns of wrapping paper, or the Olympic Rings.\n\n"}
{"id": "50594027", "url": "https://en.wikipedia.org/wiki?curid=50594027", "title": "Reason Rally", "text": "Reason Rally\n\nThe first Reason Rally was a public gathering for secularism and religious skepticism held on the National Mall in Washington, D.C. on March 24, 2012. The rally was sponsored by major atheistic and secular organizations of the United States and was regarded as a \"Woodstock for atheists and skeptics\". A second Reason Rally was held on June 4, 2016 at the Lincoln Memorial in Washington, D.C..\n\nSpeakers and performers at the first rally included biologist Richard Dawkins, physicist Lawrence M. Krauss, musician Tim Minchin, \"MythBusters\" co-host Adam Savage, actor-comedian Eddie Izzard, Paul Provenza, PZ Myers, Jessica Ahlquist, Dan Barker, and magician James Randi, among others. The punk rock band Bad Religion performed and other notables (Rep. Pete Stark, Sen. Tom Harkin, comedian Bill Maher, magician Penn Jillette) addressed the crowd by video link. Participants recited the Pledge of Allegiance, deliberately omitting the phrase \"under God\", which was added by the U.S. Congress in 1954. Veterans of the U.S. Armed Forces were represented, and a retired Army colonel, Kirk Lamb, led veterans in an affirmation of their secular military oaths. Speakers urged those assembled to contact local and national representatives and ask them to support church-state separation, science education, marriage equality for gays and lesbians, and ending government support of faith-based organizations, among other causes.\n\nAccording to the official website of the first rally, the aim of the Reason Rally was to \"unify, energize, and embolden secular people nationwide, while dispelling the negative opinions held by so much of American society.\" The website had predicted it would be \"the largest secular event in world history.\" \"The Atlantic\" said 20,000 people were in attendance. \"Religion News Service\" said 8,000–10,000. The documentary \"The Unbelievers\" says that over 30,000 people attended the rally. There are no official crowd estimates of events on the Mall.\n\nThe second rally, the Reason Rally for 2016, was billed as \"a celebration of fact-driven public policy, the value of critical thinking, and the voting power of secular Americans\". The weekend of the Rally included advocacy events and conference sessions.\n\nAccording to the first rally's official website, the event had three main goals: \n\nOrganizers said the aim of the rally was twofold: to unite individuals with similar beliefs and to show the American public that the number of people who don’t believe in God is large and growing. “We have the numbers to be taken seriously,” said Paul Fidalgo, spokesman for the Center for Inquiry, which promotes the scientific method and reasoning and was one of the organizations sponsoring the rally. “We’re not just a tiny fringe group.”\n\nAccording to rally spokesman Jesse Galef, diversity with the attendees was a focus this year, he stated 'We can't succeed if we are only coming from one demographic'\". Comparing the 2012 rally to the 2002 Godless rally which was mainly over-40 white men, the attendees were \"largely under the age of 30, at least half female and included many people of color\".\n\nSpeaking to NPR prior to the rally, American Atheist president David Silverman stated that this is a coming-of-age event for atheists, \"We'll look back at the Reason Rally as one of the game-changing events when people started to look at atheism and look at atheists in a different light\".\n\nWith goals of bringing unity, energy, and visibility to the secular demographic, the rally can be seen as a manifestation of the secular movement that emerged in America and elsewhere in the first decade of the twenty-first century. Writing for \"The Guardian\" Sarah Posner states that the Reason Rally was modeled on the LGBT movement, encouraging people to 'come out' about their non-belief and working to humanize atheism by getting \"people to personalize someone they'd always thought of as an 'other.'\" Once people realize that their neighbor, co-worker or family member is an atheist it goes a long way towards acceptance. Politics played a large part of the Rally according to Posner; considering that there is only one openly atheist American Congressperson, there is a lot of work to still be done.\n\n\n\nIn the \"Huffington Post\", Staks Rosch praised the rally. He stated that atheists \"face a great deal of discrimination and fear of discrimination for being outspoken\" and that many \"fear having their families disown them, losing their jobs, or simply being harassed by the religious.\"\n\nDavid Niose, the president of the American Humanist Association stated that \"The secular demographic does not claim to have a monopoly on rationality, but it does feel that it has something to offer. By rallying in Washington, seculars are not whining about some imagined victimization, but rather they are exercising a voice that has been silenced for too long.\"\n\nNate Phelps, an atheist and estranged son of Fred Phelps, the founder of the fringe group, Westboro Baptist Church, supported the Reason Rally and was among the event's speakers.\n\nThe Reason Rally elicited criticism for the anti-theist rhetoric and tone that some speakers employed, since its purpose is to reject religion and superstitions, in favor of employing reason and promoting a secular lifestyle. Editorial writers including Nathalie Rothschild, and Tom Gilson and representatives of various religious communities such as Rabbi Brad Hirschfield of the National Jewish Center for Learning and Leadership and William Anthony Donohue of the Catholic League all voiced disapproval.\n\n\nThe second quadrennial Reason Rally was held on June 4, 2016 at the Lincoln Memorial in Washington, D.C..\n\nThe Reason Rally for 2016 was billed as \"a celebration of fact-driven public policy, the value of critical thinking, and the voting power of secular Americans\". The weekend of the Rally included advocacy events and conference sessions.\n\nEvent organizers were targeting an attendance of 30,000, but crowds proved much smaller; sources disagree on the exact attendance..\n\n\n"}
{"id": "34104355", "url": "https://en.wikipedia.org/wiki?curid=34104355", "title": "Subjective well-being", "text": "Subjective well-being\n\nSubjective well-being (SWB) is a self-reported measure of well-being, typically obtained by questionnaire.\n\nEd Diener developed a tripartite model of subjective well-being in 1984, which describes how people experience the quality of their lives and includes both emotional reactions and cognitive judgments. It posits \"three distinct but often related components of wellbeing: frequent positive affect, infrequent negative affect, and cognitive evaluations such as life satisfaction.\"\n\nSWB therefore encompasses moods and emotions as well as evaluations of one's satisfaction with general and specific areas of one's life. Concepts encompassed by SWB include happiness. \n\nSWB tends to be stable over time and is strongly related to personality traits. There is evidence that health and SWB may mutually influence each other, as good health tends to be associated with greater happiness, and a number of studies have found that positive emotions and optimism can have a beneficial influence on health.\n\nDiener et al. argued that the various components of SWB represent distinct constructs that need to be understood separately, even though they are closely related. Hence, SWB may be considered \"a general area of scientific interest rather than a single specific construct\". Due to the specific focus on the \"subjective\" aspects of well-being, definitions of SWB typically exclude \"objective\" conditions such as material conditions or health, although these can influence ratings of SWB. Definitions of SWB therefore focus on how a person evaluates his/her own life, including emotional experiences of pleasure versus pain in response to specific events and cognitive evaluations of what a person considers a good life. Components of SWB relating to affect include positive affect (experiencing pleasant emotions and moods) and low negative affect (experiencing unpleasant, distressing emotions and moods), as well as \"overall affect\" or \"hedonic balance\", defined as the overall equilibrium between positive and negative affect, and usually measured as the difference between the two. High positive affect and low negative affect are often highly correlated, but not always.\n\nThere are two components of SWB. One is Affective Balance and the other is Life Satisfaction. An individual's scores on the two measures are summed to produce a total SWB score. In some cases, these scores are kept separate.\nAffective balance refers to the emotions, moods, and feelings a person has. These can be all positive, all negative, or a combination of both positive and negative. Some research shows also that feelings of reward are separate from positive and negative affect.\nLife satisfaction (global judgments of one's life) and satisfaction with specific life domains (e.g. work satisfaction) are considered cognitive components of SWB. The term \"happiness\" is also commonly used in regards to SWB and has been defined variously as \"satisfaction of desires and goals\" (therefore related to life satisfaction), as a \"preponderance of positive over negative affect\" (therefore related to emotional components of SWB), as \"contentment\", and as a \"consistent, optimistic mood state\" and may imply an affective evaluation of one's life as a whole. Life satisfaction can also be known as the \"stable\" component in one's life. Affective concepts of SWB can be considered in terms of momentary emotional states as well as in terms of longer-term moods and tendencies (i.e. how much positive and/or negative affect a person generally experiences over any given period of time). Life satisfaction and in some research happiness are typically considered over long durations, up to one's lifetime. \"Quality of life\" has also been studied as a conceptualization of SWB. Although its exact definition varies, it is usually measured as an aggregation of well-being across several life domains and may include both subjective and objective components.\n\nLife satisfaction and Affect balance are generally measured separately and independently. \nSometimes a single SWB question attempts to capture an overall picture.\n\nThe issue with the such measurements of life satisfaction and affective balance is that they are self-reports. The problem with self-reports is that the participants may be lying or at least not telling the whole truth on the questionnaires. Participants may be lying or holding back from revealing certain things because they are either embarrassed or they may be filling in what they believe the researcher wants to see in the results. To gain more accurate results, other methods of measurement have been used to determine one’s SWB. \n\nAnother way to corroborate or confirm that the self-report results are accurate is through informant reports. Informant reports are given to the participant’s closest friends and family and they are asked to fill out either a survey or a form asking about the participants mood, emotions, and overall lifestyle. The participant may write in the self-report that they are very happy, however that participant’s friends and family record that he/she is always depressed. This would obviously be a contradiction in results which would ultimately lead to inaccurate results. \n\nAnother method of gaining a better understanding of the true results is through ESM, or the Experience Sampling Method. In this measure, participants are given a beeper/pager that will randomly ring throughout the day. Whenever the beeper/pager sounds, the participant will stop what he/she is doing and record the activity they are currently engaged in and their current mood and feelings. Tracking this over a period of a week or a month will give researchers a better understanding of the true emotions, moods, and feelings the participant is experiencing, and how these factors interact with other thoughts and behaviors. A third measurement to ensure validity is the Day Reconstruction Method. In this measure, participants fill out a diary of the previous days’ activities. The participant is then asked to describe each activity and provide a report of how they were feeling, what mood they were experiencing, and any emotions that surfaced. Thus to ensure valid results, a researcher may tend to use self-reports along with another form of measurement mentioned above. Someone with a high level of life satisfaction and a positive affective balance is said to have a high level of SWB.\n\nTheories of the causes of SWB tend to emphasise either top-down or bottom-up influences.\n\nIn the top-down view, global features of personality influence the way a person perceives events. Individuals may therefore have a global tendency to perceive life in a consistently positive or negative manner, depending on their stable personality traits. Top-down theories of SWB suggest that people have a genetic predisposition to be happy or unhappy and this predisposition determines their SWB \"setpoint\". Set Point theory implies that a person's baseline or equilibrium level of SWB is a consequence of hereditary characteristics and therefore, almost entirely predetermined at birth. Evidence for this genetic predisposition derives from behavior-genetic studies that have found that positive and negative affectivity each have high heritability (40% and 55% respectively in one study). Numerous twin studies confirm the notion of set point theory, however, they do not rule out the possibility that is it possible for individuals to experience long term changes in SWB.\n\nDiener et al. note that heritability studies are limited in that they describe long-term SWB in a sample of people in a modern western society but may not be applicable to more extreme environments that might influence SWB and do not provide absolute indicators of genetic effects. Additionally, heritability estimates are inconsistent across studies.\n\nFurther evidence for a genetically influenced predisposition to SWB comes from findings that personality has a large influence on long-term SWB. This has led to the \"dynamic equilibrium model\" of SWB. This model proposes that personality provides a baseline for emotional responses. External events may move people away from the baseline, sometimes dramatically, but these movements tend to be of limited duration, with most people returning to their baseline eventually.\n\nFrom a bottom-up perspective, happiness represents an accumulation of happy experiences. Bottom-up influences include external events, and broad situational and demographic factors, including health and marital status. Bottom-up approaches are based on the idea that there are universal basic human needs and that happiness results from their fulfilment. In support of this view, there is evidence that daily pleasurable events are associated with increased positive affect, and daily unpleasant events or hassles are associated with increased negative affect.\n\nHowever, research suggests that external events account for a much smaller proportion of the variance in self-reports of SWB than top-down factors, such as personality. A theory proposed to explain the limited impact of external events on SWB is hedonic adaptation. Based originally on the concept of a \"hedonic treadmill\", this theory proposes that positive or negative external events temporarily increase or decrease feelings of SWB, but as time passes people tend to become habituated to their circumstances and have a tendency to return to a personal SWB \"setpoint\" or baseline level.\n\nThe hedonic treadmill theory originally proposed that most people return to a neutral level of SWB (i.e. neither happy nor unhappy) as they habituate to events. However, subsequent research has shown that for most people, the baseline level of SWB is at least mildly positive, as most people tend to report being at least somewhat happy in general and tend to experience positive mood when no adverse events are occurring. Additional refinements to this theory have shown that people do not adapt to all life events equally, as people tend to adapt rapidly to some events (e.g. imprisonment), slowly to others (e.g. the death of a loved one), and not at all to others (e.g. noise and sex).\n\nA number of studies have found that SWB constructs are strongly associated with a range of personality traits, including those in the five factor model. Findings from numerous personality studies show that genetics account for 20-48% of the variance in Five-Factor Model and the variance in subjective well-being is also heritable. Specifically, neuroticism predicts poorer subjective well-being whilst extraversion, agreeableness, conscientiousness and openness to experience tend to predict higher subjective well-being. A meta-analysis found that neuroticism, extraversion, agreeableness, and conscientiousness were significantly related to all facets of SWB examined (positive, negative, and overall affect; happiness; life satisfaction; and quality of life). Neuroticism was the strongest predictor of overall SWB and is the strongest predictor of negative affect.\n\nA large number of personality traits are related to SWB constructs, although intelligence has negligible relationships. Positive affect is most strongly predicted by extraversion, to a lesser extent agreeableness, and more weakly by openness to experience. Happiness was most strongly predicted by extraversion, and also strongly predicted by neuroticism, and to a lesser extent by the other three factors. Life satisfaction was significantly predicted by neuroticism, extraversion, agreeableness, and conscientiousness. Quality of life was very strongly predicted by neuroticism, and also strongly predicted by extraversion and conscientiousness, and to a modest extent by agreeableness and openness to experience. One study found that subjective well-being was genetically indistinct from personality traits, especially those that reflected emotional stability (low Neuroticism), and social and physical activity (high Extraversion), and constraint (high Conscientiousness).\n\nDeNeve (1999) argued that there are three trends in the relationship between personality and SWB. Firstly, SWB is closely tied to traits associated with emotional tendencies (emotional stability, positive affectivity, and tension). Secondly, relationship enhancing traits (e.g. trust, affiliation) are important for subjective well-being. Happy people tend to have strong relationships and be good at fostering them. Thirdly, the way people think about and explain events is important for subjective well-being. Appraising events in an optimistic fashion, having a sense of control, and making active coping efforts facilitates subjective well-being. Trust, a trait substantially related to SWB, as opposed to cynicism involves making positive rather than negative attributions about others. Making positive, optimistic attributions rather than negative pessimistic ones facilitates subjective well-being.\n\nThe related trait of eudaimonia or psychological well-being, is also heritable. Evidence from one study supports 5 independent genetic mechanisms underlying the Ryff facets of psychological well-being, leading to a genetic construct of eudaimonia in terms of general self-control, and four subsidiary biological mechanisms enabling the psychological capabilities of purpose, agency, growth, and positive social relations\n\nA person's level of subjective well-being is determined by many different factors and social influences prove to be a strong one. Results from the famous Framingham Heart Study indicate that friends three degrees of separation away (that is, friends of friends of friends) can affect a person's happiness. From abstract: \"A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25%.\"\n\nResearch indicates that wealth is related to many positive outcomes in life. Such outcomes include: improved health and mental health, greater longevity, lower rates of infant mortality, experience fewer stressful life events, and less frequently the victims of violent crimes However, research suggests that wealth has a smaller impact on SWB than people generally think, even though higher incomes do correlate substantially with life satisfaction reports.\n\nThe relative influence of wealth together with other material components on overall subjective well-being of a person is being studied through new researches. The Well-being Project at Human Science Lab investigates how material well-being and perceptual well-being works as relative determinants in conditioning our mind for positive emotions.\n\nIn a study done by Aknin, Norton, & Dunn (2009), researchers asked participants from across the income spectrum to report their own happiness and to predict the happiness of others and themselves at different income levels. In study 1, predicted happiness ranged between 2.4-7.9 and actual happiness ranged between 5.2-7.7. In study 2, predicted happiness ranged between 15-80 and actual happiness ranged between 50-80. These findings show that people believe that money does more for happiness than it really does. However, some research indicates that while socioeconomic measures of status do not correspond to greater happiness, measures of sociometric status (status compared to people encountered face-to-face on a daily basis) do correlate to increased subjective well-being, above and beyond the effects of extroversion and other factors.\n\nThe Easterlin Paradox also suggests that there is no connection between a society's economic development and its average level of happiness. Through time, the Easterlin has looked at the relationship between happiness and Gross Domestic Product (GDP) across countries and within countries. There are three different phenomena to look at when examining the connection between money and Subjective well-being; rising GDP within a country, relative income within a country, and differences in GDP between countries.\n\nMore specifically, when making comparisons between countries, a principle called the Diminishing Marginal Utility of Income (DMUI) stands strong. Veenhoven (1991) said, \"[W]e not only see a clear positive relationship [between happiness and GNP per capita], but also a curvilinear pattern; which suggest that wealth is subject to a law of diminishing happiness returns.\" Meaning a $1,000 increase in real income, becomes progressively smaller the higher the initial level of income, having less of an impact on subjective well-being. Easterlin (1995) proved that the DMUI is true when comparing countries, but not when looking at rising gross domestic product within countries.\n\nThere are substantial positive associations between health and SWB so that people who rate their general health as \"good\" or \"excellent\" tend to experience better SWB compared to those who rate their health as \"fair\" or \"poor\". A meta-analysis found that self-ratings of general health were more strongly related to SWB than physician ratings of health. The relationship between health and SWB may be bidirectional. There is evidence that good subjective well-being contributes to better health.\nA review of longitudinal studies found that measures of baseline subjective well-being constructs such as optimism and positive affect predicted longer-term health status and mortality. Conversely, a number of studies found that baseline depression predicted poorer longer-term health status and mortality. Baseline health may well have a causal influence on subjective well-being so causality is difficult to establish.\nA number of studies found that positive emotions and optimism had a beneficial impact on cardiovascular health and on immune functioning. Changes in mood are also known to be associated with changes in immune and cardiovascular response.\nThere is evidence that interventions that are successful in improving subjective well-being can have beneficial effects on aspects of health. For example, meditation and relaxation training have been found to increase positive affect and to reduce blood pressure. The effect of specific types of subjective well-being is not entirely clear. For example, how durable the effects of mood and emotions on health are remains unclear. Whether some types of subjective well-being predict health independently of others is also unclear. Meditation has the power to increase happiness because it can improve self-confidence and reduces anxiety, which increases your well-being. Cultivating personal strengths and resources, like humour, social/animal company, and daily occupations, also appears to help people preserve acceptable levels of SWB despite the presence of symptoms of depression, anxiety, and stress.\n\nResearch suggests that probing a patient's happiness is one of the most important things a doctor can do to predict that patient's health and longevity. In health-conscious modern societies, most people overlook the emotions as a vital component of one's health, while over focusing on diet and exercise. According to Diener & Biswas-Diener, people who are happy become less sick than people who are unhappy. There are three types of health: morbidity, survival, and longevity. Evidence suggests that all three can be improved through happiness:\n\n\nA positive relationship has been found between the volume of gray matter in the right precuneus area of the brain, and the subject's subjective happiness score. A 6 week mindfulness based intervention was found to correlate with a significant gray matter increase within the precuneus.\n\nThere are a number of domains that are thought to contribute to subjective well-being. In a study by Hribernik and Mussap (2010), leisure satisfaction was found to predict unique variance in life satisfaction, supporting its inclusion as a distinct life domain contributing to subjective well-being. Additionally, relationship status interacted with age group and gender on differences in leisure satisfaction. The relationship between leisure satisfaction and life satisfaction, however, was reduced when considering the impact of core affect (underlying mood state). This suggests that leisure satisfaction may primarily be influenced by an individual's subjective well-being level as represented by core affect. This has implications for possible limitations in the extent to which leisure satisfaction may be improved beyond pre-existing levels of well-being and mood in individuals.\n\nAlthough all cultures seem to value happiness, cultures vary in how they define happiness. There is also evidence that people in more individualistic cultures tend to rate themselves as higher in subjective well-being compared to people in more collectivistic cultures.\n\nIn Western cultures, predictors of happiness include elements that support personal independence, a sense of personal agency, and self-expression. In Eastern cultures, predictors of happiness focus on an interdependent self that is inseparable from significant others. Compared to people in individualistic cultures, people in collectivistic cultures are more likely to base their judgments of life satisfaction on how significant others appraise their life than on the balance of inner emotions experienced as pleasant versus unpleasant. Pleasant emotional experiences have a stronger social component in East Asian cultures compared to Western ones. For example, people in Japan are more likely to associate happiness with interpersonally engaging emotions (such as friendly feelings), whereas people in the United States are more likely to associate happiness with interpersonally disengaging emotions (pride, for example). There are also cultural differences in motives and goals associated with happiness. For example, Asian Americans tend to experience greater happiness after achieving goals that are pleasing to or approved of by significant others compared to European Americans. There is also evidence that high self-esteem, a sense of personal control and a consistent sense of identity relate more strongly to SWB in Western cultures than they do in Eastern ones. However, this is not to say that these things are unimportant to SWB in Eastern cultures. Research has found that even within Eastern cultures, people with high self-esteem and a more consistent sense of identity are somewhat happier than those who are low in these characteristics. There is no evidence that low self-esteem and so on are actually beneficial to SWB in any known culture.\n\nA large body of research evidence has confirmed that people in individualistic societies report higher levels of happiness than people in collectivistic ones and that socioeconomic factors alone are insufficient to explain this difference. In addition to political and economic differences, individualistic versus collectivistic nations reliably differ in a variety of psychological characteristics that are related to SWB, such as emotion norms and attitudes to the expression of individual needs. Collectivistic cultures are based around the belief that the individual exists for the benefit of the larger social unit, whereas more individualistic cultures assume the opposite. Collectivistic cultures emphasise maintaining social order and harmony and therefore expect members to suppress their personal desires when necessary in order to promote collective interests. Such cultures therefore consider self-regulation more important than self-expression or than individual rights. Individualistic cultures by contrast emphasise the inalienable value of each person and expect individuals to become self-directive and self-sufficient. Although people in collectivistic cultures may gain happiness from the social approval they receive from suppressing self-interest, research seems to suggest that self-expression produces a greater happiness \"payoff\" compared to seeking approval outside oneself.\n\nPositive psychology is particularly concerned with the study of SWB. Positive psychology was founded by Seligman and Csikszentmihalyi (2000) who identified that psychology is not just the study of pathology, weakness, and damage; but it is also the study of strength and virtue. Researchers in positive psychology have pointed out that in almost every culture studied the pursuit of happiness is regarded as one of the most valued goals in life. Understanding individual differences in SWB is of key interest in positive psychology, particularly the issue of why some people are happier than others. Some people continue to be happy in the face of adversity whereas others are chronically unhappy at the best of times. \n\nPositive psychology has investigated how people might improve their level of SWB and maintain these improvements over the longer term, rather than returning to baseline. Lyubomirsky (2001) argued that SWB is influenced by a combination of personality/genetics (studies have found that genetic influences usually account for 35-50% of the variance in happiness measures), external circumstances, and activities that affect SWB. She argued that changing one's external circumstances tends to have only a temporary effect on SWB, whereas engaging in activities (mental and/or physical) that enhance SWB can lead to more lasting improvements in SWB.\nSWB is often used in appraising the wellbeing of populations.\n\n\n"}
{"id": "46923624", "url": "https://en.wikipedia.org/wiki?curid=46923624", "title": "Sustainable markets", "text": "Sustainable markets\n\nSustainable markets can be loosely defined as those that contribute to stronger livelihoods and more sustainable environments. In linking with the pursuit of ‘sustainable development’, such markets have a multiple focus on social, environmental and economic outcomes. Sustainable markets aim to reflect the true costs (or externalities) of natural resource degradation, environmental pollution, and promote just and safe labour practices.\n\nIn transitioning to sustainable markets, formal rules sometimes called market governance mechanisms (MGMS)are key potential tools to shape and govern markets. Examples could include Fairtrade certification, sustainable reporting and metrics, payments for ecosystem services or other market-based instruments. MGMs change the behaviour of consumers, investors or producers so that their decisions result in more sustainable outcomes. MGMs could provide economic signals or incentives, for example in pricing externalities. Regulatory mechanisms could prohibit or require certain practices by consumers or producers. Or cooperative mechanisms could create voluntary or more formal partnerships around environmental norms or standards.\n\nThere are a number of contentious issues and ongoing debates around sustainable markets, particularly in how to achieve sustainable markets. There are questions around the appropriate mix of policy instruments or mechanisms, and for which context. For example, a developing world country may require specific mechanisms to build sustainable markets, particularly where more informal or fledgling economies exist. These also involve wider debates over how economic globalisation and trade can benefit countries where local capacities or institutions are weak.\n\nFurthermore, there are unresolved debates over how much regulation or government intervention is appropriate in order to govern sustainable markets. For example, in pricing pollution like greenhouse gases (GHGs), there are competing arguments over whether there should be government-mandated taxes to limit pollution by economic disincentives or whether a market should determine the price to pollute. In many cases a mixture of both may be favourable.\n\nBroader questions remain over whether sustainable markets can ever fully account for environmental externalities, such as those associated with pollution. This relates to whether an economic model focused on GDP rather than wellbeing can truly bring about the transformation outlined in sustainable development. There have been suggestions that a move away from a pure focus on GDP is required, towards an emphasis represented in methods such as natural capital accounting.\n\nA number of organizations are working in the sustainable markets field.\n\n"}
{"id": "26975661", "url": "https://en.wikipedia.org/wiki?curid=26975661", "title": "Symptoms of victimization", "text": "Symptoms of victimization\n\nVictimization refers to a person being made into a victim by someone else and can take on psychological as well as physical forms, both of which are damaging to victims. Forms of victimization include (but are not limited to) bullying or peer victimization, physical abuse, sexual abuse, verbal abuse, robbery, and assault. Some of these forms of victimization are commonly associated with certain populations, but they can happen to others as well. For example, bullying or peer victimization is most commonly studied in children and adolescents but also takes place between adults. Although anyone may be victimized, particular groups (e.g. children, the elderly, individuals with disabilities) may be more susceptible to certain types of victimization and as a result to the symptoms and consequences that follow. Individuals respond to victimization in a wide variety of ways, so noticeable symptoms of victimization will vary from person to person. These symptoms may take on several different forms (e.g. psychological, behavioral, or physical), be associated with specific forms of victimization, and be moderated by individual characteristics of the victim and/or experiences after victimization.\n\nSymptoms of victimization may include negative physical, psychological, or behavioral consequences that are direct or indirect responses (see physical symptoms section) to victimization experiences. Symptoms in these categories sometimes overlap, are closely related, or cause each other. For example, a behavioral symptom such as an increase in aggressiveness or irritability may be part of a particular psychological outcome such as posttraumatic stress disorder. Much of the research on symptoms of victimization is cross-sectional (researchers only collected data at one point in time). From a research perspective this means that the symptoms are associated with victimization, but the causal relationship is not always established and alternative explanations have not been ruled out. Some of the symptoms described also may put individuals at risk for victimization. For example, there may be a two-way relationship between victimization and certain internalizing symptoms such as depression or withdrawal, such that victimization increases these symptoms, and individuals exhibiting these symptoms may be targeted for victimization more often than others.\n\nThe experience of being victimized may cause an individual to feel vulnerable or helpless, as well as changing their view of the world and/or their self-perception; the psychological distress this causes may manifest in a number of ways. Diagnosable psychological disorders that are associated with victimization experiences include depression, anxiety, and post-traumatic stress disorder (PTSD). Psychological symptoms that are disruptive to a person's life may be present in some form even if they do not meet diagnostic criteria for a specific disorder. A variety of symptoms such as withdrawal, avoidance, and nightmares, may be part of one of these diagnosable disorders or may occur in milder or more isolated form; diagnoses of particular disorders require that these symptoms have a particular degree of severity or frequency, or that an individual exhibits a certain number of them in order to be formally diagnosed.\n\nDepression has been found to be associated with many forms of victimization, including sexual victimization, violent crime, property crime, peer victimization, and domestic abuse. Indicators of depression include irritable or sad mood for prolonged periods of time, lack of interest in most activities, significant changes in weight/appetite, activity, and sleep patterns, loss of energy and concentration, excessive feelings of guilt or worthlessness, and suicidality. The loss of energy, interest, and concentration associated with depression may impact individuals who have experienced victimization academically or professionally. Depression can impact many other areas of a person's life as well, including interpersonal relationships and physical health. Depression in response to victimization may be lethal, as it can result in suicidal ideation and suicide attempts. Examples of this include a ten-fold increase found in suicide attempts among rape victims compared to the general population, and significant correlations between being victimized in school and suicidal ideation.\n\nA connection between victimization and anxiety has been established for both children and adults. The particular types of anxiety studied in relation to victimization vary; some research references anxiety as a general term while other research references more specific types such as social anxiety. The term anxiety covers a range of difficulties and several specific diagnoses, including panic attacks, phobias, and generalized anxiety disorder. Panic attacks are relatively short, intense bursts of fear that may or may not have a trigger (a cause in the immediate environment that happens right before they occur). They are sometimes a part of other anxiety disorders. Phobias may be specific to objects, situations, people, or places. They can result in avoidance behaviors or, if avoidance is not possible, extreme anxiety or panic attacks. Generalized anxiety is characterized by long-term, uncontrolled, intense worrying in addition to other symptoms such as irritability, sleep problems, or restlessness. Anxiety has been shown to disrupt many aspects of people's lives as well, e.g. academic functioning, and to predict worse health outcomes later in life.\n\nPosttraumatic stress disorder (PTSD) is a specific anxiety disorder in response to a traumatic event in a person's life. It is often discussed in the context of mental health of combat veterans, but also occurs in individuals who have been traumatized in other ways, such as victimization. PTSD involves long-term intense fear, re-experiencing the traumatic event (e.g. nightmares), avoidance of reminders of the event, and being highly reactive (e.g. easily enraged or startled). It may include feeling detached from other people, self-guilt, and difficulty sleeping. Individuals with PTSD may experience a number of symptoms similar to those experienced in both anxiety and depression.\n\nIn addition to the established diagnostic criteria for PTSD, Frank Ochberg proposed a specific set of victimization symptoms (not formally recognized in diagnostic systems such as the DSM or ICD) that includes shame, self-blame, obsessive hatred of the person who victimized them alongside conflicting positive feelings toward that person, feeling defiled, being sexually inhibited, despair or resignation to the situation, secondary victimization (described below), and risk of revictimization.\n\nAdditional symptoms of victimization may take on physical or behavioral forms. These may be direct, individual symptoms of victimization, or they may result from the psychological outcomes described above.\n\nThe most direct and obvious physical symptoms of victimization are injuries as a result of an aggressive physical action such as assault or sexual victimization. Other physical symptoms that are not a result of injury may be indirectly caused by victimization through psychological or emotional responses. Physical symptoms with a psychological or emotional basis are called psychosomatic symptoms. Common psychosomatic symptoms associated with victimization include headaches, stomachaches and experiencing a higher frequency of illnesses such as colds and sore throats. Though psychosomatic symptoms are referred to as having psychological causes they have a biological basis as well; stress and other psychological symptoms trigger nervous system responses such as the release of various chemicals and hormones which then affect biological functioning.\n\nIndividuals who have been victimized may also exhibit behavioral symptoms after the experience. Some individuals who have been victimized show externalizing (outwardly directed) behaviors. For example, an individual who has not previously acted aggressively toward others may begin to do so as after being victimized, such as when a child who has been bullied begins to bully others. Aggressive behaviors may be associated with PTSD (described above). Externalizing behaviors associated with victimization include hyperactivity, hypervigilance, and attention problems that may resemble ADHD. Others may exhibit internalizing (inwardly directed) behavioral symptoms. Many internalizing symptoms tend to be more psychological in nature (depression and anxiety are sometimes referred to as internalization), but particular behaviors are indicative of internalization as well. Internalizing behaviors that have been documented in victimized individuals include withdrawing from social contact and avoidance of people or situations.\n\nDrug and alcohol use associated with victimization is sometimes explained as a form of self-medication, or an attempt to alleviate other symptoms resulting from victimization through substance use. Supporting this, alcohol use has been empirically connected to particular symptoms of posttraumatic stress disorder. Sexual abuse in particular has been identified as one significant precursor to serious alcohol use among women, although it is not as well-established as a causal link and may be mediated by PTSD or other psychological symptoms. Connections have been established between victimization and the use of other drugs as well. Drug use in adolescence and peer victimization based on sexual orientation are correlated. Research has drawn connections between substance use and childhood physical abuse in the general population. Drug use has also been connected to both physical and sexual victimization among high risk, incarcerated youth.\n\nSpecific types of victimization have been strongly linked to particular symptoms or outcomes. These symptoms are not exclusively associated with these forms of victimization but have been studied in association with them, possibly because of their relevance to the specific victimization experiences.\n\nSome individuals who have experienced victimization may have difficulty establishing and maintaining intimate relationships. This is not a subset of symptoms that is exclusive to sexual victimization, but the link between sexual victimization and intimacy problems has been particularly well-established in research. These difficulties may include sexual dysfunction, anxiety about sexual relationships, and dating aggression. Those who experience sexual victimization may have these difficulties long-term, as in the case of victimized children who continue to have difficulty with intimacy during adolescence and adulthood. Some research suggests that the severity of these intimacy problems is related directly to the severity of victimization, while other research suggests that self-blame and shame about sexual victimization mediates (causes) the relationship between victimization and outcomes.\n\nOne symptom that has been associated particularly with school-based peer victimization is poor academic functioning. This symptom is not exclusive to peer victimization, but is contextually relevant due to the setting in which such victimization takes place. Studies have shown poor academic functioning to be a result of peer victimization in elementary, middle, and high school in multiple countries. Though academic functioning has commonly been studied in relation to childhood bullying that takes place in schools, it is likely associated with other forms of victimization as well, as both depression and anxiety affect attention and focus.\n\nResearchers have drawn connections between childhood physical abuse and tendencies toward violent or aggressive behaviors both during childhood and later in life. This aligns logically with increases in aggression and reactivity described above (see psychological symptoms section). The increased risk for engaging in aggressive behavior may be an indirect symptom, mediated by changes in the way that individuals process social information. Increased risk does not mean that everyone who was physically victimized during childhood will continue the cycle of violence with their own children or engage in aggressive behaviors to a point that it is highly detrimental or requires legal action; estimated numbers of individuals who do continue this pattern vary based on the type of aggressive behavior being studied. For example, 16-21% of abused and/or neglected children in one particular study were arrested for violent offenses by around the age of 30.\n\nIn psychology, a moderator is a factor that changes the outcome of a particular situation. With regards to victimization, these can take the form of environmental or contextual characteristics, other people’s responses after victimization has occurred, or a victimized person’s internal responses to or views on what they have experienced.\n\nAttributions about a situation or person refer to where an individual places the blame for an event. An individual may have a different response to being victimized and exhibit different symptoms if they interpret the victimization as being their own fault, the fault of the perpetrator of the victimization, or the fault of some other external factor. Attributions also vary by how stable or controllable someone believes a situation to be. Characterological self-blame for victimization (believing that something is one's own fault, that it is a stable characteristic about themselves, and that it is unchangeable or out of their control) has been shown to make victims feel particularly helpless and to have a negative effect on psychological outcomes. While self-blaming attributions have potentially harmful moderating effects on the symptoms of victimization for those who are already prone to self-blame, it is worth noting that self-blame may itself be a result of victimization for some individuals as noted above (see section on PTSD).\n\nVictimized individuals who participate in active forms of coping experience fewer or less severe psychological symptoms after victimization.\nOne form of active coping is seeking help from others. Help-seeking can be informal (e.g. seeking help from friends or family) or formal (e.g. police reporting of victimization). Attributions about victimization may play a role in whether an individual seeks help or from whom they seek it. For example, a recent study showed that children who are being victimized by peers are less likely to seek support from friends or teachers if they attribute victimization to a group factor such as race, and more likely to seek support if they attribute victimization to more individualized personal characteristics. Similarly, adult victims who blame themselves and are ashamed of being victimized may wish to hide the experience from others, and thus be less willing to seek help. Gender may affect willingness seek help as well; men who have been victimized may be less willing to disclose this information and ask for help due to differing societal expectations for men in addition to the shame and stigmatization experienced by both men and women in response to victimization.\n\nThe increased social support that sometimes results from seeking help may alleviate some of the symptoms of victimization and decrease the risk of continued or future victimization. However, seeking help may also make the outcomes and symptoms worse, depending on the context and responses to help-seeking behavior. Help-seeking may be received more positively from some individuals than others; for example, elementary school aged girls who seek social support after victimization may benefit from it socially, while victimized boys of the same age may experience worse social problems as a result of the same support-seeking behaviors. Seeking help may also increase the severity of victimization symptoms if an individual experiences secondary victimization in the form of victim-blaming, being forced to mentally relive a victimization experience, or other negative responses from individuals or institutions from whom they seek help. Secondary victimization has been documented in victims of rape when they seek medical or psychological assistance. It has also been documented in individuals whose victimization results in criminal trials, particularly if the outcomes of those trials were not in the victims' favor.\n"}
{"id": "8589411", "url": "https://en.wikipedia.org/wiki?curid=8589411", "title": "Tempest in a teapot", "text": "Tempest in a teapot\n\nTempest in a teapot (American English), or storm in a teacup (British English), is an idiom meaning a small event that has been exaggerated out of proportion. There are also lesser known or earlier variants, such as \"tempest in a teacup\", \"storm in a cream bowl\", \"tempest in a glass of water\", \"storm in a wash-hand basin\", and \"storm in a glass of water\".\n\nCicero, in the first century BC, in his \"De Legibus\", used a similar phrase in Latin, possibly the precursor to the modern expressions, \"Excitabat enim fluctus in simpulo ut dicitur Gratidius\", translated: \"For Gratidius raised a tempest in a ladle, as the saying is\". Then in the early 3rd century AD, Athenaeus, in the \"Deipnosophistae\", has Dorion ridiculing the description of a tempest in the \"Nautilus\" of Timotheus by saying that he had seen a more formidable storm in a boiling saucepan. The phrase also appeared in its French form \"une tempête dans un verre d'eau\" (a tempest in a glass of water), to refer to the popular uprising in the Republic of Geneva near the end of the 18th century. \n\nOne of the earliest occurrences in print of the modern version is in 1815, where Britain's Lord Chancellor Thurlow, sometime during his tenure of 1783–1792, is quoted as referring to a popular uprising on the Isle of Man as a \"tempest in a teapot\". Also Lord North, Prime Minister of Great Britain, is credited for popularizing this phrase as characterizing the outbreak of American colonists against the tax on tea. This sentiment was then satirized in Carl Guttenberg's 1778 engraving of the \"Tea-Tax Tempest\" (shown above right), where Father Time flashes a magic lantern picture of an exploding teapot to America on the left and Britannia on the right, with British and American forces advancing towards the teapot. Just a little later, in 1825, in the Scottish journal \"Blackwood's Edinburgh Magazine\", a critical review of poets Hogg and Campbell also included the phrase \"tempest in a teapot\". \nThe first recorded instance of the British English version, \"storm in teacup\", occurs in Catherine Sinclair's \"Modern Accomplishments\" in 1838. There are several instances though of earlier British use of the similar phrase \"storm in a wash-hand basin\".\n\nA similar phrase exists in numerous other languages:\n\n\n"}
{"id": "20355262", "url": "https://en.wikipedia.org/wiki?curid=20355262", "title": "Temporal expressions", "text": "Temporal expressions\n\nA temporal expression in a text is a sequence of tokens (words, numbers and characters) that denote time, that is express a point in time, a duration or a frequency. \nExamples:\n\nInitially, temporal expressions were considered a type of named entities and their identification was part of the named entity recognition task. Since the Automatic Content Extraction program in 2004 there has been a separate task identified and called Temporal Expression Recognition and Normalisation (TERN). Timex evaluation is now evaluated in two major temporal annotation challenges: TempEval and i2b2, both of which prefer the TimeML-level TIMEX3 standard.\n\nSimilarly to NER systems, temporal expression taggers have been created either using linguistic grammar-based techniques or statistical models. Hand-crafted grammar-based systems typically obtained better results, but at the cost of months of work by experienced linguists. There are many such systems available now, so creating a temporal expression recognizer from scratch is generally an undesirable duplication of effort. Instead, current approaches focus on novel subclasses of timex.\n\nStatistical systems typically require a large amount of manually annotated training data and are usually applied to the recognition task only (although there is work done using machine learning algorithms to resolve certain ambiguities in the interpretation step).\n\n\n"}
{"id": "19452409", "url": "https://en.wikipedia.org/wiki?curid=19452409", "title": "The Superclass List", "text": "The Superclass List\n\nThe Superclass List is a creation of David Rothkopf which his book \"Superclass: The Global Power Elite and The World They Are Making\" (publ. March 2008) is based upon. There are four key elements of success that unite the members of the Superclass, and gives them unparalleled power over world affairs. These elements are: \"geography\", \"pedigree\", \"networking\" and \"luck\".\nIn the book Rothkopf writes that his list from 2008 contains 6,000 individuals. The grouping is, however, only defined roughly and as a statistical reality. Rothkopf also writes that list (one in a million, globally), is always in flux. (Note, world population is now 6.9 - 7 billion. so, if published today, the list may contain 7,000 names)\n\nRothkopf states that his list is not to be shown in public as there will be so much discussion about who does or does not qualify to be on the list. In interviews he mentions individuals that are on the list. This list contain names that he argues he has verified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"About 20-30 Swedes are on the list\".\n\n\n\n\n\n\n\n\n"}
{"id": "14747827", "url": "https://en.wikipedia.org/wiki?curid=14747827", "title": "The xartcollection", "text": "The xartcollection\n\nThe xartcollection was created in 1968 in Zurich, Switzerland by Sandro Bocola and three partners: Heinz Bütler, Rolf Fehlbaum and Erwin Meierhofer.\n\nXartcollection’s concept was to industrially produce three-dimensional “Multiples” in limited and signed editions. The company’s philosophy was to make contemporary Art available to a large public and its “Multiples” where sold worldwide in art galleries, design- and furniture stores namely at “Wohnbedarf” in Switzerland.\n\nInternationally renowned artists such as Max Bill, Getulio Alviani, Richard Hamilton (artist), Allen Jones (sculptor), Joe Tilson, Peter Phillips (artist) and others participated in this project. Most of xartcollection's \"Multiples\" were produced by the firm of Reif, Relo-Kunststoffe in Lörrach, Germany. \nIn 1971 xartcollection also created \"Xart Walls\" which industrially produced wallpapers designed by known artists such as Jean Tinguely, Niki de Saint Phalle. The \"Xart Walls\" where manufactured by the firm \"Marburg Wallcoverings\" in Kirchhain, Germany.\n\nDespite its international reputation the company went bankrupt and was dissolved in 1973.\n\n\n"}
{"id": "46341991", "url": "https://en.wikipedia.org/wiki?curid=46341991", "title": "Tolerance Act (Sweden)", "text": "Tolerance Act (Sweden)\n\nThe Tolerance Act () was a Swedish law, enacted by Gustav III of Sweden 24 January 1781. It guaranteed freedom of religion and full citizen rights for all Christian immigrants and residents in Sweden. \n\nSince the Uppsala Synod of 1593, Lutheranism had officially been the only religion allowed in Sweden, though the foreign embassies were given dispensation. The Tolerance Act was introduced in line with the ideals of the Age of Enlightenment. There were also an economical aspect, as it would make it easier for foreigners to work in Sweden. By the Act, all Christian immigrants were granted full freedom of religion and guaranteed their right to bring up their children in their faith as well. This act was followed in 1782 by the \"Judereglementet\" (The Jew's Law), which guaranteed the freedom of religion for Jewish immigrants. This Act was a step toward full freedom of religion in Sweden. However, they only applied to immigrants and foreigners, while the Lutheran Swedish citizens were still prohibited their freedom of religion through the \"konventikelplakatet\". \n\nThe Tolerance Act was replaced by the Dissenter Acts (Sweden) from the 1850s, which guaranteed freedom of religion to all Swedish citizens. \n\nThe final act was the law of 1951, which allowed for complete freedom of conscience by allowing citizens from leaving their religion without replacing it with another one, thereby formally also allowing atheism. \n\n"}
{"id": "1169845", "url": "https://en.wikipedia.org/wiki?curid=1169845", "title": "Upset (competition)", "text": "Upset (competition)\n\nAn upset occurs in a competition, frequently in electoral politics or sports, when the party popularly expected to win (the favorite), either loses to or draws/ties a game with an underdog whom the majority expects to lose, defying the conventional wisdom.\n\nThe meaning of the word has sometimes been erroneously attributed to the surprising defeat of the horse Man o' War by the horse Upset (the loss was the only one in Man o' War's career); the term pre-dates that 1919 race by at least several decades. In its sports coverage immediately following Upset's victory, \"the Washington Post\" wrote, \"One might make all sorts of puns about it being an upset.\"\n\nIn 2002, George Thompson, a lexicographic researcher, used the full-text online search capabilities of \"the New York Times\" databases to trace the usage of the verb \"to upset\" and the noun \"upset\". The latter was seen in usage as early as 1877. Thompson's research debunked one popular theory of the term's origin, namely that it was first used after the Thoroughbred racehorse Upset became the only horse to defeat Man o' War in 1919.\n\nThe meaning of the word \"upset\" has long included \"an overthrowing or overturn of ideas, plans, etc.\" (see OED definition 6b), from which the sports definition almost surely derived. \"Upset\" also once referred to \"a curved part of a bridle-bit, fitting over the tongue of the horse\", (now the port of a curb bit) and though the modern sports meaning of \"upset\" was first used far more for horse races than for any other competition, there is no evidence of a connection. The name of the horse \"Upset\" came from the \"trouble\" or \"distress\" meaning of word (as shown by the parallelism of the name of Upset's stablemate, Regret).\n\n"}
{"id": "22618868", "url": "https://en.wikipedia.org/wiki?curid=22618868", "title": "Velleity", "text": "Velleity\n\nVelleity is the lowest degree of volition, a slight wish or tendency.\n\nThe marketer Matt Bailey described it as \"a desire to see something done, but not enough desire to make it happen\".\n\nMatt Bailey expressed an attempt \"to bring it back, as it has more relevance now than ever.\" He writes that:\n\nFriedrich Nietzsche describes the velleity of an artist as a \"desire to \"be\" 'what he is able to represent, conceive, and express'...\" Nietzsche championed the will to power, which can be encapsulated as starting with velleity, in his free-will theorem.\n\nKeith David Wyma refers frequently to the \"concept of velleity\", citing Thomas Aquinas as a pioneer of introducing the idea into philosophy.\n\nPsychologist Avi Sion writes, \"\"Many psychological concepts may only be defined and explained with reference to velleity\".\" (\"Emphasis in original\".) An example he cites is that \"an ordinarily desirable object can only properly be called 'interesting' or 'tempting' to that agent at that time, if he manifests some velleity...\"\nHe distinguishes between the two types of velleity - \"\"to do\" something and one \"not to do\" something...\" Furthermore, he asserts, \"The concept of velleity is also important because it enables us to understand the co-existence of conflicting values.\" A person could thus have \"double velleity\" or \"a mix of velleity for something and a volition for its opposite: the latter dominates, of course, but that does not erase the fact of velleity.\"\n\nKathy Kolbe also lists velleity as a \"key concept of conation.\"\n\n"}
{"id": "36095331", "url": "https://en.wikipedia.org/wiki?curid=36095331", "title": "Vietnamese units of measurement", "text": "Vietnamese units of measurement\n\nVietnamese units of measurement () are the largely decimal units of measurement traditionally used in Vietnam until metrication. The base unit of length is the \"thước\" (chữ Nôm: 𡱩; lit. \"ruler\") or \"xích\" (). Some of the traditional unit names have been repurposed for metric units, such as \"thước\" for the metre, while other traditional names remain in translations of imperial units, such as \"dặm Anh\" for the English mile.\n\nOriginally, many \"thước\" of varying lengths were in use in Vietnam, each used for different purposes. According to Hoàng Phê (1988), the traditional system of units had at least two \"thước\" of different lengths before 1890, the \"thước ta\" (lit. \"our ruler\") or \"thước mộc\" (\"wooden ruler\"), equal to , and the \"thước đo vải\" (\"ruler for measuring cloth\"), equal to . According to historian Nguyễn Đình Đầu, the \"trường xích\" and \"điền xích\" were both equal to , while according to Phan Thanh Hải, there were three main \"thước\": the \"thước đo vải\", from ; the \"thước đo đất\" (\"ruler for measuring land\"), at ; and the \"thước mộc\", from .\n\nWith French colonization, Cochinchina converted to the metric system, the French standard, while Annam and Tonkin continued to use a \"thước đo đất\" or \"điền xích\" equal to . On June 2, 1897, Indochinese Governor-General Paul Doumer decreed that all the variations of \"thước\" (such as \"thước ta\", \"thước mộc\", and \"điền xích\") would be unified at one \"thước ta\" to , effective January 1, 1898, in Tonkin. Annam retained the old standard for measuring land, so distance and area (such as \"sào\") in Annam were 4.7/4 and (4.7/4) times the equivalent units in Tonkin, respectively.\n\nThe following table lists common units of length in Vietnam in the early 20th century, according to a United Nations Statistical Commission handbook:\n\nNotes:\n\nMiscellaneous units:\n\n\nThe following table lists common units of area in Vietnam in the early 20th century, according to the UN handbook:\n\nNotes:\n\nMiscellaneous units:\n\n\nThe following table lists common units of volume in Vietnam in the early 20th century, according to the UN handbook and Thiều Chửu:\n\nAdditionally:\n\nThe following table lists units of volume in use during French administration in Cochinchina:\n\nNotes:\n\nMiscellaneous units:\n\nThe following table lists common units of weight in Vietnam in the early 20th century:\n\nNotes:\n\nUnits for measuring precious metals:\n\nMiscellaneous units:\n\n\nTraditionally, the basic units of Vietnamese currency were \"quan\" (貫, \"quán\"), \"tiền\", and \"đồng\". One \"quan\" was 10 \"tiền\", and one \"tiền\" was between 50 and 100 \"đồng\", depending on the time period.\n\n\nUnder French colonial rule, Vietnam used the units \"hào\", \"xu\", \"chinh\", and \"cắc\". After independence, Vietnam used \"đồng\", \"hào\", and \"xu\", with 1 \"đồng\" equaling 10 \"hào\" or 100 \"xu\". After the Vietnam War, chronic inflation caused both subdivisions to fall out of use, leaving \"đồng\" as the only unit of currency. However, Overseas Vietnamese communities continue to use \"hào\" and \"xu\" to refer to the tenth and hundredth denominations, respectively, of a foreign currency, such as \"xu\" for the American cent.\n\n"}
{"id": "20901492", "url": "https://en.wikipedia.org/wiki?curid=20901492", "title": "World Day of Peace", "text": "World Day of Peace\n\nThe World Day of Peace is a feast day of the Roman Catholic Church dedicated to universal peace, held on 1 January, the Solemnity of Mary, Mother of God. Pope Paul VI established it in 1967, being inspired by the encyclical \"Pacem in Terris\" of Pope John XXIII and with reference to his own encyclical \"Populorum Progressio\". The day was first observed on 1 January 1968.\n\nWorld Day of Peace often has been an occasion on which the Popes made magisterial declarations of social doctrine. Pope Paul VI and Pope John Paul II made important declarations on the Day in each year of their pontificates regarding the United Nations, human rights, women's rights, labor unions, economic development, the right to life, international diplomacy, peace in the Holy Land (Israel), globalization, and terrorism.\n\nIn England and Wales, \"Peace Sunday\" is traditionally observed on the Second Sunday of Ordinary Time, which is the Sunday occurring between 14 and 20 January, inclusive. The British branch of the Pax Christi movement prepares suggested material for it annually.\n\n"}
